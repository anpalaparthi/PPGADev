energy_cvt_10000_v2_ant_seed_1111
is energy measures =  True
feet contact, is_energy_measures =  True
<brax.envs.ant.Ant object at 0x7fa37a53c280>
feet contact and energy
obs_shape
(87,)
action_shape
(8,)
vec env: 
<JaxToTorchWrapper<OrderEnforcing<VectorGymWrapper<brax_custom-ant-v0>>>>
wandb: Currently logged in as: anishapv (qdrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/icaros/Documents/PPGADev/wandb/run-20230711_000501-ueh7k2qo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run energy_cvt_10000_v2_ant_seed_1111
wandb: ‚≠êÔ∏è View project at https://wandb.ai/qdrl/PPGA
wandb: üöÄ View run at https://wandb.ai/qdrl/PPGA/runs/ueh7k2qo
[36m[2023-07-11 00:05:05,299][233954] Environment ant, action_dim=8, obs_dim=87[0m
bounds: 
[(0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.0, 4.0)]
using cvt archive
no kmeans
[36m[2023-07-11 00:05:33,088][233954] Created Scheduler for cma_maega with an archive learning rate of 0.1, and add mode batch, using solution dim 28816 and archive dims [10, 10, 10, 10, 10]. Min threshold is -500.0. Restart rule is no_improvement[0m
[36m[2023-07-11 00:05:51,024][233954] train() took 13.85 seconds to complete[0m
[36m[2023-07-11 00:05:51,024][233954] FPS: 277206.83[0m
[36m[2023-07-11 00:05:55,264][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:05:55,264][233954] Reward + Measures: [[-1.1022901   0.18961999  0.18875167  0.18884633  0.18920268  2.06404948]][0m
[37m[1m[2023-07-11 00:05:55,264][233954] Max Reward on eval: -1.1022901044059399[0m
[37m[1m[2023-07-11 00:05:55,265][233954] Min Reward on eval: -1.1022901044059399[0m
[37m[1m[2023-07-11 00:05:55,265][233954] Mean Reward across all agents: -1.1022901044059399[0m
[37m[1m[2023-07-11 00:05:55,265][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:06:01,058][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:06:01,058][233954] Reward + Measures: [[-28.99154538   0.2148       0.1978       0.18340001   0.20290001
    2.24615359]
 [ -5.73763334   0.16360001   0.16430001   0.16790001   0.18020001
    2.17963791]
 [ 43.89773977   0.1578       0.15650001   0.1655       0.14760001
    2.15738535]
 ...
 [ 20.54288354   0.1346       0.14049999   0.1437       0.14430001
    2.11823082]
 [ 17.21449197   0.13690001   0.1569       0.13970001   0.16230001
    2.16110778]
 [ 65.91938655   0.14140001   0.14         0.14399999   0.14240001
    2.22361588]][0m
[37m[1m[2023-07-11 00:06:01,059][233954] Max Reward on eval: 101.40343808648177[0m
[37m[1m[2023-07-11 00:06:01,059][233954] Min Reward on eval: -75.75609973557293[0m
[37m[1m[2023-07-11 00:06:01,059][233954] Mean Reward across all agents: 6.468395865901223[0m
[37m[1m[2023-07-11 00:06:01,059][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:06:01,082][233954] mean_value=504.8054034995826, max_value=601.4034380864817[0m
[37m[1m[2023-07-11 00:06:01,116][233954] New mean coefficients: [[ 2.5525432  -1.6429224  -0.62518656 -1.617023   -1.9742825  -0.5342162 ]][0m
[37m[1m[2023-07-11 00:06:01,117][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:06:10,009][233954] train() took 8.89 seconds to complete[0m
[36m[2023-07-11 00:06:10,009][233954] FPS: 431955.20[0m
[36m[2023-07-11 00:06:10,011][233954] itr=0, itrs=2000, Progress: 0.00%[0m
[36m[2023-07-11 00:06:21,839][233954] train() took 11.81 seconds to complete[0m
[36m[2023-07-11 00:06:21,839][233954] FPS: 325083.47[0m
[36m[2023-07-11 00:06:26,021][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:06:26,021][233954] Reward + Measures: [[157.05278017   0.170775     0.18264966   0.17299631   0.17406198
    1.96791649]][0m
[37m[1m[2023-07-11 00:06:26,021][233954] Max Reward on eval: 157.0527801673948[0m
[37m[1m[2023-07-11 00:06:26,022][233954] Min Reward on eval: 157.0527801673948[0m
[37m[1m[2023-07-11 00:06:26,022][233954] Mean Reward across all agents: 157.0527801673948[0m
[37m[1m[2023-07-11 00:06:26,022][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:06:31,007][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:06:31,008][233954] Reward + Measures: [[126.72403312   0.1068       0.1214       0.1026       0.1115
    2.1458745 ]
 [123.89075138   0.1015       0.16779999   0.11989999   0.15820001
    2.335706  ]
 [142.33172271   0.12630001   0.11870001   0.11870001   0.1148
    2.36040115]
 ...
 [115.74044195   0.1151       0.123        0.1099       0.1098
    2.19917727]
 [ 10.88860308   0.1261       0.1153       0.10110001   0.1145
    2.27024627]
 [252.21351243   0.1166       0.1295       0.11080001   0.13450001
    2.22084594]][0m
[37m[1m[2023-07-11 00:06:31,008][233954] Max Reward on eval: 454.46071145311[0m
[37m[1m[2023-07-11 00:06:31,009][233954] Min Reward on eval: -72.75880347937346[0m
[37m[1m[2023-07-11 00:06:31,009][233954] Mean Reward across all agents: 100.9737623422086[0m
[37m[1m[2023-07-11 00:06:31,009][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:06:31,021][233954] mean_value=253.83089635215836, max_value=749.3291561434046[0m
[37m[1m[2023-07-11 00:06:31,030][233954] New mean coefficients: [[ 2.0172594  -1.6190914   0.4555403  -1.5464238  -1.6147127  -0.91345644]][0m
[37m[1m[2023-07-11 00:06:31,031][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:06:40,058][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 00:06:40,059][233954] FPS: 425448.03[0m
[36m[2023-07-11 00:06:40,061][233954] itr=1, itrs=2000, Progress: 0.05%[0m
[36m[2023-07-11 00:06:51,639][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 00:06:51,639][233954] FPS: 332266.35[0m
[36m[2023-07-11 00:06:55,845][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:06:55,845][233954] Reward + Measures: [[278.11926985   0.14481999   0.16598967   0.14783733   0.14993399
    1.79599249]][0m
[37m[1m[2023-07-11 00:06:55,846][233954] Max Reward on eval: 278.119269847102[0m
[37m[1m[2023-07-11 00:06:55,846][233954] Min Reward on eval: 278.119269847102[0m
[37m[1m[2023-07-11 00:06:55,846][233954] Mean Reward across all agents: 278.119269847102[0m
[37m[1m[2023-07-11 00:06:55,846][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:07:00,887][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:07:00,887][233954] Reward + Measures: [[261.67949247   0.17029999   0.20870002   0.15270001   0.17850001
    1.80664444]
 [187.33946676   0.11470001   0.1523       0.14100002   0.1041
    2.17220378]
 [217.70171354   0.1279       0.14860001   0.12160001   0.116
    1.97323501]
 ...
 [238.21476453   0.1549       0.14400001   0.15390001   0.1332
    1.87118018]
 [151.87810133   0.1193       0.1772       0.1538       0.1671
    1.99978638]
 [181.12792568   0.1638       0.23260002   0.1618       0.18100002
    1.84373188]][0m
[37m[1m[2023-07-11 00:07:00,888][233954] Max Reward on eval: 465.8375129219145[0m
[37m[1m[2023-07-11 00:07:00,888][233954] Min Reward on eval: 40.172964092716576[0m
[37m[1m[2023-07-11 00:07:00,888][233954] Mean Reward across all agents: 199.4064174992508[0m
[37m[1m[2023-07-11 00:07:00,888][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:07:00,897][233954] mean_value=410.3325662456906, max_value=907.6391312788007[0m
[37m[1m[2023-07-11 00:07:00,900][233954] New mean coefficients: [[ 1.3004769  -0.62675154  0.7320828  -0.8848189  -1.0318581  -0.8569595 ]][0m
[37m[1m[2023-07-11 00:07:00,901][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:07:09,780][233954] train() took 8.88 seconds to complete[0m
[36m[2023-07-11 00:07:09,780][233954] FPS: 432561.57[0m
[36m[2023-07-11 00:07:09,782][233954] itr=2, itrs=2000, Progress: 0.10%[0m
[36m[2023-07-11 00:07:21,332][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 00:07:21,333][233954] FPS: 333088.32[0m
[36m[2023-07-11 00:07:25,571][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:07:25,571][233954] Reward + Measures: [[401.08426942   0.136272     0.16789301   0.14175899   0.14012067
    1.5713737 ]][0m
[37m[1m[2023-07-11 00:07:25,572][233954] Max Reward on eval: 401.0842694218886[0m
[37m[1m[2023-07-11 00:07:25,572][233954] Min Reward on eval: 401.0842694218886[0m
[37m[1m[2023-07-11 00:07:25,572][233954] Mean Reward across all agents: 401.0842694218886[0m
[37m[1m[2023-07-11 00:07:25,572][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:07:30,606][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:07:30,606][233954] Reward + Measures: [[320.49302365   0.19810002   0.22780001   0.23220001   0.1675
    1.67902458]
 [280.88443471   0.1184       0.1903       0.1122       0.1542
    1.70993733]
 [159.60742351   0.1427       0.1925       0.1035       0.16030002
    1.87216055]
 ...
 [267.83703127   0.17400001   0.20420001   0.15330002   0.17810002
    1.64564705]
 [359.39448318   0.13990001   0.1894       0.16739999   0.15269999
    1.62450445]
 [255.85226153   0.1506       0.2277       0.1706       0.2167
    1.65016484]][0m
[37m[1m[2023-07-11 00:07:30,607][233954] Max Reward on eval: 630.5661392793991[0m
[37m[1m[2023-07-11 00:07:30,607][233954] Min Reward on eval: 77.29986935998313[0m
[37m[1m[2023-07-11 00:07:30,607][233954] Mean Reward across all agents: 324.1023205496103[0m
[37m[1m[2023-07-11 00:07:30,607][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:07:30,616][233954] mean_value=422.98862092087535, max_value=1119.0110015707091[0m
[37m[1m[2023-07-11 00:07:30,618][233954] New mean coefficients: [[ 0.55911946 -0.4328613   0.27158806 -0.56632584 -0.75520325 -1.1709707 ]][0m
[37m[1m[2023-07-11 00:07:30,619][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:07:39,597][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 00:07:39,598][233954] FPS: 427785.09[0m
[36m[2023-07-11 00:07:39,600][233954] itr=3, itrs=2000, Progress: 0.15%[0m
[36m[2023-07-11 00:07:51,105][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 00:07:51,105][233954] FPS: 334416.46[0m
[36m[2023-07-11 00:07:55,354][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:07:55,354][233954] Reward + Measures: [[606.80796735   0.14410233   0.20072633   0.155919     0.146722
    1.22717035]][0m
[37m[1m[2023-07-11 00:07:55,355][233954] Max Reward on eval: 606.807967352332[0m
[37m[1m[2023-07-11 00:07:55,355][233954] Min Reward on eval: 606.807967352332[0m
[37m[1m[2023-07-11 00:07:55,355][233954] Mean Reward across all agents: 606.807967352332[0m
[37m[1m[2023-07-11 00:07:55,355][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:08:00,345][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:08:00,346][233954] Reward + Measures: [[277.94519685   0.1542       0.14380001   0.1486       0.1409
    1.18537629]
 [480.76945687   0.18620001   0.19720002   0.1786       0.15110001
    1.27753341]
 [563.64028452   0.1851       0.1798       0.20120001   0.1592
    1.34538925]
 ...
 [516.54099368   0.1752       0.2352       0.1865       0.21900001
    1.20823288]
 [393.48014623   0.1201       0.13270001   0.14210001   0.11189999
    1.26979876]
 [414.77551601   0.2155       0.29700002   0.17680001   0.233
    1.30609417]][0m
[37m[1m[2023-07-11 00:08:00,346][233954] Max Reward on eval: 827.1483954963857[0m
[37m[1m[2023-07-11 00:08:00,347][233954] Min Reward on eval: 172.34671501107513[0m
[37m[1m[2023-07-11 00:08:00,347][233954] Mean Reward across all agents: 483.2576059663133[0m
[37m[1m[2023-07-11 00:08:00,347][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:08:00,361][233954] mean_value=937.0057353500309, max_value=1327.1483954963855[0m
[37m[1m[2023-07-11 00:08:00,369][233954] New mean coefficients: [[ 0.79068154 -0.10516089  1.1299126   0.23886156  0.11421341 -0.4435013 ]][0m
[37m[1m[2023-07-11 00:08:00,370][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:08:09,464][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 00:08:09,464][233954] FPS: 422321.53[0m
[36m[2023-07-11 00:08:09,467][233954] itr=4, itrs=2000, Progress: 0.20%[0m
[36m[2023-07-11 00:08:21,043][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 00:08:21,044][233954] FPS: 332453.60[0m
[36m[2023-07-11 00:08:25,283][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:08:25,284][233954] Reward + Measures: [[769.87935188   0.14109667   0.21235998   0.157131     0.13668366
    1.09213841]][0m
[37m[1m[2023-07-11 00:08:25,284][233954] Max Reward on eval: 769.8793518842591[0m
[37m[1m[2023-07-11 00:08:25,284][233954] Min Reward on eval: 769.8793518842591[0m
[37m[1m[2023-07-11 00:08:25,284][233954] Mean Reward across all agents: 769.8793518842591[0m
[37m[1m[2023-07-11 00:08:25,285][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:08:30,257][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:08:30,263][233954] Reward + Measures: [[464.08295985   0.1673       0.3312       0.2167       0.2138
    1.13237023]
 [557.07789679   0.12330001   0.1969       0.16239999   0.1305
    1.1364882 ]
 [533.18997384   0.20550001   0.18840002   0.15650001   0.1736
    1.31666601]
 ...
 [374.81290821   0.25310001   0.222        0.26560003   0.1561
    1.21550596]
 [488.32790542   0.16610001   0.27260002   0.1279       0.21760002
    1.26902199]
 [295.24264911   0.1494       0.226        0.11550001   0.1939
    1.32269275]][0m
[37m[1m[2023-07-11 00:08:30,264][233954] Max Reward on eval: 1010.2159957441502[0m
[37m[1m[2023-07-11 00:08:30,264][233954] Min Reward on eval: 185.09234545119108[0m
[37m[1m[2023-07-11 00:08:30,265][233954] Mean Reward across all agents: 638.6209240114576[0m
[37m[1m[2023-07-11 00:08:30,265][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:08:30,283][233954] mean_value=554.105617283417, max_value=1441.0962715438975[0m
[37m[1m[2023-07-11 00:08:30,287][233954] New mean coefficients: [[ 0.75477016  0.5491611   0.90847766  0.266368    0.14759138 -1.70805   ]][0m
[37m[1m[2023-07-11 00:08:30,288][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:08:39,308][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 00:08:39,308][233954] FPS: 425814.45[0m
[36m[2023-07-11 00:08:39,310][233954] itr=5, itrs=2000, Progress: 0.25%[0m
[36m[2023-07-11 00:08:51,041][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 00:08:51,042][233954] FPS: 328048.03[0m
[36m[2023-07-11 00:08:55,269][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:08:55,269][233954] Reward + Measures: [[1259.83562984    0.16490601    0.29574767    0.19839901    0.14655599
     0.88300747]][0m
[37m[1m[2023-07-11 00:08:55,270][233954] Max Reward on eval: 1259.8356298398273[0m
[37m[1m[2023-07-11 00:08:55,270][233954] Min Reward on eval: 1259.8356298398273[0m
[37m[1m[2023-07-11 00:08:55,270][233954] Mean Reward across all agents: 1259.8356298398273[0m
[37m[1m[2023-07-11 00:08:55,270][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:09:00,420][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:09:00,426][233954] Reward + Measures: [[ 636.92174911    0.17299999    0.43320003    0.20480001    0.19760001
     0.97592413]
 [1136.27146533    0.21440001    0.33610001    0.22850001    0.19769999
     0.88059723]
 [ 410.54355712    0.16970001    0.5126        0.22830001    0.32100001
     0.90581304]
 ...
 [ 920.06969836    0.24270001    0.41230002    0.1839        0.18260001
     0.99037737]
 [ 229.20640661    0.182         0.39859998    0.22059999    0.38030002
     1.00732744]
 [1016.73498627    0.1399        0.27019998    0.21089999    0.1508
     0.87397718]][0m
[37m[1m[2023-07-11 00:09:00,426][233954] Max Reward on eval: 1473.343894912675[0m
[37m[1m[2023-07-11 00:09:00,426][233954] Min Reward on eval: 211.33631323967128[0m
[37m[1m[2023-07-11 00:09:00,427][233954] Mean Reward across all agents: 841.265181784566[0m
[37m[1m[2023-07-11 00:09:00,427][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:09:00,446][233954] mean_value=1239.2487313169618, max_value=1973.343894912675[0m
[37m[1m[2023-07-11 00:09:00,448][233954] New mean coefficients: [[ 0.7957248   0.16579846  0.80073243  0.5510509   0.01747696 -0.238698  ]][0m
[37m[1m[2023-07-11 00:09:00,449][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:09:09,441][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 00:09:09,441][233954] FPS: 427127.33[0m
[36m[2023-07-11 00:09:09,444][233954] itr=6, itrs=2000, Progress: 0.30%[0m
[36m[2023-07-11 00:09:21,094][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 00:09:21,094][233954] FPS: 330322.36[0m
[36m[2023-07-11 00:09:25,326][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:09:25,326][233954] Reward + Measures: [[1729.4313392     0.16977401    0.32390931    0.217012      0.14082366
     0.85319763]][0m
[37m[1m[2023-07-11 00:09:25,327][233954] Max Reward on eval: 1729.4313392017161[0m
[37m[1m[2023-07-11 00:09:25,327][233954] Min Reward on eval: 1729.4313392017161[0m
[37m[1m[2023-07-11 00:09:25,327][233954] Mean Reward across all agents: 1729.4313392017161[0m
[37m[1m[2023-07-11 00:09:25,327][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:09:30,319][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:09:30,319][233954] Reward + Measures: [[1285.04080961    0.14570001    0.3186        0.18340002    0.13160001
     0.94109023]
 [ 708.48317717    0.2462        0.41990003    0.2422        0.24659999
     0.78086442]
 [1165.96015936    0.16010001    0.2145        0.17809999    0.14920001
     0.89407885]
 ...
 [1034.32683757    0.19950001    0.36739999    0.24630001    0.1548
     0.80929643]
 [1364.34004972    0.148         0.25079998    0.21659999    0.125
     0.90333575]
 [1076.83271792    0.24139999    0.3856        0.34060001    0.19540001
     1.08890629]][0m
[37m[1m[2023-07-11 00:09:30,320][233954] Max Reward on eval: 1845.9553375297226[0m
[37m[1m[2023-07-11 00:09:30,320][233954] Min Reward on eval: 285.72109789876265[0m
[37m[1m[2023-07-11 00:09:30,320][233954] Mean Reward across all agents: 1210.0393284734027[0m
[37m[1m[2023-07-11 00:09:30,320][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:09:30,331][233954] mean_value=519.9027046428914, max_value=1752.6123596175853[0m
[37m[1m[2023-07-11 00:09:30,339][233954] New mean coefficients: [[ 0.7145786   0.21968515  0.43385023  0.12013993  0.01217101 -0.6004617 ]][0m
[37m[1m[2023-07-11 00:09:30,340][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:09:39,482][233954] train() took 9.14 seconds to complete[0m
[36m[2023-07-11 00:09:39,483][233954] FPS: 420079.90[0m
[36m[2023-07-11 00:09:39,485][233954] itr=7, itrs=2000, Progress: 0.35%[0m
[36m[2023-07-11 00:09:51,254][233954] train() took 11.75 seconds to complete[0m
[36m[2023-07-11 00:09:51,254][233954] FPS: 326899.42[0m
[36m[2023-07-11 00:09:55,545][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:09:55,545][233954] Reward + Measures: [[2245.41622833    0.17369433    0.33439934    0.22436865    0.13068901
     0.80506378]][0m
[37m[1m[2023-07-11 00:09:55,545][233954] Max Reward on eval: 2245.4162283259507[0m
[37m[1m[2023-07-11 00:09:55,546][233954] Min Reward on eval: 2245.4162283259507[0m
[37m[1m[2023-07-11 00:09:55,546][233954] Mean Reward across all agents: 2245.4162283259507[0m
[37m[1m[2023-07-11 00:09:55,546][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:10:00,521][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:10:00,522][233954] Reward + Measures: [[2128.50601192    0.17819999    0.3673        0.228         0.1441
     1.0355047 ]
 [1661.09442902    0.1365        0.2694        0.2079        0.1244
     0.71854573]
 [1929.62474054    0.19599999    0.47320005    0.24440001    0.19410001
     0.92805082]
 ...
 [1681.22807315    0.20650001    0.32729998    0.3229        0.1621
     0.84173805]
 [ 867.17402264    0.1864        0.44899997    0.229         0.24429999
     0.75110418]
 [1118.42702105    0.2155        0.42200002    0.2307        0.2588
     0.9769246 ]][0m
[37m[1m[2023-07-11 00:10:00,522][233954] Max Reward on eval: 2467.7453917883336[0m
[37m[1m[2023-07-11 00:10:00,522][233954] Min Reward on eval: 436.0516152903903[0m
[37m[1m[2023-07-11 00:10:00,522][233954] Mean Reward across all agents: 1554.534128888026[0m
[37m[1m[2023-07-11 00:10:00,523][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:10:00,533][233954] mean_value=439.8466739294767, max_value=2503.147567827301[0m
[37m[1m[2023-07-11 00:10:00,536][233954] New mean coefficients: [[ 0.5694675   0.58636403 -0.08531022  0.35569125 -0.27990335 -1.0543892 ]][0m
[37m[1m[2023-07-11 00:10:00,537][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:10:09,506][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 00:10:09,507][233954] FPS: 428186.04[0m
[36m[2023-07-11 00:10:09,509][233954] itr=8, itrs=2000, Progress: 0.40%[0m
[36m[2023-07-11 00:10:21,218][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 00:10:21,218][233954] FPS: 328680.96[0m
[36m[2023-07-11 00:10:25,636][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:10:25,636][233954] Reward + Measures: [[2651.18289667    0.16994734    0.31611136    0.22159366    0.10896433
     0.73319602]][0m
[37m[1m[2023-07-11 00:10:25,636][233954] Max Reward on eval: 2651.1828966696467[0m
[37m[1m[2023-07-11 00:10:25,637][233954] Min Reward on eval: 2651.1828966696467[0m
[37m[1m[2023-07-11 00:10:25,637][233954] Mean Reward across all agents: 2651.1828966696467[0m
[37m[1m[2023-07-11 00:10:25,637][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:10:30,794][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:10:30,795][233954] Reward + Measures: [[2065.92448429    0.14830001    0.25729999    0.19230001    0.12279999
     0.94108337]
 [1463.20736459    0.1446        0.229         0.18889999    0.1314
     0.8678295 ]
 [1816.90000915    0.1961        0.4172        0.24270001    0.10780001
     0.74821991]
 ...
 [ 496.16612627    0.27350003    0.28950003    0.36250001    0.27340001
     0.6296941 ]
 [1535.92146301    0.24519999    0.41150004    0.2694        0.21440001
     0.93726748]
 [1104.51071933    0.24499999    0.35980001    0.361         0.19410001
     0.69680399]][0m
[37m[1m[2023-07-11 00:10:30,795][233954] Max Reward on eval: 3003.8345642026516[0m
[37m[1m[2023-07-11 00:10:30,795][233954] Min Reward on eval: 152.5339828151744[0m
[37m[1m[2023-07-11 00:10:30,795][233954] Mean Reward across all agents: 1734.0457901586428[0m
[37m[1m[2023-07-11 00:10:30,796][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:10:30,804][233954] mean_value=394.93720188835687, max_value=2533.9681625308817[0m
[37m[1m[2023-07-11 00:10:30,807][233954] New mean coefficients: [[ 0.7804957   0.23249632 -0.6538253  -0.07310832 -0.37078223 -1.6722957 ]][0m
[37m[1m[2023-07-11 00:10:30,808][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:10:39,848][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 00:10:39,849][233954] FPS: 424824.30[0m
[36m[2023-07-11 00:10:39,851][233954] itr=9, itrs=2000, Progress: 0.45%[0m
[36m[2023-07-11 00:10:51,466][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 00:10:51,466][233954] FPS: 331243.06[0m
[36m[2023-07-11 00:10:55,719][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:10:55,725][233954] Reward + Measures: [[2963.95471677    0.16368732    0.27744365    0.203091      0.08769666
     0.65649527]][0m
[37m[1m[2023-07-11 00:10:55,725][233954] Max Reward on eval: 2963.954716774683[0m
[37m[1m[2023-07-11 00:10:55,726][233954] Min Reward on eval: 2963.954716774683[0m
[37m[1m[2023-07-11 00:10:55,726][233954] Mean Reward across all agents: 2963.954716774683[0m
[37m[1m[2023-07-11 00:10:55,726][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:11:00,714][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:11:00,715][233954] Reward + Measures: [[2009.70859533    0.18880002    0.33750001    0.27649999    0.16559999
     0.61452597]
 [1393.34405613    0.11610001    0.19930001    0.14240001    0.0815
     0.64362925]
 [2192.44249347    0.2045        0.32699999    0.22670002    0.1436
     0.71457428]
 ...
 [1879.1847534     0.15190001    0.21139999    0.16809998    0.1073
     0.8090502 ]
 [2334.44838712    0.1561        0.29260001    0.1992        0.0829
     0.63941765]
 [ 884.45774078    0.32740003    0.46269998    0.3222        0.23580001
     0.56809521]][0m
[37m[1m[2023-07-11 00:11:00,715][233954] Max Reward on eval: 3404.623596213199[0m
[37m[1m[2023-07-11 00:11:00,715][233954] Min Reward on eval: 204.25621888702736[0m
[37m[1m[2023-07-11 00:11:00,715][233954] Mean Reward across all agents: 1747.4596648183044[0m
[37m[1m[2023-07-11 00:11:00,716][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:11:00,725][233954] mean_value=877.894226751907, max_value=2512.4140256388105[0m
[37m[1m[2023-07-11 00:11:00,727][233954] New mean coefficients: [[ 1.290246    0.21079805 -0.25356433  0.22382012 -0.12480462 -2.0086663 ]][0m
[37m[1m[2023-07-11 00:11:00,728][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:11:09,717][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 00:11:09,717][233954] FPS: 427278.73[0m
[36m[2023-07-11 00:11:09,720][233954] itr=10, itrs=2000, Progress: 0.50%[0m
[36m[2023-07-11 00:13:43,868][233954] train() took 11.85 seconds to complete[0m
[36m[2023-07-11 00:13:43,919][233954] FPS: 324167.30[0m
[36m[2023-07-11 00:13:48,026][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:13:48,026][233954] Reward + Measures: [[3349.71443365    0.16521834    0.25947502    0.20081401    0.07861733
     0.64413607]][0m
[37m[1m[2023-07-11 00:13:48,026][233954] Max Reward on eval: 3349.7144336500055[0m
[37m[1m[2023-07-11 00:13:48,027][233954] Min Reward on eval: 3349.7144336500055[0m
[37m[1m[2023-07-11 00:13:48,027][233954] Mean Reward across all agents: 3349.7144336500055[0m
[37m[1m[2023-07-11 00:13:48,027][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:13:53,140][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:13:53,140][233954] Reward + Measures: [[3193.22225953    0.2023        0.30610001    0.21830001    0.09370001
     0.66788948]
 [1794.69834898    0.18360001    0.48530003    0.32909998    0.16330002
     0.52752841]
 [1803.28687285    0.1797        0.3475        0.26710001    0.1763
     0.64153504]
 ...
 [1659.04579162    0.20289998    0.28489998    0.23470001    0.1152
     0.67644358]
 [2277.17146396    0.1566        0.3211        0.23670001    0.1028
     0.50429136]
 [1148.86335755    0.22150002    0.53619999    0.35870001    0.19890001
     0.49568921]][0m
[37m[1m[2023-07-11 00:13:53,140][233954] Max Reward on eval: 3505.4815520656293[0m
[37m[1m[2023-07-11 00:13:53,141][233954] Min Reward on eval: 331.36934471912684[0m
[37m[1m[2023-07-11 00:13:53,141][233954] Mean Reward across all agents: 1976.9589741701075[0m
[37m[1m[2023-07-11 00:13:53,141][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:13:53,147][233954] mean_value=516.8854856955763, max_value=2828.7713317792864[0m
[37m[1m[2023-07-11 00:13:53,150][233954] New mean coefficients: [[ 0.92683315  0.6395698  -0.05117765 -0.1744886  -0.3326751  -2.39654   ]][0m
[37m[1m[2023-07-11 00:13:53,151][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:14:02,109][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 00:14:02,109][233954] FPS: 428745.02[0m
[36m[2023-07-11 00:14:02,111][233954] itr=11, itrs=2000, Progress: 0.55%[0m
[36m[2023-07-11 00:14:13,922][233954] train() took 11.79 seconds to complete[0m
[36m[2023-07-11 00:14:13,922][233954] FPS: 325750.26[0m
[36m[2023-07-11 00:14:18,257][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:14:18,257][233954] Reward + Measures: [[3823.92373502    0.16968499    0.25121665    0.20091169    0.070334
     0.63702512]][0m
[37m[1m[2023-07-11 00:14:18,257][233954] Max Reward on eval: 3823.9237350233248[0m
[37m[1m[2023-07-11 00:14:18,258][233954] Min Reward on eval: 3823.9237350233248[0m
[37m[1m[2023-07-11 00:14:18,258][233954] Mean Reward across all agents: 3823.9237350233248[0m
[37m[1m[2023-07-11 00:14:18,258][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:14:23,413][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:14:23,413][233954] Reward + Measures: [[ 269.23455426    0.36660001    0.48380002    0.37310001    0.43959999
     0.53648436]
 [1291.7595978     0.23940001    0.42829999    0.41130003    0.23
     0.50887829]
 [1366.79022983    0.2304        0.37359998    0.46170002    0.2066
     0.51909554]
 ...
 [ 934.42697142    0.27949998    0.44010001    0.42880002    0.24090002
     0.46901676]
 [2041.45686345    0.19849999    0.3592        0.35230002    0.1231
     0.4903582 ]
 [ 592.73185345    0.31329998    0.54990005    0.33870003    0.32519999
     0.5152666 ]][0m
[37m[1m[2023-07-11 00:14:23,413][233954] Max Reward on eval: 3767.6073608762586[0m
[37m[1m[2023-07-11 00:14:23,414][233954] Min Reward on eval: 130.71189926886[0m
[37m[1m[2023-07-11 00:14:23,414][233954] Mean Reward across all agents: 2050.8213444494772[0m
[37m[1m[2023-07-11 00:14:23,414][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:14:23,421][233954] mean_value=227.69874484452205, max_value=2669.161499013286[0m
[37m[1m[2023-07-11 00:14:23,424][233954] New mean coefficients: [[ 0.73126465  0.60780305 -0.43114576  0.9828833   0.005458   -1.6702154 ]][0m
[37m[1m[2023-07-11 00:14:23,425][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:14:32,467][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 00:14:32,467][233954] FPS: 424773.56[0m
[36m[2023-07-11 00:14:32,469][233954] itr=12, itrs=2000, Progress: 0.60%[0m
[36m[2023-07-11 00:14:44,180][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 00:14:44,180][233954] FPS: 328551.12[0m
[36m[2023-07-11 00:14:48,490][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:14:48,490][233954] Reward + Measures: [[4190.62861244    0.17334534    0.23808199    0.20032568    0.06272767
     0.63091636]][0m
[37m[1m[2023-07-11 00:14:48,491][233954] Max Reward on eval: 4190.628612440703[0m
[37m[1m[2023-07-11 00:14:48,491][233954] Min Reward on eval: 4190.628612440703[0m
[37m[1m[2023-07-11 00:14:48,491][233954] Mean Reward across all agents: 4190.628612440703[0m
[37m[1m[2023-07-11 00:14:48,492][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:14:53,470][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:14:53,471][233954] Reward + Measures: [[2986.22608368    0.1728        0.32010004    0.22550002    0.09670001
     0.49687406]
 [2057.20446776    0.17900001    0.39439997    0.37170002    0.1743
     0.48420516]
 [2079.26945114    0.1815        0.35759997    0.3168        0.1428
     0.49204189]
 ...
 [2961.55195616    0.16140001    0.2242        0.17560001    0.0817
     0.60166043]
 [3618.81890869    0.16790001    0.23959999    0.18700001    0.0575
     0.72691053]
 [2551.09345244    0.14700001    0.2139        0.1771        0.0873
     0.77506632]][0m
[37m[1m[2023-07-11 00:14:53,471][233954] Max Reward on eval: 4550.853515653737[0m
[37m[1m[2023-07-11 00:14:53,471][233954] Min Reward on eval: 325.7158289140556[0m
[37m[1m[2023-07-11 00:14:53,472][233954] Mean Reward across all agents: 2457.2252598319815[0m
[37m[1m[2023-07-11 00:14:53,472][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:14:53,476][233954] mean_value=16.057762354032555, max_value=1816.2925226618709[0m
[37m[1m[2023-07-11 00:14:53,479][233954] New mean coefficients: [[ 0.5039664   0.08027649 -0.4466299   1.2914813  -0.1979555  -0.30108523]][0m
[37m[1m[2023-07-11 00:14:53,480][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:15:02,487][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 00:15:02,488][233954] FPS: 426367.12[0m
[36m[2023-07-11 00:15:02,490][233954] itr=13, itrs=2000, Progress: 0.65%[0m
[36m[2023-07-11 00:15:14,073][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 00:15:14,073][233954] FPS: 332282.11[0m
[36m[2023-07-11 00:15:18,343][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:15:18,343][233954] Reward + Measures: [[4605.78748525    0.17770332    0.226124      0.19981869    0.053862
     0.65672272]][0m
[37m[1m[2023-07-11 00:15:18,344][233954] Max Reward on eval: 4605.787485249361[0m
[37m[1m[2023-07-11 00:15:18,344][233954] Min Reward on eval: 4605.787485249361[0m
[37m[1m[2023-07-11 00:15:18,344][233954] Mean Reward across all agents: 4605.787485249361[0m
[37m[1m[2023-07-11 00:15:18,344][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:15:23,319][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:15:23,319][233954] Reward + Measures: [[3434.63230895    0.21110001    0.26070002    0.20389998    0.1103
     1.0234195 ]
 [3730.21682739    0.19759999    0.31870002    0.27980003    0.1041
     0.61603373]
 [2893.54039004    0.18520001    0.22160001    0.19690001    0.0981
     0.91574508]
 ...
 [2969.14148332    0.15110001    0.25569999    0.2024        0.0751
     0.699494  ]
 [1487.88294219    0.20739999    0.57529998    0.30900002    0.22400001
     0.47251818]
 [3813.285244      0.176         0.21900001    0.2119        0.0759
     0.73311061]][0m
[37m[1m[2023-07-11 00:15:23,319][233954] Max Reward on eval: 4895.134551989194[0m
[37m[1m[2023-07-11 00:15:23,320][233954] Min Reward on eval: 834.5287966852309[0m
[37m[1m[2023-07-11 00:15:23,320][233954] Mean Reward across all agents: 2840.3603276332415[0m
[37m[1m[2023-07-11 00:15:23,320][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:15:23,326][233954] mean_value=27.66494869507631, max_value=3153.294165232981[0m
[37m[1m[2023-07-11 00:15:23,328][233954] New mean coefficients: [[ 0.33781147 -0.02660333 -0.35932612  1.0329233   0.30990285  0.17741826]][0m
[37m[1m[2023-07-11 00:15:23,329][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:15:32,332][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 00:15:32,333][233954] FPS: 426593.03[0m
[36m[2023-07-11 00:15:32,335][233954] itr=14, itrs=2000, Progress: 0.70%[0m
[36m[2023-07-11 00:15:43,941][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 00:15:43,941][233954] FPS: 331517.05[0m
[36m[2023-07-11 00:15:48,233][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:15:48,234][233954] Reward + Measures: [[4969.7824302     0.18310869    0.22242899    0.20350665    0.04957033
     0.69482929]][0m
[37m[1m[2023-07-11 00:15:48,234][233954] Max Reward on eval: 4969.782430203466[0m
[37m[1m[2023-07-11 00:15:48,234][233954] Min Reward on eval: 4969.782430203466[0m
[37m[1m[2023-07-11 00:15:48,235][233954] Mean Reward across all agents: 4969.782430203466[0m
[37m[1m[2023-07-11 00:15:48,235][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:15:53,227][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:15:53,227][233954] Reward + Measures: [[1361.30023       0.2184        0.29130003    0.25819999    0.15120001
     0.69795018]
 [2853.39929197    0.2089        0.23510002    0.21300001    0.1209
     0.96815461]
 [4448.19355769    0.19680001    0.21280001    0.20460001    0.0596
     0.79113501]
 ...
 [4325.37911982    0.2024        0.24530001    0.2138        0.0498
     0.71192616]
 [3230.76513677    0.17879999    0.2342        0.212         0.0796
     0.66847277]
 [2493.36987306    0.15880001    0.34819999    0.23439999    0.1635
     0.62989068]][0m
[37m[1m[2023-07-11 00:15:53,228][233954] Max Reward on eval: 5018.978729211167[0m
[37m[1m[2023-07-11 00:15:53,228][233954] Min Reward on eval: 914.5646019075066[0m
[37m[1m[2023-07-11 00:15:53,228][233954] Mean Reward across all agents: 3063.8390693832257[0m
[37m[1m[2023-07-11 00:15:53,228][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:15:53,232][233954] mean_value=-267.8418624773843, max_value=1993.3221387603317[0m
[37m[1m[2023-07-11 00:15:53,235][233954] New mean coefficients: [[ 0.2650013  -0.33417827 -0.1460463   0.8097898   0.93508327  1.2977371 ]][0m
[37m[1m[2023-07-11 00:15:53,236][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:16:02,334][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 00:16:02,335][233954] FPS: 422127.02[0m
[36m[2023-07-11 00:16:02,337][233954] itr=15, itrs=2000, Progress: 0.75%[0m
[36m[2023-07-11 00:16:14,149][233954] train() took 11.78 seconds to complete[0m
[36m[2023-07-11 00:16:14,149][233954] FPS: 325851.24[0m
[36m[2023-07-11 00:16:18,411][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:16:18,411][233954] Reward + Measures: [[5134.88725888    0.19160664    0.21950231    0.21253499    0.052043
     0.78343147]][0m
[37m[1m[2023-07-11 00:16:18,412][233954] Max Reward on eval: 5134.887258882392[0m
[37m[1m[2023-07-11 00:16:18,412][233954] Min Reward on eval: 5134.887258882392[0m
[37m[1m[2023-07-11 00:16:18,412][233954] Mean Reward across all agents: 5134.887258882392[0m
[37m[1m[2023-07-11 00:16:18,412][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:16:23,385][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:16:23,386][233954] Reward + Measures: [[2497.01672362    0.16129999    0.19600001    0.18069999    0.08590001
     0.79564589]
 [3907.17001723    0.1815        0.20999999    0.2031        0.06820001
     0.75592434]
 [1132.38917928    0.0983        0.1451        0.11099999    0.0683
     0.80235749]
 ...
 [3063.59934997    0.16419999    0.19540001    0.1761        0.0781
     0.87616456]
 [3764.47467041    0.18840002    0.2383        0.20820001    0.0841
     0.87243223]
 [1938.65505217    0.13590001    0.17900001    0.15140001    0.0711
     0.70512557]][0m
[37m[1m[2023-07-11 00:16:23,386][233954] Max Reward on eval: 5124.749877993588[0m
[37m[1m[2023-07-11 00:16:23,387][233954] Min Reward on eval: 576.870939252153[0m
[37m[1m[2023-07-11 00:16:23,387][233954] Mean Reward across all agents: 2913.100746558825[0m
[37m[1m[2023-07-11 00:16:23,387][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:16:23,391][233954] mean_value=-579.9293681804794, max_value=1785.7224903887522[0m
[37m[1m[2023-07-11 00:16:23,393][233954] New mean coefficients: [[ 0.7038231  -0.6702216   0.5459813   0.9477697   0.60791194  1.8831122 ]][0m
[37m[1m[2023-07-11 00:16:23,394][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:16:32,450][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 00:16:32,450][233954] FPS: 424112.63[0m
[36m[2023-07-11 00:16:32,453][233954] itr=16, itrs=2000, Progress: 0.80%[0m
[36m[2023-07-11 00:16:44,027][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 00:16:44,028][233954] FPS: 332505.41[0m
[36m[2023-07-11 00:16:48,274][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:16:48,274][233954] Reward + Measures: [[5153.89731441    0.18705967    0.20519434    0.20506766    0.05013967
     0.80199033]][0m
[37m[1m[2023-07-11 00:16:48,275][233954] Max Reward on eval: 5153.897314410406[0m
[37m[1m[2023-07-11 00:16:48,275][233954] Min Reward on eval: 5153.897314410406[0m
[37m[1m[2023-07-11 00:16:48,275][233954] Mean Reward across all agents: 5153.897314410406[0m
[37m[1m[2023-07-11 00:16:48,275][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:16:53,398][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:16:53,398][233954] Reward + Measures: [[3439.41194538    0.1807        0.19860001    0.17400001    0.0693
     0.76026672]
 [1228.96529771    0.11600001    0.1139        0.0991        0.0807
     0.81435823]
 [3643.16896055    0.15190001    0.17210002    0.16759999    0.06510001
     0.777749  ]
 ...
 [4677.08098607    0.1804        0.20580001    0.18889999    0.0482
     0.89938253]
 [ 890.12262152    0.1142        0.13810001    0.1152        0.0896
     1.12829947]
 [1960.11063388    0.153         0.191         0.1664        0.1186
     0.94432974]][0m
[37m[1m[2023-07-11 00:16:53,399][233954] Max Reward on eval: 5301.925994846341[0m
[37m[1m[2023-07-11 00:16:53,399][233954] Min Reward on eval: 785.5528297290206[0m
[37m[1m[2023-07-11 00:16:53,399][233954] Mean Reward across all agents: 2950.5609492921953[0m
[37m[1m[2023-07-11 00:16:53,399][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:16:53,403][233954] mean_value=-852.2315172494923, max_value=2307.7004470172806[0m
[37m[1m[2023-07-11 00:16:53,405][233954] New mean coefficients: [[ 0.5479178  -0.5817381   0.842899    1.145362    0.77583814  1.7668808 ]][0m
[37m[1m[2023-07-11 00:16:53,406][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:17:02,420][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 00:17:02,420][233954] FPS: 426108.17[0m
[36m[2023-07-11 00:17:02,422][233954] itr=17, itrs=2000, Progress: 0.85%[0m
[36m[2023-07-11 00:17:14,267][233954] train() took 11.82 seconds to complete[0m
[36m[2023-07-11 00:17:14,268][233954] FPS: 324831.07[0m
[36m[2023-07-11 00:17:18,674][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:17:18,679][233954] Reward + Measures: [[5374.42247227    0.18842733    0.204634      0.20404834    0.04829833
     0.83962995]][0m
[37m[1m[2023-07-11 00:17:18,679][233954] Max Reward on eval: 5374.422472270159[0m
[37m[1m[2023-07-11 00:17:18,680][233954] Min Reward on eval: 5374.422472270159[0m
[37m[1m[2023-07-11 00:17:18,680][233954] Mean Reward across all agents: 5374.422472270159[0m
[37m[1m[2023-07-11 00:17:18,680][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:17:23,707][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:17:23,713][233954] Reward + Measures: [[4641.73215485    0.20009999    0.23109999    0.2192        0.0807
     0.9274506 ]
 [3551.61324307    0.17940001    0.2141        0.18050002    0.0608
     0.9184131 ]
 [3307.25564574    0.18970001    0.21280001    0.20469999    0.0879
     1.06774211]
 ...
 [3516.33134465    0.21789999    0.22380002    0.2059        0.0877
     1.06982327]
 [2962.84054567    0.1856        0.27980003    0.20020001    0.11870001
     0.99473637]
 [2866.91630934    0.1646        0.18789999    0.16340001    0.0796
     0.79269075]][0m
[37m[1m[2023-07-11 00:17:23,713][233954] Max Reward on eval: 5355.258300757652[0m
[37m[1m[2023-07-11 00:17:23,713][233954] Min Reward on eval: 907.9321789865382[0m
[37m[1m[2023-07-11 00:17:23,714][233954] Mean Reward across all agents: 3109.5506581024874[0m
[37m[1m[2023-07-11 00:17:23,714][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:17:23,716][233954] mean_value=-1035.3901950805846, max_value=2128.9486039440176[0m
[37m[1m[2023-07-11 00:17:23,719][233954] New mean coefficients: [[-0.23667336 -0.23726696  0.8867736   0.7153251   0.69918877  2.170721  ]][0m
[37m[1m[2023-07-11 00:17:23,720][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:17:32,786][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 00:17:32,786][233954] FPS: 423654.52[0m
[36m[2023-07-11 00:17:32,788][233954] itr=18, itrs=2000, Progress: 0.90%[0m
[36m[2023-07-11 00:17:44,505][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 00:17:44,505][233954] FPS: 328406.74[0m
[36m[2023-07-11 00:17:48,798][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:17:48,799][233954] Reward + Measures: [[4979.24831235    0.20242766    0.23209099    0.22489066    0.07128833
     0.90309334]][0m
[37m[1m[2023-07-11 00:17:48,799][233954] Max Reward on eval: 4979.248312354969[0m
[37m[1m[2023-07-11 00:17:48,799][233954] Min Reward on eval: 4979.248312354969[0m
[37m[1m[2023-07-11 00:17:48,800][233954] Mean Reward across all agents: 4979.248312354969[0m
[37m[1m[2023-07-11 00:17:48,800][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:17:53,836][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:17:53,837][233954] Reward + Measures: [[1302.4751053     0.15530001    0.16159998    0.1541        0.1168
     1.38033032]
 [4449.75791927    0.18780001    0.2251        0.21610001    0.062
     0.8283003 ]
 [2956.48415371    0.1901        0.21789999    0.1868        0.0807
     1.10337436]
 ...
 [3195.54512026    0.19440001    0.29529998    0.2198        0.1543
     1.08913863]
 [2608.56062316    0.17639999    0.207         0.18120001    0.10020001
     1.00708473]
 [4365.26000976    0.19350003    0.23309998    0.22040001    0.0707
     0.8754862 ]][0m
[37m[1m[2023-07-11 00:17:53,837][233954] Max Reward on eval: 5360.956268310896[0m
[37m[1m[2023-07-11 00:17:53,837][233954] Min Reward on eval: 443.0100672439672[0m
[37m[1m[2023-07-11 00:17:53,837][233954] Mean Reward across all agents: 2798.032403399553[0m
[37m[1m[2023-07-11 00:17:53,838][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:17:53,840][233954] mean_value=-1256.4507967324635, max_value=1353.1922924629703[0m
[37m[1m[2023-07-11 00:17:53,842][233954] New mean coefficients: [[-0.3010688  -0.3362676   0.80778223  0.4648344   0.4251308   2.314188  ]][0m
[37m[1m[2023-07-11 00:17:53,843][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:18:02,871][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 00:18:02,871][233954] FPS: 425426.28[0m
[36m[2023-07-11 00:18:02,873][233954] itr=19, itrs=2000, Progress: 0.95%[0m
[36m[2023-07-11 00:18:14,508][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 00:18:14,509][233954] FPS: 330688.88[0m
[36m[2023-07-11 00:18:18,780][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:18:18,781][233954] Reward + Measures: [[3658.57322167    0.21094368    0.26602301    0.24130802    0.12409267
     0.93797541]][0m
[37m[1m[2023-07-11 00:18:18,781][233954] Max Reward on eval: 3658.573221665307[0m
[37m[1m[2023-07-11 00:18:18,781][233954] Min Reward on eval: 3658.573221665307[0m
[37m[1m[2023-07-11 00:18:18,781][233954] Mean Reward across all agents: 3658.573221665307[0m
[37m[1m[2023-07-11 00:18:18,782][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:18:23,850][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:18:23,850][233954] Reward + Measures: [[2733.3092804     0.19590001    0.24849999    0.20809999    0.14040001
     0.98308337]
 [2990.97875974    0.20379999    0.31459999    0.27149999    0.15370001
     0.94662869]
 [2238.73121638    0.18619999    0.24010001    0.20280002    0.1355
     0.9141475 ]
 ...
 [3177.71224979    0.19500001    0.24159999    0.22520001    0.11310001
     1.04493129]
 [2811.62365724    0.2211        0.38369998    0.2811        0.19240001
     0.85670376]
 [1807.37284469    0.1797        0.229         0.1859        0.1603
     0.95991033]][0m
[37m[1m[2023-07-11 00:18:23,850][233954] Max Reward on eval: 4222.406463659694[0m
[37m[1m[2023-07-11 00:18:23,851][233954] Min Reward on eval: 1099.5729255763813[0m
[37m[1m[2023-07-11 00:18:23,851][233954] Mean Reward across all agents: 2537.4230964378035[0m
[37m[1m[2023-07-11 00:18:23,851][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:18:23,854][233954] mean_value=-1290.4391690301204, max_value=2070.4087134331207[0m
[37m[1m[2023-07-11 00:18:23,856][233954] New mean coefficients: [[-0.3130916  -0.53401136  1.130194    0.5894905   0.40903884  1.6974432 ]][0m
[37m[1m[2023-07-11 00:18:23,857][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:18:32,933][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 00:18:32,934][233954] FPS: 423157.11[0m
[36m[2023-07-11 00:18:32,936][233954] itr=20, itrs=2000, Progress: 1.00%[0m
[36m[2023-07-11 00:20:55,865][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 00:20:56,001][233954] FPS: 330707.68[0m
[36m[2023-07-11 00:21:00,152][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:21:00,152][233954] Reward + Measures: [[2811.06379288    0.20825064    0.29145834    0.24792632    0.179079
     0.96857959]][0m
[37m[1m[2023-07-11 00:21:00,152][233954] Max Reward on eval: 2811.063792880319[0m
[37m[1m[2023-07-11 00:21:00,153][233954] Min Reward on eval: 2811.063792880319[0m
[37m[1m[2023-07-11 00:21:00,153][233954] Mean Reward across all agents: 2811.063792880319[0m
[37m[1m[2023-07-11 00:21:00,153][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:21:05,331][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:21:05,332][233954] Reward + Measures: [[1553.55863759    0.17600001    0.29809999    0.19050001    0.19670001
     0.78624457]
 [1851.96631622    0.1899        0.26350003    0.21150003    0.17899999
     0.882043  ]
 [3038.11180114    0.221         0.32910001    0.2617        0.17300001
     0.99618512]
 ...
 [2103.0053711     0.20830002    0.31260002    0.2543        0.21300001
     1.00910366]
 [2174.45751189    0.20359997    0.27260002    0.2271        0.19159999
     0.98476619]
 [2580.75947567    0.18800001    0.26980001    0.22920001    0.16689999
     1.06746566]][0m
[37m[1m[2023-07-11 00:21:05,332][233954] Max Reward on eval: 3474.06564336319[0m
[37m[1m[2023-07-11 00:21:05,332][233954] Min Reward on eval: 1204.056819906272[0m
[37m[1m[2023-07-11 00:21:05,333][233954] Mean Reward across all agents: 2232.472211617717[0m
[37m[1m[2023-07-11 00:21:05,333][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:21:05,335][233954] mean_value=-1244.2341316854263, max_value=1359.508286371788[0m
[37m[1m[2023-07-11 00:21:05,337][233954] New mean coefficients: [[ 0.03963071 -0.06525254  1.1903245   0.67250407  0.5816021   0.9341487 ]][0m
[37m[1m[2023-07-11 00:21:05,338][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:21:14,297][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 00:21:14,297][233954] FPS: 428722.22[0m
[36m[2023-07-11 00:21:14,299][233954] itr=21, itrs=2000, Progress: 1.05%[0m
[36m[2023-07-11 00:21:25,975][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 00:21:25,976][233954] FPS: 329543.44[0m
[36m[2023-07-11 00:21:30,246][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:21:30,246][233954] Reward + Measures: [[2883.19281183    0.21543233    0.32502434    0.26557299    0.200032
     1.0429399 ]][0m
[37m[1m[2023-07-11 00:21:30,246][233954] Max Reward on eval: 2883.192811828443[0m
[37m[1m[2023-07-11 00:21:30,247][233954] Min Reward on eval: 2883.192811828443[0m
[37m[1m[2023-07-11 00:21:30,247][233954] Mean Reward across all agents: 2883.192811828443[0m
[37m[1m[2023-07-11 00:21:30,247][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:21:35,145][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:21:35,146][233954] Reward + Measures: [[2394.67468645    0.18350001    0.30660003    0.24550001    0.15970002
     0.95247042]
 [2800.56048587    0.2191        0.37400001    0.24340001    0.19710001
     1.04420376]
 [2403.32278443    0.1911        0.296         0.24010001    0.19399999
     1.11514413]
 ...
 [2734.32969664    0.19579999    0.303         0.23920003    0.17040001
     1.00722301]
 [2645.21363068    0.19860001    0.3035        0.2455        0.18699999
     1.14230907]
 [2777.42544552    0.21659999    0.33899999    0.25799999    0.22489999
     1.13045919]][0m
[37m[1m[2023-07-11 00:21:35,146][233954] Max Reward on eval: 3430.083679169882[0m
[37m[1m[2023-07-11 00:21:35,146][233954] Min Reward on eval: 1317.391234359308[0m
[37m[1m[2023-07-11 00:21:35,146][233954] Mean Reward across all agents: 2493.3658680548324[0m
[37m[1m[2023-07-11 00:21:35,147][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:21:35,149][233954] mean_value=-1030.4194010129283, max_value=2493.4248520775454[0m
[37m[1m[2023-07-11 00:21:35,151][233954] New mean coefficients: [[-0.43122262  0.00729559  1.5416926   0.8245285   0.7938049   0.11316228]][0m
[37m[1m[2023-07-11 00:21:35,152][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:21:44,119][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 00:21:44,119][233954] FPS: 428324.02[0m
[36m[2023-07-11 00:21:44,121][233954] itr=22, itrs=2000, Progress: 1.10%[0m
[36m[2023-07-11 00:21:55,623][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 00:21:55,623][233954] FPS: 334587.69[0m
[36m[2023-07-11 00:21:59,884][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:21:59,885][233954] Reward + Measures: [[2295.10458376    0.21392167    0.34646833    0.265257      0.24505533
     1.02266991]][0m
[37m[1m[2023-07-11 00:21:59,885][233954] Max Reward on eval: 2295.1045837554716[0m
[37m[1m[2023-07-11 00:21:59,885][233954] Min Reward on eval: 2295.1045837554716[0m
[37m[1m[2023-07-11 00:21:59,885][233954] Mean Reward across all agents: 2295.1045837554716[0m
[37m[1m[2023-07-11 00:21:59,885][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:22:04,890][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:22:04,890][233954] Reward + Measures: [[2082.88110355    0.2043        0.40650001    0.2617        0.26830003
     1.08078313]
 [2207.39048768    0.2211        0.35949999    0.26139998    0.23040001
     1.02244031]
 [2238.20008851    0.23910001    0.41409999    0.30050001    0.27340001
     1.01433241]
 ...
 [2069.54479976    0.19950001    0.35190001    0.2378        0.2445
     0.93841547]
 [2319.47515868    0.22390001    0.41669998    0.31080002    0.24630001
     1.10800076]
 [2185.2591095     0.2142        0.4082        0.26680002    0.273
     1.1082232 ]][0m
[37m[1m[2023-07-11 00:22:04,890][233954] Max Reward on eval: 2786.876678457856[0m
[37m[1m[2023-07-11 00:22:04,891][233954] Min Reward on eval: 1159.0630607645028[0m
[37m[1m[2023-07-11 00:22:04,891][233954] Mean Reward across all agents: 2046.9647873285428[0m
[37m[1m[2023-07-11 00:22:04,891][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:22:04,896][233954] mean_value=-243.2997493216697, max_value=1711.2206467139595[0m
[37m[1m[2023-07-11 00:22:04,898][233954] New mean coefficients: [[-0.22431515  0.27280736  1.8110981   0.4291993   0.5602984   0.43117175]][0m
[37m[1m[2023-07-11 00:22:04,899][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:22:13,817][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 00:22:13,817][233954] FPS: 430673.43[0m
[36m[2023-07-11 00:22:13,820][233954] itr=23, itrs=2000, Progress: 1.15%[0m
[36m[2023-07-11 00:22:25,360][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 00:22:25,360][233954] FPS: 333389.77[0m
[36m[2023-07-11 00:22:29,649][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:22:29,649][233954] Reward + Measures: [[1930.12819911    0.22072501    0.38380834    0.26090834    0.289453
     1.03953862]][0m
[37m[1m[2023-07-11 00:22:29,649][233954] Max Reward on eval: 1930.1281991139385[0m
[37m[1m[2023-07-11 00:22:29,650][233954] Min Reward on eval: 1930.1281991139385[0m
[37m[1m[2023-07-11 00:22:29,650][233954] Mean Reward across all agents: 1930.1281991139385[0m
[37m[1m[2023-07-11 00:22:29,650][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:22:34,712][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:22:34,713][233954] Reward + Measures: [[1603.1323471     0.20490001    0.3486        0.24159999    0.29200003
     1.04058421]
 [1897.44392393    0.24120001    0.43130001    0.27970001    0.28739998
     1.03304636]
 [1808.28817751    0.21500002    0.44479999    0.24429999    0.30790001
     0.983118  ]
 ...
 [2512.0364533     0.221         0.3881        0.29760003    0.2237
     1.02987993]
 [1417.08357621    0.1989        0.41890001    0.23190001    0.3091
     1.01952696]
 [1805.15843584    0.19870001    0.3836        0.2304        0.26280001
     1.0062747 ]][0m
[37m[1m[2023-07-11 00:22:34,713][233954] Max Reward on eval: 2512.0364532953126[0m
[37m[1m[2023-07-11 00:22:34,713][233954] Min Reward on eval: 1207.3296813940628[0m
[37m[1m[2023-07-11 00:22:34,714][233954] Mean Reward across all agents: 1772.0188604140196[0m
[37m[1m[2023-07-11 00:22:34,714][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:22:34,717][233954] mean_value=-148.8428073923077, max_value=1591.2266498885747[0m
[37m[1m[2023-07-11 00:22:34,720][233954] New mean coefficients: [[-0.06288695  0.3383767   2.1539502  -0.43452904  1.4335239  -0.5838634 ]][0m
[37m[1m[2023-07-11 00:22:34,721][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:22:43,727][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 00:22:43,727][233954] FPS: 426437.44[0m
[36m[2023-07-11 00:22:43,729][233954] itr=24, itrs=2000, Progress: 1.20%[0m
[36m[2023-07-11 00:22:55,265][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 00:22:55,265][233954] FPS: 333535.07[0m
[36m[2023-07-11 00:22:59,613][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:22:59,614][233954] Reward + Measures: [[1637.70153441    0.224916      0.44277266    0.24861501    0.34095699
     1.01698458]][0m
[37m[1m[2023-07-11 00:22:59,614][233954] Max Reward on eval: 1637.7015344106132[0m
[37m[1m[2023-07-11 00:22:59,614][233954] Min Reward on eval: 1637.7015344106132[0m
[37m[1m[2023-07-11 00:22:59,615][233954] Mean Reward across all agents: 1637.7015344106132[0m
[37m[1m[2023-07-11 00:22:59,615][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:23:04,765][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:23:04,765][233954] Reward + Measures: [[1444.41607665    0.23440002    0.45429999    0.26530001    0.37719998
     1.03331912]
 [1435.62676669    0.19820002    0.35450003    0.2198        0.30650002
     0.93719923]
 [1334.86961365    0.20889997    0.48530003    0.2366        0.38980004
     0.9152835 ]
 ...
 [1383.20494842    0.2168        0.46820003    0.23870002    0.37310001
     0.89496213]
 [1678.88246155    0.22070001    0.46960002    0.2683        0.28260002
     1.10365701]
 [1590.58412169    0.22690001    0.52539998    0.25          0.33950001
     0.98025781]][0m
[37m[1m[2023-07-11 00:23:04,766][233954] Max Reward on eval: 2051.5511932329273[0m
[37m[1m[2023-07-11 00:23:04,766][233954] Min Reward on eval: 432.6338157452643[0m
[37m[1m[2023-07-11 00:23:04,766][233954] Mean Reward across all agents: 1415.2115463040914[0m
[37m[1m[2023-07-11 00:23:04,766][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:23:04,773][233954] mean_value=73.4564199417175, max_value=2300.1410064924976[0m
[37m[1m[2023-07-11 00:23:04,775][233954] New mean coefficients: [[ 0.7099771   0.15021537  2.5925906   0.2631379   1.085232   -0.37114388]][0m
[37m[1m[2023-07-11 00:23:04,776][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:23:13,805][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 00:23:13,806][233954] FPS: 425351.91[0m
[36m[2023-07-11 00:23:13,808][233954] itr=25, itrs=2000, Progress: 1.25%[0m
[36m[2023-07-11 00:23:25,544][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 00:23:25,544][233954] FPS: 327825.78[0m
[36m[2023-07-11 00:23:29,873][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:23:29,874][233954] Reward + Measures: [[2002.31316062    0.22198732    0.47507831    0.26367766    0.32081401
     1.0081948 ]][0m
[37m[1m[2023-07-11 00:23:29,874][233954] Max Reward on eval: 2002.3131606207537[0m
[37m[1m[2023-07-11 00:23:29,874][233954] Min Reward on eval: 2002.3131606207537[0m
[37m[1m[2023-07-11 00:23:29,874][233954] Mean Reward across all agents: 2002.3131606207537[0m
[37m[1m[2023-07-11 00:23:29,875][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:23:34,929][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:23:34,929][233954] Reward + Measures: [[1047.48311613    0.2208        0.56919998    0.22570001    0.42799997
     0.86678362]
 [1265.00006103    0.1935        0.53390002    0.25339997    0.32790002
     0.90545416]
 [1906.16979983    0.20560001    0.48670003    0.26949999    0.3039
     0.98516101]
 ...
 [1658.15715026    0.20320001    0.53119999    0.25650001    0.36380002
     0.95466739]
 [1584.37438967    0.20809999    0.56450003    0.25229999    0.35960001
     0.96594238]
 [1788.26789852    0.20220001    0.5025        0.25920001    0.31479999
     0.96855921]][0m
[37m[1m[2023-07-11 00:23:34,930][233954] Max Reward on eval: 2779.881652788224[0m
[37m[1m[2023-07-11 00:23:34,930][233954] Min Reward on eval: 753.5169763662386[0m
[37m[1m[2023-07-11 00:23:34,930][233954] Mean Reward across all agents: 1598.6766415783816[0m
[37m[1m[2023-07-11 00:23:34,930][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:23:34,935][233954] mean_value=110.95772226091084, max_value=1772.29812290879[0m
[37m[1m[2023-07-11 00:23:34,938][233954] New mean coefficients: [[ 1.044618   -0.31983012  3.3413363   0.00889036  0.89503604 -0.04822105]][0m
[37m[1m[2023-07-11 00:23:34,939][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:23:44,004][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 00:23:44,004][233954] FPS: 423678.48[0m
[36m[2023-07-11 00:23:44,007][233954] itr=26, itrs=2000, Progress: 1.30%[0m
[36m[2023-07-11 00:23:56,179][233954] train() took 12.15 seconds to complete[0m
[36m[2023-07-11 00:23:56,179][233954] FPS: 316096.99[0m
[36m[2023-07-11 00:24:00,479][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:24:00,480][233954] Reward + Measures: [[2595.34191075    0.21720731    0.48709267    0.27690235    0.27437067
     0.99950212]][0m
[37m[1m[2023-07-11 00:24:00,480][233954] Max Reward on eval: 2595.3419107493746[0m
[37m[1m[2023-07-11 00:24:00,480][233954] Min Reward on eval: 2595.3419107493746[0m
[37m[1m[2023-07-11 00:24:00,481][233954] Mean Reward across all agents: 2595.3419107493746[0m
[37m[1m[2023-07-11 00:24:00,481][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:24:05,580][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:24:05,580][233954] Reward + Measures: [[1926.22596742    0.20290001    0.51230001    0.2237        0.28280002
     0.91878575]
 [1627.2471466     0.20550001    0.56560004    0.1981        0.3285
     0.89497548]
 [1991.29246522    0.19490001    0.55269998    0.24129999    0.3283
     0.90963316]
 ...
 [2086.62165829    0.21259999    0.55680001    0.24260001    0.30040002
     0.93575519]
 [2288.55262757    0.20539999    0.5327        0.27410001    0.29380003
     0.96963006]
 [1319.65858841    0.19850002    0.5927        0.1957        0.3527
     0.81976157]][0m
[37m[1m[2023-07-11 00:24:05,581][233954] Max Reward on eval: 2665.5455627526158[0m
[37m[1m[2023-07-11 00:24:05,581][233954] Min Reward on eval: 900.4110203057528[0m
[37m[1m[2023-07-11 00:24:05,581][233954] Mean Reward across all agents: 1930.2446767345305[0m
[37m[1m[2023-07-11 00:24:05,581][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:24:05,587][233954] mean_value=62.111575481078305, max_value=1853.7931594828055[0m
[37m[1m[2023-07-11 00:24:05,590][233954] New mean coefficients: [[ 1.0001425  -0.3898256   3.1623106  -0.16771092  0.9780249  -0.3779739 ]][0m
[37m[1m[2023-07-11 00:24:05,591][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:24:14,642][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 00:24:14,642][233954] FPS: 424314.32[0m
[36m[2023-07-11 00:24:14,645][233954] itr=27, itrs=2000, Progress: 1.35%[0m
[36m[2023-07-11 00:24:26,232][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 00:24:26,232][233954] FPS: 332165.98[0m
[36m[2023-07-11 00:24:30,588][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:24:30,588][233954] Reward + Measures: [[3231.92063085    0.21040833    0.486826      0.28861099    0.22768733
     0.97509593]][0m
[37m[1m[2023-07-11 00:24:30,589][233954] Max Reward on eval: 3231.9206308532725[0m
[37m[1m[2023-07-11 00:24:30,589][233954] Min Reward on eval: 3231.9206308532725[0m
[37m[1m[2023-07-11 00:24:30,589][233954] Mean Reward across all agents: 3231.9206308532725[0m
[37m[1m[2023-07-11 00:24:30,589][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:24:35,634][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:24:35,634][233954] Reward + Measures: [[2032.89305116    0.20679998    0.53060001    0.25069997    0.31459999
     0.9978925 ]
 [2861.87710571    0.22000001    0.52770001    0.25119999    0.22190002
     0.91462976]
 [3009.7181091     0.1954        0.50990003    0.27470002    0.2507
     0.9525097 ]
 ...
 [2422.64537047    0.1981        0.52200001    0.22760001    0.2438
     0.86667967]
 [2244.44270324    0.2168        0.53119999    0.2552        0.28309998
     0.99971533]
 [2135.41688538    0.19660001    0.52600002    0.23699999    0.26809999
     0.94082373]][0m
[37m[1m[2023-07-11 00:24:35,635][233954] Max Reward on eval: 3358.7899780244566[0m
[37m[1m[2023-07-11 00:24:35,635][233954] Min Reward on eval: 1046.0505600258707[0m
[37m[1m[2023-07-11 00:24:35,635][233954] Mean Reward across all agents: 2368.8228183538818[0m
[37m[1m[2023-07-11 00:24:35,635][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:24:35,640][233954] mean_value=-10.79791128166311, max_value=2828.4072052264723[0m
[37m[1m[2023-07-11 00:24:35,643][233954] New mean coefficients: [[ 1.012591   -0.13730752  2.2330894   0.17202972  0.29407692  0.1814599 ]][0m
[37m[1m[2023-07-11 00:24:35,644][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:24:44,732][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 00:24:44,732][233954] FPS: 422602.37[0m
[36m[2023-07-11 00:24:44,734][233954] itr=28, itrs=2000, Progress: 1.40%[0m
[36m[2023-07-11 00:24:56,333][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 00:24:56,333][233954] FPS: 331722.02[0m
[36m[2023-07-11 00:25:00,657][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:25:00,657][233954] Reward + Measures: [[3794.4155037     0.20612966    0.47033301    0.29143867    0.19174667
     0.96630335]][0m
[37m[1m[2023-07-11 00:25:00,657][233954] Max Reward on eval: 3794.4155037041146[0m
[37m[1m[2023-07-11 00:25:00,658][233954] Min Reward on eval: 3794.4155037041146[0m
[37m[1m[2023-07-11 00:25:00,658][233954] Mean Reward across all agents: 3794.4155037041146[0m
[37m[1m[2023-07-11 00:25:00,658][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:25:05,751][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:25:05,751][233954] Reward + Measures: [[3702.47763059    0.20460001    0.4364        0.28460002    0.18340002
     1.04017949]
 [3657.43182377    0.1937        0.46430001    0.2719        0.2148
     1.03665113]
 [3326.43298336    0.2105        0.46709996    0.31350002    0.20560001
     1.1330874 ]
 ...
 [2888.53297042    0.17230001    0.46059999    0.234         0.21159999
     0.89334041]
 [2700.34582518    0.19630001    0.50660002    0.2581        0.24960001
     1.02731502]
 [3424.90902712    0.1901        0.48300001    0.27779999    0.2208
     1.0622381 ]][0m
[37m[1m[2023-07-11 00:25:05,752][233954] Max Reward on eval: 3891.2098998481406[0m
[37m[1m[2023-07-11 00:25:05,752][233954] Min Reward on eval: 1500.403697989625[0m
[37m[1m[2023-07-11 00:25:05,752][233954] Mean Reward across all agents: 3057.5697504587383[0m
[37m[1m[2023-07-11 00:25:05,752][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:25:05,760][233954] mean_value=314.75350935789425, max_value=4131.268951408611[0m
[37m[1m[2023-07-11 00:25:05,763][233954] New mean coefficients: [[0.7012592  0.31280944 1.0711592  0.17520395 0.33271912 0.48531133]][0m
[37m[1m[2023-07-11 00:25:05,764][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:25:14,916][233954] train() took 9.15 seconds to complete[0m
[36m[2023-07-11 00:25:14,916][233954] FPS: 419649.73[0m
[36m[2023-07-11 00:25:14,918][233954] itr=29, itrs=2000, Progress: 1.45%[0m
[36m[2023-07-11 00:25:26,542][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 00:25:26,542][233954] FPS: 331050.68[0m
[36m[2023-07-11 00:25:30,882][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:25:30,882][233954] Reward + Measures: [[4302.65455273    0.20284367    0.44359833    0.28323898    0.16308233
     0.96466517]][0m
[37m[1m[2023-07-11 00:25:30,883][233954] Max Reward on eval: 4302.654552733641[0m
[37m[1m[2023-07-11 00:25:30,883][233954] Min Reward on eval: 4302.654552733641[0m
[37m[1m[2023-07-11 00:25:30,883][233954] Mean Reward across all agents: 4302.654552733641[0m
[37m[1m[2023-07-11 00:25:30,883][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:25:36,024][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:25:36,024][233954] Reward + Measures: [[3984.81780244    0.18780001    0.43210003    0.26520002    0.16059999
     0.90787375]
 [3846.00805663    0.22679999    0.38060001    0.29509997    0.15189999
     1.03395391]
 [3900.97189335    0.21159999    0.40990001    0.2983        0.15969999
     0.99609679]
 ...
 [3901.47961431    0.20310001    0.43380004    0.28940001    0.18480001
     1.02983892]
 [3371.39709471    0.2342        0.39920002    0.2807        0.1822
     1.10217226]
 [3441.54824825    0.2362        0.3759        0.27640003    0.1732
     1.1705271 ]][0m
[37m[1m[2023-07-11 00:25:36,024][233954] Max Reward on eval: 4374.0906066618745[0m
[37m[1m[2023-07-11 00:25:36,025][233954] Min Reward on eval: 1415.3054924138123[0m
[37m[1m[2023-07-11 00:25:36,025][233954] Mean Reward across all agents: 3674.93057852296[0m
[37m[1m[2023-07-11 00:25:36,025][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:25:36,030][233954] mean_value=91.84435392202427, max_value=1986.6351781252883[0m
[37m[1m[2023-07-11 00:25:36,033][233954] New mean coefficients: [[ 0.98015124  0.57315326  0.9106678  -0.05601656  0.47896284  0.19189188]][0m
[37m[1m[2023-07-11 00:25:36,034][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:25:45,049][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 00:25:45,049][233954] FPS: 426053.70[0m
[36m[2023-07-11 00:25:45,051][233954] itr=30, itrs=2000, Progress: 1.50%[0m
[37m[1m[2023-07-11 00:27:57,863][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000010[0m
[36m[2023-07-11 00:28:10,182][233954] train() took 11.85 seconds to complete[0m
[36m[2023-07-11 00:28:10,182][233954] FPS: 324127.82[0m
[36m[2023-07-11 00:28:14,489][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:28:14,489][233954] Reward + Measures: [[4744.64930938    0.20459867    0.40787265    0.27182567    0.13832267
     0.96479833]][0m
[37m[1m[2023-07-11 00:28:14,489][233954] Max Reward on eval: 4744.649309378506[0m
[37m[1m[2023-07-11 00:28:14,489][233954] Min Reward on eval: 4744.649309378506[0m
[37m[1m[2023-07-11 00:28:14,490][233954] Mean Reward across all agents: 4744.649309378506[0m
[37m[1m[2023-07-11 00:28:14,490][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:28:19,536][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:28:19,536][233954] Reward + Measures: [[4357.57183837    0.19680001    0.44150001    0.29899999    0.15449999
     0.95954335]
 [3616.97778319    0.20010002    0.37500003    0.28580001    0.1691
     1.10857642]
 [3919.91378783    0.193         0.43270001    0.31979999    0.17129999
     0.95336276]
 ...
 [3115.57427985    0.22049999    0.41429996    0.3001        0.21159999
     1.2243377 ]
 [3089.07037349    0.1957        0.48740003    0.2326        0.1577
     0.63943106]
 [3614.71047976    0.2106        0.45280001    0.28570002    0.189
     1.04007089]][0m
[37m[1m[2023-07-11 00:28:19,537][233954] Max Reward on eval: 4806.358917203453[0m
[37m[1m[2023-07-11 00:28:19,537][233954] Min Reward on eval: 1979.824512507394[0m
[37m[1m[2023-07-11 00:28:19,537][233954] Mean Reward across all agents: 3783.380998434069[0m
[37m[1m[2023-07-11 00:28:19,537][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:28:19,541][233954] mean_value=33.2512504167199, max_value=4051.357772417045[0m
[37m[1m[2023-07-11 00:28:19,544][233954] New mean coefficients: [[ 0.8493422   0.45927018  0.669554   -0.40649557  0.3496433   0.28766316]][0m
[37m[1m[2023-07-11 00:28:19,545][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:28:28,588][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 00:28:28,588][233954] FPS: 424715.78[0m
[36m[2023-07-11 00:28:28,590][233954] itr=31, itrs=2000, Progress: 1.55%[0m
[36m[2023-07-11 00:28:40,350][233954] train() took 11.74 seconds to complete[0m
[36m[2023-07-11 00:28:40,350][233954] FPS: 327170.44[0m
[36m[2023-07-11 00:28:44,683][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:28:44,683][233954] Reward + Measures: [[5239.3349772     0.20441198    0.3532677     0.24948266    0.10589633
     0.96207339]][0m
[37m[1m[2023-07-11 00:28:44,684][233954] Max Reward on eval: 5239.334977204211[0m
[37m[1m[2023-07-11 00:28:44,684][233954] Min Reward on eval: 5239.334977204211[0m
[37m[1m[2023-07-11 00:28:44,684][233954] Mean Reward across all agents: 5239.334977204211[0m
[37m[1m[2023-07-11 00:28:44,684][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:28:49,737][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:28:49,743][233954] Reward + Measures: [[3590.28300472    0.15949999    0.4086        0.25820002    0.1345
     0.71646404]
 [3834.09613045    0.18969999    0.50380003    0.2773        0.1754
     0.8219682 ]
 [4387.22879024    0.2066        0.42840001    0.26140004    0.17
     0.95013505]
 ...
 [3710.20397949    0.192         0.43940002    0.29380003    0.1384
     0.77568704]
 [5131.59155272    0.184         0.37220001    0.25240001    0.1013
     0.84521753]
 [3892.51071173    0.19770001    0.31479999    0.22780001    0.1366
     0.96462005]][0m
[37m[1m[2023-07-11 00:28:49,743][233954] Max Reward on eval: 5409.9154358538335[0m
[37m[1m[2023-07-11 00:28:49,744][233954] Min Reward on eval: 1807.0880814045668[0m
[37m[1m[2023-07-11 00:28:49,744][233954] Mean Reward across all agents: 4032.621841003535[0m
[37m[1m[2023-07-11 00:28:49,744][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:28:49,748][233954] mean_value=-112.30848494333536, max_value=3952.703951355885[0m
[37m[1m[2023-07-11 00:28:49,751][233954] New mean coefficients: [[ 0.6868085   0.21835837  0.22806299 -0.15085873  0.16976893  0.57994115]][0m
[37m[1m[2023-07-11 00:28:49,752][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:28:58,858][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 00:28:58,858][233954] FPS: 421784.40[0m
[36m[2023-07-11 00:28:58,860][233954] itr=32, itrs=2000, Progress: 1.60%[0m
[36m[2023-07-11 00:29:10,531][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 00:29:10,532][233954] FPS: 329666.65[0m
[36m[2023-07-11 00:29:14,895][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:29:14,895][233954] Reward + Measures: [[5682.43009054    0.19717598    0.29594234    0.224433      0.07424033
     0.95284194]][0m
[37m[1m[2023-07-11 00:29:14,896][233954] Max Reward on eval: 5682.430090536505[0m
[37m[1m[2023-07-11 00:29:14,896][233954] Min Reward on eval: 5682.430090536505[0m
[37m[1m[2023-07-11 00:29:14,896][233954] Mean Reward across all agents: 5682.430090536505[0m
[37m[1m[2023-07-11 00:29:14,896][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:29:19,929][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:29:19,935][233954] Reward + Measures: [[3924.44203946    0.21610001    0.2994        0.23769999    0.13430001
     1.31637776]
 [5339.99621582    0.21710001    0.32769999    0.2462        0.09170001
     1.02371562]
 [4707.12246708    0.21180001    0.31110001    0.23190001    0.0995
     1.1129396 ]
 ...
 [4208.1200676     0.18699999    0.3231        0.22789998    0.09470001
     0.786699  ]
 [5233.76821899    0.21170001    0.31640002    0.25259998    0.10290001
     1.07258618]
 [3205.18783571    0.20250002    0.27880001    0.2158        0.11800001
     1.16405606]][0m
[37m[1m[2023-07-11 00:29:19,935][233954] Max Reward on eval: 5846.358642547188[0m
[37m[1m[2023-07-11 00:29:19,935][233954] Min Reward on eval: 1760.735862714611[0m
[37m[1m[2023-07-11 00:29:19,935][233954] Mean Reward across all agents: 4417.569266218044[0m
[37m[1m[2023-07-11 00:29:19,936][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:29:19,940][233954] mean_value=-99.12681348201538, max_value=3428.351783607104[0m
[37m[1m[2023-07-11 00:29:19,943][233954] New mean coefficients: [[ 0.38255     0.05901071  0.4892477  -0.16683722  0.15257813  1.3763618 ]][0m
[37m[1m[2023-07-11 00:29:19,944][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:29:29,018][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 00:29:29,018][233954] FPS: 423248.40[0m
[36m[2023-07-11 00:29:29,021][233954] itr=33, itrs=2000, Progress: 1.65%[0m
[36m[2023-07-11 00:29:40,752][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 00:29:40,752][233954] FPS: 328087.44[0m
[36m[2023-07-11 00:29:45,081][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:29:45,081][233954] Reward + Measures: [[5936.11623947    0.19965167    0.27796635    0.21984832    0.06236266
     0.99540794]][0m
[37m[1m[2023-07-11 00:29:45,082][233954] Max Reward on eval: 5936.116239468224[0m
[37m[1m[2023-07-11 00:29:45,082][233954] Min Reward on eval: 5936.116239468224[0m
[37m[1m[2023-07-11 00:29:45,082][233954] Mean Reward across all agents: 5936.116239468224[0m
[37m[1m[2023-07-11 00:29:45,083][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:29:50,232][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:29:50,238][233954] Reward + Measures: [[2028.60189054    0.19849999    0.24710003    0.19669999    0.1301
     1.33181989]
 [ 807.98658561    0.25370002    0.41850001    0.3019        0.31909999
     1.41832149]
 [2482.97400668    0.18200001    0.2158        0.17760001    0.10420001
     1.08768237]
 ...
 [5393.43325805    0.19590001    0.2667        0.22920001    0.0909
     0.97520584]
 [1780.95233152    0.24200001    0.40450001    0.31720001    0.27930003
     1.2785244 ]
 [4029.3349304     0.2297        0.33710003    0.23600002    0.12660001
     1.24123394]][0m
[37m[1m[2023-07-11 00:29:50,239][233954] Max Reward on eval: 5759.938964844286[0m
[37m[1m[2023-07-11 00:29:50,239][233954] Min Reward on eval: 31.55591596569866[0m
[37m[1m[2023-07-11 00:29:50,239][233954] Mean Reward across all agents: 2831.1860800492727[0m
[37m[1m[2023-07-11 00:29:50,239][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:29:50,245][233954] mean_value=-720.7833962259793, max_value=3268.4664611532353[0m
[37m[1m[2023-07-11 00:29:50,248][233954] New mean coefficients: [[ 0.3978562  -0.14369617  0.96934915 -0.24679096 -0.03039409  2.6451495 ]][0m
[37m[1m[2023-07-11 00:29:50,249][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:29:59,280][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 00:29:59,280][233954] FPS: 425260.68[0m
[36m[2023-07-11 00:29:59,283][233954] itr=34, itrs=2000, Progress: 1.70%[0m
[36m[2023-07-11 00:30:10,924][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 00:30:10,924][233954] FPS: 330506.68[0m
[36m[2023-07-11 00:30:15,292][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:30:15,293][233954] Reward + Measures: [[6083.31475572    0.20043033    0.27226168    0.21699733    0.05771499
     1.04479146]][0m
[37m[1m[2023-07-11 00:30:15,293][233954] Max Reward on eval: 6083.31475572024[0m
[37m[1m[2023-07-11 00:30:15,293][233954] Min Reward on eval: 6083.31475572024[0m
[37m[1m[2023-07-11 00:30:15,294][233954] Mean Reward across all agents: 6083.31475572024[0m
[37m[1m[2023-07-11 00:30:15,294][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:30:20,345][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:30:20,345][233954] Reward + Measures: [[ 778.18663598    0.16670001    0.29000002    0.1214        0.1972
     1.01801109]
 [4407.14956665    0.17559999    0.28659999    0.19590001    0.09240001
     0.81168479]
 [1815.42390441    0.15369999    0.2198        0.1496        0.0976
     1.27445984]
 ...
 [3097.99705508    0.20679998    0.31090003    0.25850001    0.1434
     1.16549063]
 [1248.20483207    0.1617        0.235         0.19230001    0.1441
     1.28034365]
 [4196.36372751    0.19070001    0.27379999    0.22220002    0.0977
     1.01797414]][0m
[37m[1m[2023-07-11 00:30:20,346][233954] Max Reward on eval: 5937.9517821976915[0m
[37m[1m[2023-07-11 00:30:20,346][233954] Min Reward on eval: 426.031131760776[0m
[37m[1m[2023-07-11 00:30:20,346][233954] Mean Reward across all agents: 3127.4042728034524[0m
[37m[1m[2023-07-11 00:30:20,346][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:30:20,350][233954] mean_value=-259.11079809748054, max_value=5182.260489947428[0m
[37m[1m[2023-07-11 00:30:20,353][233954] New mean coefficients: [[-0.11829266  0.15033801  0.74866277 -0.38693452  0.08453752  4.5039425 ]][0m
[37m[1m[2023-07-11 00:30:20,354][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:30:29,417][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 00:30:29,418][233954] FPS: 423751.25[0m
[36m[2023-07-11 00:30:29,420][233954] itr=35, itrs=2000, Progress: 1.75%[0m
[36m[2023-07-11 00:30:41,112][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 00:30:41,113][233954] FPS: 329058.02[0m
[36m[2023-07-11 00:30:45,340][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:30:45,341][233954] Reward + Measures: [[5125.61446796    0.22622669    0.32496867    0.24415866    0.09938399
     1.18828821]][0m
[37m[1m[2023-07-11 00:30:45,341][233954] Max Reward on eval: 5125.614467957548[0m
[37m[1m[2023-07-11 00:30:45,341][233954] Min Reward on eval: 5125.614467957548[0m
[37m[1m[2023-07-11 00:30:45,341][233954] Mean Reward across all agents: 5125.614467957548[0m
[37m[1m[2023-07-11 00:30:45,342][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:30:50,420][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:30:50,426][233954] Reward + Measures: [[2498.38576511    0.177         0.28870001    0.2131        0.13329999
     1.17449987]
 [3264.04375074    0.19389999    0.29140002    0.20440002    0.127
     1.15529418]
 [4269.96480942    0.1919        0.2676        0.20350002    0.0818
     1.103701  ]
 ...
 [1148.96396449    0.17850001    0.23730002    0.1444        0.14479999
     1.47472751]
 [2380.76699831    0.1655        0.29270002    0.20290001    0.1337
     1.1033591 ]
 [3278.85387041    0.16440001    0.27859998    0.18800001    0.105
     1.01196563]][0m
[37m[1m[2023-07-11 00:30:50,426][233954] Max Reward on eval: 5193.100372346491[0m
[37m[1m[2023-07-11 00:30:50,427][233954] Min Reward on eval: 317.9351378981024[0m
[37m[1m[2023-07-11 00:30:50,427][233954] Mean Reward across all agents: 2341.371950807115[0m
[37m[1m[2023-07-11 00:30:50,427][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:30:50,430][233954] mean_value=-1074.0383928112499, max_value=3414.677767292745[0m
[37m[1m[2023-07-11 00:30:50,433][233954] New mean coefficients: [[-0.18792275  0.04100978  0.4239696  -0.259682   -0.7850908   4.5476236 ]][0m
[37m[1m[2023-07-11 00:30:50,434][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:30:59,543][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 00:30:59,544][233954] FPS: 421594.87[0m
[36m[2023-07-11 00:30:59,546][233954] itr=36, itrs=2000, Progress: 1.80%[0m
[36m[2023-07-11 00:31:11,462][233954] train() took 11.89 seconds to complete[0m
[36m[2023-07-11 00:31:11,462][233954] FPS: 322981.13[0m
[36m[2023-07-11 00:31:15,770][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:31:15,771][233954] Reward + Measures: [[5045.00893469    0.23036233    0.31969133    0.24267165    0.09892
     1.28237867]][0m
[37m[1m[2023-07-11 00:31:15,771][233954] Max Reward on eval: 5045.008934693805[0m
[37m[1m[2023-07-11 00:31:15,771][233954] Min Reward on eval: 5045.008934693805[0m
[37m[1m[2023-07-11 00:31:15,772][233954] Mean Reward across all agents: 5045.008934693805[0m
[37m[1m[2023-07-11 00:31:15,772][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:31:20,712][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:31:20,712][233954] Reward + Measures: [[4774.62338259    0.2393        0.31570002    0.24440001    0.0952
     1.34260714]
 [3885.87480168    0.20559998    0.32149997    0.2296        0.1155
     1.10675788]
 [3914.564106      0.23040001    0.28670001    0.2203        0.1102
     1.28739059]
 ...
 [4246.36840819    0.23650001    0.30790001    0.24530001    0.0995
     1.31870353]
 [2805.64566043    0.20009999    0.25830004    0.2           0.1227
     1.21252275]
 [3216.66909792    0.2034        0.26209998    0.21589999    0.1074
     1.17679882]][0m
[37m[1m[2023-07-11 00:31:20,713][233954] Max Reward on eval: 5278.356292732618[0m
[37m[1m[2023-07-11 00:31:20,713][233954] Min Reward on eval: 1491.2725258290768[0m
[37m[1m[2023-07-11 00:31:20,713][233954] Mean Reward across all agents: 3827.958896722775[0m
[37m[1m[2023-07-11 00:31:20,713][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:31:20,716][233954] mean_value=212.913020578948, max_value=5014.4300584568455[0m
[37m[1m[2023-07-11 00:31:20,719][233954] New mean coefficients: [[ 0.3573843   0.04188492  0.92700636 -0.10114761 -0.29866225  4.2177477 ]][0m
[37m[1m[2023-07-11 00:31:20,720][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:31:29,726][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 00:31:29,726][233954] FPS: 426455.40[0m
[36m[2023-07-11 00:31:29,729][233954] itr=37, itrs=2000, Progress: 1.85%[0m
[36m[2023-07-11 00:31:41,325][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 00:31:41,326][233954] FPS: 331820.98[0m
[36m[2023-07-11 00:31:45,623][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:31:45,623][233954] Reward + Measures: [[5212.01116435    0.228921      0.305347      0.239225      0.08844066
     1.32418954]][0m
[37m[1m[2023-07-11 00:31:45,624][233954] Max Reward on eval: 5212.011164345946[0m
[37m[1m[2023-07-11 00:31:45,624][233954] Min Reward on eval: 5212.011164345946[0m
[37m[1m[2023-07-11 00:31:45,624][233954] Mean Reward across all agents: 5212.011164345946[0m
[37m[1m[2023-07-11 00:31:45,624][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:31:50,784][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:31:50,785][233954] Reward + Measures: [[4440.60687255    0.23800002    0.37550002    0.26750001    0.1357
     1.2411989 ]
 [2689.73490526    0.1822        0.28459999    0.2287        0.14140001
     1.27880466]
 [4265.7272644     0.2084        0.28730002    0.23629999    0.0894
     1.36008584]
 ...
 [2880.87313847    0.21080001    0.3646        0.30140001    0.17629999
     1.26417768]
 [4334.4281311     0.21920002    0.3224        0.26470003    0.1153
     1.36584795]
 [3491.47463988    0.19180001    0.36149999    0.23080002    0.184
     1.28133166]][0m
[37m[1m[2023-07-11 00:31:50,785][233954] Max Reward on eval: 5397.6613769609485[0m
[37m[1m[2023-07-11 00:31:50,785][233954] Min Reward on eval: 1480.139099085331[0m
[37m[1m[2023-07-11 00:31:50,786][233954] Mean Reward across all agents: 3690.9539192034645[0m
[37m[1m[2023-07-11 00:31:50,786][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:31:50,789][233954] mean_value=-443.74028593104566, max_value=4890.822082529962[0m
[37m[1m[2023-07-11 00:31:50,791][233954] New mean coefficients: [[ 0.5973082  -0.316265    1.1690596  -0.21125057 -0.69696885  3.378087  ]][0m
[37m[1m[2023-07-11 00:31:50,792][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:31:59,788][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 00:31:59,788][233954] FPS: 426957.06[0m
[36m[2023-07-11 00:31:59,790][233954] itr=38, itrs=2000, Progress: 1.90%[0m
[36m[2023-07-11 00:32:11,666][233954] train() took 11.85 seconds to complete[0m
[36m[2023-07-11 00:32:11,666][233954] FPS: 324026.16[0m
[36m[2023-07-11 00:32:16,003][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:32:16,004][233954] Reward + Measures: [[5544.07499687    0.22425866    0.28699732    0.23324467    0.072736
     1.36813426]][0m
[37m[1m[2023-07-11 00:32:16,004][233954] Max Reward on eval: 5544.074996871465[0m
[37m[1m[2023-07-11 00:32:16,004][233954] Min Reward on eval: 5544.074996871465[0m
[37m[1m[2023-07-11 00:32:16,004][233954] Mean Reward across all agents: 5544.074996871465[0m
[37m[1m[2023-07-11 00:32:16,005][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:32:21,069][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:32:21,069][233954] Reward + Measures: [[1633.56558229    0.1867        0.2897        0.2146        0.1732
     1.39395905]
 [1522.48106764    0.1613        0.25670001    0.2273        0.22749999
     1.42634606]
 [1591.13824466    0.1639        0.26410002    0.21710001    0.20920001
     1.42505538]
 ...
 [2495.8678207     0.1753        0.2789        0.2198        0.12910001
     1.23212135]
 [2640.12527469    0.18010001    0.3524        0.27720001    0.2494
     1.4554441 ]
 [2845.52165219    0.1918        0.3303        0.27350003    0.1903
     1.40766358]][0m
[37m[1m[2023-07-11 00:32:21,069][233954] Max Reward on eval: 5442.943664505333[0m
[37m[1m[2023-07-11 00:32:21,070][233954] Min Reward on eval: 899.0797734312713[0m
[37m[1m[2023-07-11 00:32:21,070][233954] Mean Reward across all agents: 3064.473972931647[0m
[37m[1m[2023-07-11 00:32:21,070][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:32:21,073][233954] mean_value=-455.8492279301863, max_value=2914.5534956418132[0m
[37m[1m[2023-07-11 00:32:21,076][233954] New mean coefficients: [[ 0.48353836 -0.38065857  1.0897411  -0.00308049 -0.7978612   2.8505063 ]][0m
[37m[1m[2023-07-11 00:32:21,077][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:32:30,132][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 00:32:30,133][233954] FPS: 424140.41[0m
[36m[2023-07-11 00:32:30,135][233954] itr=39, itrs=2000, Progress: 1.95%[0m
[36m[2023-07-11 00:32:41,695][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 00:32:41,695][233954] FPS: 332830.91[0m
[36m[2023-07-11 00:32:46,052][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:32:46,053][233954] Reward + Measures: [[5868.36194867    0.21798666    0.26207402    0.22378901    0.05643967
     1.39741206]][0m
[37m[1m[2023-07-11 00:32:46,053][233954] Max Reward on eval: 5868.361948672802[0m
[37m[1m[2023-07-11 00:32:46,053][233954] Min Reward on eval: 5868.361948672802[0m
[37m[1m[2023-07-11 00:32:46,053][233954] Mean Reward across all agents: 5868.361948672802[0m
[37m[1m[2023-07-11 00:32:46,054][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:32:51,069][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:32:51,078][233954] Reward + Measures: [[5223.51483154    0.20190001    0.23550001    0.19719999    0.035
     1.19389248]
 [4458.08042901    0.22090001    0.2994        0.2287        0.1181
     1.42072904]
 [5308.89294436    0.2254        0.30550003    0.2378        0.078
     1.33826959]
 ...
 [5151.21197507    0.21140002    0.25800002    0.2172        0.06420001
     1.26670372]
 [3682.15818778    0.21000002    0.2823        0.2139        0.13150001
     1.35053575]
 [4174.6531601     0.20299999    0.25260001    0.1992        0.1058
     1.34657252]][0m
[37m[1m[2023-07-11 00:32:51,078][233954] Max Reward on eval: 6171.473754866514[0m
[37m[1m[2023-07-11 00:32:51,079][233954] Min Reward on eval: 1550.2253170214594[0m
[37m[1m[2023-07-11 00:32:51,079][233954] Mean Reward across all agents: 4587.12843747104[0m
[37m[1m[2023-07-11 00:32:51,079][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:32:51,083][233954] mean_value=644.219682690619, max_value=4468.03035747059[0m
[37m[1m[2023-07-11 00:32:51,086][233954] New mean coefficients: [[ 0.38113314 -0.21615322  0.30667388 -0.2869593  -0.9971496   2.4924402 ]][0m
[37m[1m[2023-07-11 00:32:51,087][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:33:00,123][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 00:33:00,123][233954] FPS: 425031.16[0m
[36m[2023-07-11 00:33:00,126][233954] itr=40, itrs=2000, Progress: 2.00%[0m
[37m[1m[2023-07-11 00:35:16,270][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000020[0m
[36m[2023-07-11 00:35:28,590][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 00:35:28,590][233954] FPS: 331791.04[0m
[36m[2023-07-11 00:35:32,752][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:35:32,752][233954] Reward + Measures: [[6047.19364272    0.21469332    0.25222933    0.21839567    0.04828967
     1.4269017 ]][0m
[37m[1m[2023-07-11 00:35:32,753][233954] Max Reward on eval: 6047.193642717443[0m
[37m[1m[2023-07-11 00:35:32,753][233954] Min Reward on eval: 6047.193642717443[0m
[37m[1m[2023-07-11 00:35:32,753][233954] Mean Reward across all agents: 6047.193642717443[0m
[37m[1m[2023-07-11 00:35:32,753][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:35:37,779][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:35:37,785][233954] Reward + Measures: [[4610.50859452    0.193         0.2306        0.19670001    0.0634
     1.39041877]
 [5400.53805541    0.20640002    0.23400001    0.20700002    0.0596
     1.42769992]
 [6093.11761473    0.21479999    0.23889999    0.22130001    0.0431
     1.49849689]
 ...
 [5078.46994021    0.23239999    0.2581        0.23699999    0.0772
     1.52402782]
 [2970.63267523    0.14660001    0.1953        0.1578        0.0547
     1.19650912]
 [5298.36877446    0.2299        0.28889999    0.23910001    0.0832
     1.55020869]][0m
[37m[1m[2023-07-11 00:35:37,785][233954] Max Reward on eval: 6093.117614728306[0m
[37m[1m[2023-07-11 00:35:37,786][233954] Min Reward on eval: 1588.2387151992414[0m
[37m[1m[2023-07-11 00:35:37,786][233954] Mean Reward across all agents: 4243.149542326405[0m
[37m[1m[2023-07-11 00:35:37,786][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:35:37,789][233954] mean_value=-170.25872213091256, max_value=3541.1338082292996[0m
[37m[1m[2023-07-11 00:35:37,792][233954] New mean coefficients: [[ 0.23493765 -0.05825846  0.38572097 -0.2829567  -0.7214913   2.9477084 ]][0m
[37m[1m[2023-07-11 00:35:37,793][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:35:46,568][233954] train() took 8.77 seconds to complete[0m
[36m[2023-07-11 00:35:46,568][233954] FPS: 437673.38[0m
[36m[2023-07-11 00:35:46,570][233954] itr=41, itrs=2000, Progress: 2.05%[0m
[36m[2023-07-11 00:35:58,189][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 00:35:58,189][233954] FPS: 331161.16[0m
[36m[2023-07-11 00:36:02,432][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:36:02,432][233954] Reward + Measures: [[6110.29462164    0.21266532    0.24181034    0.21582399    0.04182233
     1.4784925 ]][0m
[37m[1m[2023-07-11 00:36:02,432][233954] Max Reward on eval: 6110.29462164218[0m
[37m[1m[2023-07-11 00:36:02,432][233954] Min Reward on eval: 6110.29462164218[0m
[37m[1m[2023-07-11 00:36:02,433][233954] Mean Reward across all agents: 6110.29462164218[0m
[37m[1m[2023-07-11 00:36:02,433][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:36:07,424][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:36:07,425][233954] Reward + Measures: [[ 695.3375006     0.14620002    0.24190001    0.15210001    0.17169999
     1.56928957]
 [2223.44021606    0.189         0.3231        0.21890001    0.1751
     1.34809268]
 [1697.62875366    0.1957        0.22430001    0.1938        0.14530002
     1.17582166]
 ...
 [ 482.71650433    0.4276        0.5546        0.3973        0.53459996
     1.69995141]
 [ 966.12809942    0.21370001    0.28200004    0.23220001    0.1771
     1.31303442]
 [ 712.55094727    0.3364        0.27340001    0.3127        0.19919999
     1.31399953]][0m
[37m[1m[2023-07-11 00:36:07,425][233954] Max Reward on eval: 5667.877563474886[0m
[37m[1m[2023-07-11 00:36:07,425][233954] Min Reward on eval: 70.76758534274995[0m
[37m[1m[2023-07-11 00:36:07,425][233954] Mean Reward across all agents: 1792.9624089351987[0m
[37m[1m[2023-07-11 00:36:07,426][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:36:07,431][233954] mean_value=-2069.1729612301383, max_value=2259.395356383752[0m
[37m[1m[2023-07-11 00:36:07,439][233954] New mean coefficients: [[ 0.4118399  -0.33863318  0.54569983 -0.54696524 -0.5417254   3.0099633 ]][0m
[37m[1m[2023-07-11 00:36:07,440][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:36:16,410][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 00:36:16,411][233954] FPS: 428141.55[0m
[36m[2023-07-11 00:36:16,413][233954] itr=42, itrs=2000, Progress: 2.10%[0m
[36m[2023-07-11 00:36:28,014][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 00:36:28,015][233954] FPS: 331648.56[0m
[36m[2023-07-11 00:36:32,263][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:36:32,264][233954] Reward + Measures: [[6156.24926534    0.21375966    0.24314798    0.21693298    0.042214
     1.51110709]][0m
[37m[1m[2023-07-11 00:36:32,264][233954] Max Reward on eval: 6156.249265341136[0m
[37m[1m[2023-07-11 00:36:32,264][233954] Min Reward on eval: 6156.249265341136[0m
[37m[1m[2023-07-11 00:36:32,264][233954] Mean Reward across all agents: 6156.249265341136[0m
[37m[1m[2023-07-11 00:36:32,265][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:36:37,461][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:36:37,467][233954] Reward + Measures: [[1423.41571906    0.2181        0.3766        0.19530001    0.25780001
     1.3030647 ]
 [3182.74641419    0.2052        0.31029999    0.2411        0.12850001
     1.37406981]
 [2481.96788787    0.20810001    0.29830003    0.22449999    0.14819999
     1.40666544]
 ...
 [1549.26340769    0.2057        0.36260003    0.19589999    0.2325
     1.27485836]
 [2260.28474808    0.16940001    0.24730001    0.1846        0.13610001
     1.30005312]
 [3965.64022825    0.18349999    0.2464        0.1954        0.08880001
     1.42540228]][0m
[37m[1m[2023-07-11 00:36:37,468][233954] Max Reward on eval: 6185.366332982105[0m
[37m[1m[2023-07-11 00:36:37,468][233954] Min Reward on eval: -10.131804698705674[0m
[37m[1m[2023-07-11 00:36:37,469][233954] Mean Reward across all agents: 2374.2165544069335[0m
[37m[1m[2023-07-11 00:36:37,469][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:36:37,483][233954] mean_value=-1028.663100545364, max_value=2203.366440933787[0m
[37m[1m[2023-07-11 00:36:37,486][233954] New mean coefficients: [[ 0.82579    -1.1366184   1.3056386  -0.44288158 -0.52682495  2.8822465 ]][0m
[37m[1m[2023-07-11 00:36:37,488][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:36:46,519][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 00:36:46,519][233954] FPS: 425278.84[0m
[36m[2023-07-11 00:36:46,521][233954] itr=43, itrs=2000, Progress: 2.15%[0m
[36m[2023-07-11 00:36:58,116][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 00:36:58,117][233954] FPS: 331831.20[0m
[36m[2023-07-11 00:37:02,418][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:37:02,423][233954] Reward + Measures: [[6346.02435783    0.21104102    0.231552      0.21346       0.03519066
     1.54508543]][0m
[37m[1m[2023-07-11 00:37:02,424][233954] Max Reward on eval: 6346.024357829285[0m
[37m[1m[2023-07-11 00:37:02,424][233954] Min Reward on eval: 6346.024357829285[0m
[37m[1m[2023-07-11 00:37:02,424][233954] Mean Reward across all agents: 6346.024357829285[0m
[37m[1m[2023-07-11 00:37:02,425][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:37:07,484][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:37:07,490][233954] Reward + Measures: [[2702.5425186     0.1365        0.1894        0.1468        0.0676
     1.33589983]
 [3613.70315552    0.1937        0.3443        0.2357        0.13
     1.56833315]
 [1408.20869062    0.1647        0.25920001    0.20100001    0.14310001
     1.74456596]
 ...
 [1237.10540389    0.1525        0.28010002    0.18609999    0.22080003
     1.59930527]
 [4662.67561344    0.20609999    0.26810002    0.2096        0.0708
     1.47221625]
 [2003.5953674     0.23079999    0.37189999    0.23280001    0.2474
     1.35022295]][0m
[37m[1m[2023-07-11 00:37:07,490][233954] Max Reward on eval: 5731.767730685696[0m
[37m[1m[2023-07-11 00:37:07,490][233954] Min Reward on eval: 323.65625616838224[0m
[37m[1m[2023-07-11 00:37:07,490][233954] Mean Reward across all agents: 2300.6469238492796[0m
[37m[1m[2023-07-11 00:37:07,491][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:37:07,496][233954] mean_value=-847.9466063807819, max_value=2334.2730197120377[0m
[37m[1m[2023-07-11 00:37:07,499][233954] New mean coefficients: [[ 0.8738227  -1.6825104   1.1326125  -1.2202654  -0.76305836  2.8698006 ]][0m
[37m[1m[2023-07-11 00:37:07,500][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:37:16,563][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 00:37:16,564][233954] FPS: 423748.74[0m
[36m[2023-07-11 00:37:16,566][233954] itr=44, itrs=2000, Progress: 2.20%[0m
[36m[2023-07-11 00:37:28,100][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 00:37:28,100][233954] FPS: 333660.25[0m
[36m[2023-07-11 00:37:32,391][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:37:32,392][233954] Reward + Measures: [[6471.8153594     0.20080265    0.21799134    0.20315665    0.02838833
     1.55443811]][0m
[37m[1m[2023-07-11 00:37:32,392][233954] Max Reward on eval: 6471.815359402095[0m
[37m[1m[2023-07-11 00:37:32,392][233954] Min Reward on eval: 6471.815359402095[0m
[37m[1m[2023-07-11 00:37:32,393][233954] Mean Reward across all agents: 6471.815359402095[0m
[37m[1m[2023-07-11 00:37:32,393][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:37:37,377][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:37:37,377][233954] Reward + Measures: [[3157.74695586    0.19240001    0.27500001    0.21350001    0.1237
     1.35183799]
 [2158.71713252    0.18980001    0.30969998    0.19920002    0.12410001
     1.3876766 ]
 [ 914.47916796    0.11589999    0.1767        0.1319        0.1158
     1.22949731]
 ...
 [1562.59776692    0.24229999    0.37550002    0.23819999    0.25540003
     1.32232726]
 [1226.72607801    0.1919        0.36489999    0.22430001    0.20560001
     1.28802049]
 [ 651.66648481    0.23810001    0.51210004    0.31389999    0.44120002
     1.51429594]][0m
[37m[1m[2023-07-11 00:37:37,377][233954] Max Reward on eval: 5103.874053944647[0m
[37m[1m[2023-07-11 00:37:37,378][233954] Min Reward on eval: 201.33484459230675[0m
[37m[1m[2023-07-11 00:37:37,378][233954] Mean Reward across all agents: 1895.949494569775[0m
[37m[1m[2023-07-11 00:37:37,378][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:37:37,382][233954] mean_value=-2024.5499309181162, max_value=2012.1073131581768[0m
[37m[1m[2023-07-11 00:37:37,384][233954] New mean coefficients: [[ 1.0599458  -1.4894011   2.4604406  -1.0720291  -0.25303537  3.7458088 ]][0m
[37m[1m[2023-07-11 00:37:37,385][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:37:46,458][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 00:37:46,458][233954] FPS: 423321.21[0m
[36m[2023-07-11 00:37:46,461][233954] itr=45, itrs=2000, Progress: 2.25%[0m
[36m[2023-07-11 00:37:58,242][233954] train() took 11.75 seconds to complete[0m
[36m[2023-07-11 00:37:58,243][233954] FPS: 326676.56[0m
[36m[2023-07-11 00:38:02,578][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:38:02,578][233954] Reward + Measures: [[6529.96942704    0.20174967    0.21582766    0.20319767    0.02637133
     1.58639741]][0m
[37m[1m[2023-07-11 00:38:02,578][233954] Max Reward on eval: 6529.969427036896[0m
[37m[1m[2023-07-11 00:38:02,579][233954] Min Reward on eval: 6529.969427036896[0m
[37m[1m[2023-07-11 00:38:02,579][233954] Mean Reward across all agents: 6529.969427036896[0m
[37m[1m[2023-07-11 00:38:02,579][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:38:07,623][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:38:07,629][233954] Reward + Measures: [[2586.96978566    0.21949999    0.34080002    0.2306        0.14530002
     1.50112951]
 [1154.77249146    0.17550001    0.2534        0.1701        0.1495
     1.60436714]
 [1948.96842194    0.20549999    0.33489999    0.22740002    0.15820001
     1.59387994]
 ...
 [ 137.88158264    0.1069        0.18180001    0.0949        0.19630001
     2.22836947]
 [ 930.11474702    0.16610001    0.2791        0.1864        0.16160001
     1.61129797]
 [2334.92913819    0.1876        0.2624        0.2045        0.11259999
     1.39269614]][0m
[37m[1m[2023-07-11 00:38:07,629][233954] Max Reward on eval: 6033.921020486628[0m
[37m[1m[2023-07-11 00:38:07,629][233954] Min Reward on eval: 2.0665758924558757[0m
[37m[1m[2023-07-11 00:38:07,630][233954] Mean Reward across all agents: 1350.7994089018146[0m
[37m[1m[2023-07-11 00:38:07,630][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:38:07,634][233954] mean_value=-1303.2611471285445, max_value=1389.3361692402377[0m
[37m[1m[2023-07-11 00:38:07,636][233954] New mean coefficients: [[ 1.1476051  -1.556563    3.1486895  -0.57643247 -0.2480832   3.387414  ]][0m
[37m[1m[2023-07-11 00:38:07,637][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:38:16,801][233954] train() took 9.16 seconds to complete[0m
[36m[2023-07-11 00:38:16,802][233954] FPS: 419105.88[0m
[36m[2023-07-11 00:38:16,804][233954] itr=46, itrs=2000, Progress: 2.30%[0m
[36m[2023-07-11 00:38:28,537][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 00:38:28,537][233954] FPS: 328034.84[0m
[36m[2023-07-11 00:38:32,868][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:38:32,869][233954] Reward + Measures: [[6665.51874154    0.20067398    0.211564      0.20297368    0.02418767
     1.61613214]][0m
[37m[1m[2023-07-11 00:38:32,869][233954] Max Reward on eval: 6665.518741543099[0m
[37m[1m[2023-07-11 00:38:32,869][233954] Min Reward on eval: 6665.518741543099[0m
[37m[1m[2023-07-11 00:38:32,870][233954] Mean Reward across all agents: 6665.518741543099[0m
[37m[1m[2023-07-11 00:38:32,870][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:38:38,010][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:38:38,010][233954] Reward + Measures: [[339.68767551   0.16039999   0.15809999   0.17820001   0.1434
    2.87083507]
 [364.87272264   0.1629       0.2041       0.20970002   0.2264
    1.78703177]
 [357.2513273    0.2306       0.21640001   0.22239999   0.2067
    2.2538681 ]
 ...
 [742.40387536   0.2253       0.2271       0.23460002   0.1929
    2.02406287]
 [ 95.46075574   0.13310002   0.149        0.15280001   0.11880001
    3.36121535]
 [234.09323083   0.1566       0.16150001   0.1468       0.12410001
    2.89042449]][0m
[37m[1m[2023-07-11 00:38:38,011][233954] Max Reward on eval: 3517.1493224937467[0m
[37m[1m[2023-07-11 00:38:38,011][233954] Min Reward on eval: -3.300339000718668[0m
[37m[1m[2023-07-11 00:38:38,011][233954] Mean Reward across all agents: 532.9211860356717[0m
[37m[1m[2023-07-11 00:38:38,011][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:38:38,021][233954] mean_value=191.29493980809846, max_value=1570.7973176228993[0m
[37m[1m[2023-07-11 00:38:38,024][233954] New mean coefficients: [[ 1.4634571  -1.7219087   3.2163596  -0.35585254 -0.15769167  3.875287  ]][0m
[37m[1m[2023-07-11 00:38:38,025][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:38:46,998][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 00:38:46,998][233954] FPS: 428038.45[0m
[36m[2023-07-11 00:38:47,000][233954] itr=47, itrs=2000, Progress: 2.35%[0m
[36m[2023-07-11 00:38:58,682][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 00:38:58,682][233954] FPS: 329370.88[0m
[36m[2023-07-11 00:39:03,013][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:39:03,013][233954] Reward + Measures: [[6657.61376175    0.19754899    0.206591      0.20112064    0.02526633
     1.62342191]][0m
[37m[1m[2023-07-11 00:39:03,014][233954] Max Reward on eval: 6657.613761750204[0m
[37m[1m[2023-07-11 00:39:03,014][233954] Min Reward on eval: 6657.613761750204[0m
[37m[1m[2023-07-11 00:39:03,014][233954] Mean Reward across all agents: 6657.613761750204[0m
[37m[1m[2023-07-11 00:39:03,014][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:39:08,073][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:39:08,073][233954] Reward + Measures: [[ 323.22462277    0.3193        0.36920002    0.26370001    0.19500001
     1.47579181]
 [ 361.63790654    0.21040002    0.2949        0.2581        0.19490002
     1.57340896]
 [ 228.04200978    0.22909999    0.26820001    0.24650002    0.1416
     1.90608013]
 ...
 [ 394.89304163    0.35139999    0.35639998    0.28640002    0.19480002
     1.70433831]
 [ 221.71413853    0.30780002    0.38299999    0.26770002    0.22319999
     1.63127863]
 [1077.71196747    0.3741        0.3831        0.36180001    0.1983
     1.19714475]][0m
[37m[1m[2023-07-11 00:39:08,074][233954] Max Reward on eval: 5501.010665910691[0m
[37m[1m[2023-07-11 00:39:08,074][233954] Min Reward on eval: 36.596314831636846[0m
[37m[1m[2023-07-11 00:39:08,074][233954] Mean Reward across all agents: 690.9563407233906[0m
[37m[1m[2023-07-11 00:39:08,074][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:39:08,078][233954] mean_value=-1716.9276002491758, max_value=1236.0172511932403[0m
[37m[1m[2023-07-11 00:39:08,081][233954] New mean coefficients: [[ 1.7540042  -2.3778887   3.5820725  -0.15872332 -0.7372057   4.3628254 ]][0m
[37m[1m[2023-07-11 00:39:08,082][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:39:17,137][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 00:39:17,138][233954] FPS: 424144.37[0m
[36m[2023-07-11 00:39:17,140][233954] itr=48, itrs=2000, Progress: 2.40%[0m
[36m[2023-07-11 00:39:29,137][233954] train() took 11.97 seconds to complete[0m
[36m[2023-07-11 00:39:29,138][233954] FPS: 320758.69[0m
[36m[2023-07-11 00:39:33,430][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:39:33,431][233954] Reward + Measures: [[6755.91635513    0.19991931    0.20453833    0.20132999    0.022778
     1.66173398]][0m
[37m[1m[2023-07-11 00:39:33,431][233954] Max Reward on eval: 6755.916355127111[0m
[37m[1m[2023-07-11 00:39:33,431][233954] Min Reward on eval: 6755.916355127111[0m
[37m[1m[2023-07-11 00:39:33,432][233954] Mean Reward across all agents: 6755.916355127111[0m
[37m[1m[2023-07-11 00:39:33,432][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:39:38,468][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:39:38,468][233954] Reward + Measures: [[ 490.26559873    0.17760001    0.26430002    0.2775        0.1691
     2.04565787]
 [1282.1688156     0.1709        0.3425        0.2172        0.19330001
     1.66985154]
 [ 325.72925162    0.14489999    0.20189999    0.1831        0.13110001
     2.2192142 ]
 ...
 [ 675.65795517    0.1679        0.36040002    0.27579999    0.22930002
     1.98589122]
 [ 596.8061371     0.17550001    0.26550004    0.2119        0.1242
     1.84856987]
 [ 496.77939895    0.15939999    0.28580001    0.22830001    0.16540001
     2.10631061]][0m
[37m[1m[2023-07-11 00:39:38,469][233954] Max Reward on eval: 6146.684020996839[0m
[37m[1m[2023-07-11 00:39:38,469][233954] Min Reward on eval: 188.21000245520844[0m
[37m[1m[2023-07-11 00:39:38,469][233954] Mean Reward across all agents: 973.6101908831898[0m
[37m[1m[2023-07-11 00:39:38,469][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:39:38,472][233954] mean_value=-1016.3057299681386, max_value=1957.7472534574567[0m
[37m[1m[2023-07-11 00:39:38,475][233954] New mean coefficients: [[ 1.8870078  -2.710718    3.7038586  -0.43966994 -0.6253342   4.2295303 ]][0m
[37m[1m[2023-07-11 00:39:38,476][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:39:47,628][233954] train() took 9.15 seconds to complete[0m
[36m[2023-07-11 00:39:47,628][233954] FPS: 419633.63[0m
[36m[2023-07-11 00:39:47,631][233954] itr=49, itrs=2000, Progress: 2.45%[0m
[36m[2023-07-11 00:39:59,418][233954] train() took 11.76 seconds to complete[0m
[36m[2023-07-11 00:39:59,418][233954] FPS: 326437.04[0m
[36m[2023-07-11 00:40:03,689][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:40:03,689][233954] Reward + Measures: [[6840.47091569    0.19489633    0.19734232    0.19549768    0.02023467
     1.66676533]][0m
[37m[1m[2023-07-11 00:40:03,689][233954] Max Reward on eval: 6840.470915689551[0m
[37m[1m[2023-07-11 00:40:03,689][233954] Min Reward on eval: 6840.470915689551[0m
[37m[1m[2023-07-11 00:40:03,690][233954] Mean Reward across all agents: 6840.470915689551[0m
[37m[1m[2023-07-11 00:40:03,690][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:40:08,730][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:40:08,730][233954] Reward + Measures: [[522.09856902   0.15350001   0.27600002   0.19499998   0.1693
    1.68773079]
 [ -2.70731453   0.12730001   0.207        0.0729       0.12580001
    2.64083123]
 [289.60279848   0.11310001   0.19159999   0.10739999   0.1186
    1.81555939]
 ...
 [564.27509009   0.16         0.28299999   0.21080001   0.1813
    1.6474005 ]
 [268.86396502   0.1188       0.21870001   0.13990001   0.14330001
    1.8291682 ]
 [232.08045531   0.2026       0.36379999   0.17580001   0.27330002
    1.9811945 ]][0m
[37m[1m[2023-07-11 00:40:08,731][233954] Max Reward on eval: 4977.766967817023[0m
[37m[1m[2023-07-11 00:40:08,731][233954] Min Reward on eval: -496.2774533388671[0m
[37m[1m[2023-07-11 00:40:08,731][233954] Mean Reward across all agents: 558.3275001519839[0m
[37m[1m[2023-07-11 00:40:08,731][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:40:08,738][233954] mean_value=-1050.5160172713556, max_value=958.1574227931[0m
[37m[1m[2023-07-11 00:40:08,741][233954] New mean coefficients: [[ 2.0407948  -3.025627    4.713224   -0.27178758 -0.42868808  4.4963346 ]][0m
[37m[1m[2023-07-11 00:40:08,742][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:40:17,744][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 00:40:17,744][233954] FPS: 426632.70[0m
[36m[2023-07-11 00:40:17,747][233954] itr=50, itrs=2000, Progress: 2.50%[0m
[37m[1m[2023-07-11 00:42:36,353][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000030[0m
[36m[2023-07-11 00:42:48,606][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 00:42:48,606][233954] FPS: 333103.83[0m
[36m[2023-07-11 00:42:52,919][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:42:52,928][233954] Reward + Measures: [[6905.60071908    0.19606468    0.19761398    0.19704667    0.01977467
     1.68834734]][0m
[37m[1m[2023-07-11 00:42:52,928][233954] Max Reward on eval: 6905.600719083716[0m
[37m[1m[2023-07-11 00:42:52,929][233954] Min Reward on eval: 6905.600719083716[0m
[37m[1m[2023-07-11 00:42:52,929][233954] Mean Reward across all agents: 6905.600719083716[0m
[37m[1m[2023-07-11 00:42:52,929][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:42:58,128][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:42:58,129][233954] Reward + Measures: [[371.16635421   0.35929999   0.24870001   0.38159999   0.2112
    2.02720046]
 [633.64595423   0.1987       0.31169999   0.18440001   0.1611
    1.81149757]
 [620.57225037   0.24800001   0.2929       0.25279999   0.1646
    1.868276  ]
 ...
 [426.41493444   0.24800001   0.24900003   0.27760002   0.176
    2.08463836]
 [619.04115962   0.24629998   0.27809998   0.27519998   0.1969
    1.98578155]
 [902.99704362   0.2929       0.3425       0.29609999   0.2146
    1.7548672 ]][0m
[37m[1m[2023-07-11 00:42:58,129][233954] Max Reward on eval: 2177.359756497294[0m
[37m[1m[2023-07-11 00:42:58,129][233954] Min Reward on eval: 86.97604076378047[0m
[37m[1m[2023-07-11 00:42:58,130][233954] Mean Reward across all agents: 601.6987060992261[0m
[37m[1m[2023-07-11 00:42:58,130][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:42:58,137][233954] mean_value=-315.14187973719294, max_value=1063.0913703033143[0m
[37m[1m[2023-07-11 00:42:58,140][233954] New mean coefficients: [[ 2.0484943  -3.248633    5.397004   -0.40277758  0.31499657  4.3995366 ]][0m
[37m[1m[2023-07-11 00:42:58,141][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:43:07,107][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 00:43:07,108][233954] FPS: 428342.01[0m
[36m[2023-07-11 00:43:07,110][233954] itr=51, itrs=2000, Progress: 2.55%[0m
[36m[2023-07-11 00:43:18,902][233954] train() took 11.76 seconds to complete[0m
[36m[2023-07-11 00:43:18,902][233954] FPS: 326392.25[0m
[36m[2023-07-11 00:43:23,155][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:43:23,155][233954] Reward + Measures: [[7039.29175232    0.19656366    0.19652334    0.19643766    0.018116
     1.71314943]][0m
[37m[1m[2023-07-11 00:43:23,155][233954] Max Reward on eval: 7039.29175231724[0m
[37m[1m[2023-07-11 00:43:23,156][233954] Min Reward on eval: 7039.29175231724[0m
[37m[1m[2023-07-11 00:43:23,156][233954] Mean Reward across all agents: 7039.29175231724[0m
[37m[1m[2023-07-11 00:43:23,156][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:43:28,163][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:43:28,164][233954] Reward + Measures: [[280.90710285   0.38390002   0.24689999   0.32220003   0.28650004
    2.17080617]
 [504.92106821   0.31029999   0.3712       0.211        0.31329998
    1.84575677]
 [384.41197109   0.32160002   0.3048       0.2369       0.30599999
    2.03917718]
 ...
 [132.03113061   0.32370001   0.25440001   0.2343       0.28310001
    2.37069345]
 [538.03314021   0.35010001   0.3362       0.28049999   0.32339999
    1.80404508]
 [214.40559768   0.38060001   0.26719999   0.30310002   0.2888
    2.18800473]][0m
[37m[1m[2023-07-11 00:43:28,164][233954] Max Reward on eval: 2864.597587583214[0m
[37m[1m[2023-07-11 00:43:28,164][233954] Min Reward on eval: -27.91974976551719[0m
[37m[1m[2023-07-11 00:43:28,164][233954] Mean Reward across all agents: 304.5334543418253[0m
[37m[1m[2023-07-11 00:43:28,165][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:43:28,173][233954] mean_value=260.72973074221375, max_value=1154.6167707339873[0m
[37m[1m[2023-07-11 00:43:28,176][233954] New mean coefficients: [[ 1.5096681  -3.1396904   4.623195    0.03005058 -0.05295473  4.317484  ]][0m
[37m[1m[2023-07-11 00:43:28,177][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:43:37,190][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 00:43:37,190][233954] FPS: 426124.30[0m
[36m[2023-07-11 00:43:37,193][233954] itr=52, itrs=2000, Progress: 2.60%[0m
[36m[2023-07-11 00:43:48,999][233954] train() took 11.78 seconds to complete[0m
[36m[2023-07-11 00:43:49,004][233954] FPS: 326009.77[0m
[36m[2023-07-11 00:43:53,313][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:43:53,313][233954] Reward + Measures: [[7143.13353221    0.19673035    0.19757934    0.19686666    0.01700433
     1.72688806]][0m
[37m[1m[2023-07-11 00:43:53,314][233954] Max Reward on eval: 7143.133532206415[0m
[37m[1m[2023-07-11 00:43:53,314][233954] Min Reward on eval: 7143.133532206415[0m
[37m[1m[2023-07-11 00:43:53,314][233954] Mean Reward across all agents: 7143.133532206415[0m
[37m[1m[2023-07-11 00:43:53,314][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:43:58,312][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:43:58,312][233954] Reward + Measures: [[ 60.236242     0.1507       0.35530001   0.16110002   0.37019998
    2.16489887]
 [376.55492209   0.11800001   0.18260001   0.1374       0.15190001
    2.17940903]
 [947.94582174   0.20659998   0.30430001   0.1961       0.21710001
    1.54459691]
 ...
 [305.2502899    0.14910001   0.28660002   0.20200001   0.27239999
    2.01565647]
 [602.12651825   0.2163       0.375        0.2263       0.3355
    1.7247076 ]
 [358.62162471   0.16070001   0.2414       0.1478       0.1857
    2.20015216]][0m
[37m[1m[2023-07-11 00:43:58,313][233954] Max Reward on eval: 2179.78869625926[0m
[37m[1m[2023-07-11 00:43:58,313][233954] Min Reward on eval: -2.4690320495516063[0m
[37m[1m[2023-07-11 00:43:58,313][233954] Mean Reward across all agents: 399.63454547499344[0m
[37m[1m[2023-07-11 00:43:58,314][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:43:58,319][233954] mean_value=-395.41658589961344, max_value=869.6025884173811[0m
[37m[1m[2023-07-11 00:43:58,321][233954] New mean coefficients: [[ 1.7512404  -3.6513627   4.5264473   0.32784104 -0.12469608  4.0578117 ]][0m
[37m[1m[2023-07-11 00:43:58,322][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:44:07,325][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 00:44:07,325][233954] FPS: 426615.06[0m
[36m[2023-07-11 00:44:07,327][233954] itr=53, itrs=2000, Progress: 2.65%[0m
[36m[2023-07-11 00:44:18,870][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 00:44:18,871][233954] FPS: 333363.64[0m
[36m[2023-07-11 00:44:23,136][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:44:23,137][233954] Reward + Measures: [[7151.16641097    0.19904867    0.20070733    0.19980033    0.01687933
     1.74315619]][0m
[37m[1m[2023-07-11 00:44:23,137][233954] Max Reward on eval: 7151.166410968922[0m
[37m[1m[2023-07-11 00:44:23,137][233954] Min Reward on eval: 7151.166410968922[0m
[37m[1m[2023-07-11 00:44:23,138][233954] Mean Reward across all agents: 7151.166410968922[0m
[37m[1m[2023-07-11 00:44:23,138][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:44:28,199][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:44:28,200][233954] Reward + Measures: [[256.63038635   0.34390002   0.2405       0.3653       0.20710002
    1.9174881 ]
 [333.60491376   0.3132       0.33839998   0.2712       0.2359
    1.93938756]
 [103.81951652   0.34780002   0.30350003   0.27950001   0.2256
    2.30016804]
 ...
 [ 98.03670321   0.2545       0.15869999   0.26070002   0.16950002
    2.36280227]
 [150.20904629   0.2115       0.1498       0.24319999   0.155
    2.39194918]
 [380.57931998   0.2326       0.28219998   0.24270001   0.20220001
    1.64199638]][0m
[37m[1m[2023-07-11 00:44:28,200][233954] Max Reward on eval: 2155.4255943526514[0m
[37m[1m[2023-07-11 00:44:28,200][233954] Min Reward on eval: -152.00516803115607[0m
[37m[1m[2023-07-11 00:44:28,200][233954] Mean Reward across all agents: 230.63006319361642[0m
[37m[1m[2023-07-11 00:44:28,201][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:44:28,207][233954] mean_value=-533.9555633347062, max_value=1129.3120498402045[0m
[37m[1m[2023-07-11 00:44:28,210][233954] New mean coefficients: [[ 1.6952994  -3.0230713   4.785638    0.15084088  0.11507925  3.686742  ]][0m
[37m[1m[2023-07-11 00:44:28,211][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:44:37,223][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 00:44:37,223][233954] FPS: 426181.57[0m
[36m[2023-07-11 00:44:37,225][233954] itr=54, itrs=2000, Progress: 2.70%[0m
[36m[2023-07-11 00:44:49,009][233954] train() took 11.76 seconds to complete[0m
[36m[2023-07-11 00:44:49,010][233954] FPS: 326536.85[0m
[36m[2023-07-11 00:44:53,304][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:44:53,304][233954] Reward + Measures: [[7192.22543728    0.19905867    0.20030467    0.19840065    0.01534633
     1.76057649]][0m
[37m[1m[2023-07-11 00:44:53,304][233954] Max Reward on eval: 7192.225437275415[0m
[37m[1m[2023-07-11 00:44:53,305][233954] Min Reward on eval: 7192.225437275415[0m
[37m[1m[2023-07-11 00:44:53,305][233954] Mean Reward across all agents: 7192.225437275415[0m
[37m[1m[2023-07-11 00:44:53,305][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:44:58,325][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:44:58,326][233954] Reward + Measures: [[143.7124211    0.127        0.20010002   0.0923       0.19350001
    2.16391158]
 [158.59376264   0.11620001   0.19360001   0.11069999   0.19220001
    2.24648571]
 [156.63404079   0.11110001   0.09610001   0.0619       0.13610001
    2.21818757]
 ...
 [ 93.51443147   0.0905       0.1346       0.1005       0.1453
    2.38812733]
 [ 85.33472982   0.1683       0.221        0.0915       0.2208
    2.56901908]
 [718.6703253    0.147        0.2261       0.11259999   0.15369999
    1.75814366]][0m
[37m[1m[2023-07-11 00:44:58,326][233954] Max Reward on eval: 1646.9090500026941[0m
[37m[1m[2023-07-11 00:44:58,326][233954] Min Reward on eval: -8.869827530905605[0m
[37m[1m[2023-07-11 00:44:58,327][233954] Mean Reward across all agents: 250.60611836006555[0m
[37m[1m[2023-07-11 00:44:58,327][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:44:58,334][233954] mean_value=-146.78767631589673, max_value=698.5203080604551[0m
[37m[1m[2023-07-11 00:44:58,337][233954] New mean coefficients: [[ 1.6971794  -3.682042    4.7344418  -0.02420931 -0.26392406  3.8065014 ]][0m
[37m[1m[2023-07-11 00:44:58,338][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:45:07,454][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 00:45:07,454][233954] FPS: 421306.71[0m
[36m[2023-07-11 00:45:07,457][233954] itr=55, itrs=2000, Progress: 2.75%[0m
[36m[2023-07-11 00:45:19,060][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 00:45:19,065][233954] FPS: 331646.77[0m
[36m[2023-07-11 00:45:23,404][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:45:23,405][233954] Reward + Measures: [[7204.7573877     0.196275      0.19615535    0.19527966    0.014717
     1.76940441]][0m
[37m[1m[2023-07-11 00:45:23,405][233954] Max Reward on eval: 7204.757387699161[0m
[37m[1m[2023-07-11 00:45:23,405][233954] Min Reward on eval: 7204.757387699161[0m
[37m[1m[2023-07-11 00:45:23,405][233954] Mean Reward across all agents: 7204.757387699161[0m
[37m[1m[2023-07-11 00:45:23,405][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:45:28,590][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:45:28,590][233954] Reward + Measures: [[1286.50743867    0.20369999    0.32210001    0.1631        0.1928
     1.20430827]
 [1499.77760318    0.17480001    0.23670001    0.14559999    0.1285
     1.1949091 ]
 [ 544.39567758    0.1383        0.21510001    0.1135        0.13970001
     1.35680377]
 ...
 [1082.4138556     0.23670001    0.23500001    0.24980001    0.14670001
     1.53008521]
 [ 724.41850951    0.1734        0.2942        0.15350001    0.1892
     1.54397583]
 [ 986.13287259    0.17389999    0.29840001    0.13410001    0.1718
     1.3110739 ]][0m
[37m[1m[2023-07-11 00:45:28,590][233954] Max Reward on eval: 4051.317783413222[0m
[37m[1m[2023-07-11 00:45:28,591][233954] Min Reward on eval: 261.0794238876086[0m
[37m[1m[2023-07-11 00:45:28,591][233954] Mean Reward across all agents: 1181.3280597206656[0m
[37m[1m[2023-07-11 00:45:28,591][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:45:28,594][233954] mean_value=-2718.8520528611657, max_value=1807.5628714125453[0m
[37m[1m[2023-07-11 00:45:28,596][233954] New mean coefficients: [[ 1.6638781  -3.2554817   5.041905   -0.13200675 -0.08485806  3.2914548 ]][0m
[37m[1m[2023-07-11 00:45:28,597][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:45:37,695][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 00:45:37,695][233954] FPS: 422138.61[0m
[36m[2023-07-11 00:45:37,698][233954] itr=56, itrs=2000, Progress: 2.80%[0m
[36m[2023-07-11 00:45:49,502][233954] train() took 11.78 seconds to complete[0m
[36m[2023-07-11 00:45:49,503][233954] FPS: 326039.90[0m
[36m[2023-07-11 00:45:53,854][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:45:53,860][233954] Reward + Measures: [[7189.26448638    0.19295634    0.19751433    0.19261065    0.01546533
     1.79246998]][0m
[37m[1m[2023-07-11 00:45:53,860][233954] Max Reward on eval: 7189.264486384372[0m
[37m[1m[2023-07-11 00:45:53,860][233954] Min Reward on eval: 7189.264486384372[0m
[37m[1m[2023-07-11 00:45:53,860][233954] Mean Reward across all agents: 7189.264486384372[0m
[37m[1m[2023-07-11 00:45:53,861][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:45:58,855][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:45:58,861][233954] Reward + Measures: [[158.68857527   0.1221       0.1973       0.13060001   0.1609
    2.48895741]
 [150.8900105    0.41770002   0.1249       0.35260001   0.3777
    2.77206349]
 [291.62240507   0.245        0.45699999   0.2484       0.41529998
    2.102633  ]
 ...
 [229.39439937   0.17950001   0.12890001   0.1489       0.13420001
    2.37697721]
 [ 74.14109629   0.17830001   0.28490001   0.14320001   0.2481
    2.73044848]
 [ 72.02428082   0.44280005   0.32580003   0.39680001   0.44150001
    2.61267328]][0m
[37m[1m[2023-07-11 00:45:58,861][233954] Max Reward on eval: 770.7121048539877[0m
[37m[1m[2023-07-11 00:45:58,861][233954] Min Reward on eval: -84.62409114921465[0m
[37m[1m[2023-07-11 00:45:58,862][233954] Mean Reward across all agents: 170.54752657534297[0m
[37m[1m[2023-07-11 00:45:58,862][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:45:58,878][233954] mean_value=142.06371451269504, max_value=940.3756886796025[0m
[37m[1m[2023-07-11 00:45:58,881][233954] New mean coefficients: [[ 1.65656   -3.2679925  5.1797285  0.6447247 -0.6069962  3.069044 ]][0m
[37m[1m[2023-07-11 00:45:58,882][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:46:07,866][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 00:46:07,866][233954] FPS: 427494.30[0m
[36m[2023-07-11 00:46:07,868][233954] itr=57, itrs=2000, Progress: 2.85%[0m
[36m[2023-07-11 00:46:19,435][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 00:46:19,436][233954] FPS: 332731.24[0m
[36m[2023-07-11 00:46:23,821][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:46:23,822][233954] Reward + Measures: [[7269.06205391    0.19638233    0.20067899    0.19537199    0.01444967
     1.79913461]][0m
[37m[1m[2023-07-11 00:46:23,822][233954] Max Reward on eval: 7269.062053905055[0m
[37m[1m[2023-07-11 00:46:23,822][233954] Min Reward on eval: 7269.062053905055[0m
[37m[1m[2023-07-11 00:46:23,822][233954] Mean Reward across all agents: 7269.062053905055[0m
[37m[1m[2023-07-11 00:46:23,823][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:46:28,929][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:46:28,935][233954] Reward + Measures: [[207.8211391    0.2414       0.2465       0.21619999   0.252
    2.29096246]
 [346.16211509   0.22660001   0.24150001   0.2192       0.23529999
    2.28749895]
 [225.86805055   0.2422       0.2142       0.21859999   0.2132
    2.23568416]
 ...
 [173.56783854   0.27860001   0.24160002   0.23670001   0.26980001
    2.23711181]
 [227.95494171   0.2168       0.21959999   0.19649999   0.2228
    2.14583015]
 [601.48672673   0.19400001   0.28620002   0.2299       0.2194
    2.04627538]][0m
[37m[1m[2023-07-11 00:46:28,935][233954] Max Reward on eval: 1709.7563439040446[0m
[37m[1m[2023-07-11 00:46:28,936][233954] Min Reward on eval: -293.773893163912[0m
[37m[1m[2023-07-11 00:46:28,936][233954] Mean Reward across all agents: 280.1925223574678[0m
[37m[1m[2023-07-11 00:46:28,936][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:46:28,941][233954] mean_value=-268.4695961476434, max_value=860.4868304677308[0m
[37m[1m[2023-07-11 00:46:28,944][233954] New mean coefficients: [[ 1.66386    -3.5429933   4.4651246   0.89046377 -1.6090806   2.6464298 ]][0m
[37m[1m[2023-07-11 00:46:28,945][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:46:38,093][233954] train() took 9.15 seconds to complete[0m
[36m[2023-07-11 00:46:38,093][233954] FPS: 419835.15[0m
[36m[2023-07-11 00:46:38,096][233954] itr=58, itrs=2000, Progress: 2.90%[0m
[36m[2023-07-11 00:46:49,860][233954] train() took 11.73 seconds to complete[0m
[36m[2023-07-11 00:46:49,861][233954] FPS: 327230.54[0m
[36m[2023-07-11 00:46:54,156][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:46:54,157][233954] Reward + Measures: [[7332.95387298    0.19411999    0.19784534    0.19423202    0.012682
     1.81583369]][0m
[37m[1m[2023-07-11 00:46:54,157][233954] Max Reward on eval: 7332.953872981603[0m
[37m[1m[2023-07-11 00:46:54,157][233954] Min Reward on eval: 7332.953872981603[0m
[37m[1m[2023-07-11 00:46:54,158][233954] Mean Reward across all agents: 7332.953872981603[0m
[37m[1m[2023-07-11 00:46:54,158][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:46:59,147][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:46:59,148][233954] Reward + Measures: [[208.41185863   0.23360001   0.55509996   0.2818       0.50099999
    2.53593397]
 [100.27324557   0.09249999   0.1938       0.1013       0.156
    2.39673734]
 [133.2489398    0.22579999   0.37670001   0.25560001   0.26369998
    2.26240039]
 ...
 [264.08973599   0.28870001   0.42930004   0.31760001   0.35880002
    2.01492429]
 [219.6611651    0.1173       0.27509999   0.14729999   0.28040001
    2.7431016 ]
 [190.25466095   0.1574       0.40130001   0.14530002   0.32140002
    2.28047919]][0m
[37m[1m[2023-07-11 00:46:59,148][233954] Max Reward on eval: 1473.8582344547845[0m
[37m[1m[2023-07-11 00:46:59,148][233954] Min Reward on eval: 34.631405589729546[0m
[37m[1m[2023-07-11 00:46:59,149][233954] Mean Reward across all agents: 233.08426683072594[0m
[37m[1m[2023-07-11 00:46:59,149][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:46:59,156][233954] mean_value=90.52915351114122, max_value=760.5825265344267[0m
[37m[1m[2023-07-11 00:46:59,159][233954] New mean coefficients: [[ 1.5563003 -3.9465456  4.454213   1.4157019 -2.5469446  2.428297 ]][0m
[37m[1m[2023-07-11 00:46:59,160][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:47:08,217][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 00:47:08,217][233954] FPS: 424057.54[0m
[36m[2023-07-11 00:47:08,219][233954] itr=59, itrs=2000, Progress: 2.95%[0m
[36m[2023-07-11 00:47:19,898][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 00:47:19,898][233954] FPS: 329506.44[0m
[36m[2023-07-11 00:47:24,183][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:47:24,183][233954] Reward + Measures: [[7410.89019428    0.19589567    0.20453233    0.19709133    0.0126
     1.82638848]][0m
[37m[1m[2023-07-11 00:47:24,183][233954] Max Reward on eval: 7410.890194283391[0m
[37m[1m[2023-07-11 00:47:24,184][233954] Min Reward on eval: 7410.890194283391[0m
[37m[1m[2023-07-11 00:47:24,184][233954] Mean Reward across all agents: 7410.890194283391[0m
[37m[1m[2023-07-11 00:47:24,184][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:47:29,376][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:47:29,382][233954] Reward + Measures: [[391.52338741   0.15720001   0.20190001   0.20479999   0.1373
    2.00665379]
 [463.37541389   0.2431       0.31010002   0.12349999   0.34629998
    1.54363167]
 [217.66263104   0.1586       0.17990001   0.22280002   0.1391
    2.15675855]
 ...
 [551.19991303   0.17380001   0.22690001   0.212        0.1389
    1.92603385]
 [527.71267798   0.19250001   0.17560001   0.22879998   0.1566
    1.65747583]
 [402.54762842   0.16510001   0.21259999   0.22399998   0.15440001
    2.13230443]][0m
[37m[1m[2023-07-11 00:47:29,382][233954] Max Reward on eval: 1596.2750950336456[0m
[37m[1m[2023-07-11 00:47:29,382][233954] Min Reward on eval: -20.022689781151712[0m
[37m[1m[2023-07-11 00:47:29,383][233954] Mean Reward across all agents: 300.3770422988089[0m
[37m[1m[2023-07-11 00:47:29,383][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:47:29,390][233954] mean_value=-282.7752545739346, max_value=1116.0145378548652[0m
[37m[1m[2023-07-11 00:47:29,393][233954] New mean coefficients: [[ 1.8131976  -3.8477955   5.032595    0.32886446 -1.3422629   2.3238416 ]][0m
[37m[1m[2023-07-11 00:47:29,394][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:47:38,566][233954] train() took 9.17 seconds to complete[0m
[36m[2023-07-11 00:47:38,566][233954] FPS: 418747.29[0m
[36m[2023-07-11 00:47:38,569][233954] itr=60, itrs=2000, Progress: 3.00%[0m
[37m[1m[2023-07-11 00:50:08,191][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000040[0m
[36m[2023-07-11 00:50:20,711][233954] train() took 11.76 seconds to complete[0m
[36m[2023-07-11 00:50:20,711][233954] FPS: 326547.43[0m
[36m[2023-07-11 00:50:24,909][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:50:24,910][233954] Reward + Measures: [[7314.43560276    0.19303033    0.21015331    0.19531234    0.016047
     1.81292915]][0m
[37m[1m[2023-07-11 00:50:24,910][233954] Max Reward on eval: 7314.435602761295[0m
[37m[1m[2023-07-11 00:50:24,910][233954] Min Reward on eval: 7314.435602761295[0m
[37m[1m[2023-07-11 00:50:24,911][233954] Mean Reward across all agents: 7314.435602761295[0m
[37m[1m[2023-07-11 00:50:24,911][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:50:29,893][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:50:29,893][233954] Reward + Measures: [[182.65959859   0.23179999   0.2359       0.1531       0.1637
    2.16266704]
 [ 53.66890047   0.16540001   0.24530001   0.11299999   0.2251
    2.68074131]
 [203.33714292   0.15450001   0.27680001   0.1374       0.25259998
    2.26277137]
 ...
 [364.20182609   0.2624       0.22449999   0.1631       0.15140001
    2.09117246]
 [108.26633294   0.16720001   0.22809999   0.1115       0.25369999
    2.38570738]
 [244.88609333   0.18859999   0.2343       0.133        0.18550001
    2.08029461]][0m
[37m[1m[2023-07-11 00:50:29,893][233954] Max Reward on eval: 2015.0193786660907[0m
[37m[1m[2023-07-11 00:50:29,894][233954] Min Reward on eval: -94.08684159170953[0m
[37m[1m[2023-07-11 00:50:29,894][233954] Mean Reward across all agents: 253.09758769512536[0m
[37m[1m[2023-07-11 00:50:29,894][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:50:29,902][233954] mean_value=-90.87888548995623, max_value=898.8072333735414[0m
[37m[1m[2023-07-11 00:50:29,904][233954] New mean coefficients: [[ 2.0371685  -4.298188    5.2254066  -0.32422525 -1.5047431   2.5995712 ]][0m
[37m[1m[2023-07-11 00:50:29,905][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:50:38,963][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 00:50:38,964][233954] FPS: 424017.94[0m
[36m[2023-07-11 00:50:38,966][233954] itr=61, itrs=2000, Progress: 3.05%[0m
[36m[2023-07-11 00:50:50,740][233954] train() took 11.75 seconds to complete[0m
[36m[2023-07-11 00:50:50,740][233954] FPS: 326929.36[0m
[36m[2023-07-11 00:50:54,951][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:50:54,957][233954] Reward + Measures: [[7421.14326691    0.19452134    0.21208599    0.19599435    0.013859
     1.81839716]][0m
[37m[1m[2023-07-11 00:50:54,957][233954] Max Reward on eval: 7421.1432669078895[0m
[37m[1m[2023-07-11 00:50:54,957][233954] Min Reward on eval: 7421.1432669078895[0m
[37m[1m[2023-07-11 00:50:54,958][233954] Mean Reward across all agents: 7421.1432669078895[0m
[37m[1m[2023-07-11 00:50:54,958][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:50:59,939][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:50:59,940][233954] Reward + Measures: [[-26.04235596   0.11669999   0.1754       0.0949       0.17230001
    2.73509383]
 [126.69276068   0.1487       0.20690003   0.07120001   0.1512
    2.68918657]
 [ 49.70023349   0.13780001   0.1812       0.0966       0.16870001
    3.09099078]
 ...
 [ 67.05479563   0.17320001   0.29480001   0.11610001   0.24600001
    3.11742449]
 [ 92.15501068   0.0972       0.1256       0.08750001   0.13859999
    2.58326602]
 [100.50848713   0.15809999   0.1619       0.0616       0.1628
    2.9788568 ]][0m
[37m[1m[2023-07-11 00:50:59,940][233954] Max Reward on eval: 708.0952939578332[0m
[37m[1m[2023-07-11 00:50:59,940][233954] Min Reward on eval: -94.83263082990888[0m
[37m[1m[2023-07-11 00:50:59,940][233954] Mean Reward across all agents: 130.94896779651944[0m
[37m[1m[2023-07-11 00:50:59,941][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:50:59,950][233954] mean_value=239.14417073368924, max_value=992.3191411427222[0m
[37m[1m[2023-07-11 00:50:59,958][233954] New mean coefficients: [[ 2.6207557  -3.8208776   5.967008    0.48054415 -1.2825006   2.7039511 ]][0m
[37m[1m[2023-07-11 00:50:59,959][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:51:08,957][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 00:51:08,957][233954] FPS: 426845.04[0m
[36m[2023-07-11 00:51:08,959][233954] itr=62, itrs=2000, Progress: 3.10%[0m
[36m[2023-07-11 00:51:20,568][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 00:51:20,569][233954] FPS: 331540.06[0m
[36m[2023-07-11 00:51:24,767][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:51:24,768][233954] Reward + Measures: [[7473.01256472    0.19426       0.21079133    0.19526233    0.012384
     1.81842256]][0m
[37m[1m[2023-07-11 00:51:24,768][233954] Max Reward on eval: 7473.012564722714[0m
[37m[1m[2023-07-11 00:51:24,768][233954] Min Reward on eval: 7473.012564722714[0m
[37m[1m[2023-07-11 00:51:24,768][233954] Mean Reward across all agents: 7473.012564722714[0m
[37m[1m[2023-07-11 00:51:24,769][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:51:29,775][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:51:29,776][233954] Reward + Measures: [[151.71837773   0.27340004   0.31750003   0.22839999   0.27320001
    2.43271995]
 [220.41562268   0.25959998   0.40040001   0.21280001   0.3881
    1.86324394]
 [320.69138305   0.28040001   0.40480003   0.17719999   0.31189999
    1.89863682]
 ...
 [  1.29790664   0.28490001   0.22479999   0.3046       0.1797
    2.97042847]
 [ 81.5703306    0.29809999   0.3249       0.29060003   0.3163
    2.49793792]
 [286.308341     0.35610002   0.31850001   0.35569999   0.31720001
    2.22219396]][0m
[37m[1m[2023-07-11 00:51:29,776][233954] Max Reward on eval: 677.9097747887834[0m
[37m[1m[2023-07-11 00:51:29,776][233954] Min Reward on eval: -34.28418301786296[0m
[37m[1m[2023-07-11 00:51:29,777][233954] Mean Reward across all agents: 145.11885872692662[0m
[37m[1m[2023-07-11 00:51:29,777][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:51:29,784][233954] mean_value=54.85245867552949, max_value=787.3593130320311[0m
[37m[1m[2023-07-11 00:51:29,787][233954] New mean coefficients: [[ 2.680806  -4.031998   6.244898   1.3916428 -1.1646008  3.2412872]][0m
[37m[1m[2023-07-11 00:51:29,788][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:51:38,854][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 00:51:38,854][233954] FPS: 423614.95[0m
[36m[2023-07-11 00:51:38,857][233954] itr=63, itrs=2000, Progress: 3.15%[0m
[36m[2023-07-11 00:51:50,432][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 00:51:50,432][233954] FPS: 332576.56[0m
[36m[2023-07-11 00:51:54,835][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:51:54,836][233954] Reward + Measures: [[7562.92920883    0.19108899    0.20654866    0.19251733    0.011622
     1.81911945]][0m
[37m[1m[2023-07-11 00:51:54,836][233954] Max Reward on eval: 7562.929208825128[0m
[37m[1m[2023-07-11 00:51:54,836][233954] Min Reward on eval: 7562.929208825128[0m
[37m[1m[2023-07-11 00:51:54,836][233954] Mean Reward across all agents: 7562.929208825128[0m
[37m[1m[2023-07-11 00:51:54,837][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:52:00,062][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:52:00,063][233954] Reward + Measures: [[393.81825256   0.20970002   0.19289999   0.19090001   0.18969999
    2.5235846 ]
 [215.68369771   0.11920001   0.1349       0.1384       0.132
    2.84311438]
 [339.16216614   0.18539999   0.18999998   0.1706       0.14049999
    2.20399642]
 ...
 [219.81224158   0.12850001   0.148        0.11189999   0.09010001
    2.60887766]
 [293.06161975   0.112        0.1777       0.2227       0.1551
    2.30355   ]
 [469.53353687   0.19509999   0.2244       0.19559999   0.17270002
    2.39093995]][0m
[37m[1m[2023-07-11 00:52:00,063][233954] Max Reward on eval: 808.3989276856184[0m
[37m[1m[2023-07-11 00:52:00,063][233954] Min Reward on eval: 87.32570243924856[0m
[37m[1m[2023-07-11 00:52:00,064][233954] Mean Reward across all agents: 329.27779429447315[0m
[37m[1m[2023-07-11 00:52:00,064][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:52:00,068][233954] mean_value=-28.584876848806733, max_value=709.4332165768701[0m
[37m[1m[2023-07-11 00:52:00,071][233954] New mean coefficients: [[ 2.872828  -4.631367   7.3100853  1.0949588 -0.6333497  3.4938636]][0m
[37m[1m[2023-07-11 00:52:00,072][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:52:09,168][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 00:52:09,168][233954] FPS: 422254.60[0m
[36m[2023-07-11 00:52:09,170][233954] itr=64, itrs=2000, Progress: 3.20%[0m
[36m[2023-07-11 00:52:20,848][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 00:52:20,849][233954] FPS: 329638.54[0m
[36m[2023-07-11 00:52:25,167][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:52:25,167][233954] Reward + Measures: [[7579.41574672    0.19110465    0.20776765    0.19318135    0.01111067
     1.83641708]][0m
[37m[1m[2023-07-11 00:52:25,167][233954] Max Reward on eval: 7579.415746719601[0m
[37m[1m[2023-07-11 00:52:25,168][233954] Min Reward on eval: 7579.415746719601[0m
[37m[1m[2023-07-11 00:52:25,168][233954] Mean Reward across all agents: 7579.415746719601[0m
[37m[1m[2023-07-11 00:52:25,168][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:52:30,200][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:52:30,201][233954] Reward + Measures: [[ 93.43590807   0.0916       0.4251       0.17650001   0.43090001
    2.08650637]
 [489.34061408   0.1919       0.2392       0.2095       0.18350001
    1.74735451]
 [801.42996401   0.1675       0.3132       0.18359999   0.24100001
    1.39871395]
 ...
 [565.34912873   0.21409999   0.2665       0.1858       0.19569999
    1.74113047]
 [771.72934054   0.15339999   0.30019999   0.18789999   0.21669999
    1.43478584]
 [607.02748396   0.19340001   0.25470001   0.2395       0.23650001
    1.48480988]][0m
[37m[1m[2023-07-11 00:52:30,201][233954] Max Reward on eval: 1809.523277292028[0m
[37m[1m[2023-07-11 00:52:30,201][233954] Min Reward on eval: -13.728447938431055[0m
[37m[1m[2023-07-11 00:52:30,201][233954] Mean Reward across all agents: 482.937391988637[0m
[37m[1m[2023-07-11 00:52:30,202][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:52:30,205][233954] mean_value=-966.2585807673455, max_value=815.684390874859[0m
[37m[1m[2023-07-11 00:52:30,207][233954] New mean coefficients: [[ 2.940729   -4.665241    7.2527056   2.1214843  -0.71018684  3.7664318 ]][0m
[37m[1m[2023-07-11 00:52:30,208][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:52:39,368][233954] train() took 9.16 seconds to complete[0m
[36m[2023-07-11 00:52:39,368][233954] FPS: 419315.66[0m
[36m[2023-07-11 00:52:39,370][233954] itr=65, itrs=2000, Progress: 3.25%[0m
[36m[2023-07-11 00:52:51,194][233954] train() took 11.80 seconds to complete[0m
[36m[2023-07-11 00:52:51,195][233954] FPS: 325554.78[0m
[36m[2023-07-11 00:52:55,544][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:52:55,545][233954] Reward + Measures: [[7616.76042797    0.19062667    0.20904131    0.19159733    0.010699
     1.84655535]][0m
[37m[1m[2023-07-11 00:52:55,545][233954] Max Reward on eval: 7616.760427973553[0m
[37m[1m[2023-07-11 00:52:55,545][233954] Min Reward on eval: 7616.760427973553[0m
[37m[1m[2023-07-11 00:52:55,545][233954] Mean Reward across all agents: 7616.760427973553[0m
[37m[1m[2023-07-11 00:52:55,546][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:53:00,575][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:53:00,576][233954] Reward + Measures: [[247.79636677   0.18410002   0.54630005   0.1529       0.48699999
    1.87888837]
 [288.02939212   0.14549999   0.1956       0.1718       0.1991
    1.9562546 ]
 [187.60464289   0.17220001   0.75710005   0.1955       0.65549994
    1.91888142]
 ...
 [303.37384627   0.23700002   0.30080003   0.28279999   0.29899999
    1.74977994]
 [447.55208873   0.2096       0.47139999   0.24190001   0.37729999
    1.56117809]
 [149.64198278   0.14390001   0.2493       0.1592       0.24510001
    2.42801738]][0m
[37m[1m[2023-07-11 00:53:00,576][233954] Max Reward on eval: 1173.2703037309227[0m
[37m[1m[2023-07-11 00:53:00,576][233954] Min Reward on eval: 51.45806397304987[0m
[37m[1m[2023-07-11 00:53:00,576][233954] Mean Reward across all agents: 369.7426831649002[0m
[37m[1m[2023-07-11 00:53:00,577][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:53:00,581][233954] mean_value=-481.4585768114678, max_value=1010.102924349904[0m
[37m[1m[2023-07-11 00:53:00,584][233954] New mean coefficients: [[ 2.5695274 -4.634522   6.4198823  1.2012696 -1.5417839  3.639903 ]][0m
[37m[1m[2023-07-11 00:53:00,585][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:53:09,591][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 00:53:09,591][233954] FPS: 426465.97[0m
[36m[2023-07-11 00:53:09,593][233954] itr=66, itrs=2000, Progress: 3.30%[0m
[36m[2023-07-11 00:53:21,138][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 00:53:21,139][233954] FPS: 333331.82[0m
[36m[2023-07-11 00:53:25,473][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:53:25,474][233954] Reward + Measures: [[7703.0756281     0.19195133    0.21276332    0.19244835    0.00951433
     1.87460852]][0m
[37m[1m[2023-07-11 00:53:25,474][233954] Max Reward on eval: 7703.0756281038[0m
[37m[1m[2023-07-11 00:53:25,474][233954] Min Reward on eval: 7703.0756281038[0m
[37m[1m[2023-07-11 00:53:25,475][233954] Mean Reward across all agents: 7703.0756281038[0m
[37m[1m[2023-07-11 00:53:25,475][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:53:30,507][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:53:30,513][233954] Reward + Measures: [[ 34.04703853   0.1141       0.15210001   0.13869999   0.1091
    2.83804083]
 [284.92142392   0.2138       0.27460003   0.21930002   0.22140001
    2.52612066]
 [ 88.70098438   0.1323       0.1638       0.15840001   0.1288
    2.70651627]
 ...
 [100.241642     0.10039999   0.148        0.0934       0.15620001
    2.56166816]
 [-36.35452052   0.21400002   0.3066       0.29350001   0.2606
    2.80758166]
 [ 48.49369855   0.1846       0.51340002   0.29949999   0.42519999
    2.58324242]][0m
[37m[1m[2023-07-11 00:53:30,513][233954] Max Reward on eval: 791.6670627780259[0m
[37m[1m[2023-07-11 00:53:30,514][233954] Min Reward on eval: -142.79180977176875[0m
[37m[1m[2023-07-11 00:53:30,514][233954] Mean Reward across all agents: 112.10144196188693[0m
[37m[1m[2023-07-11 00:53:30,514][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:53:30,519][233954] mean_value=-116.72226439857246, max_value=573.8125395431153[0m
[37m[1m[2023-07-11 00:53:30,522][233954] New mean coefficients: [[ 2.405252  -4.9443774  5.6654606  0.8853074 -2.4593349  3.5965588]][0m
[37m[1m[2023-07-11 00:53:30,523][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:53:39,589][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 00:53:39,589][233954] FPS: 423619.21[0m
[36m[2023-07-11 00:53:39,592][233954] itr=67, itrs=2000, Progress: 3.35%[0m
[36m[2023-07-11 00:53:51,497][233954] train() took 11.88 seconds to complete[0m
[36m[2023-07-11 00:53:51,503][233954] FPS: 323323.10[0m
[36m[2023-07-11 00:53:55,816][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:53:55,817][233954] Reward + Measures: [[7713.62029671    0.18836632    0.20978332    0.18985632    0.01037767
     1.87225199]][0m
[37m[1m[2023-07-11 00:53:55,817][233954] Max Reward on eval: 7713.620296708618[0m
[37m[1m[2023-07-11 00:53:55,817][233954] Min Reward on eval: 7713.620296708618[0m
[37m[1m[2023-07-11 00:53:55,817][233954] Mean Reward across all agents: 7713.620296708618[0m
[37m[1m[2023-07-11 00:53:55,818][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:54:00,966][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:54:00,967][233954] Reward + Measures: [[342.15303446   0.2014       0.36649999   0.20030002   0.29350001
    1.86283481]
 [165.18747832   0.19660001   0.1533       0.1858       0.17959999
    2.32509112]
 [172.62866972   0.29270002   0.37719998   0.28420001   0.34190002
    2.60995245]
 ...
 [177.44109175   0.35460001   0.33769998   0.34820002   0.35190001
    1.92170525]
 [210.9086149    0.23889999   0.36349997   0.2122       0.34780002
    2.10514235]
 [150.68330926   0.21640001   0.27489999   0.2045       0.29010001
    2.2448175 ]][0m
[37m[1m[2023-07-11 00:54:00,967][233954] Max Reward on eval: 743.1777267348953[0m
[37m[1m[2023-07-11 00:54:00,967][233954] Min Reward on eval: 23.31987719684839[0m
[37m[1m[2023-07-11 00:54:00,968][233954] Mean Reward across all agents: 223.43521928328278[0m
[37m[1m[2023-07-11 00:54:00,968][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:54:00,972][233954] mean_value=-402.474990798491, max_value=725.5933175416664[0m
[37m[1m[2023-07-11 00:54:00,974][233954] New mean coefficients: [[ 2.1888447 -4.6151066  6.219484   0.8202853 -2.9663     2.7403746]][0m
[37m[1m[2023-07-11 00:54:00,975][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:54:09,954][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 00:54:09,954][233954] FPS: 427768.39[0m
[36m[2023-07-11 00:54:09,956][233954] itr=68, itrs=2000, Progress: 3.40%[0m
[36m[2023-07-11 00:54:21,559][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 00:54:21,559][233954] FPS: 331700.08[0m
[36m[2023-07-11 00:54:25,858][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:54:25,859][233954] Reward + Measures: [[7760.76240563    0.19063835    0.211208      0.191899      0.01007
     1.88774025]][0m
[37m[1m[2023-07-11 00:54:25,859][233954] Max Reward on eval: 7760.762405629352[0m
[37m[1m[2023-07-11 00:54:25,859][233954] Min Reward on eval: 7760.762405629352[0m
[37m[1m[2023-07-11 00:54:25,859][233954] Mean Reward across all agents: 7760.762405629352[0m
[37m[1m[2023-07-11 00:54:25,859][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:54:30,927][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:54:30,928][233954] Reward + Measures: [[-353.99581403    0.72679996    0.75380003    0.76100004    0.045
     2.94563484]
 [  68.21298311    0.19160001    0.28800002    0.31309998    0.13599999
     2.72735953]
 [  84.12212176    0.2168        0.2349        0.23530002    0.1626
     2.90586591]
 ...
 [  12.69844579    0.4113        0.39390001    0.47690001    0.10070001
     2.89331102]
 [   5.90092526    0.30340004    0.40500003    0.33960003    0.32169998
     2.92408228]
 [-149.36380186    0.7919001     0.77880001    0.80360001    0.0446
     2.73749971]][0m
[37m[1m[2023-07-11 00:54:30,928][233954] Max Reward on eval: 597.1762275528629[0m
[37m[1m[2023-07-11 00:54:30,928][233954] Min Reward on eval: -432.0256998295896[0m
[37m[1m[2023-07-11 00:54:30,928][233954] Mean Reward across all agents: 31.78557684207899[0m
[37m[1m[2023-07-11 00:54:30,929][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:54:30,943][233954] mean_value=362.12026541422216, max_value=729.4691396579146[0m
[37m[1m[2023-07-11 00:54:30,946][233954] New mean coefficients: [[ 2.0832577 -4.426349   6.4458528  0.7693387 -2.4982805  3.0214214]][0m
[37m[1m[2023-07-11 00:54:30,947][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:54:39,936][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 00:54:39,937][233954] FPS: 427247.12[0m
[36m[2023-07-11 00:54:39,939][233954] itr=69, itrs=2000, Progress: 3.45%[0m
[36m[2023-07-11 00:54:51,653][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 00:54:51,653][233954] FPS: 328536.16[0m
[36m[2023-07-11 00:54:55,989][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:54:55,990][233954] Reward + Measures: [[7803.39823208    0.18827799    0.21102032    0.18981467    0.008501
     1.88600922]][0m
[37m[1m[2023-07-11 00:54:55,990][233954] Max Reward on eval: 7803.398232079623[0m
[37m[1m[2023-07-11 00:54:55,990][233954] Min Reward on eval: 7803.398232079623[0m
[37m[1m[2023-07-11 00:54:55,991][233954] Mean Reward across all agents: 7803.398232079623[0m
[37m[1m[2023-07-11 00:54:55,991][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:55:01,048][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:55:01,048][233954] Reward + Measures: [[244.69022952   0.31760001   0.39210001   0.38289997   0.31720001
    2.14235091]
 [137.26241517   0.22239999   0.30560002   0.1691       0.23810001
    2.86501956]
 [332.8524549    0.22350001   0.331        0.19810002   0.2332
    1.83153057]
 ...
 [140.01278424   0.26230001   0.33580002   0.15030001   0.27419999
    2.5449388 ]
 [ 33.16333147   0.3477       0.4014       0.3337       0.29949999
    2.68392539]
 [149.4707527    0.1983       0.30310002   0.13080001   0.2009
    2.91239357]][0m
[37m[1m[2023-07-11 00:55:01,049][233954] Max Reward on eval: 522.0001964092255[0m
[37m[1m[2023-07-11 00:55:01,049][233954] Min Reward on eval: -18.039962428808213[0m
[37m[1m[2023-07-11 00:55:01,049][233954] Mean Reward across all agents: 127.09022303964751[0m
[37m[1m[2023-07-11 00:55:01,049][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:55:01,056][233954] mean_value=-1.9256052953622445, max_value=704.643471802515[0m
[37m[1m[2023-07-11 00:55:01,059][233954] New mean coefficients: [[ 1.7558241 -4.2697086  6.2730527  1.4110323 -2.9066203  2.8658729]][0m
[37m[1m[2023-07-11 00:55:01,060][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:55:10,178][233954] train() took 9.12 seconds to complete[0m
[36m[2023-07-11 00:55:10,179][233954] FPS: 421180.10[0m
[36m[2023-07-11 00:55:10,181][233954] itr=70, itrs=2000, Progress: 3.50%[0m
[37m[1m[2023-07-11 00:57:45,610][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000050[0m
[36m[2023-07-11 00:57:58,228][233954] train() took 11.84 seconds to complete[0m
[36m[2023-07-11 00:57:58,228][233954] FPS: 324426.76[0m
[36m[2023-07-11 00:58:02,543][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:58:02,543][233954] Reward + Measures: [[7830.05197031    0.19041634    0.21725766    0.19184932    0.008105
     1.91204226]][0m
[37m[1m[2023-07-11 00:58:02,544][233954] Max Reward on eval: 7830.051970305403[0m
[37m[1m[2023-07-11 00:58:02,544][233954] Min Reward on eval: 7830.051970305403[0m
[37m[1m[2023-07-11 00:58:02,544][233954] Mean Reward across all agents: 7830.051970305403[0m
[37m[1m[2023-07-11 00:58:02,544][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:58:07,546][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:58:07,546][233954] Reward + Measures: [[70.2239225   0.24819998  0.22610001  0.20039999  0.2194      2.82127476]
 [82.16652441  0.2106      0.13340001  0.20420003  0.1446      2.81796288]
 [63.29542519  0.2793      0.19050001  0.2359      0.2098      2.4290514 ]
 ...
 [12.80680363  0.19590001  0.13710001  0.20020001  0.1627      2.88009429]
 [77.9191544   0.3021      0.20439999  0.23570001  0.24439998  2.36838984]
 [36.17260862  0.32960001  0.2773      0.2454      0.27069998  2.61243701]][0m
[37m[1m[2023-07-11 00:58:07,547][233954] Max Reward on eval: 788.5911216393113[0m
[37m[1m[2023-07-11 00:58:07,547][233954] Min Reward on eval: -59.07612313535064[0m
[37m[1m[2023-07-11 00:58:07,547][233954] Mean Reward across all agents: 49.39935369282603[0m
[37m[1m[2023-07-11 00:58:07,547][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:58:07,552][233954] mean_value=-68.69520236907228, max_value=704.5495386395603[0m
[37m[1m[2023-07-11 00:58:07,555][233954] New mean coefficients: [[ 1.9765329 -4.0957036  6.6494107  1.5225171 -2.9714437  1.862894 ]][0m
[37m[1m[2023-07-11 00:58:07,556][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:58:16,444][233954] train() took 8.89 seconds to complete[0m
[36m[2023-07-11 00:58:16,445][233954] FPS: 432088.27[0m
[36m[2023-07-11 00:58:16,447][233954] itr=71, itrs=2000, Progress: 3.55%[0m
[36m[2023-07-11 00:58:28,123][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 00:58:28,123][233954] FPS: 329739.82[0m
[36m[2023-07-11 00:58:32,351][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:58:32,351][233954] Reward + Measures: [[7804.30143372    0.18675266    0.21786234    0.18890165    0.01031933
     1.8870703 ]][0m
[37m[1m[2023-07-11 00:58:32,351][233954] Max Reward on eval: 7804.301433715266[0m
[37m[1m[2023-07-11 00:58:32,352][233954] Min Reward on eval: 7804.301433715266[0m
[37m[1m[2023-07-11 00:58:32,352][233954] Mean Reward across all agents: 7804.301433715266[0m
[37m[1m[2023-07-11 00:58:32,352][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:58:37,466][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:58:37,467][233954] Reward + Measures: [[  5.67366407   0.1147       0.13870001   0.125        0.12530001
    2.90270996]
 [ 54.94856094   0.21710001   0.25490001   0.22020002   0.2227
    3.03197455]
 [ 29.32327568   0.42280003   0.40649995   0.38550001   0.44250003
    2.81240845]
 ...
 [ 21.82681282   0.36020002   0.14830001   0.35319999   0.23959999
    2.71759701]
 [170.03582289   0.29950002   0.29180002   0.26550001   0.33109999
    1.98900223]
 [ 36.49533281   0.36450002   0.24319999   0.35530001   0.26929998
    2.86321068]][0m
[37m[1m[2023-07-11 00:58:37,467][233954] Max Reward on eval: 299.4367504021153[0m
[37m[1m[2023-07-11 00:58:37,467][233954] Min Reward on eval: -81.25290205478669[0m
[37m[1m[2023-07-11 00:58:37,467][233954] Mean Reward across all agents: 55.014271131117226[0m
[37m[1m[2023-07-11 00:58:37,467][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:58:37,475][233954] mean_value=49.92208451211179, max_value=636.5097012119368[0m
[37m[1m[2023-07-11 00:58:37,478][233954] New mean coefficients: [[ 2.1878326 -4.5082254  6.7686143  1.8763264 -2.594007   2.146098 ]][0m
[37m[1m[2023-07-11 00:58:37,479][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:58:46,469][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 00:58:46,470][233954] FPS: 427196.43[0m
[36m[2023-07-11 00:58:46,472][233954] itr=72, itrs=2000, Progress: 3.60%[0m
[36m[2023-07-11 00:58:58,144][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 00:58:58,144][233954] FPS: 329865.24[0m
[36m[2023-07-11 00:59:02,506][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:59:02,507][233954] Reward + Measures: [[7855.68714687    0.18598068    0.21492067    0.18796065    0.01066667
     1.88291013]][0m
[37m[1m[2023-07-11 00:59:02,507][233954] Max Reward on eval: 7855.687146870376[0m
[37m[1m[2023-07-11 00:59:02,507][233954] Min Reward on eval: 7855.687146870376[0m
[37m[1m[2023-07-11 00:59:02,508][233954] Mean Reward across all agents: 7855.687146870376[0m
[37m[1m[2023-07-11 00:59:02,508][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:59:07,572][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:59:07,572][233954] Reward + Measures: [[181.02583363   0.21760002   0.366        0.1196       0.34630004
    2.49664497]
 [ 77.27432189   0.15140001   0.4161       0.20469999   0.41
    2.66140366]
 [ 81.36030225   0.19900002   0.51590002   0.0915       0.5068
    2.31257772]
 ...
 [ 57.53092398   0.13169999   0.50800002   0.18340002   0.55409998
    2.48798633]
 [ 44.34744053   0.12         0.65240002   0.1858       0.65090007
    2.1481328 ]
 [ 66.58303333   0.0981       0.49060002   0.22859998   0.50170004
    2.52867365]][0m
[37m[1m[2023-07-11 00:59:07,573][233954] Max Reward on eval: 609.1095089959912[0m
[37m[1m[2023-07-11 00:59:07,573][233954] Min Reward on eval: -43.58934945990332[0m
[37m[1m[2023-07-11 00:59:07,573][233954] Mean Reward across all agents: 90.75981971892489[0m
[37m[1m[2023-07-11 00:59:07,573][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:59:07,583][233954] mean_value=189.42306671186856, max_value=790.6637769126334[0m
[37m[1m[2023-07-11 00:59:07,585][233954] New mean coefficients: [[ 2.0936515 -3.8736966  6.2753544  2.717708  -2.9662435  1.6278404]][0m
[37m[1m[2023-07-11 00:59:07,586][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:59:16,701][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 00:59:16,702][233954] FPS: 421351.33[0m
[36m[2023-07-11 00:59:16,704][233954] itr=73, itrs=2000, Progress: 3.65%[0m
[36m[2023-07-11 00:59:28,407][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 00:59:28,407][233954] FPS: 328916.97[0m
[36m[2023-07-11 00:59:32,831][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:59:32,832][233954] Reward + Measures: [[7890.9164179     0.188777      0.22355498    0.19050598    0.01059267
     1.88341415]][0m
[37m[1m[2023-07-11 00:59:32,832][233954] Max Reward on eval: 7890.916417897073[0m
[37m[1m[2023-07-11 00:59:32,832][233954] Min Reward on eval: 7890.916417897073[0m
[37m[1m[2023-07-11 00:59:32,832][233954] Mean Reward across all agents: 7890.916417897073[0m
[37m[1m[2023-07-11 00:59:32,833][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:59:37,850][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 00:59:37,850][233954] Reward + Measures: [[601.19002719   0.18599999   0.41670004   0.23210001   0.36230001
    1.82440686]
 [649.02271083   0.11900001   0.25130001   0.1441       0.15339999
    1.69752884]
 [627.486279     0.1184       0.2696       0.16870001   0.20149998
    1.66421223]
 ...
 [ 80.64572063   0.18290001   0.20150001   0.2031       0.1621
    2.83156776]
 [199.66306563   0.18710001   0.18249999   0.20570002   0.1247
    2.8339119 ]
 [193.52945993   0.1163       0.17690001   0.17290001   0.10820001
    2.750314  ]][0m
[37m[1m[2023-07-11 00:59:37,850][233954] Max Reward on eval: 1808.3079776600935[0m
[37m[1m[2023-07-11 00:59:37,851][233954] Min Reward on eval: 12.087485999893397[0m
[37m[1m[2023-07-11 00:59:37,851][233954] Mean Reward across all agents: 453.29194112955497[0m
[37m[1m[2023-07-11 00:59:37,851][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 00:59:37,855][233954] mean_value=-269.8071491374715, max_value=595.9564502164721[0m
[37m[1m[2023-07-11 00:59:37,858][233954] New mean coefficients: [[ 1.9920533 -4.1836057  6.215093   2.6872056 -2.495357   2.0070548]][0m
[37m[1m[2023-07-11 00:59:37,859][233954] Moving the mean solution point...[0m
[36m[2023-07-11 00:59:46,891][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 00:59:46,891][233954] FPS: 425246.32[0m
[36m[2023-07-11 00:59:46,893][233954] itr=74, itrs=2000, Progress: 3.70%[0m
[36m[2023-07-11 00:59:58,397][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 00:59:58,397][233954] FPS: 334583.95[0m
[36m[2023-07-11 01:00:02,687][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:00:02,687][233954] Reward + Measures: [[7897.792407      0.18633533    0.21824101    0.18737866    0.01076567
     1.87188232]][0m
[37m[1m[2023-07-11 01:00:02,688][233954] Max Reward on eval: 7897.792407004344[0m
[37m[1m[2023-07-11 01:00:02,688][233954] Min Reward on eval: 7897.792407004344[0m
[37m[1m[2023-07-11 01:00:02,688][233954] Mean Reward across all agents: 7897.792407004344[0m
[37m[1m[2023-07-11 01:00:02,688][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:00:07,724][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:00:07,725][233954] Reward + Measures: [[ -0.14211745   0.26570001   0.75290006   0.1674       0.73150003
    3.13201404]
 [ 92.18274833   0.37090001   0.42379999   0.30230004   0.50520003
    2.74791789]
 [174.65380658   0.25700003   0.3574       0.18370001   0.4492
    2.54148459]
 ...
 [  4.81327396   0.22420001   0.36620003   0.17220001   0.56650007
    2.77739215]
 [ 32.46341266   0.15650001   0.21339999   0.24299999   0.30450001
    2.68581748]
 [205.24604288   0.20970002   0.2888       0.24370001   0.33590001
    2.4707613 ]][0m
[37m[1m[2023-07-11 01:00:07,725][233954] Max Reward on eval: 424.1957988638431[0m
[37m[1m[2023-07-11 01:00:07,725][233954] Min Reward on eval: -220.60616164021195[0m
[37m[1m[2023-07-11 01:00:07,726][233954] Mean Reward across all agents: 89.20201556590925[0m
[37m[1m[2023-07-11 01:00:07,726][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:00:07,742][233954] mean_value=243.51422181578934, max_value=684.047738077119[0m
[37m[1m[2023-07-11 01:00:07,744][233954] New mean coefficients: [[ 1.6204267 -3.742472   5.2570605  3.4245586 -3.0476534  1.5339653]][0m
[37m[1m[2023-07-11 01:00:07,745][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:00:16,840][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 01:00:16,841][233954] FPS: 422289.17[0m
[36m[2023-07-11 01:00:16,843][233954] itr=75, itrs=2000, Progress: 3.75%[0m
[36m[2023-07-11 01:00:28,622][233954] train() took 11.75 seconds to complete[0m
[36m[2023-07-11 01:00:28,622][233954] FPS: 326853.95[0m
[36m[2023-07-11 01:00:32,852][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:00:32,853][233954] Reward + Measures: [[7994.0274541     0.18704866    0.21533968    0.188309      0.00874033
     1.90848398]][0m
[37m[1m[2023-07-11 01:00:32,853][233954] Max Reward on eval: 7994.027454097717[0m
[37m[1m[2023-07-11 01:00:32,853][233954] Min Reward on eval: 7994.027454097717[0m
[37m[1m[2023-07-11 01:00:32,853][233954] Mean Reward across all agents: 7994.027454097717[0m
[37m[1m[2023-07-11 01:00:32,854][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:00:37,966][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:00:37,967][233954] Reward + Measures: [[192.21633542   0.16880001   0.24070001   0.0923       0.1578
    3.33541727]
 [114.28505974   0.0939       0.14130001   0.11979999   0.08350001
    2.80592918]
 [263.55944403   0.1496       0.27330002   0.1806       0.1718
    2.60383511]
 ...
 [274.41626819   0.07910001   0.1894       0.11269999   0.09670001
    3.03426027]
 [198.97270036   0.1049       0.2036       0.122        0.0857
    3.00889587]
 [399.63106296   0.1084       0.1956       0.13750002   0.111
    3.12056422]][0m
[37m[1m[2023-07-11 01:00:37,967][233954] Max Reward on eval: 797.0251541354694[0m
[37m[1m[2023-07-11 01:00:37,967][233954] Min Reward on eval: -11.06117051858455[0m
[37m[1m[2023-07-11 01:00:37,968][233954] Mean Reward across all agents: 245.03435809494715[0m
[37m[1m[2023-07-11 01:00:37,968][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:00:37,978][233954] mean_value=175.94207137049628, max_value=1054.6872215541546[0m
[37m[1m[2023-07-11 01:00:37,981][233954] New mean coefficients: [[ 1.915071  -3.4101603  6.656213   2.4942744 -1.4909154  1.5848114]][0m
[37m[1m[2023-07-11 01:00:37,982][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:00:46,972][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 01:00:46,972][233954] FPS: 427212.12[0m
[36m[2023-07-11 01:00:46,975][233954] itr=76, itrs=2000, Progress: 3.80%[0m
[36m[2023-07-11 01:00:58,724][233954] train() took 11.72 seconds to complete[0m
[36m[2023-07-11 01:00:58,724][233954] FPS: 327670.12[0m
[36m[2023-07-11 01:01:02,983][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:01:02,984][233954] Reward + Measures: [[7988.27825699    0.18663166    0.21376801    0.18767633    0.00982633
     1.89835572]][0m
[37m[1m[2023-07-11 01:01:02,984][233954] Max Reward on eval: 7988.278256986738[0m
[37m[1m[2023-07-11 01:01:02,984][233954] Min Reward on eval: 7988.278256986738[0m
[37m[1m[2023-07-11 01:01:02,984][233954] Mean Reward across all agents: 7988.278256986738[0m
[37m[1m[2023-07-11 01:01:02,985][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:01:07,947][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:01:07,948][233954] Reward + Measures: [[ 80.98493081   0.23319998   0.44980001   0.30370003   0.40349999
    2.9772191 ]
 [127.54438807   0.45949998   0.36380002   0.2086       0.36809999
    2.81144333]
 [ 71.72078802   0.2397       0.43479997   0.21949999   0.38209999
    2.96351218]
 ...
 [ 35.36434757   0.1286       0.40110001   0.26019999   0.39040002
    2.92999601]
 [ 10.23048714   0.27660003   0.51610005   0.2631       0.44280005
    2.93696356]
 [105.63989335   0.199        0.47189999   0.252        0.46190006
    2.74274945]][0m
[37m[1m[2023-07-11 01:01:07,948][233954] Max Reward on eval: 422.20812319833783[0m
[37m[1m[2023-07-11 01:01:07,948][233954] Min Reward on eval: -400.2277331172489[0m
[37m[1m[2023-07-11 01:01:07,948][233954] Mean Reward across all agents: 61.778579289747675[0m
[37m[1m[2023-07-11 01:01:07,949][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:01:07,964][233954] mean_value=282.9662180015494, max_value=828.3261804813519[0m
[37m[1m[2023-07-11 01:01:07,967][233954] New mean coefficients: [[ 1.8731241  -2.6123147   7.665744    2.1744475  -0.77739143  1.7905834 ]][0m
[37m[1m[2023-07-11 01:01:07,968][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:01:16,961][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 01:01:16,961][233954] FPS: 427077.65[0m
[36m[2023-07-11 01:01:16,963][233954] itr=77, itrs=2000, Progress: 3.85%[0m
[36m[2023-07-11 01:01:28,681][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 01:01:28,681][233954] FPS: 328473.64[0m
[36m[2023-07-11 01:01:33,024][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:01:33,025][233954] Reward + Measures: [[7979.93281796    0.18797967    0.22322467    0.189035      0.00924067
     1.89845669]][0m
[37m[1m[2023-07-11 01:01:33,025][233954] Max Reward on eval: 7979.932817955593[0m
[37m[1m[2023-07-11 01:01:33,025][233954] Min Reward on eval: 7979.932817955593[0m
[37m[1m[2023-07-11 01:01:33,025][233954] Mean Reward across all agents: 7979.932817955593[0m
[37m[1m[2023-07-11 01:01:33,026][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:01:38,077][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:01:38,077][233954] Reward + Measures: [[ 76.69471378   0.1419       0.19770001   0.15019999   0.1841
    2.52137136]
 [-21.09120903   0.22490001   0.21960001   0.20640002   0.20580001
    3.19679427]
 [107.01282491   0.17709999   0.1612       0.16599999   0.15440001
    2.31254292]
 ...
 [ 66.43312152   0.2077       0.30450001   0.16440001   0.27110001
    2.42834735]
 [-63.51611229   0.32269999   0.48459998   0.2624       0.40669999
    3.14645123]
 [ 24.01659419   0.1038       0.13510001   0.1142       0.12390001
    2.75129271]][0m
[37m[1m[2023-07-11 01:01:38,077][233954] Max Reward on eval: 279.4999762713909[0m
[37m[1m[2023-07-11 01:01:38,078][233954] Min Reward on eval: -131.16144696194678[0m
[37m[1m[2023-07-11 01:01:38,078][233954] Mean Reward across all agents: 45.71304455915136[0m
[37m[1m[2023-07-11 01:01:38,078][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:01:38,081][233954] mean_value=-188.868086199981, max_value=585.0945153608743[0m
[37m[1m[2023-07-11 01:01:38,083][233954] New mean coefficients: [[ 1.6547202  -2.5759892   7.746379    2.0106664  -0.33629993  1.7061471 ]][0m
[37m[1m[2023-07-11 01:01:38,084][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:01:47,106][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 01:01:47,106][233954] FPS: 425706.52[0m
[36m[2023-07-11 01:01:47,109][233954] itr=78, itrs=2000, Progress: 3.90%[0m
[36m[2023-07-11 01:01:58,805][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 01:01:58,810][233954] FPS: 329111.00[0m
[36m[2023-07-11 01:02:03,103][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:02:03,104][233954] Reward + Measures: [[8007.585761      0.18980899    0.23368901    0.19091865    0.00858133
     1.90793848]][0m
[37m[1m[2023-07-11 01:02:03,104][233954] Max Reward on eval: 8007.585761002255[0m
[37m[1m[2023-07-11 01:02:03,104][233954] Min Reward on eval: 8007.585761002255[0m
[37m[1m[2023-07-11 01:02:03,105][233954] Mean Reward across all agents: 8007.585761002255[0m
[37m[1m[2023-07-11 01:02:03,105][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:02:08,134][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:02:08,135][233954] Reward + Measures: [[134.25635064   0.18499999   0.18529999   0.20320001   0.19599999
    2.67672729]
 [ 72.96027557   0.0846       0.48860002   0.39229998   0.53560001
    2.37251258]
 [116.80517756   0.0477       0.4526       0.41549999   0.49710003
    2.39169502]
 ...
 [184.67725179   0.1592       0.1813       0.14359999   0.1513
    2.3232553 ]
 [ 39.99265459   0.1664       0.39510003   0.43659997   0.43139997
    2.53707266]
 [ 74.71657405   0.24340001   0.27680001   0.28979999   0.27219999
    2.67037845]][0m
[37m[1m[2023-07-11 01:02:08,135][233954] Max Reward on eval: 1427.4960174935318[0m
[37m[1m[2023-07-11 01:02:08,135][233954] Min Reward on eval: -76.30738367550074[0m
[37m[1m[2023-07-11 01:02:08,136][233954] Mean Reward across all agents: 245.06434876339458[0m
[37m[1m[2023-07-11 01:02:08,136][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:02:08,146][233954] mean_value=17.921129517898905, max_value=826.7965354774147[0m
[37m[1m[2023-07-11 01:02:08,148][233954] New mean coefficients: [[ 1.6588508 -2.187366   7.130927   3.6330738 -0.893209   1.8724009]][0m
[37m[1m[2023-07-11 01:02:08,149][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:02:17,165][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 01:02:17,166][233954] FPS: 425984.83[0m
[36m[2023-07-11 01:02:17,168][233954] itr=79, itrs=2000, Progress: 3.95%[0m
[36m[2023-07-11 01:02:28,810][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 01:02:28,811][233954] FPS: 330602.66[0m
[36m[2023-07-11 01:02:33,101][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:02:33,102][233954] Reward + Measures: [[8035.27028619    0.18917799    0.23434401    0.188287      0.00814133
     1.91532993]][0m
[37m[1m[2023-07-11 01:02:33,102][233954] Max Reward on eval: 8035.270286187802[0m
[37m[1m[2023-07-11 01:02:33,102][233954] Min Reward on eval: 8035.270286187802[0m
[37m[1m[2023-07-11 01:02:33,102][233954] Mean Reward across all agents: 8035.270286187802[0m
[37m[1m[2023-07-11 01:02:33,102][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:02:38,111][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:02:38,112][233954] Reward + Measures: [[ 140.86340983    0.39860001    0.41490003    0.1128        0.44220001
     3.00192237]
 [  77.18797448    0.4815        0.36129999    0.45160004    0.51069999
     2.53059363]
 [-713.7187576     0.92389995    0.95629996    0.0048        0.98860008
     3.32463622]
 ...
 [ 200.14997321    0.38060004    0.30579996    0.33419999    0.37959999
     2.43313718]
 [  -7.1123071     0.34279999    0.34839997    0.1727        0.46639997
     2.60963416]
 [ 128.92024001    0.1665        0.2045        0.20320001    0.192
     2.46266985]][0m
[37m[1m[2023-07-11 01:02:38,112][233954] Max Reward on eval: 272.3277864548378[0m
[37m[1m[2023-07-11 01:02:38,112][233954] Min Reward on eval: -758.3456382996403[0m
[37m[1m[2023-07-11 01:02:38,112][233954] Mean Reward across all agents: -51.80591380525806[0m
[37m[1m[2023-07-11 01:02:38,113][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:02:38,123][233954] mean_value=-40.664423283150256, max_value=635.9000110291411[0m
[37m[1m[2023-07-11 01:02:38,126][233954] New mean coefficients: [[ 1.1534821 -1.9581683  5.736102   4.4243255 -2.1902592  1.6827275]][0m
[37m[1m[2023-07-11 01:02:38,127][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:02:47,136][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 01:02:47,136][233954] FPS: 426314.46[0m
[36m[2023-07-11 01:02:47,139][233954] itr=80, itrs=2000, Progress: 4.00%[0m
[37m[1m[2023-07-11 01:05:29,190][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000060[0m
[36m[2023-07-11 01:05:41,519][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 01:05:41,519][233954] FPS: 327858.36[0m
[36m[2023-07-11 01:05:45,645][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:05:45,645][233954] Reward + Measures: [[8067.6308398     0.19200334    0.23882633    0.19094235    0.00725667
     1.93559539]][0m
[37m[1m[2023-07-11 01:05:45,645][233954] Max Reward on eval: 8067.630839796529[0m
[37m[1m[2023-07-11 01:05:45,646][233954] Min Reward on eval: 8067.630839796529[0m
[37m[1m[2023-07-11 01:05:45,646][233954] Mean Reward across all agents: 8067.630839796529[0m
[37m[1m[2023-07-11 01:05:45,646][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:05:50,615][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:05:50,616][233954] Reward + Measures: [[ 21.41011833   0.1849       0.43670002   0.1954       0.44449997
    2.89276958]
 [  5.68022694   0.1367       0.2447       0.12970001   0.23330002
    3.03878212]
 [159.02404124   0.14890002   0.51069999   0.30140001   0.54680002
    2.52357578]
 ...
 [-14.4640526    0.16790001   0.2561       0.14000002   0.2388
    2.90379405]
 [ 51.89862875   0.199        0.31009999   0.1778       0.27610001
    2.43134451]
 [ 41.27654359   0.05070001   0.83899993   0.60620004   0.9084
    2.71435332]][0m
[37m[1m[2023-07-11 01:05:50,616][233954] Max Reward on eval: 472.9561214128509[0m
[37m[1m[2023-07-11 01:05:50,616][233954] Min Reward on eval: -252.3327076677233[0m
[37m[1m[2023-07-11 01:05:50,616][233954] Mean Reward across all agents: 100.56109741395325[0m
[37m[1m[2023-07-11 01:05:50,617][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:05:50,629][233954] mean_value=215.67220284249996, max_value=964.0578737270087[0m
[37m[1m[2023-07-11 01:05:50,631][233954] New mean coefficients: [[ 0.90168625 -2.3584538   5.7277746   3.6240144  -3.2916613   1.0590148 ]][0m
[37m[1m[2023-07-11 01:05:50,632][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:05:59,558][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 01:05:59,559][233954] FPS: 430285.55[0m
[36m[2023-07-11 01:05:59,561][233954] itr=81, itrs=2000, Progress: 4.05%[0m
[36m[2023-07-11 01:06:11,386][233954] train() took 11.80 seconds to complete[0m
[36m[2023-07-11 01:06:11,387][233954] FPS: 325485.01[0m
[36m[2023-07-11 01:06:15,617][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:06:15,617][233954] Reward + Measures: [[8009.82742371    0.19274467    0.24809399    0.19182833    0.007805
     1.93675911]][0m
[37m[1m[2023-07-11 01:06:15,618][233954] Max Reward on eval: 8009.827423713102[0m
[37m[1m[2023-07-11 01:06:15,618][233954] Min Reward on eval: 8009.827423713102[0m
[37m[1m[2023-07-11 01:06:15,618][233954] Mean Reward across all agents: 8009.827423713102[0m
[37m[1m[2023-07-11 01:06:15,618][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:06:20,609][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:06:20,609][233954] Reward + Measures: [[   0.04755118    0.2559        0.38569999    0.2581        0.37920004
     3.19714189]
 [-120.02168818    0.0787        0.51520002    0.35820001    0.4901
     3.12782454]
 [ -86.06961046    0.1026        0.43920001    0.27900001    0.38710001
     3.26247144]
 ...
 [ -21.27626718    0.1638        0.26139998    0.0991        0.19580001
     2.77290082]
 [  26.8069397     0.24770001    0.27850002    0.2383        0.25209999
     2.50849319]
 [  51.66891505    0.3238        0.38839999    0.26120004    0.36880001
     3.16696835]][0m
[37m[1m[2023-07-11 01:06:20,609][233954] Max Reward on eval: 280.7060557305813[0m
[37m[1m[2023-07-11 01:06:20,610][233954] Min Reward on eval: -205.01521208398043[0m
[37m[1m[2023-07-11 01:06:20,610][233954] Mean Reward across all agents: 8.47414759668117[0m
[37m[1m[2023-07-11 01:06:20,610][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:06:20,617][233954] mean_value=37.430613846315474, max_value=657.4116073392541[0m
[37m[1m[2023-07-11 01:06:20,625][233954] New mean coefficients: [[ 0.78111255 -2.7921648   5.838747    3.799536   -3.8414853   1.2804376 ]][0m
[37m[1m[2023-07-11 01:06:20,626][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:06:29,554][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 01:06:29,554][233954] FPS: 430195.67[0m
[36m[2023-07-11 01:06:29,557][233954] itr=82, itrs=2000, Progress: 4.10%[0m
[36m[2023-07-11 01:06:41,274][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 01:06:41,274][233954] FPS: 328539.33[0m
[36m[2023-07-11 01:06:45,584][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:06:45,585][233954] Reward + Measures: [[8019.93953747    0.19532666    0.25496766    0.19514298    0.00807733
     1.94083154]][0m
[37m[1m[2023-07-11 01:06:45,585][233954] Max Reward on eval: 8019.939537466527[0m
[37m[1m[2023-07-11 01:06:45,585][233954] Min Reward on eval: 8019.939537466527[0m
[37m[1m[2023-07-11 01:06:45,585][233954] Mean Reward across all agents: 8019.939537466527[0m
[37m[1m[2023-07-11 01:06:45,586][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:06:50,766][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:06:50,772][233954] Reward + Measures: [[-29.72303526   0.1258       0.3353       0.2031       0.38260001
    2.9273994 ]
 [-21.13166352   0.24730001   0.55989999   0.19749999   0.62250006
    2.7451427 ]
 [ 71.82428738   0.119        0.14740001   0.1074       0.13609998
    3.28527308]
 ...
 [225.64221097   0.39739999   0.15000001   0.4542       0.32099998
    2.35331964]
 [ 63.94847294   0.0918       0.39560002   0.26530001   0.3883
    2.8830564 ]
 [ 17.07083309   0.1062       0.3479       0.22570001   0.33579999
    2.9912169 ]][0m
[37m[1m[2023-07-11 01:06:50,772][233954] Max Reward on eval: 365.0214013591118[0m
[37m[1m[2023-07-11 01:06:50,772][233954] Min Reward on eval: -185.41283442302375[0m
[37m[1m[2023-07-11 01:06:50,773][233954] Mean Reward across all agents: 75.95911014953109[0m
[37m[1m[2023-07-11 01:06:50,773][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:06:50,784][233954] mean_value=116.9694313629555, max_value=645.3876120899245[0m
[37m[1m[2023-07-11 01:06:50,787][233954] New mean coefficients: [[ 0.8460792 -3.1896641  5.740346   4.0204754 -3.4244099  1.9013635]][0m
[37m[1m[2023-07-11 01:06:50,788][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:06:59,779][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 01:06:59,779][233954] FPS: 427198.64[0m
[36m[2023-07-11 01:06:59,781][233954] itr=83, itrs=2000, Progress: 4.15%[0m
[36m[2023-07-11 01:07:11,487][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 01:07:11,488][233954] FPS: 328860.44[0m
[36m[2023-07-11 01:07:15,732][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:07:15,732][233954] Reward + Measures: [[8019.13184704    0.195308      0.25728732    0.195842      0.00825833
     1.953035  ]][0m
[37m[1m[2023-07-11 01:07:15,733][233954] Max Reward on eval: 8019.131847042161[0m
[37m[1m[2023-07-11 01:07:15,733][233954] Min Reward on eval: 8019.131847042161[0m
[37m[1m[2023-07-11 01:07:15,733][233954] Mean Reward across all agents: 8019.131847042161[0m
[37m[1m[2023-07-11 01:07:15,734][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:07:20,748][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:07:20,754][233954] Reward + Measures: [[  -0.24361466    0.21140002    0.2335        0.1999        0.2148
     2.85654616]
 [ 355.99949762    0.33160001    0.26630002    0.34549999    0.26970002
     2.2348485 ]
 [ 178.95788576    0.17040001    0.23740001    0.18860002    0.22160001
     2.56441498]
 ...
 [-105.3515476     0.73050004    0.31479999    0.67260003    0.1902
     2.81491494]
 [  67.54018521    0.1036        0.1683        0.11370001    0.15530001
     2.73713088]
 [  27.88025201    0.12          0.23840001    0.11700001    0.2174
     2.79221034]][0m
[37m[1m[2023-07-11 01:07:20,755][233954] Max Reward on eval: 355.9994976167567[0m
[37m[1m[2023-07-11 01:07:20,756][233954] Min Reward on eval: -398.92117782616987[0m
[37m[1m[2023-07-11 01:07:20,757][233954] Mean Reward across all agents: 41.256451307850156[0m
[37m[1m[2023-07-11 01:07:20,757][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:07:20,773][233954] mean_value=-85.62470523812352, max_value=689.3523469478762[0m
[37m[1m[2023-07-11 01:07:20,777][233954] New mean coefficients: [[ 0.65183663 -2.5741918   4.3463316   5.389636   -4.485183    1.8113321 ]][0m
[37m[1m[2023-07-11 01:07:20,778][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:07:29,774][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 01:07:29,774][233954] FPS: 426949.42[0m
[36m[2023-07-11 01:07:29,776][233954] itr=84, itrs=2000, Progress: 4.20%[0m
[36m[2023-07-11 01:07:41,364][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 01:07:41,364][233954] FPS: 332327.86[0m
[36m[2023-07-11 01:07:45,740][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:07:45,740][233954] Reward + Measures: [[8077.37442639    0.19301867    0.25717565    0.193243      0.00718233
     1.96958268]][0m
[37m[1m[2023-07-11 01:07:45,740][233954] Max Reward on eval: 8077.374426393072[0m
[37m[1m[2023-07-11 01:07:45,740][233954] Min Reward on eval: 8077.374426393072[0m
[37m[1m[2023-07-11 01:07:45,741][233954] Mean Reward across all agents: 8077.374426393072[0m
[37m[1m[2023-07-11 01:07:45,741][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:07:50,758][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:07:50,764][233954] Reward + Measures: [[189.07709794   0.249        0.31040001   0.26069999   0.27680001
    2.27895904]
 [-59.14156245   0.23109999   0.259        0.20710002   0.1786
    3.32934952]
 [280.92617175   0.1267       0.22550002   0.18010001   0.148
    2.68213129]
 ...
 [198.47636157   0.1354       0.185        0.16940001   0.1207
    2.87603545]
 [298.54512782   0.17830001   0.2242       0.21780001   0.14230001
    2.68345118]
 [108.45037333   0.1066       0.12         0.12910001   0.09550001
    2.91566086]][0m
[37m[1m[2023-07-11 01:07:50,764][233954] Max Reward on eval: 1413.782442114642[0m
[37m[1m[2023-07-11 01:07:50,765][233954] Min Reward on eval: -141.97993725477718[0m
[37m[1m[2023-07-11 01:07:50,765][233954] Mean Reward across all agents: 182.53939628024256[0m
[37m[1m[2023-07-11 01:07:50,765][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:07:50,769][233954] mean_value=-85.66496859722392, max_value=706.2975004070742[0m
[37m[1m[2023-07-11 01:07:50,771][233954] New mean coefficients: [[ 0.5345375 -2.6889174  3.6287017  6.286618  -5.7733145  1.832572 ]][0m
[37m[1m[2023-07-11 01:07:50,772][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:07:59,729][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 01:07:59,730][233954] FPS: 428785.31[0m
[36m[2023-07-11 01:07:59,732][233954] itr=85, itrs=2000, Progress: 4.25%[0m
[36m[2023-07-11 01:08:11,276][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 01:08:11,277][233954] FPS: 333562.33[0m
[36m[2023-07-11 01:08:15,507][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:08:15,507][233954] Reward + Measures: [[8081.06096565    0.19362199    0.26307034    0.19528833    0.008016
     1.98312199]][0m
[37m[1m[2023-07-11 01:08:15,508][233954] Max Reward on eval: 8081.06096565334[0m
[37m[1m[2023-07-11 01:08:15,508][233954] Min Reward on eval: 8081.06096565334[0m
[37m[1m[2023-07-11 01:08:15,508][233954] Mean Reward across all agents: 8081.06096565334[0m
[37m[1m[2023-07-11 01:08:15,508][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:08:20,570][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:08:20,571][233954] Reward + Measures: [[ 7.285766    0.14320001  0.1245      0.12900001  0.1122      2.88182354]
 [ 8.01277927  0.18339999  0.1829      0.13450001  0.14060001  2.63507485]
 [ 4.05523042  0.08850001  0.0931      0.0789      0.09030001  3.02004814]
 ...
 [43.12732082  0.16299999  0.42200002  0.14719999  0.32280001  3.03309035]
 [69.3849138   0.1389      0.33990002  0.12709999  0.2807      3.16937613]
 [-3.03052838  0.0883      0.0768      0.06590001  0.11740001  2.95953012]][0m
[37m[1m[2023-07-11 01:08:20,571][233954] Max Reward on eval: 211.14066503308715[0m
[37m[1m[2023-07-11 01:08:20,572][233954] Min Reward on eval: -68.31286324486136[0m
[37m[1m[2023-07-11 01:08:20,572][233954] Mean Reward across all agents: 49.08399464066358[0m
[37m[1m[2023-07-11 01:08:20,572][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:08:20,574][233954] mean_value=-197.45522504624546, max_value=672.6922232302837[0m
[37m[1m[2023-07-11 01:08:20,577][233954] New mean coefficients: [[ 0.52625376 -3.4443593   3.5211806   5.1612043  -6.2117925   1.8956175 ]][0m
[37m[1m[2023-07-11 01:08:20,578][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:08:29,694][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 01:08:29,694][233954] FPS: 421320.41[0m
[36m[2023-07-11 01:08:29,696][233954] itr=86, itrs=2000, Progress: 4.30%[0m
[36m[2023-07-11 01:08:41,410][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 01:08:41,410][233954] FPS: 328645.64[0m
[36m[2023-07-11 01:08:45,784][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:08:45,785][233954] Reward + Measures: [[8016.9584888     0.19312866    0.26710469    0.19671266    0.00857167
     1.98939645]][0m
[37m[1m[2023-07-11 01:08:45,785][233954] Max Reward on eval: 8016.958488797001[0m
[37m[1m[2023-07-11 01:08:45,785][233954] Min Reward on eval: 8016.958488797001[0m
[37m[1m[2023-07-11 01:08:45,785][233954] Mean Reward across all agents: 8016.958488797001[0m
[37m[1m[2023-07-11 01:08:45,786][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:08:50,836][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:08:50,841][233954] Reward + Measures: [[ -1.15558589   0.18830001   0.22330001   0.1937       0.1935
    2.98564696]
 [  5.35008988   0.30419999   0.3619       0.32220003   0.35930002
    3.04643607]
 [-16.91736405   0.2089       0.43020001   0.2854       0.40580001
    2.59845328]
 ...
 [-12.18179178   0.0685       0.0852       0.11740001   0.0653
    3.26277733]
 [ 38.63456845   0.16040002   0.22570002   0.15449999   0.18529999
    2.96509457]
 [ 50.79459558   0.2789       0.32620001   0.2764       0.27480003
    3.24822617]][0m
[37m[1m[2023-07-11 01:08:50,842][233954] Max Reward on eval: 383.85727024851366[0m
[37m[1m[2023-07-11 01:08:50,842][233954] Min Reward on eval: -173.17676721792668[0m
[37m[1m[2023-07-11 01:08:50,842][233954] Mean Reward across all agents: 20.552475271978782[0m
[37m[1m[2023-07-11 01:08:50,843][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:08:50,855][233954] mean_value=92.94237100525164, max_value=771.788055485487[0m
[37m[1m[2023-07-11 01:08:50,858][233954] New mean coefficients: [[ 0.6915092 -2.6059618  4.174838   5.3078523 -5.180585   1.474117 ]][0m
[37m[1m[2023-07-11 01:08:50,859][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:08:59,995][233954] train() took 9.13 seconds to complete[0m
[36m[2023-07-11 01:08:59,996][233954] FPS: 420375.17[0m
[36m[2023-07-11 01:08:59,998][233954] itr=87, itrs=2000, Progress: 4.35%[0m
[36m[2023-07-11 01:09:11,562][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 01:09:11,562][233954] FPS: 333012.12[0m
[36m[2023-07-11 01:09:15,791][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:09:15,791][233954] Reward + Measures: [[7988.74521009    0.192441      0.26060432    0.19503835    0.00828767
     1.99732196]][0m
[37m[1m[2023-07-11 01:09:15,791][233954] Max Reward on eval: 7988.745210090223[0m
[37m[1m[2023-07-11 01:09:15,791][233954] Min Reward on eval: 7988.745210090223[0m
[37m[1m[2023-07-11 01:09:15,792][233954] Mean Reward across all agents: 7988.745210090223[0m
[37m[1m[2023-07-11 01:09:15,792][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:09:20,890][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:09:20,896][233954] Reward + Measures: [[190.74028828   0.30899999   0.24170001   0.37059999   0.2017
    2.95477057]
 [171.97257663   0.4808       0.27620003   0.52710003   0.26160002
    2.68735671]
 [-67.58534496   0.62290001   0.24850002   0.73120004   0.3475
    2.87829089]
 ...
 [ 10.63240143   0.51520002   0.26820001   0.53299999   0.17640002
    3.02717614]
 [-43.83392404   0.52180004   0.25409999   0.60490006   0.32880002
    2.79532409]
 [154.91478542   0.4039       0.32520002   0.49590001   0.22909999
    3.12383795]][0m
[37m[1m[2023-07-11 01:09:20,896][233954] Max Reward on eval: 577.373587584123[0m
[37m[1m[2023-07-11 01:09:20,897][233954] Min Reward on eval: -153.05060440204107[0m
[37m[1m[2023-07-11 01:09:20,897][233954] Mean Reward across all agents: 107.99043296814081[0m
[37m[1m[2023-07-11 01:09:20,897][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:09:20,914][233954] mean_value=257.63301550769927, max_value=828.5023154908419[0m
[37m[1m[2023-07-11 01:09:20,917][233954] New mean coefficients: [[ 0.98769134 -3.011816    5.145172    5.2019315  -3.9652705   2.5762744 ]][0m
[37m[1m[2023-07-11 01:09:20,918][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:09:29,885][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 01:09:29,885][233954] FPS: 428302.18[0m
[36m[2023-07-11 01:09:29,887][233954] itr=88, itrs=2000, Progress: 4.40%[0m
[36m[2023-07-11 01:09:41,421][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 01:09:41,421][233954] FPS: 333786.24[0m
[36m[2023-07-11 01:09:45,707][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:09:45,708][233954] Reward + Measures: [[7983.21015217    0.19391398    0.26518765    0.19631267    0.00850367
     1.99884045]][0m
[37m[1m[2023-07-11 01:09:45,708][233954] Max Reward on eval: 7983.210152166854[0m
[37m[1m[2023-07-11 01:09:45,708][233954] Min Reward on eval: 7983.210152166854[0m
[37m[1m[2023-07-11 01:09:45,709][233954] Mean Reward across all agents: 7983.210152166854[0m
[37m[1m[2023-07-11 01:09:45,709][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:09:50,719][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:09:50,725][233954] Reward + Measures: [[ -20.70869666    0.0038        0.8804        0.85350001    0.90490001
     3.5466888 ]
 [-184.53299476    0.0157        0.74920005    0.72719997    0.78279996
     3.12865758]
 [ 164.39677611    0.0364        0.66949999    0.61430001    0.65700001
     3.33476424]
 ...
 [ 157.54898518    0.12609999    0.17330001    0.09010001    0.1096
     3.29050183]
 [ -43.53050125    0.0478        0.38050002    0.33700004    0.40440002
     2.77229428]
 [  90.74662395    0.0204        0.79260004    0.73990005    0.8064
     3.42184901]][0m
[37m[1m[2023-07-11 01:09:50,725][233954] Max Reward on eval: 436.5929575189948[0m
[37m[1m[2023-07-11 01:09:50,725][233954] Min Reward on eval: -318.6005153543316[0m
[37m[1m[2023-07-11 01:09:50,726][233954] Mean Reward across all agents: 88.93435618121516[0m
[37m[1m[2023-07-11 01:09:50,726][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:09:50,735][233954] mean_value=163.42921070305007, max_value=784.8381371257826[0m
[37m[1m[2023-07-11 01:09:50,738][233954] New mean coefficients: [[ 1.1108959 -3.0782876  5.617001   6.319165  -4.312484   2.8506346]][0m
[37m[1m[2023-07-11 01:09:50,739][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:09:59,773][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 01:09:59,773][233954] FPS: 425105.81[0m
[36m[2023-07-11 01:09:59,776][233954] itr=89, itrs=2000, Progress: 4.45%[0m
[36m[2023-07-11 01:10:11,738][233954] train() took 11.93 seconds to complete[0m
[36m[2023-07-11 01:10:11,739][233954] FPS: 321795.68[0m
[36m[2023-07-11 01:10:16,073][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:10:16,073][233954] Reward + Measures: [[7942.59711258    0.194776      0.26996633    0.19931766    0.00954167
     1.99369204]][0m
[37m[1m[2023-07-11 01:10:16,073][233954] Max Reward on eval: 7942.597112579175[0m
[37m[1m[2023-07-11 01:10:16,074][233954] Min Reward on eval: 7942.597112579175[0m
[37m[1m[2023-07-11 01:10:16,074][233954] Mean Reward across all agents: 7942.597112579175[0m
[37m[1m[2023-07-11 01:10:16,074][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:10:21,077][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:10:21,128][233954] Reward + Measures: [[ 13.19147246   0.0964       0.1005       0.1296       0.1066
    2.73834801]
 [322.03670971   0.50949997   0.66370004   0.12370002   0.6358
    3.15471911]
 [ 47.99055334   0.147        0.29800001   0.18959999   0.2588
    2.46699142]
 ...
 [205.43777846   0.44840002   0.70230001   0.16070001   0.63430005
    2.81624198]
 [251.11041686   0.41599998   0.59619999   0.1243       0.56720001
    3.06064606]
 [ 98.06982614   0.34380004   0.4804       0.1347       0.45520002
    2.43665361]][0m
[37m[1m[2023-07-11 01:10:21,129][233954] Max Reward on eval: 353.2624397397041[0m
[37m[1m[2023-07-11 01:10:21,129][233954] Min Reward on eval: -89.74947570846416[0m
[37m[1m[2023-07-11 01:10:21,129][233954] Mean Reward across all agents: 67.8020342259101[0m
[37m[1m[2023-07-11 01:10:21,129][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:10:21,134][233954] mean_value=-120.41122065638042, max_value=836.2214931998402[0m
[37m[1m[2023-07-11 01:10:21,137][233954] New mean coefficients: [[ 1.0542424 -4.3171864  5.9242353  4.602997  -4.401171   3.1449046]][0m
[37m[1m[2023-07-11 01:10:21,138][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:10:30,224][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 01:10:30,224][233954] FPS: 422705.96[0m
[36m[2023-07-11 01:10:30,226][233954] itr=90, itrs=2000, Progress: 4.50%[0m
[37m[1m[2023-07-11 01:12:49,032][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000070[0m
[36m[2023-07-11 01:13:01,195][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 01:13:01,195][233954] FPS: 332800.29[0m
[36m[2023-07-11 01:13:05,448][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:13:05,465][233954] Reward + Measures: [[7986.93630884    0.19369401    0.26742834    0.20000365    0.00951767
     2.01051331]][0m
[37m[1m[2023-07-11 01:13:05,466][233954] Max Reward on eval: 7986.936308844052[0m
[37m[1m[2023-07-11 01:13:05,466][233954] Min Reward on eval: 7986.936308844052[0m
[37m[1m[2023-07-11 01:13:05,466][233954] Mean Reward across all agents: 7986.936308844052[0m
[37m[1m[2023-07-11 01:13:05,466][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:13:10,504][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:13:10,505][233954] Reward + Measures: [[ 90.90905086   0.2041       0.25600001   0.1718       0.2235
    2.79812741]
 [179.11781216   0.36359999   0.44569999   0.2764       0.44280002
    2.32802367]
 [ 96.94519643   0.28240001   0.34810001   0.27850002   0.34150001
    3.05135775]
 ...
 [210.76545095   0.15090001   0.24880002   0.17780001   0.1981
    2.7108078 ]
 [181.55289839   0.58700001   0.72119999   0.42129999   0.71939999
    3.17942619]
 [ 86.13352073   0.0886       0.1312       0.07359999   0.08060001
    2.95408106]][0m
[37m[1m[2023-07-11 01:13:10,505][233954] Max Reward on eval: 401.91498803682623[0m
[37m[1m[2023-07-11 01:13:10,505][233954] Min Reward on eval: -720.6329993918538[0m
[37m[1m[2023-07-11 01:13:10,505][233954] Mean Reward across all agents: 86.92778013396892[0m
[37m[1m[2023-07-11 01:13:10,506][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:13:10,514][233954] mean_value=-7.017043719536693, max_value=748.2608246622607[0m
[37m[1m[2023-07-11 01:13:10,517][233954] New mean coefficients: [[ 0.5980364 -4.4969635  4.731723   5.6786613 -5.5409527  2.7603369]][0m
[37m[1m[2023-07-11 01:13:10,518][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:13:19,593][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 01:13:19,594][233954] FPS: 423203.02[0m
[36m[2023-07-11 01:13:19,596][233954] itr=91, itrs=2000, Progress: 4.55%[0m
[36m[2023-07-11 01:13:31,328][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 01:13:31,328][233954] FPS: 328186.86[0m
[36m[2023-07-11 01:13:35,617][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:13:35,617][233954] Reward + Measures: [[8052.15722116    0.19442867    0.26782402    0.20077099    0.00766733
     2.03227258]][0m
[37m[1m[2023-07-11 01:13:35,618][233954] Max Reward on eval: 8052.157221158065[0m
[37m[1m[2023-07-11 01:13:35,618][233954] Min Reward on eval: 8052.157221158065[0m
[37m[1m[2023-07-11 01:13:35,618][233954] Mean Reward across all agents: 8052.157221158065[0m
[37m[1m[2023-07-11 01:13:35,618][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:13:40,604][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:13:40,610][233954] Reward + Measures: [[ 36.4806824    0.70029998   0.1383       0.65939999   0.66779995
    3.23034096]
 [ 77.27685698   0.46420002   0.4113       0.1901       0.38120005
    3.1069119 ]
 [ 71.71744098   0.1603       0.1146       0.1489       0.0663
    3.22234416]
 ...
 [  5.00217701   0.4797       0.0868       0.4621       0.40780002
    3.04387331]
 [-39.68666131   0.45050001   0.26809999   0.41910002   0.2498
    3.0792191 ]
 [ 58.4987438    0.21520002   0.27019998   0.1513       0.1894
    3.49099612]][0m
[37m[1m[2023-07-11 01:13:40,610][233954] Max Reward on eval: 196.97575567872846[0m
[37m[1m[2023-07-11 01:13:40,610][233954] Min Reward on eval: -143.8837990436703[0m
[37m[1m[2023-07-11 01:13:40,611][233954] Mean Reward across all agents: 38.07728480036817[0m
[37m[1m[2023-07-11 01:13:40,611][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:13:40,626][233954] mean_value=219.46139105464002, max_value=664.9749674474821[0m
[37m[1m[2023-07-11 01:13:40,629][233954] New mean coefficients: [[ 0.40015918 -4.9679275   4.738278    5.8274584  -5.7329206   2.4200745 ]][0m
[37m[1m[2023-07-11 01:13:40,630][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:13:49,591][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 01:13:49,591][233954] FPS: 428618.59[0m
[36m[2023-07-11 01:13:49,593][233954] itr=92, itrs=2000, Progress: 4.60%[0m
[36m[2023-07-11 01:14:01,239][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 01:14:01,239][233954] FPS: 330683.36[0m
[36m[2023-07-11 01:14:05,530][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:14:05,530][233954] Reward + Measures: [[8023.91851666    0.19563501    0.27448201    0.20150267    0.00826467
     2.05410528]][0m
[37m[1m[2023-07-11 01:14:05,531][233954] Max Reward on eval: 8023.918516663507[0m
[37m[1m[2023-07-11 01:14:05,531][233954] Min Reward on eval: 8023.918516663507[0m
[37m[1m[2023-07-11 01:14:05,531][233954] Mean Reward across all agents: 8023.918516663507[0m
[37m[1m[2023-07-11 01:14:05,531][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:14:10,533][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:14:10,539][233954] Reward + Measures: [[ -72.99874131    0.46880004    0.4384        0.45280001    0.15100001
     3.27884674]
 [  -2.13336184    0.33810005    0.29990003    0.34220001    0.10079999
     3.08778358]
 [  17.14170875    0.44890004    0.27229998    0.45380002    0.16159999
     3.22121859]
 ...
 [ 129.8960643     0.31040001    0.38210002    0.26860002    0.22909999
     2.94028211]
 [  45.3089833     0.2053        0.35520002    0.1146        0.2825
     3.30801439]
 [-188.05851627    0.40980002    0.44420001    0.44029999    0.08160001
     3.27183414]][0m
[37m[1m[2023-07-11 01:14:10,539][233954] Max Reward on eval: 537.7298984578578[0m
[37m[1m[2023-07-11 01:14:10,539][233954] Min Reward on eval: -491.04052158053963[0m
[37m[1m[2023-07-11 01:14:10,539][233954] Mean Reward across all agents: -6.521679642444758[0m
[37m[1m[2023-07-11 01:14:10,540][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:14:10,548][233954] mean_value=65.84074957732416, max_value=551.3692993601784[0m
[37m[1m[2023-07-11 01:14:10,551][233954] New mean coefficients: [[ 0.61180645 -5.020289    5.268958    5.641039   -5.1637845   2.2660363 ]][0m
[37m[1m[2023-07-11 01:14:10,552][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:14:19,652][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 01:14:19,652][233954] FPS: 422072.80[0m
[36m[2023-07-11 01:14:19,655][233954] itr=93, itrs=2000, Progress: 4.65%[0m
[36m[2023-07-11 01:14:31,437][233954] train() took 11.75 seconds to complete[0m
[36m[2023-07-11 01:14:31,438][233954] FPS: 326842.96[0m
[36m[2023-07-11 01:14:35,727][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:14:35,728][233954] Reward + Measures: [[8054.54125678    0.194637      0.27462965    0.20031433    0.007222
     2.06844258]][0m
[37m[1m[2023-07-11 01:14:35,728][233954] Max Reward on eval: 8054.541256776059[0m
[37m[1m[2023-07-11 01:14:35,728][233954] Min Reward on eval: 8054.541256776059[0m
[37m[1m[2023-07-11 01:14:35,728][233954] Mean Reward across all agents: 8054.541256776059[0m
[37m[1m[2023-07-11 01:14:35,729][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:14:40,705][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:14:40,711][233954] Reward + Measures: [[123.63822685   0.12920001   0.1178       0.07740001   0.16670001
    3.00006986]
 [ 35.60519073   0.0792       0.1286       0.1023       0.09690001
    3.35589576]
 [ 86.13057826   0.13060002   0.19079998   0.10290001   0.18800001
    2.93964911]
 ...
 [ 81.35405943   0.1099       0.1547       0.1073       0.113
    2.95932841]
 [179.07050015   0.10140001   0.116        0.08310001   0.1222
    2.89616251]
 [ 28.22292785   0.08890001   0.113        0.0872       0.0868
    2.8745842 ]][0m
[37m[1m[2023-07-11 01:14:40,711][233954] Max Reward on eval: 319.91641616038976[0m
[37m[1m[2023-07-11 01:14:40,711][233954] Min Reward on eval: -114.28116708826273[0m
[37m[1m[2023-07-11 01:14:40,712][233954] Mean Reward across all agents: 82.71156601316957[0m
[37m[1m[2023-07-11 01:14:40,712][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:14:40,714][233954] mean_value=-162.95052318266875, max_value=731.6653604395688[0m
[37m[1m[2023-07-11 01:14:40,716][233954] New mean coefficients: [[ 0.9835037 -4.9153514  6.0421553  4.783819  -5.143136   1.8417258]][0m
[37m[1m[2023-07-11 01:14:40,717][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:14:49,687][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 01:14:49,688][233954] FPS: 428172.71[0m
[36m[2023-07-11 01:14:49,690][233954] itr=94, itrs=2000, Progress: 4.70%[0m
[36m[2023-07-11 01:15:01,564][233954] train() took 11.84 seconds to complete[0m
[36m[2023-07-11 01:15:01,565][233954] FPS: 324212.60[0m
[36m[2023-07-11 01:15:05,912][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:15:05,913][233954] Reward + Measures: [[7995.64762861    0.19553433    0.28135663    0.202198      0.01085967
     2.06898212]][0m
[37m[1m[2023-07-11 01:15:05,913][233954] Max Reward on eval: 7995.64762860657[0m
[37m[1m[2023-07-11 01:15:05,913][233954] Min Reward on eval: 7995.64762860657[0m
[37m[1m[2023-07-11 01:15:05,914][233954] Mean Reward across all agents: 7995.64762860657[0m
[37m[1m[2023-07-11 01:15:05,914][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:15:11,070][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:15:11,070][233954] Reward + Measures: [[ 65.62872046   0.19329999   0.27579999   0.23710001   0.26440001
    2.98691583]
 [ 35.79314354   0.1114       0.13440001   0.12670001   0.1157
    2.95581746]
 [ 24.02782952   0.1708       0.34029999   0.27200001   0.22059999
    3.01220584]
 ...
 [176.64095544   0.10040001   0.18990001   0.163        0.14649999
    3.08793855]
 [158.89925558   0.17879999   0.3001       0.24920002   0.23800002
    3.13063288]
 [132.09920764   0.13780001   0.19769999   0.16110002   0.16779999
    3.06254625]][0m
[37m[1m[2023-07-11 01:15:11,070][233954] Max Reward on eval: 330.40624645161444[0m
[37m[1m[2023-07-11 01:15:11,071][233954] Min Reward on eval: -105.26882337271236[0m
[37m[1m[2023-07-11 01:15:11,071][233954] Mean Reward across all agents: 106.06153987469202[0m
[37m[1m[2023-07-11 01:15:11,071][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:15:11,075][233954] mean_value=-82.25020444358059, max_value=558.7336896839957[0m
[37m[1m[2023-07-11 01:15:11,078][233954] New mean coefficients: [[ 0.7344475 -5.4162197  5.5384097  3.914672  -5.8645806  1.5045764]][0m
[37m[1m[2023-07-11 01:15:11,078][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:15:20,107][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 01:15:20,107][233954] FPS: 425386.16[0m
[36m[2023-07-11 01:15:20,109][233954] itr=95, itrs=2000, Progress: 4.75%[0m
[36m[2023-07-11 01:15:31,708][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 01:15:31,708][233954] FPS: 331996.08[0m
[36m[2023-07-11 01:15:35,985][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:15:35,985][233954] Reward + Measures: [[7908.56106797    0.19250834    0.26797432    0.19581734    0.00947633
     2.06128192]][0m
[37m[1m[2023-07-11 01:15:35,985][233954] Max Reward on eval: 7908.561067973914[0m
[37m[1m[2023-07-11 01:15:35,986][233954] Min Reward on eval: 7908.561067973914[0m
[37m[1m[2023-07-11 01:15:35,986][233954] Mean Reward across all agents: 7908.561067973914[0m
[37m[1m[2023-07-11 01:15:35,986][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:15:40,931][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:15:40,931][233954] Reward + Measures: [[ 83.7782452    0.1794       0.13860001   0.1666       0.12279999
    3.29540038]
 [153.90967541   0.33790001   0.17940001   0.19070001   0.22399998
    3.25632215]
 [148.39140229   0.1495       0.1531       0.11130001   0.10820001
    3.03586507]
 ...
 [139.34946873   0.25139999   0.1062       0.229        0.19600001
    3.330796  ]
 [ 54.20773912   0.27160001   0.1674       0.1237       0.13610001
    3.01106048]
 [161.85296486   0.1627       0.1631       0.12840001   0.1277
    3.03099799]][0m
[37m[1m[2023-07-11 01:15:40,932][233954] Max Reward on eval: 286.64741613361986[0m
[37m[1m[2023-07-11 01:15:40,932][233954] Min Reward on eval: -17.986117212148383[0m
[37m[1m[2023-07-11 01:15:40,932][233954] Mean Reward across all agents: 97.25550103918278[0m
[37m[1m[2023-07-11 01:15:40,932][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:15:40,937][233954] mean_value=46.37414758075678, max_value=689.7817354393658[0m
[37m[1m[2023-07-11 01:15:40,940][233954] New mean coefficients: [[ 0.8126687 -5.8370776  6.2973137  4.6157007 -6.0000286  1.1346312]][0m
[37m[1m[2023-07-11 01:15:40,941][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:15:49,883][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 01:15:49,883][233954] FPS: 429534.18[0m
[36m[2023-07-11 01:15:49,885][233954] itr=96, itrs=2000, Progress: 4.80%[0m
[36m[2023-07-11 01:16:01,416][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 01:16:01,417][233954] FPS: 333865.23[0m
[36m[2023-07-11 01:16:05,679][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:16:05,680][233954] Reward + Measures: [[7921.80620461    0.19291767    0.27309632    0.196787      0.01021167
     2.05351281]][0m
[37m[1m[2023-07-11 01:16:05,680][233954] Max Reward on eval: 7921.806204613381[0m
[37m[1m[2023-07-11 01:16:05,680][233954] Min Reward on eval: 7921.806204613381[0m
[37m[1m[2023-07-11 01:16:05,680][233954] Mean Reward across all agents: 7921.806204613381[0m
[37m[1m[2023-07-11 01:16:05,681][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:16:10,676][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:16:10,682][233954] Reward + Measures: [[ 17.2011061    0.36609998   0.0834       0.28290001   0.18410002
    2.95378017]
 [216.22885746   0.28570005   0.1567       0.25560001   0.2235
    2.86282897]
 [145.33764348   0.29270002   0.10569999   0.24200001   0.23550001
    2.77249384]
 ...
 [256.88424039   0.25929999   0.16480002   0.26210001   0.2084
    2.60535383]
 [140.55056289   0.25980002   0.13950001   0.18200001   0.1772
    2.83654475]
 [100.7871821    0.2534       0.12150001   0.19789998   0.14820001
    2.76174378]][0m
[37m[1m[2023-07-11 01:16:10,682][233954] Max Reward on eval: 554.0799858756363[0m
[37m[1m[2023-07-11 01:16:10,682][233954] Min Reward on eval: -57.79862349387258[0m
[37m[1m[2023-07-11 01:16:10,683][233954] Mean Reward across all agents: 151.76454732537542[0m
[37m[1m[2023-07-11 01:16:10,683][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:16:10,689][233954] mean_value=-3.950315310349421, max_value=622.1320069285109[0m
[37m[1m[2023-07-11 01:16:10,692][233954] New mean coefficients: [[ 0.7566693 -5.052555   6.1587925  5.072918  -5.5590944  1.0857112]][0m
[37m[1m[2023-07-11 01:16:10,693][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:16:19,657][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 01:16:19,657][233954] FPS: 428461.76[0m
[36m[2023-07-11 01:16:19,659][233954] itr=97, itrs=2000, Progress: 4.85%[0m
[36m[2023-07-11 01:16:31,213][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 01:16:31,213][233954] FPS: 333269.86[0m
[36m[2023-07-11 01:16:35,549][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:16:35,550][233954] Reward + Measures: [[7968.78129408    0.19348334    0.27428433    0.19629931    0.008755
     2.07829428]][0m
[37m[1m[2023-07-11 01:16:35,550][233954] Max Reward on eval: 7968.781294079145[0m
[37m[1m[2023-07-11 01:16:35,550][233954] Min Reward on eval: 7968.781294079145[0m
[37m[1m[2023-07-11 01:16:35,550][233954] Mean Reward across all agents: 7968.781294079145[0m
[37m[1m[2023-07-11 01:16:35,551][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:16:40,562][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:16:40,568][233954] Reward + Measures: [[ 96.3196999    0.17220001   0.16679999   0.15750001   0.13950001
    2.98670506]
 [192.45808982   0.1115       0.2237       0.1698       0.16520001
    2.88627696]
 [439.66080041   0.1287       0.234        0.17470001   0.19020002
    2.55117774]
 ...
 [134.36629726   0.19780003   0.1921       0.1425       0.20550001
    2.52972388]
 [104.75125881   0.1151       0.1248       0.0922       0.0893
    2.77957821]
 [ 86.07087021   0.14479999   0.19570002   0.171        0.1543
    2.93565154]][0m
[37m[1m[2023-07-11 01:16:40,569][233954] Max Reward on eval: 439.66080041378734[0m
[37m[1m[2023-07-11 01:16:40,569][233954] Min Reward on eval: -339.6414242213592[0m
[37m[1m[2023-07-11 01:16:40,569][233954] Mean Reward across all agents: 76.74817351630509[0m
[37m[1m[2023-07-11 01:16:40,569][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:16:40,572][233954] mean_value=-161.15519627956536, max_value=511.50468065477907[0m
[37m[1m[2023-07-11 01:16:40,574][233954] New mean coefficients: [[ 0.81887215 -3.975977    5.812455    6.4904513  -5.2227845   1.0448668 ]][0m
[37m[1m[2023-07-11 01:16:40,575][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:16:49,583][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 01:16:49,583][233954] FPS: 426376.30[0m
[36m[2023-07-11 01:16:49,585][233954] itr=98, itrs=2000, Progress: 4.90%[0m
[36m[2023-07-11 01:17:01,495][233954] train() took 11.88 seconds to complete[0m
[36m[2023-07-11 01:17:01,495][233954] FPS: 323279.03[0m
[36m[2023-07-11 01:17:05,855][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:17:05,856][233954] Reward + Measures: [[8044.8724792     0.19496733    0.27741802    0.19771264    0.00799367
     2.08794689]][0m
[37m[1m[2023-07-11 01:17:05,856][233954] Max Reward on eval: 8044.872479204434[0m
[37m[1m[2023-07-11 01:17:05,856][233954] Min Reward on eval: 8044.872479204434[0m
[37m[1m[2023-07-11 01:17:05,857][233954] Mean Reward across all agents: 8044.872479204434[0m
[37m[1m[2023-07-11 01:17:05,857][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:17:11,028][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:17:11,029][233954] Reward + Measures: [[ 57.80443507   0.28490001   0.1046       0.30310002   0.2536
    3.3671391 ]
 [227.20085577   0.14749999   0.1442       0.14830001   0.1056
    3.43704724]
 [163.21446439   0.1074       0.1023       0.09060001   0.0832
    3.32732272]
 ...
 [107.23785046   0.59100002   0.0498       0.5959       0.56580001
    3.14210749]
 [ 94.02356671   0.22350001   0.088        0.22239999   0.205
    3.21123099]
 [177.59520013   0.23989999   0.13070001   0.17510001   0.11919999
    3.31390572]][0m
[37m[1m[2023-07-11 01:17:11,029][233954] Max Reward on eval: 320.2207660496235[0m
[37m[1m[2023-07-11 01:17:11,030][233954] Min Reward on eval: -206.79043145477772[0m
[37m[1m[2023-07-11 01:17:11,030][233954] Mean Reward across all agents: 112.47173747963153[0m
[37m[1m[2023-07-11 01:17:11,030][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:17:11,040][233954] mean_value=133.5852443690271, max_value=650.9700581488305[0m
[37m[1m[2023-07-11 01:17:11,042][233954] New mean coefficients: [[ 0.8198863 -3.8531308  5.9129734  7.4881783 -5.411078   1.2430258]][0m
[37m[1m[2023-07-11 01:17:11,043][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:17:20,164][233954] train() took 9.12 seconds to complete[0m
[36m[2023-07-11 01:17:20,164][233954] FPS: 421091.12[0m
[36m[2023-07-11 01:17:20,167][233954] itr=99, itrs=2000, Progress: 4.95%[0m
[36m[2023-07-11 01:17:31,845][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 01:17:31,845][233954] FPS: 329705.11[0m
[36m[2023-07-11 01:17:36,187][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:17:36,187][233954] Reward + Measures: [[7878.32649853    0.19308834    0.278447      0.19864002    0.01158967
     2.06602859]][0m
[37m[1m[2023-07-11 01:17:36,187][233954] Max Reward on eval: 7878.326498527067[0m
[37m[1m[2023-07-11 01:17:36,187][233954] Min Reward on eval: 7878.326498527067[0m
[37m[1m[2023-07-11 01:17:36,188][233954] Mean Reward across all agents: 7878.326498527067[0m
[37m[1m[2023-07-11 01:17:36,188][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:17:41,230][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:17:41,236][233954] Reward + Measures: [[  51.60131398    0.43290001    0.58700001    0.26910001    0.55470002
     3.15957427]
 [ -97.51581053    0.49720001    0.20390001    0.41360003    0.4147
     3.33647704]
 [  13.64572975    0.41490003    0.59849995    0.1504        0.60869998
     3.24312568]
 ...
 [  92.86800935    0.46150002    0.26949999    0.45089999    0.33130002
     3.01068759]
 [ -70.66984992    0.30070001    0.60650003    0.1495        0.55870003
     3.08158565]
 [-184.27837472    0.59380001    0.17549999    0.53730005    0.48859999
     3.48302269]][0m
[37m[1m[2023-07-11 01:17:41,236][233954] Max Reward on eval: 472.90725515815427[0m
[37m[1m[2023-07-11 01:17:41,236][233954] Min Reward on eval: -381.2482757201418[0m
[37m[1m[2023-07-11 01:17:41,236][233954] Mean Reward across all agents: -21.802646771392368[0m
[37m[1m[2023-07-11 01:17:41,237][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:17:41,251][233954] mean_value=250.8514271354079, max_value=586.972657450661[0m
[37m[1m[2023-07-11 01:17:41,254][233954] New mean coefficients: [[ 0.55964935 -4.095903    5.0559983   8.519221   -6.9504642   0.85402226]][0m
[37m[1m[2023-07-11 01:17:41,255][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:17:50,296][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 01:17:50,296][233954] FPS: 424786.33[0m
[36m[2023-07-11 01:17:50,299][233954] itr=100, itrs=2000, Progress: 5.00%[0m
[37m[1m[2023-07-11 01:20:10,652][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000080[0m
[36m[2023-07-11 01:20:22,891][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 01:20:22,892][233954] FPS: 329408.30[0m
[36m[2023-07-11 01:20:27,164][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:20:27,164][233954] Reward + Measures: [[7917.79343781    0.19440566    0.28288034    0.199109      0.01028467
     2.07634854]][0m
[37m[1m[2023-07-11 01:20:27,164][233954] Max Reward on eval: 7917.793437812473[0m
[37m[1m[2023-07-11 01:20:27,165][233954] Min Reward on eval: 7917.793437812473[0m
[37m[1m[2023-07-11 01:20:27,165][233954] Mean Reward across all agents: 7917.793437812473[0m
[37m[1m[2023-07-11 01:20:27,165][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:20:32,153][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:20:32,154][233954] Reward + Measures: [[128.40980593   0.29280001   0.37060001   0.2369       0.41279998
    2.81921315]
 [-38.17512641   0.28830001   0.30040002   0.26210001   0.32300001
    2.95202708]
 [ 87.90318864   0.25390002   0.2581       0.16110002   0.2814
    3.01679516]
 ...
 [ 47.59669462   0.25120002   0.28569999   0.19770001   0.3105
    3.00814009]
 [122.73498059   0.34099999   0.4075       0.28200004   0.433
    2.99645424]
 [ -6.71585887   0.22239999   0.36739999   0.1969       0.39610001
    2.72561383]][0m
[37m[1m[2023-07-11 01:20:32,154][233954] Max Reward on eval: 212.44467641357332[0m
[37m[1m[2023-07-11 01:20:32,154][233954] Min Reward on eval: -157.23811821490526[0m
[37m[1m[2023-07-11 01:20:32,154][233954] Mean Reward across all agents: 45.67739854695378[0m
[37m[1m[2023-07-11 01:20:32,155][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:20:32,159][233954] mean_value=-96.85696393077843, max_value=577.8393986029706[0m
[37m[1m[2023-07-11 01:20:32,161][233954] New mean coefficients: [[ 0.4794113  -3.370738    4.7214823   9.0388975  -6.7459455   0.80179024]][0m
[37m[1m[2023-07-11 01:20:32,162][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:20:41,104][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 01:20:41,105][233954] FPS: 429515.48[0m
[36m[2023-07-11 01:20:41,107][233954] itr=101, itrs=2000, Progress: 5.05%[0m
[36m[2023-07-11 01:20:52,890][233954] train() took 11.75 seconds to complete[0m
[36m[2023-07-11 01:20:52,891][233954] FPS: 326727.09[0m
[36m[2023-07-11 01:20:57,100][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:20:57,101][233954] Reward + Measures: [[7969.07055873    0.194518      0.28568199    0.200114      0.00895333
     2.09116817]][0m
[37m[1m[2023-07-11 01:20:57,101][233954] Max Reward on eval: 7969.070558732701[0m
[37m[1m[2023-07-11 01:20:57,101][233954] Min Reward on eval: 7969.070558732701[0m
[37m[1m[2023-07-11 01:20:57,102][233954] Mean Reward across all agents: 7969.070558732701[0m
[37m[1m[2023-07-11 01:20:57,102][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:21:02,267][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:21:02,268][233954] Reward + Measures: [[ 53.06772404   0.114        0.1603       0.12410001   0.0994
    3.00739217]
 [123.76803603   0.15809999   0.1478       0.17290001   0.1434
    3.15014887]
 [-24.57152819   0.1243       0.15030001   0.0837       0.1357
    3.10963202]
 ...
 [-14.66768939   0.25690004   0.26320001   0.25009999   0.09790001
    3.31453013]
 [-21.94930576   0.2438       0.22500001   0.24209999   0.0666
    3.241184  ]
 [ 47.92466633   0.11189999   0.13079999   0.13179998   0.08200001
    3.20872188]][0m
[37m[1m[2023-07-11 01:21:02,268][233954] Max Reward on eval: 198.6849224348087[0m
[37m[1m[2023-07-11 01:21:02,268][233954] Min Reward on eval: -262.84571961946784[0m
[37m[1m[2023-07-11 01:21:02,269][233954] Mean Reward across all agents: 0.3666358565411165[0m
[37m[1m[2023-07-11 01:21:02,269][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:21:02,273][233954] mean_value=-61.33129551277401, max_value=552.8911322115921[0m
[37m[1m[2023-07-11 01:21:02,276][233954] New mean coefficients: [[ 0.21330419 -3.9141905   3.9970129  10.055969   -8.153638    0.08182156]][0m
[37m[1m[2023-07-11 01:21:02,277][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:21:11,359][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 01:21:11,359][233954] FPS: 422921.22[0m
[36m[2023-07-11 01:21:11,361][233954] itr=102, itrs=2000, Progress: 5.10%[0m
[36m[2023-07-11 01:21:23,079][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 01:21:23,079][233954] FPS: 328581.96[0m
[36m[2023-07-11 01:21:27,309][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:21:27,310][233954] Reward + Measures: [[7955.938604      0.19774467    0.298756      0.20408933    0.00951233
     2.08489347]][0m
[37m[1m[2023-07-11 01:21:27,310][233954] Max Reward on eval: 7955.938604002713[0m
[37m[1m[2023-07-11 01:21:27,310][233954] Min Reward on eval: 7955.938604002713[0m
[37m[1m[2023-07-11 01:21:27,311][233954] Mean Reward across all agents: 7955.938604002713[0m
[37m[1m[2023-07-11 01:21:27,311][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:21:32,305][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:21:32,305][233954] Reward + Measures: [[ 78.6290758    0.0938       0.0764       0.07250001   0.0653
    3.37421274]
 [-28.01868546   0.32780001   0.2881       0.32780001   0.0619
    3.33424735]
 [ 53.01433288   0.1005       0.09810001   0.0839       0.0757
    3.44946027]
 ...
 [ 97.2234312    0.14659999   0.1236       0.12460001   0.0903
    3.3223145 ]
 [ 73.45740166   0.1079       0.108        0.0847       0.0812
    3.32460213]
 [107.74824594   0.10860001   0.07539999   0.0911       0.069
    3.09222269]][0m
[37m[1m[2023-07-11 01:21:32,306][233954] Max Reward on eval: 233.80161423468962[0m
[37m[1m[2023-07-11 01:21:32,306][233954] Min Reward on eval: -130.39286966957152[0m
[37m[1m[2023-07-11 01:21:32,306][233954] Mean Reward across all agents: 69.35233210176312[0m
[37m[1m[2023-07-11 01:21:32,306][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:21:32,309][233954] mean_value=-85.49343134796469, max_value=633.5130481174216[0m
[37m[1m[2023-07-11 01:21:32,312][233954] New mean coefficients: [[ 0.2570277 -4.20177    4.3116655  9.367599  -8.6696205 -0.2024861]][0m
[37m[1m[2023-07-11 01:21:32,313][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:21:41,251][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 01:21:41,251][233954] FPS: 429699.49[0m
[36m[2023-07-11 01:21:41,253][233954] itr=103, itrs=2000, Progress: 5.15%[0m
[36m[2023-07-11 01:21:52,875][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 01:21:52,875][233954] FPS: 331340.14[0m
[36m[2023-07-11 01:21:57,143][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:21:57,143][233954] Reward + Measures: [[7635.5671722     0.19856365    0.29957935    0.20763867    0.01281333
     2.04436302]][0m
[37m[1m[2023-07-11 01:21:57,144][233954] Max Reward on eval: 7635.567172196395[0m
[37m[1m[2023-07-11 01:21:57,144][233954] Min Reward on eval: 7635.567172196395[0m
[37m[1m[2023-07-11 01:21:57,144][233954] Mean Reward across all agents: 7635.567172196395[0m
[37m[1m[2023-07-11 01:21:57,144][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:22:02,189][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:22:02,190][233954] Reward + Measures: [[101.90443717   0.1664       0.11730001   0.0981       0.0873
    3.35714531]
 [ 78.06386592   0.1948       0.1463       0.08630001   0.12549999
    3.32617879]
 [ 71.34254215   0.2053       0.1682       0.1497       0.16239999
    3.05571604]
 ...
 [ 44.47069747   0.13630001   0.1217       0.1037       0.10910001
    3.24807811]
 [106.84825636   0.1673       0.1513       0.11989999   0.13970001
    3.30198073]
 [ 90.54729448   0.20560001   0.1883       0.1287       0.13030002
    3.26703715]][0m
[37m[1m[2023-07-11 01:22:02,190][233954] Max Reward on eval: 367.8644318806473[0m
[37m[1m[2023-07-11 01:22:02,191][233954] Min Reward on eval: -42.45386369060725[0m
[37m[1m[2023-07-11 01:22:02,191][233954] Mean Reward across all agents: 77.78944879899282[0m
[37m[1m[2023-07-11 01:22:02,191][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:22:02,195][233954] mean_value=-57.420383337128904, max_value=562.5570070383139[0m
[37m[1m[2023-07-11 01:22:02,198][233954] New mean coefficients: [[ 0.28286624 -5.0851083   5.121969    8.147926   -7.6611643  -0.22667262]][0m
[37m[1m[2023-07-11 01:22:02,199][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:22:11,208][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 01:22:11,209][233954] FPS: 426271.58[0m
[36m[2023-07-11 01:22:11,211][233954] itr=104, itrs=2000, Progress: 5.20%[0m
[36m[2023-07-11 01:22:22,875][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 01:22:22,875][233954] FPS: 330096.29[0m
[36m[2023-07-11 01:22:27,196][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:22:27,197][233954] Reward + Measures: [[7547.57677457    0.19921334    0.30834433    0.20860532    0.01332267
     2.01555657]][0m
[37m[1m[2023-07-11 01:22:27,197][233954] Max Reward on eval: 7547.576774571673[0m
[37m[1m[2023-07-11 01:22:27,197][233954] Min Reward on eval: 7547.576774571673[0m
[37m[1m[2023-07-11 01:22:27,198][233954] Mean Reward across all agents: 7547.576774571673[0m
[37m[1m[2023-07-11 01:22:27,198][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:22:32,217][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:22:32,217][233954] Reward + Measures: [[ 30.63533006   0.1059       0.42250004   0.15290001   0.44260001
    3.14894795]
 [-23.01239167   0.14220001   0.62790006   0.33549997   0.62429994
    3.55789495]
 [112.16125218   0.29190001   0.47219998   0.1793       0.3664
    2.5815773 ]
 ...
 [ -7.30694398   0.1336       0.50990003   0.14         0.51609999
    3.30041671]
 [ 19.96318816   0.1637       0.58779997   0.1487       0.56690001
    2.97533774]
 [-51.68330166   0.1833       0.46330005   0.15539999   0.4429
    2.98689842]][0m
[37m[1m[2023-07-11 01:22:32,218][233954] Max Reward on eval: 256.29008045755324[0m
[37m[1m[2023-07-11 01:22:32,218][233954] Min Reward on eval: -399.417116151005[0m
[37m[1m[2023-07-11 01:22:32,218][233954] Mean Reward across all agents: -4.363538631777793[0m
[37m[1m[2023-07-11 01:22:32,218][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:22:32,228][233954] mean_value=163.86500063268716, max_value=611.2356561833992[0m
[37m[1m[2023-07-11 01:22:32,230][233954] New mean coefficients: [[ 0.22353321 -6.0141745   4.515339    8.646624   -8.582863   -0.5390359 ]][0m
[37m[1m[2023-07-11 01:22:32,231][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:22:41,358][233954] train() took 9.13 seconds to complete[0m
[36m[2023-07-11 01:22:41,359][233954] FPS: 420798.99[0m
[36m[2023-07-11 01:22:41,361][233954] itr=105, itrs=2000, Progress: 5.25%[0m
[36m[2023-07-11 01:22:53,048][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 01:22:53,048][233954] FPS: 329456.24[0m
[36m[2023-07-11 01:22:57,360][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:22:57,361][233954] Reward + Measures: [[7792.47739452    0.19625899    0.30830264    0.20542601    0.010805
     2.04188371]][0m
[37m[1m[2023-07-11 01:22:57,361][233954] Max Reward on eval: 7792.477394515061[0m
[37m[1m[2023-07-11 01:22:57,361][233954] Min Reward on eval: 7792.477394515061[0m
[37m[1m[2023-07-11 01:22:57,361][233954] Mean Reward across all agents: 7792.477394515061[0m
[37m[1m[2023-07-11 01:22:57,362][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:23:02,473][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:23:02,478][233954] Reward + Measures: [[-74.75289244   0.55610007   0.42009997   0.64440006   0.51770008
    3.37506723]
 [-34.49169032   0.2335       0.36740002   0.27200001   0.33680001
    3.67461777]
 [ 61.82616701   0.26270002   0.5693       0.40009999   0.54350001
    3.18942094]
 ...
 [  2.12530255   0.68149996   0.74809998   0.71110004   0.70480007
    3.2705369 ]
 [-32.78587031   0.32570001   0.61980003   0.51320004   0.51429999
    3.33485723]
 [-34.57037301   0.69810003   0.72950006   0.72710001   0.70850003
    3.19546199]][0m
[37m[1m[2023-07-11 01:23:02,478][233954] Max Reward on eval: 142.03398470827378[0m
[37m[1m[2023-07-11 01:23:02,479][233954] Min Reward on eval: -119.80579639673233[0m
[37m[1m[2023-07-11 01:23:02,479][233954] Mean Reward across all agents: -12.208542169334171[0m
[37m[1m[2023-07-11 01:23:02,479][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:23:02,493][233954] mean_value=231.10222184450976, max_value=600.1080641496926[0m
[37m[1m[2023-07-11 01:23:02,495][233954] New mean coefficients: [[ 0.5526798 -6.916289   5.1223307  7.7821107 -8.067931  -1.0092437]][0m
[37m[1m[2023-07-11 01:23:02,496][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:23:11,517][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 01:23:11,517][233954] FPS: 425758.10[0m
[36m[2023-07-11 01:23:11,520][233954] itr=106, itrs=2000, Progress: 5.30%[0m
[36m[2023-07-11 01:23:23,125][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 01:23:23,126][233954] FPS: 331765.72[0m
[36m[2023-07-11 01:23:27,466][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:23:27,466][233954] Reward + Measures: [[7847.28448868    0.19718499    0.31478363    0.206297      0.00975167
     2.03624415]][0m
[37m[1m[2023-07-11 01:23:27,466][233954] Max Reward on eval: 7847.284488681446[0m
[37m[1m[2023-07-11 01:23:27,466][233954] Min Reward on eval: 7847.284488681446[0m
[37m[1m[2023-07-11 01:23:27,467][233954] Mean Reward across all agents: 7847.284488681446[0m
[37m[1m[2023-07-11 01:23:27,467][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:23:32,671][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:23:32,671][233954] Reward + Measures: [[-131.11956367    0.28380001    0.29819998    0.0875        0.25709999
     3.41630554]
 [-145.61246744    0.28790003    0.35179999    0.0594        0.31710002
     3.28073859]
 [ -53.7331467     0.23150001    0.31820002    0.0535        0.25549999
     3.12400699]
 ...
 [ -27.48116905    0.2471        0.36879998    0.084         0.29530001
     3.0815289 ]
 [ -88.39487608    0.28960001    0.31570002    0.0692        0.27849999
     3.39116979]
 [-184.71509296    0.3567        0.39089999    0.0565        0.36540002
     3.48569345]][0m
[37m[1m[2023-07-11 01:23:32,671][233954] Max Reward on eval: 156.09087355774827[0m
[37m[1m[2023-07-11 01:23:32,672][233954] Min Reward on eval: -537.6107976417989[0m
[37m[1m[2023-07-11 01:23:32,672][233954] Mean Reward across all agents: -88.02197119679448[0m
[37m[1m[2023-07-11 01:23:32,672][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:23:32,677][233954] mean_value=-110.59961271485896, max_value=479.4074092522962[0m
[37m[1m[2023-07-11 01:23:32,680][233954] New mean coefficients: [[ 0.00061852 -6.8071933   3.8285954   9.223403   -9.109881   -0.9721422 ]][0m
[37m[1m[2023-07-11 01:23:32,681][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:23:41,779][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 01:23:41,779][233954] FPS: 422143.26[0m
[36m[2023-07-11 01:23:41,782][233954] itr=107, itrs=2000, Progress: 5.35%[0m
[36m[2023-07-11 01:23:53,493][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 01:23:53,493][233954] FPS: 328788.67[0m
[36m[2023-07-11 01:23:57,850][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:23:57,850][233954] Reward + Measures: [[7762.87561787    0.19694465    0.31720033    0.206644      0.01063733
     2.01015067]][0m
[37m[1m[2023-07-11 01:23:57,850][233954] Max Reward on eval: 7762.875617872291[0m
[37m[1m[2023-07-11 01:23:57,851][233954] Min Reward on eval: 7762.875617872291[0m
[37m[1m[2023-07-11 01:23:57,851][233954] Mean Reward across all agents: 7762.875617872291[0m
[37m[1m[2023-07-11 01:23:57,851][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:24:02,824][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:24:02,824][233954] Reward + Measures: [[71.41641656  0.35520002  0.17999999  0.31590003  0.18550001  3.29302597]
 [60.4816034   0.2507      0.12089999  0.21120003  0.15010001  3.17342734]
 [40.74052955  0.29980001  0.14930001  0.32820001  0.1503      3.37661791]
 ...
 [24.21615395  0.22810002  0.1247      0.13950001  0.1002      3.3329742 ]
 [92.6858829   0.2397      0.16070001  0.2084      0.1591      3.42191243]
 [34.54383486  0.29340002  0.17990001  0.2447      0.1717      3.31991625]][0m
[37m[1m[2023-07-11 01:24:02,824][233954] Max Reward on eval: 160.67617451772094[0m
[37m[1m[2023-07-11 01:24:02,825][233954] Min Reward on eval: -282.00898035448046[0m
[37m[1m[2023-07-11 01:24:02,825][233954] Mean Reward across all agents: 49.7405045505068[0m
[37m[1m[2023-07-11 01:24:02,825][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:24:02,830][233954] mean_value=-38.00571746739473, max_value=578.6284747031983[0m
[37m[1m[2023-07-11 01:24:02,832][233954] New mean coefficients: [[ 0.04015247 -6.7122707   4.296013    8.879979   -8.466662   -1.0219517 ]][0m
[37m[1m[2023-07-11 01:24:02,833][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:24:11,872][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 01:24:11,873][233954] FPS: 424909.17[0m
[36m[2023-07-11 01:24:11,875][233954] itr=108, itrs=2000, Progress: 5.40%[0m
[36m[2023-07-11 01:24:23,673][233954] train() took 11.76 seconds to complete[0m
[36m[2023-07-11 01:24:23,674][233954] FPS: 326398.32[0m
[36m[2023-07-11 01:24:28,010][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:24:28,011][233954] Reward + Measures: [[7642.76416215    0.20097898    0.33601332    0.21254602    0.01177467
     1.97092855]][0m
[37m[1m[2023-07-11 01:24:28,011][233954] Max Reward on eval: 7642.764162151048[0m
[37m[1m[2023-07-11 01:24:28,011][233954] Min Reward on eval: 7642.764162151048[0m
[37m[1m[2023-07-11 01:24:28,012][233954] Mean Reward across all agents: 7642.764162151048[0m
[37m[1m[2023-07-11 01:24:28,012][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:24:33,046][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:24:33,046][233954] Reward + Measures: [[-15.64025733   0.0891       0.3091       0.20939998   0.28099999
    3.35147262]
 [ -7.18092804   0.095        0.20190001   0.12630001   0.1787
    3.49855423]
 [-57.80337454   0.18430001   0.3856       0.1538       0.43439999
    3.45489621]
 ...
 [-43.28725101   0.1434       0.27760002   0.1358       0.28420001
    3.51762009]
 [-33.43476526   0.24879999   0.51899999   0.11240001   0.59100002
    3.23172045]
 [ 75.84030203   0.13850001   0.32679999   0.1337       0.34650001
    3.1580174 ]][0m
[37m[1m[2023-07-11 01:24:33,046][233954] Max Reward on eval: 230.64436437729745[0m
[37m[1m[2023-07-11 01:24:33,047][233954] Min Reward on eval: -162.39546919725836[0m
[37m[1m[2023-07-11 01:24:33,047][233954] Mean Reward across all agents: 3.4114461418831[0m
[37m[1m[2023-07-11 01:24:33,047][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:24:33,052][233954] mean_value=-32.83813498636483, max_value=501.22719966638834[0m
[37m[1m[2023-07-11 01:24:33,054][233954] New mean coefficients: [[ 0.18858893 -6.4907293   4.6287704   8.293996   -8.603282   -1.2270957 ]][0m
[37m[1m[2023-07-11 01:24:33,055][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:24:42,134][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 01:24:42,135][233954] FPS: 423032.82[0m
[36m[2023-07-11 01:24:42,137][233954] itr=109, itrs=2000, Progress: 5.45%[0m
[36m[2023-07-11 01:24:53,839][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 01:24:53,839][233954] FPS: 329138.46[0m
[36m[2023-07-11 01:24:58,082][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:24:58,083][233954] Reward + Measures: [[7607.82515434    0.20232832    0.34782401    0.21523131    0.01243067
     1.94977522]][0m
[37m[1m[2023-07-11 01:24:58,083][233954] Max Reward on eval: 7607.825154336305[0m
[37m[1m[2023-07-11 01:24:58,083][233954] Min Reward on eval: 7607.825154336305[0m
[37m[1m[2023-07-11 01:24:58,083][233954] Mean Reward across all agents: 7607.825154336305[0m
[37m[1m[2023-07-11 01:24:58,084][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:25:03,139][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:25:03,139][233954] Reward + Measures: [[111.18089078   0.13370001   0.23339999   0.18980001   0.24689999
    3.1249969 ]
 [117.88624518   0.0949       0.1567       0.0921       0.0915
    3.27446747]
 [127.47942058   0.17490001   0.26079997   0.15710001   0.21060002
    3.23082519]
 ...
 [ 84.70354751   0.1312       0.1109       0.13960001   0.10339999
    3.42551231]
 [ -5.93927326   0.0728       0.1869       0.11360001   0.1146
    3.41396832]
 [ 85.264476     0.1043       0.13259999   0.11619999   0.10879999
    3.37807536]][0m
[37m[1m[2023-07-11 01:25:03,139][233954] Max Reward on eval: 193.03634614329786[0m
[37m[1m[2023-07-11 01:25:03,140][233954] Min Reward on eval: -48.544884980283676[0m
[37m[1m[2023-07-11 01:25:03,140][233954] Mean Reward across all agents: 70.95040554777977[0m
[37m[1m[2023-07-11 01:25:03,140][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:25:03,142][233954] mean_value=-81.85218500624788, max_value=412.1910637923198[0m
[37m[1m[2023-07-11 01:25:03,145][233954] New mean coefficients: [[ 0.03447111 -6.712796    5.0296125   7.7142467  -8.357058   -1.2815437 ]][0m
[37m[1m[2023-07-11 01:25:03,146][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:25:12,193][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 01:25:12,193][233954] FPS: 424530.38[0m
[36m[2023-07-11 01:25:12,196][233954] itr=110, itrs=2000, Progress: 5.50%[0m
[37m[1m[2023-07-11 01:27:46,350][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000090[0m
[36m[2023-07-11 01:27:58,657][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 01:27:58,658][233954] FPS: 328535.16[0m
[36m[2023-07-11 01:28:02,757][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:28:02,757][233954] Reward + Measures: [[7340.06098209    0.20364733    0.36319304    0.21956201    0.01530367
     1.89850676]][0m
[37m[1m[2023-07-11 01:28:02,757][233954] Max Reward on eval: 7340.060982091007[0m
[37m[1m[2023-07-11 01:28:02,758][233954] Min Reward on eval: 7340.060982091007[0m
[37m[1m[2023-07-11 01:28:02,758][233954] Mean Reward across all agents: 7340.060982091007[0m
[37m[1m[2023-07-11 01:28:02,758][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:28:07,772][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:28:07,773][233954] Reward + Measures: [[-60.80478881   0.36820003   0.29960001   0.40050003   0.0852
    3.07520628]
 [ -6.36653075   0.47010002   0.33990002   0.41370001   0.1202
    2.84713531]
 [-15.30336179   0.382        0.30889997   0.32599998   0.1184
    2.92853785]
 ...
 [-52.09216974   0.54430002   0.50200003   0.45739999   0.30329999
    3.41802025]
 [ 13.1308459    0.46870002   0.45230004   0.49629998   0.0804
    3.02886367]
 [ -7.48378888   0.4844       0.38729998   0.37269998   0.22049999
    3.11341262]][0m
[37m[1m[2023-07-11 01:28:07,773][233954] Max Reward on eval: 183.36018782649188[0m
[37m[1m[2023-07-11 01:28:07,773][233954] Min Reward on eval: -288.0698108907789[0m
[37m[1m[2023-07-11 01:28:07,773][233954] Mean Reward across all agents: -2.9798520215101987[0m
[37m[1m[2023-07-11 01:28:07,774][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:28:07,780][233954] mean_value=7.955085516474756, max_value=502.91624715571294[0m
[37m[1m[2023-07-11 01:28:07,782][233954] New mean coefficients: [[-0.09063028 -5.7044487   5.32866     7.3753114  -7.5654945  -0.9663373 ]][0m
[37m[1m[2023-07-11 01:28:07,783][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:28:16,777][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 01:28:16,778][233954] FPS: 427026.59[0m
[36m[2023-07-11 01:28:16,780][233954] itr=111, itrs=2000, Progress: 5.55%[0m
[36m[2023-07-11 01:28:28,441][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 01:28:28,441][233954] FPS: 330181.98[0m
[36m[2023-07-11 01:28:32,815][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:28:32,815][233954] Reward + Measures: [[6730.42662911    0.20497966    0.37952638    0.23018731    0.02493467
     1.82098067]][0m
[37m[1m[2023-07-11 01:28:32,816][233954] Max Reward on eval: 6730.426629107509[0m
[37m[1m[2023-07-11 01:28:32,816][233954] Min Reward on eval: 6730.426629107509[0m
[37m[1m[2023-07-11 01:28:32,816][233954] Mean Reward across all agents: 6730.426629107509[0m
[37m[1m[2023-07-11 01:28:32,817][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:28:37,824][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:28:37,825][233954] Reward + Measures: [[97.56665134  0.21040002  0.1874      0.15970001  0.24679999  3.35536742]
 [49.63268709  0.28790003  0.28        0.1189      0.40970001  2.9651227 ]
 [65.57319893  0.29190001  0.23380001  0.1663      0.32350001  3.24813151]
 ...
 [73.63193132  0.20920001  0.21300001  0.19579999  0.21710001  3.00251293]
 [44.82934506  0.15249999  0.09769999  0.0995      0.1339      3.39491844]
 [97.52983456  0.10420001  0.13510001  0.15140001  0.1005      3.12471986]][0m
[37m[1m[2023-07-11 01:28:37,825][233954] Max Reward on eval: 217.74229433867148[0m
[37m[1m[2023-07-11 01:28:37,825][233954] Min Reward on eval: -49.27906334893778[0m
[37m[1m[2023-07-11 01:28:37,825][233954] Mean Reward across all agents: 79.14766660713572[0m
[37m[1m[2023-07-11 01:28:37,826][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:28:37,832][233954] mean_value=50.28896300414333, max_value=643.839924390614[0m
[37m[1m[2023-07-11 01:28:37,835][233954] New mean coefficients: [[-0.3819508 -5.357105   4.315669   9.057258  -9.363033  -1.554641 ]][0m
[37m[1m[2023-07-11 01:28:37,836][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:28:46,834][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 01:28:46,834][233954] FPS: 426824.84[0m
[36m[2023-07-11 01:28:46,836][233954] itr=112, itrs=2000, Progress: 5.60%[0m
[36m[2023-07-11 01:28:58,486][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 01:28:58,486][233954] FPS: 330571.55[0m
[36m[2023-07-11 01:29:02,781][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:29:02,781][233954] Reward + Measures: [[5393.23195961    0.19918734    0.38872868    0.25465399    0.05357833
     1.66693664]][0m
[37m[1m[2023-07-11 01:29:02,781][233954] Max Reward on eval: 5393.23195961302[0m
[37m[1m[2023-07-11 01:29:02,782][233954] Min Reward on eval: 5393.23195961302[0m
[37m[1m[2023-07-11 01:29:02,782][233954] Mean Reward across all agents: 5393.23195961302[0m
[37m[1m[2023-07-11 01:29:02,782][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:29:07,932][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:29:07,933][233954] Reward + Measures: [[ 75.01176367   0.28210002   0.3626       0.2261       0.37670001
    3.32135081]
 [ 57.82673755   0.28200001   0.3136       0.17999999   0.33720002
    3.28724861]
 [ 13.66276      0.1869       0.18080001   0.1708       0.1865
    3.27580762]
 ...
 [ 70.46844591   0.18440001   0.2326       0.182        0.2352
    3.18975115]
 [ 91.73289911   0.29840001   0.39200002   0.1936       0.44029999
    3.34953666]
 [108.82532189   0.2656       0.30909997   0.18870001   0.33840001
    3.21861649]][0m
[37m[1m[2023-07-11 01:29:07,933][233954] Max Reward on eval: 233.29839650057255[0m
[37m[1m[2023-07-11 01:29:07,933][233954] Min Reward on eval: -93.86412494033576[0m
[37m[1m[2023-07-11 01:29:07,933][233954] Mean Reward across all agents: 98.73230024285466[0m
[37m[1m[2023-07-11 01:29:07,934][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:29:07,940][233954] mean_value=-2.7333155605170636, max_value=686.8935821007005[0m
[37m[1m[2023-07-11 01:29:07,943][233954] New mean coefficients: [[-0.2412675 -4.8531713  4.145317  10.0521965 -8.730106  -1.0752279]][0m
[37m[1m[2023-07-11 01:29:07,944][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:29:17,017][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 01:29:17,017][233954] FPS: 423308.97[0m
[36m[2023-07-11 01:29:17,019][233954] itr=113, itrs=2000, Progress: 5.65%[0m
[36m[2023-07-11 01:29:28,727][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 01:29:28,728][233954] FPS: 328878.08[0m
[36m[2023-07-11 01:29:32,975][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:29:32,980][233954] Reward + Measures: [[4292.72631703    0.19412634    0.39479667    0.29057866    0.083643
     1.55812168]][0m
[37m[1m[2023-07-11 01:29:32,981][233954] Max Reward on eval: 4292.726317028991[0m
[37m[1m[2023-07-11 01:29:32,981][233954] Min Reward on eval: 4292.726317028991[0m
[37m[1m[2023-07-11 01:29:32,981][233954] Mean Reward across all agents: 4292.726317028991[0m
[37m[1m[2023-07-11 01:29:32,981][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:29:37,984][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:29:37,990][233954] Reward + Measures: [[29.85228085  0.1611      0.1654      0.17740002  0.1199      3.43004107]
 [62.51362716  0.2421      0.1628      0.24320002  0.12629999  3.25862956]
 [28.47117337  0.35600001  0.22309999  0.391       0.24859999  2.91730714]
 ...
 [85.08087368  0.17940001  0.16629998  0.2106      0.15260002  3.20181632]
 [23.33106307  0.12100001  0.0862      0.0975      0.0983      3.32945037]
 [-9.92875263  0.48809996  0.11919999  0.59640002  0.38919997  2.52663326]][0m
[37m[1m[2023-07-11 01:29:37,990][233954] Max Reward on eval: 234.49924039300532[0m
[37m[1m[2023-07-11 01:29:37,991][233954] Min Reward on eval: -107.86578674577177[0m
[37m[1m[2023-07-11 01:29:37,991][233954] Mean Reward across all agents: 73.82145469355173[0m
[37m[1m[2023-07-11 01:29:37,991][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:29:37,994][233954] mean_value=-87.28405133148262, max_value=512.90334161498[0m
[37m[1m[2023-07-11 01:29:37,997][233954] New mean coefficients: [[ 0.24438825 -3.5385964   5.369957    9.386296   -6.5387945  -0.56539595]][0m
[37m[1m[2023-07-11 01:29:37,998][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:29:47,001][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 01:29:47,001][233954] FPS: 426586.76[0m
[36m[2023-07-11 01:29:47,003][233954] itr=114, itrs=2000, Progress: 5.70%[0m
[36m[2023-07-11 01:29:58,545][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 01:29:58,545][233954] FPS: 333742.13[0m
[36m[2023-07-11 01:30:02,808][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:30:02,808][233954] Reward + Measures: [[4258.69463545    0.19087833    0.403541      0.29989702    0.085206
     1.55326855]][0m
[37m[1m[2023-07-11 01:30:02,809][233954] Max Reward on eval: 4258.694635449371[0m
[37m[1m[2023-07-11 01:30:02,809][233954] Min Reward on eval: 4258.694635449371[0m
[37m[1m[2023-07-11 01:30:02,809][233954] Mean Reward across all agents: 4258.694635449371[0m
[37m[1m[2023-07-11 01:30:02,809][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:30:07,835][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:30:07,836][233954] Reward + Measures: [[185.65388295   0.16779999   0.1353       0.12550001   0.0746
    2.925349  ]
 [  1.42216475   0.1275       0.1455       0.15049998   0.13680001
    3.47902679]
 [ 56.54586616   0.19000001   0.1567       0.17960002   0.082
    3.34817314]
 ...
 [298.28341985   0.25049999   0.21159999   0.2361       0.15280001
    3.11972594]
 [105.07751083   0.15119998   0.1058       0.1441       0.09450001
    3.39453483]
 [106.81686638   0.1547       0.1701       0.1427       0.15150002
    3.12839675]][0m
[37m[1m[2023-07-11 01:30:07,836][233954] Max Reward on eval: 563.5278702326119[0m
[37m[1m[2023-07-11 01:30:07,836][233954] Min Reward on eval: -155.22126493528486[0m
[37m[1m[2023-07-11 01:30:07,837][233954] Mean Reward across all agents: 107.29588103105431[0m
[37m[1m[2023-07-11 01:30:07,837][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:30:07,841][233954] mean_value=-64.02266064675743, max_value=1032.9785259406697[0m
[37m[1m[2023-07-11 01:30:07,844][233954] New mean coefficients: [[ 0.91871464 -2.1664062   8.463209    7.1346083  -2.1335878  -0.15841955]][0m
[37m[1m[2023-07-11 01:30:07,844][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:30:16,826][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 01:30:16,827][233954] FPS: 427603.94[0m
[36m[2023-07-11 01:30:16,829][233954] itr=115, itrs=2000, Progress: 5.75%[0m
[36m[2023-07-11 01:30:28,651][233954] train() took 11.79 seconds to complete[0m
[36m[2023-07-11 01:30:28,656][233954] FPS: 325687.33[0m
[36m[2023-07-11 01:30:32,937][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:30:32,937][233954] Reward + Measures: [[5157.0921395     0.19693634    0.40226734    0.27559134    0.06102067
     1.63682306]][0m
[37m[1m[2023-07-11 01:30:32,937][233954] Max Reward on eval: 5157.092139499544[0m
[37m[1m[2023-07-11 01:30:32,938][233954] Min Reward on eval: 5157.092139499544[0m
[37m[1m[2023-07-11 01:30:32,938][233954] Mean Reward across all agents: 5157.092139499544[0m
[37m[1m[2023-07-11 01:30:32,938][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:30:37,859][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:30:37,860][233954] Reward + Measures: [[ 40.05399493   0.13429999   0.1635       0.16150001   0.17330001
    2.81828165]
 [ 51.98331616   0.31660002   0.49530002   0.46339998   0.51660007
    2.98203087]
 [-30.96816723   0.23020001   0.22500001   0.15869999   0.2086
    3.0674057 ]
 ...
 [ 17.55791119   0.2027       0.32870004   0.2924       0.36939999
    3.05891156]
 [ 25.45638174   0.10780001   0.1211       0.1165       0.1339
    2.94641757]
 [ 31.96516896   0.08720001   0.41540003   0.37200001   0.43310004
    3.08080363]][0m
[37m[1m[2023-07-11 01:30:37,860][233954] Max Reward on eval: 344.62872646674515[0m
[37m[1m[2023-07-11 01:30:37,860][233954] Min Reward on eval: -150.8418830024544[0m
[37m[1m[2023-07-11 01:30:37,861][233954] Mean Reward across all agents: 46.77961647742783[0m
[37m[1m[2023-07-11 01:30:37,861][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:30:37,866][233954] mean_value=-54.019999173992225, max_value=632.9441976889036[0m
[37m[1m[2023-07-11 01:30:37,868][233954] New mean coefficients: [[ 1.249008   -1.2529078   9.316191    6.42737    -0.17132032  0.21573654]][0m
[37m[1m[2023-07-11 01:30:37,869][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:30:46,814][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 01:30:46,814][233954] FPS: 429382.57[0m
[36m[2023-07-11 01:30:46,817][233954] itr=116, itrs=2000, Progress: 5.80%[0m
[36m[2023-07-11 01:30:58,801][233954] train() took 11.95 seconds to complete[0m
[36m[2023-07-11 01:30:58,806][233954] FPS: 321416.03[0m
[36m[2023-07-11 01:31:03,102][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:31:03,103][233954] Reward + Measures: [[6596.01215893    0.20697798    0.39926264    0.24349034    0.02873633
     1.78124595]][0m
[37m[1m[2023-07-11 01:31:03,103][233954] Max Reward on eval: 6596.012158931696[0m
[37m[1m[2023-07-11 01:31:03,103][233954] Min Reward on eval: 6596.012158931696[0m
[37m[1m[2023-07-11 01:31:03,103][233954] Mean Reward across all agents: 6596.012158931696[0m
[37m[1m[2023-07-11 01:31:03,104][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:31:08,087][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:31:08,088][233954] Reward + Measures: [[ 10.1354251    0.20439999   0.1517       0.1568       0.1078
    3.02850413]
 [  5.16349903   0.15920001   0.13129999   0.15019999   0.0918
    3.16698074]
 [  2.35784762   0.14479999   0.14350002   0.14430001   0.1102
    3.17332602]
 ...
 [-68.00264714   0.12639999   0.0947       0.0949       0.1248
    3.36474729]
 [-49.61993727   0.1006       0.0732       0.0737       0.0974
    3.28121948]
 [ 34.3592044    0.18370001   0.1565       0.1664       0.0897
    3.409724  ]][0m
[37m[1m[2023-07-11 01:31:08,088][233954] Max Reward on eval: 101.14703369950875[0m
[37m[1m[2023-07-11 01:31:08,088][233954] Min Reward on eval: -112.5448008755222[0m
[37m[1m[2023-07-11 01:31:08,088][233954] Mean Reward across all agents: 1.6997518733510224[0m
[37m[1m[2023-07-11 01:31:08,089][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:31:08,090][233954] mean_value=-241.0798511248432, max_value=444.9299569152738[0m
[37m[1m[2023-07-11 01:31:08,092][233954] New mean coefficients: [[ 1.2932669 -1.685508   9.197539   7.5327015 -1.3504341 -0.2112304]][0m
[37m[1m[2023-07-11 01:31:08,093][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:31:17,133][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 01:31:17,133][233954] FPS: 424878.91[0m
[36m[2023-07-11 01:31:17,135][233954] itr=117, itrs=2000, Progress: 5.85%[0m
[36m[2023-07-11 01:31:28,898][233954] train() took 11.73 seconds to complete[0m
[36m[2023-07-11 01:31:28,898][233954] FPS: 327336.18[0m
[36m[2023-07-11 01:31:33,221][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:31:33,221][233954] Reward + Measures: [[7257.2097955     0.20732167    0.39450234    0.22989734    0.016487
     1.84288609]][0m
[37m[1m[2023-07-11 01:31:33,221][233954] Max Reward on eval: 7257.209795499166[0m
[37m[1m[2023-07-11 01:31:33,222][233954] Min Reward on eval: 7257.209795499166[0m
[37m[1m[2023-07-11 01:31:33,222][233954] Mean Reward across all agents: 7257.209795499166[0m
[37m[1m[2023-07-11 01:31:33,222][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:31:38,410][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:31:38,410][233954] Reward + Measures: [[-163.73843052    0.76099998    0.1033        0.86420006    0.30469999
     2.85670829]
 [-130.98239901    0.79520005    0.0885        0.87290001    0.3436
     2.80529523]
 [-289.1839668     0.52590007    0.44890004    0.56150001    0.0861
     3.50882006]
 ...
 [-121.60819626    0.73880005    0.0416        0.92509997    0.33910003
     3.43355536]
 [ -97.2954438     0.7651        0.0654        0.84699994    0.3996
     3.30286646]
 [ -41.14543697    0.83220005    0.18580002    0.82770008    0.46030003
     3.08508611]][0m
[37m[1m[2023-07-11 01:31:38,411][233954] Max Reward on eval: 157.97133925091475[0m
[37m[1m[2023-07-11 01:31:38,411][233954] Min Reward on eval: -331.4382367298007[0m
[37m[1m[2023-07-11 01:31:38,411][233954] Mean Reward across all agents: -60.196879528553815[0m
[37m[1m[2023-07-11 01:31:38,411][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:31:38,430][233954] mean_value=256.6327866019053, max_value=657.9713392509148[0m
[37m[1m[2023-07-11 01:31:38,433][233954] New mean coefficients: [[ 1.2123971  -2.0100093   8.555993    8.990611   -3.0158157  -0.27989715]][0m
[37m[1m[2023-07-11 01:31:38,434][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:31:47,558][233954] train() took 9.12 seconds to complete[0m
[36m[2023-07-11 01:31:47,558][233954] FPS: 420946.84[0m
[36m[2023-07-11 01:31:47,560][233954] itr=118, itrs=2000, Progress: 5.90%[0m
[36m[2023-07-11 01:31:59,250][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 01:31:59,250][233954] FPS: 329413.44[0m
[36m[2023-07-11 01:32:03,576][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:32:03,577][233954] Reward + Measures: [[7616.08976819    0.20298967    0.37949467    0.22135       0.01256133
     1.86766636]][0m
[37m[1m[2023-07-11 01:32:03,577][233954] Max Reward on eval: 7616.0897681870365[0m
[37m[1m[2023-07-11 01:32:03,577][233954] Min Reward on eval: 7616.0897681870365[0m
[37m[1m[2023-07-11 01:32:03,577][233954] Mean Reward across all agents: 7616.0897681870365[0m
[37m[1m[2023-07-11 01:32:03,578][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:32:08,670][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:32:08,670][233954] Reward + Measures: [[ 79.81728468   0.20290001   0.2225       0.21460001   0.23099999
    2.74290442]
 [-16.95112392   0.2895       0.23559999   0.21540001   0.19319999
    2.92482114]
 [-66.67788885   0.3477       0.30380002   0.24429999   0.24180003
    3.14313126]
 ...
 [ 12.90604131   0.257        0.264        0.26120001   0.28979999
    2.79398775]
 [-16.07718849   0.3026       0.31260002   0.2027       0.23550001
    3.17301178]
 [ 30.5850411    0.26340005   0.25409999   0.27260002   0.2545
    2.75595665]][0m
[37m[1m[2023-07-11 01:32:08,671][233954] Max Reward on eval: 245.90401080222802[0m
[37m[1m[2023-07-11 01:32:08,671][233954] Min Reward on eval: -194.61926725776866[0m
[37m[1m[2023-07-11 01:32:08,671][233954] Mean Reward across all agents: 18.884051070201416[0m
[37m[1m[2023-07-11 01:32:08,671][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:32:08,675][233954] mean_value=-158.55228848138393, max_value=478.6374439901346[0m
[37m[1m[2023-07-11 01:32:08,678][233954] New mean coefficients: [[ 0.8617759  -1.5974228   7.2773623   9.553648   -4.870716   -0.35269877]][0m
[37m[1m[2023-07-11 01:32:08,679][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:32:17,760][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 01:32:17,760][233954] FPS: 422924.72[0m
[36m[2023-07-11 01:32:17,762][233954] itr=119, itrs=2000, Progress: 5.95%[0m
[36m[2023-07-11 01:32:29,356][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 01:32:29,356][233954] FPS: 332145.78[0m
[36m[2023-07-11 01:32:33,641][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:32:33,642][233954] Reward + Measures: [[7729.6066955     0.20194       0.37722567    0.21793933    0.01051667
     1.88431346]][0m
[37m[1m[2023-07-11 01:32:33,642][233954] Max Reward on eval: 7729.606695500892[0m
[37m[1m[2023-07-11 01:32:33,642][233954] Min Reward on eval: 7729.606695500892[0m
[37m[1m[2023-07-11 01:32:33,642][233954] Mean Reward across all agents: 7729.606695500892[0m
[37m[1m[2023-07-11 01:32:33,643][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:32:38,654][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:32:38,654][233954] Reward + Measures: [[-16.48693414   0.20850001   0.35999998   0.26710001   0.32590002
    2.69117045]
 [ 10.80384269   0.108        0.25550005   0.16580001   0.21900001
    3.2662015 ]
 [ 25.09638831   0.21949999   0.27910003   0.2393       0.18780001
    2.92290592]
 ...
 [ 48.48811572   0.15100001   0.65439999   0.4316       0.63340002
    2.20860171]
 [ 50.1950903    0.0979       0.2234       0.13160001   0.1715
    3.12206316]
 [ 97.06326705   0.3132       0.41940004   0.24700001   0.38100001
    2.92387104]][0m
[37m[1m[2023-07-11 01:32:38,655][233954] Max Reward on eval: 172.55080658637453[0m
[37m[1m[2023-07-11 01:32:38,655][233954] Min Reward on eval: -264.64958113245666[0m
[37m[1m[2023-07-11 01:32:38,655][233954] Mean Reward across all agents: -14.108826159133562[0m
[37m[1m[2023-07-11 01:32:38,655][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:32:38,659][233954] mean_value=-167.3419912163754, max_value=503.2088054694235[0m
[37m[1m[2023-07-11 01:32:38,661][233954] New mean coefficients: [[ 1.1213266  -2.141171    8.174476    7.493985   -3.5974894  -0.03554234]][0m
[37m[1m[2023-07-11 01:32:38,662][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:32:47,696][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 01:32:47,696][233954] FPS: 425151.24[0m
[36m[2023-07-11 01:32:47,698][233954] itr=120, itrs=2000, Progress: 6.00%[0m
[37m[1m[2023-07-11 01:35:09,344][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000100[0m
[36m[2023-07-11 01:35:21,646][233954] train() took 11.75 seconds to complete[0m
[36m[2023-07-11 01:35:21,647][233954] FPS: 326889.42[0m
[36m[2023-07-11 01:35:25,870][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:35:25,870][233954] Reward + Measures: [[7862.85586273    0.20179667    0.36858264    0.21514232    0.008835
     1.88951468]][0m
[37m[1m[2023-07-11 01:35:25,870][233954] Max Reward on eval: 7862.85586273353[0m
[37m[1m[2023-07-11 01:35:25,870][233954] Min Reward on eval: 7862.85586273353[0m
[37m[1m[2023-07-11 01:35:25,871][233954] Mean Reward across all agents: 7862.85586273353[0m
[37m[1m[2023-07-11 01:35:25,871][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:35:30,844][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:35:30,845][233954] Reward + Measures: [[ 16.68015972   0.17670001   0.4131       0.32320002   0.34690002
    3.21639132]
 [ 15.46118154   0.27900001   0.25170001   0.25240001   0.12029999
    2.92333531]
 [-11.20396198   0.15940002   0.1559       0.1153       0.1135
    3.46917129]
 ...
 [ 25.69660747   0.44889998   0.41919994   0.40889999   0.12959999
    3.15902448]
 [ 22.27494458   0.22879998   0.23190001   0.16379999   0.1524
    3.31643105]
 [ -5.42976564   0.36199999   0.33179998   0.36859998   0.29370001
    2.85102057]][0m
[37m[1m[2023-07-11 01:35:30,845][233954] Max Reward on eval: 195.894468870759[0m
[37m[1m[2023-07-11 01:35:30,845][233954] Min Reward on eval: -226.77771095293573[0m
[37m[1m[2023-07-11 01:35:30,845][233954] Mean Reward across all agents: 8.504462262126598[0m
[37m[1m[2023-07-11 01:35:30,846][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:35:30,849][233954] mean_value=-134.42441267198052, max_value=584.6274845483713[0m
[37m[1m[2023-07-11 01:35:30,852][233954] New mean coefficients: [[ 1.3547449  -1.8710566   8.828044    6.803302   -2.35801     0.01048463]][0m
[37m[1m[2023-07-11 01:35:30,853][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:35:39,735][233954] train() took 8.88 seconds to complete[0m
[36m[2023-07-11 01:35:39,735][233954] FPS: 432401.70[0m
[36m[2023-07-11 01:35:39,737][233954] itr=121, itrs=2000, Progress: 6.05%[0m
[36m[2023-07-11 01:35:51,425][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 01:35:51,426][233954] FPS: 329455.31[0m
[36m[2023-07-11 01:35:55,674][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:35:55,674][233954] Reward + Measures: [[7933.77444908    0.20059668    0.36660165    0.21272701    0.00836767
     1.89137471]][0m
[37m[1m[2023-07-11 01:35:55,675][233954] Max Reward on eval: 7933.774449081154[0m
[37m[1m[2023-07-11 01:35:55,675][233954] Min Reward on eval: 7933.774449081154[0m
[37m[1m[2023-07-11 01:35:55,675][233954] Mean Reward across all agents: 7933.774449081154[0m
[37m[1m[2023-07-11 01:35:55,676][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:36:00,764][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:36:00,765][233954] Reward + Measures: [[ 67.15676575   0.0639       0.36840001   0.24270001   0.34650001
    3.14928818]
 [582.79106281   0.0172       0.86149997   0.80219996   0.85289997
    3.55840802]
 [ 15.43243272   0.31590003   0.3504       0.3125       0.22670002
    3.16543508]
 ...
 [145.85852718   0.1583       0.39859998   0.21689999   0.39820001
    2.73072124]
 [233.75019273   0.0609       0.60250002   0.42290002   0.61879998
    3.20168304]
 [ 61.4627526    0.13700001   0.2273       0.09850001   0.16060001
    3.14394927]][0m
[37m[1m[2023-07-11 01:36:00,765][233954] Max Reward on eval: 582.7910628104582[0m
[37m[1m[2023-07-11 01:36:00,765][233954] Min Reward on eval: -145.42697048969566[0m
[37m[1m[2023-07-11 01:36:00,766][233954] Mean Reward across all agents: 123.98209356870049[0m
[37m[1m[2023-07-11 01:36:00,766][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:36:00,774][233954] mean_value=36.54815005621521, max_value=966.0246964697944[0m
[37m[1m[2023-07-11 01:36:00,776][233954] New mean coefficients: [[ 1.3410933 -1.8065358  9.381072   6.3832192 -1.617964   0.0732427]][0m
[37m[1m[2023-07-11 01:36:00,777][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:36:09,848][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 01:36:09,848][233954] FPS: 423415.21[0m
[36m[2023-07-11 01:36:09,851][233954] itr=122, itrs=2000, Progress: 6.10%[0m
[36m[2023-07-11 01:36:21,503][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 01:36:21,503][233954] FPS: 330542.90[0m
[36m[2023-07-11 01:36:25,791][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:36:25,791][233954] Reward + Measures: [[4615.03468262    0.17817333    0.29141867    0.22302599    0.05794733
     1.62288654]][0m
[37m[1m[2023-07-11 01:36:25,792][233954] Max Reward on eval: 4615.0346826177765[0m
[37m[1m[2023-07-11 01:36:25,792][233954] Min Reward on eval: 4615.0346826177765[0m
[37m[1m[2023-07-11 01:36:25,792][233954] Mean Reward across all agents: 4615.0346826177765[0m
[37m[1m[2023-07-11 01:36:25,792][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:36:30,832][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:36:30,833][233954] Reward + Measures: [[264.9572452    0.1373       0.3757       0.29590002   0.33290002
    2.79493213]
 [170.00495199   0.12639999   0.1858       0.13600001   0.16970001
    3.21911979]
 [-62.51234378   0.48590001   0.0834       0.44350004   0.39809999
    3.19957805]
 ...
 [ 84.03473849   0.1441       0.1329       0.1234       0.16929999
    3.59120917]
 [-55.62420226   0.0597       0.5837       0.51990002   0.57170004
    3.17163134]
 [ 72.47706501   0.071        0.29669997   0.27950001   0.31490001
    2.77171016]][0m
[37m[1m[2023-07-11 01:36:30,833][233954] Max Reward on eval: 349.5849036980653[0m
[37m[1m[2023-07-11 01:36:30,833][233954] Min Reward on eval: -408.0502414897084[0m
[37m[1m[2023-07-11 01:36:30,834][233954] Mean Reward across all agents: 113.03766812250467[0m
[37m[1m[2023-07-11 01:36:30,834][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:36:30,845][233954] mean_value=70.01303198498468, max_value=821.2404283337696[0m
[37m[1m[2023-07-11 01:36:30,848][233954] New mean coefficients: [[ 1.7648606  -1.4791055  11.77442     3.0927353   1.7530768   0.28600818]][0m
[37m[1m[2023-07-11 01:36:30,848][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:36:39,916][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 01:36:39,916][233954] FPS: 423568.16[0m
[36m[2023-07-11 01:36:39,919][233954] itr=123, itrs=2000, Progress: 6.15%[0m
[36m[2023-07-11 01:36:51,551][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 01:36:51,552][233954] FPS: 331101.87[0m
[36m[2023-07-11 01:36:55,833][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:36:55,833][233954] Reward + Measures: [[5482.39096627    0.17885134    0.27957934    0.20665167    0.04061767
     1.69418001]][0m
[37m[1m[2023-07-11 01:36:55,833][233954] Max Reward on eval: 5482.390966269965[0m
[37m[1m[2023-07-11 01:36:55,834][233954] Min Reward on eval: 5482.390966269965[0m
[37m[1m[2023-07-11 01:36:55,834][233954] Mean Reward across all agents: 5482.390966269965[0m
[37m[1m[2023-07-11 01:36:55,834][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:37:00,969][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:37:00,970][233954] Reward + Measures: [[242.40166694   0.22059999   0.40540001   0.36940002   0.30109999
    3.10771537]
 [ 89.60229772   0.285        0.30829999   0.35549998   0.31669998
    3.09153605]
 [ 59.40673573   0.23559999   0.1983       0.1904       0.20700002
    3.24801111]
 ...
 [  5.40572448   0.61009997   0.60219997   0.64170003   0.58700001
    3.20428443]
 [ 16.17909454   0.38800004   0.3901       0.40099999   0.39050001
    3.20002437]
 [117.42478465   0.1103       0.27630001   0.19369999   0.28290001
    3.52140474]][0m
[37m[1m[2023-07-11 01:37:00,970][233954] Max Reward on eval: 373.083494184725[0m
[37m[1m[2023-07-11 01:37:00,970][233954] Min Reward on eval: -50.05120461769402[0m
[37m[1m[2023-07-11 01:37:00,971][233954] Mean Reward across all agents: 130.0210886265695[0m
[37m[1m[2023-07-11 01:37:00,971][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:37:00,981][233954] mean_value=77.3535368085897, max_value=824.2122020748444[0m
[37m[1m[2023-07-11 01:37:00,984][233954] New mean coefficients: [[ 1.4437284  -2.2344255  10.082662    5.487069   -1.0815403  -0.40817624]][0m
[37m[1m[2023-07-11 01:37:00,985][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:37:10,016][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 01:37:10,016][233954] FPS: 425283.70[0m
[36m[2023-07-11 01:37:10,018][233954] itr=124, itrs=2000, Progress: 6.20%[0m
[36m[2023-07-11 01:37:21,726][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 01:37:21,727][233954] FPS: 328902.40[0m
[36m[2023-07-11 01:37:25,989][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:37:25,989][233954] Reward + Measures: [[6562.82785692    0.18609899    0.28841665    0.20299301    0.024353
     1.77508461]][0m
[37m[1m[2023-07-11 01:37:25,990][233954] Max Reward on eval: 6562.827856924894[0m
[37m[1m[2023-07-11 01:37:25,990][233954] Min Reward on eval: 6562.827856924894[0m
[37m[1m[2023-07-11 01:37:25,990][233954] Mean Reward across all agents: 6562.827856924894[0m
[37m[1m[2023-07-11 01:37:25,990][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:37:31,007][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:37:31,012][233954] Reward + Measures: [[ -1.47454483   0.16030002   0.1454       0.19500001   0.14740001
    3.28173232]
 [  1.80484915   0.1743       0.15280001   0.2198       0.1495
    3.07624483]
 [ 71.67010027   0.19400001   0.26120001   0.17919999   0.20739999
    3.4427011 ]
 ...
 [ 45.26684125   0.21080001   0.27139997   0.1973       0.23029999
    3.2192719 ]
 [113.41018548   0.1741       0.18290001   0.18910001   0.163
    2.92224741]
 [214.20279714   0.21370001   0.25340003   0.2299       0.1885
    2.79504848]][0m
[37m[1m[2023-07-11 01:37:31,013][233954] Max Reward on eval: 298.0225663478486[0m
[37m[1m[2023-07-11 01:37:31,013][233954] Min Reward on eval: -72.84435959453694[0m
[37m[1m[2023-07-11 01:37:31,013][233954] Mean Reward across all agents: 68.7610436296278[0m
[37m[1m[2023-07-11 01:37:31,013][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:37:31,015][233954] mean_value=-162.00291720301226, max_value=473.19920667782424[0m
[37m[1m[2023-07-11 01:37:31,018][233954] New mean coefficients: [[ 1.2135491 -1.6496547  8.952227   7.045473  -2.2816253 -0.0361498]][0m
[37m[1m[2023-07-11 01:37:31,019][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:37:40,036][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 01:37:40,036][233954] FPS: 425947.03[0m
[36m[2023-07-11 01:37:40,038][233954] itr=125, itrs=2000, Progress: 6.25%[0m
[36m[2023-07-11 01:37:51,900][233954] train() took 11.83 seconds to complete[0m
[36m[2023-07-11 01:37:51,900][233954] FPS: 324638.95[0m
[36m[2023-07-11 01:37:56,241][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:37:56,241][233954] Reward + Measures: [[7220.50431805    0.18972367    0.29361767    0.20211999    0.01466733
     1.83160996]][0m
[37m[1m[2023-07-11 01:37:56,241][233954] Max Reward on eval: 7220.50431804876[0m
[37m[1m[2023-07-11 01:37:56,242][233954] Min Reward on eval: 7220.50431804876[0m
[37m[1m[2023-07-11 01:37:56,242][233954] Mean Reward across all agents: 7220.50431804876[0m
[37m[1m[2023-07-11 01:37:56,242][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:38:01,267][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:38:01,267][233954] Reward + Measures: [[ -4.60620675   0.0683       0.27379999   0.22239999   0.28940001
    3.17605543]
 [338.43462733   0.18010001   0.24609999   0.31609997   0.30039999
    2.3672092 ]
 [ 51.99396762   0.12360001   0.34540001   0.24039999   0.2595
    3.07228398]
 ...
 [ 13.54372584   0.08220001   0.2208       0.1596       0.1754
    3.29648638]
 [127.22961952   0.18210001   0.2502       0.2624       0.28800002
    2.88973522]
 [112.48337204   0.24489999   0.32539999   0.31659999   0.29529998
    3.02652335]][0m
[37m[1m[2023-07-11 01:38:01,267][233954] Max Reward on eval: 497.97036746153606[0m
[37m[1m[2023-07-11 01:38:01,268][233954] Min Reward on eval: -552.8116512276232[0m
[37m[1m[2023-07-11 01:38:01,268][233954] Mean Reward across all agents: 66.06925336573357[0m
[37m[1m[2023-07-11 01:38:01,268][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:38:01,275][233954] mean_value=-62.85334898295633, max_value=791.1221159885912[0m
[37m[1m[2023-07-11 01:38:01,277][233954] New mean coefficients: [[ 1.7013552  -1.1758283  11.555198    3.594574    1.4395852   0.00677277]][0m
[37m[1m[2023-07-11 01:38:01,278][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:38:10,330][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 01:38:10,330][233954] FPS: 424307.02[0m
[36m[2023-07-11 01:38:10,332][233954] itr=126, itrs=2000, Progress: 6.30%[0m
[36m[2023-07-11 01:38:22,122][233954] train() took 11.76 seconds to complete[0m
[36m[2023-07-11 01:38:22,123][233954] FPS: 326623.09[0m
[36m[2023-07-11 01:38:26,438][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:38:26,439][233954] Reward + Measures: [[7580.23572866    0.19095467    0.30282       0.20213799    0.01124633
     1.86123049]][0m
[37m[1m[2023-07-11 01:38:26,439][233954] Max Reward on eval: 7580.235728664848[0m
[37m[1m[2023-07-11 01:38:26,439][233954] Min Reward on eval: 7580.235728664848[0m
[37m[1m[2023-07-11 01:38:26,440][233954] Mean Reward across all agents: 7580.235728664848[0m
[37m[1m[2023-07-11 01:38:26,440][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:38:31,499][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:38:31,500][233954] Reward + Measures: [[ 74.47248624   0.0838       0.39209998   0.19719999   0.35059997
    3.49097323]
 [133.44055936   0.13820003   0.48189998   0.2617       0.4684
    3.11413431]
 [ 16.73868912   0.0869       0.0814       0.09370001   0.0707
    3.55211449]
 ...
 [-26.71326647   0.17569999   0.212        0.13870001   0.22259998
    3.2090385 ]
 [ 20.44577655   0.15460001   0.3263       0.1955       0.34870002
    3.17924666]
 [ 39.38014106   0.0891       0.28770003   0.13209999   0.26280001
    3.45159411]][0m
[37m[1m[2023-07-11 01:38:31,500][233954] Max Reward on eval: 258.7073245262727[0m
[37m[1m[2023-07-11 01:38:31,500][233954] Min Reward on eval: -98.62017230885104[0m
[37m[1m[2023-07-11 01:38:31,500][233954] Mean Reward across all agents: 56.942168609226705[0m
[37m[1m[2023-07-11 01:38:31,501][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:38:31,504][233954] mean_value=-66.42541123923003, max_value=540.6877958084457[0m
[37m[1m[2023-07-11 01:38:31,507][233954] New mean coefficients: [[ 1.8588263  -0.52461034 12.918822    2.6162608   2.912767   -0.39201665]][0m
[37m[1m[2023-07-11 01:38:31,508][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:38:40,518][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 01:38:40,518][233954] FPS: 426296.48[0m
[36m[2023-07-11 01:38:40,520][233954] itr=127, itrs=2000, Progress: 6.35%[0m
[36m[2023-07-11 01:38:52,150][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 01:38:52,151][233954] FPS: 331218.48[0m
[36m[2023-07-11 01:38:56,478][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:38:56,479][233954] Reward + Measures: [[7816.50714274    0.191718      0.31587934    0.20058833    0.008492
     1.87572849]][0m
[37m[1m[2023-07-11 01:38:56,479][233954] Max Reward on eval: 7816.507142739686[0m
[37m[1m[2023-07-11 01:38:56,479][233954] Min Reward on eval: 7816.507142739686[0m
[37m[1m[2023-07-11 01:38:56,479][233954] Mean Reward across all agents: 7816.507142739686[0m
[37m[1m[2023-07-11 01:38:56,480][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:39:01,684][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:39:01,685][233954] Reward + Measures: [[95.94471479  0.0896      0.0887      0.07910001  0.0949      3.27948236]
 [21.02316955  0.11720001  0.0869      0.09710001  0.12800001  3.42308927]
 [82.8980127   0.0815      0.10539999  0.0983      0.09530001  3.23614311]
 ...
 [-8.27260396  0.0993      0.07919999  0.0972      0.1104      3.22298288]
 [58.98524147  0.17470001  0.13920002  0.1719      0.08350001  3.19112515]
 [25.11972641  0.08270001  0.083       0.0665      0.07000001  3.33824801]][0m
[37m[1m[2023-07-11 01:39:01,685][233954] Max Reward on eval: 224.3248623870546[0m
[37m[1m[2023-07-11 01:39:01,685][233954] Min Reward on eval: -89.44009807612747[0m
[37m[1m[2023-07-11 01:39:01,685][233954] Mean Reward across all agents: 30.735925899854145[0m
[37m[1m[2023-07-11 01:39:01,686][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:39:01,687][233954] mean_value=-192.3245578276094, max_value=353.21733852224895[0m
[37m[1m[2023-07-11 01:39:01,690][233954] New mean coefficients: [[ 1.5151345  -0.87822634 10.869924    4.628536   -0.06655264 -1.1020958 ]][0m
[37m[1m[2023-07-11 01:39:01,691][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:39:10,751][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 01:39:10,751][233954] FPS: 423897.37[0m
[36m[2023-07-11 01:39:10,754][233954] itr=128, itrs=2000, Progress: 6.40%[0m
[36m[2023-07-11 01:39:22,480][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 01:39:22,481][233954] FPS: 328411.61[0m
[36m[2023-07-11 01:39:26,819][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:39:26,819][233954] Reward + Measures: [[7912.59347403    0.19237433    0.32672799    0.20051168    0.007637
     1.87145698]][0m
[37m[1m[2023-07-11 01:39:26,819][233954] Max Reward on eval: 7912.593474027226[0m
[37m[1m[2023-07-11 01:39:26,820][233954] Min Reward on eval: 7912.593474027226[0m
[37m[1m[2023-07-11 01:39:26,820][233954] Mean Reward across all agents: 7912.593474027226[0m
[37m[1m[2023-07-11 01:39:26,820][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:39:31,880][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:39:31,881][233954] Reward + Measures: [[126.59714049   0.22300003   0.34870002   0.22640002   0.3125
    3.05312967]
 [ 46.35911941   0.27850002   0.36400002   0.1559       0.30450001
    2.94291472]
 [146.55331798   0.24070001   0.34759998   0.22420001   0.30920002
    3.10823822]
 ...
 [  2.66844532   0.24609999   0.32440001   0.15009999   0.27780002
    3.1680932 ]
 [140.55306848   0.26199999   0.36160001   0.2703       0.32699999
    3.07403421]
 [ -1.80661584   0.1671       0.30130002   0.15939999   0.23940001
    3.1971612 ]][0m
[37m[1m[2023-07-11 01:39:31,881][233954] Max Reward on eval: 492.66733833323235[0m
[37m[1m[2023-07-11 01:39:31,881][233954] Min Reward on eval: -272.7226447869092[0m
[37m[1m[2023-07-11 01:39:31,881][233954] Mean Reward across all agents: 38.88942246786759[0m
[37m[1m[2023-07-11 01:39:31,882][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:39:31,886][233954] mean_value=-99.13791628066846, max_value=657.4628081427421[0m
[37m[1m[2023-07-11 01:39:31,889][233954] New mean coefficients: [[ 1.566776   -0.06564808 10.759748    5.0558853   0.6588024  -0.8155045 ]][0m
[37m[1m[2023-07-11 01:39:31,890][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:39:40,958][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 01:39:40,958][233954] FPS: 423546.92[0m
[36m[2023-07-11 01:39:40,960][233954] itr=129, itrs=2000, Progress: 6.45%[0m
[36m[2023-07-11 01:39:52,572][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 01:39:52,572][233954] FPS: 331655.97[0m
[36m[2023-07-11 01:39:56,823][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:39:56,824][233954] Reward + Measures: [[7517.91433776    0.18737799    0.29595599    0.19238733    0.00887467
     1.82902312]][0m
[37m[1m[2023-07-11 01:39:56,824][233954] Max Reward on eval: 7517.9143377599985[0m
[37m[1m[2023-07-11 01:39:56,824][233954] Min Reward on eval: 7517.9143377599985[0m
[37m[1m[2023-07-11 01:39:56,824][233954] Mean Reward across all agents: 7517.9143377599985[0m
[37m[1m[2023-07-11 01:39:56,824][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:40:01,807][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:40:01,808][233954] Reward + Measures: [[ 17.30874754   0.51100004   0.44899997   0.46960002   0.43309999
    3.20223403]
 [-26.39648538   0.1452       0.27080002   0.1494       0.2458
    3.11184311]
 [134.46999058   0.26089999   0.18440001   0.3328       0.18069999
    2.50078368]
 ...
 [-60.72085044   0.1957       0.35230002   0.2264       0.36570001
    3.01048326]
 [-28.26878471   0.25760001   0.2299       0.2502       0.1997
    2.74666905]
 [-15.13291018   0.1947       0.2494       0.2246       0.23420003
    2.87538981]][0m
[37m[1m[2023-07-11 01:40:01,808][233954] Max Reward on eval: 254.92933020568452[0m
[37m[1m[2023-07-11 01:40:01,808][233954] Min Reward on eval: -525.1259017866105[0m
[37m[1m[2023-07-11 01:40:01,809][233954] Mean Reward across all agents: -34.43428769971636[0m
[37m[1m[2023-07-11 01:40:01,809][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:40:01,812][233954] mean_value=-201.33360611280094, max_value=677.7366747695953[0m
[37m[1m[2023-07-11 01:40:01,815][233954] New mean coefficients: [[ 0.7665584 -1.2129757  7.155801   8.603893  -4.3982787 -1.4406466]][0m
[37m[1m[2023-07-11 01:40:01,816][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:40:10,763][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 01:40:10,763][233954] FPS: 429258.84[0m
[36m[2023-07-11 01:40:10,766][233954] itr=130, itrs=2000, Progress: 6.50%[0m
[37m[1m[2023-07-11 01:42:27,850][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000110[0m
[36m[2023-07-11 01:42:40,066][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 01:42:40,066][233954] FPS: 331160.14[0m
[36m[2023-07-11 01:42:44,366][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:42:44,371][233954] Reward + Measures: [[7698.73404862    0.190699      0.31329134    0.19740799    0.007252
     1.83445847]][0m
[37m[1m[2023-07-11 01:42:44,371][233954] Max Reward on eval: 7698.7340486165685[0m
[37m[1m[2023-07-11 01:42:44,372][233954] Min Reward on eval: 7698.7340486165685[0m
[37m[1m[2023-07-11 01:42:44,372][233954] Mean Reward across all agents: 7698.7340486165685[0m
[37m[1m[2023-07-11 01:42:44,372][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:42:49,396][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:42:49,428][233954] Reward + Measures: [[  98.47908588    0.2845        0.42729998    0.479         0.35050002
     2.89145303]
 [-174.92489666    0.56170005    0.47800002    0.58489996    0.0657
     3.08451366]
 [   8.79338229    0.1234        0.15020001    0.17620002    0.13410001
     3.37627959]
 ...
 [   8.96356427    0.1938        0.17639999    0.21230002    0.1284
     3.14575863]
 [ 111.82977817    0.1033        0.14750002    0.15390001    0.1279
     3.25396919]
 [ -84.10041998    0.64799994    0.59320003    0.65369999    0.0556
     3.06388044]][0m
[37m[1m[2023-07-11 01:42:49,428][233954] Max Reward on eval: 285.87354278899727[0m
[37m[1m[2023-07-11 01:42:49,429][233954] Min Reward on eval: -174.92489665597677[0m
[37m[1m[2023-07-11 01:42:49,429][233954] Mean Reward across all agents: 71.70939377208104[0m
[37m[1m[2023-07-11 01:42:49,429][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:42:49,434][233954] mean_value=-86.31213927953902, max_value=598.4790858779452[0m
[37m[1m[2023-07-11 01:42:49,437][233954] New mean coefficients: [[ 1.0715781  -0.43122566  9.84712     4.811716    0.30351353 -0.56062365]][0m
[37m[1m[2023-07-11 01:42:49,438][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:42:58,538][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 01:42:58,538][233954] FPS: 422044.87[0m
[36m[2023-07-11 01:42:58,540][233954] itr=131, itrs=2000, Progress: 6.55%[0m
[36m[2023-07-11 01:43:10,213][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 01:43:10,213][233954] FPS: 330021.61[0m
[36m[2023-07-11 01:43:14,459][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:43:14,465][233954] Reward + Measures: [[7685.69256342    0.191725      0.323636      0.19709867    0.00738867
     1.83272755]][0m
[37m[1m[2023-07-11 01:43:14,465][233954] Max Reward on eval: 7685.692563420959[0m
[37m[1m[2023-07-11 01:43:14,466][233954] Min Reward on eval: 7685.692563420959[0m
[37m[1m[2023-07-11 01:43:14,466][233954] Mean Reward across all agents: 7685.692563420959[0m
[37m[1m[2023-07-11 01:43:14,466][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:43:19,422][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:43:19,423][233954] Reward + Measures: [[  7.68623223   0.1432       0.12720001   0.1085       0.1063
    3.18797183]
 [ 41.69385412   0.31549999   0.4183       0.18959999   0.22880001
    2.80267572]
 [ 89.45574947   0.149        0.14040001   0.08310001   0.15700001
    3.3516438 ]
 ...
 [ 18.23625549   0.1251       0.12060001   0.1128       0.1301
    3.36144567]
 [  8.73439126   0.1249       0.22670002   0.1314       0.21130002
    3.49882174]
 [106.69723109   0.1543       0.1376       0.10030001   0.1362
    3.16976905]][0m
[37m[1m[2023-07-11 01:43:19,423][233954] Max Reward on eval: 395.07943148315246[0m
[37m[1m[2023-07-11 01:43:19,423][233954] Min Reward on eval: -108.49004461169243[0m
[37m[1m[2023-07-11 01:43:19,423][233954] Mean Reward across all agents: 45.07981206363034[0m
[37m[1m[2023-07-11 01:43:19,424][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:43:19,427][233954] mean_value=-116.9799266743314, max_value=649.8929168594442[0m
[37m[1m[2023-07-11 01:43:19,430][233954] New mean coefficients: [[ 0.93279004 -0.29921043  9.414607    5.833617   -0.10039958 -0.8119018 ]][0m
[37m[1m[2023-07-11 01:43:19,431][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:43:28,406][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 01:43:28,406][233954] FPS: 427907.74[0m
[36m[2023-07-11 01:43:28,409][233954] itr=132, itrs=2000, Progress: 6.60%[0m
[36m[2023-07-11 01:43:40,185][233954] train() took 11.74 seconds to complete[0m
[36m[2023-07-11 01:43:40,186][233954] FPS: 326990.89[0m
[36m[2023-07-11 01:43:44,494][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:43:44,500][233954] Reward + Measures: [[7863.61345786    0.196091      0.34166634    0.20264497    0.005863
     1.84333038]][0m
[37m[1m[2023-07-11 01:43:44,500][233954] Max Reward on eval: 7863.613457863274[0m
[37m[1m[2023-07-11 01:43:44,500][233954] Min Reward on eval: 7863.613457863274[0m
[37m[1m[2023-07-11 01:43:44,500][233954] Mean Reward across all agents: 7863.613457863274[0m
[37m[1m[2023-07-11 01:43:44,501][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:43:49,517][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:43:49,523][233954] Reward + Measures: [[-138.75026376    0.48650002    0.55520004    0.52360004    0.465
     3.20368052]
 [ -65.64535893    0.27580002    0.21070002    0.30680004    0.1628
     3.11577559]
 [ 118.6195948     0.28490001    0.19410001    0.33489999    0.22650002
     3.14627266]
 ...
 [  85.33678298    0.16860001    0.13500001    0.15430002    0.15820001
     3.38189769]
 [ 119.38163946    0.1204        0.2043        0.13170001    0.14229999
     3.03595519]
 [  68.00024717    0.18089999    0.27599999    0.17839999    0.25400001
     3.18199706]][0m
[37m[1m[2023-07-11 01:43:49,523][233954] Max Reward on eval: 219.31649302393197[0m
[37m[1m[2023-07-11 01:43:49,523][233954] Min Reward on eval: -483.50875757932664[0m
[37m[1m[2023-07-11 01:43:49,524][233954] Mean Reward across all agents: 54.06679125014775[0m
[37m[1m[2023-07-11 01:43:49,524][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:43:49,529][233954] mean_value=-47.68301604793971, max_value=542.7587876402958[0m
[37m[1m[2023-07-11 01:43:49,532][233954] New mean coefficients: [[ 1.554811    0.35951465 12.933441    1.4426937   4.9053445  -0.03941381]][0m
[37m[1m[2023-07-11 01:43:49,533][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:43:58,608][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 01:43:58,608][233954] FPS: 423209.06[0m
[36m[2023-07-11 01:43:58,611][233954] itr=133, itrs=2000, Progress: 6.65%[0m
[36m[2023-07-11 01:44:10,272][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 01:44:10,273][233954] FPS: 330369.00[0m
[36m[2023-07-11 01:44:14,513][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:44:14,519][233954] Reward + Measures: [[7862.76425455    0.19435233    0.337697      0.20132133    0.00653067
     1.83728158]][0m
[37m[1m[2023-07-11 01:44:14,519][233954] Max Reward on eval: 7862.76425454704[0m
[37m[1m[2023-07-11 01:44:14,519][233954] Min Reward on eval: 7862.76425454704[0m
[37m[1m[2023-07-11 01:44:14,520][233954] Mean Reward across all agents: 7862.76425454704[0m
[37m[1m[2023-07-11 01:44:14,520][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:44:19,531][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:44:19,537][233954] Reward + Measures: [[-33.97016247   0.0823       0.18820001   0.1452       0.28779998
    3.30515742]
 [ -2.21279649   0.08890001   0.15890001   0.12930001   0.2059
    3.24952388]
 [116.54759659   0.08830001   0.26569998   0.19680001   0.24879999
    2.90235853]
 ...
 [ 75.10744723   0.08399999   0.2023       0.16500001   0.2208
    3.30166054]
 [ 40.33983024   0.0931       0.22740002   0.16540001   0.30990002
    3.11591101]
 [ 55.54790519   0.1481       0.1948       0.0902       0.2906
    3.24656463]][0m
[37m[1m[2023-07-11 01:44:19,537][233954] Max Reward on eval: 249.82442741803825[0m
[37m[1m[2023-07-11 01:44:19,537][233954] Min Reward on eval: -156.91757467314602[0m
[37m[1m[2023-07-11 01:44:19,538][233954] Mean Reward across all agents: 42.13787285753062[0m
[37m[1m[2023-07-11 01:44:19,538][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:44:19,543][233954] mean_value=-30.643238939501074, max_value=597.608203642536[0m
[37m[1m[2023-07-11 01:44:19,546][233954] New mean coefficients: [[ 2.1660106   0.5144682  15.706757   -2.5135608   9.238758    0.65830904]][0m
[37m[1m[2023-07-11 01:44:19,547][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:44:28,541][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 01:44:28,542][233954] FPS: 426987.92[0m
[36m[2023-07-11 01:44:28,544][233954] itr=134, itrs=2000, Progress: 6.70%[0m
[36m[2023-07-11 01:44:40,083][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 01:44:40,083][233954] FPS: 333752.67[0m
[36m[2023-07-11 01:44:44,419][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:44:44,425][233954] Reward + Measures: [[7844.33853013    0.19360498    0.34179997    0.19967033    0.00707567
     1.84168005]][0m
[37m[1m[2023-07-11 01:44:44,425][233954] Max Reward on eval: 7844.338530129912[0m
[37m[1m[2023-07-11 01:44:44,425][233954] Min Reward on eval: 7844.338530129912[0m
[37m[1m[2023-07-11 01:44:44,426][233954] Mean Reward across all agents: 7844.338530129912[0m
[37m[1m[2023-07-11 01:44:44,426][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:44:49,662][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:44:49,668][233954] Reward + Measures: [[ -32.97596151    0.077         0.12810001    0.1229        0.15360001
     3.40149355]
 [-148.90398268    0.06720001    0.46380001    0.36380002    0.43539998
     3.40803409]
 [  -0.90764519    0.1464        0.40949997    0.26019999    0.35929999
     2.78243947]
 ...
 [  76.105215      0.0681        0.22540002    0.1701        0.19649999
     3.27585077]
 [ -23.73754294    0.0832        0.1638        0.13590001    0.16990001
     3.55393982]
 [  -2.52011031    0.08630001    0.11900001    0.102         0.1072
     3.49363875]][0m
[37m[1m[2023-07-11 01:44:49,668][233954] Max Reward on eval: 143.02678499743342[0m
[37m[1m[2023-07-11 01:44:49,668][233954] Min Reward on eval: -285.26739288307726[0m
[37m[1m[2023-07-11 01:44:49,669][233954] Mean Reward across all agents: -12.004356289050396[0m
[37m[1m[2023-07-11 01:44:49,669][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:44:49,672][233954] mean_value=-117.94551850043838, max_value=508.6419330663846[0m
[37m[1m[2023-07-11 01:44:49,674][233954] New mean coefficients: [[ 1.9081925  -0.38790154 14.421682   -0.84244204  7.733016    0.4795905 ]][0m
[37m[1m[2023-07-11 01:44:49,675][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:44:58,734][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 01:44:58,735][233954] FPS: 423963.41[0m
[36m[2023-07-11 01:44:58,737][233954] itr=135, itrs=2000, Progress: 6.75%[0m
[36m[2023-07-11 01:45:10,426][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 01:45:10,426][233954] FPS: 329564.47[0m
[36m[2023-07-11 01:45:14,834][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:45:14,840][233954] Reward + Measures: [[7988.16673677    0.19223331    0.34813967    0.19937731    0.006411
     1.84331489]][0m
[37m[1m[2023-07-11 01:45:14,840][233954] Max Reward on eval: 7988.166736772118[0m
[37m[1m[2023-07-11 01:45:14,840][233954] Min Reward on eval: 7988.166736772118[0m
[37m[1m[2023-07-11 01:45:14,841][233954] Mean Reward across all agents: 7988.166736772118[0m
[37m[1m[2023-07-11 01:45:14,841][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:45:19,874][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:45:19,880][233954] Reward + Measures: [[ 11.41325994   0.16110002   0.0889       0.0914       0.1081
    3.14678311]
 [ 78.6957904    0.154        0.17830001   0.12460001   0.12550001
    3.44935656]
 [ -9.72834397   0.76710004   0.73439997   0.73159999   0.74579996
    3.5538609 ]
 ...
 [125.84699676   0.1287       0.1375       0.13110001   0.1134
    3.44681668]
 [ 38.09618338   0.13410001   0.10730001   0.1165       0.098
    3.49843407]
 [ 72.36059062   0.16100001   0.10899999   0.139        0.1612
    3.45078135]][0m
[37m[1m[2023-07-11 01:45:19,880][233954] Max Reward on eval: 386.80828712652436[0m
[37m[1m[2023-07-11 01:45:19,881][233954] Min Reward on eval: -114.0894630244351[0m
[37m[1m[2023-07-11 01:45:19,881][233954] Mean Reward across all agents: 76.09702482616805[0m
[37m[1m[2023-07-11 01:45:19,881][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:45:19,885][233954] mean_value=-58.771280987001795, max_value=543.721605717903[0m
[37m[1m[2023-07-11 01:45:19,888][233954] New mean coefficients: [[ 2.0109491  -1.2323337  15.660474   -2.6937675   8.734237    0.09104314]][0m
[37m[1m[2023-07-11 01:45:19,889][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:45:28,990][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 01:45:28,990][233954] FPS: 421987.11[0m
[36m[2023-07-11 01:45:28,993][233954] itr=136, itrs=2000, Progress: 6.80%[0m
[36m[2023-07-11 01:45:40,647][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 01:45:40,647][233954] FPS: 330436.50[0m
[36m[2023-07-11 01:45:45,039][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:45:45,039][233954] Reward + Measures: [[8040.65300966    0.19124866    0.35404265    0.19977202    0.006053
     1.83104706]][0m
[37m[1m[2023-07-11 01:45:45,040][233954] Max Reward on eval: 8040.6530096599445[0m
[37m[1m[2023-07-11 01:45:45,040][233954] Min Reward on eval: 8040.6530096599445[0m
[37m[1m[2023-07-11 01:45:45,040][233954] Mean Reward across all agents: 8040.6530096599445[0m
[37m[1m[2023-07-11 01:45:45,040][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:45:50,088][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:45:50,088][233954] Reward + Measures: [[106.01215298   0.1207       0.22860001   0.21660002   0.2613
    3.05717897]
 [-41.85416639   0.40260002   0.141        0.32650003   0.2192
    2.9366219 ]
 [ 56.73478582   0.0777       0.1267       0.13679999   0.13810001
    3.44962955]
 ...
 [ 43.62972671   0.0808       0.1487       0.15809999   0.16690001
    3.500494  ]
 [ 30.11362407   0.1085       0.13470002   0.15360001   0.1452
    3.21897364]
 [ 53.46224542   0.09350001   0.1434       0.1048       0.1424
    3.55644107]][0m
[37m[1m[2023-07-11 01:45:50,089][233954] Max Reward on eval: 576.1825752059697[0m
[37m[1m[2023-07-11 01:45:50,089][233954] Min Reward on eval: -190.71708103511483[0m
[37m[1m[2023-07-11 01:45:50,089][233954] Mean Reward across all agents: 53.22282702459842[0m
[37m[1m[2023-07-11 01:45:50,089][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:45:50,092][233954] mean_value=-96.12878294215176, max_value=765.1555854563046[0m
[37m[1m[2023-07-11 01:45:50,095][233954] New mean coefficients: [[ 2.0470138  -2.5723038  15.66702    -3.300049    8.346142   -0.25985894]][0m
[37m[1m[2023-07-11 01:45:50,096][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:45:59,133][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 01:45:59,133][233954] FPS: 425022.07[0m
[36m[2023-07-11 01:45:59,135][233954] itr=137, itrs=2000, Progress: 6.85%[0m
[36m[2023-07-11 01:46:10,849][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 01:46:10,850][233954] FPS: 328798.44[0m
[36m[2023-07-11 01:46:15,151][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:46:15,152][233954] Reward + Measures: [[8105.50398591    0.19082902    0.35422698    0.19858667    0.00600733
     1.83298707]][0m
[37m[1m[2023-07-11 01:46:15,152][233954] Max Reward on eval: 8105.503985908586[0m
[37m[1m[2023-07-11 01:46:15,152][233954] Min Reward on eval: 8105.503985908586[0m
[37m[1m[2023-07-11 01:46:15,153][233954] Mean Reward across all agents: 8105.503985908586[0m
[37m[1m[2023-07-11 01:46:15,153][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:46:20,203][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:46:20,209][233954] Reward + Measures: [[101.21106254   0.5485       0.4052       0.5219       0.12640001
    3.58650589]
 [ 74.07787777   0.1155       0.18380001   0.1578       0.1971
    3.45204139]
 [ 45.51722908   0.14070001   0.1432       0.1445       0.1054
    3.48741007]
 ...
 [ -9.93511265   0.0962       0.20870002   0.0986       0.1181
    3.70356297]
 [ 65.93449334   0.1237       0.15790001   0.1283       0.1357
    3.1657598 ]
 [ 43.76039934   0.10770001   0.24519996   0.22980002   0.23800002
    3.58512044]][0m
[37m[1m[2023-07-11 01:46:20,209][233954] Max Reward on eval: 228.0534481972456[0m
[37m[1m[2023-07-11 01:46:20,209][233954] Min Reward on eval: -88.89346935511567[0m
[37m[1m[2023-07-11 01:46:20,210][233954] Mean Reward across all agents: 21.16048772346409[0m
[37m[1m[2023-07-11 01:46:20,210][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:46:20,214][233954] mean_value=-77.49537125980167, max_value=509.0299460948445[0m
[37m[1m[2023-07-11 01:46:20,216][233954] New mean coefficients: [[ 2.6336527 -1.4605204 18.869448  -7.0689306 13.648912   0.381151 ]][0m
[37m[1m[2023-07-11 01:46:20,217][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:46:29,296][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 01:46:29,296][233954] FPS: 423058.37[0m
[36m[2023-07-11 01:46:29,298][233954] itr=138, itrs=2000, Progress: 6.90%[0m
[36m[2023-07-11 01:46:41,019][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 01:46:41,019][233954] FPS: 328575.65[0m
[36m[2023-07-11 01:46:45,326][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:46:45,332][233954] Reward + Measures: [[8098.80824002    0.19117966    0.36571968    0.19877632    0.00594533
     1.83757353]][0m
[37m[1m[2023-07-11 01:46:45,332][233954] Max Reward on eval: 8098.808240021661[0m
[37m[1m[2023-07-11 01:46:45,332][233954] Min Reward on eval: 8098.808240021661[0m
[37m[1m[2023-07-11 01:46:45,332][233954] Mean Reward across all agents: 8098.808240021661[0m
[37m[1m[2023-07-11 01:46:45,333][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:46:50,337][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:46:50,342][233954] Reward + Measures: [[ 15.35423554   0.08350001   0.11709999   0.1059       0.0988
    3.48652267]
 [-23.26219693   0.09980001   0.13410001   0.12220001   0.11540001
    3.45693707]
 [-33.26854619   0.0987       0.19500001   0.1542       0.16300002
    3.38238454]
 ...
 [-39.48475012   0.11789999   0.1768       0.1542       0.13689999
    3.40671539]
 [-31.92804301   0.28850001   0.67309999   0.50370002   0.51750004
    3.24624443]
 [-30.48837174   0.14100002   0.25750002   0.2096       0.21210001
    3.40648961]][0m
[37m[1m[2023-07-11 01:46:50,343][233954] Max Reward on eval: 112.64079952123575[0m
[37m[1m[2023-07-11 01:46:50,343][233954] Min Reward on eval: -365.8377671924885[0m
[37m[1m[2023-07-11 01:46:50,343][233954] Mean Reward across all agents: -42.23215913947663[0m
[37m[1m[2023-07-11 01:46:50,343][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:46:50,350][233954] mean_value=-45.03028225878032, max_value=612.6407995212357[0m
[37m[1m[2023-07-11 01:46:50,353][233954] New mean coefficients: [[ 1.7185755 -3.7736454 13.628649  -0.8770366  4.7196274 -1.7441096]][0m
[37m[1m[2023-07-11 01:46:50,354][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:46:59,332][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 01:46:59,332][233954] FPS: 427798.01[0m
[36m[2023-07-11 01:46:59,334][233954] itr=139, itrs=2000, Progress: 6.95%[0m
[36m[2023-07-11 01:47:11,364][233954] train() took 12.00 seconds to complete[0m
[36m[2023-07-11 01:47:11,364][233954] FPS: 320091.44[0m
[36m[2023-07-11 01:47:15,669][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:47:15,675][233954] Reward + Measures: [[8033.44893722    0.18644167    0.34677196    0.19296767    0.006405
     1.79864252]][0m
[37m[1m[2023-07-11 01:47:15,675][233954] Max Reward on eval: 8033.448937221083[0m
[37m[1m[2023-07-11 01:47:15,676][233954] Min Reward on eval: 8033.448937221083[0m
[37m[1m[2023-07-11 01:47:15,676][233954] Mean Reward across all agents: 8033.448937221083[0m
[37m[1m[2023-07-11 01:47:15,676][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:47:20,863][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:47:20,869][233954] Reward + Measures: [[ -4.27894675   0.22389999   0.30669999   0.0702       0.30109999
    2.95032024]
 [193.99711654   0.4619       0.4691       0.026        0.49130002
    3.64267087]
 [ 43.93954938   0.22980002   0.25509998   0.0488       0.27449998
    3.51253009]
 ...
 [ 16.2500736    0.11730001   0.15930001   0.1017       0.13710001
    3.48307681]
 [ -6.54512869   0.13869999   0.18350001   0.1017       0.1574
    3.38652086]
 [  4.26105441   0.1135       0.10880001   0.0984       0.10600001
    3.37588191]][0m
[37m[1m[2023-07-11 01:47:20,869][233954] Max Reward on eval: 312.9142694973387[0m
[37m[1m[2023-07-11 01:47:20,870][233954] Min Reward on eval: -329.502553940285[0m
[37m[1m[2023-07-11 01:47:20,870][233954] Mean Reward across all agents: 37.458514420546194[0m
[37m[1m[2023-07-11 01:47:20,870][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:47:20,877][233954] mean_value=103.17454107419238, max_value=812.9142694973386[0m
[37m[1m[2023-07-11 01:47:20,879][233954] New mean coefficients: [[ 0.8132773 -4.8715296  8.852282   5.5760436 -3.5434723 -3.089127 ]][0m
[37m[1m[2023-07-11 01:47:20,880][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:47:29,861][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 01:47:29,862][233954] FPS: 427642.84[0m
[36m[2023-07-11 01:47:29,864][233954] itr=140, itrs=2000, Progress: 7.00%[0m
[37m[1m[2023-07-11 01:49:54,758][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000120[0m
[36m[2023-07-11 01:50:07,124][233954] train() took 11.74 seconds to complete[0m
[36m[2023-07-11 01:50:07,125][233954] FPS: 326960.24[0m
[36m[2023-07-11 01:50:11,351][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:50:11,352][233954] Reward + Measures: [[7976.98866326    0.19058032    0.36895934    0.197148      0.00620233
     1.77745998]][0m
[37m[1m[2023-07-11 01:50:11,352][233954] Max Reward on eval: 7976.988663255557[0m
[37m[1m[2023-07-11 01:50:11,352][233954] Min Reward on eval: 7976.988663255557[0m
[37m[1m[2023-07-11 01:50:11,353][233954] Mean Reward across all agents: 7976.988663255557[0m
[37m[1m[2023-07-11 01:50:11,353][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:50:16,343][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:50:16,344][233954] Reward + Measures: [[ 82.29346645   0.092        0.12800001   0.0728       0.10040001
    3.30700088]
 [103.19796418   0.4533       0.39670002   0.3308       0.32950002
    2.62656665]
 [-56.40146182   0.0844       0.0982       0.0735       0.0866
    3.45533752]
 ...
 [  7.31568708   0.13520001   0.175        0.1445       0.117
    3.21197319]
 [  9.14974019   0.2168       0.22570001   0.21890001   0.24510001
    2.9600668 ]
 [118.62898804   0.50920004   0.2904       0.50190002   0.28270003
    2.6738739 ]][0m
[37m[1m[2023-07-11 01:50:16,344][233954] Max Reward on eval: 265.53278227485714[0m
[37m[1m[2023-07-11 01:50:16,344][233954] Min Reward on eval: -287.91527463458476[0m
[37m[1m[2023-07-11 01:50:16,344][233954] Mean Reward across all agents: 37.61081773765907[0m
[37m[1m[2023-07-11 01:50:16,345][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:50:16,351][233954] mean_value=-63.14924194878199, max_value=765.5327822748571[0m
[37m[1m[2023-07-11 01:50:16,353][233954] New mean coefficients: [[ 1.3336403  -3.625771   12.600168    0.49632645  3.0888796  -1.5196605 ]][0m
[37m[1m[2023-07-11 01:50:16,354][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:50:25,317][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 01:50:25,317][233954] FPS: 428505.57[0m
[36m[2023-07-11 01:50:25,319][233954] itr=141, itrs=2000, Progress: 7.05%[0m
[36m[2023-07-11 01:50:36,970][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 01:50:36,970][233954] FPS: 330659.68[0m
[36m[2023-07-11 01:50:41,250][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:50:41,250][233954] Reward + Measures: [[7976.91991054    0.19103466    0.38344467    0.19847965    0.00601267
     1.76585448]][0m
[37m[1m[2023-07-11 01:50:41,251][233954] Max Reward on eval: 7976.919910539583[0m
[37m[1m[2023-07-11 01:50:41,251][233954] Min Reward on eval: 7976.919910539583[0m
[37m[1m[2023-07-11 01:50:41,251][233954] Mean Reward across all agents: 7976.919910539583[0m
[37m[1m[2023-07-11 01:50:41,251][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:50:46,226][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:50:46,227][233954] Reward + Measures: [[  70.23859334    0.5345        0.49130002    0.55770004    0.47550002
     2.75752378]
 [   7.25967687    0.31500003    0.46110001    0.40809998    0.43059999
     2.87020302]
 [ -44.23050784    0.21280001    0.09370001    0.1744        0.1227
     3.21459579]
 ...
 [ -25.38827298    0.22510003    0.1217        0.19160001    0.09240001
     3.37014198]
 [-121.57676819    0.61920005    0.25420001    0.57230002    0.18890001
     3.52624202]
 [ -24.28335278    0.18159999    0.20910001    0.1648        0.17490001
     3.17165017]][0m
[37m[1m[2023-07-11 01:50:46,227][233954] Max Reward on eval: 172.67003441462293[0m
[37m[1m[2023-07-11 01:50:46,227][233954] Min Reward on eval: -569.9076271103695[0m
[37m[1m[2023-07-11 01:50:46,227][233954] Mean Reward across all agents: -39.12484168409894[0m
[37m[1m[2023-07-11 01:50:46,228][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:50:46,233][233954] mean_value=-125.38438494323985, max_value=563.3185548383742[0m
[37m[1m[2023-07-11 01:50:46,235][233954] New mean coefficients: [[ 0.74854827 -5.697075    8.147927    4.996194   -3.8082824  -3.2684114 ]][0m
[37m[1m[2023-07-11 01:50:46,236][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:50:55,204][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 01:50:55,204][233954] FPS: 428265.73[0m
[36m[2023-07-11 01:50:55,207][233954] itr=142, itrs=2000, Progress: 7.10%[0m
[36m[2023-07-11 01:51:07,038][233954] train() took 11.80 seconds to complete[0m
[36m[2023-07-11 01:51:07,039][233954] FPS: 325491.48[0m
[36m[2023-07-11 01:51:11,356][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:51:11,356][233954] Reward + Measures: [[8074.77989675    0.18721333    0.37419567    0.194948      0.005684
     1.74037218]][0m
[37m[1m[2023-07-11 01:51:11,357][233954] Max Reward on eval: 8074.779896749514[0m
[37m[1m[2023-07-11 01:51:11,357][233954] Min Reward on eval: 8074.779896749514[0m
[37m[1m[2023-07-11 01:51:11,357][233954] Mean Reward across all agents: 8074.779896749514[0m
[37m[1m[2023-07-11 01:51:11,357][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:51:16,327][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:51:16,328][233954] Reward + Measures: [[213.04732568   0.5244       0.2967       0.51870006   0.2062
    3.64377379]
 [106.12159269   0.53320003   0.50529999   0.53010005   0.52930003
    3.7113378 ]
 [161.1605265    0.25929999   0.1585       0.29100004   0.18539999
    3.20212364]
 ...
 [ 99.83527599   0.30900002   0.21620002   0.32000002   0.12
    3.5802784 ]
 [ 12.03767998   0.16059999   0.1331       0.1374       0.11520001
    3.46731353]
 [ 27.83281526   0.24609999   0.16500001   0.20540002   0.1355
    3.35757709]][0m
[37m[1m[2023-07-11 01:51:16,328][233954] Max Reward on eval: 519.7089834135957[0m
[37m[1m[2023-07-11 01:51:16,329][233954] Min Reward on eval: -116.63104960750789[0m
[37m[1m[2023-07-11 01:51:16,329][233954] Mean Reward across all agents: 95.20808618018935[0m
[37m[1m[2023-07-11 01:51:16,329][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:51:16,339][233954] mean_value=132.03295973578895, max_value=821.088883663481[0m
[37m[1m[2023-07-11 01:51:16,342][233954] New mean coefficients: [[ 1.2431829 -3.9918737 11.008825   1.5411596  1.4035072 -1.7107247]][0m
[37m[1m[2023-07-11 01:51:16,343][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:51:25,367][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 01:51:25,367][233954] FPS: 425600.65[0m
[36m[2023-07-11 01:51:25,370][233954] itr=143, itrs=2000, Progress: 7.15%[0m
[36m[2023-07-11 01:51:37,046][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 01:51:37,052][233954] FPS: 329940.76[0m
[36m[2023-07-11 01:51:41,275][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:51:41,275][233954] Reward + Measures: [[8099.27471484    0.18741134    0.38132569    0.19531965    0.005759
     1.73878062]][0m
[37m[1m[2023-07-11 01:51:41,276][233954] Max Reward on eval: 8099.274714838709[0m
[37m[1m[2023-07-11 01:51:41,276][233954] Min Reward on eval: 8099.274714838709[0m
[37m[1m[2023-07-11 01:51:41,276][233954] Mean Reward across all agents: 8099.274714838709[0m
[37m[1m[2023-07-11 01:51:41,276][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:51:46,208][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:51:46,209][233954] Reward + Measures: [[-14.31186835   0.15790001   0.0938       0.0981       0.0764
    3.57371759]
 [ 68.23460067   0.0982       0.29539999   0.1974       0.25710002
    3.20845389]
 [ 10.99117083   0.32430002   0.28600001   0.2924       0.20479999
    2.93881035]
 ...
 [-25.77397422   0.1469       0.22739999   0.17739999   0.22729997
    3.18782306]
 [ 24.90520096   0.13810001   0.18529999   0.19430001   0.17170002
    3.5751884 ]
 [-51.79845838   0.16140001   0.10580001   0.1336       0.0851
    3.74581838]][0m
[37m[1m[2023-07-11 01:51:46,209][233954] Max Reward on eval: 201.7749767745845[0m
[37m[1m[2023-07-11 01:51:46,209][233954] Min Reward on eval: -300.2795234695077[0m
[37m[1m[2023-07-11 01:51:46,210][233954] Mean Reward across all agents: 17.729079724840563[0m
[37m[1m[2023-07-11 01:51:46,210][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:51:46,212][233954] mean_value=-145.62321451018033, max_value=491.30205915280624[0m
[37m[1m[2023-07-11 01:51:46,215][233954] New mean coefficients: [[ 1.7517037  -2.706225   13.697276   -2.6210785   6.9340568  -0.18911123]][0m
[37m[1m[2023-07-11 01:51:46,216][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:51:55,199][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 01:51:55,199][233954] FPS: 427540.79[0m
[36m[2023-07-11 01:51:55,201][233954] itr=144, itrs=2000, Progress: 7.20%[0m
[36m[2023-07-11 01:52:06,951][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 01:52:06,951][233954] FPS: 327812.21[0m
[36m[2023-07-11 01:52:11,274][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:52:11,274][233954] Reward + Measures: [[8168.68554691    0.18967934    0.39184129    0.19507666    0.00570433
     1.7612921 ]][0m
[37m[1m[2023-07-11 01:52:11,275][233954] Max Reward on eval: 8168.6855469123475[0m
[37m[1m[2023-07-11 01:52:11,275][233954] Min Reward on eval: 8168.6855469123475[0m
[37m[1m[2023-07-11 01:52:11,275][233954] Mean Reward across all agents: 8168.6855469123475[0m
[37m[1m[2023-07-11 01:52:11,275][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:52:16,318][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:52:16,319][233954] Reward + Measures: [[ -36.66899202    0.0856        0.0716        0.0683        0.07790001
     3.66074824]
 [-105.55585121    0.3459        0.28270003    0.33089998    0.1071
     3.26701403]
 [  98.74031435    0.17200001    0.23430002    0.21959999    0.17199999
     3.38757372]
 ...
 [  61.69912657    0.26210004    0.23309998    0.2343        0.0952
     3.56909251]
 [ -80.63094692    0.10120001    0.11009999    0.0881        0.113
     3.75017715]
 [ -41.96811114    0.15580001    0.1408        0.1267        0.13080001
     3.5705483 ]][0m
[37m[1m[2023-07-11 01:52:16,319][233954] Max Reward on eval: 287.2474278062582[0m
[37m[1m[2023-07-11 01:52:16,319][233954] Min Reward on eval: -298.58930418174714[0m
[37m[1m[2023-07-11 01:52:16,319][233954] Mean Reward across all agents: -22.997252306362604[0m
[37m[1m[2023-07-11 01:52:16,320][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:52:16,323][233954] mean_value=-135.482214781155, max_value=391.76589639027543[0m
[37m[1m[2023-07-11 01:52:16,325][233954] New mean coefficients: [[ 2.0687697  -2.492774   14.968423   -3.990814    9.391667    0.09130323]][0m
[37m[1m[2023-07-11 01:52:16,326][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:52:25,361][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 01:52:25,362][233954] FPS: 425075.41[0m
[36m[2023-07-11 01:52:25,364][233954] itr=145, itrs=2000, Progress: 7.25%[0m
[36m[2023-07-11 01:52:36,915][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 01:52:36,915][233954] FPS: 333578.49[0m
[36m[2023-07-11 01:52:41,386][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:52:41,386][233954] Reward + Measures: [[8108.42898436    0.18772633    0.38336998    0.19309001    0.006263
     1.75780416]][0m
[37m[1m[2023-07-11 01:52:41,387][233954] Max Reward on eval: 8108.428984362187[0m
[37m[1m[2023-07-11 01:52:41,387][233954] Min Reward on eval: 8108.428984362187[0m
[37m[1m[2023-07-11 01:52:41,387][233954] Mean Reward across all agents: 8108.428984362187[0m
[37m[1m[2023-07-11 01:52:41,387][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:52:46,439][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:52:46,439][233954] Reward + Measures: [[-80.02749392   0.32280001   0.1147       0.37219998   0.28550002
    3.45740294]
 [ 19.8210849    0.26360002   0.34660003   0.35839999   0.40809998
    3.46436238]
 [ 18.35190335   0.0857       0.0803       0.14600001   0.16160001
    3.49557662]
 ...
 [-19.43898968   0.1929       0.41870004   0.36090001   0.52360004
    3.01835251]
 [ 11.94423167   0.54730004   0.0875       0.59490001   0.44060001
    3.47615361]
 [ 76.14517301   0.14570001   0.19509999   0.19149999   0.25009999
    3.52814293]][0m
[37m[1m[2023-07-11 01:52:46,440][233954] Max Reward on eval: 225.53898514667526[0m
[37m[1m[2023-07-11 01:52:46,440][233954] Min Reward on eval: -180.88120341217146[0m
[37m[1m[2023-07-11 01:52:46,440][233954] Mean Reward across all agents: 23.874156130936367[0m
[37m[1m[2023-07-11 01:52:46,440][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:52:46,449][233954] mean_value=44.12792805852378, max_value=669.329388911929[0m
[37m[1m[2023-07-11 01:52:46,451][233954] New mean coefficients: [[ 1.5011728 -3.678484  11.343161   0.2506392  3.237461  -1.4330262]][0m
[37m[1m[2023-07-11 01:52:46,453][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:52:55,549][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 01:52:55,549][233954] FPS: 422220.43[0m
[36m[2023-07-11 01:52:55,552][233954] itr=146, itrs=2000, Progress: 7.30%[0m
[36m[2023-07-11 01:53:07,304][233954] train() took 11.72 seconds to complete[0m
[36m[2023-07-11 01:53:07,305][233954] FPS: 327689.34[0m
[36m[2023-07-11 01:53:11,585][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:53:11,585][233954] Reward + Measures: [[8183.57359393    0.18788834    0.386926      0.19270201    0.00578433
     1.74880624]][0m
[37m[1m[2023-07-11 01:53:11,585][233954] Max Reward on eval: 8183.573593928575[0m
[37m[1m[2023-07-11 01:53:11,586][233954] Min Reward on eval: 8183.573593928575[0m
[37m[1m[2023-07-11 01:53:11,586][233954] Mean Reward across all agents: 8183.573593928575[0m
[37m[1m[2023-07-11 01:53:11,586][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:53:16,591][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:53:16,591][233954] Reward + Measures: [[120.4652195    0.65109998   0.57190001   0.45690003   0.69010001
    3.31045508]
 [ 18.75682048   0.26890001   0.072        0.21139999   0.18870001
    3.16552711]
 [-21.95657453   0.47319999   0.08560001   0.42589998   0.3524
    3.18793845]
 ...
 [-53.99554913   0.3091       0.60880005   0.30590001   0.69929999
    3.25572824]
 [-46.02939773   0.1148       0.091        0.0968       0.1001
    3.51781821]
 [ 11.07911116   0.2024       0.10640001   0.17620002   0.14740001
    3.30541039]][0m
[37m[1m[2023-07-11 01:53:16,591][233954] Max Reward on eval: 310.4284614417702[0m
[37m[1m[2023-07-11 01:53:16,592][233954] Min Reward on eval: -207.59369268920273[0m
[37m[1m[2023-07-11 01:53:16,592][233954] Mean Reward across all agents: 11.08903799407396[0m
[37m[1m[2023-07-11 01:53:16,592][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:53:16,598][233954] mean_value=-67.84649139068037, max_value=686.1896257543006[0m
[37m[1m[2023-07-11 01:53:16,601][233954] New mean coefficients: [[ 1.529783  -4.906225  10.241564   1.8887802  0.4304073 -2.4844952]][0m
[37m[1m[2023-07-11 01:53:16,602][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:53:25,526][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 01:53:25,526][233954] FPS: 430370.84[0m
[36m[2023-07-11 01:53:25,528][233954] itr=147, itrs=2000, Progress: 7.35%[0m
[36m[2023-07-11 01:53:37,232][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 01:53:37,232][233954] FPS: 329170.03[0m
[36m[2023-07-11 01:53:41,602][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:53:41,603][233954] Reward + Measures: [[8174.38142069    0.18983699    0.39301097    0.19367       0.00614067
     1.75657618]][0m
[37m[1m[2023-07-11 01:53:41,603][233954] Max Reward on eval: 8174.381420690485[0m
[37m[1m[2023-07-11 01:53:41,603][233954] Min Reward on eval: 8174.381420690485[0m
[37m[1m[2023-07-11 01:53:41,603][233954] Mean Reward across all agents: 8174.381420690485[0m
[37m[1m[2023-07-11 01:53:41,604][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:53:46,587][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:53:46,587][233954] Reward + Measures: [[   3.47771755    0.0781        0.15900001    0.11439999    0.14579999
     3.14146614]
 [  -9.38772401    0.1409        0.32119998    0.1269        0.3285
     3.48511004]
 [ 113.13210908    0.1175        0.29350001    0.15980001    0.28439999
     3.37630153]
 ...
 [-115.78432031    0.18959999    0.36740002    0.152         0.37080002
     3.46099401]
 [  23.17108921    0.40809998    0.09480001    0.44080001    0.2146
     3.28659368]
 [ -27.15166745    0.12620001    0.1815        0.07470001    0.20510001
     3.51788449]][0m
[37m[1m[2023-07-11 01:53:46,587][233954] Max Reward on eval: 289.5944378037937[0m
[37m[1m[2023-07-11 01:53:46,588][233954] Min Reward on eval: -344.02451344653963[0m
[37m[1m[2023-07-11 01:53:46,588][233954] Mean Reward across all agents: 33.88097755475251[0m
[37m[1m[2023-07-11 01:53:46,588][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:53:46,592][233954] mean_value=-96.70648998348958, max_value=570.4698118403344[0m
[37m[1m[2023-07-11 01:53:46,595][233954] New mean coefficients: [[ 1.4832392 -3.952138   9.960508   2.6962519  0.701087  -1.9226568]][0m
[37m[1m[2023-07-11 01:53:46,596][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:53:55,559][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 01:53:55,559][233954] FPS: 428502.27[0m
[36m[2023-07-11 01:53:55,561][233954] itr=148, itrs=2000, Progress: 7.40%[0m
[36m[2023-07-11 01:54:07,293][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 01:54:07,293][233954] FPS: 328381.19[0m
[36m[2023-07-11 01:54:11,618][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:54:11,619][233954] Reward + Measures: [[8130.21801557    0.18951833    0.36027464    0.19768301    0.0064
     1.73000824]][0m
[37m[1m[2023-07-11 01:54:11,619][233954] Max Reward on eval: 8130.218015571591[0m
[37m[1m[2023-07-11 01:54:11,619][233954] Min Reward on eval: 8130.218015571591[0m
[37m[1m[2023-07-11 01:54:11,620][233954] Mean Reward across all agents: 8130.218015571591[0m
[37m[1m[2023-07-11 01:54:11,620][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:54:16,650][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:54:16,651][233954] Reward + Measures: [[37.60381461  0.36940002  0.092       0.38280001  0.2633      3.40480995]
 [34.9739223   0.53649998  0.34769997  0.57590002  0.0978      3.57502556]
 [ 3.80461879  0.3312      0.21799998  0.35890001  0.0811      3.20057368]
 ...
 [46.73285643  0.67610002  0.0658      0.68410009  0.58910006  3.70658612]
 [ 6.43903038  0.1231      0.13560002  0.09420001  0.12950002  3.50931978]
 [98.37634296  0.19600001  0.1214      0.2172      0.1662      3.50669861]][0m
[37m[1m[2023-07-11 01:54:16,651][233954] Max Reward on eval: 762.2737854315899[0m
[37m[1m[2023-07-11 01:54:16,652][233954] Min Reward on eval: -483.2644505509641[0m
[37m[1m[2023-07-11 01:54:16,652][233954] Mean Reward across all agents: 16.165695680400123[0m
[37m[1m[2023-07-11 01:54:16,652][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:54:16,658][233954] mean_value=-44.4946904428799, max_value=1063.6109861946143[0m
[37m[1m[2023-07-11 01:54:16,661][233954] New mean coefficients: [[ 1.2494875 -4.327403   7.6227913  5.78      -3.741714  -2.5633206]][0m
[37m[1m[2023-07-11 01:54:16,662][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:54:25,671][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 01:54:25,672][233954] FPS: 426290.99[0m
[36m[2023-07-11 01:54:25,674][233954] itr=149, itrs=2000, Progress: 7.45%[0m
[36m[2023-07-11 01:54:37,220][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 01:54:37,220][233954] FPS: 333689.71[0m
[36m[2023-07-11 01:54:41,434][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:54:41,435][233954] Reward + Measures: [[8137.98732006    0.18799832    0.35377735    0.19641532    0.006433
     1.70824742]][0m
[37m[1m[2023-07-11 01:54:41,435][233954] Max Reward on eval: 8137.987320063642[0m
[37m[1m[2023-07-11 01:54:41,435][233954] Min Reward on eval: 8137.987320063642[0m
[37m[1m[2023-07-11 01:54:41,436][233954] Mean Reward across all agents: 8137.987320063642[0m
[37m[1m[2023-07-11 01:54:41,436][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:54:46,570][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:54:46,570][233954] Reward + Measures: [[ -56.95326368    0.3511        0.23869999    0.3378        0.18440001
     3.16450572]
 [-169.62192254    0.53870004    0.51519996    0.61090004    0.0669
     3.39889312]
 [  -5.71397761    0.0997        0.0858        0.11010001    0.0781
     3.34856224]
 ...
 [  46.62386089    0.15750001    0.13620001    0.155         0.13060001
     3.2509613 ]
 [ -45.46097273    0.11180001    0.0793        0.0984        0.1107
     3.6001637 ]
 [  28.56068128    0.098         0.07740001    0.09409999    0.11689999
     3.24581194]][0m
[37m[1m[2023-07-11 01:54:46,571][233954] Max Reward on eval: 201.1400127524277[0m
[37m[1m[2023-07-11 01:54:46,571][233954] Min Reward on eval: -252.9607348835096[0m
[37m[1m[2023-07-11 01:54:46,571][233954] Mean Reward across all agents: -18.93243046523793[0m
[37m[1m[2023-07-11 01:54:46,571][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:54:46,573][233954] mean_value=-172.99405390197143, max_value=254.1261303278589[0m
[37m[1m[2023-07-11 01:54:46,576][233954] New mean coefficients: [[ 1.4163135 -3.6703773  8.472429   3.9295917 -1.6760352 -2.254479 ]][0m
[37m[1m[2023-07-11 01:54:46,577][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:54:55,598][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 01:54:55,599][233954] FPS: 425723.85[0m
[36m[2023-07-11 01:54:55,601][233954] itr=150, itrs=2000, Progress: 7.50%[0m
[37m[1m[2023-07-11 01:57:20,027][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000130[0m
[36m[2023-07-11 01:57:32,398][233954] train() took 11.79 seconds to complete[0m
[36m[2023-07-11 01:57:32,399][233954] FPS: 325679.40[0m
[36m[2023-07-11 01:57:36,571][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:57:36,571][233954] Reward + Measures: [[7442.42683245    0.19233967    0.35383332    0.20549932    0.010655
     1.66196167]][0m
[37m[1m[2023-07-11 01:57:36,572][233954] Max Reward on eval: 7442.426832452317[0m
[37m[1m[2023-07-11 01:57:36,572][233954] Min Reward on eval: 7442.426832452317[0m
[37m[1m[2023-07-11 01:57:36,572][233954] Mean Reward across all agents: 7442.426832452317[0m
[37m[1m[2023-07-11 01:57:36,572][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:57:41,715][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:57:41,716][233954] Reward + Measures: [[  -1.72663558    0.1735        0.12969999    0.1692        0.0787
     3.23026085]
 [   0.84768494    0.0871        0.2172        0.16510001    0.1876
     3.36215329]
 [-117.01146746    0.7173        0.67329997    0.74420005    0.0336
     3.864573  ]
 ...
 [ 111.06725643    0.1109        0.09519999    0.11740001    0.11990001
     2.87265778]
 [  55.12800349    0.1135        0.0728        0.1107        0.088
     3.28481293]
 [  29.4524409     0.09060001    0.08790001    0.1427        0.0865
     3.14092493]][0m
[37m[1m[2023-07-11 01:57:41,716][233954] Max Reward on eval: 294.54263922860264[0m
[37m[1m[2023-07-11 01:57:41,716][233954] Min Reward on eval: -471.42243579351344[0m
[37m[1m[2023-07-11 01:57:41,717][233954] Mean Reward across all agents: -2.6866291004759244[0m
[37m[1m[2023-07-11 01:57:41,717][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:57:41,723][233954] mean_value=-8.967352910727593, max_value=657.6728515030584[0m
[37m[1m[2023-07-11 01:57:41,726][233954] New mean coefficients: [[ 1.0480099 -4.727168   5.0483065  8.514795  -7.217059  -3.808125 ]][0m
[37m[1m[2023-07-11 01:57:41,727][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:57:50,494][233954] train() took 8.77 seconds to complete[0m
[36m[2023-07-11 01:57:50,494][233954] FPS: 438067.43[0m
[36m[2023-07-11 01:57:50,497][233954] itr=151, itrs=2000, Progress: 7.55%[0m
[36m[2023-07-11 01:58:02,064][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 01:58:02,070][233954] FPS: 332974.68[0m
[36m[2023-07-11 01:58:06,296][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:58:06,297][233954] Reward + Measures: [[8055.35148668    0.18836035    0.35306033    0.19927967    0.00613233
     1.64047086]][0m
[37m[1m[2023-07-11 01:58:06,297][233954] Max Reward on eval: 8055.3514866769365[0m
[37m[1m[2023-07-11 01:58:06,297][233954] Min Reward on eval: 8055.3514866769365[0m
[37m[1m[2023-07-11 01:58:06,297][233954] Mean Reward across all agents: 8055.3514866769365[0m
[37m[1m[2023-07-11 01:58:06,297][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:58:11,277][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:58:11,277][233954] Reward + Measures: [[ 82.30438803   0.13440001   0.1013       0.14330001   0.11630001
    3.13193321]
 [ 80.85344242   0.20810001   0.23510002   0.20650001   0.2335
    3.61174083]
 [-97.26856296   0.287        0.2366       0.3348       0.22220002
    2.90625429]
 ...
 [ 44.39139362   0.15769999   0.1865       0.22919999   0.1655
    3.57020116]
 [ 35.07813005   0.15550001   0.1911       0.18629999   0.18090001
    3.66904068]
 [-82.23868528   0.51640004   0.46540004   0.37680003   0.21510001
    3.2638638 ]][0m
[37m[1m[2023-07-11 01:58:11,277][233954] Max Reward on eval: 253.97520350068808[0m
[37m[1m[2023-07-11 01:58:11,278][233954] Min Reward on eval: -265.8011226963252[0m
[37m[1m[2023-07-11 01:58:11,278][233954] Mean Reward across all agents: 20.65778287375834[0m
[37m[1m[2023-07-11 01:58:11,278][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:58:11,283][233954] mean_value=-87.16775167922084, max_value=480.98126028711135[0m
[37m[1m[2023-07-11 01:58:11,286][233954] New mean coefficients: [[ 0.757005  -4.3907995  3.2692623 11.081203  -9.882654  -4.8274584]][0m
[37m[1m[2023-07-11 01:58:11,287][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:58:20,341][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 01:58:20,342][233954] FPS: 424177.44[0m
[36m[2023-07-11 01:58:20,344][233954] itr=152, itrs=2000, Progress: 7.60%[0m
[36m[2023-07-11 01:58:32,039][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 01:58:32,040][233954] FPS: 329421.21[0m
[36m[2023-07-11 01:58:36,243][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:58:36,244][233954] Reward + Measures: [[8051.58256208    0.18087564    0.32906535    0.19044466    0.00746533
     1.55962598]][0m
[37m[1m[2023-07-11 01:58:36,244][233954] Max Reward on eval: 8051.582562080169[0m
[37m[1m[2023-07-11 01:58:36,244][233954] Min Reward on eval: 8051.582562080169[0m
[37m[1m[2023-07-11 01:58:36,244][233954] Mean Reward across all agents: 8051.582562080169[0m
[37m[1m[2023-07-11 01:58:36,245][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:58:41,252][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:58:41,257][233954] Reward + Measures: [[ 95.47905021   0.1701       0.20060001   0.17560001   0.223
    3.01645637]
 [ 23.3314713    0.226        0.2314       0.27220002   0.2674
    3.54193163]
 [ 81.7411103    0.1035       0.14230001   0.15249999   0.0517
    3.25894403]
 ...
 [  5.50923435   0.38190001   0.34759998   0.23010002   0.53220004
    2.90542579]
 [-37.97387123   0.48930001   0.50489998   0.09680001   0.53690004
    3.06126165]
 [ 10.2283413    0.48650002   0.36460003   0.20500003   0.45520002
    3.13750386]][0m
[37m[1m[2023-07-11 01:58:41,258][233954] Max Reward on eval: 316.33758053155617[0m
[37m[1m[2023-07-11 01:58:41,258][233954] Min Reward on eval: -614.0830726066604[0m
[37m[1m[2023-07-11 01:58:41,258][233954] Mean Reward across all agents: -3.4138932111473204[0m
[37m[1m[2023-07-11 01:58:41,258][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:58:41,264][233954] mean_value=-81.9248579532181, max_value=730.3661164445105[0m
[37m[1m[2023-07-11 01:58:41,267][233954] New mean coefficients: [[ 1.0345709 -3.348878   4.88593    8.344662  -6.2703757 -3.964594 ]][0m
[37m[1m[2023-07-11 01:58:41,267][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:58:50,236][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 01:58:50,236][233954] FPS: 428233.79[0m
[36m[2023-07-11 01:58:50,238][233954] itr=153, itrs=2000, Progress: 7.65%[0m
[36m[2023-07-11 01:59:01,702][233954] train() took 11.43 seconds to complete[0m
[36m[2023-07-11 01:59:01,702][233954] FPS: 336003.17[0m
[36m[2023-07-11 01:59:05,975][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:59:05,975][233954] Reward + Measures: [[8221.77180215    0.182264      0.33944198    0.193986      0.00649233
     1.55167425]][0m
[37m[1m[2023-07-11 01:59:05,976][233954] Max Reward on eval: 8221.771802145[0m
[37m[1m[2023-07-11 01:59:05,976][233954] Min Reward on eval: 8221.771802145[0m
[37m[1m[2023-07-11 01:59:05,976][233954] Mean Reward across all agents: 8221.771802145[0m
[37m[1m[2023-07-11 01:59:05,976][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:59:10,972][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:59:10,973][233954] Reward + Measures: [[-30.19658506   0.0823       0.1134       0.076        0.0698
    3.45122266]
 [-33.21290336   0.09230001   0.0842       0.07360001   0.09069999
    3.74126601]
 [-34.61613575   0.25170001   0.3048       0.0586       0.31459999
    3.40961695]
 ...
 [-44.29059785   0.08790001   0.0897       0.07610001   0.091
    3.6846664 ]
 [ 42.84917547   0.23699999   0.14389999   0.2678       0.09820001
    3.350214  ]
 [ 27.03498469   0.31310001   0.36140001   0.0704       0.38940001
    3.45957923]][0m
[37m[1m[2023-07-11 01:59:10,973][233954] Max Reward on eval: 290.51202963255344[0m
[37m[1m[2023-07-11 01:59:10,973][233954] Min Reward on eval: -242.8993099063635[0m
[37m[1m[2023-07-11 01:59:10,973][233954] Mean Reward across all agents: 40.07966173428171[0m
[37m[1m[2023-07-11 01:59:10,974][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:59:10,976][233954] mean_value=-114.50320386895018, max_value=498.181156008807[0m
[37m[1m[2023-07-11 01:59:10,979][233954] New mean coefficients: [[ 0.9865029 -2.755014   5.4400845  7.290142  -4.754746  -3.7027798]][0m
[37m[1m[2023-07-11 01:59:10,980][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:59:20,045][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 01:59:20,045][233954] FPS: 423693.77[0m
[36m[2023-07-11 01:59:20,047][233954] itr=154, itrs=2000, Progress: 7.70%[0m
[36m[2023-07-11 01:59:31,628][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 01:59:31,628][233954] FPS: 332714.05[0m
[36m[2023-07-11 01:59:35,823][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:59:35,824][233954] Reward + Measures: [[5257.6699419     0.15576567    0.23957732    0.172748      0.03587667
     1.36062908]][0m
[37m[1m[2023-07-11 01:59:35,824][233954] Max Reward on eval: 5257.669941896611[0m
[37m[1m[2023-07-11 01:59:35,824][233954] Min Reward on eval: 5257.669941896611[0m
[37m[1m[2023-07-11 01:59:35,824][233954] Mean Reward across all agents: 5257.669941896611[0m
[37m[1m[2023-07-11 01:59:35,825][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:59:41,010][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 01:59:41,011][233954] Reward + Measures: [[ 25.25053313   0.3915       0.38930002   0.4059       0.44000003
    3.16228056]
 [ 28.10025801   0.0228       0.44750005   0.46230003   0.55500001
    3.30542684]
 [ 16.32491425   0.2323       0.123        0.28340003   0.1305
    3.51888251]
 ...
 [ 70.44108675   0.0039       0.70069999   0.73410004   0.80419999
    3.232476  ]
 [-67.61239344   0.49200001   0.21869998   0.56160003   0.20640002
    3.5904038 ]
 [132.60162799   0.0197       0.61129999   0.60129994   0.60710001
    3.55123973]][0m
[37m[1m[2023-07-11 01:59:41,011][233954] Max Reward on eval: 535.9629774167203[0m
[37m[1m[2023-07-11 01:59:41,011][233954] Min Reward on eval: -232.3647448518779[0m
[37m[1m[2023-07-11 01:59:41,011][233954] Mean Reward across all agents: 13.964897450023688[0m
[37m[1m[2023-07-11 01:59:41,011][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 01:59:41,020][233954] mean_value=22.591781701874414, max_value=707.980941808112[0m
[37m[1m[2023-07-11 01:59:41,023][233954] New mean coefficients: [[ 0.8038857 -2.6382742  5.1256914  7.6219416 -5.353687  -3.7553887]][0m
[37m[1m[2023-07-11 01:59:41,024][233954] Moving the mean solution point...[0m
[36m[2023-07-11 01:59:50,053][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 01:59:50,054][233954] FPS: 425345.68[0m
[36m[2023-07-11 01:59:50,056][233954] itr=155, itrs=2000, Progress: 7.75%[0m
[36m[2023-07-11 02:00:01,993][233954] train() took 11.90 seconds to complete[0m
[36m[2023-07-11 02:00:01,994][233954] FPS: 322656.78[0m
[36m[2023-07-11 02:00:06,325][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:00:06,326][233954] Reward + Measures: [[2879.66804001    0.16903932    0.27976599    0.19225931    0.07697867
     1.25467443]][0m
[37m[1m[2023-07-11 02:00:06,326][233954] Max Reward on eval: 2879.6680400100686[0m
[37m[1m[2023-07-11 02:00:06,326][233954] Min Reward on eval: 2879.6680400100686[0m
[37m[1m[2023-07-11 02:00:06,327][233954] Mean Reward across all agents: 2879.6680400100686[0m
[37m[1m[2023-07-11 02:00:06,327][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:00:11,319][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:00:11,320][233954] Reward + Measures: [[-39.60980348   0.0682       0.0802       0.0927       0.09600001
    3.47219253]
 [ 41.58684203   0.21329999   0.1691       0.24190001   0.148
    3.33060622]
 [ -8.34782219   0.22309999   0.40970001   0.24990001   0.36559999
    2.94032383]
 ...
 [ 47.28217673   0.19100001   0.1311       0.1877       0.12890001
    2.84198475]
 [-82.05080188   0.33940002   0.2976       0.3427       0.08880001
    3.02197623]
 [-97.06104556   0.26790002   0.21090002   0.24170001   0.17200001
    3.06840944]][0m
[37m[1m[2023-07-11 02:00:11,320][233954] Max Reward on eval: 664.8523893609643[0m
[37m[1m[2023-07-11 02:00:11,320][233954] Min Reward on eval: -190.74288298338652[0m
[37m[1m[2023-07-11 02:00:11,321][233954] Mean Reward across all agents: 54.256569840018905[0m
[37m[1m[2023-07-11 02:00:11,321][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:00:11,327][233954] mean_value=-77.91632284869925, max_value=680.4590987522155[0m
[37m[1m[2023-07-11 02:00:11,329][233954] New mean coefficients: [[ 1.1476984  -0.5730362   8.048679    4.9659476  -0.69147205 -2.5492992 ]][0m
[37m[1m[2023-07-11 02:00:11,330][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:00:20,404][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 02:00:20,404][233954] FPS: 423277.25[0m
[36m[2023-07-11 02:00:20,407][233954] itr=156, itrs=2000, Progress: 7.80%[0m
[36m[2023-07-11 02:00:32,050][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 02:00:32,051][233954] FPS: 330833.63[0m
[36m[2023-07-11 02:00:36,332][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:00:36,333][233954] Reward + Measures: [[3855.29866252    0.17181401    0.283764      0.19259033    0.055626
     1.26207376]][0m
[37m[1m[2023-07-11 02:00:36,333][233954] Max Reward on eval: 3855.298662524963[0m
[37m[1m[2023-07-11 02:00:36,333][233954] Min Reward on eval: 3855.298662524963[0m
[37m[1m[2023-07-11 02:00:36,334][233954] Mean Reward across all agents: 3855.298662524963[0m
[37m[1m[2023-07-11 02:00:36,334][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:00:41,368][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:00:41,368][233954] Reward + Measures: [[ 15.38319656   0.47520003   0.48030001   0.36760002   0.42599997
    3.52566767]
 [ 34.0800911    0.44319996   0.46570006   0.36980003   0.39949998
    3.2876153 ]
 [-66.38948347   0.72960001   0.1109       0.75870001   0.38970003
    3.35932279]
 ...
 [ -3.00882344   0.29130003   0.1578       0.30809999   0.2599
    3.42210317]
 [ 19.22590489   0.47869998   0.15290001   0.47890002   0.27719998
    3.19445848]
 [ 26.68250576   0.44109997   0.0786       0.42799997   0.26120001
    3.65126348]][0m
[37m[1m[2023-07-11 02:00:41,368][233954] Max Reward on eval: 317.2834815571085[0m
[37m[1m[2023-07-11 02:00:41,369][233954] Min Reward on eval: -237.04596775695683[0m
[37m[1m[2023-07-11 02:00:41,369][233954] Mean Reward across all agents: 27.19297807630288[0m
[37m[1m[2023-07-11 02:00:41,369][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:00:41,378][233954] mean_value=36.10308231255857, max_value=636.6754192169489[0m
[37m[1m[2023-07-11 02:00:41,381][233954] New mean coefficients: [[ 0.880361  -0.8119843  6.253721   6.942852  -3.3329496 -3.5367794]][0m
[37m[1m[2023-07-11 02:00:41,382][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:00:50,437][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 02:00:50,437][233954] FPS: 424172.47[0m
[36m[2023-07-11 02:00:50,439][233954] itr=157, itrs=2000, Progress: 7.85%[0m
[36m[2023-07-11 02:01:02,495][233954] train() took 12.02 seconds to complete[0m
[36m[2023-07-11 02:01:02,500][233954] FPS: 319470.14[0m
[36m[2023-07-11 02:01:06,768][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:01:06,768][233954] Reward + Measures: [[3797.85002817    0.16134967    0.27596566    0.17652665    0.043338
     1.18875134]][0m
[37m[1m[2023-07-11 02:01:06,768][233954] Max Reward on eval: 3797.8500281706415[0m
[37m[1m[2023-07-11 02:01:06,769][233954] Min Reward on eval: 3797.8500281706415[0m
[37m[1m[2023-07-11 02:01:06,769][233954] Mean Reward across all agents: 3797.8500281706415[0m
[37m[1m[2023-07-11 02:01:06,769][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:01:11,800][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:01:11,801][233954] Reward + Measures: [[ -1.3651559    0.37369999   0.46110001   0.1175       0.39669999
    2.76129127]
 [ 54.98249826   0.2218       0.17400001   0.18530001   0.20550001
    3.27638364]
 [125.27046342   0.24630001   0.28220001   0.16510001   0.2447
    3.05438805]
 ...
 [ 36.23953546   0.30320001   0.36129999   0.17830001   0.29280004
    3.11586761]
 [146.25801918   0.38140002   0.51659995   0.3391       0.4172
    2.76493359]
 [103.46095014   0.45919999   0.39209998   0.42120001   0.36649999
    2.91342807]][0m
[37m[1m[2023-07-11 02:01:11,801][233954] Max Reward on eval: 573.025811672723[0m
[37m[1m[2023-07-11 02:01:11,801][233954] Min Reward on eval: -146.79882228597998[0m
[37m[1m[2023-07-11 02:01:11,802][233954] Mean Reward across all agents: 73.87004976429932[0m
[37m[1m[2023-07-11 02:01:11,802][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:01:11,806][233954] mean_value=-114.6634790452693, max_value=555.1342828266695[0m
[37m[1m[2023-07-11 02:01:11,808][233954] New mean coefficients: [[ 0.739956  -0.5636488  6.4830403  7.1558175 -3.236082  -3.6628613]][0m
[37m[1m[2023-07-11 02:01:11,809][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:01:20,874][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 02:01:20,874][233954] FPS: 423701.23[0m
[36m[2023-07-11 02:01:20,876][233954] itr=158, itrs=2000, Progress: 7.90%[0m
[36m[2023-07-11 02:01:32,586][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 02:01:32,586][233954] FPS: 329046.64[0m
[36m[2023-07-11 02:01:36,963][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:01:36,963][233954] Reward + Measures: [[4670.48953424    0.17355368    0.30116633    0.18561833    0.03173367
     1.22882926]][0m
[37m[1m[2023-07-11 02:01:36,963][233954] Max Reward on eval: 4670.489534243004[0m
[37m[1m[2023-07-11 02:01:36,964][233954] Min Reward on eval: 4670.489534243004[0m
[37m[1m[2023-07-11 02:01:36,964][233954] Mean Reward across all agents: 4670.489534243004[0m
[37m[1m[2023-07-11 02:01:36,964][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:01:41,972][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:01:41,977][233954] Reward + Measures: [[-50.70841314   0.30720001   0.35709998   0.1665       0.40719995
    3.04308176]
 [ 75.11890699   0.2244       0.17920001   0.2339       0.12160001
    3.30284548]
 [ 52.82414842   0.27250001   0.19679998   0.2395       0.14120001
    3.1024704 ]
 ...
 [ 38.09335615   0.35450003   0.27870002   0.34830004   0.2122
    2.83923483]
 [-17.4248871    0.31299999   0.1964       0.2498       0.12080001
    3.22718692]
 [116.37902349   0.49649999   0.2185       0.44060001   0.19090001
    3.29024744]][0m
[37m[1m[2023-07-11 02:01:41,978][233954] Max Reward on eval: 322.4368800543249[0m
[37m[1m[2023-07-11 02:01:41,978][233954] Min Reward on eval: -250.7283461280167[0m
[37m[1m[2023-07-11 02:01:41,978][233954] Mean Reward across all agents: 3.74613988548035[0m
[37m[1m[2023-07-11 02:01:41,979][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:01:41,984][233954] mean_value=-64.82697082449776, max_value=564.0919895756524[0m
[37m[1m[2023-07-11 02:01:41,987][233954] New mean coefficients: [[ 0.6360092 -0.5755186  6.5352707  7.4673944 -3.4537783 -3.0766447]][0m
[37m[1m[2023-07-11 02:01:41,988][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:01:50,969][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 02:01:50,970][233954] FPS: 427625.44[0m
[36m[2023-07-11 02:01:50,972][233954] itr=159, itrs=2000, Progress: 7.95%[0m
[36m[2023-07-11 02:02:02,546][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 02:02:02,546][233954] FPS: 332904.51[0m
[36m[2023-07-11 02:02:06,829][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:02:06,829][233954] Reward + Measures: [[5761.11034575    0.187406      0.338211      0.19587599    0.01829367
     1.2954843 ]][0m
[37m[1m[2023-07-11 02:02:06,829][233954] Max Reward on eval: 5761.110345749556[0m
[37m[1m[2023-07-11 02:02:06,830][233954] Min Reward on eval: 5761.110345749556[0m
[37m[1m[2023-07-11 02:02:06,830][233954] Mean Reward across all agents: 5761.110345749556[0m
[37m[1m[2023-07-11 02:02:06,830][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:02:11,979][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:02:11,979][233954] Reward + Measures: [[ 227.65913866    0.16060001    0.4923        0.28830001    0.45860001
     3.01803064]
 [ -20.2722761     0.1804        0.41009998    0.163         0.41090003
     3.64420748]
 [  21.79122919    0.22830001    0.47490001    0.2441        0.43630001
     3.3516202 ]
 ...
 [-145.87229296    0.4522        0.63859999    0.43700001    0.53439999
     3.30986714]
 [ -28.05626961    0.41590005    0.3335        0.3563        0.2307
     3.06885839]
 [  26.49440957    0.18780002    0.41009998    0.1965        0.40500003
     3.21531725]][0m
[37m[1m[2023-07-11 02:02:11,980][233954] Max Reward on eval: 518.9320831598714[0m
[37m[1m[2023-07-11 02:02:11,980][233954] Min Reward on eval: -241.8578691288829[0m
[37m[1m[2023-07-11 02:02:11,980][233954] Mean Reward across all agents: 35.15857118875087[0m
[37m[1m[2023-07-11 02:02:11,980][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:02:11,986][233954] mean_value=-87.2164545907881, max_value=584.0463899538561[0m
[37m[1m[2023-07-11 02:02:11,989][233954] New mean coefficients: [[ 1.338605   0.8950615 10.48259    1.3704886  4.361247  -1.1921405]][0m
[37m[1m[2023-07-11 02:02:11,990][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:02:21,102][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 02:02:21,103][233954] FPS: 421484.05[0m
[36m[2023-07-11 02:02:21,105][233954] itr=160, itrs=2000, Progress: 8.00%[0m
[37m[1m[2023-07-11 02:04:50,344][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000140[0m
[36m[2023-07-11 02:05:02,783][233954] train() took 11.86 seconds to complete[0m
[36m[2023-07-11 02:05:02,783][233954] FPS: 323848.15[0m
[36m[2023-07-11 02:05:07,149][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:05:07,149][233954] Reward + Measures: [[6129.1368913     0.189634      0.34916064    0.19470866    0.013488
     1.31446898]][0m
[37m[1m[2023-07-11 02:05:07,149][233954] Max Reward on eval: 6129.136891298499[0m
[37m[1m[2023-07-11 02:05:07,150][233954] Min Reward on eval: 6129.136891298499[0m
[37m[1m[2023-07-11 02:05:07,150][233954] Mean Reward across all agents: 6129.136891298499[0m
[37m[1m[2023-07-11 02:05:07,150][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:05:12,142][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:05:12,142][233954] Reward + Measures: [[  9.7980655    0.1086       0.0905       0.13130002   0.1006
    3.37275004]
 [101.88501473   0.44099998   0.0514       0.46940002   0.29899999
    3.42116547]
 [ -3.8862715    0.38510001   0.18350001   0.4127       0.15360001
    3.33515477]
 ...
 [ 91.80012723   0.1627       0.1542       0.23860002   0.0983
    2.96573305]
 [ 41.54566505   0.23980001   0.19050001   0.27809998   0.24950002
    3.03678989]
 [ 79.93628999   0.28920001   0.36340001   0.29710001   0.3346
    3.31691217]][0m
[37m[1m[2023-07-11 02:05:12,143][233954] Max Reward on eval: 994.4555692688097[0m
[37m[1m[2023-07-11 02:05:12,143][233954] Min Reward on eval: -197.49738377630712[0m
[37m[1m[2023-07-11 02:05:12,143][233954] Mean Reward across all agents: 57.25841645623264[0m
[37m[1m[2023-07-11 02:05:12,143][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:05:12,148][233954] mean_value=-110.97538979017244, max_value=647.4892536420375[0m
[37m[1m[2023-07-11 02:05:12,150][233954] New mean coefficients: [[ 0.8082566   0.3017236   7.383356    4.8469434   0.36520338 -2.02313   ]][0m
[37m[1m[2023-07-11 02:05:12,151][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:05:21,195][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 02:05:21,195][233954] FPS: 424683.96[0m
[36m[2023-07-11 02:05:21,197][233954] itr=161, itrs=2000, Progress: 8.05%[0m
[36m[2023-07-11 02:05:32,752][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 02:05:32,752][233954] FPS: 333420.28[0m
[36m[2023-07-11 02:05:37,007][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:05:37,007][233954] Reward + Measures: [[6611.43557958    0.19269434    0.36390835    0.19430499    0.00893467
     1.33408201]][0m
[37m[1m[2023-07-11 02:05:37,008][233954] Max Reward on eval: 6611.435579578784[0m
[37m[1m[2023-07-11 02:05:37,008][233954] Min Reward on eval: 6611.435579578784[0m
[37m[1m[2023-07-11 02:05:37,008][233954] Mean Reward across all agents: 6611.435579578784[0m
[37m[1m[2023-07-11 02:05:37,008][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:05:41,936][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:05:41,936][233954] Reward + Measures: [[  -4.12202008    0.37820002    0.155         0.37310001    0.19020002
     3.13719511]
 [ 104.1878035     0.19759999    0.39879999    0.16559999    0.3989
     2.81560206]
 [   9.03001496    0.1928        0.1035        0.1883        0.1122
     2.99125361]
 ...
 [ 203.6284723     0.17940001    0.27779999    0.33259997    0.38730001
     2.40956283]
 [-209.09352952    0.10470001    0.41339999    0.27579999    0.3944
     3.27770591]
 [-100.48179247    0.15220001    0.35820001    0.22879998    0.32050002
     2.74284649]][0m
[37m[1m[2023-07-11 02:05:41,937][233954] Max Reward on eval: 670.7966003295035[0m
[37m[1m[2023-07-11 02:05:41,937][233954] Min Reward on eval: -420.0400181253441[0m
[37m[1m[2023-07-11 02:05:41,937][233954] Mean Reward across all agents: 12.637663081400715[0m
[37m[1m[2023-07-11 02:05:41,938][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:05:41,941][233954] mean_value=-169.1438301293383, max_value=791.0150117803365[0m
[37m[1m[2023-07-11 02:05:41,944][233954] New mean coefficients: [[ 1.3787384 -0.4896744  9.409012   1.7252629  3.388321  -1.8462576]][0m
[37m[1m[2023-07-11 02:05:41,945][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:05:50,924][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 02:05:50,924][233954] FPS: 427761.25[0m
[36m[2023-07-11 02:05:50,926][233954] itr=162, itrs=2000, Progress: 8.10%[0m
[36m[2023-07-11 02:06:02,513][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 02:06:02,513][233954] FPS: 332541.77[0m
[36m[2023-07-11 02:06:06,805][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:06:06,805][233954] Reward + Measures: [[6880.52748159    0.19192301    0.36981168    0.19190267    0.007294
     1.33506036]][0m
[37m[1m[2023-07-11 02:06:06,805][233954] Max Reward on eval: 6880.527481592596[0m
[37m[1m[2023-07-11 02:06:06,806][233954] Min Reward on eval: 6880.527481592596[0m
[37m[1m[2023-07-11 02:06:06,806][233954] Mean Reward across all agents: 6880.527481592596[0m
[37m[1m[2023-07-11 02:06:06,806][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:06:11,802][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:06:11,802][233954] Reward + Measures: [[  48.69778445    0.1209        0.15630001    0.1517        0.114
     3.5950048 ]
 [-145.69424689    0.65249997    0.5765        0.67219996    0.17989999
     3.43271065]
 [  33.12815756    0.18390001    0.18100002    0.19410001    0.16370001
     3.44423556]
 ...
 [  15.70736679    0.21080001    0.16070001    0.1847        0.14430001
     3.48722267]
 [  17.069564      0.28800002    0.31359997    0.15870002    0.24419999
     3.37583923]
 [  22.07872059    0.1145        0.12379999    0.0882        0.0705
     3.63655353]][0m
[37m[1m[2023-07-11 02:06:11,803][233954] Max Reward on eval: 411.3949272546917[0m
[37m[1m[2023-07-11 02:06:11,803][233954] Min Reward on eval: -368.26522778486833[0m
[37m[1m[2023-07-11 02:06:11,803][233954] Mean Reward across all agents: 38.28021642446652[0m
[37m[1m[2023-07-11 02:06:11,803][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:06:11,810][233954] mean_value=-35.19685601402137, max_value=745.644581823051[0m
[37m[1m[2023-07-11 02:06:11,813][233954] New mean coefficients: [[ 0.8046748 -1.7920592  6.2908382  6.804699  -2.7736378 -3.181685 ]][0m
[37m[1m[2023-07-11 02:06:11,814][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:06:20,870][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 02:06:20,871][233954] FPS: 424080.44[0m
[36m[2023-07-11 02:06:20,873][233954] itr=163, itrs=2000, Progress: 8.15%[0m
[36m[2023-07-11 02:06:32,556][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 02:06:32,556][233954] FPS: 329802.22[0m
[36m[2023-07-11 02:06:36,892][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:06:36,892][233954] Reward + Measures: [[7109.56777216    0.19205001    0.38085133    0.193112      0.006208
     1.33731914]][0m
[37m[1m[2023-07-11 02:06:36,892][233954] Max Reward on eval: 7109.567772161932[0m
[37m[1m[2023-07-11 02:06:36,893][233954] Min Reward on eval: 7109.567772161932[0m
[37m[1m[2023-07-11 02:06:36,893][233954] Mean Reward across all agents: 7109.567772161932[0m
[37m[1m[2023-07-11 02:06:36,893][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:06:41,898][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:06:41,899][233954] Reward + Measures: [[ 80.12117409   0.1256       0.46610004   0.30749997   0.48210001
    3.22515035]
 [ 72.69218479   0.1287       0.38600001   0.30850002   0.36660001
    3.1883285 ]
 [ 57.32974946   0.24679999   0.50159997   0.10699999   0.45859995
    2.71371508]
 ...
 [  8.04825698   0.20580001   0.3177       0.1059       0.31130001
    3.07713246]
 [ 51.88892172   0.26500002   0.5341       0.0652       0.69340008
    3.09672308]
 [156.17814611   0.1283       0.25799999   0.13680001   0.2536
    2.93155551]][0m
[37m[1m[2023-07-11 02:06:41,899][233954] Max Reward on eval: 558.0728988591582[0m
[37m[1m[2023-07-11 02:06:41,899][233954] Min Reward on eval: -76.9177765449509[0m
[37m[1m[2023-07-11 02:06:41,899][233954] Mean Reward across all agents: 81.14554451628544[0m
[37m[1m[2023-07-11 02:06:41,900][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:06:41,905][233954] mean_value=-22.257650325057682, max_value=579.3748474420514[0m
[37m[1m[2023-07-11 02:06:41,908][233954] New mean coefficients: [[ 0.6802163 -1.8583539  5.919733   7.7934375 -3.8401973 -3.4258204]][0m
[37m[1m[2023-07-11 02:06:41,909][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:06:50,932][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 02:06:50,932][233954] FPS: 425667.82[0m
[36m[2023-07-11 02:06:50,934][233954] itr=164, itrs=2000, Progress: 8.20%[0m
[36m[2023-07-11 02:07:02,738][233954] train() took 11.76 seconds to complete[0m
[36m[2023-07-11 02:07:02,738][233954] FPS: 326404.42[0m
[36m[2023-07-11 02:07:07,024][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:07:07,025][233954] Reward + Measures: [[7258.73442343    0.18911099    0.40182298    0.19436701    0.00496333
     1.30927908]][0m
[37m[1m[2023-07-11 02:07:07,025][233954] Max Reward on eval: 7258.734423433825[0m
[37m[1m[2023-07-11 02:07:07,025][233954] Min Reward on eval: 7258.734423433825[0m
[37m[1m[2023-07-11 02:07:07,025][233954] Mean Reward across all agents: 7258.734423433825[0m
[37m[1m[2023-07-11 02:07:07,026][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:07:12,140][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:07:12,140][233954] Reward + Measures: [[ 44.61394817   0.19970001   0.27130002   0.18910001   0.26359999
    2.80512142]
 [129.8271513    0.19399999   0.1657       0.21460001   0.2
    2.84611869]
 [ 45.10123761   0.21960001   0.25300002   0.1191       0.27490002
    3.28982329]
 ...
 [100.30356018   0.1058       0.0914       0.14070001   0.0995
    3.07742023]
 [  0.83049923   0.1829       0.2263       0.2323       0.25099999
    2.93722725]
 [  2.57566369   0.2529       0.20549999   0.30850002   0.2376
    3.51661849]][0m
[37m[1m[2023-07-11 02:07:12,140][233954] Max Reward on eval: 343.50057362440276[0m
[37m[1m[2023-07-11 02:07:12,141][233954] Min Reward on eval: -141.73293983640616[0m
[37m[1m[2023-07-11 02:07:12,141][233954] Mean Reward across all agents: 46.77111829130255[0m
[37m[1m[2023-07-11 02:07:12,141][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:07:12,145][233954] mean_value=-90.64839336612718, max_value=621.2851615272463[0m
[37m[1m[2023-07-11 02:07:12,148][233954] New mean coefficients: [[-0.04813141 -2.5542111   2.332491   11.383447   -8.788201   -4.487346  ]][0m
[37m[1m[2023-07-11 02:07:12,149][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:07:21,128][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 02:07:21,129][233954] FPS: 427726.67[0m
[36m[2023-07-11 02:07:21,131][233954] itr=165, itrs=2000, Progress: 8.25%[0m
[36m[2023-07-11 02:07:32,846][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 02:07:32,846][233954] FPS: 328893.54[0m
[36m[2023-07-11 02:07:37,136][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:07:37,136][233954] Reward + Measures: [[7185.30476433    0.18433332    0.38814771    0.19142768    0.005132
     1.24473763]][0m
[37m[1m[2023-07-11 02:07:37,136][233954] Max Reward on eval: 7185.304764332786[0m
[37m[1m[2023-07-11 02:07:37,137][233954] Min Reward on eval: 7185.304764332786[0m
[37m[1m[2023-07-11 02:07:37,137][233954] Mean Reward across all agents: 7185.304764332786[0m
[37m[1m[2023-07-11 02:07:37,137][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:07:42,148][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:07:42,149][233954] Reward + Measures: [[ 43.34954857   0.1859       0.1734       0.1522       0.14360002
    3.25638747]
 [118.37042886   0.08970001   0.2          0.11900001   0.153
    3.03883314]
 [ 82.10128116   0.1779       0.28389999   0.24399999   0.33559999
    2.98737836]
 ...
 [ 17.44476959   0.15549999   0.20060001   0.18719999   0.21689999
    3.24598742]
 [-26.61164002   0.1036       0.0875       0.0855       0.12550001
    3.55527687]
 [ 33.95740692   0.15450001   0.22430001   0.1472       0.25530002
    3.19022822]][0m
[37m[1m[2023-07-11 02:07:42,149][233954] Max Reward on eval: 298.5409938832396[0m
[37m[1m[2023-07-11 02:07:42,150][233954] Min Reward on eval: -94.71677577719092[0m
[37m[1m[2023-07-11 02:07:42,150][233954] Mean Reward across all agents: 53.77378819603357[0m
[37m[1m[2023-07-11 02:07:42,150][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:07:42,153][233954] mean_value=-119.40466664409033, max_value=466.7169303072072[0m
[37m[1m[2023-07-11 02:07:42,155][233954] New mean coefficients: [[-0.14024271 -3.2522347   1.1634607  11.438434   -9.649305   -5.100915  ]][0m
[37m[1m[2023-07-11 02:07:42,156][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:07:51,163][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 02:07:51,164][233954] FPS: 426420.34[0m
[36m[2023-07-11 02:07:51,166][233954] itr=166, itrs=2000, Progress: 8.30%[0m
[36m[2023-07-11 02:08:02,743][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 02:08:02,743][233954] FPS: 332829.09[0m
[36m[2023-07-11 02:08:07,027][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:08:07,027][233954] Reward + Measures: [[6911.54318899    0.17994566    0.39255866    0.19005333    0.00570333
     1.17686653]][0m
[37m[1m[2023-07-11 02:08:07,027][233954] Max Reward on eval: 6911.543188990658[0m
[37m[1m[2023-07-11 02:08:07,028][233954] Min Reward on eval: 6911.543188990658[0m
[37m[1m[2023-07-11 02:08:07,028][233954] Mean Reward across all agents: 6911.543188990658[0m
[37m[1m[2023-07-11 02:08:07,028][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:08:12,018][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:08:12,019][233954] Reward + Measures: [[-59.25494436   0.0873       0.3962       0.32010001   0.36750004
    3.66792083]
 [-18.17289341   0.23720001   0.3132       0.35080001   0.1982
    2.92009425]
 [ 81.6333379    0.1046       0.25600001   0.2033       0.20909999
    3.458606  ]
 ...
 [ 89.16748272   0.0941       0.23189998   0.19599999   0.18620001
    3.45834136]
 [186.61462643   0.0903       0.37639999   0.26289999   0.31569999
    3.38250232]
 [ -1.10893482   0.17090002   0.39270002   0.37200001   0.43959999
    2.77453876]][0m
[37m[1m[2023-07-11 02:08:12,019][233954] Max Reward on eval: 352.2840976893902[0m
[37m[1m[2023-07-11 02:08:12,019][233954] Min Reward on eval: -262.48504736609755[0m
[37m[1m[2023-07-11 02:08:12,020][233954] Mean Reward across all agents: 8.643322646131487[0m
[37m[1m[2023-07-11 02:08:12,020][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:08:12,024][233954] mean_value=-104.9891793884057, max_value=693.4020023398101[0m
[37m[1m[2023-07-11 02:08:12,027][233954] New mean coefficients: [[-0.10038441 -1.4214506   1.9910269   8.963463   -6.283164   -4.029774  ]][0m
[37m[1m[2023-07-11 02:08:12,028][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:08:21,005][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 02:08:21,005][233954] FPS: 427826.61[0m
[36m[2023-07-11 02:08:21,008][233954] itr=167, itrs=2000, Progress: 8.35%[0m
[36m[2023-07-11 02:08:32,851][233954] train() took 11.81 seconds to complete[0m
[36m[2023-07-11 02:08:32,851][233954] FPS: 325256.17[0m
[36m[2023-07-11 02:08:37,199][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:08:37,199][233954] Reward + Measures: [[5785.42799431    0.167945      0.36237565    0.18048866    0.012881
     1.05042005]][0m
[37m[1m[2023-07-11 02:08:37,199][233954] Max Reward on eval: 5785.427994314956[0m
[37m[1m[2023-07-11 02:08:37,200][233954] Min Reward on eval: 5785.427994314956[0m
[37m[1m[2023-07-11 02:08:37,200][233954] Mean Reward across all agents: 5785.427994314956[0m
[37m[1m[2023-07-11 02:08:37,200][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:08:42,263][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:08:42,269][233954] Reward + Measures: [[ 178.08460412    0.3423        0.71160001    0.3319        0.72650003
     3.25313926]
 [  40.47267367    0.0204        0.91949999    0.81659997    0.92980003
     3.50563931]
 [ -40.39090122    0.18190001    0.1027        0.22060001    0.15640001
     3.57755899]
 ...
 [  71.1484973     0.0921        0.21659999    0.1433        0.20809999
     3.45789576]
 [-126.24588624    0.12819999    0.0948        0.19          0.1664
     3.65706372]
 [ -21.41041609    0.24010001    0.26519999    0.22189999    0.31210002
     2.74789119]][0m
[37m[1m[2023-07-11 02:08:42,270][233954] Max Reward on eval: 299.8962860336527[0m
[37m[1m[2023-07-11 02:08:42,271][233954] Min Reward on eval: -375.43625389151276[0m
[37m[1m[2023-07-11 02:08:42,271][233954] Mean Reward across all agents: -14.815060851016606[0m
[37m[1m[2023-07-11 02:08:42,272][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:08:42,285][233954] mean_value=-24.314694423741386, max_value=619.5356048643962[0m
[37m[1m[2023-07-11 02:08:42,289][233954] New mean coefficients: [[ 0.15207207 -0.1898464   3.1375365   8.324271   -3.9785247  -3.2664492 ]][0m
[37m[1m[2023-07-11 02:08:42,290][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:08:51,383][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 02:08:51,383][233954] FPS: 422392.65[0m
[36m[2023-07-11 02:08:51,386][233954] itr=168, itrs=2000, Progress: 8.40%[0m
[36m[2023-07-11 02:09:03,152][233954] train() took 11.73 seconds to complete[0m
[36m[2023-07-11 02:09:03,152][233954] FPS: 327352.28[0m
[36m[2023-07-11 02:09:07,434][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:09:07,435][233954] Reward + Measures: [[5828.3598168     0.17804433    0.41482732    0.18966301    0.00946867
     1.08995473]][0m
[37m[1m[2023-07-11 02:09:07,435][233954] Max Reward on eval: 5828.3598168006465[0m
[37m[1m[2023-07-11 02:09:07,435][233954] Min Reward on eval: 5828.3598168006465[0m
[37m[1m[2023-07-11 02:09:07,435][233954] Mean Reward across all agents: 5828.3598168006465[0m
[37m[1m[2023-07-11 02:09:07,435][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:09:12,450][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:09:12,450][233954] Reward + Measures: [[-77.84378819   0.2527       0.37100002   0.2375       0.36700001
    2.52938056]
 [-76.30794862   0.24860001   0.39359999   0.30289999   0.35440001
    3.08732986]
 [-19.94860516   0.13609999   0.1559       0.12720001   0.14
    3.29964375]
 ...
 [254.16737661   0.15710001   0.20130001   0.15009999   0.14750002
    2.99690008]
 [ 98.19913071   0.30520001   0.3867       0.12910001   0.31979999
    2.74948931]
 [ 54.7533462    0.12460001   0.25929999   0.17470001   0.1935
    3.15618873]][0m
[37m[1m[2023-07-11 02:09:12,451][233954] Max Reward on eval: 422.49914456307886[0m
[37m[1m[2023-07-11 02:09:12,451][233954] Min Reward on eval: -273.54006756376475[0m
[37m[1m[2023-07-11 02:09:12,451][233954] Mean Reward across all agents: 31.948627497594554[0m
[37m[1m[2023-07-11 02:09:12,451][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:09:12,456][233954] mean_value=-185.18388988779938, max_value=653.4816648826562[0m
[37m[1m[2023-07-11 02:09:12,459][233954] New mean coefficients: [[ 0.38952002  1.4032207   5.868317    4.962982    0.22255468 -2.1689048 ]][0m
[37m[1m[2023-07-11 02:09:12,460][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:09:21,415][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 02:09:21,416][233954] FPS: 428869.77[0m
[36m[2023-07-11 02:09:21,418][233954] itr=169, itrs=2000, Progress: 8.45%[0m
[36m[2023-07-11 02:09:32,997][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 02:09:32,997][233954] FPS: 332701.65[0m
[36m[2023-07-11 02:09:37,233][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:09:37,234][233954] Reward + Measures: [[5983.31582406    0.18314634    0.45121899    0.19502866    0.00773233
     1.09852147]][0m
[37m[1m[2023-07-11 02:09:37,234][233954] Max Reward on eval: 5983.315824062199[0m
[37m[1m[2023-07-11 02:09:37,234][233954] Min Reward on eval: 5983.315824062199[0m
[37m[1m[2023-07-11 02:09:37,234][233954] Mean Reward across all agents: 5983.315824062199[0m
[37m[1m[2023-07-11 02:09:37,235][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:09:42,364][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:09:42,365][233954] Reward + Measures: [[ 80.73514755   0.08900001   0.0865       0.0881       0.0867
    3.4262588 ]
 [  3.41727462   0.3048       0.29270002   0.26989999   0.31870005
    2.95000792]
 [167.28303337   0.1243       0.1701       0.15100001   0.1514
    2.88105369]
 ...
 [-31.37524079   0.1437       0.2139       0.13759999   0.18569998
    3.39319158]
 [-12.15353541   0.30720001   0.4382       0.31580004   0.4165
    2.244241  ]
 [108.26140124   0.1184       0.0707       0.0896       0.1115
    2.85797095]][0m
[37m[1m[2023-07-11 02:09:42,365][233954] Max Reward on eval: 456.4019059866667[0m
[37m[1m[2023-07-11 02:09:42,365][233954] Min Reward on eval: -222.6116049192846[0m
[37m[1m[2023-07-11 02:09:42,366][233954] Mean Reward across all agents: 49.47639513624398[0m
[37m[1m[2023-07-11 02:09:42,366][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:09:42,369][233954] mean_value=-192.4924925972484, max_value=496.0323990388494[0m
[37m[1m[2023-07-11 02:09:42,371][233954] New mean coefficients: [[ 0.48719394  2.3772383   6.883889    2.762385    3.104059   -1.5363263 ]][0m
[37m[1m[2023-07-11 02:09:42,372][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:09:51,365][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 02:09:51,365][233954] FPS: 427088.86[0m
[36m[2023-07-11 02:09:51,367][233954] itr=170, itrs=2000, Progress: 8.50%[0m
[37m[1m[2023-07-11 02:12:15,812][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000150[0m
[36m[2023-07-11 02:12:28,091][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 02:12:28,092][233954] FPS: 329222.84[0m
[36m[2023-07-11 02:12:32,371][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:12:32,372][233954] Reward + Measures: [[6181.21436997    0.18763566    0.48762035    0.197906      0.007142
     1.11517429]][0m
[37m[1m[2023-07-11 02:12:32,372][233954] Max Reward on eval: 6181.214369974384[0m
[37m[1m[2023-07-11 02:12:32,372][233954] Min Reward on eval: 6181.214369974384[0m
[37m[1m[2023-07-11 02:12:32,372][233954] Mean Reward across all agents: 6181.214369974384[0m
[37m[1m[2023-07-11 02:12:32,373][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:12:37,345][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:12:37,346][233954] Reward + Measures: [[ 28.76420263   0.34960002   0.27509999   0.3312       0.20720001
    3.38593793]
 [-18.47359777   0.08100001   0.16580001   0.11700001   0.0783
    3.39156508]
 [ 93.14319798   0.16340001   0.2467       0.18460001   0.1717
    2.92999983]
 ...
 [ 67.1425912    0.317        0.44769999   0.41739997   0.12590002
    3.46106339]
 [ 58.97067343   0.1212       0.14909999   0.17569999   0.133
    3.32195663]
 [ -7.72827786   0.47679996   0.1499       0.46070001   0.37530002
    2.93152118]][0m
[37m[1m[2023-07-11 02:12:37,346][233954] Max Reward on eval: 449.85404298320645[0m
[37m[1m[2023-07-11 02:12:37,347][233954] Min Reward on eval: -232.7339867725037[0m
[37m[1m[2023-07-11 02:12:37,347][233954] Mean Reward across all agents: 50.789932739912224[0m
[37m[1m[2023-07-11 02:12:37,347][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:12:37,353][233954] mean_value=-71.05362794666821, max_value=618.4842693877872[0m
[37m[1m[2023-07-11 02:12:37,356][233954] New mean coefficients: [[ 0.42526942  1.5184423   5.3828697   5.045994    0.5122411  -1.9426178 ]][0m
[37m[1m[2023-07-11 02:12:37,357][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:12:46,260][233954] train() took 8.90 seconds to complete[0m
[36m[2023-07-11 02:12:46,260][233954] FPS: 431383.92[0m
[36m[2023-07-11 02:12:46,262][233954] itr=171, itrs=2000, Progress: 8.55%[0m
[36m[2023-07-11 02:12:57,761][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 02:12:57,762][233954] FPS: 335080.86[0m
[36m[2023-07-11 02:13:01,964][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:13:01,965][233954] Reward + Measures: [[5078.93175173    0.19462931    0.4218047     0.21962565    0.02643033
     1.21056151]][0m
[37m[1m[2023-07-11 02:13:01,965][233954] Max Reward on eval: 5078.93175172678[0m
[37m[1m[2023-07-11 02:13:01,965][233954] Min Reward on eval: 5078.93175172678[0m
[37m[1m[2023-07-11 02:13:01,965][233954] Mean Reward across all agents: 5078.93175172678[0m
[37m[1m[2023-07-11 02:13:01,966][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:13:06,875][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:13:06,876][233954] Reward + Measures: [[-74.53929185   0.22270003   0.4041       0.19239999   0.45150003
    3.49972153]
 [-29.67208061   0.48480001   0.34400004   0.47890002   0.0921
    2.633075  ]
 [ 55.61199688   0.1882       0.1497       0.16540001   0.15290001
    3.21082568]
 ...
 [-62.7119648    0.65950006   0.66169995   0.61369997   0.67610002
    3.10348725]
 [-67.21256823   0.727        0.7184       0.71280003   0.71789998
    3.20961261]
 [ 16.55982062   0.29510003   0.2112       0.21710001   0.10649999
    3.30579305]][0m
[37m[1m[2023-07-11 02:13:06,876][233954] Max Reward on eval: 352.0061430970207[0m
[37m[1m[2023-07-11 02:13:06,876][233954] Min Reward on eval: -148.20425225328654[0m
[37m[1m[2023-07-11 02:13:06,876][233954] Mean Reward across all agents: 28.043623350348888[0m
[37m[1m[2023-07-11 02:13:06,876][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:13:06,882][233954] mean_value=-70.42251585661208, max_value=787.4268545911648[0m
[37m[1m[2023-07-11 02:13:06,885][233954] New mean coefficients: [[ 0.52786356  1.0068619   6.6376586   1.5265594   2.669987   -2.5382285 ]][0m
[37m[1m[2023-07-11 02:13:06,886][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:13:15,902][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 02:13:15,902][233954] FPS: 425992.44[0m
[36m[2023-07-11 02:13:15,904][233954] itr=172, itrs=2000, Progress: 8.60%[0m
[36m[2023-07-11 02:13:27,477][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 02:13:27,477][233954] FPS: 332970.16[0m
[36m[2023-07-11 02:13:31,729][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:13:31,730][233954] Reward + Measures: [[6073.53427984    0.19542599    0.44747168    0.20875534    0.01119633
     1.23087347]][0m
[37m[1m[2023-07-11 02:13:31,730][233954] Max Reward on eval: 6073.534279835838[0m
[37m[1m[2023-07-11 02:13:31,730][233954] Min Reward on eval: 6073.534279835838[0m
[37m[1m[2023-07-11 02:13:31,731][233954] Mean Reward across all agents: 6073.534279835838[0m
[37m[1m[2023-07-11 02:13:31,731][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:13:36,632][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:13:36,632][233954] Reward + Measures: [[  -1.52929958    0.41750002    0.2034        0.42750001    0.37860003
     2.89332461]
 [-215.87510232    0.0591        0.72119999    0.40790001    0.69150001
     3.16667151]
 [  90.88366649    0.1542        0.20130001    0.16170001    0.1559
     3.08478856]
 ...
 [  21.13832601    0.25530002    0.57429999    0.228         0.48290005
     2.14277244]
 [  72.97507147    0.16430001    0.58920002    0.29980001    0.5873
     2.61332178]
 [ 174.08574482    0.26050001    0.25620005    0.27059999    0.19560002
     3.09251142]][0m
[37m[1m[2023-07-11 02:13:36,633][233954] Max Reward on eval: 334.25048529393973[0m
[37m[1m[2023-07-11 02:13:36,633][233954] Min Reward on eval: -324.93444662522523[0m
[37m[1m[2023-07-11 02:13:36,633][233954] Mean Reward across all agents: 14.674770899779865[0m
[37m[1m[2023-07-11 02:13:36,633][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:13:36,640][233954] mean_value=-77.93258702703551, max_value=657.9832568192063[0m
[37m[1m[2023-07-11 02:13:36,643][233954] New mean coefficients: [[ 0.8159524  2.0336928  7.904237  -0.8279774  6.0736036 -1.6725868]][0m
[37m[1m[2023-07-11 02:13:36,644][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:13:45,567][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 02:13:45,567][233954] FPS: 430421.24[0m
[36m[2023-07-11 02:13:45,570][233954] itr=173, itrs=2000, Progress: 8.65%[0m
[36m[2023-07-11 02:13:57,148][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 02:13:57,149][233954] FPS: 332795.53[0m
[36m[2023-07-11 02:14:01,364][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:14:01,365][233954] Reward + Measures: [[6390.1650167     0.19373733    0.46504337    0.20586431    0.00772967
     1.23141408]][0m
[37m[1m[2023-07-11 02:14:01,365][233954] Max Reward on eval: 6390.16501669742[0m
[37m[1m[2023-07-11 02:14:01,365][233954] Min Reward on eval: 6390.16501669742[0m
[37m[1m[2023-07-11 02:14:01,366][233954] Mean Reward across all agents: 6390.16501669742[0m
[37m[1m[2023-07-11 02:14:01,366][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:14:06,366][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:14:06,372][233954] Reward + Measures: [[134.26081578   0.1925       0.3671       0.26770002   0.30270001
    1.92201424]
 [-66.0003835    0.56119996   0.38929999   0.60430002   0.0613
    3.23899627]
 [-95.89215841   0.18609999   0.55290002   0.50060004   0.57370001
    3.20813227]
 ...
 [ 55.22606004   0.37190002   0.43959999   0.40799999   0.43509999
    2.46261454]
 [-15.16163804   0.27910003   0.40109998   0.36539999   0.42060003
    2.98765063]
 [ -2.32884943   0.26220003   0.15800001   0.26370001   0.0842
    3.38360262]][0m
[37m[1m[2023-07-11 02:14:06,372][233954] Max Reward on eval: 1336.9500656053424[0m
[37m[1m[2023-07-11 02:14:06,373][233954] Min Reward on eval: -503.22357365604256[0m
[37m[1m[2023-07-11 02:14:06,373][233954] Mean Reward across all agents: -29.63053145167403[0m
[37m[1m[2023-07-11 02:14:06,373][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:14:06,380][233954] mean_value=-178.996485445205, max_value=555.8949194067624[0m
[37m[1m[2023-07-11 02:14:06,383][233954] New mean coefficients: [[ 0.4588323   0.00634933  5.469218    0.9851285   1.5789366  -2.9800506 ]][0m
[37m[1m[2023-07-11 02:14:06,384][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:14:15,384][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 02:14:15,385][233954] FPS: 426718.20[0m
[36m[2023-07-11 02:14:15,387][233954] itr=174, itrs=2000, Progress: 8.70%[0m
[36m[2023-07-11 02:14:26,940][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 02:14:26,940][233954] FPS: 333457.56[0m
[36m[2023-07-11 02:14:31,291][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:14:31,292][233954] Reward + Measures: [[6561.83278394    0.19149967    0.48321265    0.20414734    0.00607667
     1.20842826]][0m
[37m[1m[2023-07-11 02:14:31,292][233954] Max Reward on eval: 6561.832783936291[0m
[37m[1m[2023-07-11 02:14:31,292][233954] Min Reward on eval: 6561.832783936291[0m
[37m[1m[2023-07-11 02:14:31,292][233954] Mean Reward across all agents: 6561.832783936291[0m
[37m[1m[2023-07-11 02:14:31,293][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:14:36,466][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:14:36,472][233954] Reward + Measures: [[ -97.4975178     0.85900003    0.80200005    0.82720006    0.0346
     3.79725575]
 [ 416.00656697    0.1754        0.18450002    0.1846        0.1213
     1.58171833]
 [-369.53941725    0.84399998    0.80800003    0.81049997    0.0352
     3.90789008]
 ...
 [ 171.17555081    0.14380001    0.183         0.16950001    0.14069998
     2.14081049]
 [ 201.08207131    0.13680001    0.23559999    0.16140001    0.10120001
     2.13042688]
 [  82.44339528    0.3876        0.28839999    0.3761        0.2225
     3.3743248 ]][0m
[37m[1m[2023-07-11 02:14:36,472][233954] Max Reward on eval: 512.512729064282[0m
[37m[1m[2023-07-11 02:14:36,473][233954] Min Reward on eval: -388.75895849997175[0m
[37m[1m[2023-07-11 02:14:36,473][233954] Mean Reward across all agents: 12.197337922664666[0m
[37m[1m[2023-07-11 02:14:36,473][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:14:36,484][233954] mean_value=13.16870601992528, max_value=889.3198089401703[0m
[37m[1m[2023-07-11 02:14:36,487][233954] New mean coefficients: [[ 0.3829157  1.4519191  5.9783773 -1.2290212  4.660346  -2.078799 ]][0m
[37m[1m[2023-07-11 02:14:36,488][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:14:45,511][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 02:14:45,512][233954] FPS: 425624.78[0m
[36m[2023-07-11 02:14:45,514][233954] itr=175, itrs=2000, Progress: 8.75%[0m
[36m[2023-07-11 02:14:57,097][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 02:14:57,097][233954] FPS: 332677.21[0m
[36m[2023-07-11 02:15:01,385][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:15:01,385][233954] Reward + Measures: [[5827.97119897    0.17695932    0.38623631    0.19694999    0.00833933
     1.09453845]][0m
[37m[1m[2023-07-11 02:15:01,386][233954] Max Reward on eval: 5827.971198969732[0m
[37m[1m[2023-07-11 02:15:01,386][233954] Min Reward on eval: 5827.971198969732[0m
[37m[1m[2023-07-11 02:15:01,386][233954] Mean Reward across all agents: 5827.971198969732[0m
[37m[1m[2023-07-11 02:15:01,386][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:15:06,345][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:15:06,351][233954] Reward + Measures: [[242.61127504   0.39180002   0.0517       0.37030002   0.33580002
    2.54074097]
 [-42.44161068   0.112        0.1601       0.0799       0.1133
    3.52096939]
 [-10.09165691   0.0887       0.1596       0.11369999   0.11369999
    3.39030528]
 ...
 [192.43260171   0.33559999   0.07610001   0.32179999   0.27770001
    2.68447351]
 [ 79.85709086   0.2069       0.32160002   0.24959998   0.32280001
    2.89873934]
 [114.09620645   0.34090003   0.0871       0.36950001   0.27710003
    2.61390042]][0m
[37m[1m[2023-07-11 02:15:06,351][233954] Max Reward on eval: 1028.1982269331813[0m
[37m[1m[2023-07-11 02:15:06,352][233954] Min Reward on eval: -254.70386062003672[0m
[37m[1m[2023-07-11 02:15:06,352][233954] Mean Reward across all agents: 71.45315589908549[0m
[37m[1m[2023-07-11 02:15:06,352][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:15:06,357][233954] mean_value=-213.94344462361008, max_value=801.2437184114578[0m
[37m[1m[2023-07-11 02:15:06,360][233954] New mean coefficients: [[-0.10558566  0.727522    3.2455134   1.7032567   0.7598636  -2.7414749 ]][0m
[37m[1m[2023-07-11 02:15:06,361][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:15:15,370][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 02:15:15,371][233954] FPS: 426307.39[0m
[36m[2023-07-11 02:15:15,373][233954] itr=176, itrs=2000, Progress: 8.80%[0m
[36m[2023-07-11 02:15:27,045][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 02:15:27,045][233954] FPS: 330072.68[0m
[36m[2023-07-11 02:15:31,368][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:15:31,368][233954] Reward + Measures: [[5734.25651131    0.17651233    0.40967166    0.19676168    0.007737
     1.06678438]][0m
[37m[1m[2023-07-11 02:15:31,369][233954] Max Reward on eval: 5734.256511310879[0m
[37m[1m[2023-07-11 02:15:31,369][233954] Min Reward on eval: 5734.256511310879[0m
[37m[1m[2023-07-11 02:15:31,369][233954] Mean Reward across all agents: 5734.256511310879[0m
[37m[1m[2023-07-11 02:15:31,369][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:15:36,363][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:15:36,364][233954] Reward + Measures: [[-140.30948437    0.06390001    0.3786        0.2994        0.32189998
     3.28822899]
 [  29.95306943    0.11540001    0.19160001    0.18599999    0.14509998
     2.73057866]
 [  22.26997651    0.14910001    0.48369995    0.2299        0.45860001
     2.64952707]
 ...
 [ 199.66576193    0.1186        0.26760003    0.1706        0.18350001
     2.00033212]
 [ 136.36464371    0.32930002    0.38040003    0.1311        0.4206
     3.28286219]
 [  -6.75933894    0.21949999    0.24870001    0.26830003    0.1347
     2.17759013]][0m
[37m[1m[2023-07-11 02:15:36,364][233954] Max Reward on eval: 1165.432906827517[0m
[37m[1m[2023-07-11 02:15:36,364][233954] Min Reward on eval: -387.1844744910486[0m
[37m[1m[2023-07-11 02:15:36,364][233954] Mean Reward across all agents: 52.36915580052944[0m
[37m[1m[2023-07-11 02:15:36,365][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:15:36,372][233954] mean_value=-232.84052810569838, max_value=649.5738715924323[0m
[37m[1m[2023-07-11 02:15:36,375][233954] New mean coefficients: [[ 0.10191393  0.91294163  4.721835   -0.41936243  2.4270873  -2.6764784 ]][0m
[37m[1m[2023-07-11 02:15:36,376][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:15:45,412][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 02:15:45,412][233954] FPS: 425042.80[0m
[36m[2023-07-11 02:15:45,414][233954] itr=177, itrs=2000, Progress: 8.85%[0m
[36m[2023-07-11 02:15:57,066][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 02:15:57,066][233954] FPS: 330711.67[0m
[36m[2023-07-11 02:16:01,298][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:16:01,299][233954] Reward + Measures: [[5807.90261873    0.18965434    0.52286929    0.20194499    0.00729633
     1.07367373]][0m
[37m[1m[2023-07-11 02:16:01,299][233954] Max Reward on eval: 5807.902618731586[0m
[37m[1m[2023-07-11 02:16:01,299][233954] Min Reward on eval: 5807.902618731586[0m
[37m[1m[2023-07-11 02:16:01,300][233954] Mean Reward across all agents: 5807.902618731586[0m
[37m[1m[2023-07-11 02:16:01,300][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:16:06,305][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:16:06,311][233954] Reward + Measures: [[299.39106414   0.16290002   0.38960001   0.26479998   0.18249999
    1.52471125]
 [-50.35929639   0.278        0.30149999   0.25690004   0.28550002
    3.18328214]
 [507.76730685   0.24890001   0.22210002   0.28040001   0.11369999
    2.63963437]
 ...
 [-47.42571232   0.34029999   0.611        0.25050002   0.5072
    2.17237735]
 [ 33.36607447   0.37689999   0.32610002   0.421        0.1517
    3.0925808 ]
 [ 63.11155637   0.1151       0.3856       0.1787       0.30660003
    2.79313254]][0m
[37m[1m[2023-07-11 02:16:06,311][233954] Max Reward on eval: 853.3165321260691[0m
[37m[1m[2023-07-11 02:16:06,311][233954] Min Reward on eval: -620.1845681484323[0m
[37m[1m[2023-07-11 02:16:06,311][233954] Mean Reward across all agents: 49.65175518746782[0m
[37m[1m[2023-07-11 02:16:06,312][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:16:06,320][233954] mean_value=-83.24699969738155, max_value=1099.534263585601[0m
[37m[1m[2023-07-11 02:16:06,323][233954] New mean coefficients: [[-0.12311634  0.2701103   2.9851968   2.0995946  -0.4347644  -3.2582197 ]][0m
[37m[1m[2023-07-11 02:16:06,324][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:16:15,490][233954] train() took 9.16 seconds to complete[0m
[36m[2023-07-11 02:16:15,490][233954] FPS: 419012.37[0m
[36m[2023-07-11 02:16:15,492][233954] itr=178, itrs=2000, Progress: 8.90%[0m
[36m[2023-07-11 02:16:27,202][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 02:16:27,203][233954] FPS: 329065.65[0m
[36m[2023-07-11 02:16:31,514][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:16:31,514][233954] Reward + Measures: [[5800.53686103    0.18661466    0.547104      0.19969998    0.00644133
     1.02890813]][0m
[37m[1m[2023-07-11 02:16:31,514][233954] Max Reward on eval: 5800.536861027812[0m
[37m[1m[2023-07-11 02:16:31,515][233954] Min Reward on eval: 5800.536861027812[0m
[37m[1m[2023-07-11 02:16:31,515][233954] Mean Reward across all agents: 5800.536861027812[0m
[37m[1m[2023-07-11 02:16:31,515][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:16:36,500][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:16:36,501][233954] Reward + Measures: [[  14.18720937    0.19020002    0.2           0.15699999    0.18610001
     2.6268158 ]
 [  50.87085801    0.15010001    0.56220001    0.27070004    0.54900002
     2.4677856 ]
 [-132.57917551    0.50920004    0.20289998    0.48459998    0.21169999
     3.44317174]
 ...
 [   6.82393345    0.29260001    0.32029998    0.27200001    0.30720001
     2.82467628]
 [-166.20314403    0.50570005    0.40489998    0.4619        0.0982
     3.49443054]
 [ -58.61247091    0.21130002    0.32479998    0.15350001    0.3414
     3.04637384]][0m
[37m[1m[2023-07-11 02:16:36,501][233954] Max Reward on eval: 1209.6355037510396[0m
[37m[1m[2023-07-11 02:16:36,502][233954] Min Reward on eval: -398.1460538295563[0m
[37m[1m[2023-07-11 02:16:36,502][233954] Mean Reward across all agents: 24.28846280579585[0m
[37m[1m[2023-07-11 02:16:36,502][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:16:36,513][233954] mean_value=-138.22081125222866, max_value=1181.8770905270242[0m
[37m[1m[2023-07-11 02:16:36,515][233954] New mean coefficients: [[ 0.03874214  0.9412258   4.310032    2.5828876   0.36429238 -2.6865153 ]][0m
[37m[1m[2023-07-11 02:16:36,516][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:16:45,512][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 02:16:45,512][233954] FPS: 426969.03[0m
[36m[2023-07-11 02:16:45,514][233954] itr=179, itrs=2000, Progress: 8.95%[0m
[36m[2023-07-11 02:16:57,396][233954] train() took 11.84 seconds to complete[0m
[36m[2023-07-11 02:16:57,397][233954] FPS: 324285.85[0m
[36m[2023-07-11 02:17:01,698][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:17:01,698][233954] Reward + Measures: [[1294.81811861    0.14936167    0.29248166    0.19856465    0.09045933
     0.99551123]][0m
[37m[1m[2023-07-11 02:17:01,699][233954] Max Reward on eval: 1294.8181186081358[0m
[37m[1m[2023-07-11 02:17:01,699][233954] Min Reward on eval: 1294.8181186081358[0m
[37m[1m[2023-07-11 02:17:01,699][233954] Mean Reward across all agents: 1294.8181186081358[0m
[37m[1m[2023-07-11 02:17:01,700][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:17:06,894][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:17:06,895][233954] Reward + Measures: [[311.91513176   0.40650001   0.4664       0.41330001   0.285
    1.94369018]
 [ 98.60057337   0.294        0.21710001   0.3035       0.15110001
    2.78442693]
 [ 13.67132997   0.29320002   0.27330002   0.30640003   0.127
    3.30987334]
 ...
 [ 46.24126918   0.51679999   0.56019998   0.47620001   0.48270002
    0.91642618]
 [-38.90909738   0.29650003   0.25530002   0.28370002   0.13880001
    3.13175845]
 [ 69.4850752    0.33510002   0.18960001   0.49070001   0.36660001
    2.8756175 ]][0m
[37m[1m[2023-07-11 02:17:06,895][233954] Max Reward on eval: 1021.9429340226576[0m
[37m[1m[2023-07-11 02:17:06,895][233954] Min Reward on eval: -122.9333815192338[0m
[37m[1m[2023-07-11 02:17:06,896][233954] Mean Reward across all agents: 80.14642310426905[0m
[37m[1m[2023-07-11 02:17:06,896][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:17:06,906][233954] mean_value=-27.816224000716296, max_value=934.9749504307918[0m
[37m[1m[2023-07-11 02:17:06,909][233954] New mean coefficients: [[-0.02812582  2.236327    5.299223   -0.06087589  2.9440405  -2.3960552 ]][0m
[37m[1m[2023-07-11 02:17:06,910][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:17:16,015][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 02:17:16,015][233954] FPS: 421818.12[0m
[36m[2023-07-11 02:17:16,018][233954] itr=180, itrs=2000, Progress: 9.00%[0m
[37m[1m[2023-07-11 02:19:42,657][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000160[0m
[36m[2023-07-11 02:19:55,032][233954] train() took 11.77 seconds to complete[0m
[36m[2023-07-11 02:19:55,032][233954] FPS: 326337.44[0m
[36m[2023-07-11 02:19:59,200][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:19:59,201][233954] Reward + Measures: [[2473.21183671    0.17655167    0.38040203    0.23796433    0.05226067
     0.99159193]][0m
[37m[1m[2023-07-11 02:19:59,201][233954] Max Reward on eval: 2473.2118367053763[0m
[37m[1m[2023-07-11 02:19:59,201][233954] Min Reward on eval: 2473.2118367053763[0m
[37m[1m[2023-07-11 02:19:59,201][233954] Mean Reward across all agents: 2473.2118367053763[0m
[37m[1m[2023-07-11 02:19:59,202][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:20:04,254][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:20:04,255][233954] Reward + Measures: [[ -47.08692708    0.45820004    0.1133        0.45339999    0.32119998
     3.52607203]
 [ 358.43171024    0.83560002    0.023         0.80849999    0.72970003
     2.87308002]
 [-260.86476258    0.60659999    0.0453        0.565         0.43220001
     3.39425707]
 ...
 [ 134.38636379    0.14659999    0.40869999    0.4341        0.4707
     3.12660098]
 [ 216.45792925    0.12920001    0.41619998    0.2798        0.32840002
     1.90326583]
 [  58.32030717    0.16800001    0.3628        0.1489        0.31759998
     2.75458002]][0m
[37m[1m[2023-07-11 02:20:04,255][233954] Max Reward on eval: 1110.6537456341787[0m
[37m[1m[2023-07-11 02:20:04,256][233954] Min Reward on eval: -714.5422439582413[0m
[37m[1m[2023-07-11 02:20:04,256][233954] Mean Reward across all agents: 72.23245941203785[0m
[37m[1m[2023-07-11 02:20:04,256][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:20:04,267][233954] mean_value=-96.3719110654073, max_value=755.1726462207362[0m
[37m[1m[2023-07-11 02:20:04,269][233954] New mean coefficients: [[-0.1816141  0.7932036  3.860351   1.2687527 -0.5780244 -3.4905457]][0m
[37m[1m[2023-07-11 02:20:04,270][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:20:13,207][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 02:20:13,207][233954] FPS: 429775.93[0m
[36m[2023-07-11 02:20:13,209][233954] itr=181, itrs=2000, Progress: 9.05%[0m
[36m[2023-07-11 02:20:25,066][233954] train() took 11.81 seconds to complete[0m
[36m[2023-07-11 02:20:25,066][233954] FPS: 325028.25[0m
[36m[2023-07-11 02:20:29,486][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:20:29,486][233954] Reward + Measures: [[1104.62407131    0.173288      0.35658699    0.27606699    0.10105233
     1.33869898]][0m
[37m[1m[2023-07-11 02:20:29,487][233954] Max Reward on eval: 1104.6240713087727[0m
[37m[1m[2023-07-11 02:20:29,487][233954] Min Reward on eval: 1104.6240713087727[0m
[37m[1m[2023-07-11 02:20:29,487][233954] Mean Reward across all agents: 1104.6240713087727[0m
[37m[1m[2023-07-11 02:20:29,487][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:20:34,508][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:20:34,509][233954] Reward + Measures: [[  8.64801275   0.0637       0.23709999   0.16060001   0.19159999
    2.23565173]
 [-13.41592958   0.12030001   0.2823       0.1109       0.2172
    2.54605222]
 [168.71946538   0.13090001   0.2922       0.22060001   0.3105
    2.68840957]
 ...
 [-15.03958225   0.2852       0.06780001   0.3211       0.24679999
    2.7234025 ]
 [109.23620009   0.0927       0.33849999   0.2942       0.3184
    2.6624651 ]
 [ 54.73961092   0.0723       0.59980005   0.57580006   0.6473
    2.89316726]][0m
[37m[1m[2023-07-11 02:20:34,509][233954] Max Reward on eval: 751.1281356536783[0m
[37m[1m[2023-07-11 02:20:34,509][233954] Min Reward on eval: -266.115928622894[0m
[37m[1m[2023-07-11 02:20:34,510][233954] Mean Reward across all agents: 86.55737976790122[0m
[37m[1m[2023-07-11 02:20:34,510][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:20:34,515][233954] mean_value=-170.3600008101542, max_value=780.6419448995032[0m
[37m[1m[2023-07-11 02:20:34,518][233954] New mean coefficients: [[-0.3895136  1.3454559  3.2645726  1.5271195 -1.3938951 -3.4734468]][0m
[37m[1m[2023-07-11 02:20:34,519][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:20:43,599][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 02:20:43,600][233954] FPS: 422958.90[0m
[36m[2023-07-11 02:20:43,602][233954] itr=182, itrs=2000, Progress: 9.10%[0m
[36m[2023-07-11 02:20:55,314][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 02:20:55,315][233954] FPS: 328941.70[0m
[36m[2023-07-11 02:20:59,659][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:20:59,659][233954] Reward + Measures: [[1000.50474384    0.17897034    0.38125268    0.28471601    0.11163367
     1.30566657]][0m
[37m[1m[2023-07-11 02:20:59,659][233954] Max Reward on eval: 1000.5047438435543[0m
[37m[1m[2023-07-11 02:20:59,660][233954] Min Reward on eval: 1000.5047438435543[0m
[37m[1m[2023-07-11 02:20:59,660][233954] Mean Reward across all agents: 1000.5047438435543[0m
[37m[1m[2023-07-11 02:20:59,660][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:21:04,655][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:21:04,656][233954] Reward + Measures: [[ 77.27081269   0.13780001   0.29030001   0.1454       0.20800002
    2.1712184 ]
 [184.34850767   0.10570001   0.18020001   0.13630001   0.08630001
    1.87036884]
 [ 48.3082711    0.41640002   0.50120002   0.0952       0.46630001
    3.05587745]
 ...
 [ 16.37767418   0.2189       0.34150001   0.3001       0.35999998
    3.12484646]
 [-15.5359302    0.30050001   0.3908       0.26460001   0.3292
    3.0441072 ]
 [319.113559     0.32480001   0.3522       0.3409       0.32879999
    2.73771381]][0m
[37m[1m[2023-07-11 02:21:04,656][233954] Max Reward on eval: 1155.2481098234653[0m
[37m[1m[2023-07-11 02:21:04,656][233954] Min Reward on eval: -161.48028269149364[0m
[37m[1m[2023-07-11 02:21:04,656][233954] Mean Reward across all agents: 73.24254244382261[0m
[37m[1m[2023-07-11 02:21:04,657][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:21:04,661][233954] mean_value=-210.2227368357027, max_value=640.8166176894191[0m
[37m[1m[2023-07-11 02:21:04,664][233954] New mean coefficients: [[-0.5519892  1.1312416  1.8996085  3.083115  -3.0301685 -3.5779376]][0m
[37m[1m[2023-07-11 02:21:04,665][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:21:13,647][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 02:21:13,647][233954] FPS: 427617.19[0m
[36m[2023-07-11 02:21:13,649][233954] itr=183, itrs=2000, Progress: 9.15%[0m
[36m[2023-07-11 02:21:25,367][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 02:21:25,367][233954] FPS: 328771.58[0m
[36m[2023-07-11 02:21:29,621][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:21:29,622][233954] Reward + Measures: [[873.66206844   0.18166398   0.39895698   0.28550699   0.12484733
    1.27772307]][0m
[37m[1m[2023-07-11 02:21:29,622][233954] Max Reward on eval: 873.6620684412454[0m
[37m[1m[2023-07-11 02:21:29,622][233954] Min Reward on eval: 873.6620684412454[0m
[37m[1m[2023-07-11 02:21:29,622][233954] Mean Reward across all agents: 873.6620684412454[0m
[37m[1m[2023-07-11 02:21:29,623][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:21:34,606][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:21:34,607][233954] Reward + Measures: [[202.15902008   0.27880001   0.1665       0.3001       0.15629999
    3.00157619]
 [ 87.02821108   0.10220001   0.43889999   0.33470002   0.43590003
    3.0537405 ]
 [265.77372705   0.1046       0.16689999   0.1488       0.14120001
    2.2570281 ]
 ...
 [271.79440259   0.2753       0.3953       0.1662       0.39009997
    2.55960774]
 [246.64387177   0.23559999   0.3215       0.14289999   0.3127
    1.98823476]
 [175.00829128   0.22950001   0.14500001   0.1714       0.18520001
    2.12281275]][0m
[37m[1m[2023-07-11 02:21:34,607][233954] Max Reward on eval: 1003.7156668122858[0m
[37m[1m[2023-07-11 02:21:34,607][233954] Min Reward on eval: -288.47428345680237[0m
[37m[1m[2023-07-11 02:21:34,608][233954] Mean Reward across all agents: 86.76818546065442[0m
[37m[1m[2023-07-11 02:21:34,608][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:21:34,613][233954] mean_value=-215.91001042359056, max_value=596.4913829469602[0m
[37m[1m[2023-07-11 02:21:34,615][233954] New mean coefficients: [[-0.50883096  2.0167835   2.720689    1.7556487  -0.84060836 -2.746726  ]][0m
[37m[1m[2023-07-11 02:21:34,616][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:21:43,700][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 02:21:43,700][233954] FPS: 422827.50[0m
[36m[2023-07-11 02:21:43,702][233954] itr=184, itrs=2000, Progress: 9.20%[0m
[36m[2023-07-11 02:21:55,317][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 02:21:55,317][233954] FPS: 331695.92[0m
[36m[2023-07-11 02:21:59,647][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:21:59,647][233954] Reward + Measures: [[725.33954712   0.18525033   0.40869868   0.27757034   0.14602834
    1.2481215 ]][0m
[37m[1m[2023-07-11 02:21:59,647][233954] Max Reward on eval: 725.3395471216045[0m
[37m[1m[2023-07-11 02:21:59,648][233954] Min Reward on eval: 725.3395471216045[0m
[37m[1m[2023-07-11 02:21:59,648][233954] Mean Reward across all agents: 725.3395471216045[0m
[37m[1m[2023-07-11 02:21:59,648][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:22:04,800][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:22:04,801][233954] Reward + Measures: [[ 71.52084195   0.1459       0.34689999   0.33329996   0.33039999
    2.38599181]
 [156.68792795   0.34700003   0.62350005   0.1831       0.54150003
    2.69018674]
 [146.59042567   0.3858       0.29280001   0.30380002   0.27970001
    2.2616818 ]
 ...
 [ 59.4703364    0.14140001   0.30399999   0.14040001   0.23230003
    2.51128268]
 [ 96.77973911   0.2237       0.32300001   0.22550002   0.2931
    2.09432149]
 [ 98.59120577   0.1804       0.1072       0.12899999   0.1191
    2.7012639 ]][0m
[37m[1m[2023-07-11 02:22:04,801][233954] Max Reward on eval: 1032.2559566710843[0m
[37m[1m[2023-07-11 02:22:04,801][233954] Min Reward on eval: -162.98223797790706[0m
[37m[1m[2023-07-11 02:22:04,801][233954] Mean Reward across all agents: 165.38975570063948[0m
[37m[1m[2023-07-11 02:22:04,802][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:22:04,807][233954] mean_value=-567.2536745801498, max_value=927.1577365912148[0m
[37m[1m[2023-07-11 02:22:04,809][233954] New mean coefficients: [[-0.573071   2.8737888  2.444395   1.8161285 -1.1689928 -2.820288 ]][0m
[37m[1m[2023-07-11 02:22:04,810][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:22:13,914][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 02:22:13,914][233954] FPS: 421872.00[0m
[36m[2023-07-11 02:22:13,917][233954] itr=185, itrs=2000, Progress: 9.25%[0m
[36m[2023-07-11 02:22:25,530][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 02:22:25,530][233954] FPS: 331736.09[0m
[36m[2023-07-11 02:22:29,781][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:22:29,781][233954] Reward + Measures: [[648.62891518   0.19606131   0.43729165   0.279468     0.17225501
    1.21482289]][0m
[37m[1m[2023-07-11 02:22:29,781][233954] Max Reward on eval: 648.6289151755207[0m
[37m[1m[2023-07-11 02:22:29,782][233954] Min Reward on eval: 648.6289151755207[0m
[37m[1m[2023-07-11 02:22:29,782][233954] Mean Reward across all agents: 648.6289151755207[0m
[37m[1m[2023-07-11 02:22:29,782][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:22:34,848][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:22:34,849][233954] Reward + Measures: [[552.10968395   0.2494       0.3285       0.35320002   0.1858
    2.05227089]
 [ 34.57862263   0.50380003   0.3089       0.50960004   0.27050003
    2.61453223]
 [ 14.7354169    0.22839999   0.59490001   0.33910003   0.52240002
    2.49412227]
 ...
 [135.82666345   0.60070002   0.67770004   0.48319998   0.24169998
    3.35265207]
 [ 93.53910053   0.33500001   0.5176       0.28389999   0.32539999
    2.30981326]
 [ 31.4328638    0.12590002   0.0903       0.2033       0.1593
    3.07382894]][0m
[37m[1m[2023-07-11 02:22:34,849][233954] Max Reward on eval: 994.4322776786518[0m
[37m[1m[2023-07-11 02:22:34,849][233954] Min Reward on eval: -203.29469102504663[0m
[37m[1m[2023-07-11 02:22:34,850][233954] Mean Reward across all agents: 125.71194913817158[0m
[37m[1m[2023-07-11 02:22:34,850][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:22:34,860][233954] mean_value=-166.31018607191692, max_value=945.5779266338795[0m
[37m[1m[2023-07-11 02:22:34,863][233954] New mean coefficients: [[-0.3272417  5.5743585  4.204732  -1.1725466  3.6493714 -1.4225788]][0m
[37m[1m[2023-07-11 02:22:34,864][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:22:43,976][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 02:22:43,976][233954] FPS: 421487.14[0m
[36m[2023-07-11 02:22:43,979][233954] itr=186, itrs=2000, Progress: 9.30%[0m
[36m[2023-07-11 02:22:55,610][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 02:22:55,610][233954] FPS: 331264.40[0m
[36m[2023-07-11 02:22:59,930][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:22:59,930][233954] Reward + Measures: [[467.75072328   0.21395965   0.47078732   0.25622568   0.22950634
    1.17388022]][0m
[37m[1m[2023-07-11 02:22:59,930][233954] Max Reward on eval: 467.7507232786943[0m
[37m[1m[2023-07-11 02:22:59,931][233954] Min Reward on eval: 467.7507232786943[0m
[37m[1m[2023-07-11 02:22:59,931][233954] Mean Reward across all agents: 467.7507232786943[0m
[37m[1m[2023-07-11 02:22:59,931][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:23:04,967][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:23:04,968][233954] Reward + Measures: [[-147.31761467    0.13150001    0.47049999    0.31760001    0.43470001
     3.32437706]
 [  29.99674939    0.12229999    0.31860003    0.245         0.26969999
     2.76528406]
 [-163.58128643    0.0781        0.68400002    0.36500001    0.61619997
     2.28024173]
 ...
 [ -23.68271262    0.13420002    0.44169998    0.21159999    0.3689
     2.25453067]
 [-140.57891084    0.1204        0.75299996    0.25210002    0.60579997
     3.07925773]
 [ 137.28857347    0.2005        0.2237        0.2208        0.1909
     2.02660441]][0m
[37m[1m[2023-07-11 02:23:04,968][233954] Max Reward on eval: 806.6817359277978[0m
[37m[1m[2023-07-11 02:23:04,969][233954] Min Reward on eval: -421.2686939299107[0m
[37m[1m[2023-07-11 02:23:04,969][233954] Mean Reward across all agents: -31.0414670981878[0m
[37m[1m[2023-07-11 02:23:04,969][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:23:04,975][233954] mean_value=-272.39592535924425, max_value=762.3726108325366[0m
[37m[1m[2023-07-11 02:23:04,978][233954] New mean coefficients: [[-0.91799784  3.6971145   1.2622917   1.2834451  -1.238931   -3.13409   ]][0m
[37m[1m[2023-07-11 02:23:04,979][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:23:14,014][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 02:23:14,014][233954] FPS: 425119.20[0m
[36m[2023-07-11 02:23:14,016][233954] itr=187, itrs=2000, Progress: 9.35%[0m
[36m[2023-07-11 02:23:25,635][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 02:23:25,635][233954] FPS: 331597.22[0m
[36m[2023-07-11 02:23:30,010][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:23:30,010][233954] Reward + Measures: [[326.70291007   0.23950498   0.49118266   0.22673532   0.27588764
    1.12524366]][0m
[37m[1m[2023-07-11 02:23:30,011][233954] Max Reward on eval: 326.7029100696091[0m
[37m[1m[2023-07-11 02:23:30,011][233954] Min Reward on eval: 326.7029100696091[0m
[37m[1m[2023-07-11 02:23:30,011][233954] Mean Reward across all agents: 326.7029100696091[0m
[37m[1m[2023-07-11 02:23:30,012][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:23:35,013][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:23:35,014][233954] Reward + Measures: [[129.20984502   0.18719999   0.168        0.21519999   0.1481
    2.89336467]
 [-39.39478512   0.48210001   0.26430002   0.57700002   0.3757
    2.74219036]
 [ 26.53353972   0.31529999   0.42529997   0.53909999   0.47760001
    3.21731448]
 ...
 [146.08004393   0.77640003   0.18450001   0.74360001   0.26690003
    2.64634442]
 [-95.67084641   0.29370001   0.1883       0.32600003   0.2218
    2.91514254]
 [ 50.28454406   0.39040002   0.33750001   0.5061       0.36919999
    2.30736136]][0m
[37m[1m[2023-07-11 02:23:35,014][233954] Max Reward on eval: 426.19641196206214[0m
[37m[1m[2023-07-11 02:23:35,014][233954] Min Reward on eval: -118.21297269426286[0m
[37m[1m[2023-07-11 02:23:35,014][233954] Mean Reward across all agents: 48.40866297225111[0m
[37m[1m[2023-07-11 02:23:35,015][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:23:35,028][233954] mean_value=-38.69118237556528, max_value=686.2090565558523[0m
[37m[1m[2023-07-11 02:23:35,031][233954] New mean coefficients: [[-1.0272189  2.8055253  1.020594   1.3475919 -2.0504084 -2.848007 ]][0m
[37m[1m[2023-07-11 02:23:35,032][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:23:44,090][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 02:23:44,091][233954] FPS: 423991.50[0m
[36m[2023-07-11 02:23:44,093][233954] itr=188, itrs=2000, Progress: 9.40%[0m
[36m[2023-07-11 02:23:55,740][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 02:23:55,741][233954] FPS: 330769.32[0m
[36m[2023-07-11 02:24:00,040][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:24:00,041][233954] Reward + Measures: [[178.59608808   0.273619     0.52069032   0.18955667   0.34311166
    1.06384182]][0m
[37m[1m[2023-07-11 02:24:00,041][233954] Max Reward on eval: 178.5960880821788[0m
[37m[1m[2023-07-11 02:24:00,041][233954] Min Reward on eval: 178.5960880821788[0m
[37m[1m[2023-07-11 02:24:00,041][233954] Mean Reward across all agents: 178.5960880821788[0m
[37m[1m[2023-07-11 02:24:00,042][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:24:05,009][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:24:05,010][233954] Reward + Measures: [[202.24825408   0.3062       0.4639       0.31810001   0.2877
    1.83679104]
 [344.18244175   0.18530001   0.20229998   0.13640001   0.15549999
    1.90418816]
 [ 71.13174796   0.0969       0.2886       0.1805       0.18789999
    3.27767348]
 ...
 [151.47305133   0.0542       0.33740002   0.28550002   0.28220001
    3.02683091]
 [102.90502872   0.1055       0.40219998   0.31909999   0.42829996
    2.81576848]
 [110.39046977   0.0763       0.12899999   0.09610001   0.0867
    3.20291567]][0m
[37m[1m[2023-07-11 02:24:05,010][233954] Max Reward on eval: 418.1157956341282[0m
[37m[1m[2023-07-11 02:24:05,010][233954] Min Reward on eval: -263.5614838467911[0m
[37m[1m[2023-07-11 02:24:05,011][233954] Mean Reward across all agents: 99.97543466485428[0m
[37m[1m[2023-07-11 02:24:05,011][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:24:05,019][233954] mean_value=-126.90196029657963, max_value=706.6504320148379[0m
[37m[1m[2023-07-11 02:24:05,021][233954] New mean coefficients: [[-1.0667148   1.9210043   0.94390595 -0.18968558 -2.2232924  -2.9356768 ]][0m
[37m[1m[2023-07-11 02:24:05,022][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:24:14,014][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 02:24:14,015][233954] FPS: 427121.80[0m
[36m[2023-07-11 02:24:14,017][233954] itr=189, itrs=2000, Progress: 9.45%[0m
[36m[2023-07-11 02:24:25,529][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 02:24:25,529][233954] FPS: 334752.10[0m
[36m[2023-07-11 02:24:29,793][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:24:29,793][233954] Reward + Measures: [[44.74911827  0.30504566  0.54776633  0.14527634  0.40678331  0.99056572]][0m
[37m[1m[2023-07-11 02:24:29,794][233954] Max Reward on eval: 44.749118266931646[0m
[37m[1m[2023-07-11 02:24:29,794][233954] Min Reward on eval: 44.749118266931646[0m
[37m[1m[2023-07-11 02:24:29,794][233954] Mean Reward across all agents: 44.749118266931646[0m
[37m[1m[2023-07-11 02:24:29,794][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:24:34,948][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:24:34,949][233954] Reward + Measures: [[163.91762687   0.127        0.19109999   0.13450001   0.10090001
    1.85484207]
 [ 66.59908042   0.17380001   0.2441       0.19240001   0.21739998
    3.04340529]
 [ 92.68070842   0.42079997   0.48780003   0.05710001   0.43619999
    2.41623545]
 ...
 [180.8605238    0.23450001   0.60980004   0.25170001   0.53590006
    1.94498813]
 [124.18726259   0.421        0.60070002   0.09060001   0.4743
    2.38498306]
 [ 19.33654373   0.33940002   0.29640001   0.36040002   0.2696
    3.0102725 ]][0m
[37m[1m[2023-07-11 02:24:34,949][233954] Max Reward on eval: 679.49306677063[0m
[37m[1m[2023-07-11 02:24:34,949][233954] Min Reward on eval: -81.69477375242859[0m
[37m[1m[2023-07-11 02:24:34,949][233954] Mean Reward across all agents: 91.98712453222103[0m
[37m[1m[2023-07-11 02:24:34,950][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:24:34,955][233954] mean_value=-289.73420016945477, max_value=709.1998833120796[0m
[37m[1m[2023-07-11 02:24:34,958][233954] New mean coefficients: [[-1.0738798   2.8236618   2.0079498  -0.68790895 -0.33770776 -2.6097448 ]][0m
[37m[1m[2023-07-11 02:24:34,959][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:24:43,896][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 02:24:43,897][233954] FPS: 429703.38[0m
[36m[2023-07-11 02:24:43,899][233954] itr=190, itrs=2000, Progress: 9.50%[0m
[37m[1m[2023-07-11 02:27:16,947][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000170[0m
[36m[2023-07-11 02:27:29,120][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 02:27:29,121][233954] FPS: 329683.73[0m
[36m[2023-07-11 02:27:33,367][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:27:33,367][233954] Reward + Measures: [[9.71146711 0.328237   0.54893637 0.11803433 0.42750564 0.94641703]][0m
[37m[1m[2023-07-11 02:27:33,368][233954] Max Reward on eval: 9.71146711380776[0m
[37m[1m[2023-07-11 02:27:33,368][233954] Min Reward on eval: 9.71146711380776[0m
[37m[1m[2023-07-11 02:27:33,368][233954] Mean Reward across all agents: 9.71146711380776[0m
[37m[1m[2023-07-11 02:27:33,368][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:27:38,269][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:27:38,269][233954] Reward + Measures: [[ 197.426269      0.17550001    0.18440001    0.14350002    0.16909999
     2.70281339]
 [-113.14055344    0.39120004    0.55950004    0.14049999    0.50710005
     2.34793353]
 [ -88.43958983    0.33170003    0.55500001    0.1671        0.49149999
     2.3683567 ]
 ...
 [ 289.03748896    0.1591        0.3294        0.24659999    0.252
     1.91663253]
 [ 130.66229227    0.26550004    0.42469999    0.16070001    0.32090002
     1.77510762]
 [ 244.29253054    0.1538        0.15509999    0.17309999    0.13510001
     2.49429011]][0m
[37m[1m[2023-07-11 02:27:38,270][233954] Max Reward on eval: 875.6083288488444[0m
[37m[1m[2023-07-11 02:27:38,270][233954] Min Reward on eval: -192.03397179460154[0m
[37m[1m[2023-07-11 02:27:38,270][233954] Mean Reward across all agents: 148.85839701368425[0m
[37m[1m[2023-07-11 02:27:38,270][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:27:38,277][233954] mean_value=-495.628636768404, max_value=837.2520656376844[0m
[37m[1m[2023-07-11 02:27:38,280][233954] New mean coefficients: [[-0.81347597  0.50713015  1.7848057   0.3132767  -2.2659245  -3.1630669 ]][0m
[37m[1m[2023-07-11 02:27:38,280][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:27:46,961][233954] train() took 8.68 seconds to complete[0m
[36m[2023-07-11 02:27:46,961][233954] FPS: 442450.55[0m
[36m[2023-07-11 02:27:46,963][233954] itr=191, itrs=2000, Progress: 9.55%[0m
[36m[2023-07-11 02:27:58,508][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 02:27:58,509][233954] FPS: 333845.43[0m
[36m[2023-07-11 02:28:02,722][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:28:02,723][233954] Reward + Measures: [[-13.18822066   0.34363499   0.56765395   0.10725533   0.42961133
    0.88693637]][0m
[37m[1m[2023-07-11 02:28:02,723][233954] Max Reward on eval: -13.188220664142214[0m
[37m[1m[2023-07-11 02:28:02,723][233954] Min Reward on eval: -13.188220664142214[0m
[37m[1m[2023-07-11 02:28:02,723][233954] Mean Reward across all agents: -13.188220664142214[0m
[37m[1m[2023-07-11 02:28:02,724][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:28:07,658][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:28:07,658][233954] Reward + Measures: [[ 82.95664812   0.1972       0.30500001   0.0768       0.24349999
    1.75601387]
 [152.29083268   0.2332       0.48569998   0.15720001   0.41139999
    1.80685937]
 [ -4.09885984   0.19330001   0.10820001   0.19230001   0.15460001
    2.84683967]
 ...
 [ 75.57374976   0.34629998   0.147        0.33670002   0.287
    3.05560279]
 [ 57.66835964   0.1838       0.25360003   0.08400001   0.21700001
    2.53492355]
 [-68.56113045   0.49750003   0.41170001   0.47040001   0.43630001
    1.91729355]][0m
[37m[1m[2023-07-11 02:28:07,658][233954] Max Reward on eval: 605.7850647434592[0m
[37m[1m[2023-07-11 02:28:07,659][233954] Min Reward on eval: -164.67873401753604[0m
[37m[1m[2023-07-11 02:28:07,659][233954] Mean Reward across all agents: 75.94922119271655[0m
[37m[1m[2023-07-11 02:28:07,659][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:28:07,668][233954] mean_value=-191.1132081832942, max_value=658.8124609653745[0m
[37m[1m[2023-07-11 02:28:07,671][233954] New mean coefficients: [[-0.6780091   0.42891303  2.32214    -0.53807026 -1.6177757  -3.2413733 ]][0m
[37m[1m[2023-07-11 02:28:07,672][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:28:16,716][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 02:28:16,717][233954] FPS: 424646.93[0m
[36m[2023-07-11 02:28:16,719][233954] itr=192, itrs=2000, Progress: 9.60%[0m
[36m[2023-07-11 02:28:28,334][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 02:28:28,334][233954] FPS: 331737.00[0m
[36m[2023-07-11 02:28:32,649][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:28:32,650][233954] Reward + Measures: [[-31.99672314   0.35493162   0.58787465   0.09300967   0.42931664
    0.83598691]][0m
[37m[1m[2023-07-11 02:28:32,650][233954] Max Reward on eval: -31.996723138969934[0m
[37m[1m[2023-07-11 02:28:32,650][233954] Min Reward on eval: -31.996723138969934[0m
[37m[1m[2023-07-11 02:28:32,651][233954] Mean Reward across all agents: -31.996723138969934[0m
[37m[1m[2023-07-11 02:28:32,651][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:28:37,612][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:28:37,613][233954] Reward + Measures: [[194.07165624   0.46099997   0.30650002   0.42640001   0.14100002
    1.76368845]
 [137.68572875   0.48270002   0.1183       0.52180004   0.27130002
    2.41366076]
 [140.3434673    0.50030005   0.17749999   0.5068       0.21069999
    2.43839908]
 ...
 [ 77.19593105   0.1362       0.20679998   0.16         0.1248
    1.98089635]
 [ 59.81081528   0.35420001   0.22590001   0.27850002   0.1846
    2.5598352 ]
 [207.16912847   0.3698       0.24679999   0.44919997   0.18929999
    1.38037646]][0m
[37m[1m[2023-07-11 02:28:37,613][233954] Max Reward on eval: 711.6599769670516[0m
[37m[1m[2023-07-11 02:28:37,614][233954] Min Reward on eval: -105.21118280962109[0m
[37m[1m[2023-07-11 02:28:37,614][233954] Mean Reward across all agents: 125.94037169852217[0m
[37m[1m[2023-07-11 02:28:37,614][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:28:37,623][233954] mean_value=-816.7301307357169, max_value=825.7501139919157[0m
[37m[1m[2023-07-11 02:28:37,626][233954] New mean coefficients: [[-0.7515514   0.39083475  2.6361294  -1.2629452  -0.66155875 -2.9935377 ]][0m
[37m[1m[2023-07-11 02:28:37,627][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:28:46,617][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 02:28:46,618][233954] FPS: 427180.59[0m
[36m[2023-07-11 02:28:46,620][233954] itr=193, itrs=2000, Progress: 9.65%[0m
[36m[2023-07-11 02:28:58,119][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 02:28:58,119][233954] FPS: 335139.10[0m
[36m[2023-07-11 02:29:02,320][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:29:02,320][233954] Reward + Measures: [[-28.62121116   0.38232031   0.62719899   0.08422734   0.43891999
    0.7996071 ]][0m
[37m[1m[2023-07-11 02:29:02,320][233954] Max Reward on eval: -28.621211164266462[0m
[37m[1m[2023-07-11 02:29:02,321][233954] Min Reward on eval: -28.621211164266462[0m
[37m[1m[2023-07-11 02:29:02,321][233954] Mean Reward across all agents: -28.621211164266462[0m
[37m[1m[2023-07-11 02:29:02,321][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:29:07,342][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:29:07,342][233954] Reward + Measures: [[  4.96917592   0.60370004   0.51789999   0.58219999   0.64300007
    2.87549734]
 [ 69.48668076   0.75169998   0.79650003   0.65080005   0.72279996
    2.14908624]
 [ 68.84165265   0.264        0.25870001   0.25630003   0.23290001
    3.1403017 ]
 ...
 [ 90.03395553   0.48090002   0.54279995   0.35810003   0.43099999
    1.75221062]
 [238.25548459   0.17300001   0.35770002   0.22909999   0.22149999
    1.8711313 ]
 [ 58.41069467   0.41120002   0.48010001   0.2129       0.44410005
    1.27094674]][0m
[37m[1m[2023-07-11 02:29:07,343][233954] Max Reward on eval: 520.4592218091711[0m
[37m[1m[2023-07-11 02:29:07,343][233954] Min Reward on eval: -166.86336802076548[0m
[37m[1m[2023-07-11 02:29:07,343][233954] Mean Reward across all agents: 98.22843084926441[0m
[37m[1m[2023-07-11 02:29:07,343][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:29:07,355][233954] mean_value=-65.2373002760902, max_value=908.2809061261127[0m
[37m[1m[2023-07-11 02:29:07,358][233954] New mean coefficients: [[-0.99589276  0.7469424   3.1459599  -2.3605266   0.525941   -2.4099197 ]][0m
[37m[1m[2023-07-11 02:29:07,359][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:29:16,423][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 02:29:16,423][233954] FPS: 423736.31[0m
[36m[2023-07-11 02:29:16,426][233954] itr=194, itrs=2000, Progress: 9.70%[0m
[36m[2023-07-11 02:29:28,065][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 02:29:28,066][233954] FPS: 331148.46[0m
[36m[2023-07-11 02:29:32,536][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:29:32,536][233954] Reward + Measures: [[-24.94486206   0.41303635   0.664074     0.07556266   0.45901862
    0.76256794]][0m
[37m[1m[2023-07-11 02:29:32,537][233954] Max Reward on eval: -24.944862057310363[0m
[37m[1m[2023-07-11 02:29:32,537][233954] Min Reward on eval: -24.944862057310363[0m
[37m[1m[2023-07-11 02:29:32,537][233954] Mean Reward across all agents: -24.944862057310363[0m
[37m[1m[2023-07-11 02:29:32,538][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:29:37,550][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:29:37,550][233954] Reward + Measures: [[ -2.72287915   0.42430001   0.0768       0.34520003   0.2969
    2.31824899]
 [ 75.1968958    0.11960001   0.1869       0.1277       0.14480001
    2.90449786]
 [ 35.71497745   0.29159999   0.28940001   0.2254       0.2667
    1.837304  ]
 ...
 [-12.44759496   0.2861       0.3901       0.4289       0.40650001
    2.77654696]
 [ 35.54630659   0.33559999   0.35499999   0.27410001   0.2746
    1.86840045]
 [ -6.70156082   0.50550002   0.0837       0.45820004   0.27270004
    2.32418299]][0m
[37m[1m[2023-07-11 02:29:37,551][233954] Max Reward on eval: 555.0842705025337[0m
[37m[1m[2023-07-11 02:29:37,551][233954] Min Reward on eval: -94.5703630930744[0m
[37m[1m[2023-07-11 02:29:37,551][233954] Mean Reward across all agents: 85.04696975163266[0m
[37m[1m[2023-07-11 02:29:37,551][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:29:37,563][233954] mean_value=-405.1567639009822, max_value=815.8606151536806[0m
[37m[1m[2023-07-11 02:29:37,566][233954] New mean coefficients: [[-0.73490715  0.82171035  4.0891986  -2.0029945   1.2308986  -2.068828  ]][0m
[37m[1m[2023-07-11 02:29:37,567][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:29:46,491][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 02:29:46,491][233954] FPS: 430373.36[0m
[36m[2023-07-11 02:29:46,494][233954] itr=195, itrs=2000, Progress: 9.75%[0m
[36m[2023-07-11 02:29:58,205][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 02:29:58,205][233954] FPS: 329044.09[0m
[36m[2023-07-11 02:30:02,487][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:30:02,487][233954] Reward + Measures: [[-24.31972032   0.44739202   0.71194196   0.06227767   0.50266463
    0.73813808]][0m
[37m[1m[2023-07-11 02:30:02,487][233954] Max Reward on eval: -24.319720320462856[0m
[37m[1m[2023-07-11 02:30:02,488][233954] Min Reward on eval: -24.319720320462856[0m
[37m[1m[2023-07-11 02:30:02,488][233954] Mean Reward across all agents: -24.319720320462856[0m
[37m[1m[2023-07-11 02:30:02,488][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:30:07,458][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:30:07,459][233954] Reward + Measures: [[309.33389283   0.61899996   0.29530001   0.53670007   0.1918
    1.7690438 ]
 [-32.39444193   0.1541       0.62529999   0.21180001   0.51940006
    1.43699622]
 [316.35664988   0.2043       0.32430002   0.31659999   0.1202
    1.90248573]
 ...
 [663.97736929   0.42300007   0.292        0.41289997   0.15799999
    1.73750782]
 [ 70.69497777   0.1044       0.16010001   0.1434       0.1227
    2.37611461]
 [ 34.02931797   0.19350001   0.20040002   0.23480001   0.0997
    1.72909701]][0m
[37m[1m[2023-07-11 02:30:07,459][233954] Max Reward on eval: 1150.606388099864[0m
[37m[1m[2023-07-11 02:30:07,459][233954] Min Reward on eval: -157.29315529076848[0m
[37m[1m[2023-07-11 02:30:07,460][233954] Mean Reward across all agents: 186.15736472029266[0m
[37m[1m[2023-07-11 02:30:07,460][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:30:07,471][233954] mean_value=-649.1341164537506, max_value=1031.4871520635904[0m
[37m[1m[2023-07-11 02:30:07,474][233954] New mean coefficients: [[-0.9254434   0.6761501   2.985252   -1.1492388  -0.41209555 -2.5441997 ]][0m
[37m[1m[2023-07-11 02:30:07,475][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:30:16,513][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 02:30:16,513][233954] FPS: 424956.09[0m
[36m[2023-07-11 02:30:16,515][233954] itr=196, itrs=2000, Progress: 9.80%[0m
[36m[2023-07-11 02:30:28,174][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 02:30:28,174][233954] FPS: 330616.23[0m
[36m[2023-07-11 02:30:32,506][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:30:32,507][233954] Reward + Measures: [[-30.02223781   0.48684603   0.76554334   0.059512     0.52501065
    0.74300748]][0m
[37m[1m[2023-07-11 02:30:32,507][233954] Max Reward on eval: -30.022237812991612[0m
[37m[1m[2023-07-11 02:30:32,507][233954] Min Reward on eval: -30.022237812991612[0m
[37m[1m[2023-07-11 02:30:32,508][233954] Mean Reward across all agents: -30.022237812991612[0m
[37m[1m[2023-07-11 02:30:32,508][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:30:37,497][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:30:37,498][233954] Reward + Measures: [[ -47.76786275    0.22760001    0.35100001    0.1481        0.19950001
     1.98128498]
 [-102.54182485    0.0546        0.8537001     0.3809        0.824
     2.66352201]
 [ 117.58677062    0.61739999    0.78099996    0.07080001    0.74579996
     2.23600078]
 ...
 [ 173.20251281    0.21030001    0.329         0.266         0.271
     2.48627925]
 [  50.16585125    0.60170001    0.60729998    0.58719999    0.56239998
     2.64748049]
 [ 213.73326135    0.40519997    0.33520001    0.34029999    0.1157
     1.98443604]][0m
[37m[1m[2023-07-11 02:30:37,498][233954] Max Reward on eval: 940.1837043866515[0m
[37m[1m[2023-07-11 02:30:37,498][233954] Min Reward on eval: -177.31624649204315[0m
[37m[1m[2023-07-11 02:30:37,499][233954] Mean Reward across all agents: 159.92246690676762[0m
[37m[1m[2023-07-11 02:30:37,499][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:30:37,507][233954] mean_value=-535.0181877780815, max_value=906.8252105835825[0m
[37m[1m[2023-07-11 02:30:37,510][233954] New mean coefficients: [[-0.8549527   0.6036013   3.1404314  -0.81930804 -0.551813   -2.5986035 ]][0m
[37m[1m[2023-07-11 02:30:37,511][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:30:46,570][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 02:30:46,570][233954] FPS: 423976.73[0m
[36m[2023-07-11 02:30:46,572][233954] itr=197, itrs=2000, Progress: 9.85%[0m
[36m[2023-07-11 02:30:58,089][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 02:30:58,094][233954] FPS: 334627.68[0m
[36m[2023-07-11 02:31:02,338][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:31:02,339][233954] Reward + Measures: [[-25.3138165    0.54822105   0.81387931   0.05664366   0.55244333
    0.7119118 ]][0m
[37m[1m[2023-07-11 02:31:02,339][233954] Max Reward on eval: -25.313816502712196[0m
[37m[1m[2023-07-11 02:31:02,339][233954] Min Reward on eval: -25.313816502712196[0m
[37m[1m[2023-07-11 02:31:02,339][233954] Mean Reward across all agents: -25.313816502712196[0m
[37m[1m[2023-07-11 02:31:02,340][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:31:07,366][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:31:07,367][233954] Reward + Measures: [[135.52133378   0.30450001   0.15809999   0.33759999   0.1945
    2.39607072]
 [ -4.95178211   0.2069       0.32459998   0.34740001   0.31119999
    2.2859695 ]
 [156.54997299   0.20499997   0.4269       0.21350001   0.36670002
    1.95641351]
 ...
 [ 91.44880692   0.35090002   0.44099998   0.16650002   0.39340001
    1.6437037 ]
 [ 92.31462099   0.39000002   0.6268       0.20969999   0.47370005
    0.90917635]
 [180.39847216   0.22690001   0.33840001   0.40440002   0.23670001
    1.84083521]][0m
[37m[1m[2023-07-11 02:31:07,367][233954] Max Reward on eval: 1054.2521743683144[0m
[37m[1m[2023-07-11 02:31:07,368][233954] Min Reward on eval: -132.79309795359148[0m
[37m[1m[2023-07-11 02:31:07,368][233954] Mean Reward across all agents: 177.82010969589166[0m
[37m[1m[2023-07-11 02:31:07,368][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:31:07,384][233954] mean_value=-139.78947375331626, max_value=1425.4084167662077[0m
[37m[1m[2023-07-11 02:31:07,386][233954] New mean coefficients: [[-0.96893436  0.38171482  2.7493603  -0.5676856  -0.7896165  -2.4467561 ]][0m
[37m[1m[2023-07-11 02:31:07,387][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:31:16,306][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 02:31:16,307][233954] FPS: 430625.06[0m
[36m[2023-07-11 02:31:16,309][233954] itr=198, itrs=2000, Progress: 9.90%[0m
[36m[2023-07-11 02:31:27,871][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 02:31:27,871][233954] FPS: 333289.35[0m
[36m[2023-07-11 02:31:32,214][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:31:32,219][233954] Reward + Measures: [[-30.04215492   0.60270697   0.84693635   0.04691767   0.57188165
    0.64361578]][0m
[37m[1m[2023-07-11 02:31:32,220][233954] Max Reward on eval: -30.042154922353976[0m
[37m[1m[2023-07-11 02:31:32,220][233954] Min Reward on eval: -30.042154922353976[0m
[37m[1m[2023-07-11 02:31:32,220][233954] Mean Reward across all agents: -30.042154922353976[0m
[37m[1m[2023-07-11 02:31:32,221][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:31:37,376][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:31:37,381][233954] Reward + Measures: [[380.29107857   0.27770001   0.46370003   0.34900004   0.43789998
    1.6537298 ]
 [ 17.8532956    0.28740001   0.3224       0.0769       0.31020004
    2.645082  ]
 [221.6364169    0.17820001   0.46869999   0.245        0.40529999
    2.18479156]
 ...
 [179.66511822   0.075        0.67839998   0.32100001   0.56210005
    2.62805533]
 [164.03425563   0.20080002   0.4619       0.22749999   0.32510003
    2.11188769]
 [217.66355992   0.0856       0.52500004   0.24160002   0.4382
    2.83996439]][0m
[37m[1m[2023-07-11 02:31:37,382][233954] Max Reward on eval: 400.91219420819544[0m
[37m[1m[2023-07-11 02:31:37,382][233954] Min Reward on eval: -362.4291916131973[0m
[37m[1m[2023-07-11 02:31:37,382][233954] Mean Reward across all agents: 115.69909226950163[0m
[37m[1m[2023-07-11 02:31:37,382][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:31:37,393][233954] mean_value=-353.5752361883872, max_value=831.4657926802698[0m
[37m[1m[2023-07-11 02:31:37,396][233954] New mean coefficients: [[-0.5538074   0.19123863  2.62497     1.5435996  -1.6619368  -2.3795936 ]][0m
[37m[1m[2023-07-11 02:31:37,397][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:31:46,458][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 02:31:46,459][233954] FPS: 423861.57[0m
[36m[2023-07-11 02:31:46,461][233954] itr=199, itrs=2000, Progress: 9.95%[0m
[36m[2023-07-11 02:31:58,002][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 02:31:58,003][233954] FPS: 334014.42[0m
[36m[2023-07-11 02:32:02,306][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:32:02,306][233954] Reward + Measures: [[-39.7443661    0.61211365   0.85896432   0.04008967   0.53116935
    0.59863442]][0m
[37m[1m[2023-07-11 02:32:02,307][233954] Max Reward on eval: -39.74436609931323[0m
[37m[1m[2023-07-11 02:32:02,307][233954] Min Reward on eval: -39.74436609931323[0m
[37m[1m[2023-07-11 02:32:02,307][233954] Mean Reward across all agents: -39.74436609931323[0m
[37m[1m[2023-07-11 02:32:02,307][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:32:07,298][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:32:07,299][233954] Reward + Measures: [[ 71.7388173    0.3258       0.49650002   0.22360002   0.33530003
    1.10122049]
 [ 98.11563539   0.34900001   0.38150001   0.35080001   0.34349999
    1.67791545]
 [145.13686085   0.87629998   0.18380001   0.88590002   0.42539999
    2.88062358]
 ...
 [ 46.84605933   0.38189998   0.34660003   0.41669998   0.3281
    1.66976702]
 [ 37.47539887   0.12820001   0.24609999   0.1556       0.2261
    1.35630381]
 [163.63505746   0.17340001   0.43010002   0.10950001   0.41359997
    2.3388145 ]][0m
[37m[1m[2023-07-11 02:32:07,299][233954] Max Reward on eval: 548.8769893072546[0m
[37m[1m[2023-07-11 02:32:07,299][233954] Min Reward on eval: -179.10564995326567[0m
[37m[1m[2023-07-11 02:32:07,300][233954] Mean Reward across all agents: 110.99989955889633[0m
[37m[1m[2023-07-11 02:32:07,300][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:32:07,314][233954] mean_value=-137.21231008116447, max_value=853.0409218435314[0m
[37m[1m[2023-07-11 02:32:07,316][233954] New mean coefficients: [[-0.6797394   1.7308497   3.1741347   0.69761914  0.57013154 -1.5744269 ]][0m
[37m[1m[2023-07-11 02:32:07,317][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:32:16,385][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 02:32:16,385][233954] FPS: 423557.18[0m
[36m[2023-07-11 02:32:16,388][233954] itr=200, itrs=2000, Progress: 10.00%[0m
[37m[1m[2023-07-11 02:34:48,879][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000180[0m
[36m[2023-07-11 02:35:01,100][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 02:35:01,101][233954] FPS: 330590.06[0m
[36m[2023-07-11 02:35:05,398][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:35:05,399][233954] Reward + Measures: [[-43.51027979   0.63024765   0.87177896   0.03623467   0.52024966
    0.57050461]][0m
[37m[1m[2023-07-11 02:35:05,399][233954] Max Reward on eval: -43.510279793657894[0m
[37m[1m[2023-07-11 02:35:05,399][233954] Min Reward on eval: -43.510279793657894[0m
[37m[1m[2023-07-11 02:35:05,399][233954] Mean Reward across all agents: -43.510279793657894[0m
[37m[1m[2023-07-11 02:35:05,400][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:35:10,286][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:35:10,287][233954] Reward + Measures: [[ 69.25037206   0.5474       0.31780002   0.53109998   0.38680002
    2.68683076]
 [256.14430915   0.26540002   0.37009999   0.11130001   0.30790001
    1.40117252]
 [194.43500799   0.72760004   0.23400001   0.47110006   0.4887
    3.29738116]
 ...
 [247.60375888   0.21620002   0.32200003   0.30860001   0.24190001
    2.31455922]
 [370.42598553   0.31420001   0.53820002   0.1173       0.3594
    1.27276456]
 [272.43423465   0.2225       0.31420001   0.1981       0.2174
    2.07236719]][0m
[37m[1m[2023-07-11 02:35:10,287][233954] Max Reward on eval: 692.1821880325209[0m
[37m[1m[2023-07-11 02:35:10,287][233954] Min Reward on eval: -225.1786251053214[0m
[37m[1m[2023-07-11 02:35:10,287][233954] Mean Reward across all agents: 156.57973539550827[0m
[37m[1m[2023-07-11 02:35:10,288][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:35:10,304][233954] mean_value=-70.37535637783823, max_value=965.8018396373303[0m
[37m[1m[2023-07-11 02:35:10,307][233954] New mean coefficients: [[-0.5942861  1.7646796  3.7391737 -1.3000572  1.990791  -1.2915802]][0m
[37m[1m[2023-07-11 02:35:10,308][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:35:19,199][233954] train() took 8.89 seconds to complete[0m
[36m[2023-07-11 02:35:19,199][233954] FPS: 431980.54[0m
[36m[2023-07-11 02:35:19,201][233954] itr=201, itrs=2000, Progress: 10.05%[0m
[36m[2023-07-11 02:35:30,891][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 02:35:30,891][233954] FPS: 329683.22[0m
[36m[2023-07-11 02:35:35,096][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:35:35,096][233954] Reward + Measures: [[-19.6685617    0.67514837   0.88702101   0.036194     0.57370263
    0.51118147]][0m
[37m[1m[2023-07-11 02:35:35,096][233954] Max Reward on eval: -19.668561698516825[0m
[37m[1m[2023-07-11 02:35:35,097][233954] Min Reward on eval: -19.668561698516825[0m
[37m[1m[2023-07-11 02:35:35,097][233954] Mean Reward across all agents: -19.668561698516825[0m
[37m[1m[2023-07-11 02:35:35,097][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:35:39,826][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:35:39,827][233954] Reward + Measures: [[ 86.44531128   0.25400001   0.391        0.26130003   0.34520003
    2.19297075]
 [ 53.94894132   0.77160001   0.83249998   0.80150002   0.82650006
    1.68010998]
 [139.86119196   0.28730002   0.52060002   0.31669998   0.51230001
    1.76644289]
 ...
 [ 81.44501187   0.27320001   0.84890002   0.33820003   0.75460005
    0.82518673]
 [-56.8917348    0.32440001   0.33339998   0.2933       0.31560001
    2.13796401]
 [317.4341288    0.1706       0.30239999   0.20680001   0.2366
    2.53196001]][0m
[37m[1m[2023-07-11 02:35:39,827][233954] Max Reward on eval: 886.0418517765007[0m
[37m[1m[2023-07-11 02:35:39,827][233954] Min Reward on eval: -198.50926015861333[0m
[37m[1m[2023-07-11 02:35:39,828][233954] Mean Reward across all agents: 95.81558807627322[0m
[37m[1m[2023-07-11 02:35:39,828][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:35:39,842][233954] mean_value=-251.1129975076759, max_value=1081.8720299849479[0m
[37m[1m[2023-07-11 02:35:39,845][233954] New mean coefficients: [[-0.33087778  1.3398105   3.6442366  -0.19963932  1.0433991  -1.8563586 ]][0m
[37m[1m[2023-07-11 02:35:39,846][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:35:48,698][233954] train() took 8.85 seconds to complete[0m
[36m[2023-07-11 02:35:48,698][233954] FPS: 433906.43[0m
[36m[2023-07-11 02:35:48,700][233954] itr=202, itrs=2000, Progress: 10.10%[0m
[36m[2023-07-11 02:36:00,290][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 02:36:00,290][233954] FPS: 332508.52[0m
[36m[2023-07-11 02:36:04,611][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:36:04,611][233954] Reward + Measures: [[-8.23678413  0.71132761  0.90022999  0.035306    0.6360023   0.48730379]][0m
[37m[1m[2023-07-11 02:36:04,612][233954] Max Reward on eval: -8.23678412808979[0m
[37m[1m[2023-07-11 02:36:04,612][233954] Min Reward on eval: -8.23678412808979[0m
[37m[1m[2023-07-11 02:36:04,612][233954] Mean Reward across all agents: -8.23678412808979[0m
[37m[1m[2023-07-11 02:36:04,612][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:36:09,600][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:36:09,600][233954] Reward + Measures: [[ 56.64318559   0.1217       0.1358       0.1303       0.1163
    2.82799149]
 [515.42828942   0.33050001   0.39089999   0.38930002   0.18620001
    1.20667589]
 [293.28019357   0.21610001   0.21390001   0.24919999   0.18270001
    1.86954117]
 ...
 [ 65.33365559   0.14960001   0.14060001   0.14480001   0.1184
    2.5593164 ]
 [100.00886859   0.28549999   0.12890001   0.30250001   0.16860001
    1.84139335]
 [144.9086113    0.45290002   0.65290004   0.40089998   0.46059999
    1.09266031]][0m
[37m[1m[2023-07-11 02:36:09,600][233954] Max Reward on eval: 942.3249397028237[0m
[37m[1m[2023-07-11 02:36:09,601][233954] Min Reward on eval: -153.63406085099558[0m
[37m[1m[2023-07-11 02:36:09,601][233954] Mean Reward across all agents: 111.72234817808464[0m
[37m[1m[2023-07-11 02:36:09,601][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:36:09,612][233954] mean_value=-219.61463929915936, max_value=802.6660528827458[0m
[37m[1m[2023-07-11 02:36:09,615][233954] New mean coefficients: [[-0.00072154  0.77480125  4.092555   -0.7650011   1.380398   -1.864119  ]][0m
[37m[1m[2023-07-11 02:36:09,616][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:36:18,601][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 02:36:18,601][233954] FPS: 427429.09[0m
[36m[2023-07-11 02:36:18,604][233954] itr=203, itrs=2000, Progress: 10.15%[0m
[36m[2023-07-11 02:36:30,132][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 02:36:30,132][233954] FPS: 334317.42[0m
[36m[2023-07-11 02:36:34,388][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:36:34,388][233954] Reward + Measures: [[-20.45557246   0.73172605   0.92070067   0.03320467   0.70679641
    0.47136599]][0m
[37m[1m[2023-07-11 02:36:34,388][233954] Max Reward on eval: -20.45557246136271[0m
[37m[1m[2023-07-11 02:36:34,389][233954] Min Reward on eval: -20.45557246136271[0m
[37m[1m[2023-07-11 02:36:34,389][233954] Mean Reward across all agents: -20.45557246136271[0m
[37m[1m[2023-07-11 02:36:34,389][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:36:39,582][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:36:39,582][233954] Reward + Measures: [[-73.50566505   0.58120006   0.58700001   0.55689996   0.52359998
    3.48143244]
 [267.66539818   0.1849       0.42550001   0.1938       0.28119999
    1.53028047]
 [-31.74815161   0.70830005   0.40109998   0.69029999   0.0619
    2.68921351]
 ...
 [-62.47899988   0.40430003   0.44060001   0.39009997   0.38420001
    3.08123207]
 [166.61415816   0.21610001   0.38340002   0.28190002   0.266
    1.53994048]
 [-43.20205964   0.47440001   0.54220003   0.4199       0.43140003
    2.6478188 ]][0m
[37m[1m[2023-07-11 02:36:39,582][233954] Max Reward on eval: 956.5228844150901[0m
[37m[1m[2023-07-11 02:36:39,583][233954] Min Reward on eval: -217.20798634607345[0m
[37m[1m[2023-07-11 02:36:39,583][233954] Mean Reward across all agents: 75.78962797150128[0m
[37m[1m[2023-07-11 02:36:39,583][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:36:39,591][233954] mean_value=-636.4909249719716, max_value=695.1059867247188[0m
[37m[1m[2023-07-11 02:36:39,593][233954] New mean coefficients: [[ 0.01851248  0.09192371  3.2927814  -0.47798315  0.2608341  -1.6556861 ]][0m
[37m[1m[2023-07-11 02:36:39,594][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:36:48,636][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 02:36:48,636][233954] FPS: 424785.76[0m
[36m[2023-07-11 02:36:48,639][233954] itr=204, itrs=2000, Progress: 10.20%[0m
[36m[2023-07-11 02:37:00,269][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 02:37:00,269][233954] FPS: 331409.44[0m
[36m[2023-07-11 02:37:04,529][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:37:04,534][233954] Reward + Measures: [[15.18867393  0.7742964   0.93262094  0.03556333  0.74668002  0.45394605]][0m
[37m[1m[2023-07-11 02:37:04,534][233954] Max Reward on eval: 15.188673933792058[0m
[37m[1m[2023-07-11 02:37:04,535][233954] Min Reward on eval: 15.188673933792058[0m
[37m[1m[2023-07-11 02:37:04,535][233954] Mean Reward across all agents: 15.188673933792058[0m
[37m[1m[2023-07-11 02:37:04,535][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:37:09,483][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:37:09,484][233954] Reward + Measures: [[132.44317511   0.2112       0.51960003   0.28210002   0.48050004
    2.25136256]
 [ 55.85456478   0.2103       0.49260002   0.28740001   0.33930001
    1.08739889]
 [116.95598984   0.58669996   0.81150001   0.09770001   0.68390006
    1.64383149]
 ...
 [ 13.11676698   0.46379995   0.66420001   0.12849998   0.68580002
    1.93988168]
 [135.70346212   0.15949999   0.17460001   0.1488       0.14070001
    1.81901169]
 [ 70.6845998    0.2721       0.45320001   0.43239999   0.43099999
    1.32489419]][0m
[37m[1m[2023-07-11 02:37:09,484][233954] Max Reward on eval: 442.1040687825531[0m
[37m[1m[2023-07-11 02:37:09,484][233954] Min Reward on eval: -121.61885691182688[0m
[37m[1m[2023-07-11 02:37:09,484][233954] Mean Reward across all agents: 109.71207909136275[0m
[37m[1m[2023-07-11 02:37:09,485][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:37:09,498][233954] mean_value=-481.28145383443245, max_value=740.348298895359[0m
[37m[1m[2023-07-11 02:37:09,501][233954] New mean coefficients: [[-0.443136    0.08807516  2.6760225  -0.57797635 -0.16924608 -1.828707  ]][0m
[37m[1m[2023-07-11 02:37:09,502][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:37:18,437][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 02:37:18,437][233954] FPS: 429840.03[0m
[36m[2023-07-11 02:37:18,440][233954] itr=205, itrs=2000, Progress: 10.25%[0m
[36m[2023-07-11 02:37:30,303][233954] train() took 11.82 seconds to complete[0m
[36m[2023-07-11 02:37:30,303][233954] FPS: 324963.09[0m
[36m[2023-07-11 02:37:34,550][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:37:34,551][233954] Reward + Measures: [[12.66227905  0.77651161  0.93629134  0.033853    0.74700767  0.41463441]][0m
[37m[1m[2023-07-11 02:37:34,551][233954] Max Reward on eval: 12.662279054854459[0m
[37m[1m[2023-07-11 02:37:34,551][233954] Min Reward on eval: 12.662279054854459[0m
[37m[1m[2023-07-11 02:37:34,552][233954] Mean Reward across all agents: 12.662279054854459[0m
[37m[1m[2023-07-11 02:37:34,552][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:37:39,503][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:37:39,504][233954] Reward + Measures: [[ 108.62797402    0.46850005    0.79160005    0.3184        0.63279998
     1.03499079]
 [-122.15813993    0.40430003    0.52170002    0.43900004    0.5697
     2.06636596]
 [  11.24464532    0.2036        0.57989997    0.2782        0.5485
     1.29429626]
 ...
 [  62.80087252    0.69880003    0.52540004    0.59839994    0.20369999
     0.54707426]
 [ 480.75129125    0.43220001    0.61860001    0.42590004    0.52790004
     1.80240715]
 [  87.70607878    0.51370001    0.72720003    0.671         0.67440003
     2.0596745 ]][0m
[37m[1m[2023-07-11 02:37:39,504][233954] Max Reward on eval: 822.8723792811855[0m
[37m[1m[2023-07-11 02:37:39,504][233954] Min Reward on eval: -186.26261842017993[0m
[37m[1m[2023-07-11 02:37:39,504][233954] Mean Reward across all agents: 130.37970225586963[0m
[37m[1m[2023-07-11 02:37:39,505][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:37:39,526][233954] mean_value=241.1245633539158, max_value=925.1665996674894[0m
[37m[1m[2023-07-11 02:37:39,529][233954] New mean coefficients: [[-0.07708761  0.08460037  2.499122    1.3152663  -1.2832044  -2.0999706 ]][0m
[37m[1m[2023-07-11 02:37:39,530][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:37:48,545][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 02:37:48,546][233954] FPS: 426027.48[0m
[36m[2023-07-11 02:37:48,548][233954] itr=206, itrs=2000, Progress: 10.30%[0m
[36m[2023-07-11 02:38:00,232][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 02:38:00,233][233954] FPS: 329953.90[0m
[36m[2023-07-11 02:38:04,501][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:38:04,502][233954] Reward + Measures: [[-1.28214232  0.76550192  0.93723094  0.03491433  0.680237    0.37584049]][0m
[37m[1m[2023-07-11 02:38:04,502][233954] Max Reward on eval: -1.2821423238183585[0m
[37m[1m[2023-07-11 02:38:04,502][233954] Min Reward on eval: -1.2821423238183585[0m
[37m[1m[2023-07-11 02:38:04,503][233954] Mean Reward across all agents: -1.2821423238183585[0m
[37m[1m[2023-07-11 02:38:04,503][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:38:09,487][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:38:09,488][233954] Reward + Measures: [[465.64005659   0.25820002   0.35679999   0.3351       0.15869999
    1.1523782 ]
 [440.44385917   0.29749998   0.2999       0.29899999   0.19599999
    0.96030086]
 [187.45592501   0.37029999   0.30599999   0.52899998   0.1714
    2.07539344]
 ...
 [452.05171296   0.32119998   0.32699999   0.49359998   0.1026
    1.30564404]
 [622.19374843   0.2342       0.3935       0.3457       0.2494
    1.58826244]
 [977.02487182   0.26519999   0.43940002   0.3687       0.1086
    1.15584493]][0m
[37m[1m[2023-07-11 02:38:09,488][233954] Max Reward on eval: 1005.8045937797054[0m
[37m[1m[2023-07-11 02:38:09,488][233954] Min Reward on eval: -131.30926612392068[0m
[37m[1m[2023-07-11 02:38:09,488][233954] Mean Reward across all agents: 215.9003965812359[0m
[37m[1m[2023-07-11 02:38:09,489][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:38:09,507][233954] mean_value=-409.5276183242754, max_value=1195.8847121686658[0m
[37m[1m[2023-07-11 02:38:09,510][233954] New mean coefficients: [[-0.3210107   0.07272817  3.0087376   0.03087175 -0.47273052 -1.7740593 ]][0m
[37m[1m[2023-07-11 02:38:09,511][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:38:18,457][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 02:38:18,458][233954] FPS: 429289.34[0m
[36m[2023-07-11 02:38:18,460][233954] itr=207, itrs=2000, Progress: 10.35%[0m
[36m[2023-07-11 02:38:30,008][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 02:38:30,009][233954] FPS: 333785.57[0m
[36m[2023-07-11 02:38:34,308][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:38:34,309][233954] Reward + Measures: [[-4.77612985  0.76251763  0.94226193  0.03037333  0.65653133  0.35695446]][0m
[37m[1m[2023-07-11 02:38:34,309][233954] Max Reward on eval: -4.776129851163341[0m
[37m[1m[2023-07-11 02:38:34,309][233954] Min Reward on eval: -4.776129851163341[0m
[37m[1m[2023-07-11 02:38:34,310][233954] Mean Reward across all agents: -4.776129851163341[0m
[37m[1m[2023-07-11 02:38:34,310][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:38:39,497][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:38:39,497][233954] Reward + Measures: [[120.77046778   0.27360001   0.7191       0.3818       0.55120003
    1.31914055]
 [174.85050773   0.52239996   0.15030001   0.56720001   0.48629999
    2.72444034]
 [142.06842136   0.24429999   0.62280005   0.22740002   0.53579998
    1.15709054]
 ...
 [405.76181935   0.23330002   0.40630004   0.3263       0.26460001
    0.99333841]
 [ 25.52836872   0.19990002   0.50819999   0.29359999   0.4409
    1.38915408]
 [125.09939435   0.48140001   0.71460003   0.18080001   0.47319999
    1.55253541]][0m
[37m[1m[2023-07-11 02:38:39,497][233954] Max Reward on eval: 688.9363770999014[0m
[37m[1m[2023-07-11 02:38:39,498][233954] Min Reward on eval: -70.40127181400312[0m
[37m[1m[2023-07-11 02:38:39,498][233954] Mean Reward across all agents: 141.11405349274335[0m
[37m[1m[2023-07-11 02:38:39,498][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:38:39,517][233954] mean_value=-478.5015303050907, max_value=891.9870662907372[0m
[37m[1m[2023-07-11 02:38:39,520][233954] New mean coefficients: [[-0.30571374 -0.6721976   2.7294056   1.7985989  -1.6391596  -1.7017701 ]][0m
[37m[1m[2023-07-11 02:38:39,521][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:38:48,575][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 02:38:48,575][233954] FPS: 424188.51[0m
[36m[2023-07-11 02:38:48,578][233954] itr=208, itrs=2000, Progress: 10.40%[0m
[36m[2023-07-11 02:39:00,240][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 02:39:00,241][233954] FPS: 330528.04[0m
[36m[2023-07-11 02:39:04,542][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:39:04,542][233954] Reward + Measures: [[-6.49121907  0.27157968  0.7790153   0.25867367  0.55014771  0.9804917 ]][0m
[37m[1m[2023-07-11 02:39:04,543][233954] Max Reward on eval: -6.491219073855066[0m
[37m[1m[2023-07-11 02:39:04,543][233954] Min Reward on eval: -6.491219073855066[0m
[37m[1m[2023-07-11 02:39:04,543][233954] Mean Reward across all agents: -6.491219073855066[0m
[37m[1m[2023-07-11 02:39:04,543][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:39:09,567][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:39:09,567][233954] Reward + Measures: [[229.59082354   0.1815       0.3633       0.23720002   0.31070003
    1.83153939]
 [-53.8312521    0.3515       0.61809999   0.0915       0.5492
    1.60021484]
 [ 73.47878153   0.45380002   0.46290007   0.41120002   0.46160004
    2.29800916]
 ...
 [ 33.24501525   0.38690001   0.63880002   0.26370001   0.53049999
    1.62836957]
 [ 71.31542209   0.1981       0.29410002   0.1146       0.28780004
    1.88306487]
 [237.43108147   0.155        0.63550007   0.42379999   0.49449998
    1.37205291]][0m
[37m[1m[2023-07-11 02:39:09,567][233954] Max Reward on eval: 432.0223418450449[0m
[37m[1m[2023-07-11 02:39:09,568][233954] Min Reward on eval: -130.88606354389339[0m
[37m[1m[2023-07-11 02:39:09,568][233954] Mean Reward across all agents: 70.37091253618911[0m
[37m[1m[2023-07-11 02:39:09,568][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:39:09,582][233954] mean_value=-227.28640654889804, max_value=678.3929775599483[0m
[37m[1m[2023-07-11 02:39:09,585][233954] New mean coefficients: [[-0.6074257 -1.0292218  2.6441643  2.1316655 -1.8176329 -2.0325947]][0m
[37m[1m[2023-07-11 02:39:09,586][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:39:18,631][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 02:39:18,631][233954] FPS: 424607.17[0m
[36m[2023-07-11 02:39:18,633][233954] itr=209, itrs=2000, Progress: 10.45%[0m
[36m[2023-07-11 02:39:30,304][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 02:39:30,304][233954] FPS: 330299.22[0m
[36m[2023-07-11 02:39:34,628][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:39:34,629][233954] Reward + Measures: [[-142.70252144    0.47065768    0.8140527     0.048654      0.43165332
     0.63757682]][0m
[37m[1m[2023-07-11 02:39:34,629][233954] Max Reward on eval: -142.7025214442114[0m
[37m[1m[2023-07-11 02:39:34,629][233954] Min Reward on eval: -142.7025214442114[0m
[37m[1m[2023-07-11 02:39:34,629][233954] Mean Reward across all agents: -142.7025214442114[0m
[37m[1m[2023-07-11 02:39:34,630][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:39:39,605][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:39:39,605][233954] Reward + Measures: [[ 31.09852619   0.3272       0.4427       0.38610002   0.1459
    1.72430408]
 [237.97801783   0.4501       0.58260006   0.2263       0.46919999
    2.60624504]
 [203.3508139    0.43670002   0.52630001   0.27410004   0.44160005
    2.6614151 ]
 ...
 [ 91.21286943   0.3863       0.67909998   0.28         0.64679998
    2.41405177]
 [ 43.09542633   0.19680001   0.37559998   0.1533       0.32440004
    1.54075563]
 [289.7490415    0.56109995   0.65539998   0.23190001   0.55970001
    2.16467094]][0m
[37m[1m[2023-07-11 02:39:39,606][233954] Max Reward on eval: 370.39266105815767[0m
[37m[1m[2023-07-11 02:39:39,606][233954] Min Reward on eval: -187.2324511998333[0m
[37m[1m[2023-07-11 02:39:39,606][233954] Mean Reward across all agents: 117.66024104345007[0m
[37m[1m[2023-07-11 02:39:39,606][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:39:39,620][233954] mean_value=-231.49110954897643, max_value=821.4139022557065[0m
[37m[1m[2023-07-11 02:39:39,623][233954] New mean coefficients: [[-0.56133616 -1.2332823   2.2722847   3.3690143  -2.252915   -2.0648763 ]][0m
[37m[1m[2023-07-11 02:39:39,624][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:39:48,652][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 02:39:48,653][233954] FPS: 425378.58[0m
[36m[2023-07-11 02:39:48,655][233954] itr=210, itrs=2000, Progress: 10.50%[0m
[37m[1m[2023-07-11 02:42:25,651][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000190[0m
[36m[2023-07-11 02:42:37,986][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 02:42:37,986][233954] FPS: 329728.32[0m
[36m[2023-07-11 02:42:42,266][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:42:42,267][233954] Reward + Measures: [[-249.90434163    0.45835799    0.83154136    0.03839967    0.37230331
     0.59352744]][0m
[37m[1m[2023-07-11 02:42:42,267][233954] Max Reward on eval: -249.90434162668217[0m
[37m[1m[2023-07-11 02:42:42,267][233954] Min Reward on eval: -249.90434162668217[0m
[37m[1m[2023-07-11 02:42:42,267][233954] Mean Reward across all agents: -249.90434162668217[0m
[37m[1m[2023-07-11 02:42:42,268][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:42:47,228][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:42:47,229][233954] Reward + Measures: [[ 81.3618011    0.13740002   0.43399999   0.28299999   0.29480001
    2.16922092]
 [182.91761975   0.88199997   0.0122       0.79039997   0.74720001
    2.66544604]
 [248.57711956   0.77060002   0.0044       0.78619999   0.7737
    2.95269823]
 ...
 [371.16557213   0.88430005   0.0269       0.85170001   0.79170007
    2.00401378]
 [113.07314973   0.79220003   0.0311       0.71970004   0.70410007
    2.87181234]
 [-11.11839011   0.44309998   0.21950002   0.36340004   0.34959999
    2.40748572]][0m
[37m[1m[2023-07-11 02:42:47,229][233954] Max Reward on eval: 715.2673892738763[0m
[37m[1m[2023-07-11 02:42:47,229][233954] Min Reward on eval: -186.02493576221167[0m
[37m[1m[2023-07-11 02:42:47,229][233954] Mean Reward across all agents: 134.70328268219706[0m
[37m[1m[2023-07-11 02:42:47,230][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:42:47,247][233954] mean_value=69.67723471967464, max_value=1018.9842300191522[0m
[37m[1m[2023-07-11 02:42:47,250][233954] New mean coefficients: [[-0.5215081 -1.3567942  1.857508   3.4181857 -3.0269194 -2.392246 ]][0m
[37m[1m[2023-07-11 02:42:47,251][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:42:56,092][233954] train() took 8.84 seconds to complete[0m
[36m[2023-07-11 02:42:56,092][233954] FPS: 434422.99[0m
[36m[2023-07-11 02:42:56,094][233954] itr=211, itrs=2000, Progress: 10.55%[0m
[36m[2023-07-11 02:43:07,769][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 02:43:07,769][233954] FPS: 330170.34[0m
[36m[2023-07-11 02:43:12,006][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:43:12,007][233954] Reward + Measures: [[-312.98123246    0.43747231    0.83547801    0.03661034    0.33012936
     0.56100023]][0m
[37m[1m[2023-07-11 02:43:12,007][233954] Max Reward on eval: -312.9812324622686[0m
[37m[1m[2023-07-11 02:43:12,007][233954] Min Reward on eval: -312.9812324622686[0m
[37m[1m[2023-07-11 02:43:12,007][233954] Mean Reward across all agents: -312.9812324622686[0m
[37m[1m[2023-07-11 02:43:12,008][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:43:17,190][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:43:17,190][233954] Reward + Measures: [[207.45156717   0.31440002   0.65829998   0.20560001   0.58820003
    2.0539608 ]
 [ 84.05444878   0.39680001   0.52679998   0.35750005   0.44969997
    2.26052523]
 [ 79.57571126   0.50590008   0.80550003   0.1135       0.74650002
    1.53737593]
 ...
 [422.72131535   0.31000003   0.31819999   0.40330002   0.1365
    1.38958228]
 [198.39802116   0.56669998   0.69859999   0.2746       0.61620003
    1.42633235]
 [121.70564606   0.50229996   0.73600006   0.0301       0.479
    1.32624602]][0m
[37m[1m[2023-07-11 02:43:17,190][233954] Max Reward on eval: 422.7213153488934[0m
[37m[1m[2023-07-11 02:43:17,191][233954] Min Reward on eval: -157.40632867105305[0m
[37m[1m[2023-07-11 02:43:17,191][233954] Mean Reward across all agents: 88.73980288637819[0m
[37m[1m[2023-07-11 02:43:17,191][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:43:17,212][233954] mean_value=202.80939835420807, max_value=810.4770021018572[0m
[37m[1m[2023-07-11 02:43:17,215][233954] New mean coefficients: [[-0.47111776 -1.2882918   2.3064382   3.2105818  -2.350053   -2.396376  ]][0m
[37m[1m[2023-07-11 02:43:17,215][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:43:26,331][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 02:43:26,331][233954] FPS: 421346.67[0m
[36m[2023-07-11 02:43:26,333][233954] itr=212, itrs=2000, Progress: 10.60%[0m
[36m[2023-07-11 02:43:37,981][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 02:43:37,981][233954] FPS: 331098.45[0m
[36m[2023-07-11 02:43:42,370][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:43:42,371][233954] Reward + Measures: [[-347.0758396     0.43885329    0.85745037    0.037396      0.31314534
     0.49354517]][0m
[37m[1m[2023-07-11 02:43:42,371][233954] Max Reward on eval: -347.07583959813326[0m
[37m[1m[2023-07-11 02:43:42,371][233954] Min Reward on eval: -347.07583959813326[0m
[37m[1m[2023-07-11 02:43:42,371][233954] Mean Reward across all agents: -347.07583959813326[0m
[37m[1m[2023-07-11 02:43:42,372][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:43:47,393][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:43:47,399][233954] Reward + Measures: [[ 97.08406129   0.27110001   0.47639999   0.48890001   0.35859999
    2.32250071]
 [-83.1694456    0.435        0.48919997   0.32329997   0.37840003
    0.82706833]
 [236.24259136   0.2142       0.42470002   0.42320004   0.31909999
    2.31768966]
 ...
 [153.02779983   0.40550002   0.70270008   0.38860002   0.53520006
    1.69504201]
 [ 90.10041998   0.39809999   0.7202       0.28870001   0.59100002
    1.68867326]
 [ 57.85834387   0.2375       0.53929996   0.43990001   0.4499
    2.1665659 ]][0m
[37m[1m[2023-07-11 02:43:47,399][233954] Max Reward on eval: 555.153789496608[0m
[37m[1m[2023-07-11 02:43:47,400][233954] Min Reward on eval: -303.2419815550558[0m
[37m[1m[2023-07-11 02:43:47,400][233954] Mean Reward across all agents: 100.33196057816[0m
[37m[1m[2023-07-11 02:43:47,400][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:43:47,416][233954] mean_value=80.43696770019568, max_value=756.9559418746736[0m
[37m[1m[2023-07-11 02:43:47,419][233954] New mean coefficients: [[-0.52858824 -1.614512    2.7964907   2.1830895  -1.5426536  -2.15454   ]][0m
[37m[1m[2023-07-11 02:43:47,419][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:43:56,452][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 02:43:56,452][233954] FPS: 425211.67[0m
[36m[2023-07-11 02:43:56,455][233954] itr=213, itrs=2000, Progress: 10.65%[0m
[36m[2023-07-11 02:44:08,091][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 02:44:08,091][233954] FPS: 331410.82[0m
[36m[2023-07-11 02:44:12,313][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:44:12,313][233954] Reward + Measures: [[-407.1912378     0.42223868    0.87380695    0.03547433    0.30410033
     0.47556153]][0m
[37m[1m[2023-07-11 02:44:12,313][233954] Max Reward on eval: -407.1912378021855[0m
[37m[1m[2023-07-11 02:44:12,314][233954] Min Reward on eval: -407.1912378021855[0m
[37m[1m[2023-07-11 02:44:12,314][233954] Mean Reward across all agents: -407.1912378021855[0m
[37m[1m[2023-07-11 02:44:12,314][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:44:17,286][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:44:17,287][233954] Reward + Measures: [[422.01429651   0.25310001   0.20109999   0.26729998   0.14839999
    1.2984761 ]
 [-26.30284375   0.51920003   0.62849998   0.79940003   0.37979999
    2.08267784]
 [195.29690255   0.6904       0.44580004   0.73100001   0.13340001
    1.36907971]
 ...
 [459.43646907   0.39879999   0.20910001   0.4258       0.23959999
    1.45026088]
 [ 32.55817993   0.51190007   0.47870001   0.4842       0.42589998
    0.74399537]
 [338.32049562   0.40279999   0.1851       0.39109999   0.1974
    1.48252165]][0m
[37m[1m[2023-07-11 02:44:17,287][233954] Max Reward on eval: 677.9036178739741[0m
[37m[1m[2023-07-11 02:44:17,287][233954] Min Reward on eval: -366.8168944954872[0m
[37m[1m[2023-07-11 02:44:17,287][233954] Mean Reward across all agents: 187.58503453448546[0m
[37m[1m[2023-07-11 02:44:17,288][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:44:17,304][233954] mean_value=-267.53168161426544, max_value=871.3830480720848[0m
[37m[1m[2023-07-11 02:44:17,306][233954] New mean coefficients: [[-0.7168795 -1.7268473  2.154283   2.307513  -2.2715049 -1.976416 ]][0m
[37m[1m[2023-07-11 02:44:17,307][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:44:26,269][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 02:44:26,269][233954] FPS: 428572.26[0m
[36m[2023-07-11 02:44:26,271][233954] itr=214, itrs=2000, Progress: 10.70%[0m
[36m[2023-07-11 02:44:37,739][233954] train() took 11.42 seconds to complete[0m
[36m[2023-07-11 02:44:37,739][233954] FPS: 336215.28[0m
[36m[2023-07-11 02:44:41,925][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:44:41,926][233954] Reward + Measures: [[-396.9110287     0.3950437     0.88715494    0.03603133    0.29921398
     0.46894982]][0m
[37m[1m[2023-07-11 02:44:41,926][233954] Max Reward on eval: -396.91102870486736[0m
[37m[1m[2023-07-11 02:44:41,926][233954] Min Reward on eval: -396.91102870486736[0m
[37m[1m[2023-07-11 02:44:41,927][233954] Mean Reward across all agents: -396.91102870486736[0m
[37m[1m[2023-07-11 02:44:41,927][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:44:46,935][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:44:46,935][233954] Reward + Measures: [[ 37.14659821   0.27379999   0.2385       0.2823       0.18009999
    1.201087  ]
 [333.20343017   0.93899995   0.89089996   0.94139999   0.0704
    3.43284035]
 [-41.76807213   0.50200003   0.4474       0.29440001   0.40450001
    0.83403122]
 ...
 [ 98.40361381   0.23020001   0.34990001   0.2861       0.25510001
    1.66970098]
 [ 19.21690238   0.43799996   0.50980002   0.34920001   0.45609999
    1.13968289]
 [ 60.6576128    0.29440001   0.27110001   0.29390001   0.29290003
    1.72358918]][0m
[37m[1m[2023-07-11 02:44:46,935][233954] Max Reward on eval: 410.9399757243693[0m
[37m[1m[2023-07-11 02:44:46,936][233954] Min Reward on eval: -291.4690857604146[0m
[37m[1m[2023-07-11 02:44:46,936][233954] Mean Reward across all agents: 80.08583336002849[0m
[37m[1m[2023-07-11 02:44:46,936][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:44:46,950][233954] mean_value=-553.039064086644, max_value=746.7563981803135[0m
[37m[1m[2023-07-11 02:44:46,953][233954] New mean coefficients: [[-0.6459768 -1.4197547  1.769613   2.7566    -2.9234717 -1.9621669]][0m
[37m[1m[2023-07-11 02:44:46,954][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:44:55,984][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 02:44:55,984][233954] FPS: 425324.60[0m
[36m[2023-07-11 02:44:55,986][233954] itr=215, itrs=2000, Progress: 10.75%[0m
[36m[2023-07-11 02:45:07,689][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 02:45:07,689][233954] FPS: 329575.85[0m
[36m[2023-07-11 02:45:11,954][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:45:11,955][233954] Reward + Measures: [[-428.94285842    0.36541134    0.89406365    0.036035      0.28981265
     0.46826163]][0m
[37m[1m[2023-07-11 02:45:11,955][233954] Max Reward on eval: -428.9428584214887[0m
[37m[1m[2023-07-11 02:45:11,955][233954] Min Reward on eval: -428.9428584214887[0m
[37m[1m[2023-07-11 02:45:11,955][233954] Mean Reward across all agents: -428.9428584214887[0m
[37m[1m[2023-07-11 02:45:11,956][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:45:17,140][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:45:17,141][233954] Reward + Measures: [[311.55833578   0.34300002   0.2976       0.45300004   0.1248
    1.3525492 ]
 [133.59425399   0.4271       0.29669997   0.49340007   0.19410001
    1.33967948]
 [213.90489769   0.45080003   0.23909998   0.45170003   0.24050002
    1.45551264]
 ...
 [ 56.90725184   0.83000004   0.90450001   0.87869996   0.82730007
    1.86672401]
 [ 19.79096756   0.34630004   0.22860001   0.40689999   0.19640002
    2.12102842]
 [ 85.06553603   0.77759999   0.8452       0.0518       0.89239997
    0.86562157]][0m
[37m[1m[2023-07-11 02:45:17,141][233954] Max Reward on eval: 397.49065589377426[0m
[37m[1m[2023-07-11 02:45:17,141][233954] Min Reward on eval: -195.7611622718163[0m
[37m[1m[2023-07-11 02:45:17,142][233954] Mean Reward across all agents: 107.89753408111974[0m
[37m[1m[2023-07-11 02:45:17,142][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:45:17,157][233954] mean_value=-355.0191014622378, max_value=736.3590602762997[0m
[37m[1m[2023-07-11 02:45:17,160][233954] New mean coefficients: [[-0.55511105 -0.7968915   2.2153711   2.9270098  -2.3555562  -1.6790096 ]][0m
[37m[1m[2023-07-11 02:45:17,161][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:45:26,271][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 02:45:26,271][233954] FPS: 421562.88[0m
[36m[2023-07-11 02:45:26,274][233954] itr=216, itrs=2000, Progress: 10.80%[0m
[36m[2023-07-11 02:45:37,933][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 02:45:37,933][233954] FPS: 330804.13[0m
[36m[2023-07-11 02:45:42,234][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:45:42,234][233954] Reward + Measures: [[-463.41410324    0.37366134    0.89550161    0.036253      0.289186
     0.42894465]][0m
[37m[1m[2023-07-11 02:45:42,235][233954] Max Reward on eval: -463.41410323668373[0m
[37m[1m[2023-07-11 02:45:42,235][233954] Min Reward on eval: -463.41410323668373[0m
[37m[1m[2023-07-11 02:45:42,235][233954] Mean Reward across all agents: -463.41410323668373[0m
[37m[1m[2023-07-11 02:45:42,235][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:45:47,168][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:45:47,168][233954] Reward + Measures: [[-71.93601396   0.0471       0.59700006   0.43569994   0.46399999
    1.14021051]
 [-25.01767842   0.23169999   0.204        0.26449999   0.12250002
    2.59839487]
 [ 94.28011029   0.4639       0.41779995   0.51780003   0.15530001
    1.69797862]
 ...
 [ 33.98607033   0.40830001   0.4849       0.55839998   0.16409999
    1.64835632]
 [ 99.29732183   0.12550001   0.21030001   0.15220001   0.2218
    2.54278827]
 [-61.92449132   0.18450001   0.20130001   0.24679999   0.19289999
    2.90778708]][0m
[37m[1m[2023-07-11 02:45:47,168][233954] Max Reward on eval: 432.9961031538434[0m
[37m[1m[2023-07-11 02:45:47,169][233954] Min Reward on eval: -174.8254279813729[0m
[37m[1m[2023-07-11 02:45:47,169][233954] Mean Reward across all agents: 36.802071522968596[0m
[37m[1m[2023-07-11 02:45:47,169][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:45:47,183][233954] mean_value=-78.6696192185698, max_value=629.5791167989373[0m
[37m[1m[2023-07-11 02:45:47,186][233954] New mean coefficients: [[-0.8072051  -0.08428806  2.5558183   1.6875062  -0.8489723  -1.7491956 ]][0m
[37m[1m[2023-07-11 02:45:47,187][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:45:56,204][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 02:45:56,204][233954] FPS: 425938.09[0m
[36m[2023-07-11 02:45:56,207][233954] itr=217, itrs=2000, Progress: 10.85%[0m
[36m[2023-07-11 02:46:07,871][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 02:46:07,871][233954] FPS: 330670.63[0m
[36m[2023-07-11 02:46:12,084][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:46:12,085][233954] Reward + Measures: [[-483.25512171    0.40154332    0.90614998    0.03688933    0.29493234
     0.37938517]][0m
[37m[1m[2023-07-11 02:46:12,085][233954] Max Reward on eval: -483.2551217121706[0m
[37m[1m[2023-07-11 02:46:12,085][233954] Min Reward on eval: -483.2551217121706[0m
[37m[1m[2023-07-11 02:46:12,085][233954] Mean Reward across all agents: -483.2551217121706[0m
[37m[1m[2023-07-11 02:46:12,086][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:46:17,085][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:46:17,085][233954] Reward + Measures: [[ 35.50645057   0.3448       0.71620005   0.25050002   0.58840001
    1.93075407]
 [-30.26471257   0.1001       0.45660001   0.35769996   0.47
    2.48141026]
 [239.04601666   0.80370009   0.83840001   0.0807       0.80480003
    2.09356737]
 ...
 [  8.13393595   0.11960001   0.24249999   0.15890001   0.20780002
    1.79320681]
 [ 50.54643028   0.66869998   0.66079998   0.25960001   0.45619997
    1.70186043]
 [ 90.6683965    0.80059999   0.83780003   0.81820005   0.83459997
    1.71074903]][0m
[37m[1m[2023-07-11 02:46:17,085][233954] Max Reward on eval: 394.8498659268022[0m
[37m[1m[2023-07-11 02:46:17,086][233954] Min Reward on eval: -282.2388897046447[0m
[37m[1m[2023-07-11 02:46:17,086][233954] Mean Reward across all agents: 69.07172252480814[0m
[37m[1m[2023-07-11 02:46:17,086][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:46:17,098][233954] mean_value=-357.0629443890871, max_value=667.1448025610484[0m
[37m[1m[2023-07-11 02:46:17,101][233954] New mean coefficients: [[-0.7556895   0.17253888  3.2468753   0.918593    0.2703228  -1.5006512 ]][0m
[37m[1m[2023-07-11 02:46:17,102][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:46:26,118][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 02:46:26,119][233954] FPS: 425953.10[0m
[36m[2023-07-11 02:46:26,121][233954] itr=218, itrs=2000, Progress: 10.90%[0m
[36m[2023-07-11 02:46:37,683][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 02:46:37,683][233954] FPS: 333485.69[0m
[36m[2023-07-11 02:46:42,003][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:46:42,004][233954] Reward + Measures: [[-512.80173315    0.43116865    0.91390634    0.038888      0.29473767
     0.35804015]][0m
[37m[1m[2023-07-11 02:46:42,004][233954] Max Reward on eval: -512.8017331472904[0m
[37m[1m[2023-07-11 02:46:42,004][233954] Min Reward on eval: -512.8017331472904[0m
[37m[1m[2023-07-11 02:46:42,005][233954] Mean Reward across all agents: -512.8017331472904[0m
[37m[1m[2023-07-11 02:46:42,005][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:46:47,000][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:46:47,001][233954] Reward + Measures: [[-11.84107261   0.44440004   0.55439997   0.0959       0.48009998
    1.48595524]
 [256.98367024   0.52100009   0.21860002   0.44239998   0.54740006
    2.10719347]
 [126.52535822   0.3141       0.64340001   0.19380002   0.57349998
    0.8844409 ]
 ...
 [131.45622246   0.29000002   0.54750007   0.2357       0.4341
    1.16901004]
 [181.99296476   0.90360004   0.0692       0.815        0.83619994
    1.41153395]
 [-72.32743866   0.3249       0.56380004   0.1252       0.51830006
    1.14766657]][0m
[37m[1m[2023-07-11 02:46:47,001][233954] Max Reward on eval: 513.7196903780452[0m
[37m[1m[2023-07-11 02:46:47,001][233954] Min Reward on eval: -250.69132795429323[0m
[37m[1m[2023-07-11 02:46:47,001][233954] Mean Reward across all agents: 111.81762927264691[0m
[37m[1m[2023-07-11 02:46:47,002][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:46:47,018][233954] mean_value=-18.899234668891733, max_value=950.4347129794711[0m
[37m[1m[2023-07-11 02:46:47,020][233954] New mean coefficients: [[-0.9628296   0.45382392  2.6777577   2.9893858  -0.23106146 -1.6642382 ]][0m
[37m[1m[2023-07-11 02:46:47,021][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:46:56,037][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 02:46:56,037][233954] FPS: 426002.50[0m
[36m[2023-07-11 02:46:56,040][233954] itr=219, itrs=2000, Progress: 10.95%[0m
[36m[2023-07-11 02:47:07,813][233954] train() took 11.73 seconds to complete[0m
[36m[2023-07-11 02:47:07,814][233954] FPS: 327481.91[0m
[36m[2023-07-11 02:47:12,165][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:47:12,165][233954] Reward + Measures: [[-530.07045716    0.45846468    0.91346437    0.047813      0.29689631
     0.32857156]][0m
[37m[1m[2023-07-11 02:47:12,166][233954] Max Reward on eval: -530.0704571624311[0m
[37m[1m[2023-07-11 02:47:12,166][233954] Min Reward on eval: -530.0704571624311[0m
[37m[1m[2023-07-11 02:47:12,166][233954] Mean Reward across all agents: -530.0704571624311[0m
[37m[1m[2023-07-11 02:47:12,166][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:47:17,222][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:47:17,223][233954] Reward + Measures: [[ 56.20830368   0.3427       0.2809       0.40770003   0.2527
    1.71787107]
 [ 69.11214036   0.39069998   0.60210001   0.43289995   0.44390002
    1.22309554]
 [602.56071568   0.21659999   0.28889999   0.2412       0.1141
    1.21465337]
 ...
 [ 53.93290577   0.61650008   0.61580002   0.61619997   0.22379999
    1.35127044]
 [ 38.58344066   0.89340001   0.8215       0.90259999   0.0071
    2.352144  ]
 [136.87437247   0.19590001   0.48270008   0.44250003   0.50590003
    0.79199928]][0m
[37m[1m[2023-07-11 02:47:17,223][233954] Max Reward on eval: 602.5607156775892[0m
[37m[1m[2023-07-11 02:47:17,223][233954] Min Reward on eval: -466.1324684964493[0m
[37m[1m[2023-07-11 02:47:17,224][233954] Mean Reward across all agents: 84.77377623836752[0m
[37m[1m[2023-07-11 02:47:17,224][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:47:17,243][233954] mean_value=-39.130879982558014, max_value=850.8406696611783[0m
[37m[1m[2023-07-11 02:47:17,246][233954] New mean coefficients: [[-1.3109994  0.348336   2.1549556  2.7497516 -0.8227268 -1.7768484]][0m
[37m[1m[2023-07-11 02:47:17,247][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:47:26,295][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 02:47:26,295][233954] FPS: 424481.56[0m
[36m[2023-07-11 02:47:26,297][233954] itr=220, itrs=2000, Progress: 11.00%[0m
[37m[1m[2023-07-11 02:50:04,776][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000200[0m
[36m[2023-07-11 02:50:17,340][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 02:50:17,340][233954] FPS: 329654.88[0m
[36m[2023-07-11 02:50:21,584][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:50:21,585][233954] Reward + Measures: [[-588.63327798    0.46985602    0.92417938    0.04926733    0.29765633
     0.33507699]][0m
[37m[1m[2023-07-11 02:50:21,585][233954] Max Reward on eval: -588.6332779753969[0m
[37m[1m[2023-07-11 02:50:21,585][233954] Min Reward on eval: -588.6332779753969[0m
[37m[1m[2023-07-11 02:50:21,585][233954] Mean Reward across all agents: -588.6332779753969[0m
[37m[1m[2023-07-11 02:50:21,586][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:50:26,402][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:50:26,403][233954] Reward + Measures: [[214.65088604   0.4427       0.25759998   0.44119999   0.30159998
    2.39496112]
 [-53.85498814   0.245        0.77569997   0.32030001   0.49589998
    0.900787  ]
 [346.04623124   0.31740004   0.35129997   0.47460005   0.30920002
    2.14047194]
 ...
 [128.14662459   0.19500001   0.45770001   0.25760001   0.28029999
    1.54340446]
 [267.69775392   0.28560001   0.39050001   0.37900001   0.29840001
    1.81138074]
 [-37.72333271   0.324        0.50610006   0.19140001   0.41840002
    1.71304727]][0m
[37m[1m[2023-07-11 02:50:26,403][233954] Max Reward on eval: 368.6607303712517[0m
[37m[1m[2023-07-11 02:50:26,404][233954] Min Reward on eval: -410.0485439233249[0m
[37m[1m[2023-07-11 02:50:26,404][233954] Mean Reward across all agents: 50.74074648976824[0m
[37m[1m[2023-07-11 02:50:26,404][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:50:26,419][233954] mean_value=-21.66900446034357, max_value=868.6607303712517[0m
[37m[1m[2023-07-11 02:50:26,422][233954] New mean coefficients: [[-0.91578305  0.3283171   2.1932716   3.5137382  -0.7482971  -1.6371963 ]][0m
[37m[1m[2023-07-11 02:50:26,423][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:50:35,158][233954] train() took 8.73 seconds to complete[0m
[36m[2023-07-11 02:50:35,158][233954] FPS: 439693.34[0m
[36m[2023-07-11 02:50:35,160][233954] itr=221, itrs=2000, Progress: 11.05%[0m
[36m[2023-07-11 02:50:46,845][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 02:50:46,845][233954] FPS: 330083.92[0m
[36m[2023-07-11 02:50:51,080][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:50:51,080][233954] Reward + Measures: [[-654.65969397    0.49376398    0.91534269    0.064021      0.29392067
     0.31687313]][0m
[37m[1m[2023-07-11 02:50:51,080][233954] Max Reward on eval: -654.6596939667832[0m
[37m[1m[2023-07-11 02:50:51,081][233954] Min Reward on eval: -654.6596939667832[0m
[37m[1m[2023-07-11 02:50:51,081][233954] Mean Reward across all agents: -654.6596939667832[0m
[37m[1m[2023-07-11 02:50:51,081][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:50:56,017][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:50:56,018][233954] Reward + Measures: [[ 6.88222855  0.25690001  0.18359999  0.27879998  0.16700001  1.92409861]
 [10.32721403  0.52380002  0.24119997  0.43450004  0.2886      2.45904636]
 [26.4023093   0.3811      0.24090002  0.38890001  0.25120002  1.39375353]
 ...
 [-0.90339103  0.37310001  0.31119999  0.28730002  0.31309995  1.83095288]
 [13.19134152  0.45170003  0.28169999  0.34210002  0.33229998  2.3047843 ]
 [16.11861156  0.48109999  0.25620002  0.35419998  0.2198      1.83219361]][0m
[37m[1m[2023-07-11 02:50:56,018][233954] Max Reward on eval: 364.62017875928433[0m
[37m[1m[2023-07-11 02:50:56,018][233954] Min Reward on eval: -438.2986069286242[0m
[37m[1m[2023-07-11 02:50:56,018][233954] Mean Reward across all agents: 5.8963279571054255[0m
[37m[1m[2023-07-11 02:50:56,018][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:50:56,031][233954] mean_value=-349.731253726226, max_value=564.5396763561666[0m
[37m[1m[2023-07-11 02:50:56,034][233954] New mean coefficients: [[-0.4513962  -0.06316838  2.6707153   3.9868574  -1.3725274  -1.7616634 ]][0m
[37m[1m[2023-07-11 02:50:56,034][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:51:05,080][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 02:51:05,080][233954] FPS: 424605.59[0m
[36m[2023-07-11 02:51:05,082][233954] itr=222, itrs=2000, Progress: 11.10%[0m
[36m[2023-07-11 02:51:17,143][233954] train() took 12.01 seconds to complete[0m
[36m[2023-07-11 02:51:17,144][233954] FPS: 319652.02[0m
[36m[2023-07-11 02:51:21,423][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:51:21,424][233954] Reward + Measures: [[-716.50393687    0.46667898    0.91370696    0.078835      0.292914
     0.3285087 ]][0m
[37m[1m[2023-07-11 02:51:21,424][233954] Max Reward on eval: -716.5039368662351[0m
[37m[1m[2023-07-11 02:51:21,424][233954] Min Reward on eval: -716.5039368662351[0m
[37m[1m[2023-07-11 02:51:21,425][233954] Mean Reward across all agents: -716.5039368662351[0m
[37m[1m[2023-07-11 02:51:21,425][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:51:26,363][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:51:26,364][233954] Reward + Measures: [[-225.66212713    0.25740001    0.55389994    0.31          0.25320002
     0.9961583 ]
 [ 534.54348134    0.0934        0.49600002    0.40949997    0.37060001
     2.30076957]
 [ 356.15983081    0.0211        0.84100002    0.73520011    0.76660007
     2.96359849]
 ...
 [ 152.57389868    0.0796        0.52679998    0.34639999    0.44280002
     2.21844482]
 [ -86.84680161    0.19660001    0.38030002    0.24300002    0.2309
     1.2836889 ]
 [-120.86712665    0.36549997    0.66569996    0.33699998    0.3107
     0.97010154]][0m
[37m[1m[2023-07-11 02:51:26,364][233954] Max Reward on eval: 981.1287670525256[0m
[37m[1m[2023-07-11 02:51:26,364][233954] Min Reward on eval: -313.15603062505835[0m
[37m[1m[2023-07-11 02:51:26,364][233954] Mean Reward across all agents: 80.2823669410579[0m
[37m[1m[2023-07-11 02:51:26,365][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:51:26,376][233954] mean_value=-242.5452098696483, max_value=721.6759929349832[0m
[37m[1m[2023-07-11 02:51:26,378][233954] New mean coefficients: [[-0.353373    0.14311746  3.201583    3.0636997  -0.60167664 -2.1407838 ]][0m
[37m[1m[2023-07-11 02:51:26,379][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:51:35,423][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 02:51:35,423][233954] FPS: 424705.44[0m
[36m[2023-07-11 02:51:35,425][233954] itr=223, itrs=2000, Progress: 11.15%[0m
[36m[2023-07-11 02:51:47,064][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 02:51:47,064][233954] FPS: 331304.29[0m
[36m[2023-07-11 02:51:51,329][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:51:51,329][233954] Reward + Measures: [[-697.82997969    0.49116069    0.91767365    0.08335467    0.29578403
     0.31036353]][0m
[37m[1m[2023-07-11 02:51:51,330][233954] Max Reward on eval: -697.8299796937162[0m
[37m[1m[2023-07-11 02:51:51,330][233954] Min Reward on eval: -697.8299796937162[0m
[37m[1m[2023-07-11 02:51:51,330][233954] Mean Reward across all agents: -697.8299796937162[0m
[37m[1m[2023-07-11 02:51:51,330][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:51:56,487][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:51:56,488][233954] Reward + Measures: [[  53.90975729    0.19499999    0.76200002    0.28010002    0.68260002
     1.54320455]
 [  44.94003932    0.55509996    0.63620007    0.43689999    0.38589999
     1.52856886]
 [ 150.63390155    0.42460003    0.0829        0.53850001    0.3145
     1.66278708]
 ...
 [  39.40939878    0.46960002    0.6103        0.48049998    0.29579997
     0.98494196]
 [ 135.44633147    0.24890001    0.50480002    0.1453        0.37289998
     1.20567501]
 [-149.5259696     0.42469999    0.61379999    0.06130001    0.41960001
     0.8106249 ]][0m
[37m[1m[2023-07-11 02:51:56,488][233954] Max Reward on eval: 696.531021115277[0m
[37m[1m[2023-07-11 02:51:56,488][233954] Min Reward on eval: -548.8070412128233[0m
[37m[1m[2023-07-11 02:51:56,488][233954] Mean Reward across all agents: 76.51514687896706[0m
[37m[1m[2023-07-11 02:51:56,489][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:51:56,500][233954] mean_value=-647.5530300732124, max_value=1003.5593910172581[0m
[37m[1m[2023-07-11 02:51:56,502][233954] New mean coefficients: [[ 0.03319424  0.5105384   3.3856108   2.4186     -0.18734655 -2.0885887 ]][0m
[37m[1m[2023-07-11 02:51:56,503][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:52:05,485][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 02:52:05,485][233954] FPS: 427610.45[0m
[36m[2023-07-11 02:52:05,487][233954] itr=224, itrs=2000, Progress: 11.20%[0m
[36m[2023-07-11 02:52:17,205][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 02:52:17,205][233954] FPS: 329072.14[0m
[36m[2023-07-11 02:52:21,488][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:52:21,489][233954] Reward + Measures: [[-624.8645884     0.50194067    0.9401896     0.072005      0.30863765
     0.27493   ]][0m
[37m[1m[2023-07-11 02:52:21,489][233954] Max Reward on eval: -624.8645884011545[0m
[37m[1m[2023-07-11 02:52:21,489][233954] Min Reward on eval: -624.8645884011545[0m
[37m[1m[2023-07-11 02:52:21,489][233954] Mean Reward across all agents: -624.8645884011545[0m
[37m[1m[2023-07-11 02:52:21,490][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:52:26,455][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:52:26,456][233954] Reward + Measures: [[ -53.77653619    0.46420002    0.80120003    0.3757        0.44099998
     0.92080611]
 [-196.27436032    0.1631        0.56199998    0.22289999    0.34770003
     1.12390089]
 [ -36.7440474     0.23650001    0.73299998    0.23640001    0.433
     0.68162346]
 ...
 [ 110.25082006    0.52270001    0.58179998    0.55059999    0.0869
     2.3790195 ]
 [  34.15383239    0.34170005    0.53039998    0.35900003    0.23669998
     1.52499259]
 [ -57.63816462    0.66730005    0.71390003    0.62219995    0.2343
     1.24845755]][0m
[37m[1m[2023-07-11 02:52:26,456][233954] Max Reward on eval: 506.63353633657096[0m
[37m[1m[2023-07-11 02:52:26,456][233954] Min Reward on eval: -459.5208854207769[0m
[37m[1m[2023-07-11 02:52:26,456][233954] Mean Reward across all agents: 28.51817016330231[0m
[37m[1m[2023-07-11 02:52:26,457][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:52:26,476][233954] mean_value=17.951505872399938, max_value=821.1537779492828[0m
[37m[1m[2023-07-11 02:52:26,479][233954] New mean coefficients: [[ 0.14668041  0.4241122   3.3921533   3.2695055  -0.3045463  -1.4376531 ]][0m
[37m[1m[2023-07-11 02:52:26,480][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:52:35,452][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 02:52:35,452][233954] FPS: 428069.60[0m
[36m[2023-07-11 02:52:35,454][233954] itr=225, itrs=2000, Progress: 11.25%[0m
[36m[2023-07-11 02:52:47,027][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 02:52:47,027][233954] FPS: 333228.31[0m
[36m[2023-07-11 02:52:51,314][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:52:51,314][233954] Reward + Measures: [[-613.26063906    0.49979833    0.94386357    0.09050467    0.32133466
     0.26481351]][0m
[37m[1m[2023-07-11 02:52:51,315][233954] Max Reward on eval: -613.2606390641437[0m
[37m[1m[2023-07-11 02:52:51,315][233954] Min Reward on eval: -613.2606390641437[0m
[37m[1m[2023-07-11 02:52:51,315][233954] Mean Reward across all agents: -613.2606390641437[0m
[37m[1m[2023-07-11 02:52:51,315][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:52:56,221][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:52:56,222][233954] Reward + Measures: [[151.16316208   0.41629997   0.46360001   0.37920001   0.25509998
    1.33685827]
 [ 40.43882016   0.38529998   0.48230004   0.43179998   0.39159998
    0.72238863]
 [ -5.756721     0.2105       0.4296       0.156        0.32550001
    2.30083203]
 ...
 [224.73855009   0.4104       0.574        0.3317       0.3928
    1.43449497]
 [ 81.41300505   0.2088       0.68259996   0.27400002   0.43730003
    1.47704244]
 [ 86.71111112   0.1247       0.1578       0.13140002   0.16140001
    2.63403726]][0m
[37m[1m[2023-07-11 02:52:56,222][233954] Max Reward on eval: 594.1021194849163[0m
[37m[1m[2023-07-11 02:52:56,222][233954] Min Reward on eval: -350.09018133853095[0m
[37m[1m[2023-07-11 02:52:56,222][233954] Mean Reward across all agents: 84.6057905729206[0m
[37m[1m[2023-07-11 02:52:56,223][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:52:56,230][233954] mean_value=-625.9023869799256, max_value=826.5069833026998[0m
[37m[1m[2023-07-11 02:52:56,232][233954] New mean coefficients: [[-0.05763124  0.09294963  3.1808345   1.7851697  -0.46080115 -1.5680522 ]][0m
[37m[1m[2023-07-11 02:52:56,233][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:53:05,142][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 02:53:05,142][233954] FPS: 431093.40[0m
[36m[2023-07-11 02:53:05,145][233954] itr=226, itrs=2000, Progress: 11.30%[0m
[36m[2023-07-11 02:53:16,629][233954] train() took 11.43 seconds to complete[0m
[36m[2023-07-11 02:53:16,629][233954] FPS: 335961.45[0m
[36m[2023-07-11 02:53:20,902][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:53:20,902][233954] Reward + Measures: [[-588.69773353    0.47370434    0.94882965    0.08812166    0.32165068
     0.25516096]][0m
[37m[1m[2023-07-11 02:53:20,902][233954] Max Reward on eval: -588.6977335318693[0m
[37m[1m[2023-07-11 02:53:20,903][233954] Min Reward on eval: -588.6977335318693[0m
[37m[1m[2023-07-11 02:53:20,903][233954] Mean Reward across all agents: -588.6977335318693[0m
[37m[1m[2023-07-11 02:53:20,903][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:53:25,830][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:53:25,831][233954] Reward + Measures: [[-29.38949499   0.3511       0.62130004   0.46869999   0.42290002
    0.66938555]
 [128.91667771   0.44189999   0.62839997   0.1287       0.58630002
    1.27053535]
 [293.91817772   0.79109997   0.90560007   0.42209998   0.86780006
    1.72957158]
 ...
 [ -9.40495509   0.25900003   0.3326       0.36180001   0.2122
    1.14311302]
 [125.42686033   0.5399       0.31200001   0.57260001   0.24960001
    2.34981918]
 [-94.51610635   0.9485001    0.82230008   0.96049994   0.007
    1.97348809]][0m
[37m[1m[2023-07-11 02:53:25,831][233954] Max Reward on eval: 537.6601281234995[0m
[37m[1m[2023-07-11 02:53:25,832][233954] Min Reward on eval: -611.2049064319988[0m
[37m[1m[2023-07-11 02:53:25,832][233954] Mean Reward across all agents: 38.83816492596101[0m
[37m[1m[2023-07-11 02:53:25,832][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:53:25,847][233954] mean_value=-354.95942831755076, max_value=901.4840631651064[0m
[37m[1m[2023-07-11 02:53:25,850][233954] New mean coefficients: [[ 0.0740281  -0.36059213  3.113369    3.5074792  -1.2169904  -1.8307683 ]][0m
[37m[1m[2023-07-11 02:53:25,851][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:53:34,875][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 02:53:34,875][233954] FPS: 425628.43[0m
[36m[2023-07-11 02:53:34,877][233954] itr=227, itrs=2000, Progress: 11.35%[0m
[36m[2023-07-11 02:53:46,535][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 02:53:46,535][233954] FPS: 330823.88[0m
[36m[2023-07-11 02:53:50,748][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:53:50,749][233954] Reward + Measures: [[-605.74366255    0.47730866    0.95127356    0.10096934    0.31691897
     0.25492579]][0m
[37m[1m[2023-07-11 02:53:50,749][233954] Max Reward on eval: -605.7436625482454[0m
[37m[1m[2023-07-11 02:53:50,749][233954] Min Reward on eval: -605.7436625482454[0m
[37m[1m[2023-07-11 02:53:50,750][233954] Mean Reward across all agents: -605.7436625482454[0m
[37m[1m[2023-07-11 02:53:50,750][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:53:55,719][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:53:55,720][233954] Reward + Measures: [[ 49.06488224   0.25959998   0.66890001   0.15799999   0.54629999
    0.78442949]
 [ 75.87922284   0.2369       0.61739999   0.32669997   0.28200001
    0.78652769]
 [164.55232186   0.10030001   0.87649995   0.447        0.67259997
    0.93099737]
 ...
 [118.51939568   0.35120001   0.2895       0.4621       0.36100003
    1.6304276 ]
 [ 41.83424066   0.41170001   0.36989999   0.46159998   0.31060001
    1.71125948]
 [-31.3695564    0.41950002   0.50389999   0.50209999   0.36849999
    1.23311782]][0m
[37m[1m[2023-07-11 02:53:55,720][233954] Max Reward on eval: 319.30150982216[0m
[37m[1m[2023-07-11 02:53:55,720][233954] Min Reward on eval: -281.73487333068624[0m
[37m[1m[2023-07-11 02:53:55,721][233954] Mean Reward across all agents: 17.15142506174516[0m
[37m[1m[2023-07-11 02:53:55,721][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:53:55,736][233954] mean_value=-139.84135050388838, max_value=664.4222307323944[0m
[37m[1m[2023-07-11 02:53:55,738][233954] New mean coefficients: [[ 0.36354586  0.50451285  3.9942966   2.5624747   0.30612886 -1.6381348 ]][0m
[37m[1m[2023-07-11 02:53:55,739][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:54:04,850][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 02:54:04,851][233954] FPS: 421537.94[0m
[36m[2023-07-11 02:54:04,853][233954] itr=228, itrs=2000, Progress: 11.40%[0m
[36m[2023-07-11 02:54:16,407][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 02:54:16,407][233954] FPS: 333886.59[0m
[36m[2023-07-11 02:54:20,682][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:54:20,683][233954] Reward + Measures: [[-560.29841515    0.48160401    0.95033592    0.09685567    0.32746133
     0.25050524]][0m
[37m[1m[2023-07-11 02:54:20,683][233954] Max Reward on eval: -560.2984151495838[0m
[37m[1m[2023-07-11 02:54:20,683][233954] Min Reward on eval: -560.2984151495838[0m
[37m[1m[2023-07-11 02:54:20,683][233954] Mean Reward across all agents: -560.2984151495838[0m
[37m[1m[2023-07-11 02:54:20,684][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:54:25,895][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:54:25,896][233954] Reward + Measures: [[ 51.97097563   0.60159999   0.81900007   0.06800001   0.73970002
    0.4648512 ]
 [200.08864138   0.34850001   0.48499998   0.45170003   0.24000001
    0.97857153]
 [138.74579476   0.64740002   0.59200001   0.67739999   0.11920001
    1.31300187]
 ...
 [ 10.54697988   0.48499998   0.72350001   0.36390004   0.51949996
    0.58568752]
 [ 41.53359719   0.36040002   0.44849998   0.49939999   0.30370003
    1.06018817]
 [131.32818555   0.79640001   0.12149999   0.74960005   0.64490002
    1.12771297]][0m
[37m[1m[2023-07-11 02:54:25,896][233954] Max Reward on eval: 706.6035041898489[0m
[37m[1m[2023-07-11 02:54:25,896][233954] Min Reward on eval: -234.2178306910442[0m
[37m[1m[2023-07-11 02:54:25,897][233954] Mean Reward across all agents: 201.5240768614042[0m
[37m[1m[2023-07-11 02:54:25,897][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:54:25,913][233954] mean_value=-226.61614952065943, max_value=760.9111080182134[0m
[37m[1m[2023-07-11 02:54:25,916][233954] New mean coefficients: [[ 0.09837374  0.35906073  4.07482     1.9250387   0.8465401  -1.7081517 ]][0m
[37m[1m[2023-07-11 02:54:25,917][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:54:34,989][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 02:54:34,989][233954] FPS: 423355.78[0m
[36m[2023-07-11 02:54:34,991][233954] itr=229, itrs=2000, Progress: 11.45%[0m
[36m[2023-07-11 02:54:46,689][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 02:54:46,689][233954] FPS: 329820.22[0m
[36m[2023-07-11 02:54:50,917][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:54:50,917][233954] Reward + Measures: [[-503.43683645    0.46672899    0.9524793     0.08945066    0.34416702
     0.24858177]][0m
[37m[1m[2023-07-11 02:54:50,918][233954] Max Reward on eval: -503.4368364493571[0m
[37m[1m[2023-07-11 02:54:50,918][233954] Min Reward on eval: -503.4368364493571[0m
[37m[1m[2023-07-11 02:54:50,918][233954] Mean Reward across all agents: -503.4368364493571[0m
[37m[1m[2023-07-11 02:54:50,918][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:54:55,903][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:54:55,904][233954] Reward + Measures: [[ 62.07251375   0.53010005   0.52299994   0.50019997   0.50699997
    0.70521051]
 [  3.09351535   0.63549995   0.79450005   0.17059998   0.75650001
    0.87482995]
 [128.5845212    0.51780003   0.43350002   0.38079998   0.32030001
    1.48667669]
 ...
 [-38.22528521   0.7518       0.77399999   0.14049999   0.8441
    0.83504409]
 [ 15.00407074   0.51090002   0.63319999   0.43880001   0.69089997
    1.83198357]
 [ 65.19318018   0.40109998   0.55739999   0.40930006   0.35630003
    0.65030259]][0m
[37m[1m[2023-07-11 02:54:55,904][233954] Max Reward on eval: 739.13203051202[0m
[37m[1m[2023-07-11 02:54:55,904][233954] Min Reward on eval: -399.4948911047366[0m
[37m[1m[2023-07-11 02:54:55,904][233954] Mean Reward across all agents: 152.224937380983[0m
[37m[1m[2023-07-11 02:54:55,904][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:54:55,920][233954] mean_value=-565.2991662477176, max_value=898.9500496237504[0m
[37m[1m[2023-07-11 02:54:55,923][233954] New mean coefficients: [[ 0.54632014  0.24624607  4.3021774   2.5510168   0.2605127  -1.8941602 ]][0m
[37m[1m[2023-07-11 02:54:55,924][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:55:04,912][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 02:55:04,912][233954] FPS: 427307.42[0m
[36m[2023-07-11 02:55:04,914][233954] itr=230, itrs=2000, Progress: 11.50%[0m
[37m[1m[2023-07-11 02:57:47,595][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000210[0m
[36m[2023-07-11 02:58:00,341][233954] train() took 11.86 seconds to complete[0m
[36m[2023-07-11 02:58:00,341][233954] FPS: 323850.62[0m
[36m[2023-07-11 02:58:04,615][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:58:04,616][233954] Reward + Measures: [[-384.9849569     0.47283033    0.9532243     0.09267067    0.377974
     0.2380096 ]][0m
[37m[1m[2023-07-11 02:58:04,616][233954] Max Reward on eval: -384.984956900713[0m
[37m[1m[2023-07-11 02:58:04,616][233954] Min Reward on eval: -384.984956900713[0m
[37m[1m[2023-07-11 02:58:04,617][233954] Mean Reward across all agents: -384.984956900713[0m
[37m[1m[2023-07-11 02:58:04,617][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:58:09,862][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:58:09,863][233954] Reward + Measures: [[  49.80023725    0.42180005    0.5729        0.12390001    0.51959997
     0.8880381 ]
 [ 118.93674606    0.19060002    0.69460005    0.34150001    0.59079999
     0.92073786]
 [-166.64293955    0.33390003    0.73240006    0.54549998    0.40490004
     1.00061047]
 ...
 [  14.11708114    0.197         0.48699999    0.28050002    0.38019997
     1.00336123]
 [ -19.45719915    0.18719999    0.62150002    0.3524        0.51840001
     0.92207164]
 [ 116.6847961     0.1672        0.77869999    0.36870003    0.7245
     1.46366322]][0m
[37m[1m[2023-07-11 02:58:09,863][233954] Max Reward on eval: 261.0171661786735[0m
[37m[1m[2023-07-11 02:58:09,864][233954] Min Reward on eval: -381.93948354055175[0m
[37m[1m[2023-07-11 02:58:09,864][233954] Mean Reward across all agents: 23.479586427600964[0m
[37m[1m[2023-07-11 02:58:09,864][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:58:09,884][233954] mean_value=65.95129959751755, max_value=755.0094414252787[0m
[37m[1m[2023-07-11 02:58:09,886][233954] New mean coefficients: [[ 0.63297576  0.24796626  5.0647535   2.1406026   0.28344324 -2.3409307 ]][0m
[37m[1m[2023-07-11 02:58:09,887][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:58:18,745][233954] train() took 8.86 seconds to complete[0m
[36m[2023-07-11 02:58:18,745][233954] FPS: 433596.12[0m
[36m[2023-07-11 02:58:18,747][233954] itr=231, itrs=2000, Progress: 11.55%[0m
[36m[2023-07-11 02:58:30,261][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 02:58:30,262][233954] FPS: 335029.49[0m
[36m[2023-07-11 02:58:34,599][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:58:34,599][233954] Reward + Measures: [[-358.59756224    0.50040436    0.95086068    0.10042433    0.39053398
     0.22934377]][0m
[37m[1m[2023-07-11 02:58:34,600][233954] Max Reward on eval: -358.5975622378687[0m
[37m[1m[2023-07-11 02:58:34,600][233954] Min Reward on eval: -358.5975622378687[0m
[37m[1m[2023-07-11 02:58:34,600][233954] Mean Reward across all agents: -358.5975622378687[0m
[37m[1m[2023-07-11 02:58:34,600][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:58:39,583][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:58:39,584][233954] Reward + Measures: [[ 69.38512442   0.16680001   0.55159998   0.48470002   0.52360004
    1.64502227]
 [183.39323319   0.61269999   0.29860002   0.46310002   0.15049998
    1.16228998]
 [ 96.59378267   0.15019999   0.67130005   0.42349997   0.54659998
    1.41389012]
 ...
 [192.38323305   0.2076       0.56070006   0.40700004   0.29260001
    0.58924812]
 [110.59363606   0.6706       0.0893       0.73710001   0.66350001
    1.34485734]
 [ 31.98285462   0.67810005   0.1531       0.68849999   0.37499997
    1.32433951]][0m
[37m[1m[2023-07-11 02:58:39,584][233954] Max Reward on eval: 481.52556799519806[0m
[37m[1m[2023-07-11 02:58:39,584][233954] Min Reward on eval: -214.0866226171609[0m
[37m[1m[2023-07-11 02:58:39,585][233954] Mean Reward across all agents: 62.03243186956233[0m
[37m[1m[2023-07-11 02:58:39,585][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:58:39,602][233954] mean_value=60.81003902065507, max_value=854.0147824464179[0m
[37m[1m[2023-07-11 02:58:39,604][233954] New mean coefficients: [[ 0.3624367  0.0414855  4.801458   1.6298404  0.357612  -2.928293 ]][0m
[37m[1m[2023-07-11 02:58:39,605][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:58:48,529][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 02:58:48,530][233954] FPS: 430378.05[0m
[36m[2023-07-11 02:58:48,532][233954] itr=232, itrs=2000, Progress: 11.60%[0m
[36m[2023-07-11 02:59:00,189][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 02:59:00,190][233954] FPS: 330863.42[0m
[36m[2023-07-11 02:59:04,506][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:59:04,507][233954] Reward + Measures: [[-325.81140557    0.53751433    0.95383698    0.09288733    0.39680898
     0.20924321]][0m
[37m[1m[2023-07-11 02:59:04,507][233954] Max Reward on eval: -325.8114055730768[0m
[37m[1m[2023-07-11 02:59:04,507][233954] Min Reward on eval: -325.8114055730768[0m
[37m[1m[2023-07-11 02:59:04,507][233954] Mean Reward across all agents: -325.8114055730768[0m
[37m[1m[2023-07-11 02:59:04,508][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:59:09,452][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:59:09,453][233954] Reward + Measures: [[158.9239178    0.60030001   0.91839999   0.68090004   0.88990003
    1.88522661]
 [145.43551163   0.58380002   0.44330001   0.57170004   0.373
    0.79790729]
 [-15.6084366    0.55899996   0.65130001   0.40769997   0.59490007
    0.46932441]
 ...
 [ 73.20275452   0.76489997   0.87150002   0.79580003   0.86750013
    2.54061866]
 [ 61.73507639   0.76630002   0.8707       0.80199999   0.84549999
    1.63757765]
 [-55.44304848   0.22260001   0.71719998   0.50990003   0.65630001
    1.34741843]][0m
[37m[1m[2023-07-11 02:59:09,453][233954] Max Reward on eval: 559.6262559987605[0m
[37m[1m[2023-07-11 02:59:09,453][233954] Min Reward on eval: -340.1510353401769[0m
[37m[1m[2023-07-11 02:59:09,453][233954] Mean Reward across all agents: 60.49348052213868[0m
[37m[1m[2023-07-11 02:59:09,454][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:59:09,471][233954] mean_value=100.05993033365036, max_value=673.6423969254829[0m
[37m[1m[2023-07-11 02:59:09,473][233954] New mean coefficients: [[ 0.67137444  0.40822363  5.3328056   2.6691651   0.7098814  -2.8438873 ]][0m
[37m[1m[2023-07-11 02:59:09,474][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:59:18,410][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 02:59:18,410][233954] FPS: 429822.76[0m
[36m[2023-07-11 02:59:18,412][233954] itr=233, itrs=2000, Progress: 11.65%[0m
[36m[2023-07-11 02:59:30,380][233954] train() took 11.92 seconds to complete[0m
[36m[2023-07-11 02:59:30,380][233954] FPS: 322274.31[0m
[36m[2023-07-11 02:59:34,599][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:59:34,600][233954] Reward + Measures: [[-280.62757245    0.53307235    0.95695573    0.09373401    0.42616832
     0.20415273]][0m
[37m[1m[2023-07-11 02:59:34,600][233954] Max Reward on eval: -280.6275724540677[0m
[37m[1m[2023-07-11 02:59:34,600][233954] Min Reward on eval: -280.6275724540677[0m
[37m[1m[2023-07-11 02:59:34,600][233954] Mean Reward across all agents: -280.6275724540677[0m
[37m[1m[2023-07-11 02:59:34,601][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:59:39,521][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 02:59:39,522][233954] Reward + Measures: [[ -97.23742182    0.37439999    0.77080005    0.19939999    0.55520004
     1.52588499]
 [  36.67582661    0.8125        0.81950009    0.82380003    0.0467
     0.85942119]
 [  71.36772809    0.67189997    0.57950002    0.61910003    0.16680001
     1.01857984]
 ...
 [-115.2030992     0.80419999    0.85120004    0.7726        0.83020002
     1.90604305]
 [  54.06149029    0.7919001     0.80559999    0.84060001    0.84699994
     1.21510112]
 [ 108.47318504    0.4224        0.56389999    0.53860003    0.50990003
     0.9274832 ]][0m
[37m[1m[2023-07-11 02:59:39,522][233954] Max Reward on eval: 431.38629056960343[0m
[37m[1m[2023-07-11 02:59:39,522][233954] Min Reward on eval: -196.09504415090197[0m
[37m[1m[2023-07-11 02:59:39,523][233954] Mean Reward across all agents: 21.223944494051413[0m
[37m[1m[2023-07-11 02:59:39,523][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 02:59:39,539][233954] mean_value=54.552018510544826, max_value=757.679671379977[0m
[37m[1m[2023-07-11 02:59:39,542][233954] New mean coefficients: [[ 0.05495459  0.09294698  4.738102    1.2440383   0.70132536 -3.2430656 ]][0m
[37m[1m[2023-07-11 02:59:39,543][233954] Moving the mean solution point...[0m
[36m[2023-07-11 02:59:48,485][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 02:59:48,485][233954] FPS: 429481.00[0m
[36m[2023-07-11 02:59:48,488][233954] itr=234, itrs=2000, Progress: 11.70%[0m
[36m[2023-07-11 03:00:00,048][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 03:00:00,048][233954] FPS: 333718.19[0m
[36m[2023-07-11 03:00:04,254][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:00:04,255][233954] Reward + Measures: [[-279.65299202    0.55099767    0.95958471    0.09460266    0.44949764
     0.19944046]][0m
[37m[1m[2023-07-11 03:00:04,255][233954] Max Reward on eval: -279.6529920162279[0m
[37m[1m[2023-07-11 03:00:04,255][233954] Min Reward on eval: -279.6529920162279[0m
[37m[1m[2023-07-11 03:00:04,255][233954] Mean Reward across all agents: -279.6529920162279[0m
[37m[1m[2023-07-11 03:00:04,255][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:00:09,233][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:00:09,234][233954] Reward + Measures: [[ 58.61904433   0.1392       0.68990004   0.43790004   0.46110001
    0.905927  ]
 [ 95.14336966   0.69559997   0.75649995   0.46650001   0.40059996
    0.84421158]
 [ 57.97171071   0.80610001   0.78049999   0.84250003   0.14070001
    0.78330201]
 ...
 [ 78.82292359   0.44109997   0.49540001   0.59560001   0.26519999
    2.27223372]
 [199.74342341   0.27860001   0.54700005   0.17080002   0.38580003
    1.19193447]
 [ 69.43475255   0.33970001   0.27830002   0.36139998   0.18440001
    1.19566143]][0m
[37m[1m[2023-07-11 03:00:09,234][233954] Max Reward on eval: 433.32263564597815[0m
[37m[1m[2023-07-11 03:00:09,234][233954] Min Reward on eval: -358.9295353693422[0m
[37m[1m[2023-07-11 03:00:09,235][233954] Mean Reward across all agents: 107.50511937335912[0m
[37m[1m[2023-07-11 03:00:09,235][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:00:09,250][233954] mean_value=-157.9464841506938, max_value=710.307262424007[0m
[37m[1m[2023-07-11 03:00:09,252][233954] New mean coefficients: [[-0.35588783 -0.2531258   4.083221    0.65814877  0.07426775 -3.1772099 ]][0m
[37m[1m[2023-07-11 03:00:09,253][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:00:18,173][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 03:00:18,173][233954] FPS: 430598.94[0m
[36m[2023-07-11 03:00:18,175][233954] itr=235, itrs=2000, Progress: 11.75%[0m
[36m[2023-07-11 03:00:29,842][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 03:00:29,842][233954] FPS: 330626.47[0m
[36m[2023-07-11 03:00:34,168][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:00:34,168][233954] Reward + Measures: [[-314.5542872     0.563609      0.96030039    0.078313      0.41308135
     0.19276212]][0m
[37m[1m[2023-07-11 03:00:34,168][233954] Max Reward on eval: -314.5542872014606[0m
[37m[1m[2023-07-11 03:00:34,169][233954] Min Reward on eval: -314.5542872014606[0m
[37m[1m[2023-07-11 03:00:34,169][233954] Mean Reward across all agents: -314.5542872014606[0m
[37m[1m[2023-07-11 03:00:34,169][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:00:39,379][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:00:39,379][233954] Reward + Measures: [[-31.50878935   0.5582       0.80240005   0.56759995   0.72820002
    1.40478516]
 [-19.97385673   0.653        0.44179997   0.54789996   0.185
    1.19958842]
 [ -5.64158882   0.81850004   0.81599998   0.86679995   0.74360001
    0.53129894]
 ...
 [-97.74845718   0.42030001   0.76839995   0.17410001   0.65679997
    0.45877361]
 [192.99345493   0.2379       0.72970003   0.39720002   0.5557
    1.05904603]
 [ 94.64743006   0.15449999   0.52500004   0.34860003   0.54189998
    1.59335732]][0m
[37m[1m[2023-07-11 03:00:39,380][233954] Max Reward on eval: 371.4138927465072[0m
[37m[1m[2023-07-11 03:00:39,380][233954] Min Reward on eval: -250.6958436882589[0m
[37m[1m[2023-07-11 03:00:39,380][233954] Mean Reward across all agents: 45.50000857010783[0m
[37m[1m[2023-07-11 03:00:39,380][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:00:39,397][233954] mean_value=35.465347916916045, max_value=744.5981903076987[0m
[37m[1m[2023-07-11 03:00:39,399][233954] New mean coefficients: [[-0.3507416   0.24206734  3.95317     1.6673905  -0.11889309 -3.0876791 ]][0m
[37m[1m[2023-07-11 03:00:39,400][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:00:48,464][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 03:00:48,465][233954] FPS: 423736.62[0m
[36m[2023-07-11 03:00:48,467][233954] itr=236, itrs=2000, Progress: 11.80%[0m
[36m[2023-07-11 03:00:59,924][233954] train() took 11.40 seconds to complete[0m
[36m[2023-07-11 03:00:59,924][233954] FPS: 336842.72[0m
[36m[2023-07-11 03:01:04,154][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:01:04,154][233954] Reward + Measures: [[-336.90175915    0.52774465    0.92303306    0.11268633    0.38699567
     0.20500012]][0m
[37m[1m[2023-07-11 03:01:04,154][233954] Max Reward on eval: -336.9017591496035[0m
[37m[1m[2023-07-11 03:01:04,155][233954] Min Reward on eval: -336.9017591496035[0m
[37m[1m[2023-07-11 03:01:04,155][233954] Mean Reward across all agents: -336.9017591496035[0m
[37m[1m[2023-07-11 03:01:04,155][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:01:09,127][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:01:09,128][233954] Reward + Measures: [[  64.23193619    0.55730003    0.75549996    0.3132        0.4443
     1.19206607]
 [ -92.74740886    0.24980001    0.51840001    0.21169999    0.25770003
     1.18923163]
 [-148.5212446     0.336         0.67019999    0.28290001    0.33790001
     1.10452342]
 ...
 [ -94.70113163    0.3712        0.81820005    0.13240001    0.66359997
     1.68870771]
 [ -40.45037732    0.30360001    0.48520002    0.23020001    0.35100001
     2.28059268]
 [ 146.59137634    0.21250001    0.74629992    0.66009998    0.68280005
     1.18348777]][0m
[37m[1m[2023-07-11 03:01:09,128][233954] Max Reward on eval: 447.73499298309906[0m
[37m[1m[2023-07-11 03:01:09,128][233954] Min Reward on eval: -377.73115063561124[0m
[37m[1m[2023-07-11 03:01:09,129][233954] Mean Reward across all agents: 6.637404712937525[0m
[37m[1m[2023-07-11 03:01:09,129][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:01:09,139][233954] mean_value=-549.0724349984694, max_value=644.1384128821082[0m
[37m[1m[2023-07-11 03:01:09,142][233954] New mean coefficients: [[-0.03436339  0.7969022   4.3255744   0.95690256  0.29628903 -2.0428486 ]][0m
[37m[1m[2023-07-11 03:01:09,143][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:01:18,243][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 03:01:18,243][233954] FPS: 422062.90[0m
[36m[2023-07-11 03:01:18,245][233954] itr=237, itrs=2000, Progress: 11.85%[0m
[36m[2023-07-11 03:01:29,757][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 03:01:29,758][233954] FPS: 335120.00[0m
[36m[2023-07-11 03:01:34,094][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:01:34,094][233954] Reward + Measures: [[-334.57294015    0.58038628    0.95650667    0.08152233    0.40102798
     0.19041041]][0m
[37m[1m[2023-07-11 03:01:34,095][233954] Max Reward on eval: -334.57294014952345[0m
[37m[1m[2023-07-11 03:01:34,095][233954] Min Reward on eval: -334.57294014952345[0m
[37m[1m[2023-07-11 03:01:34,095][233954] Mean Reward across all agents: -334.57294014952345[0m
[37m[1m[2023-07-11 03:01:34,096][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:01:39,090][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:01:39,090][233954] Reward + Measures: [[357.96042444   0.50669998   0.63709998   0.14670001   0.3486
    1.09000587]
 [ 84.23856031   0.49540001   0.62530005   0.55580002   0.13689999
    0.67242855]
 [218.16148092   0.34570003   0.54449999   0.50629997   0.20180002
    1.16139686]
 ...
 [ -9.21034498   0.345        0.76819998   0.55840009   0.64309996
    0.90016073]
 [-96.31959552   0.29159999   0.58250004   0.52980006   0.2902
    0.6252144 ]
 [ 20.0061947    0.66460001   0.75129998   0.74060005   0.11110001
    0.61011881]][0m
[37m[1m[2023-07-11 03:01:39,090][233954] Max Reward on eval: 451.1696786654182[0m
[37m[1m[2023-07-11 03:01:39,091][233954] Min Reward on eval: -217.73690316602588[0m
[37m[1m[2023-07-11 03:01:39,091][233954] Mean Reward across all agents: 79.16450108473313[0m
[37m[1m[2023-07-11 03:01:39,091][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:01:39,108][233954] mean_value=42.57968854788868, max_value=872.112446336057[0m
[37m[1m[2023-07-11 03:01:39,110][233954] New mean coefficients: [[-0.0888317   0.6276492   4.2086596   0.71092606 -0.02509412 -2.5885966 ]][0m
[37m[1m[2023-07-11 03:01:39,111][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:01:48,045][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 03:01:48,045][233954] FPS: 429908.51[0m
[36m[2023-07-11 03:01:48,048][233954] itr=238, itrs=2000, Progress: 11.90%[0m
[36m[2023-07-11 03:01:59,744][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 03:01:59,745][233954] FPS: 329856.47[0m
[36m[2023-07-11 03:02:04,099][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:02:04,099][233954] Reward + Measures: [[-320.34418566    0.6204927     0.95718259    0.10063501    0.43210065
     0.1755776 ]][0m
[37m[1m[2023-07-11 03:02:04,099][233954] Max Reward on eval: -320.34418565563385[0m
[37m[1m[2023-07-11 03:02:04,100][233954] Min Reward on eval: -320.34418565563385[0m
[37m[1m[2023-07-11 03:02:04,100][233954] Mean Reward across all agents: -320.34418565563385[0m
[37m[1m[2023-07-11 03:02:04,100][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:02:09,083][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:02:09,083][233954] Reward + Measures: [[ -28.5949831     0.3644        0.76130003    0.0873        0.67650002
     0.61563247]
 [  16.69020874    0.44860002    0.62190002    0.43120003    0.45240003
     1.07397878]
 [  54.59495406    0.7608        0.21230002    0.6979        0.19240001
     1.45405543]
 ...
 [-130.79517414    0.61180001    0.40889999    0.51289999    0.14100002
     1.21650934]
 [  91.32916067    0.56350005    0.41949996    0.4777        0.40440002
     2.03162265]
 [  38.86206341    0.16339999    0.28510001    0.14310001    0.2309
     1.56451344]][0m
[37m[1m[2023-07-11 03:02:09,084][233954] Max Reward on eval: 514.4170608649962[0m
[37m[1m[2023-07-11 03:02:09,084][233954] Min Reward on eval: -392.3467640597373[0m
[37m[1m[2023-07-11 03:02:09,084][233954] Mean Reward across all agents: 64.93892949568102[0m
[37m[1m[2023-07-11 03:02:09,084][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:02:09,102][233954] mean_value=132.28007055883782, max_value=919.0050623858348[0m
[37m[1m[2023-07-11 03:02:09,105][233954] New mean coefficients: [[-0.39961943  1.1932306   4.15693     0.37806433  0.28444877 -2.5257597 ]][0m
[37m[1m[2023-07-11 03:02:09,106][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:02:18,099][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 03:02:18,099][233954] FPS: 427094.34[0m
[36m[2023-07-11 03:02:18,101][233954] itr=239, itrs=2000, Progress: 11.95%[0m
[36m[2023-07-11 03:02:29,602][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 03:02:29,607][233954] FPS: 335447.06[0m
[36m[2023-07-11 03:02:33,919][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:02:33,919][233954] Reward + Measures: [[-307.45544662    0.65456098    0.94833666    0.079782      0.43575469
     0.16950999]][0m
[37m[1m[2023-07-11 03:02:33,920][233954] Max Reward on eval: -307.4554466171822[0m
[37m[1m[2023-07-11 03:02:33,920][233954] Min Reward on eval: -307.4554466171822[0m
[37m[1m[2023-07-11 03:02:33,920][233954] Mean Reward across all agents: -307.4554466171822[0m
[37m[1m[2023-07-11 03:02:33,920][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:02:38,947][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:02:38,948][233954] Reward + Measures: [[  69.73134445    0.50080007    0.7762        0.1019        0.79940003
     1.35538161]
 [ 122.23345255    0.25099999    0.565         0.20469999    0.35420001
     1.08477759]
 [  77.35946408    0.4522        0.55500001    0.54220003    0.5564
     1.30256498]
 ...
 [-153.0888854     0.1207        0.8125        0.45410004    0.77780002
     1.64111221]
 [  95.02497492    0.31060001    0.65319997    0.25910002    0.62830001
     1.48045516]
 [ 269.00397251    0.4375        0.73659998    0.12890001    0.75370002
     1.62638175]][0m
[37m[1m[2023-07-11 03:02:38,948][233954] Max Reward on eval: 317.29703330397604[0m
[37m[1m[2023-07-11 03:02:38,948][233954] Min Reward on eval: -259.9929319035262[0m
[37m[1m[2023-07-11 03:02:38,948][233954] Mean Reward across all agents: 61.0254138725812[0m
[37m[1m[2023-07-11 03:02:38,948][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:02:38,964][233954] mean_value=-57.58062992766769, max_value=817.297033303976[0m
[37m[1m[2023-07-11 03:02:38,967][233954] New mean coefficients: [[-0.30838695  1.7146537   4.4580636  -0.5402688   0.84076965 -2.2493248 ]][0m
[37m[1m[2023-07-11 03:02:38,968][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:02:47,976][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 03:02:47,976][233954] FPS: 426368.94[0m
[36m[2023-07-11 03:02:47,979][233954] itr=240, itrs=2000, Progress: 12.00%[0m
[37m[1m[2023-07-11 03:05:37,146][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000220[0m
[36m[2023-07-11 03:05:49,788][233954] train() took 11.85 seconds to complete[0m
[36m[2023-07-11 03:05:49,789][233954] FPS: 324060.62[0m
[36m[2023-07-11 03:05:54,054][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:05:54,055][233954] Reward + Measures: [[-283.26605949    0.7593087     0.96874505    0.03561534    0.48954296
     0.15407312]][0m
[37m[1m[2023-07-11 03:05:54,055][233954] Max Reward on eval: -283.2660594896208[0m
[37m[1m[2023-07-11 03:05:54,055][233954] Min Reward on eval: -283.2660594896208[0m
[37m[1m[2023-07-11 03:05:54,055][233954] Mean Reward across all agents: -283.2660594896208[0m
[37m[1m[2023-07-11 03:05:54,056][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:05:58,956][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:05:58,956][233954] Reward + Measures: [[ 43.00921408   0.73989999   0.68940002   0.76550001   0.61569995
    1.50087535]
 [ 92.19709599   0.59210002   0.66509998   0.63860005   0.58959997
    1.4619118 ]
 [ 81.97926903   0.59090006   0.87670004   0.186        0.81770003
    1.45062482]
 ...
 [ 68.82712803   0.40019998   0.50620002   0.2194       0.48730001
    1.70379758]
 [115.16057059   0.29930001   0.77150005   0.2368       0.71209997
    1.21322739]
 [-37.38138555   0.95739996   0.9702       0.95279998   0.96579999
    2.32726932]][0m
[37m[1m[2023-07-11 03:05:58,957][233954] Max Reward on eval: 441.9721556186676[0m
[37m[1m[2023-07-11 03:05:58,957][233954] Min Reward on eval: -341.8645477041602[0m
[37m[1m[2023-07-11 03:05:58,957][233954] Mean Reward across all agents: 58.99831024332585[0m
[37m[1m[2023-07-11 03:05:58,957][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:05:58,971][233954] mean_value=-64.75002121378985, max_value=850.8742789982819[0m
[37m[1m[2023-07-11 03:05:59,043][233954] New mean coefficients: [[-0.77685124  1.3346524   3.7963583  -0.19548583  0.39850214 -3.2036643 ]][0m
[37m[1m[2023-07-11 03:05:59,044][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:06:08,127][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 03:06:08,127][233954] FPS: 422868.49[0m
[36m[2023-07-11 03:06:08,129][233954] itr=241, itrs=2000, Progress: 12.05%[0m
[36m[2023-07-11 03:06:19,698][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 03:06:19,703][233954] FPS: 333497.80[0m
[36m[2023-07-11 03:06:23,935][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:06:23,936][233954] Reward + Measures: [[-258.8612227     0.78679997    0.96624267    0.03785367    0.49503532
     0.16030513]][0m
[37m[1m[2023-07-11 03:06:23,936][233954] Max Reward on eval: -258.86122269598866[0m
[37m[1m[2023-07-11 03:06:23,936][233954] Min Reward on eval: -258.86122269598866[0m
[37m[1m[2023-07-11 03:06:23,936][233954] Mean Reward across all agents: -258.86122269598866[0m
[37m[1m[2023-07-11 03:06:23,936][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:06:29,127][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:06:29,128][233954] Reward + Measures: [[-38.16826506   0.19220001   0.74809998   0.25559998   0.64399999
    1.03530443]
 [  7.96125422   0.3682       0.71020001   0.45140001   0.69810003
    1.11798894]
 [ 83.71808165   0.33400002   0.69610006   0.40380001   0.66719997
    1.38897455]
 ...
 [-69.49473477   0.60220003   0.83520001   0.2852       0.67309999
    0.51450902]
 [ 10.72245913   0.48850003   0.90860003   0.48640004   0.90270007
    1.28490055]
 [-71.47437236   0.54390001   0.69880003   0.60149997   0.20869999
    0.93103659]][0m
[37m[1m[2023-07-11 03:06:29,128][233954] Max Reward on eval: 378.85132600425277[0m
[37m[1m[2023-07-11 03:06:29,128][233954] Min Reward on eval: -136.28067250018938[0m
[37m[1m[2023-07-11 03:06:29,128][233954] Mean Reward across all agents: 22.55809193241167[0m
[37m[1m[2023-07-11 03:06:29,129][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:06:29,146][233954] mean_value=163.44017220039817, max_value=878.8513260042528[0m
[37m[1m[2023-07-11 03:06:29,149][233954] New mean coefficients: [[-0.79355973  1.7232254   4.131585    0.6979577   0.8801749  -3.0318255 ]][0m
[37m[1m[2023-07-11 03:06:29,150][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:06:38,199][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 03:06:38,199][233954] FPS: 424444.03[0m
[36m[2023-07-11 03:06:38,201][233954] itr=242, itrs=2000, Progress: 12.10%[0m
[36m[2023-07-11 03:06:49,878][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 03:06:49,883][233954] FPS: 330458.60[0m
[36m[2023-07-11 03:06:54,195][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:06:54,196][233954] Reward + Measures: [[-226.13789076    0.80932236    0.95620668    0.03731167    0.51473099
     0.15353638]][0m
[37m[1m[2023-07-11 03:06:54,196][233954] Max Reward on eval: -226.13789075745558[0m
[37m[1m[2023-07-11 03:06:54,196][233954] Min Reward on eval: -226.13789075745558[0m
[37m[1m[2023-07-11 03:06:54,197][233954] Mean Reward across all agents: -226.13789075745558[0m
[37m[1m[2023-07-11 03:06:54,197][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:06:59,261][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:06:59,262][233954] Reward + Measures: [[102.80014779   0.46869999   0.68099993   0.46259999   0.54970002
    0.7308861 ]
 [230.6332426    0.34710002   0.37939999   0.3177       0.34639999
    1.51833272]
 [129.15675615   0.19839999   0.42290002   0.3344       0.32409999
    0.98052233]
 ...
 [-71.99111592   0.42440006   0.4492       0.25799999   0.36619997
    1.12639141]
 [ 78.32178243   0.32330003   0.48930001   0.2158       0.37189999
    1.74852145]
 [  4.65747124   0.56540006   0.89819998   0.17760001   0.83370012
    0.46942121]][0m
[37m[1m[2023-07-11 03:06:59,262][233954] Max Reward on eval: 395.6387482102029[0m
[37m[1m[2023-07-11 03:06:59,262][233954] Min Reward on eval: -162.9768967455253[0m
[37m[1m[2023-07-11 03:06:59,263][233954] Mean Reward across all agents: 80.29389739957196[0m
[37m[1m[2023-07-11 03:06:59,263][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:06:59,274][233954] mean_value=-229.13751730124622, max_value=737.131424655985[0m
[37m[1m[2023-07-11 03:06:59,277][233954] New mean coefficients: [[-0.7180749   1.1773412   3.7085295  -0.33406645  0.65143836 -3.3711367 ]][0m
[37m[1m[2023-07-11 03:06:59,278][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:07:08,246][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 03:07:08,246][233954] FPS: 428280.01[0m
[36m[2023-07-11 03:07:08,248][233954] itr=243, itrs=2000, Progress: 12.15%[0m
[36m[2023-07-11 03:07:19,764][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 03:07:19,765][233954] FPS: 335175.40[0m
[36m[2023-07-11 03:07:24,011][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:07:24,011][233954] Reward + Measures: [[-220.4114645     0.83094132    0.96737832    0.031608      0.57023871
     0.13983974]][0m
[37m[1m[2023-07-11 03:07:24,011][233954] Max Reward on eval: -220.4114645040827[0m
[37m[1m[2023-07-11 03:07:24,012][233954] Min Reward on eval: -220.4114645040827[0m
[37m[1m[2023-07-11 03:07:24,012][233954] Mean Reward across all agents: -220.4114645040827[0m
[37m[1m[2023-07-11 03:07:24,012][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:07:28,982][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:07:28,983][233954] Reward + Measures: [[   5.37337282    0.308         0.81910002    0.4021        0.57300001
     1.46154344]
 [ 113.57214631    0.82000011    0.92490005    0.83389997    0.88690007
     0.71842712]
 [-141.0027121     0.5546        0.65709996    0.74720001    0.31769997
     1.49566889]
 ...
 [  11.65797912    0.5941        0.73750001    0.69060004    0.2247
     2.009547  ]
 [  47.22002305    0.53790003    0.71320003    0.55249995    0.33740002
     2.22700286]
 [ 175.51479623    0.0455        0.87620002    0.45560002    0.90140003
     1.95995796]][0m
[37m[1m[2023-07-11 03:07:28,983][233954] Max Reward on eval: 299.6849470127374[0m
[37m[1m[2023-07-11 03:07:28,983][233954] Min Reward on eval: -273.1206236802042[0m
[37m[1m[2023-07-11 03:07:28,984][233954] Mean Reward across all agents: 29.456893713008316[0m
[37m[1m[2023-07-11 03:07:28,984][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:07:29,002][233954] mean_value=171.87112757138354, max_value=742.1091785475612[0m
[37m[1m[2023-07-11 03:07:29,005][233954] New mean coefficients: [[-1.0996466  1.7003515  3.3496685 -1.0820441  1.0705206 -3.4570575]][0m
[37m[1m[2023-07-11 03:07:29,006][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:07:37,937][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 03:07:37,937][233954] FPS: 430038.81[0m
[36m[2023-07-11 03:07:37,939][233954] itr=244, itrs=2000, Progress: 12.20%[0m
[36m[2023-07-11 03:07:49,374][233954] train() took 11.38 seconds to complete[0m
[36m[2023-07-11 03:07:49,374][233954] FPS: 337457.44[0m
[36m[2023-07-11 03:07:53,585][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:07:53,586][233954] Reward + Measures: [[-203.31641615    0.88908935    0.97150803    0.01987267    0.62572628
     0.14028323]][0m
[37m[1m[2023-07-11 03:07:53,586][233954] Max Reward on eval: -203.31641615374824[0m
[37m[1m[2023-07-11 03:07:53,586][233954] Min Reward on eval: -203.31641615374824[0m
[37m[1m[2023-07-11 03:07:53,586][233954] Mean Reward across all agents: -203.31641615374824[0m
[37m[1m[2023-07-11 03:07:53,587][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:07:58,571][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:07:58,572][233954] Reward + Measures: [[  30.04784281    0.43070003    0.54570001    0.43649998    0.46819997
     1.47773731]
 [ 111.41597703    0.26770002    0.53900003    0.29569998    0.46599999
     1.44522464]
 [ -43.70579055    0.74499995    0.70679998    0.49840003    0.2527
     1.94089913]
 ...
 [-201.76937387    0.5388        0.55179995    0.69600004    0.15110001
     2.19699264]
 [ 196.95323584    0.28140002    0.55410004    0.37490001    0.33880001
     1.13600886]
 [ -10.30593128    0.42050001    0.59510005    0.3752        0.47740003
     1.54604626]][0m
[37m[1m[2023-07-11 03:07:58,572][233954] Max Reward on eval: 542.4611511088908[0m
[37m[1m[2023-07-11 03:07:58,572][233954] Min Reward on eval: -367.53102498671507[0m
[37m[1m[2023-07-11 03:07:58,573][233954] Mean Reward across all agents: 15.123805666323927[0m
[37m[1m[2023-07-11 03:07:58,573][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:07:58,587][233954] mean_value=-272.5847450499164, max_value=633.9946267371531[0m
[37m[1m[2023-07-11 03:07:58,590][233954] New mean coefficients: [[-1.080568   1.9008396  3.1558535 -1.0935073  1.2968792 -2.7815406]][0m
[37m[1m[2023-07-11 03:07:58,591][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:08:07,542][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 03:08:07,542][233954] FPS: 429063.60[0m
[36m[2023-07-11 03:08:07,545][233954] itr=245, itrs=2000, Progress: 12.25%[0m
[36m[2023-07-11 03:08:19,106][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 03:08:19,106][233954] FPS: 333808.14[0m
[36m[2023-07-11 03:08:23,420][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:08:23,421][233954] Reward + Measures: [[-180.19552178    0.86078596    0.96968627    0.02202266    0.72722608
     0.16069892]][0m
[37m[1m[2023-07-11 03:08:23,421][233954] Max Reward on eval: -180.1955217782353[0m
[37m[1m[2023-07-11 03:08:23,421][233954] Min Reward on eval: -180.1955217782353[0m
[37m[1m[2023-07-11 03:08:23,421][233954] Mean Reward across all agents: -180.1955217782353[0m
[37m[1m[2023-07-11 03:08:23,422][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:08:28,392][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:08:28,393][233954] Reward + Measures: [[ -9.00429707   0.71029997   0.61980003   0.51560003   0.09219999
    0.68102151]
 [-53.30556317   0.36840001   0.47279999   0.43460003   0.25970003
    1.83630335]
 [-93.96325929   0.43829998   0.53330004   0.41149998   0.32730001
    1.89771461]
 ...
 [-88.56639671   0.36960003   0.52010006   0.38770005   0.46090004
    1.18568552]
 [146.05652573   0.1631       0.39910001   0.21530001   0.46370003
    2.38663912]
 [  7.98871911   0.68979996   0.61250001   0.50879997   0.10210001
    1.64650095]][0m
[37m[1m[2023-07-11 03:08:28,393][233954] Max Reward on eval: 485.3231010347605[0m
[37m[1m[2023-07-11 03:08:28,393][233954] Min Reward on eval: -444.54922914281485[0m
[37m[1m[2023-07-11 03:08:28,393][233954] Mean Reward across all agents: 45.41463377637146[0m
[37m[1m[2023-07-11 03:08:28,394][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:08:28,409][233954] mean_value=-6.515770500107125, max_value=838.1962649811991[0m
[37m[1m[2023-07-11 03:08:28,412][233954] New mean coefficients: [[-0.7203436   1.8767297   3.4168854   0.31800508  1.1610521  -2.6242692 ]][0m
[37m[1m[2023-07-11 03:08:28,413][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:08:37,416][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 03:08:37,416][233954] FPS: 426597.90[0m
[36m[2023-07-11 03:08:37,418][233954] itr=246, itrs=2000, Progress: 12.30%[0m
[36m[2023-07-11 03:08:49,126][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 03:08:49,126][233954] FPS: 329591.28[0m
[36m[2023-07-11 03:08:53,334][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:08:53,334][233954] Reward + Measures: [[-98.15317878   0.84950131   0.96856833   0.040306     0.84626865
    0.21314308]][0m
[37m[1m[2023-07-11 03:08:53,335][233954] Max Reward on eval: -98.15317877946863[0m
[37m[1m[2023-07-11 03:08:53,335][233954] Min Reward on eval: -98.15317877946863[0m
[37m[1m[2023-07-11 03:08:53,335][233954] Mean Reward across all agents: -98.15317877946863[0m
[37m[1m[2023-07-11 03:08:53,335][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:08:58,486][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:08:58,486][233954] Reward + Measures: [[ -18.13408059    0.37820002    0.86830008    0.38519999    0.78150004
     0.62696946]
 [  28.13351099    0.61900002    0.66900009    0.27689999    0.74549997
     2.50876784]
 [  42.74958015    0.72640002    0.27579999    0.55269998    0.54250002
     1.9798727 ]
 ...
 [ -41.50049119    0.72530001    0.31020001    0.52290004    0.54449999
     2.02230096]
 [-438.88448907    0.82919997    0.95900005    0.0208        0.9842
     2.23217511]
 [ -39.64226043    0.63160002    0.80740005    0.59020001    0.80100006
     1.21962726]][0m
[37m[1m[2023-07-11 03:08:58,486][233954] Max Reward on eval: 273.4993204936385[0m
[37m[1m[2023-07-11 03:08:58,487][233954] Min Reward on eval: -530.753665921744[0m
[37m[1m[2023-07-11 03:08:58,487][233954] Mean Reward across all agents: -24.854910336103853[0m
[37m[1m[2023-07-11 03:08:58,487][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:08:58,510][233954] mean_value=219.57835474297516, max_value=690.5329992622137[0m
[37m[1m[2023-07-11 03:08:58,512][233954] New mean coefficients: [[-0.47964358  2.1145422   3.645083   -0.00362596  1.1586065  -2.484256  ]][0m
[37m[1m[2023-07-11 03:08:58,513][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:09:07,464][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 03:09:07,464][233954] FPS: 429102.79[0m
[36m[2023-07-11 03:09:07,466][233954] itr=247, itrs=2000, Progress: 12.35%[0m
[36m[2023-07-11 03:09:19,084][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 03:09:19,084][233954] FPS: 332151.00[0m
[36m[2023-07-11 03:09:23,373][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:09:23,373][233954] Reward + Measures: [[-115.4535459     0.88173032    0.96390802    0.021702      0.80313468
     0.1780621 ]][0m
[37m[1m[2023-07-11 03:09:23,373][233954] Max Reward on eval: -115.45354590301092[0m
[37m[1m[2023-07-11 03:09:23,374][233954] Min Reward on eval: -115.45354590301092[0m
[37m[1m[2023-07-11 03:09:23,374][233954] Mean Reward across all agents: -115.45354590301092[0m
[37m[1m[2023-07-11 03:09:23,374][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:09:28,381][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:09:28,382][233954] Reward + Measures: [[ 13.71494958   0.95520002   0.90119994   0.94980001   0.90900004
    1.65939558]
 [ 42.83458196   0.88959998   0.89519995   0.87480003   0.81019992
    1.34253395]
 [132.09795094   0.45989999   0.94509995   0.35840002   0.9346
    1.4803158 ]
 ...
 [107.7380119    0.27779999   0.66420001   0.5564       0.41480002
    1.27064729]
 [  4.01321994   0.75260007   0.85089999   0.72980005   0.70250005
    0.80918151]
 [ 20.2765615    0.5948       0.68379998   0.56010002   0.81860012
    1.54969347]][0m
[37m[1m[2023-07-11 03:09:28,382][233954] Max Reward on eval: 274.8651823582128[0m
[37m[1m[2023-07-11 03:09:28,382][233954] Min Reward on eval: -457.0496124673635[0m
[37m[1m[2023-07-11 03:09:28,383][233954] Mean Reward across all agents: 1.0409272765955757[0m
[37m[1m[2023-07-11 03:09:28,383][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:09:28,405][233954] mean_value=234.79294331044366, max_value=632.097950944677[0m
[37m[1m[2023-07-11 03:09:28,407][233954] New mean coefficients: [[-0.403314   2.4578502  3.8423452  0.6074258  1.2212585 -1.8760521]][0m
[37m[1m[2023-07-11 03:09:28,408][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:09:37,422][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 03:09:37,422][233954] FPS: 426115.18[0m
[36m[2023-07-11 03:09:37,424][233954] itr=248, itrs=2000, Progress: 12.40%[0m
[36m[2023-07-11 03:09:49,509][233954] train() took 12.03 seconds to complete[0m
[36m[2023-07-11 03:09:49,509][233954] FPS: 319301.49[0m
[36m[2023-07-11 03:09:53,800][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:09:53,806][233954] Reward + Measures: [[-78.84862795   0.90979499   0.97367203   0.01401433   0.87403834
    0.16675754]][0m
[37m[1m[2023-07-11 03:09:53,806][233954] Max Reward on eval: -78.84862795013007[0m
[37m[1m[2023-07-11 03:09:53,807][233954] Min Reward on eval: -78.84862795013007[0m
[37m[1m[2023-07-11 03:09:53,807][233954] Mean Reward across all agents: -78.84862795013007[0m
[37m[1m[2023-07-11 03:09:53,807][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:09:58,809][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:09:58,815][233954] Reward + Measures: [[  1.96625318   0.59009999   0.69209999   0.0298       0.73900002
    0.89578861]
 [-43.65273558   0.47550002   0.63510001   0.47259998   0.68290007
    1.68624496]
 [  1.61065689   0.85579997   0.87630004   0.86020005   0.86499995
    1.65243649]
 ...
 [-58.67004891   0.5844       0.73890001   0.54409999   0.73069996
    1.5519613 ]
 [-20.3546603    0.28440002   0.52559996   0.15110001   0.50350004
    1.43308675]
 [  9.62770537   0.8544001    0.87639999   0.84790003   0.87050003
    1.45416296]][0m
[37m[1m[2023-07-11 03:09:58,815][233954] Max Reward on eval: 110.4579963542521[0m
[37m[1m[2023-07-11 03:09:58,815][233954] Min Reward on eval: -428.61361312940716[0m
[37m[1m[2023-07-11 03:09:58,816][233954] Mean Reward across all agents: -31.95431493085831[0m
[37m[1m[2023-07-11 03:09:58,816][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:09:58,828][233954] mean_value=4.025522241963069, max_value=520.6680587030947[0m
[37m[1m[2023-07-11 03:09:58,830][233954] New mean coefficients: [[-0.56658655  2.084462    3.5791526  -0.21035099  1.097265   -1.427092  ]][0m
[37m[1m[2023-07-11 03:09:58,831][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:10:07,843][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 03:10:07,844][233954] FPS: 426167.17[0m
[36m[2023-07-11 03:10:07,846][233954] itr=249, itrs=2000, Progress: 12.45%[0m
[36m[2023-07-11 03:10:19,543][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 03:10:19,544][233954] FPS: 330017.77[0m
[36m[2023-07-11 03:10:23,857][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:10:23,857][233954] Reward + Measures: [[-34.49222537   0.93424529   0.97709399   0.01246233   0.91593164
    0.16149825]][0m
[37m[1m[2023-07-11 03:10:23,858][233954] Max Reward on eval: -34.49222537240046[0m
[37m[1m[2023-07-11 03:10:23,858][233954] Min Reward on eval: -34.49222537240046[0m
[37m[1m[2023-07-11 03:10:23,858][233954] Mean Reward across all agents: -34.49222537240046[0m
[37m[1m[2023-07-11 03:10:23,858][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:10:28,873][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:10:28,873][233954] Reward + Measures: [[-30.94768099   0.78619999   0.75509995   0.1062       0.92369998
    1.77654541]
 [ 29.02506402   0.6717       0.84219998   0.43940002   0.88090003
    0.58169234]
 [ -1.15860096   0.39860001   0.67970002   0.38649997   0.68400002
    0.93939954]
 ...
 [-40.30394502   0.50629997   0.80610001   0.41589999   0.85570002
    1.58951461]
 [-46.93237788   0.40190002   0.55580002   0.1807       0.5406
    1.0880698 ]
 [-30.57566634   0.10090001   0.90389997   0.7511       0.91300005
    1.54404306]][0m
[37m[1m[2023-07-11 03:10:28,874][233954] Max Reward on eval: 213.3144454985857[0m
[37m[1m[2023-07-11 03:10:28,874][233954] Min Reward on eval: -501.6926841374487[0m
[37m[1m[2023-07-11 03:10:28,874][233954] Mean Reward across all agents: -47.38456725621243[0m
[37m[1m[2023-07-11 03:10:28,874][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:10:28,892][233954] mean_value=-3.090245489122487, max_value=582.7086880089715[0m
[37m[1m[2023-07-11 03:10:28,895][233954] New mean coefficients: [[-0.84505504  1.416368    3.4877758  -1.6457186   0.7052249  -2.22447   ]][0m
[37m[1m[2023-07-11 03:10:28,895][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:10:37,943][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 03:10:37,943][233954] FPS: 424495.28[0m
[36m[2023-07-11 03:10:37,946][233954] itr=250, itrs=2000, Progress: 12.50%[0m
[37m[1m[2023-07-11 03:13:22,378][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000230[0m
[36m[2023-07-11 03:13:35,039][233954] train() took 11.95 seconds to complete[0m
[36m[2023-07-11 03:13:35,039][233954] FPS: 321359.56[0m
[36m[2023-07-11 03:13:39,355][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:13:39,355][233954] Reward + Measures: [[-37.50480018   0.92673975   0.96299529   0.01423833   0.91346264
    0.174491  ]][0m
[37m[1m[2023-07-11 03:13:39,355][233954] Max Reward on eval: -37.50480017812913[0m
[37m[1m[2023-07-11 03:13:39,356][233954] Min Reward on eval: -37.50480017812913[0m
[37m[1m[2023-07-11 03:13:39,356][233954] Mean Reward across all agents: -37.50480017812913[0m
[37m[1m[2023-07-11 03:13:39,356][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:13:44,309][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:13:44,310][233954] Reward + Measures: [[  70.39930679    0.85789996    0.60869998    0.45640001    0.50210005
     1.01696527]
 [ -26.28906239    0.90760005    0.2378        0.90249997    0.1367
     0.73470205]
 [  89.2783647     0.76379997    0.80450004    0.13500001    0.78150004
     1.26441956]
 ...
 [ -27.05880601    0.91679996    0.2825        0.93139994    0.11930001
     0.84505141]
 [-114.89894102    0.93330002    0.91140002    0.93230003    0.0259
     0.84525818]
 [  12.92577772    0.8955        0.39300001    0.89260006    0.12409999
     1.44231987]][0m
[37m[1m[2023-07-11 03:13:44,310][233954] Max Reward on eval: 238.8069310390856[0m
[37m[1m[2023-07-11 03:13:44,310][233954] Min Reward on eval: -384.404847623501[0m
[37m[1m[2023-07-11 03:13:44,311][233954] Mean Reward across all agents: 14.812503312486758[0m
[37m[1m[2023-07-11 03:13:44,311][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:13:44,333][233954] mean_value=247.8682821446603, max_value=628.5906746780163[0m
[37m[1m[2023-07-11 03:13:44,374][233954] New mean coefficients: [[-0.6573962   1.5707926   4.0616236  -1.485908    0.53391206 -1.880196  ]][0m
[37m[1m[2023-07-11 03:13:44,375][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:13:53,258][233954] train() took 8.88 seconds to complete[0m
[36m[2023-07-11 03:13:53,258][233954] FPS: 432359.77[0m
[36m[2023-07-11 03:13:53,260][233954] itr=251, itrs=2000, Progress: 12.55%[0m
[36m[2023-07-11 03:14:04,918][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 03:14:04,918][233954] FPS: 331062.72[0m
[36m[2023-07-11 03:14:09,233][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:14:09,234][233954] Reward + Measures: [[-29.63652711   0.93661463   0.97491264   0.01888567   0.93242568
    0.17171703]][0m
[37m[1m[2023-07-11 03:14:09,234][233954] Max Reward on eval: -29.636527106663774[0m
[37m[1m[2023-07-11 03:14:09,234][233954] Min Reward on eval: -29.636527106663774[0m
[37m[1m[2023-07-11 03:14:09,235][233954] Mean Reward across all agents: -29.636527106663774[0m
[37m[1m[2023-07-11 03:14:09,235][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:14:14,410][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:14:14,410][233954] Reward + Measures: [[  45.85315757    0.3928        0.67589998    0.53290004    0.52220005
     1.60885811]
 [ 249.39368678    0.72690004    0.78410006    0.21529999    0.7834
     2.02435565]
 [ -49.98125275    0.8035        0.87219995    0.0552        0.86750001
     0.99218953]
 ...
 [ 177.35440875    0.80480003    0.79710001    0.47830001    0.5948
     1.51629651]
 [-180.45927903    0.9084        0.82300007    0.79250002    0.10220001
     2.28466392]
 [ -17.50062002    0.53900003    0.72549999    0.53509998    0.52539998
     0.69983655]][0m
[37m[1m[2023-07-11 03:14:14,411][233954] Max Reward on eval: 544.5589256163454[0m
[37m[1m[2023-07-11 03:14:14,411][233954] Min Reward on eval: -380.8410014785826[0m
[37m[1m[2023-07-11 03:14:14,411][233954] Mean Reward across all agents: 42.79762794639714[0m
[37m[1m[2023-07-11 03:14:14,411][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:14:14,428][233954] mean_value=88.54932213359818, max_value=762.8800917651308[0m
[37m[1m[2023-07-11 03:14:14,431][233954] New mean coefficients: [[-0.58942664  1.3862855   4.1168957  -2.5180144   0.79898703 -1.4555146 ]][0m
[37m[1m[2023-07-11 03:14:14,432][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:14:23,497][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 03:14:23,497][233954] FPS: 423652.53[0m
[36m[2023-07-11 03:14:23,500][233954] itr=252, itrs=2000, Progress: 12.60%[0m
[36m[2023-07-11 03:14:35,045][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 03:14:35,045][233954] FPS: 334325.68[0m
[36m[2023-07-11 03:14:39,279][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:14:39,279][233954] Reward + Measures: [[-10.86068733   0.9298383    0.96574366   0.02450033   0.93034303
    0.18451445]][0m
[37m[1m[2023-07-11 03:14:39,280][233954] Max Reward on eval: -10.860687325923335[0m
[37m[1m[2023-07-11 03:14:39,280][233954] Min Reward on eval: -10.860687325923335[0m
[37m[1m[2023-07-11 03:14:39,280][233954] Mean Reward across all agents: -10.860687325923335[0m
[37m[1m[2023-07-11 03:14:39,280][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:14:44,212][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:14:44,212][233954] Reward + Measures: [[ -35.36494307    0.85369998    0.055         0.824         0.67440003
     2.05899048]
 [-139.39265501    0.98670006    0.0085        0.96079999    0.66790003
     1.28491771]
 [-117.78558329    0.32469997    0.2802        0.36340001    0.25120002
     2.43046927]
 ...
 [  91.31722477    0.28830001    0.63760006    0.2066        0.55619997
     1.76262605]
 [ 122.08370278    0.77360004    0.16300002    0.67119998    0.51010001
     1.85974395]
 [  27.02932146    0.69870001    0.20469999    0.71790004    0.3721
     1.58487701]][0m
[37m[1m[2023-07-11 03:14:44,213][233954] Max Reward on eval: 354.8546256835107[0m
[37m[1m[2023-07-11 03:14:44,213][233954] Min Reward on eval: -358.7971162417904[0m
[37m[1m[2023-07-11 03:14:44,213][233954] Mean Reward across all agents: -7.592045279488393[0m
[37m[1m[2023-07-11 03:14:44,213][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:14:44,227][233954] mean_value=34.56690929900675, max_value=613.4396051166668[0m
[37m[1m[2023-07-11 03:14:44,230][233954] New mean coefficients: [[-0.93503714  1.7631751   3.8486133  -1.1662242   0.9800492  -1.2596148 ]][0m
[37m[1m[2023-07-11 03:14:44,231][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:14:53,197][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 03:14:53,197][233954] FPS: 428335.39[0m
[36m[2023-07-11 03:14:53,200][233954] itr=253, itrs=2000, Progress: 12.65%[0m
[36m[2023-07-11 03:15:04,699][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 03:15:04,700][233954] FPS: 335695.49[0m
[36m[2023-07-11 03:15:08,930][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:15:08,931][233954] Reward + Measures: [[5.99225404 0.91109991 0.94638067 0.02718467 0.91038233 0.20939022]][0m
[37m[1m[2023-07-11 03:15:08,931][233954] Max Reward on eval: 5.992254035336994[0m
[37m[1m[2023-07-11 03:15:08,931][233954] Min Reward on eval: 5.992254035336994[0m
[37m[1m[2023-07-11 03:15:08,931][233954] Mean Reward across all agents: 5.992254035336994[0m
[37m[1m[2023-07-11 03:15:08,932][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:15:13,924][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:15:13,929][233954] Reward + Measures: [[159.90300693   0.73940003   0.16410001   0.65480006   0.6354
    2.96762609]
 [ -1.09823534   0.81610006   0.68279999   0.2438       0.74400002
    1.54658997]
 [ 64.03630829   0.64660001   0.54139996   0.3935       0.537
    0.94503224]
 ...
 [ 62.95232994   0.71679997   0.20229998   0.68310004   0.33580002
    1.43111598]
 [ 53.68274834   0.86770004   0.34900001   0.57959998   0.77759999
    1.9903481 ]
 [-37.53999205   0.61629999   0.28140002   0.51570004   0.4061
    2.29555655]][0m
[37m[1m[2023-07-11 03:15:13,930][233954] Max Reward on eval: 397.0523854048923[0m
[37m[1m[2023-07-11 03:15:13,930][233954] Min Reward on eval: -157.92995117306708[0m
[37m[1m[2023-07-11 03:15:13,930][233954] Mean Reward across all agents: 27.184697688730854[0m
[37m[1m[2023-07-11 03:15:13,930][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:15:13,953][233954] mean_value=290.93996697588676, max_value=861.2176858924049[0m
[37m[1m[2023-07-11 03:15:13,956][233954] New mean coefficients: [[-0.35451823  1.5586724   4.1821494  -1.6393758   0.8107016  -1.5679407 ]][0m
[37m[1m[2023-07-11 03:15:13,957][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:15:22,888][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 03:15:22,893][233954] FPS: 430066.25[0m
[36m[2023-07-11 03:15:22,896][233954] itr=254, itrs=2000, Progress: 12.70%[0m
[36m[2023-07-11 03:15:34,423][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 03:15:34,423][233954] FPS: 334880.57[0m
[36m[2023-07-11 03:15:38,654][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:15:38,660][233954] Reward + Measures: [[-1.25508641  0.94079864  0.96726203  0.02686733  0.94748813  0.19822758]][0m
[37m[1m[2023-07-11 03:15:38,660][233954] Max Reward on eval: -1.255086413357174[0m
[37m[1m[2023-07-11 03:15:38,660][233954] Min Reward on eval: -1.255086413357174[0m
[37m[1m[2023-07-11 03:15:38,661][233954] Mean Reward across all agents: -1.255086413357174[0m
[37m[1m[2023-07-11 03:15:38,661][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:15:43,553][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:15:43,554][233954] Reward + Measures: [[-32.66621343   0.92840004   0.75400001   0.8592       0.55360001
    1.29328728]
 [-49.46724435   0.76460004   0.64160007   0.60930002   0.7784
    1.47502327]
 [-60.29170205   0.61879998   0.18410002   0.57129997   0.71429998
    1.66673172]
 ...
 [-43.55247873   0.43350002   0.36010003   0.4192       0.58109999
    1.3921721 ]
 [  2.48414935   0.55870003   0.30879998   0.55299997   0.68069994
    1.94666004]
 [ -7.4738974    0.57749999   0.84720004   0.07359999   0.80849999
    0.60181868]][0m
[37m[1m[2023-07-11 03:15:43,554][233954] Max Reward on eval: 355.1617336720694[0m
[37m[1m[2023-07-11 03:15:43,554][233954] Min Reward on eval: -396.5040149666369[0m
[37m[1m[2023-07-11 03:15:43,554][233954] Mean Reward across all agents: -6.982662910527671[0m
[37m[1m[2023-07-11 03:15:43,554][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:15:43,575][233954] mean_value=217.63433961186402, max_value=717.5555076606106[0m
[37m[1m[2023-07-11 03:15:43,578][233954] New mean coefficients: [[ 0.09225905  1.7860193   4.442909   -1.1050427   0.8799755  -1.2545743 ]][0m
[37m[1m[2023-07-11 03:15:43,579][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:15:52,554][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 03:15:52,559][233954] FPS: 427938.52[0m
[36m[2023-07-11 03:15:52,562][233954] itr=255, itrs=2000, Progress: 12.75%[0m
[36m[2023-07-11 03:16:04,241][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 03:16:04,241][233954] FPS: 330510.99[0m
[36m[2023-07-11 03:16:08,498][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:16:08,503][233954] Reward + Measures: [[4.50674767 0.95415902 0.97399431 0.017825   0.96393669 0.18519059]][0m
[37m[1m[2023-07-11 03:16:08,504][233954] Max Reward on eval: 4.506747673754173[0m
[37m[1m[2023-07-11 03:16:08,504][233954] Min Reward on eval: 4.506747673754173[0m
[37m[1m[2023-07-11 03:16:08,504][233954] Mean Reward across all agents: 4.506747673754173[0m
[37m[1m[2023-07-11 03:16:08,505][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:16:13,486][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:16:13,487][233954] Reward + Measures: [[  0.27524264   0.52700001   0.59850007   0.0764       0.64599997
    0.86190122]
 [-22.19064293   0.80660003   0.86040002   0.0425       0.83390009
    0.98883981]
 [174.42349816   0.90480006   0.0525       0.90439999   0.82349998
    1.67577589]
 ...
 [ 59.127779     0.81010002   0.1408       0.81520003   0.54890001
    1.77483928]
 [ 66.4638796    0.65070003   0.71039999   0.0691       0.72940004
    0.4192358 ]
 [-67.42313018   0.2719       0.45170003   0.16150001   0.41750002
    1.70195043]][0m
[37m[1m[2023-07-11 03:16:13,487][233954] Max Reward on eval: 245.52223301734776[0m
[37m[1m[2023-07-11 03:16:13,487][233954] Min Reward on eval: -285.3948688682634[0m
[37m[1m[2023-07-11 03:16:13,487][233954] Mean Reward across all agents: 12.862830430777086[0m
[37m[1m[2023-07-11 03:16:13,488][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:16:13,505][233954] mean_value=190.98810040186981, max_value=607.1901254404709[0m
[37m[1m[2023-07-11 03:16:13,513][233954] New mean coefficients: [[ 0.19192293  2.0459423   4.6574407  -2.217494    1.305506   -1.2666854 ]][0m
[37m[1m[2023-07-11 03:16:13,514][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:16:22,625][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 03:16:22,631][233954] FPS: 421514.00[0m
[36m[2023-07-11 03:16:22,634][233954] itr=256, itrs=2000, Progress: 12.80%[0m
[36m[2023-07-11 03:16:34,299][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 03:16:34,299][233954] FPS: 331015.31[0m
[36m[2023-07-11 03:16:38,599][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:16:38,604][233954] Reward + Measures: [[10.2742198   0.95216668  0.98070866  0.01505567  0.96178198  0.18259159]][0m
[37m[1m[2023-07-11 03:16:38,605][233954] Max Reward on eval: 10.274219796148264[0m
[37m[1m[2023-07-11 03:16:38,605][233954] Min Reward on eval: 10.274219796148264[0m
[37m[1m[2023-07-11 03:16:38,605][233954] Mean Reward across all agents: 10.274219796148264[0m
[37m[1m[2023-07-11 03:16:38,606][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:16:43,753][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:16:43,759][233954] Reward + Measures: [[-46.55583632   0.3698       0.5625       0.18730001   0.43789998
    1.65281165]
 [ 12.88371277   0.90900004   0.24700001   0.90090001   0.30500004
    0.73895341]
 [ 36.36160753   0.91420001   0.21870001   0.86320001   0.3687
    0.69402391]
 ...
 [ -4.99339146   0.82100004   0.88239998   0.0153       0.81540006
    0.31116387]
 [ 45.48156042   0.28770003   0.44850001   0.1974       0.33650002
    1.56866014]
 [  1.58662308   0.84829998   0.92980003   0.0196       0.90550005
    0.24440761]][0m
[37m[1m[2023-07-11 03:16:43,759][233954] Max Reward on eval: 158.5199135677307[0m
[37m[1m[2023-07-11 03:16:43,759][233954] Min Reward on eval: -304.05852123228834[0m
[37m[1m[2023-07-11 03:16:43,760][233954] Mean Reward across all agents: -1.6647516157162605[0m
[37m[1m[2023-07-11 03:16:43,760][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:16:43,774][233954] mean_value=17.651769894491128, max_value=617.2743120025843[0m
[37m[1m[2023-07-11 03:16:43,777][233954] New mean coefficients: [[-0.01629944  2.111769    4.5805593  -1.2148111   1.5926826  -1.1279722 ]][0m
[37m[1m[2023-07-11 03:16:43,778][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:16:52,836][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 03:16:52,841][233954] FPS: 424024.06[0m
[36m[2023-07-11 03:16:52,844][233954] itr=257, itrs=2000, Progress: 12.85%[0m
[36m[2023-07-11 03:17:04,529][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 03:17:04,529][233954] FPS: 330392.42[0m
[36m[2023-07-11 03:17:08,755][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:17:08,756][233954] Reward + Measures: [[19.63158686  0.95498163  0.97682893  0.01571433  0.96096069  0.2298788 ]][0m
[37m[1m[2023-07-11 03:17:08,756][233954] Max Reward on eval: 19.631586863167506[0m
[37m[1m[2023-07-11 03:17:08,756][233954] Min Reward on eval: 19.631586863167506[0m
[37m[1m[2023-07-11 03:17:08,756][233954] Mean Reward across all agents: 19.631586863167506[0m
[37m[1m[2023-07-11 03:17:08,757][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:17:13,803][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:17:13,808][233954] Reward + Measures: [[-167.76430187    0.5363        0.6146        0.16060001    0.66940004
     2.19957042]
 [   9.84426618    0.58710003    0.71750003    0.12609999    0.83859998
     1.25968456]
 [ -32.81048386    0.44520003    0.38299999    0.2483        0.42130002
     1.9697994 ]
 ...
 [-162.8038974     0.77719998    0.51780003    0.49700004    0.4815
     1.79348183]
 [ -58.60352652    0.58720005    0.67810005    0.14129999    0.72650003
     1.51131105]
 [  46.4457775     0.29860002    0.69859999    0.35660002    0.68610001
     1.03746915]][0m
[37m[1m[2023-07-11 03:17:13,809][233954] Max Reward on eval: 152.58230105666445[0m
[37m[1m[2023-07-11 03:17:13,809][233954] Min Reward on eval: -316.729712443985[0m
[37m[1m[2023-07-11 03:17:13,809][233954] Mean Reward across all agents: -25.459425572876807[0m
[37m[1m[2023-07-11 03:17:13,809][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:17:13,824][233954] mean_value=92.94484292486176, max_value=652.5823010566644[0m
[37m[1m[2023-07-11 03:17:13,827][233954] New mean coefficients: [[ 0.02402804  1.9934008   4.671322    0.0607307   1.5297695  -1.3031719 ]][0m
[37m[1m[2023-07-11 03:17:13,828][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:17:22,873][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 03:17:22,878][233954] FPS: 424657.79[0m
[36m[2023-07-11 03:17:22,881][233954] itr=258, itrs=2000, Progress: 12.90%[0m
[36m[2023-07-11 03:17:34,631][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 03:17:34,632][233954] FPS: 328615.09[0m
[36m[2023-07-11 03:17:38,945][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:17:38,945][233954] Reward + Measures: [[19.87744081  0.94909     0.96857244  0.024328    0.9565267   0.19842513]][0m
[37m[1m[2023-07-11 03:17:38,945][233954] Max Reward on eval: 19.87744080626925[0m
[37m[1m[2023-07-11 03:17:38,946][233954] Min Reward on eval: 19.87744080626925[0m
[37m[1m[2023-07-11 03:17:38,946][233954] Mean Reward across all agents: 19.87744080626925[0m
[37m[1m[2023-07-11 03:17:38,946][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:17:43,963][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:17:43,963][233954] Reward + Measures: [[146.12151124   0.30779999   0.49540001   0.30809999   0.49699998
    1.57831824]
 [  1.72272837   0.85459995   0.1425       0.8915       0.67229998
    1.30091512]
 [-99.55221651   0.30990002   0.70810002   0.1141       0.63930005
    1.66818607]
 ...
 [102.00842473   0.80779999   0.78479999   0.77630007   0.82139999
    0.92892247]
 [  1.20580795   0.67610002   0.89659995   0.0566       0.89120001
    1.42557275]
 [-71.73678159   0.5395       0.1701       0.53050005   0.4601
    1.94950259]][0m
[37m[1m[2023-07-11 03:17:43,963][233954] Max Reward on eval: 287.9279451963492[0m
[37m[1m[2023-07-11 03:17:43,964][233954] Min Reward on eval: -348.6507530789357[0m
[37m[1m[2023-07-11 03:17:43,964][233954] Mean Reward across all agents: 58.986583561425796[0m
[37m[1m[2023-07-11 03:17:43,964][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:17:43,980][233954] mean_value=123.22366523774659, max_value=612.1334459521574[0m
[37m[1m[2023-07-11 03:17:43,983][233954] New mean coefficients: [[-0.55325973  1.7312948   4.190762   -0.62709767  1.501822   -1.4562554 ]][0m
[37m[1m[2023-07-11 03:17:43,984][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:17:53,020][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 03:17:53,021][233954] FPS: 425012.12[0m
[36m[2023-07-11 03:17:53,023][233954] itr=259, itrs=2000, Progress: 12.95%[0m
[36m[2023-07-11 03:18:04,526][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 03:18:04,527][233954] FPS: 335586.51[0m
[36m[2023-07-11 03:18:08,779][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:18:08,779][233954] Reward + Measures: [[3.85647319 0.95271134 0.97732258 0.01777767 0.96780503 0.19771789]][0m
[37m[1m[2023-07-11 03:18:08,779][233954] Max Reward on eval: 3.8564731893534967[0m
[37m[1m[2023-07-11 03:18:08,780][233954] Min Reward on eval: 3.8564731893534967[0m
[37m[1m[2023-07-11 03:18:08,780][233954] Mean Reward across all agents: 3.8564731893534967[0m
[37m[1m[2023-07-11 03:18:08,780][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:18:13,802][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:18:13,802][233954] Reward + Measures: [[ -0.82419099   0.41430002   0.43590003   0.55059999   0.44189999
    1.43461537]
 [ 29.33001642   0.47270003   0.51119995   0.35350001   0.60280001
    1.51508093]
 [ -0.95077175   0.65080005   0.63920003   0.76299995   0.69379997
    1.25098836]
 ...
 [133.71255755   0.78950006   0.0755       0.81000006   0.74990004
    1.51281917]
 [  3.26156655   0.57929999   0.48280001   0.69990009   0.51940006
    0.94172806]
 [ 32.70421797   0.56110001   0.38189998   0.3565       0.46299997
    2.40462255]][0m
[37m[1m[2023-07-11 03:18:13,803][233954] Max Reward on eval: 404.0387115245685[0m
[37m[1m[2023-07-11 03:18:13,803][233954] Min Reward on eval: -514.7639694323764[0m
[37m[1m[2023-07-11 03:18:13,803][233954] Mean Reward across all agents: 29.390784945451475[0m
[37m[1m[2023-07-11 03:18:13,803][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:18:13,822][233954] mean_value=160.59510815370143, max_value=691.2534713793546[0m
[37m[1m[2023-07-11 03:18:13,825][233954] New mean coefficients: [[-0.5171107   2.16859     4.18357    -0.15241635  1.7326396  -1.4756684 ]][0m
[37m[1m[2023-07-11 03:18:13,826][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:18:22,905][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 03:18:22,905][233954] FPS: 423043.67[0m
[36m[2023-07-11 03:18:22,907][233954] itr=260, itrs=2000, Progress: 13.00%[0m
[37m[1m[2023-07-11 03:21:19,994][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000240[0m
[36m[2023-07-11 03:21:32,174][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 03:21:32,180][233954] FPS: 329604.07[0m
[36m[2023-07-11 03:21:36,327][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:21:36,328][233954] Reward + Measures: [[10.39696659  0.94611096  0.97666192  0.01542367  0.966034    0.24016882]][0m
[37m[1m[2023-07-11 03:21:36,328][233954] Max Reward on eval: 10.396966594617275[0m
[37m[1m[2023-07-11 03:21:36,328][233954] Min Reward on eval: 10.396966594617275[0m
[37m[1m[2023-07-11 03:21:36,329][233954] Mean Reward across all agents: 10.396966594617275[0m
[37m[1m[2023-07-11 03:21:36,329][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:21:41,228][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:21:41,229][233954] Reward + Measures: [[   7.18703901    0.54339999    0.83570004    0.19059999    0.78660005
     0.92434675]
 [ -42.91123366    0.50709999    0.74420005    0.1166        0.73549998
     1.27895057]
 [ -21.56766626    0.61790007    0.78450006    0.0608        0.77150005
     0.7506935 ]
 ...
 [  14.03633228    0.90369999    0.94280005    0.84960002    0.91940004
     0.37007841]
 [  40.2338574     0.94060004    0.97039998    0.0204        0.95679998
     0.36511406]
 [-170.1944651     0.35300002    0.54640001    0.22930001    0.48149997
     1.21812975]][0m
[37m[1m[2023-07-11 03:21:41,229][233954] Max Reward on eval: 198.41137118265033[0m
[37m[1m[2023-07-11 03:21:41,230][233954] Min Reward on eval: -341.7156982358545[0m
[37m[1m[2023-07-11 03:21:41,230][233954] Mean Reward across all agents: -11.189927770979963[0m
[37m[1m[2023-07-11 03:21:41,230][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:21:41,240][233954] mean_value=24.44210881631527, max_value=587.27632079926[0m
[37m[1m[2023-07-11 03:21:41,242][233954] New mean coefficients: [[ 0.05052304  2.3236394   4.4133897   0.6701738   1.602282   -1.1881099 ]][0m
[37m[1m[2023-07-11 03:21:41,243][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:21:50,167][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 03:21:50,167][233954] FPS: 430387.10[0m
[36m[2023-07-11 03:21:50,169][233954] itr=261, itrs=2000, Progress: 13.05%[0m
[36m[2023-07-11 03:22:01,845][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 03:22:01,845][233954] FPS: 330678.94[0m
[36m[2023-07-11 03:22:06,074][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:22:06,074][233954] Reward + Measures: [[15.79407606  0.91731328  0.94762164  0.01549067  0.95306772  0.32975549]][0m
[37m[1m[2023-07-11 03:22:06,075][233954] Max Reward on eval: 15.794076062630358[0m
[37m[1m[2023-07-11 03:22:06,075][233954] Min Reward on eval: 15.794076062630358[0m
[37m[1m[2023-07-11 03:22:06,075][233954] Mean Reward across all agents: 15.794076062630358[0m
[37m[1m[2023-07-11 03:22:06,075][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:22:11,038][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:22:11,039][233954] Reward + Measures: [[-44.4858598    0.72830003   0.54530001   0.58420002   0.41190001
    0.35493913]
 [149.17082212   0.55880004   0.81290007   0.05520001   0.86440003
    1.57958758]
 [ -9.52209257   0.53520006   0.20700002   0.59569997   0.41490003
    1.0666213 ]
 ...
 [-43.79323975   0.70329994   0.6882       0.3994       0.72640002
    0.21706657]
 [ 71.03447583   0.76920003   0.84530002   0.0157       0.86469996
    0.98837703]
 [ 22.43106223   0.54480004   0.29730001   0.67309999   0.20580001
    0.69486678]][0m
[37m[1m[2023-07-11 03:22:11,039][233954] Max Reward on eval: 240.11004601307212[0m
[37m[1m[2023-07-11 03:22:11,039][233954] Min Reward on eval: -117.73313852078282[0m
[37m[1m[2023-07-11 03:22:11,039][233954] Mean Reward across all agents: 33.27228363380163[0m
[37m[1m[2023-07-11 03:22:11,040][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:22:11,054][233954] mean_value=89.03422498613828, max_value=666.5095598403137[0m
[37m[1m[2023-07-11 03:22:11,057][233954] New mean coefficients: [[ 0.66934246  2.2539556   4.7994404   1.0884778   1.5430739  -0.87066555]][0m
[37m[1m[2023-07-11 03:22:11,058][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:22:20,064][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 03:22:20,065][233954] FPS: 426439.17[0m
[36m[2023-07-11 03:22:20,067][233954] itr=262, itrs=2000, Progress: 13.10%[0m
[36m[2023-07-11 03:22:31,621][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 03:22:31,621][233954] FPS: 334200.49[0m
[36m[2023-07-11 03:22:35,859][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:22:35,860][233954] Reward + Measures: [[5.83338437 0.95831496 0.98092633 0.01263367 0.97339308 0.18988949]][0m
[37m[1m[2023-07-11 03:22:35,860][233954] Max Reward on eval: 5.833384366426569[0m
[37m[1m[2023-07-11 03:22:35,860][233954] Min Reward on eval: 5.833384366426569[0m
[37m[1m[2023-07-11 03:22:35,860][233954] Mean Reward across all agents: 5.833384366426569[0m
[37m[1m[2023-07-11 03:22:35,861][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:22:40,965][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:22:40,965][233954] Reward + Measures: [[-146.41592267    0.83050007    0.7374        0.57339996    0.31720003
     1.24955809]
 [ -16.05752959    0.49270001    0.78459996    0.13090001    0.83710003
     0.81257421]
 [ -37.18535902    0.55340004    0.49129996    0.51270002    0.50099999
     1.43508554]
 ...
 [  73.86967302    0.45070001    0.41949996    0.66079998    0.52950001
     2.37330556]
 [  18.57623679    0.81220001    0.32080001    0.708         0.27239999
     1.63300169]
 [ -58.33874611    0.69440001    0.78179997    0.65650004    0.2904
     0.84528935]][0m
[37m[1m[2023-07-11 03:22:40,966][233954] Max Reward on eval: 233.94138336093164[0m
[37m[1m[2023-07-11 03:22:40,966][233954] Min Reward on eval: -264.5212268490344[0m
[37m[1m[2023-07-11 03:22:40,966][233954] Mean Reward across all agents: -25.347904959729195[0m
[37m[1m[2023-07-11 03:22:40,966][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:22:40,980][233954] mean_value=118.20362350908799, max_value=594.1124506600202[0m
[37m[1m[2023-07-11 03:22:40,983][233954] New mean coefficients: [[ 0.8800522   2.2454631   4.8353925   1.1475539   1.6063488  -0.40485516]][0m
[37m[1m[2023-07-11 03:22:40,984][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:22:49,978][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 03:22:49,978][233954] FPS: 427008.78[0m
[36m[2023-07-11 03:22:49,981][233954] itr=263, itrs=2000, Progress: 13.15%[0m
[36m[2023-07-11 03:23:01,517][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 03:23:01,517][233954] FPS: 334802.77[0m
[36m[2023-07-11 03:23:05,742][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:23:05,743][233954] Reward + Measures: [[15.43288696  0.949117    0.97867233  0.01183367  0.97358871  0.26156539]][0m
[37m[1m[2023-07-11 03:23:05,743][233954] Max Reward on eval: 15.432886959731851[0m
[37m[1m[2023-07-11 03:23:05,743][233954] Min Reward on eval: 15.432886959731851[0m
[37m[1m[2023-07-11 03:23:05,743][233954] Mean Reward across all agents: 15.432886959731851[0m
[37m[1m[2023-07-11 03:23:05,743][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:23:10,594][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:23:10,595][233954] Reward + Measures: [[-218.60256767    0.49680001    0.65320003    0.31080002    0.53459996
     1.29494703]
 [-170.21670247    0.56750005    0.65109998    0.4612        0.61339998
     1.08921659]
 [  86.30819962    0.71630001    0.82910007    0.0266        0.86610001
     0.9655475 ]
 ...
 [  -0.58568747    0.52460003    0.82070011    0.0582        0.83430004
     0.7460376 ]
 [ -87.79853913    0.74970001    0.61129999    0.60070002    0.44170004
     1.70767844]
 [-130.17000869    0.62309998    0.53800005    0.57489997    0.36830002
     1.3187319 ]][0m
[37m[1m[2023-07-11 03:23:10,595][233954] Max Reward on eval: 137.8585753524676[0m
[37m[1m[2023-07-11 03:23:10,595][233954] Min Reward on eval: -347.44018172612414[0m
[37m[1m[2023-07-11 03:23:10,596][233954] Mean Reward across all agents: -54.498182653156945[0m
[37m[1m[2023-07-11 03:23:10,596][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:23:10,605][233954] mean_value=14.447238509887502, max_value=477.71307800803334[0m
[37m[1m[2023-07-11 03:23:10,607][233954] New mean coefficients: [[1.2023317  2.059988   4.9758224  1.2231705  1.3980227  0.04988214]][0m
[37m[1m[2023-07-11 03:23:10,608][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:23:19,565][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 03:23:19,565][233954] FPS: 428781.97[0m
[36m[2023-07-11 03:23:19,568][233954] itr=264, itrs=2000, Progress: 13.20%[0m
[36m[2023-07-11 03:23:31,093][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 03:23:31,093][233954] FPS: 335099.50[0m
[36m[2023-07-11 03:23:35,423][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:23:35,424][233954] Reward + Measures: [[28.60366294  0.9485153   0.98069757  0.01319533  0.97454798  0.32710245]][0m
[37m[1m[2023-07-11 03:23:35,424][233954] Max Reward on eval: 28.603662941352553[0m
[37m[1m[2023-07-11 03:23:35,424][233954] Min Reward on eval: 28.603662941352553[0m
[37m[1m[2023-07-11 03:23:35,424][233954] Mean Reward across all agents: 28.603662941352553[0m
[37m[1m[2023-07-11 03:23:35,425][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:23:40,413][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:23:40,413][233954] Reward + Measures: [[143.71081259   0.27500001   0.6997       0.70050001   0.93129998
    1.99980342]
 [-93.02103002   0.31020001   0.63920003   0.13880001   0.53289998
    1.87794709]
 [-24.20810255   0.53310007   0.89040005   0.046        0.81969994
    0.49143344]
 ...
 [ 74.18058228   0.5582       0.42279997   0.68730003   0.68879998
    1.51610422]
 [192.19639014   0.12990001   0.74980003   0.40060002   0.80919999
    3.01377749]
 [ 13.9863191    0.81400007   0.15540001   0.87180007   0.62840003
    0.6514228 ]][0m
[37m[1m[2023-07-11 03:23:40,414][233954] Max Reward on eval: 339.43485791864805[0m
[37m[1m[2023-07-11 03:23:40,414][233954] Min Reward on eval: -177.31671427041292[0m
[37m[1m[2023-07-11 03:23:40,414][233954] Mean Reward across all agents: 58.01155896380696[0m
[37m[1m[2023-07-11 03:23:40,414][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:23:40,434][233954] mean_value=234.48987091554145, max_value=839.434857918648[0m
[37m[1m[2023-07-11 03:23:40,437][233954] New mean coefficients: [[1.6452274  2.130263   5.398807   3.2731829  1.2386457  0.30448118]][0m
[37m[1m[2023-07-11 03:23:40,438][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:23:49,310][233954] train() took 8.87 seconds to complete[0m
[36m[2023-07-11 03:23:49,310][233954] FPS: 432887.32[0m
[36m[2023-07-11 03:23:49,312][233954] itr=265, itrs=2000, Progress: 13.25%[0m
[36m[2023-07-11 03:24:01,113][233954] train() took 11.73 seconds to complete[0m
[36m[2023-07-11 03:24:01,113][233954] FPS: 327300.56[0m
[36m[2023-07-11 03:24:05,441][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:24:05,441][233954] Reward + Measures: [[24.73599502  0.93553066  0.97858298  0.017262    0.9691087   0.33859947]][0m
[37m[1m[2023-07-11 03:24:05,442][233954] Max Reward on eval: 24.735995021463047[0m
[37m[1m[2023-07-11 03:24:05,442][233954] Min Reward on eval: 24.735995021463047[0m
[37m[1m[2023-07-11 03:24:05,442][233954] Mean Reward across all agents: 24.735995021463047[0m
[37m[1m[2023-07-11 03:24:05,442][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:24:10,403][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:24:10,403][233954] Reward + Measures: [[-126.75708291    0.6433        0.54879999    0.5869        0.39039999
     1.87180257]
 [  26.89066923    0.90930003    0.97389996    0.0338        0.96730006
     0.44343844]
 [-111.66779469    0.64360005    0.31869999    0.73920006    0.16860001
     1.22805917]
 ...
 [-171.87739451    0.49490005    0.4948        0.30270001    0.42019996
     2.13728952]
 [-167.78019138    0.47410002    0.53360003    0.46470004    0.23309998
     1.69316232]
 [-161.67413524    0.53210002    0.63440001    0.74489999    0.0814
     1.24772537]][0m
[37m[1m[2023-07-11 03:24:10,403][233954] Max Reward on eval: 88.69549937415869[0m
[37m[1m[2023-07-11 03:24:10,404][233954] Min Reward on eval: -427.4215202882886[0m
[37m[1m[2023-07-11 03:24:10,404][233954] Mean Reward across all agents: -116.67857892464139[0m
[37m[1m[2023-07-11 03:24:10,404][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:24:10,415][233954] mean_value=8.798754952642716, max_value=537.5138994657434[0m
[37m[1m[2023-07-11 03:24:10,418][233954] New mean coefficients: [[1.6582104  2.0102122  5.320479   2.4577358  1.3821479  0.13903584]][0m
[37m[1m[2023-07-11 03:24:10,419][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:24:19,449][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 03:24:19,449][233954] FPS: 425334.64[0m
[36m[2023-07-11 03:24:19,451][233954] itr=266, itrs=2000, Progress: 13.30%[0m
[36m[2023-07-11 03:24:31,018][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 03:24:31,018][233954] FPS: 333826.60[0m
[36m[2023-07-11 03:24:35,287][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:24:35,288][233954] Reward + Measures: [[30.27889234  0.93364596  0.9781633   0.01820633  0.97008395  0.39217684]][0m
[37m[1m[2023-07-11 03:24:35,288][233954] Max Reward on eval: 30.27889233555282[0m
[37m[1m[2023-07-11 03:24:35,288][233954] Min Reward on eval: 30.27889233555282[0m
[37m[1m[2023-07-11 03:24:35,288][233954] Mean Reward across all agents: 30.27889233555282[0m
[37m[1m[2023-07-11 03:24:35,289][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:24:40,326][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:24:40,326][233954] Reward + Measures: [[-55.93303223   0.4237       0.57750005   0.56780005   0.30590001
    1.13850439]
 [  2.68571331   0.84500009   0.64710009   0.78039998   0.1251
    1.49236929]
 [ 80.15668465   0.0669       0.84230006   0.68629998   0.6807
    1.34650242]
 ...
 [-84.15847311   0.31360003   0.87390006   0.19230001   0.72079998
    1.10119617]
 [-62.06576378   0.67559999   0.66280001   0.79269999   0.07750001
    1.00153947]
 [-61.14964868   0.9005       0.91619998   0.88450003   0.042
    1.17537391]][0m
[37m[1m[2023-07-11 03:24:40,327][233954] Max Reward on eval: 281.21861623888833[0m
[37m[1m[2023-07-11 03:24:40,327][233954] Min Reward on eval: -205.95555696310475[0m
[37m[1m[2023-07-11 03:24:40,327][233954] Mean Reward across all agents: -4.976297721106346[0m
[37m[1m[2023-07-11 03:24:40,327][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:24:40,340][233954] mean_value=-70.56524575199697, max_value=646.5936139083612[0m
[37m[1m[2023-07-11 03:24:40,343][233954] New mean coefficients: [[ 1.4145429   1.5934696   5.1641316   2.5495164   1.1220435  -0.20839651]][0m
[37m[1m[2023-07-11 03:24:40,344][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:24:49,455][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 03:24:49,455][233954] FPS: 421538.81[0m
[36m[2023-07-11 03:24:49,457][233954] itr=267, itrs=2000, Progress: 13.35%[0m
[36m[2023-07-11 03:25:01,079][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 03:25:01,079][233954] FPS: 332225.01[0m
[36m[2023-07-11 03:25:05,256][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:25:05,257][233954] Reward + Measures: [[55.10774171  0.91675407  0.97565675  0.03106934  0.96198028  0.54114717]][0m
[37m[1m[2023-07-11 03:25:05,257][233954] Max Reward on eval: 55.107741707076514[0m
[37m[1m[2023-07-11 03:25:05,257][233954] Min Reward on eval: 55.107741707076514[0m
[37m[1m[2023-07-11 03:25:05,257][233954] Mean Reward across all agents: 55.107741707076514[0m
[37m[1m[2023-07-11 03:25:05,257][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:25:10,419][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:25:10,419][233954] Reward + Measures: [[ -52.9415992     0.63330001    0.71550006    0.0658        0.7177
     2.15986323]
 [  17.35961224    0.91060001    0.95529997    0.0218        0.96149999
     1.64444888]
 [ -43.92269506    0.73159999    0.79829997    0.0387        0.80470002
     2.02057338]
 ...
 [ -16.44883552    0.39789999    0.28130001    0.52240002    0.34449998
     1.1080488 ]
 [-193.31659701    0.53860003    0.81599998    0.0235        0.93239993
     1.87466991]
 [   2.01004148    0.75279999    0.92220002    0.0343        0.93260002
     1.47179139]][0m
[37m[1m[2023-07-11 03:25:10,419][233954] Max Reward on eval: 163.5433956027031[0m
[37m[1m[2023-07-11 03:25:10,420][233954] Min Reward on eval: -248.68754386082293[0m
[37m[1m[2023-07-11 03:25:10,420][233954] Mean Reward across all agents: -20.20493468535621[0m
[37m[1m[2023-07-11 03:25:10,420][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:25:10,428][233954] mean_value=22.325004722542786, max_value=599.8444034909829[0m
[37m[1m[2023-07-11 03:25:10,430][233954] New mean coefficients: [[ 1.5281667   1.8003219   5.1569653   2.9938214   1.2054062  -0.15480687]][0m
[37m[1m[2023-07-11 03:25:10,432][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:25:19,434][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 03:25:19,435][233954] FPS: 426608.07[0m
[36m[2023-07-11 03:25:19,437][233954] itr=268, itrs=2000, Progress: 13.40%[0m
[36m[2023-07-11 03:25:30,962][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 03:25:30,963][233954] FPS: 335094.19[0m
[36m[2023-07-11 03:25:35,200][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:25:35,200][233954] Reward + Measures: [[59.93080576  0.92102796  0.98007601  0.02926733  0.96528769  0.58450013]][0m
[37m[1m[2023-07-11 03:25:35,200][233954] Max Reward on eval: 59.930805764428065[0m
[37m[1m[2023-07-11 03:25:35,201][233954] Min Reward on eval: 59.930805764428065[0m
[37m[1m[2023-07-11 03:25:35,201][233954] Mean Reward across all agents: 59.930805764428065[0m
[37m[1m[2023-07-11 03:25:35,201][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:25:40,199][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:25:40,200][233954] Reward + Measures: [[ 25.15162898   0.78310007   0.88619995   0.88510001   0.85909998
    1.98632014]
 [ 33.14526499   0.4244       0.64660001   0.6006       0.62760001
    1.55703199]
 [-31.4455168    0.31669998   0.84530002   0.41020003   0.83719999
    0.65390015]
 ...
 [  7.39324438   0.43969998   0.50670004   0.68200004   0.458
    1.57503533]
 [-54.0704367    0.5212       0.74220002   0.24770001   0.83210003
    1.06743848]
 [-33.35574653   0.24700001   0.79729998   0.38609999   0.7888
    1.00691152]][0m
[37m[1m[2023-07-11 03:25:40,200][233954] Max Reward on eval: 184.33624649425036[0m
[37m[1m[2023-07-11 03:25:40,201][233954] Min Reward on eval: -227.21538734845817[0m
[37m[1m[2023-07-11 03:25:40,201][233954] Mean Reward across all agents: -24.254842766865234[0m
[37m[1m[2023-07-11 03:25:40,201][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:25:40,214][233954] mean_value=97.79830281875283, max_value=547.6256546627731[0m
[37m[1m[2023-07-11 03:25:40,217][233954] New mean coefficients: [[1.8439071  1.9007478  6.224116   4.2739425  1.2170972  0.08149624]][0m
[37m[1m[2023-07-11 03:25:40,218][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:25:49,235][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 03:25:49,235][233954] FPS: 425933.46[0m
[36m[2023-07-11 03:25:49,237][233954] itr=269, itrs=2000, Progress: 13.45%[0m
[36m[2023-07-11 03:26:00,959][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 03:26:00,964][233954] FPS: 329443.06[0m
[36m[2023-07-11 03:26:05,236][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:26:05,236][233954] Reward + Measures: [[69.588968    0.87309766  0.97388476  0.05199067  0.96236801  0.70389956]][0m
[37m[1m[2023-07-11 03:26:05,237][233954] Max Reward on eval: 69.5889679970195[0m
[37m[1m[2023-07-11 03:26:05,237][233954] Min Reward on eval: 69.5889679970195[0m
[37m[1m[2023-07-11 03:26:05,237][233954] Mean Reward across all agents: 69.5889679970195[0m
[37m[1m[2023-07-11 03:26:05,237][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:26:10,249][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:26:10,249][233954] Reward + Measures: [[ 20.56088918   0.5776       0.71939999   0.366        0.58939999
    0.8484866 ]
 [ -9.54743735   0.2392       0.37510002   0.22049999   0.37610003
    2.39383507]
 [ -2.12710746   0.1584       0.35390002   0.212        0.29930001
    2.86583829]
 ...
 [-38.09838915   0.0191       0.87890005   0.65039998   0.77790004
    0.70024484]
 [ 51.2926417    0.65990001   0.35789999   0.78470004   0.41840002
    1.76411629]
 [-88.50877824   0.23909998   0.35909998   0.30570003   0.2836
    2.60451174]][0m
[37m[1m[2023-07-11 03:26:10,249][233954] Max Reward on eval: 188.27121292501687[0m
[37m[1m[2023-07-11 03:26:10,250][233954] Min Reward on eval: -185.9795370195061[0m
[37m[1m[2023-07-11 03:26:10,250][233954] Mean Reward across all agents: -8.76991432459703[0m
[37m[1m[2023-07-11 03:26:10,250][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:26:10,262][233954] mean_value=55.82707278650443, max_value=685.6503305299208[0m
[37m[1m[2023-07-11 03:26:10,265][233954] New mean coefficients: [[2.1413207  2.2912107  6.141039   3.1484137  1.668757   0.15727861]][0m
[37m[1m[2023-07-11 03:26:10,266][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:26:19,295][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 03:26:19,295][233954] FPS: 425387.18[0m
[36m[2023-07-11 03:26:19,297][233954] itr=270, itrs=2000, Progress: 13.50%[0m
[37m[1m[2023-07-11 03:29:12,109][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000250[0m
[36m[2023-07-11 03:29:24,314][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 03:29:24,315][233954] FPS: 333783.84[0m
[36m[2023-07-11 03:29:28,600][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:29:28,601][233954] Reward + Measures: [[68.76899615  0.72329706  0.96506566  0.132664    0.96164227  0.7695086 ]][0m
[37m[1m[2023-07-11 03:29:28,601][233954] Max Reward on eval: 68.76899614912338[0m
[37m[1m[2023-07-11 03:29:28,602][233954] Min Reward on eval: 68.76899614912338[0m
[37m[1m[2023-07-11 03:29:28,602][233954] Mean Reward across all agents: 68.76899614912338[0m
[37m[1m[2023-07-11 03:29:28,602][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:29:33,587][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:29:33,588][233954] Reward + Measures: [[-25.55810099   0.3249       0.89320004   0.1112       0.95719999
    1.29942763]
 [  0.02628646   0.24630001   0.94440001   0.35180002   0.95819998
    0.91383803]
 [-77.58101844   0.62660003   0.77140009   0.21519999   0.70349997
    1.12631416]
 ...
 [-48.93409745   0.7525       0.77330011   0.74480003   0.74959999
    1.584144  ]
 [ -3.16713839   0.69400001   0.95780003   0.0876       0.94359988
    0.64442652]
 [-83.6614288    0.73290008   0.84450001   0.15799999   0.77420002
    1.11441505]][0m
[37m[1m[2023-07-11 03:29:33,588][233954] Max Reward on eval: 201.54108643138315[0m
[37m[1m[2023-07-11 03:29:33,588][233954] Min Reward on eval: -213.87561416188254[0m
[37m[1m[2023-07-11 03:29:33,589][233954] Mean Reward across all agents: -67.05585429507815[0m
[37m[1m[2023-07-11 03:29:33,589][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:29:33,599][233954] mean_value=0.3577318418061016, max_value=538.38859797786[0m
[37m[1m[2023-07-11 03:29:33,601][233954] New mean coefficients: [[ 1.6872721   2.061311    5.7269416   0.9765148   1.9312203  -0.34478503]][0m
[37m[1m[2023-07-11 03:29:33,602][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:29:42,435][233954] train() took 8.83 seconds to complete[0m
[36m[2023-07-11 03:29:42,436][233954] FPS: 434808.00[0m
[36m[2023-07-11 03:29:42,438][233954] itr=271, itrs=2000, Progress: 13.55%[0m
[36m[2023-07-11 03:29:54,012][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 03:29:54,012][233954] FPS: 333745.42[0m
[36m[2023-07-11 03:29:58,233][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:29:58,239][233954] Reward + Measures: [[63.12742129  0.37774736  0.96268064  0.46240735  0.9492923   0.71330994]][0m
[37m[1m[2023-07-11 03:29:58,239][233954] Max Reward on eval: 63.127421290952576[0m
[37m[1m[2023-07-11 03:29:58,239][233954] Min Reward on eval: 63.127421290952576[0m
[37m[1m[2023-07-11 03:29:58,240][233954] Mean Reward across all agents: 63.127421290952576[0m
[37m[1m[2023-07-11 03:29:58,240][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:30:03,379][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:30:03,385][233954] Reward + Measures: [[ 9.40153399  0.27779999  0.70910001  0.625       0.58950001  0.80338347]
 [-3.62101194  0.0044      0.81630003  0.6972      0.76910001  0.7486791 ]
 [-0.70113472  0.0513      0.83540004  0.6523      0.80299997  1.21510947]
 ...
 [ 9.26218128  0.53090006  0.85970002  0.75610006  0.60409999  0.85274887]
 [52.63391381  0.0342      0.8021      0.74890006  0.81930012  1.22233832]
 [47.30830696  0.1391      0.89820004  0.74980003  0.84799999  1.33890235]][0m
[37m[1m[2023-07-11 03:30:03,385][233954] Max Reward on eval: 213.4326267141616[0m
[37m[1m[2023-07-11 03:30:03,385][233954] Min Reward on eval: -265.70279693985356[0m
[37m[1m[2023-07-11 03:30:03,386][233954] Mean Reward across all agents: 11.216036098703947[0m
[37m[1m[2023-07-11 03:30:03,386][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:30:03,401][233954] mean_value=170.24332848352338, max_value=588.6924502686351[0m
[37m[1m[2023-07-11 03:30:03,404][233954] New mean coefficients: [[ 1.8684136  1.718555   5.7893815  1.320198   1.6864328 -0.5498093]][0m
[37m[1m[2023-07-11 03:30:03,405][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:30:12,451][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 03:30:12,451][233954] FPS: 424568.54[0m
[36m[2023-07-11 03:30:12,454][233954] itr=272, itrs=2000, Progress: 13.60%[0m
[36m[2023-07-11 03:30:24,015][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 03:30:24,015][233954] FPS: 334033.00[0m
[36m[2023-07-11 03:30:28,320][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:30:28,325][233954] Reward + Measures: [[70.40212869  0.46075633  0.97297865  0.42039862  0.96509302  0.72081739]][0m
[37m[1m[2023-07-11 03:30:28,325][233954] Max Reward on eval: 70.4021286878332[0m
[37m[1m[2023-07-11 03:30:28,326][233954] Min Reward on eval: 70.4021286878332[0m
[37m[1m[2023-07-11 03:30:28,326][233954] Mean Reward across all agents: 70.4021286878332[0m
[37m[1m[2023-07-11 03:30:28,326][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:30:33,315][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:30:33,316][233954] Reward + Measures: [[111.93519019   0.51120001   0.54949999   0.43780002   0.74850005
    1.90534437]
 [100.62061912   0.24730001   0.61770004   0.72559994   0.84310001
    1.32866907]
 [204.82568073   0.54650003   0.38750002   0.73460007   0.79860002
    2.16691637]
 ...
 [-64.07478671   0.10930001   0.92840004   0.24749999   0.9235
    0.8147499 ]
 [ 85.96245961   0.65200001   0.52160001   0.58700007   0.87760001
    1.59675348]
 [ 70.47875979   0.70530003   0.87330002   0.0977       0.8950001
    1.07103789]][0m
[37m[1m[2023-07-11 03:30:33,316][233954] Max Reward on eval: 403.39427568130196[0m
[37m[1m[2023-07-11 03:30:33,316][233954] Min Reward on eval: -179.9473464431241[0m
[37m[1m[2023-07-11 03:30:33,316][233954] Mean Reward across all agents: 57.089829946502725[0m
[37m[1m[2023-07-11 03:30:33,317][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:30:33,346][233954] mean_value=365.96499307186855, max_value=757.2769952048209[0m
[37m[1m[2023-07-11 03:30:33,349][233954] New mean coefficients: [[ 2.0156326   2.2790856   5.8162417   2.343553    1.8657192  -0.48705786]][0m
[37m[1m[2023-07-11 03:30:33,350][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:30:42,351][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 03:30:42,351][233954] FPS: 426694.62[0m
[36m[2023-07-11 03:30:42,354][233954] itr=273, itrs=2000, Progress: 13.65%[0m
[36m[2023-07-11 03:30:54,084][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 03:30:54,085][233954] FPS: 329219.40[0m
[36m[2023-07-11 03:30:58,365][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:30:58,370][233954] Reward + Measures: [[55.60932474  0.17131533  0.97723365  0.71421468  0.969275    0.73292893]][0m
[37m[1m[2023-07-11 03:30:58,370][233954] Max Reward on eval: 55.60932473912404[0m
[37m[1m[2023-07-11 03:30:58,371][233954] Min Reward on eval: 55.60932473912404[0m
[37m[1m[2023-07-11 03:30:58,371][233954] Mean Reward across all agents: 55.60932473912404[0m
[37m[1m[2023-07-11 03:30:58,371][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:31:03,350][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:31:03,351][233954] Reward + Measures: [[-14.08375425   0.36970001   0.64530003   0.46300003   0.53420007
    0.72183555]
 [-40.44081179   0.52500004   0.84389991   0.13770001   0.8901
    1.05679333]
 [ 18.66021519   0.63670003   0.88169998   0.0848       0.87200004
    0.55248553]
 ...
 [ 68.20288205   0.48090002   0.35520002   0.45520002   0.33320004
    1.32023513]
 [ 25.61930711   0.62         0.44600001   0.57109994   0.35870001
    0.9083752 ]
 [ 37.30489946   0.73379999   0.67670006   0.6322       0.40470001
    0.9533909 ]][0m
[37m[1m[2023-07-11 03:31:03,351][233954] Max Reward on eval: 290.5091400353238[0m
[37m[1m[2023-07-11 03:31:03,352][233954] Min Reward on eval: -184.51280784755946[0m
[37m[1m[2023-07-11 03:31:03,352][233954] Mean Reward across all agents: -24.12025417404213[0m
[37m[1m[2023-07-11 03:31:03,352][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:31:03,366][233954] mean_value=110.67828351744421, max_value=579.5381104752421[0m
[37m[1m[2023-07-11 03:31:03,374][233954] New mean coefficients: [[ 1.8594561  2.2035427  5.5165005  2.2164276  1.7618847 -0.7840051]][0m
[37m[1m[2023-07-11 03:31:03,374][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:31:12,305][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 03:31:12,306][233954] FPS: 430045.96[0m
[36m[2023-07-11 03:31:12,308][233954] itr=274, itrs=2000, Progress: 13.70%[0m
[36m[2023-07-11 03:31:24,017][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 03:31:24,017][233954] FPS: 329908.95[0m
[36m[2023-07-11 03:31:28,245][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:31:28,245][233954] Reward + Measures: [[28.55288133  0.02335833  0.98689061  0.91528863  0.98184896  0.70875633]][0m
[37m[1m[2023-07-11 03:31:28,246][233954] Max Reward on eval: 28.55288133047389[0m
[37m[1m[2023-07-11 03:31:28,246][233954] Min Reward on eval: 28.55288133047389[0m
[37m[1m[2023-07-11 03:31:28,246][233954] Mean Reward across all agents: 28.55288133047389[0m
[37m[1m[2023-07-11 03:31:28,246][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:31:33,203][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:31:33,204][233954] Reward + Measures: [[-153.99852563    0.76599997    0.75419998    0.10570001    0.7349
     1.3639468 ]
 [  27.24943638    0.61410004    0.31420001    0.63420004    0.59560001
     0.61773366]
 [ -51.50334687    0.78459996    0.42670003    0.48370001    0.75159997
     0.90217268]
 ...
 [  25.78866966    0.77930003    0.34760001    0.64410007    0.2033
     0.66559631]
 [ -85.68130156    0.45989999    0.5438        0.70209998    0.583
     0.80725157]
 [ -74.60493825    0.91070002    0.89840001    0.59000003    0.91880006
     1.89277542]][0m
[37m[1m[2023-07-11 03:31:33,204][233954] Max Reward on eval: 220.68349934220313[0m
[37m[1m[2023-07-11 03:31:33,204][233954] Min Reward on eval: -275.8830728888512[0m
[37m[1m[2023-07-11 03:31:33,204][233954] Mean Reward across all agents: -12.623668171787973[0m
[37m[1m[2023-07-11 03:31:33,205][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:31:33,222][233954] mean_value=162.4592246877199, max_value=720.6834993422032[0m
[37m[1m[2023-07-11 03:31:33,224][233954] New mean coefficients: [[ 1.6131381  2.065022   5.4244013  1.9250294  1.7664294 -1.1093731]][0m
[37m[1m[2023-07-11 03:31:33,225][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:31:42,158][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 03:31:42,158][233954] FPS: 429964.38[0m
[36m[2023-07-11 03:31:42,161][233954] itr=275, itrs=2000, Progress: 13.75%[0m
[36m[2023-07-11 03:31:53,908][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 03:31:53,908][233954] FPS: 328843.91[0m
[36m[2023-07-11 03:31:58,184][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:31:58,185][233954] Reward + Measures: [[40.43839288  0.435507    0.93999428  0.45312101  0.95331299  0.71249074]][0m
[37m[1m[2023-07-11 03:31:58,185][233954] Max Reward on eval: 40.43839287921467[0m
[37m[1m[2023-07-11 03:31:58,185][233954] Min Reward on eval: 40.43839287921467[0m
[37m[1m[2023-07-11 03:31:58,186][233954] Mean Reward across all agents: 40.43839287921467[0m
[37m[1m[2023-07-11 03:31:58,186][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:32:03,153][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:32:03,153][233954] Reward + Measures: [[ 27.16448832   0.34189999   0.5934       0.20640002   0.58339995
    1.18208909]
 [ 15.83296343   0.48730001   0.50780004   0.4725       0.60969996
    1.33919144]
 [-50.39518094   0.59009999   0.50440001   0.6825       0.72010005
    1.72221649]
 ...
 [ 30.83777508   0.28740001   0.32730001   0.48800001   0.42799997
    2.24616051]
 [-12.88184446   0.88380003   0.95640004   0.90879995   0.9436
    1.5625906 ]
 [105.86054801   0.28199998   0.75040001   0.33780003   0.71500003
    0.81298554]][0m
[37m[1m[2023-07-11 03:32:03,154][233954] Max Reward on eval: 259.92639155611397[0m
[37m[1m[2023-07-11 03:32:03,154][233954] Min Reward on eval: -168.17953014584492[0m
[37m[1m[2023-07-11 03:32:03,154][233954] Mean Reward across all agents: 24.271357613033167[0m
[37m[1m[2023-07-11 03:32:03,154][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:32:03,170][233954] mean_value=56.3791897137295, max_value=576.8705805305088[0m
[37m[1m[2023-07-11 03:32:03,172][233954] New mean coefficients: [[ 1.0977522  1.9269309  4.7893124  2.3849106  1.6561954 -0.9855967]][0m
[37m[1m[2023-07-11 03:32:03,173][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:32:12,192][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 03:32:12,193][233954] FPS: 425851.40[0m
[36m[2023-07-11 03:32:12,195][233954] itr=276, itrs=2000, Progress: 13.80%[0m
[36m[2023-07-11 03:32:23,926][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 03:32:23,926][233954] FPS: 329210.82[0m
[36m[2023-07-11 03:32:28,225][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:32:28,226][233954] Reward + Measures: [[-51.9707485    0.60057396   0.83874297   0.24673834   0.89835197
    0.51606512]][0m
[37m[1m[2023-07-11 03:32:28,226][233954] Max Reward on eval: -51.970748496950975[0m
[37m[1m[2023-07-11 03:32:28,226][233954] Min Reward on eval: -51.970748496950975[0m
[37m[1m[2023-07-11 03:32:28,227][233954] Mean Reward across all agents: -51.970748496950975[0m
[37m[1m[2023-07-11 03:32:28,227][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:32:33,382][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:32:33,388][233954] Reward + Measures: [[ 96.068563     0.56529999   0.54329997   0.64670002   0.55260003
    0.81733602]
 [ 84.64614223   0.56319994   0.49540001   0.6433       0.46409997
    1.46696186]
 [-40.60931286   0.69700003   0.70399994   0.51419997   0.74780005
    0.60027248]
 ...
 [ 39.28225006   0.49489999   0.78740001   0.34810001   0.7094
    1.1032871 ]
 [ 19.24235758   0.79390001   0.77100003   0.74020004   0.71829998
    0.47802171]
 [  6.85120196   0.7274       0.73480004   0.71660006   0.7105
    1.10725176]][0m
[37m[1m[2023-07-11 03:32:33,388][233954] Max Reward on eval: 160.14846142970492[0m
[37m[1m[2023-07-11 03:32:33,388][233954] Min Reward on eval: -326.9436092365533[0m
[37m[1m[2023-07-11 03:32:33,388][233954] Mean Reward across all agents: 8.633155355864004[0m
[37m[1m[2023-07-11 03:32:33,389][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:32:33,404][233954] mean_value=120.0528320297905, max_value=574.9803256989643[0m
[37m[1m[2023-07-11 03:32:33,406][233954] New mean coefficients: [[ 1.4867129   1.6142355   4.983379    2.8346136   1.6251589  -0.46322918]][0m
[37m[1m[2023-07-11 03:32:33,407][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:32:42,514][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 03:32:42,515][233954] FPS: 421721.18[0m
[36m[2023-07-11 03:32:42,517][233954] itr=277, itrs=2000, Progress: 13.85%[0m
[36m[2023-07-11 03:32:54,208][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 03:32:54,208][233954] FPS: 330467.57[0m
[36m[2023-07-11 03:32:58,522][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:32:58,523][233954] Reward + Measures: [[-228.15251927    0.96330696    0.99613535    0.00238567    0.96993065
     1.48656571]][0m
[37m[1m[2023-07-11 03:32:58,523][233954] Max Reward on eval: -228.15251927362786[0m
[37m[1m[2023-07-11 03:32:58,523][233954] Min Reward on eval: -228.15251927362786[0m
[37m[1m[2023-07-11 03:32:58,523][233954] Mean Reward across all agents: -228.15251927362786[0m
[37m[1m[2023-07-11 03:32:58,524][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:33:03,523][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:33:03,528][233954] Reward + Measures: [[ -65.25264748    0.29210001    0.56379998    0.28889999    0.64950001
     1.93295026]
 [  27.00128505    0.67079997    0.89370006    0.0722        0.87460005
     0.53639162]
 [ 171.47280979    0.            0.97459996    0.9702999     0.9544
     1.52678967]
 ...
 [-132.83852674    0.0264        0.77899998    0.75640005    0.83070004
     2.3967042 ]
 [  14.53827333    0.0052        0.97770005    0.89779997    0.96730006
     1.90698242]
 [ -69.43539346    0.0017        0.98110008    0.95039999    0.98920006
     2.69860554]][0m
[37m[1m[2023-07-11 03:33:03,529][233954] Max Reward on eval: 491.26974106244745[0m
[37m[1m[2023-07-11 03:33:03,529][233954] Min Reward on eval: -523.0918102376163[0m
[37m[1m[2023-07-11 03:33:03,529][233954] Mean Reward across all agents: 67.71900566674024[0m
[37m[1m[2023-07-11 03:33:03,529][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:33:03,540][233954] mean_value=208.36505815552988, max_value=730.0642520800939[0m
[37m[1m[2023-07-11 03:33:03,543][233954] New mean coefficients: [[ 1.9819118  1.6104717  6.101675   2.9420936  1.2225137 -0.5878548]][0m
[37m[1m[2023-07-11 03:33:03,544][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:33:12,598][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 03:33:12,598][233954] FPS: 424192.93[0m
[36m[2023-07-11 03:33:12,600][233954] itr=278, itrs=2000, Progress: 13.90%[0m
[36m[2023-07-11 03:33:24,280][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 03:33:24,280][233954] FPS: 330809.74[0m
[36m[2023-07-11 03:33:28,627][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:33:28,628][233954] Reward + Measures: [[-151.35667303    0.63682699    0.976044      0.06885701    0.98145467
     1.75916505]][0m
[37m[1m[2023-07-11 03:33:28,628][233954] Max Reward on eval: -151.35667302589593[0m
[37m[1m[2023-07-11 03:33:28,628][233954] Min Reward on eval: -151.35667302589593[0m
[37m[1m[2023-07-11 03:33:28,629][233954] Mean Reward across all agents: -151.35667302589593[0m
[37m[1m[2023-07-11 03:33:28,629][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:33:33,629][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:33:33,630][233954] Reward + Measures: [[ -86.63004205    0.49659997    0.91189998    0.0359        0.87849998
     1.58781135]
 [-162.98908759    0.49110004    0.97000009    0.11070001    0.97250003
     2.22208977]
 [-186.30527819    0.67259997    0.98159999    0.0089        0.97270006
     2.26901484]
 ...
 [-220.11548996    0.74270004    0.99209994    0.0042        0.98859996
     2.44415998]
 [-167.33643625    0.61720002    0.98870003    0.0074        0.98239994
     1.703722  ]
 [-107.72644901    0.39880002    0.94140005    0.13689999    0.95599997
     2.29143572]][0m
[37m[1m[2023-07-11 03:33:33,630][233954] Max Reward on eval: 84.20793952613603[0m
[37m[1m[2023-07-11 03:33:33,630][233954] Min Reward on eval: -312.9251575172646[0m
[37m[1m[2023-07-11 03:33:33,631][233954] Mean Reward across all agents: -148.9428625927442[0m
[37m[1m[2023-07-11 03:33:33,631][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:33:33,638][233954] mean_value=22.353387807732336, max_value=529.9125119562959[0m
[37m[1m[2023-07-11 03:33:33,641][233954] New mean coefficients: [[ 2.2359598   0.6874717   6.1620827   3.3552132   0.35951227 -0.47731936]][0m
[37m[1m[2023-07-11 03:33:33,642][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:33:42,756][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 03:33:42,756][233954] FPS: 421385.34[0m
[36m[2023-07-11 03:33:42,759][233954] itr=279, itrs=2000, Progress: 13.95%[0m
[36m[2023-07-11 03:33:54,533][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 03:33:54,533][233954] FPS: 328115.59[0m
[36m[2023-07-11 03:33:58,791][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:33:58,792][233954] Reward + Measures: [[39.08679623  0.54374766  0.84122235  0.44235766  0.84689534  0.85141355]][0m
[37m[1m[2023-07-11 03:33:58,792][233954] Max Reward on eval: 39.08679623261151[0m
[37m[1m[2023-07-11 03:33:58,792][233954] Min Reward on eval: 39.08679623261151[0m
[37m[1m[2023-07-11 03:33:58,792][233954] Mean Reward across all agents: 39.08679623261151[0m
[37m[1m[2023-07-11 03:33:58,793][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:34:03,817][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:34:03,823][233954] Reward + Measures: [[ -7.20148781   0.62630004   0.93330002   0.8488       0.91839999
    1.29968202]
 [ 16.62109413   0.1998       0.96359998   0.79260004   0.96740001
    1.3761946 ]
 [ 27.04571244   0.77290004   0.92039996   0.74680001   0.83270007
    0.90384102]
 ...
 [  5.6455771    0.3436       0.96710008   0.85420001   0.95319998
    1.47256172]
 [-22.89935744   0.34660003   0.86409998   0.26830003   0.80439997
    0.63503659]
 [ 29.57442929   0.6663       0.92369998   0.76070005   0.9271
    0.90620691]][0m
[37m[1m[2023-07-11 03:34:03,823][233954] Max Reward on eval: 249.6210479696747[0m
[37m[1m[2023-07-11 03:34:03,823][233954] Min Reward on eval: -60.062695981282744[0m
[37m[1m[2023-07-11 03:34:03,824][233954] Mean Reward across all agents: 16.152228419159265[0m
[37m[1m[2023-07-11 03:34:03,824][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:34:03,838][233954] mean_value=245.18037985560503, max_value=530.1105618877336[0m
[37m[1m[2023-07-11 03:34:03,841][233954] New mean coefficients: [[ 2.4951854   1.6123865   6.507018    3.8238251   1.0750039  -0.07513177]][0m
[37m[1m[2023-07-11 03:34:03,842][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:34:12,915][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 03:34:12,915][233954] FPS: 423284.50[0m
[36m[2023-07-11 03:34:12,918][233954] itr=280, itrs=2000, Progress: 14.00%[0m
[37m[1m[2023-07-11 03:37:20,580][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000260[0m
[36m[2023-07-11 03:37:33,002][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 03:37:33,002][233954] FPS: 329673.53[0m
[36m[2023-07-11 03:37:37,144][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:37:37,144][233954] Reward + Measures: [[9.70967945 0.32136601 0.84530973 0.42739102 0.93158871 0.94525522]][0m
[37m[1m[2023-07-11 03:37:37,144][233954] Max Reward on eval: 9.70967944607828[0m
[37m[1m[2023-07-11 03:37:37,145][233954] Min Reward on eval: 9.70967944607828[0m
[37m[1m[2023-07-11 03:37:37,145][233954] Mean Reward across all agents: 9.70967944607828[0m
[37m[1m[2023-07-11 03:37:37,145][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:37:42,083][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:37:42,083][233954] Reward + Measures: [[216.6624758    0.66650003   0.51160002   0.48199996   0.7768001
    1.42027378]
 [ 23.80768406   0.4941       0.31350002   0.55800003   0.68120003
    1.52984631]
 [195.36988963   0.65609998   0.22320001   0.63330001   0.65280002
    2.06976891]
 ...
 [194.5073738    0.76849997   0.0958       0.84219998   0.65000004
    1.76117384]
 [148.07212435   0.54470003   0.27270004   0.57139999   0.60530007
    1.81992936]
 [ 54.92793246   0.63609999   0.51610005   0.47589999   0.80380005
    1.26374328]][0m
[37m[1m[2023-07-11 03:37:42,084][233954] Max Reward on eval: 300.6658348850906[0m
[37m[1m[2023-07-11 03:37:42,084][233954] Min Reward on eval: -155.82759854253382[0m
[37m[1m[2023-07-11 03:37:42,084][233954] Mean Reward across all agents: 100.82696565618872[0m
[37m[1m[2023-07-11 03:37:42,084][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:37:42,103][233954] mean_value=342.4482729030373, max_value=756.569913466014[0m
[37m[1m[2023-07-11 03:37:42,106][233954] New mean coefficients: [[ 2.8530416   1.3810215   6.6480026   3.9667485   0.6335604  -0.04480761]][0m
[37m[1m[2023-07-11 03:37:42,107][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:37:50,968][233954] train() took 8.86 seconds to complete[0m
[36m[2023-07-11 03:37:50,968][233954] FPS: 433443.32[0m
[36m[2023-07-11 03:37:50,970][233954] itr=281, itrs=2000, Progress: 14.05%[0m
[36m[2023-07-11 03:38:02,454][233954] train() took 11.42 seconds to complete[0m
[36m[2023-07-11 03:38:02,454][233954] FPS: 336357.02[0m
[36m[2023-07-11 03:38:06,706][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:38:06,706][233954] Reward + Measures: [[51.40792269  0.28130767  0.80708838  0.58467495  0.89558697  1.04929829]][0m
[37m[1m[2023-07-11 03:38:06,706][233954] Max Reward on eval: 51.407922691719016[0m
[37m[1m[2023-07-11 03:38:06,707][233954] Min Reward on eval: 51.407922691719016[0m
[37m[1m[2023-07-11 03:38:06,707][233954] Mean Reward across all agents: 51.407922691719016[0m
[37m[1m[2023-07-11 03:38:06,707][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:38:11,565][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:38:11,566][233954] Reward + Measures: [[-42.76147339   0.29000002   0.62160003   0.27740002   0.58200002
    1.26553142]
 [ 99.44331649   0.55229998   0.42840001   0.81810009   0.79250002
    1.52870214]
 [150.2857499    0.36140001   0.57930005   0.55159998   0.68760002
    2.04823613]
 ...
 [165.17868944   0.36280003   0.66950005   0.60339999   0.85709995
    1.15070856]
 [183.27613543   0.25229999   0.70450002   0.61570001   0.79079998
    1.8133539 ]
 [ 91.84406023   0.59280008   0.37200001   0.84569997   0.89169997
    1.82137048]][0m
[37m[1m[2023-07-11 03:38:11,566][233954] Max Reward on eval: 552.1374130062759[0m
[37m[1m[2023-07-11 03:38:11,566][233954] Min Reward on eval: -201.86750126052647[0m
[37m[1m[2023-07-11 03:38:11,566][233954] Mean Reward across all agents: 53.13426306730785[0m
[37m[1m[2023-07-11 03:38:11,567][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:38:11,580][233954] mean_value=109.98700801345046, max_value=1052.137413006276[0m
[37m[1m[2023-07-11 03:38:11,582][233954] New mean coefficients: [[ 2.6350856   0.72163796  5.7729177   4.8665414   0.00482535 -1.0604033 ]][0m
[37m[1m[2023-07-11 03:38:11,583][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:38:20,487][233954] train() took 8.90 seconds to complete[0m
[36m[2023-07-11 03:38:20,488][233954] FPS: 431339.17[0m
[36m[2023-07-11 03:38:20,490][233954] itr=282, itrs=2000, Progress: 14.10%[0m
[36m[2023-07-11 03:38:32,001][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 03:38:32,001][233954] FPS: 335549.39[0m
[36m[2023-07-11 03:38:36,246][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:38:36,246][233954] Reward + Measures: [[124.41318468   0.87833095   0.97326368   0.69185972   0.97586793
    2.00304365]][0m
[37m[1m[2023-07-11 03:38:36,247][233954] Max Reward on eval: 124.41318467554578[0m
[37m[1m[2023-07-11 03:38:36,247][233954] Min Reward on eval: 124.41318467554578[0m
[37m[1m[2023-07-11 03:38:36,247][233954] Mean Reward across all agents: 124.41318467554578[0m
[37m[1m[2023-07-11 03:38:36,247][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:38:41,236][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:38:41,236][233954] Reward + Measures: [[ -13.80309007    0.67620003    0.61079997    0.70230001    0.73949999
     1.32646823]
 [ -44.13861416    0.94500011    0.26140001    0.98949999    0.89469999
     1.80843389]
 [ -33.61182592    0.55070001    0.76099998    0.57030004    0.83630002
     1.40703976]
 ...
 [  11.67906356    0.96900004    0.97719997    0.98570007    0.98569995
     1.76872945]
 [ -69.48170233    0.95720005    0.0049        0.99250001    0.92130005
     2.12311912]
 [-110.08161927    0.54589999    0.76920003    0.2208        0.77180004
     1.62775171]][0m
[37m[1m[2023-07-11 03:38:41,237][233954] Max Reward on eval: 153.40338181592523[0m
[37m[1m[2023-07-11 03:38:41,237][233954] Min Reward on eval: -354.7797417799011[0m
[37m[1m[2023-07-11 03:38:41,237][233954] Mean Reward across all agents: -58.009965139170845[0m
[37m[1m[2023-07-11 03:38:41,237][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:38:41,251][233954] mean_value=124.3777233837207, max_value=501.46099048685284[0m
[37m[1m[2023-07-11 03:38:41,259][233954] New mean coefficients: [[ 2.3054264   0.8417947   5.408036    4.4668026   0.24584162 -0.9074235 ]][0m
[37m[1m[2023-07-11 03:38:41,260][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:38:50,328][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 03:38:50,329][233954] FPS: 423519.12[0m
[36m[2023-07-11 03:38:50,331][233954] itr=283, itrs=2000, Progress: 14.15%[0m
[36m[2023-07-11 03:39:02,077][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 03:39:02,077][233954] FPS: 328940.65[0m
[36m[2023-07-11 03:39:06,412][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:39:06,413][233954] Reward + Measures: [[-101.33998043    0.922014      0.88075203    0.44128799    0.89688265
     1.32879508]][0m
[37m[1m[2023-07-11 03:39:06,413][233954] Max Reward on eval: -101.33998042834459[0m
[37m[1m[2023-07-11 03:39:06,413][233954] Min Reward on eval: -101.33998042834459[0m
[37m[1m[2023-07-11 03:39:06,413][233954] Mean Reward across all agents: -101.33998042834459[0m
[37m[1m[2023-07-11 03:39:06,414][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:39:11,400][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:39:11,401][233954] Reward + Measures: [[-110.4148617     0.80219996    0.62650001    0.47730002    0.81220001
     1.37026966]
 [  12.70748016    0.87530005    0.35320002    0.85150003    0.44390002
     1.16843498]
 [-134.15671061    0.95009995    0.83080006    0.85529995    0.93740004
     1.28745544]
 ...
 [ -87.69314528    0.85260004    0.79910004    0.79899997    0.8858
     1.33938503]
 [  -3.91170786    0.94059992    0.90550005    0.92329997    0.94369996
     1.39337528]
 [  18.27516025    0.96899998    0.97230005    0.96390003    0.98110002
     1.55622256]][0m
[37m[1m[2023-07-11 03:39:11,401][233954] Max Reward on eval: 140.85763263403206[0m
[37m[1m[2023-07-11 03:39:11,401][233954] Min Reward on eval: -201.51466369822157[0m
[37m[1m[2023-07-11 03:39:11,402][233954] Mean Reward across all agents: -52.93109252985956[0m
[37m[1m[2023-07-11 03:39:11,402][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:39:11,415][233954] mean_value=152.19691379090918, max_value=567.9058650615392[0m
[37m[1m[2023-07-11 03:39:11,418][233954] New mean coefficients: [[ 2.424676    1.2039433   6.323342    5.166654    0.29053482 -1.0549378 ]][0m
[37m[1m[2023-07-11 03:39:11,419][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:39:20,345][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 03:39:20,345][233954] FPS: 430294.99[0m
[36m[2023-07-11 03:39:20,347][233954] itr=284, itrs=2000, Progress: 14.20%[0m
[36m[2023-07-11 03:39:31,912][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 03:39:31,912][233954] FPS: 334022.08[0m
[36m[2023-07-11 03:39:36,200][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:39:36,201][233954] Reward + Measures: [[-274.41540189    0.51761264    0.997603      0.01427933    0.99639964
     2.61989141]][0m
[37m[1m[2023-07-11 03:39:36,201][233954] Max Reward on eval: -274.4154018919638[0m
[37m[1m[2023-07-11 03:39:36,201][233954] Min Reward on eval: -274.4154018919638[0m
[37m[1m[2023-07-11 03:39:36,202][233954] Mean Reward across all agents: -274.4154018919638[0m
[37m[1m[2023-07-11 03:39:36,202][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:39:41,120][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:39:41,121][233954] Reward + Measures: [[-179.74240279    0.86539996    0.90280002    0.77469999    0.90459996
     3.35198903]
 [-241.43604278    0.57380003    0.88450003    0.37989998    0.96029997
     3.09016562]
 [-120.22270728    0.90640002    0.92379999    0.85570002    0.93180001
     2.97610545]
 ...
 [-102.32303764    0.83850002    0.88070005    0.57059997    0.86949998
     2.77326441]
 [-167.02433252    0.95809996    0.94250005    0.88940001    0.95069999
     3.19332266]
 [  52.57891876    0.91239995    0.92010003    0.66750002    0.93330002
     3.71209121]][0m
[37m[1m[2023-07-11 03:39:41,121][233954] Max Reward on eval: 244.0519944450818[0m
[37m[1m[2023-07-11 03:39:41,121][233954] Min Reward on eval: -478.1342563344166[0m
[37m[1m[2023-07-11 03:39:41,121][233954] Mean Reward across all agents: -81.67084361721408[0m
[37m[1m[2023-07-11 03:39:41,122][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:39:41,137][233954] mean_value=202.94267941302311, max_value=571.3088174739503[0m
[37m[1m[2023-07-11 03:39:41,140][233954] New mean coefficients: [[ 2.3824582   1.6951036   6.2835817   4.7655773   0.61960876 -1.1168698 ]][0m
[37m[1m[2023-07-11 03:39:41,141][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:39:50,119][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 03:39:50,120][233954] FPS: 427755.97[0m
[36m[2023-07-11 03:39:50,122][233954] itr=285, itrs=2000, Progress: 14.25%[0m
[36m[2023-07-11 03:40:01,857][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 03:40:01,858][233954] FPS: 329130.10[0m
[36m[2023-07-11 03:40:06,097][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:40:06,098][233954] Reward + Measures: [[-18.95574872   0.8218587    0.29119733   0.92198902   0.27632532
    1.8346082 ]][0m
[37m[1m[2023-07-11 03:40:06,098][233954] Max Reward on eval: -18.95574871881729[0m
[37m[1m[2023-07-11 03:40:06,098][233954] Min Reward on eval: -18.95574871881729[0m
[37m[1m[2023-07-11 03:40:06,098][233954] Mean Reward across all agents: -18.95574871881729[0m
[37m[1m[2023-07-11 03:40:06,099][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:40:11,071][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:40:11,072][233954] Reward + Measures: [[-147.21139144    0.67379999    0.17830001    0.52780002    0.71270001
     2.06125307]
 [-152.01680376    0.74119997    0.08990001    0.72530001    0.78479999
     1.92098773]
 [  85.43086612    0.78870004    0.32119998    0.54650003    0.87620002
     2.17218733]
 ...
 [  25.69630389    0.39210001    0.56740004    0.1349        0.57129997
     2.62918591]
 [-192.47210406    0.76740003    0.09190001    0.70300001    0.7748
     2.11781812]
 [-102.96034933    0.70090002    0.34960002    0.5399        0.8120001
     1.78005278]][0m
[37m[1m[2023-07-11 03:40:11,072][233954] Max Reward on eval: 404.72752761878075[0m
[37m[1m[2023-07-11 03:40:11,073][233954] Min Reward on eval: -384.99349590973[0m
[37m[1m[2023-07-11 03:40:11,073][233954] Mean Reward across all agents: -85.4996486170971[0m
[37m[1m[2023-07-11 03:40:11,073][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:40:11,090][233954] mean_value=160.47487555252943, max_value=766.8477029687026[0m
[37m[1m[2023-07-11 03:40:11,093][233954] New mean coefficients: [[ 2.6151826  2.1860125  7.0172906  4.751104   0.4071516 -1.144162 ]][0m
[37m[1m[2023-07-11 03:40:11,094][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:40:20,204][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 03:40:20,205][233954] FPS: 421562.03[0m
[36m[2023-07-11 03:40:20,207][233954] itr=286, itrs=2000, Progress: 14.30%[0m
[36m[2023-07-11 03:40:31,819][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 03:40:31,819][233954] FPS: 332771.46[0m
[36m[2023-07-11 03:40:36,045][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:40:36,045][233954] Reward + Measures: [[-19.2448433    0.15935333   0.82856739   0.62639105   0.81428564
    1.14476728]][0m
[37m[1m[2023-07-11 03:40:36,045][233954] Max Reward on eval: -19.244843304320966[0m
[37m[1m[2023-07-11 03:40:36,046][233954] Min Reward on eval: -19.244843304320966[0m
[37m[1m[2023-07-11 03:40:36,046][233954] Mean Reward across all agents: -19.244843304320966[0m
[37m[1m[2023-07-11 03:40:36,046][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:40:41,190][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:40:41,195][233954] Reward + Measures: [[ -54.09530502    0.42929998    0.75819999    0.61210006    0.57460004
     0.73882997]
 [ -63.13637543    0.35520002    0.71759999    0.59189999    0.57069999
     0.76185083]
 [-105.04306792    0.40860006    0.48260003    0.67470002    0.41770002
     0.77508658]
 ...
 [-101.70916604    0.26350003    0.44190001    0.53640002    0.43070003
     1.2655077 ]
 [ -63.52994349    0.38590002    0.7432        0.65539998    0.58849996
     0.74293762]
 [ -44.47778213    0.36710003    0.64090002    0.56770003    0.3897
     0.92926371]][0m
[37m[1m[2023-07-11 03:40:41,195][233954] Max Reward on eval: 252.99937247559427[0m
[37m[1m[2023-07-11 03:40:41,196][233954] Min Reward on eval: -165.013965600729[0m
[37m[1m[2023-07-11 03:40:41,196][233954] Mean Reward across all agents: -47.35761900563468[0m
[37m[1m[2023-07-11 03:40:41,196][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:40:41,203][233954] mean_value=19.440708700835156, max_value=704.6082519612648[0m
[37m[1m[2023-07-11 03:40:41,206][233954] New mean coefficients: [[ 3.3920226  2.5860069  7.3451524  4.1995792  0.7384679 -0.8892987]][0m
[37m[1m[2023-07-11 03:40:41,207][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:40:50,176][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 03:40:50,176][233954] FPS: 428220.71[0m
[36m[2023-07-11 03:40:50,178][233954] itr=287, itrs=2000, Progress: 14.35%[0m
[36m[2023-07-11 03:41:01,736][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 03:41:01,736][233954] FPS: 334276.40[0m
[36m[2023-07-11 03:41:05,974][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:41:05,975][233954] Reward + Measures: [[7.08908983 0.18317668 0.91139865 0.84473169 0.88446766 0.96569991]][0m
[37m[1m[2023-07-11 03:41:05,975][233954] Max Reward on eval: 7.089089832183013[0m
[37m[1m[2023-07-11 03:41:05,975][233954] Min Reward on eval: 7.089089832183013[0m
[37m[1m[2023-07-11 03:41:05,976][233954] Mean Reward across all agents: 7.089089832183013[0m
[37m[1m[2023-07-11 03:41:05,976][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:41:10,972][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:41:10,972][233954] Reward + Measures: [[-150.46434043    0.30700001    0.41760001    0.20510001    0.45839998
     2.26946878]
 [-117.51109219    0.62290001    0.43420002    0.52219999    0.39699998
     1.8330338 ]
 [-117.56675724    0.5758        0.44350001    0.49939999    0.39750001
     1.57286835]
 ...
 [ -58.94536084    0.46620002    0.55500001    0.38210002    0.56900001
     1.54775167]
 [ -29.9900852     0.4337        0.80310005    0.44530001    0.76669997
     0.78459859]
 [-139.02208205    0.2           0.42529997    0.24959998    0.4384
     1.694857  ]][0m
[37m[1m[2023-07-11 03:41:10,973][233954] Max Reward on eval: 70.45046968210954[0m
[37m[1m[2023-07-11 03:41:10,973][233954] Min Reward on eval: -242.59264655783772[0m
[37m[1m[2023-07-11 03:41:10,973][233954] Mean Reward across all agents: -108.49858517439473[0m
[37m[1m[2023-07-11 03:41:10,973][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:41:10,977][233954] mean_value=-266.4131377750945, max_value=419.39102313079405[0m
[37m[1m[2023-07-11 03:41:10,979][233954] New mean coefficients: [[ 2.4444585   1.8344647   6.0619893   2.9180808  -0.06444341 -1.5931407 ]][0m
[37m[1m[2023-07-11 03:41:10,980][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:41:19,956][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 03:41:19,957][233954] FPS: 427880.90[0m
[36m[2023-07-11 03:41:19,959][233954] itr=288, itrs=2000, Progress: 14.40%[0m
[36m[2023-07-11 03:41:31,686][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 03:41:31,686][233954] FPS: 329523.49[0m
[36m[2023-07-11 03:41:36,002][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:41:36,003][233954] Reward + Measures: [[-272.54572137    0.95601368    0.77949166    0.98713899    0.81312627
     2.45674157]][0m
[37m[1m[2023-07-11 03:41:36,003][233954] Max Reward on eval: -272.54572136544414[0m
[37m[1m[2023-07-11 03:41:36,003][233954] Min Reward on eval: -272.54572136544414[0m
[37m[1m[2023-07-11 03:41:36,004][233954] Mean Reward across all agents: -272.54572136544414[0m
[37m[1m[2023-07-11 03:41:36,004][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:41:41,094][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:41:41,100][233954] Reward + Measures: [[ 140.58303499    0.80289996    0.95139998    0.7924        0.96399993
     2.622401  ]
 [ 268.34978295    0.4621        0.91619998    0.51709998    0.93199998
     2.34466529]
 [ 207.54395102    0.0845        0.95780003    0.19450001    0.96119994
     2.53898859]
 ...
 [ 279.34641837    0.0842        0.96270001    0.21440001    0.97200006
     3.08895373]
 [-160.97899746    0.74550003    0.87659997    0.84580004    0.87460005
     1.95748067]
 [ 231.35967445    0.0546        0.98460007    0.23699999    0.98600006
     2.55183506]][0m
[37m[1m[2023-07-11 03:41:41,100][233954] Max Reward on eval: 279.9683284719475[0m
[37m[1m[2023-07-11 03:41:41,101][233954] Min Reward on eval: -285.6870174402371[0m
[37m[1m[2023-07-11 03:41:41,101][233954] Mean Reward across all agents: 74.01206789682507[0m
[37m[1m[2023-07-11 03:41:41,101][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:41:41,117][233954] mean_value=425.45604195964717, max_value=779.3464183728211[0m
[37m[1m[2023-07-11 03:41:41,120][233954] New mean coefficients: [[ 3.483337    2.6118505   6.0868607   2.8440695   0.73822194 -0.8440408 ]][0m
[37m[1m[2023-07-11 03:41:41,121][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:41:50,220][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 03:41:50,220][233954] FPS: 422088.77[0m
[36m[2023-07-11 03:41:50,223][233954] itr=289, itrs=2000, Progress: 14.45%[0m
[36m[2023-07-11 03:42:01,901][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 03:42:01,901][233954] FPS: 330856.52[0m
[36m[2023-07-11 03:42:06,122][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:42:06,122][233954] Reward + Measures: [[-117.94786194    0.390374      0.89198631    0.35924736    0.84498525
     1.0960983 ]][0m
[37m[1m[2023-07-11 03:42:06,123][233954] Max Reward on eval: -117.9478619432779[0m
[37m[1m[2023-07-11 03:42:06,123][233954] Min Reward on eval: -117.9478619432779[0m
[37m[1m[2023-07-11 03:42:06,123][233954] Mean Reward across all agents: -117.9478619432779[0m
[37m[1m[2023-07-11 03:42:06,123][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:42:11,059][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:42:11,059][233954] Reward + Measures: [[-120.01307454    0.36970001    0.8682        0.10290001    0.83820003
     2.22749066]
 [-147.72051143    0.50139999    0.98470002    0.0144        0.95459998
     2.37456989]
 [-123.99236395    0.7098        0.9853999     0.0031        0.97290003
     2.40097308]
 ...
 [ -84.3413        0.66259998    0.98229998    0.0089        0.98029995
     2.29760623]
 [   7.94189025    0.06879999    0.86740011    0.39429998    0.95360005
     2.63014388]
 [-170.19444751    0.48719999    0.9752        0.0849        0.90789998
     2.45383477]][0m
[37m[1m[2023-07-11 03:42:11,059][233954] Max Reward on eval: 53.20687186634168[0m
[37m[1m[2023-07-11 03:42:11,060][233954] Min Reward on eval: -365.65114591866734[0m
[37m[1m[2023-07-11 03:42:11,060][233954] Mean Reward across all agents: -64.02746692994656[0m
[37m[1m[2023-07-11 03:42:11,060][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:42:11,074][233954] mean_value=124.24837387367758, max_value=504.6531555909663[0m
[37m[1m[2023-07-11 03:42:11,077][233954] New mean coefficients: [[ 3.4766548  2.63466    6.39287    3.0267684  0.6921807 -1.2547638]][0m
[37m[1m[2023-07-11 03:42:11,078][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:42:20,088][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 03:42:20,088][233954] FPS: 426262.85[0m
[36m[2023-07-11 03:42:20,090][233954] itr=290, itrs=2000, Progress: 14.50%[0m
[37m[1m[2023-07-11 03:45:27,774][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000270[0m
[36m[2023-07-11 03:45:40,012][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 03:45:40,012][233954] FPS: 328831.08[0m
[36m[2023-07-11 03:45:44,273][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:45:44,273][233954] Reward + Measures: [[30.82016089  0.00468533  0.99682599  0.95454264  0.97704899  1.80642891]][0m
[37m[1m[2023-07-11 03:45:44,274][233954] Max Reward on eval: 30.820160894841198[0m
[37m[1m[2023-07-11 03:45:44,274][233954] Min Reward on eval: 30.820160894841198[0m
[37m[1m[2023-07-11 03:45:44,274][233954] Mean Reward across all agents: 30.820160894841198[0m
[37m[1m[2023-07-11 03:45:44,274][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:45:49,107][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:45:49,108][233954] Reward + Measures: [[-34.46396358   0.58490002   0.67160004   0.25139999   0.66620004
    0.91948825]
 [ 59.27708536   0.69749993   0.61410004   0.4896       0.5729
    1.39452112]
 [ 23.69227161   0.82930005   0.6577       0.90070003   0.75299996
    1.0949949 ]
 ...
 [ 67.87486184   0.52200001   0.47910005   0.52320004   0.54320002
    1.04073787]
 [ 35.3662246    0.60159999   0.72279996   0.38370004   0.57950002
    1.03126752]
 [ -4.81360225   0.56230003   0.70760006   0.2366       0.72189999
    0.94042701]][0m
[37m[1m[2023-07-11 03:45:49,108][233954] Max Reward on eval: 122.97196983136237[0m
[37m[1m[2023-07-11 03:45:49,109][233954] Min Reward on eval: -107.48583364337682[0m
[37m[1m[2023-07-11 03:45:49,109][233954] Mean Reward across all agents: 11.07861260270273[0m
[37m[1m[2023-07-11 03:45:49,109][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:45:49,125][233954] mean_value=158.45711678509585, max_value=550.4794263544493[0m
[37m[1m[2023-07-11 03:45:49,128][233954] New mean coefficients: [[ 2.9506392   2.9717236   5.7410645   2.913382    0.92354006 -1.4274447 ]][0m
[37m[1m[2023-07-11 03:45:49,129][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:45:58,122][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 03:45:58,123][233954] FPS: 427063.93[0m
[36m[2023-07-11 03:45:58,125][233954] itr=291, itrs=2000, Progress: 14.55%[0m
[36m[2023-07-11 03:46:09,721][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 03:46:09,721][233954] FPS: 333292.39[0m
[36m[2023-07-11 03:46:13,899][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:46:13,900][233954] Reward + Measures: [[-32.50734902   0.99082202   0.98810303   0.98847735   0.94848126
    1.98007894]][0m
[37m[1m[2023-07-11 03:46:13,900][233954] Max Reward on eval: -32.507349024969926[0m
[37m[1m[2023-07-11 03:46:13,900][233954] Min Reward on eval: -32.507349024969926[0m
[37m[1m[2023-07-11 03:46:13,900][233954] Mean Reward across all agents: -32.507349024969926[0m
[37m[1m[2023-07-11 03:46:13,901][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:46:18,828][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:46:18,828][233954] Reward + Measures: [[-16.36254854   0.99560004   0.94999999   0.99500006   0.0032
    2.15436363]
 [-12.70662443   0.99389994   0.96210003   0.99480003   0.004
    2.1396482 ]
 [ -9.13121639   0.98920006   0.8531       0.99230003   0.0042
    2.26868558]
 ...
 [ -5.91429872   0.99540007   0.96999997   0.99449998   0.2784
    2.15521169]
 [-14.5337043    0.92550004   0.8818       0.92290002   0.0585
    2.19612432]
 [-32.31058741   0.9938001    0.92570001   0.99480003   0.0029
    2.25339484]][0m
[37m[1m[2023-07-11 03:46:18,829][233954] Max Reward on eval: 16.13783614188433[0m
[37m[1m[2023-07-11 03:46:18,829][233954] Min Reward on eval: -108.58301983801648[0m
[37m[1m[2023-07-11 03:46:18,829][233954] Mean Reward across all agents: -12.909228722775907[0m
[37m[1m[2023-07-11 03:46:18,829][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:46:18,835][233954] mean_value=168.4248384211503, max_value=458.3099031314859[0m
[37m[1m[2023-07-11 03:46:18,838][233954] New mean coefficients: [[ 1.7231646   2.0258927   3.517275    2.4460993   0.10260147 -2.3994622 ]][0m
[37m[1m[2023-07-11 03:46:18,839][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:46:27,806][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 03:46:27,806][233954] FPS: 428317.88[0m
[36m[2023-07-11 03:46:27,808][233954] itr=292, itrs=2000, Progress: 14.60%[0m
[36m[2023-07-11 03:46:39,313][233954] train() took 11.43 seconds to complete[0m
[36m[2023-07-11 03:46:39,314][233954] FPS: 335861.75[0m
[36m[2023-07-11 03:46:43,535][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:46:43,535][233954] Reward + Measures: [[-24.43914643   0.47163436   0.82584494   0.34288898   0.79077506
    0.7335698 ]][0m
[37m[1m[2023-07-11 03:46:43,536][233954] Max Reward on eval: -24.439146431076203[0m
[37m[1m[2023-07-11 03:46:43,536][233954] Min Reward on eval: -24.439146431076203[0m
[37m[1m[2023-07-11 03:46:43,536][233954] Mean Reward across all agents: -24.439146431076203[0m
[37m[1m[2023-07-11 03:46:43,536][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:46:48,489][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:46:48,489][233954] Reward + Measures: [[ 18.94423788   0.36480001   0.83960003   0.43720004   0.78470004
    0.84351254]
 [ 16.82464896   0.58039999   0.80100006   0.5801       0.75610006
    0.73440409]
 [-17.23668788   0.56490004   0.7852       0.5848       0.77780002
    0.65440094]
 ...
 [  0.847052     0.63360006   0.87709999   0.6936       0.76529998
    1.11495149]
 [ 10.07351199   0.43670002   0.89300007   0.56750005   0.83350003
    0.9888711 ]
 [ 38.04550261   0.33260003   0.79039997   0.4289       0.71759999
    0.87152815]][0m
[37m[1m[2023-07-11 03:46:48,490][233954] Max Reward on eval: 67.3319320961833[0m
[37m[1m[2023-07-11 03:46:48,490][233954] Min Reward on eval: -47.74310685172677[0m
[37m[1m[2023-07-11 03:46:48,490][233954] Mean Reward across all agents: 14.534229533181241[0m
[37m[1m[2023-07-11 03:46:48,491][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:46:48,498][233954] mean_value=107.29803884586735, max_value=537.5924722428433[0m
[37m[1m[2023-07-11 03:46:48,501][233954] New mean coefficients: [[ 1.3653198  1.5512681  2.8464923  2.950406  -0.6062725 -2.9047027]][0m
[37m[1m[2023-07-11 03:46:48,502][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:46:57,402][233954] train() took 8.90 seconds to complete[0m
[36m[2023-07-11 03:46:57,403][233954] FPS: 431503.35[0m
[36m[2023-07-11 03:46:57,405][233954] itr=293, itrs=2000, Progress: 14.65%[0m
[36m[2023-07-11 03:47:08,873][233954] train() took 11.40 seconds to complete[0m
[36m[2023-07-11 03:47:08,873][233954] FPS: 336986.91[0m
[36m[2023-07-11 03:47:13,079][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:47:13,079][233954] Reward + Measures: [[3.04586712 0.70474631 0.90962529 0.6629653  0.87422699 0.6086216 ]][0m
[37m[1m[2023-07-11 03:47:13,079][233954] Max Reward on eval: 3.0458671225844496[0m
[37m[1m[2023-07-11 03:47:13,080][233954] Min Reward on eval: 3.0458671225844496[0m
[37m[1m[2023-07-11 03:47:13,080][233954] Mean Reward across all agents: 3.0458671225844496[0m
[37m[1m[2023-07-11 03:47:13,080][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:47:18,026][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:47:18,027][233954] Reward + Measures: [[16.82652819  0.2031      0.81940001  0.3179      0.80400002  1.03162742]
 [20.51800921  0.2129      0.81999999  0.31420001  0.82800001  1.05793786]
 [29.57905346  0.2359      0.77699995  0.23120001  0.75009996  0.94213659]
 ...
 [36.93630009  0.2383      0.80930007  0.34369999  0.7798      0.99997801]
 [18.37111304  0.64790004  0.94740003  0.68919998  0.92220002  0.80045491]
 [12.35386707  0.51489997  0.90259999  0.58529997  0.89070004  0.70336533]][0m
[37m[1m[2023-07-11 03:47:18,027][233954] Max Reward on eval: 111.45613813772798[0m
[37m[1m[2023-07-11 03:47:18,027][233954] Min Reward on eval: -15.070774640515447[0m
[37m[1m[2023-07-11 03:47:18,028][233954] Mean Reward across all agents: 27.517953760610837[0m
[37m[1m[2023-07-11 03:47:18,028][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:47:18,035][233954] mean_value=65.6637270200655, max_value=452.82088386300023[0m
[37m[1m[2023-07-11 03:47:18,038][233954] New mean coefficients: [[ 2.0193813   2.2649083   4.118682    3.9255726   0.22672057 -2.3094215 ]][0m
[37m[1m[2023-07-11 03:47:18,039][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:47:26,985][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 03:47:26,986][233954] FPS: 429299.30[0m
[36m[2023-07-11 03:47:26,988][233954] itr=294, itrs=2000, Progress: 14.70%[0m
[36m[2023-07-11 03:47:38,605][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 03:47:38,606][233954] FPS: 332678.48[0m
[36m[2023-07-11 03:47:42,836][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:47:42,837][233954] Reward + Measures: [[3.38992425 0.77415901 0.94119036 0.78280467 0.91719764 0.58004111]][0m
[37m[1m[2023-07-11 03:47:42,837][233954] Max Reward on eval: 3.3899242505369185[0m
[37m[1m[2023-07-11 03:47:42,837][233954] Min Reward on eval: 3.3899242505369185[0m
[37m[1m[2023-07-11 03:47:42,837][233954] Mean Reward across all agents: 3.3899242505369185[0m
[37m[1m[2023-07-11 03:47:42,838][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:47:47,786][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:47:47,787][233954] Reward + Measures: [[-52.78979757   0.60320002   0.86750001   0.49710003   0.81189996
    0.69365036]
 [ 16.17888854   0.77969998   0.94230002   0.84510005   0.93050003
    0.56700015]
 [ -2.33384136   0.68300003   0.92440003   0.71200001   0.91409999
    0.71161646]
 ...
 [  4.9969337    0.59079999   0.79559994   0.32560003   0.75360006
    0.86922473]
 [ 51.41721917   0.65310001   0.89970011   0.7367       0.86490005
    0.8089447 ]
 [-43.42695927   0.58230001   0.89169997   0.45080003   0.85719997
    0.66811067]][0m
[37m[1m[2023-07-11 03:47:47,787][233954] Max Reward on eval: 79.14094802129549[0m
[37m[1m[2023-07-11 03:47:47,787][233954] Min Reward on eval: -115.3711500653997[0m
[37m[1m[2023-07-11 03:47:47,787][233954] Mean Reward across all agents: -7.472539566495843[0m
[37m[1m[2023-07-11 03:47:47,788][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:47:47,794][233954] mean_value=111.06708164937594, max_value=514.9774859024212[0m
[37m[1m[2023-07-11 03:47:47,797][233954] New mean coefficients: [[ 2.077949    2.5789      3.817965    4.450382    0.98493654 -1.8660321 ]][0m
[37m[1m[2023-07-11 03:47:47,798][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:47:56,756][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 03:47:56,756][233954] FPS: 428734.19[0m
[36m[2023-07-11 03:47:56,759][233954] itr=295, itrs=2000, Progress: 14.75%[0m
[36m[2023-07-11 03:48:08,363][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 03:48:08,363][233954] FPS: 332928.14[0m
[36m[2023-07-11 03:48:12,675][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:48:12,675][233954] Reward + Measures: [[2.1501557  0.89331567 0.964535   0.90247899 0.95436001 0.71107423]][0m
[37m[1m[2023-07-11 03:48:12,676][233954] Max Reward on eval: 2.1501557009253616[0m
[37m[1m[2023-07-11 03:48:12,676][233954] Min Reward on eval: 2.1501557009253616[0m
[37m[1m[2023-07-11 03:48:12,676][233954] Mean Reward across all agents: 2.1501557009253616[0m
[37m[1m[2023-07-11 03:48:12,676][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:48:17,635][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:48:17,635][233954] Reward + Measures: [[40.58808007  0.93240005  0.97390002  0.94810009  0.96750003  1.05263174]
 [11.57344221  0.96709996  0.95590001  0.97119999  0.94950002  2.02601314]
 [ 1.49978088  0.9794001   0.97959995  0.98480004  0.9774      0.84552634]
 ...
 [30.33844024  0.89279997  0.90609992  0.89230007  0.89419997  1.63848555]
 [29.68982456  0.99040002  0.99250001  0.99060005  0.99139994  1.75692141]
 [33.21470956  0.83090001  0.95620006  0.86250001  0.92950004  0.98698276]][0m
[37m[1m[2023-07-11 03:48:17,635][233954] Max Reward on eval: 198.0825881895609[0m
[37m[1m[2023-07-11 03:48:17,636][233954] Min Reward on eval: -26.443256316334008[0m
[37m[1m[2023-07-11 03:48:17,636][233954] Mean Reward across all agents: 38.20390517330466[0m
[37m[1m[2023-07-11 03:48:17,636][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:48:17,643][233954] mean_value=56.03729551504992, max_value=523.4991471610963[0m
[37m[1m[2023-07-11 03:48:17,646][233954] New mean coefficients: [[ 2.2757082   2.5537713   3.9438465   5.1792254   0.85991216 -1.9262873 ]][0m
[37m[1m[2023-07-11 03:48:17,646][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:48:26,548][233954] train() took 8.90 seconds to complete[0m
[36m[2023-07-11 03:48:26,548][233954] FPS: 431458.35[0m
[36m[2023-07-11 03:48:26,551][233954] itr=296, itrs=2000, Progress: 14.80%[0m
[36m[2023-07-11 03:48:38,102][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 03:48:38,103][233954] FPS: 334575.45[0m
[36m[2023-07-11 03:48:42,327][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:48:42,327][233954] Reward + Measures: [[1.10926901 0.9176563  0.97267926 0.92536336 0.96192861 0.71835268]][0m
[37m[1m[2023-07-11 03:48:42,328][233954] Max Reward on eval: 1.109269010147109[0m
[37m[1m[2023-07-11 03:48:42,328][233954] Min Reward on eval: 1.109269010147109[0m
[37m[1m[2023-07-11 03:48:42,328][233954] Mean Reward across all agents: 1.109269010147109[0m
[37m[1m[2023-07-11 03:48:42,328][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:48:47,328][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:48:47,381][233954] Reward + Measures: [[ 14.69984944   0.85410005   0.95340008   0.87900001   0.92869997
    0.61154968]
 [-17.95655533   0.90410006   0.94319993   0.0547       0.84860003
    1.84325659]
 [ 34.80416686   0.82870001   0.94709998   0.0561       0.84750003
    1.71988833]
 ...
 [  8.95059787   0.67690003   0.92830008   0.77999997   0.89449996
    0.58079708]
 [ 43.12112826   0.30560002   0.82159996   0.37680003   0.69069999
    0.91414928]
 [ 28.22477139   0.49869999   0.79300004   0.44169998   0.62530005
    1.13811266]][0m
[37m[1m[2023-07-11 03:48:47,381][233954] Max Reward on eval: 96.54686307711526[0m
[37m[1m[2023-07-11 03:48:47,381][233954] Min Reward on eval: -135.81856681741775[0m
[37m[1m[2023-07-11 03:48:47,382][233954] Mean Reward across all agents: 23.058848710953704[0m
[37m[1m[2023-07-11 03:48:47,382][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:48:47,393][233954] mean_value=108.3082265522496, max_value=589.8366665864363[0m
[37m[1m[2023-07-11 03:48:47,396][233954] New mean coefficients: [[ 2.9427319  3.642566   4.234044   5.831442   1.9266257 -1.0385389]][0m
[37m[1m[2023-07-11 03:48:47,397][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:48:56,441][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 03:48:56,441][233954] FPS: 424662.22[0m
[36m[2023-07-11 03:48:56,444][233954] itr=297, itrs=2000, Progress: 14.85%[0m
[36m[2023-07-11 03:49:08,087][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 03:49:08,092][233954] FPS: 331906.88[0m
[36m[2023-07-11 03:49:12,416][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:49:12,416][233954] Reward + Measures: [[0.11225084 0.92874932 0.9761408  0.93687564 0.9669956  0.70964372]][0m
[37m[1m[2023-07-11 03:49:12,417][233954] Max Reward on eval: 0.11225084380808403[0m
[37m[1m[2023-07-11 03:49:12,417][233954] Min Reward on eval: 0.11225084380808403[0m
[37m[1m[2023-07-11 03:49:12,417][233954] Mean Reward across all agents: 0.11225084380808403[0m
[37m[1m[2023-07-11 03:49:12,417][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:49:17,644][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:49:17,645][233954] Reward + Measures: [[-18.29164772   0.84699994   0.92750007   0.86210006   0.93049997
    0.8903929 ]
 [-21.04114626   0.77219999   0.90820009   0.7967       0.92729998
    0.91156524]
 [  7.07265299   0.90340006   0.95030004   0.89260006   0.96420002
    0.87323827]
 ...
 [ 25.89333546   0.75720006   0.88899994   0.78680009   0.76069999
    1.10297   ]
 [ -5.84228116   0.87980002   0.9188       0.89399999   0.92020005
    0.86091137]
 [-23.10070085   0.80389994   0.90550005   0.83840007   0.93030006
    0.90903753]][0m
[37m[1m[2023-07-11 03:49:17,645][233954] Max Reward on eval: 25.893335458170622[0m
[37m[1m[2023-07-11 03:49:17,645][233954] Min Reward on eval: -37.82021163718309[0m
[37m[1m[2023-07-11 03:49:17,645][233954] Mean Reward across all agents: -13.158085540077787[0m
[37m[1m[2023-07-11 03:49:17,646][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:49:17,649][233954] mean_value=7.481882908537994, max_value=396.27987732480454[0m
[37m[1m[2023-07-11 03:49:17,652][233954] New mean coefficients: [[ 2.9008296  3.1160765  4.277771   6.0138497  1.1271924 -1.561525 ]][0m
[37m[1m[2023-07-11 03:49:17,653][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:49:26,670][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 03:49:26,670][233954] FPS: 425923.67[0m
[36m[2023-07-11 03:49:26,673][233954] itr=298, itrs=2000, Progress: 14.90%[0m
[36m[2023-07-11 03:49:38,351][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 03:49:38,351][233954] FPS: 330828.17[0m
[36m[2023-07-11 03:49:42,646][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:49:42,647][233954] Reward + Measures: [[-0.18175352  0.94093561  0.98085767  0.94763738  0.97394204  0.71469057]][0m
[37m[1m[2023-07-11 03:49:42,647][233954] Max Reward on eval: -0.18175352058919572[0m
[37m[1m[2023-07-11 03:49:42,647][233954] Min Reward on eval: -0.18175352058919572[0m
[37m[1m[2023-07-11 03:49:42,647][233954] Mean Reward across all agents: -0.18175352058919572[0m
[37m[1m[2023-07-11 03:49:42,648][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:49:47,615][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:49:47,615][233954] Reward + Measures: [[ 5.1164296   0.72970003  0.89899999  0.78689998  0.89590007  0.82395822]
 [ 6.42580809  0.82989997  0.87470001  0.85140002  0.88819999  0.89258951]
 [-7.81817899  0.88500005  0.9526      0.93290007  0.95630008  0.82496089]
 ...
 [ 2.94412962  0.77030003  0.89480001  0.78719997  0.87600005  0.92350113]
 [ 4.3135295   0.92370003  0.95370001  0.97090006  0.96020001  0.92738003]
 [ 5.21747011  0.90009993  0.95230001  0.94380009  0.96169996  0.85309643]][0m
[37m[1m[2023-07-11 03:49:47,616][233954] Max Reward on eval: 25.38036847440526[0m
[37m[1m[2023-07-11 03:49:47,616][233954] Min Reward on eval: -42.91843716893345[0m
[37m[1m[2023-07-11 03:49:47,616][233954] Mean Reward across all agents: 4.473919551797314[0m
[37m[1m[2023-07-11 03:49:47,616][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:49:47,621][233954] mean_value=12.235801196616446, max_value=425.3407177932737[0m
[37m[1m[2023-07-11 03:49:47,623][233954] New mean coefficients: [[ 2.7701232  3.4875448  4.5796833  5.8383846  1.4695522 -1.7974184]][0m
[37m[1m[2023-07-11 03:49:47,624][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:49:56,649][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 03:49:56,649][233954] FPS: 425573.66[0m
[36m[2023-07-11 03:49:56,652][233954] itr=299, itrs=2000, Progress: 14.95%[0m
[36m[2023-07-11 03:50:08,410][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 03:50:08,415][233954] FPS: 328591.54[0m
[36m[2023-07-11 03:50:12,667][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:50:12,668][233954] Reward + Measures: [[-1.13379721  0.96546173  0.98623729  0.96982402  0.98631167  0.72601527]][0m
[37m[1m[2023-07-11 03:50:12,668][233954] Max Reward on eval: -1.133797213197385[0m
[37m[1m[2023-07-11 03:50:12,668][233954] Min Reward on eval: -1.133797213197385[0m
[37m[1m[2023-07-11 03:50:12,668][233954] Mean Reward across all agents: -1.133797213197385[0m
[37m[1m[2023-07-11 03:50:12,669][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:50:17,648][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:50:17,649][233954] Reward + Measures: [[  2.38692783   0.84419996   0.95430005   0.85120004   0.97749996
    0.64163417]
 [181.95162518   0.0529       0.74659997   0.53940004   0.81560004
    1.58940208]
 [-25.21916028   0.2177       0.65670007   0.46720001   0.71969998
    1.03520679]
 ...
 [ -0.50548339   0.8373       0.95700008   0.83219999   0.9752
    0.60221618]
 [-13.12656525   0.69499999   0.91650003   0.72370005   0.95219994
    0.75254178]
 [ -2.36807122   0.81100005   0.93329996   0.81999999   0.96350002
    0.61854213]][0m
[37m[1m[2023-07-11 03:50:17,649][233954] Max Reward on eval: 446.6307415938936[0m
[37m[1m[2023-07-11 03:50:17,649][233954] Min Reward on eval: -34.583618749235754[0m
[37m[1m[2023-07-11 03:50:17,649][233954] Mean Reward across all agents: 18.029252320868682[0m
[37m[1m[2023-07-11 03:50:17,649][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:50:17,660][233954] mean_value=144.0449970480474, max_value=572.8910883722432[0m
[37m[1m[2023-07-11 03:50:17,662][233954] New mean coefficients: [[ 3.766284   3.7326272  5.558071   6.435535   1.748346  -1.0530682]][0m
[37m[1m[2023-07-11 03:50:17,663][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:50:26,646][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 03:50:26,646][233954] FPS: 427581.97[0m
[36m[2023-07-11 03:50:26,648][233954] itr=300, itrs=2000, Progress: 15.00%[0m
[37m[1m[2023-07-11 03:53:32,999][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000280[0m
[36m[2023-07-11 03:53:45,499][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 03:53:45,499][233954] FPS: 329055.68[0m
[36m[2023-07-11 03:53:49,713][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:53:49,713][233954] Reward + Measures: [[-16.11493538   0.99003303   0.99141395   0.99379629   0.98174399
    1.44306707]][0m
[37m[1m[2023-07-11 03:53:49,713][233954] Max Reward on eval: -16.11493537650884[0m
[37m[1m[2023-07-11 03:53:49,714][233954] Min Reward on eval: -16.11493537650884[0m
[37m[1m[2023-07-11 03:53:49,714][233954] Mean Reward across all agents: -16.11493537650884[0m
[37m[1m[2023-07-11 03:53:49,714][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:53:54,542][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:53:54,543][233954] Reward + Measures: [[70.37089602  0.419       0.708       0.41510001  0.70960003  2.47577977]
 [78.10928297  0.65920001  0.94440001  0.58070004  0.94680005  1.87088335]
 [95.40834426  0.60179996  0.91909999  0.55269998  0.92150003  2.0512836 ]
 ...
 [53.34479527  0.77880001  0.49759999  0.82779998  0.66689998  1.61734796]
 [94.6574319   0.46429998  0.87979996  0.41529998  0.88609999  2.22970581]
 [12.64965167  0.76290005  0.38730001  0.85670006  0.65400004  1.49788821]][0m
[37m[1m[2023-07-11 03:53:54,543][233954] Max Reward on eval: 139.95392752513288[0m
[37m[1m[2023-07-11 03:53:54,543][233954] Min Reward on eval: -17.46223427145742[0m
[37m[1m[2023-07-11 03:53:54,544][233954] Mean Reward across all agents: 65.66849839906635[0m
[37m[1m[2023-07-11 03:53:54,544][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:53:54,556][233954] mean_value=332.1304106692862, max_value=611.64503764119[0m
[37m[1m[2023-07-11 03:53:54,559][233954] New mean coefficients: [[ 4.1230707   3.7436368   6.007863    7.7278748   1.957976   -0.53438216]][0m
[37m[1m[2023-07-11 03:53:54,560][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:54:03,482][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 03:54:03,483][233954] FPS: 430475.24[0m
[36m[2023-07-11 03:54:03,485][233954] itr=301, itrs=2000, Progress: 15.05%[0m
[36m[2023-07-11 03:54:15,118][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 03:54:15,119][233954] FPS: 332135.06[0m
[36m[2023-07-11 03:54:19,369][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:54:19,370][233954] Reward + Measures: [[-13.64862355   0.99020571   0.98400033   0.99424136   0.98306996
    1.97820342]][0m
[37m[1m[2023-07-11 03:54:19,370][233954] Max Reward on eval: -13.648623553804045[0m
[37m[1m[2023-07-11 03:54:19,370][233954] Min Reward on eval: -13.648623553804045[0m
[37m[1m[2023-07-11 03:54:19,371][233954] Mean Reward across all agents: -13.648623553804045[0m
[37m[1m[2023-07-11 03:54:19,371][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:54:24,288][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:54:24,289][233954] Reward + Measures: [[  50.04126979    0.49659997    0.46750003    0.91610003    0.86980003
     2.98244739]
 [ 112.98008825    0.5589        0.39760002    0.89610004    0.86059999
     2.91654825]
 [  28.12797976    0.56900001    0.36999997    0.91680002    0.83190006
     2.99471784]
 ...
 [ -28.07730699    0.69080001    0.1987        0.87979996    0.84300005
     3.0596962 ]
 [-115.5213802     0.71880001    0.0587        0.82669991    0.7313
     3.25237203]
 [  93.12399764    0.67290002    0.028         0.76049995    0.65200007
     3.26517272]][0m
[37m[1m[2023-07-11 03:54:24,289][233954] Max Reward on eval: 310.5414659938775[0m
[37m[1m[2023-07-11 03:54:24,289][233954] Min Reward on eval: -134.89114427901805[0m
[37m[1m[2023-07-11 03:54:24,290][233954] Mean Reward across all agents: 30.123560899699065[0m
[37m[1m[2023-07-11 03:54:24,290][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:54:24,302][233954] mean_value=410.9453938151218, max_value=810.5414659938775[0m
[37m[1m[2023-07-11 03:54:24,305][233954] New mean coefficients: [[ 3.5533772  3.2195616  5.292492   6.464303   1.0600557 -1.9211223]][0m
[37m[1m[2023-07-11 03:54:24,306][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:54:33,307][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 03:54:33,307][233954] FPS: 426669.06[0m
[36m[2023-07-11 03:54:33,310][233954] itr=302, itrs=2000, Progress: 15.10%[0m
[36m[2023-07-11 03:54:44,947][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 03:54:44,948][233954] FPS: 332106.22[0m
[36m[2023-07-11 03:54:49,211][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:54:49,212][233954] Reward + Measures: [[33.46510671  0.97627658  0.94307166  0.99600601  0.00646367  1.52858377]][0m
[37m[1m[2023-07-11 03:54:49,212][233954] Max Reward on eval: 33.465106710928445[0m
[37m[1m[2023-07-11 03:54:49,212][233954] Min Reward on eval: 33.465106710928445[0m
[37m[1m[2023-07-11 03:54:49,213][233954] Mean Reward across all agents: 33.465106710928445[0m
[37m[1m[2023-07-11 03:54:49,213][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:54:54,234][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:54:54,235][233954] Reward + Measures: [[ 24.86746435   0.69309998   0.81340009   0.59930003   0.48680001
    1.86535072]
 [-17.15605223   0.62350005   0.79679996   0.52010006   0.47750002
    1.82824862]
 [ 56.24233772   0.52860004   0.80140001   0.44549999   0.57300001
    1.91560555]
 ...
 [-48.00770137   0.66240001   0.74980003   0.55489999   0.42080003
    1.83775556]
 [ 99.22507287   0.2296       0.86359996   0.2431       0.76660007
    2.35564852]
 [ 47.0564431    0.74650002   0.78920001   0.69160002   0.26640001
    1.71350026]][0m
[37m[1m[2023-07-11 03:54:54,235][233954] Max Reward on eval: 113.87008428503759[0m
[37m[1m[2023-07-11 03:54:54,236][233954] Min Reward on eval: -378.31402630945666[0m
[37m[1m[2023-07-11 03:54:54,236][233954] Mean Reward across all agents: 20.14508990790984[0m
[37m[1m[2023-07-11 03:54:54,236][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:54:54,249][233954] mean_value=279.1369359153416, max_value=591.5251183338463[0m
[37m[1m[2023-07-11 03:54:54,252][233954] New mean coefficients: [[ 3.6375482  2.8760011  5.0765014  5.1402373  0.765389  -1.3487027]][0m
[37m[1m[2023-07-11 03:54:54,253][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:55:03,226][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 03:55:03,226][233954] FPS: 428016.59[0m
[36m[2023-07-11 03:55:03,229][233954] itr=303, itrs=2000, Progress: 15.15%[0m
[36m[2023-07-11 03:55:15,213][233954] train() took 11.91 seconds to complete[0m
[36m[2023-07-11 03:55:15,214][233954] FPS: 322350.53[0m
[36m[2023-07-11 03:55:19,557][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:55:19,558][233954] Reward + Measures: [[27.84033897  0.98295635  0.95603561  0.88563573  0.61004198  1.87394595]][0m
[37m[1m[2023-07-11 03:55:19,558][233954] Max Reward on eval: 27.84033896887448[0m
[37m[1m[2023-07-11 03:55:19,558][233954] Min Reward on eval: 27.84033896887448[0m
[37m[1m[2023-07-11 03:55:19,559][233954] Mean Reward across all agents: 27.84033896887448[0m
[37m[1m[2023-07-11 03:55:19,559][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:55:24,589][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:55:24,590][233954] Reward + Measures: [[625.37263106   0.0038       0.96210003   0.87849998   0.9684
    2.40489936]
 [ 15.98307373   0.86809999   0.93480009   0.8448       0.96040004
    1.66432655]
 [110.00854399   0.0152       0.8761       0.44619998   0.95780003
    2.31955957]
 ...
 [  7.54189364   0.75570005   0.75330001   0.70910001   0.76209998
    1.60781324]
 [  9.89969871   0.88829994   0.93689996   0.88510007   0.96490002
    1.56492865]
 [ -9.30481512   0.98190004   0.84580004   0.94729996   0.0313
    1.89392841]][0m
[37m[1m[2023-07-11 03:55:24,590][233954] Max Reward on eval: 699.2195052996278[0m
[37m[1m[2023-07-11 03:55:24,590][233954] Min Reward on eval: -147.80817411784082[0m
[37m[1m[2023-07-11 03:55:24,590][233954] Mean Reward across all agents: 118.61595828616939[0m
[37m[1m[2023-07-11 03:55:24,591][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:55:24,605][233954] mean_value=247.10128737244216, max_value=960.8605786217697[0m
[37m[1m[2023-07-11 03:55:24,608][233954] New mean coefficients: [[ 3.9713776   4.4278507   5.850541    6.1361628   2.2573647  -0.28962064]][0m
[37m[1m[2023-07-11 03:55:24,609][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:55:33,680][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 03:55:33,680][233954] FPS: 423428.80[0m
[36m[2023-07-11 03:55:33,682][233954] itr=304, itrs=2000, Progress: 15.20%[0m
[36m[2023-07-11 03:55:45,291][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 03:55:45,292][233954] FPS: 332935.02[0m
[36m[2023-07-11 03:55:49,543][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:55:49,544][233954] Reward + Measures: [[16.45778547  0.35910201  0.94495296  0.341474    0.88095099  2.15046763]][0m
[37m[1m[2023-07-11 03:55:49,544][233954] Max Reward on eval: 16.457785474606062[0m
[37m[1m[2023-07-11 03:55:49,544][233954] Min Reward on eval: 16.457785474606062[0m
[37m[1m[2023-07-11 03:55:49,545][233954] Mean Reward across all agents: 16.457785474606062[0m
[37m[1m[2023-07-11 03:55:49,545][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:55:54,552][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:55:54,553][233954] Reward + Measures: [[  7.34853543   0.65420002   0.89589995   0.7252       0.88459998
    1.66606259]
 [-14.06017399   0.60900003   0.81980002   0.67180008   0.80270004
    1.75917363]
 [-21.8024545    0.49000001   0.68110007   0.5794       0.64040005
    1.77414548]
 ...
 [-29.71281552   0.60780001   0.77899998   0.69620007   0.73570001
    1.8536762 ]
 [ -9.89573283   0.37280002   0.60799998   0.46720001   0.5927
    1.92540383]
 [ 46.82409692   0.60669994   0.94399995   0.71950001   0.92810005
    1.82045257]][0m
[37m[1m[2023-07-11 03:55:54,553][233954] Max Reward on eval: 93.20392001513392[0m
[37m[1m[2023-07-11 03:55:54,553][233954] Min Reward on eval: -72.20592002235352[0m
[37m[1m[2023-07-11 03:55:54,554][233954] Mean Reward across all agents: 7.999372340263839[0m
[37m[1m[2023-07-11 03:55:54,554][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:55:54,561][233954] mean_value=101.60179800926463, max_value=552.8056860340387[0m
[37m[1m[2023-07-11 03:55:54,564][233954] New mean coefficients: [[ 2.9693727  3.7200553  3.509903   5.787072   1.6903508 -0.8227344]][0m
[37m[1m[2023-07-11 03:55:54,565][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:56:03,526][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 03:56:03,526][233954] FPS: 428614.29[0m
[36m[2023-07-11 03:56:03,528][233954] itr=305, itrs=2000, Progress: 15.25%[0m
[36m[2023-07-11 03:56:15,276][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 03:56:15,276][233954] FPS: 329030.72[0m
[36m[2023-07-11 03:56:19,525][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:56:19,526][233954] Reward + Measures: [[-0.61375546  0.99137163  0.99317127  0.99087238  0.99280232  2.51550984]][0m
[37m[1m[2023-07-11 03:56:19,526][233954] Max Reward on eval: -0.6137554579198865[0m
[37m[1m[2023-07-11 03:56:19,526][233954] Min Reward on eval: -0.6137554579198865[0m
[37m[1m[2023-07-11 03:56:19,526][233954] Mean Reward across all agents: -0.6137554579198865[0m
[37m[1m[2023-07-11 03:56:19,527][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:56:24,786][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:56:24,786][233954] Reward + Measures: [[-66.6574291    0.99410003   0.98769999   0.99060005   0.99420005
    3.07679558]
 [ -8.33917744   0.6397       0.84860003   0.84769994   0.85420001
    2.80689216]
 [-31.94702651   0.99169999   0.99180001   0.9896       0.99430001
    2.88130546]
 ...
 [-37.25304575   0.97590011   0.97910005   0.98470002   0.991
    3.40896726]
 [-52.16145742   0.99340004   0.98710006   0.98970002   0.99419993
    3.13544345]
 [ 75.53260021   0.97160006   0.97960007   0.98530006   0.99119997
    3.37479949]][0m
[37m[1m[2023-07-11 03:56:24,786][233954] Max Reward on eval: 124.2905700225383[0m
[37m[1m[2023-07-11 03:56:24,787][233954] Min Reward on eval: -158.67837595031597[0m
[37m[1m[2023-07-11 03:56:24,787][233954] Mean Reward across all agents: -39.93323360788105[0m
[37m[1m[2023-07-11 03:56:24,787][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:56:24,792][233954] mean_value=144.9970625438146, max_value=562.7365158322453[0m
[37m[1m[2023-07-11 03:56:24,794][233954] New mean coefficients: [[ 3.2917562  3.6873252  4.600505   6.812198   1.7337943 -0.6859208]][0m
[37m[1m[2023-07-11 03:56:24,795][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:56:33,867][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 03:56:33,867][233954] FPS: 423377.55[0m
[36m[2023-07-11 03:56:33,869][233954] itr=306, itrs=2000, Progress: 15.30%[0m
[36m[2023-07-11 03:56:45,519][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 03:56:45,520][233954] FPS: 331808.53[0m
[36m[2023-07-11 03:56:49,726][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:56:49,726][233954] Reward + Measures: [[10.57492505  0.99071103  0.99436963  0.99222493  0.99177325  2.57989049]][0m
[37m[1m[2023-07-11 03:56:49,726][233954] Max Reward on eval: 10.574925046844815[0m
[37m[1m[2023-07-11 03:56:49,727][233954] Min Reward on eval: 10.574925046844815[0m
[37m[1m[2023-07-11 03:56:49,727][233954] Mean Reward across all agents: 10.574925046844815[0m
[37m[1m[2023-07-11 03:56:49,727][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:56:54,669][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:56:54,670][233954] Reward + Measures: [[ -1.47821331   0.2552       0.84720004   0.87730008   0.90879995
    3.28962016]
 [ 73.70185139   0.2879       0.87190002   0.90249997   0.94340003
    3.31723022]
 [ 34.18580434   0.70050001   0.93370003   0.94169998   0.95310003
    2.89143586]
 ...
 [101.01228702   0.65620005   0.97270006   0.95819998   0.95520002
    3.28269053]
 [ 73.50801539   0.46510002   0.95710003   0.95570004   0.96919996
    3.37695575]
 [-23.58224706   0.296        0.84470004   0.87279999   0.91590005
    3.30631566]][0m
[37m[1m[2023-07-11 03:56:54,670][233954] Max Reward on eval: 102.79011540529318[0m
[37m[1m[2023-07-11 03:56:54,671][233954] Min Reward on eval: -120.83308769408613[0m
[37m[1m[2023-07-11 03:56:54,671][233954] Mean Reward across all agents: 20.217013226010586[0m
[37m[1m[2023-07-11 03:56:54,671][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:56:54,680][233954] mean_value=315.42238383569514, max_value=601.5734285184183[0m
[37m[1m[2023-07-11 03:56:54,683][233954] New mean coefficients: [[ 3.8572168   4.196762    5.2083225   6.9379435   2.525405   -0.06860816]][0m
[37m[1m[2023-07-11 03:56:54,684][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:57:03,639][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 03:57:03,639][233954] FPS: 428897.15[0m
[36m[2023-07-11 03:57:03,641][233954] itr=307, itrs=2000, Progress: 15.35%[0m
[36m[2023-07-11 03:57:15,314][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 03:57:15,314][233954] FPS: 331015.86[0m
[36m[2023-07-11 03:57:19,654][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:57:19,655][233954] Reward + Measures: [[19.5341973   0.99004132  0.99392062  0.99105501  0.99254996  2.53900433]][0m
[37m[1m[2023-07-11 03:57:19,655][233954] Max Reward on eval: 19.53419730428715[0m
[37m[1m[2023-07-11 03:57:19,655][233954] Min Reward on eval: 19.53419730428715[0m
[37m[1m[2023-07-11 03:57:19,655][233954] Mean Reward across all agents: 19.53419730428715[0m
[37m[1m[2023-07-11 03:57:19,656][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:57:24,633][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:57:24,633][233954] Reward + Measures: [[  8.28953667   0.82209998   0.9835       0.97690004   0.98940003
    2.17436528]
 [-16.24510834   0.0007       0.94239998   0.85820001   0.93599999
    2.38214469]
 [  2.60620118   0.98639995   0.99309999   0.9891001    0.99119997
    2.17285585]
 ...
 [-29.18927798   0.98250002   0.99249995   0.991        0.99280006
    2.61019683]
 [ 23.27441281   0.79179996   0.96439999   0.96199989   0.97109997
    2.33037806]
 [ 38.17097447   0.13900001   0.87000006   0.87010002   0.94889992
    2.47321939]][0m
[37m[1m[2023-07-11 03:57:24,633][233954] Max Reward on eval: 81.99112242078408[0m
[37m[1m[2023-07-11 03:57:24,634][233954] Min Reward on eval: -66.27811531722546[0m
[37m[1m[2023-07-11 03:57:24,634][233954] Mean Reward across all agents: 11.174253617756767[0m
[37m[1m[2023-07-11 03:57:24,634][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:57:24,638][233954] mean_value=17.689046447964028, max_value=557.5960240300744[0m
[37m[1m[2023-07-11 03:57:24,640][233954] New mean coefficients: [[4.845142   4.9365463  6.721361   8.036176   3.2059138  0.82797724]][0m
[37m[1m[2023-07-11 03:57:24,641][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:57:33,634][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 03:57:33,634][233954] FPS: 427090.84[0m
[36m[2023-07-11 03:57:33,636][233954] itr=308, itrs=2000, Progress: 15.40%[0m
[36m[2023-07-11 03:57:45,160][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 03:57:45,161][233954] FPS: 335340.69[0m
[36m[2023-07-11 03:57:49,445][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:57:49,446][233954] Reward + Measures: [[-21.89208642   0.98638391   0.99391299   0.98831362   0.99180031
    2.73854351]][0m
[37m[1m[2023-07-11 03:57:49,446][233954] Max Reward on eval: -21.892086415432356[0m
[37m[1m[2023-07-11 03:57:49,446][233954] Min Reward on eval: -21.892086415432356[0m
[37m[1m[2023-07-11 03:57:49,446][233954] Mean Reward across all agents: -21.892086415432356[0m
[37m[1m[2023-07-11 03:57:49,447][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:57:54,414][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:57:54,414][233954] Reward + Measures: [[-89.89389028   0.73979998   0.51459998   0.71519995   0.71080005
    2.93146873]
 [-52.27490091   0.90189999   0.80979997   0.87830001   0.91040003
    2.78959346]
 [  3.94967127   0.90079993   0.85260004   0.88440001   0.90910006
    2.6931169 ]
 ...
 [-64.07554378   0.88889998   0.76930004   0.86149997   0.89209998
    2.76141715]
 [ -8.66004434   0.82690001   0.74690002   0.79909992   0.86149997
    2.81655335]
 [-25.52664351   0.93289995   0.91749996   0.91429996   0.93669999
    2.39735937]][0m
[37m[1m[2023-07-11 03:57:54,415][233954] Max Reward on eval: 659.786396022886[0m
[37m[1m[2023-07-11 03:57:54,415][233954] Min Reward on eval: -110.80253751482815[0m
[37m[1m[2023-07-11 03:57:54,415][233954] Mean Reward across all agents: -10.19596535722038[0m
[37m[1m[2023-07-11 03:57:54,415][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:57:54,426][233954] mean_value=152.8203194962759, max_value=951.4525796566224[0m
[37m[1m[2023-07-11 03:57:54,429][233954] New mean coefficients: [[6.653126  5.3167896 7.780666  8.834628  3.3634832 1.4748861]][0m
[37m[1m[2023-07-11 03:57:54,430][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:58:03,352][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 03:58:03,352][233954] FPS: 430445.68[0m
[36m[2023-07-11 03:58:03,355][233954] itr=309, itrs=2000, Progress: 15.45%[0m
[36m[2023-07-11 03:58:14,948][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 03:58:14,948][233954] FPS: 333424.26[0m
[36m[2023-07-11 03:58:19,194][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:58:19,195][233954] Reward + Measures: [[-85.34189319   0.98909062   0.99106103   0.98163134   0.98720831
    2.63858294]][0m
[37m[1m[2023-07-11 03:58:19,195][233954] Max Reward on eval: -85.3418931881414[0m
[37m[1m[2023-07-11 03:58:19,195][233954] Min Reward on eval: -85.3418931881414[0m
[37m[1m[2023-07-11 03:58:19,196][233954] Mean Reward across all agents: -85.3418931881414[0m
[37m[1m[2023-07-11 03:58:19,196][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:58:24,143][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 03:58:24,144][233954] Reward + Measures: [[ 39.38928569   0.63030005   0.71329999   0.75370002   0.78349996
    3.64694285]
 [-78.94819023   0.88160002   0.90630001   0.93189996   0.71810001
    3.63189769]
 [ 14.2970867    0.73760003   0.73940003   0.78330004   0.77460003
    3.89366388]
 ...
 [749.93531035   0.           0.99090004   0.98009998   0.99560004
    3.60531116]
 [-77.27296446   0.87929994   0.86949998   0.90900004   0.78980005
    3.7616837 ]
 [ 86.10563089   0.71930003   0.90289992   0.90010005   0.94160002
    3.51266408]][0m
[37m[1m[2023-07-11 03:58:24,144][233954] Max Reward on eval: 810.1347885259427[0m
[37m[1m[2023-07-11 03:58:24,144][233954] Min Reward on eval: -292.96402549310585[0m
[37m[1m[2023-07-11 03:58:24,144][233954] Mean Reward across all agents: 90.9159461419521[0m
[37m[1m[2023-07-11 03:58:24,145][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 03:58:24,157][233954] mean_value=359.9608431618221, max_value=1310.1347885259427[0m
[37m[1m[2023-07-11 03:58:24,160][233954] New mean coefficients: [[6.4538655 4.308752  5.831869  8.398772  2.2949028 0.8043229]][0m
[37m[1m[2023-07-11 03:58:24,161][233954] Moving the mean solution point...[0m
[36m[2023-07-11 03:58:33,061][233954] train() took 8.90 seconds to complete[0m
[36m[2023-07-11 03:58:33,062][233954] FPS: 431515.64[0m
[36m[2023-07-11 03:58:33,064][233954] itr=310, itrs=2000, Progress: 15.50%[0m
[37m[1m[2023-07-11 04:01:41,029][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000290[0m
[36m[2023-07-11 04:01:53,311][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 04:01:53,311][233954] FPS: 332133.30[0m
[36m[2023-07-11 04:01:57,585][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:01:57,585][233954] Reward + Measures: [[-74.99457401   0.99068594   0.99348092   0.98349333   0.989784
    2.71072888]][0m
[37m[1m[2023-07-11 04:01:57,585][233954] Max Reward on eval: -74.99457400919555[0m
[37m[1m[2023-07-11 04:01:57,586][233954] Min Reward on eval: -74.99457400919555[0m
[37m[1m[2023-07-11 04:01:57,586][233954] Mean Reward across all agents: -74.99457400919555[0m
[37m[1m[2023-07-11 04:01:57,586][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:02:02,470][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:02:02,479][233954] Reward + Measures: [[ 16.03236549   0.9788       0.99300003   0.98920006   0.98759997
    2.80189013]
 [-76.89606448   0.98079997   0.9867       0.96429998   0.98360008
    2.77851439]
 [-98.64627241   0.98670006   0.99190009   0.98430008   0.98930007
    2.90115213]
 ...
 [ 49.43033863   0.95430005   0.98999995   0.98750001   0.97980005
    2.88461447]
 [ 46.20279194   0.96900004   0.99209994   0.98859996   0.98710006
    2.83414149]
 [-39.48248994   0.97610009   0.98620003   0.97259998   0.98309994
    2.55675793]][0m
[37m[1m[2023-07-11 04:02:02,479][233954] Max Reward on eval: 315.7273435186595[0m
[37m[1m[2023-07-11 04:02:02,479][233954] Min Reward on eval: -203.94888877943157[0m
[37m[1m[2023-07-11 04:02:02,479][233954] Mean Reward across all agents: -22.573095233199343[0m
[37m[1m[2023-07-11 04:02:02,480][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:02:02,482][233954] mean_value=-77.11172470615223, max_value=686.4098071910441[0m
[37m[1m[2023-07-11 04:02:02,485][233954] New mean coefficients: [[7.85906   5.844741  8.811292  9.864336  4.6341043 2.594082 ]][0m
[37m[1m[2023-07-11 04:02:02,486][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:02:11,542][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 04:02:11,542][233954] FPS: 424097.63[0m
[36m[2023-07-11 04:02:11,544][233954] itr=311, itrs=2000, Progress: 15.55%[0m
[36m[2023-07-11 04:02:23,268][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 04:02:23,268][233954] FPS: 329613.19[0m
[36m[2023-07-11 04:02:27,512][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:02:27,518][233954] Reward + Measures: [[72.98142721  0.00388033  0.99586463  0.663809    0.99277198  2.9627974 ]][0m
[37m[1m[2023-07-11 04:02:27,518][233954] Max Reward on eval: 72.98142721178615[0m
[37m[1m[2023-07-11 04:02:27,518][233954] Min Reward on eval: 72.98142721178615[0m
[37m[1m[2023-07-11 04:02:27,519][233954] Mean Reward across all agents: 72.98142721178615[0m
[37m[1m[2023-07-11 04:02:27,519][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:02:32,519][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:02:32,525][233954] Reward + Measures: [[-241.95394722    0.64480001    0.66180009    0.67550004    0.61480004
     3.09755445]
 [ -98.32119588    0.1323        0.66210002    0.57260001    0.62169999
     3.0280304 ]
 [  30.18195084    0.43829998    0.48070002    0.51430005    0.45580003
     3.16071892]
 ...
 [-165.54426091    0.29860002    0.55109996    0.3743        0.49259996
     2.64149213]
 [-104.59825295    0.1869        0.68580002    0.51210004    0.62360001
     2.96206093]
 [-144.77362436    0.1762        0.1999        0.20850001    0.1892
     3.14400935]][0m
[37m[1m[2023-07-11 04:02:32,525][233954] Max Reward on eval: 114.21457213182002[0m
[37m[1m[2023-07-11 04:02:32,525][233954] Min Reward on eval: -450.1782817684114[0m
[37m[1m[2023-07-11 04:02:32,525][233954] Mean Reward across all agents: -119.37333576866129[0m
[37m[1m[2023-07-11 04:02:32,526][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:02:32,531][233954] mean_value=-144.64960646772315, max_value=534.3147780186497[0m
[37m[1m[2023-07-11 04:02:32,534][233954] New mean coefficients: [[7.1862936 4.7126465 8.323548  9.101944  2.815663  0.9856106]][0m
[37m[1m[2023-07-11 04:02:32,535][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:02:41,643][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 04:02:41,643][233954] FPS: 421691.93[0m
[36m[2023-07-11 04:02:41,645][233954] itr=312, itrs=2000, Progress: 15.60%[0m
[36m[2023-07-11 04:02:53,414][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 04:02:53,414][233954] FPS: 328314.01[0m
[36m[2023-07-11 04:02:57,760][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:02:57,765][233954] Reward + Measures: [[113.78193006   0.05204567   0.96487898   0.70906126   0.96731037
    3.08371902]][0m
[37m[1m[2023-07-11 04:02:57,765][233954] Max Reward on eval: 113.7819300629806[0m
[37m[1m[2023-07-11 04:02:57,766][233954] Min Reward on eval: 113.7819300629806[0m
[37m[1m[2023-07-11 04:02:57,766][233954] Mean Reward across all agents: 113.7819300629806[0m
[37m[1m[2023-07-11 04:02:57,766][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:03:02,796][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:03:02,802][233954] Reward + Measures: [[118.92098024   0.4192       0.34549999   0.46120006   0.43280002
    3.14613318]
 [-44.16084375   0.67420006   0.77630007   0.82849997   0.76380002
    3.35266614]
 [-77.19901752   0.88700008   0.96649992   0.96519995   0.92249995
    3.4479568 ]
 ...
 [110.75314138   0.45499998   0.40600005   0.57600003   0.52920002
    3.15532804]
 [ 93.11692586   0.40459999   0.28279999   0.5183       0.49239999
    3.01078105]
 [ 57.50287918   0.38519999   0.3396       0.41580001   0.3928
    3.15797687]][0m
[37m[1m[2023-07-11 04:03:02,802][233954] Max Reward on eval: 639.040813440457[0m
[37m[1m[2023-07-11 04:03:02,803][233954] Min Reward on eval: -189.11613846044057[0m
[37m[1m[2023-07-11 04:03:02,803][233954] Mean Reward across all agents: 85.72012146529228[0m
[37m[1m[2023-07-11 04:03:02,803][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:03:02,820][233954] mean_value=209.7109647470096, max_value=900.6160745522008[0m
[37m[1m[2023-07-11 04:03:02,823][233954] New mean coefficients: [[6.8025436  4.5792737  6.982525   9.519526   2.6231923  0.86713636]][0m
[37m[1m[2023-07-11 04:03:02,824][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:03:11,837][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 04:03:11,838][233954] FPS: 426092.47[0m
[36m[2023-07-11 04:03:11,840][233954] itr=313, itrs=2000, Progress: 15.65%[0m
[36m[2023-07-11 04:03:23,448][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 04:03:23,448][233954] FPS: 332912.82[0m
[36m[2023-07-11 04:03:27,718][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:03:27,719][233954] Reward + Measures: [[149.26882524   0.00345667   0.9967137    0.66582203   0.99487627
    3.40220499]][0m
[37m[1m[2023-07-11 04:03:27,719][233954] Max Reward on eval: 149.2688252444778[0m
[37m[1m[2023-07-11 04:03:27,719][233954] Min Reward on eval: 149.2688252444778[0m
[37m[1m[2023-07-11 04:03:27,720][233954] Mean Reward across all agents: 149.2688252444778[0m
[37m[1m[2023-07-11 04:03:27,720][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:03:32,706][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:03:32,707][233954] Reward + Measures: [[ 53.22565299   0.39200002   0.18620001   0.35560003   0.20570002
    2.7217958 ]
 [-45.37685773   0.3211       0.1441       0.33540002   0.2325
    2.79596019]
 [-39.39179705   0.9465       0.31509998   0.95590001   0.49320003
    3.84117365]
 ...
 [ -1.44161744   0.46169996   0.42920002   0.43379998   0.32370001
    2.45564127]
 [-40.67633482   0.32089999   0.10290001   0.30370003   0.1876
    2.81177211]
 [100.65986827   0.4111       0.25960001   0.43199998   0.36569998
    2.56372929]][0m
[37m[1m[2023-07-11 04:03:32,707][233954] Max Reward on eval: 445.2297745271586[0m
[37m[1m[2023-07-11 04:03:32,707][233954] Min Reward on eval: -183.6583042340586[0m
[37m[1m[2023-07-11 04:03:32,708][233954] Mean Reward across all agents: 73.15317500442626[0m
[37m[1m[2023-07-11 04:03:32,708][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:03:32,718][233954] mean_value=142.71264009810088, max_value=879.183660210669[0m
[37m[1m[2023-07-11 04:03:32,720][233954] New mean coefficients: [[ 7.0347953  5.6745443  7.849791  10.328563   4.1431885  2.112106 ]][0m
[37m[1m[2023-07-11 04:03:32,721][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:03:41,755][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 04:03:41,756][233954] FPS: 425130.75[0m
[36m[2023-07-11 04:03:41,758][233954] itr=314, itrs=2000, Progress: 15.70%[0m
[36m[2023-07-11 04:03:53,401][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 04:03:53,402][233954] FPS: 331945.76[0m
[36m[2023-07-11 04:03:57,756][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:03:57,761][233954] Reward + Measures: [[550.36926378   0.00253067   0.99678195   0.7479564    0.99619842
    3.17297792]][0m
[37m[1m[2023-07-11 04:03:57,762][233954] Max Reward on eval: 550.3692637831408[0m
[37m[1m[2023-07-11 04:03:57,762][233954] Min Reward on eval: 550.3692637831408[0m
[37m[1m[2023-07-11 04:03:57,762][233954] Mean Reward across all agents: 550.3692637831408[0m
[37m[1m[2023-07-11 04:03:57,763][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:04:03,044][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:04:03,045][233954] Reward + Measures: [[-427.51676559    0.87650007    0.99560004    0.07300001    0.98870003
     3.67384219]
 [ 507.85606004    0.22230001    0.94210005    0.52430004    0.95889997
     3.6721096 ]
 [ 498.71260643    0.1974        0.86510003    0.55129999    0.89399999
     3.89427543]
 ...
 [-482.0138893     0.87939996    0.99550003    0.0895        0.98899996
     3.94022298]
 [  19.0284944     0.52609998    0.81469995    0.13870001    0.80979997
     3.49485707]
 [-479.29238797    0.81710005    0.99379998    0.089         0.98509997
     3.71703386]][0m
[37m[1m[2023-07-11 04:04:03,045][233954] Max Reward on eval: 630.352496627951[0m
[37m[1m[2023-07-11 04:04:03,045][233954] Min Reward on eval: -766.394905085268[0m
[37m[1m[2023-07-11 04:04:03,046][233954] Mean Reward across all agents: -11.037884923910882[0m
[37m[1m[2023-07-11 04:04:03,046][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:04:03,057][233954] mean_value=276.83345736695713, max_value=1130.352496627951[0m
[37m[1m[2023-07-11 04:04:03,060][233954] New mean coefficients: [[ 7.8140554  6.8862276 10.988419  10.912478   5.4137964  2.7027853]][0m
[37m[1m[2023-07-11 04:04:03,061][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:04:12,134][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 04:04:12,134][233954] FPS: 423330.78[0m
[36m[2023-07-11 04:04:12,136][233954] itr=315, itrs=2000, Progress: 15.75%[0m
[36m[2023-07-11 04:04:23,881][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 04:04:23,881][233954] FPS: 329051.11[0m
[36m[2023-07-11 04:04:28,106][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:04:28,106][233954] Reward + Measures: [[39.07692333  0.00585267  0.99359667  0.68948436  0.99058968  3.13121963]][0m
[37m[1m[2023-07-11 04:04:28,107][233954] Max Reward on eval: 39.07692333437943[0m
[37m[1m[2023-07-11 04:04:28,107][233954] Min Reward on eval: 39.07692333437943[0m
[37m[1m[2023-07-11 04:04:28,107][233954] Mean Reward across all agents: 39.07692333437943[0m
[37m[1m[2023-07-11 04:04:28,107][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:04:33,067][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:04:33,068][233954] Reward + Measures: [[-214.66440488    0.55520004    0.38440001    0.36320004    0.18970001
     2.94450355]
 [ -24.85891098    0.41050002    0.31430003    0.39410001    0.1224
     3.42084575]
 [-121.95217848    0.53750002    0.39640003    0.4384        0.1391
     3.15263534]
 ...
 [ -41.94022609    0.32879999    0.27100003    0.31680003    0.1336
     3.52524042]
 [-155.42438125    0.7676        0.55269998    0.63050002    0.09020001
     3.2955544 ]
 [ -66.71442044    0.39499995    0.32249999    0.36230001    0.1199
     3.41806149]][0m
[37m[1m[2023-07-11 04:04:33,068][233954] Max Reward on eval: -7.131376929581165[0m
[37m[1m[2023-07-11 04:04:33,068][233954] Min Reward on eval: -345.86941911168395[0m
[37m[1m[2023-07-11 04:04:33,069][233954] Mean Reward across all agents: -105.76992360461108[0m
[37m[1m[2023-07-11 04:04:33,069][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:04:33,072][233954] mean_value=-131.737823705368, max_value=431.1109280354343[0m
[37m[1m[2023-07-11 04:04:33,074][233954] New mean coefficients: [[ 6.8733554  6.7795253  9.249504  10.402277   5.386812   2.3584962]][0m
[37m[1m[2023-07-11 04:04:33,075][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:04:42,108][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 04:04:42,108][233954] FPS: 425209.69[0m
[36m[2023-07-11 04:04:42,110][233954] itr=316, itrs=2000, Progress: 15.80%[0m
[36m[2023-07-11 04:04:53,747][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 04:04:53,747][233954] FPS: 332157.40[0m
[36m[2023-07-11 04:04:58,006][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:04:58,006][233954] Reward + Measures: [[80.23798825  0.019036    0.98779994  0.65839165  0.98295134  3.15252948]][0m
[37m[1m[2023-07-11 04:04:58,006][233954] Max Reward on eval: 80.23798825460726[0m
[37m[1m[2023-07-11 04:04:58,007][233954] Min Reward on eval: 80.23798825460726[0m
[37m[1m[2023-07-11 04:04:58,007][233954] Mean Reward across all agents: 80.23798825460726[0m
[37m[1m[2023-07-11 04:04:58,007][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:05:02,975][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:05:02,975][233954] Reward + Measures: [[ -89.13818987    0.30580002    0.32179999    0.39990002    0.4285
     3.17225361]
 [ -21.80591658    0.54809999    0.1517        0.59630001    0.58660001
     3.29907799]
 [ -48.44446154    0.23559999    0.23959999    0.25330004    0.2545
     3.2885716 ]
 ...
 [ -62.38619117    0.34489998    0.21710001    0.3653        0.34870002
     3.1133256 ]
 [ -44.60043996    0.27860001    0.24600001    0.3434        0.3339
     3.31663013]
 [-135.90036203    0.4377        0.303         0.44280002    0.46480003
     3.05571389]][0m
[37m[1m[2023-07-11 04:05:02,975][233954] Max Reward on eval: 566.0779747992754[0m
[37m[1m[2023-07-11 04:05:02,976][233954] Min Reward on eval: -296.6182198787108[0m
[37m[1m[2023-07-11 04:05:02,976][233954] Mean Reward across all agents: -62.79302496752474[0m
[37m[1m[2023-07-11 04:05:02,976][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:05:02,982][233954] mean_value=-168.6567662246665, max_value=737.0417928929616[0m
[37m[1m[2023-07-11 04:05:02,984][233954] New mean coefficients: [[ 7.06339    6.2369776  9.013537  10.38618    4.933858   2.6221352]][0m
[37m[1m[2023-07-11 04:05:02,985][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:05:11,935][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 04:05:11,936][233954] FPS: 429126.91[0m
[36m[2023-07-11 04:05:11,938][233954] itr=317, itrs=2000, Progress: 15.85%[0m
[36m[2023-07-11 04:05:23,468][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 04:05:23,469][233954] FPS: 335313.76[0m
[36m[2023-07-11 04:05:27,838][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:05:27,839][233954] Reward + Measures: [[88.22869804  0.019877    0.99020767  0.65960801  0.98766863  3.14231348]][0m
[37m[1m[2023-07-11 04:05:27,839][233954] Max Reward on eval: 88.22869803882493[0m
[37m[1m[2023-07-11 04:05:27,839][233954] Min Reward on eval: 88.22869803882493[0m
[37m[1m[2023-07-11 04:05:27,840][233954] Mean Reward across all agents: 88.22869803882493[0m
[37m[1m[2023-07-11 04:05:27,840][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:05:32,917][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:05:32,918][233954] Reward + Measures: [[102.50794467   0.29240003   0.48300001   0.0823       0.4341
    3.24495935]
 [317.00662109   0.66500002   0.7281       0.0398       0.72100002
    3.49419403]
 [-46.37061704   0.97550005   0.97170001   0.97680008   0.97119999
    3.24834991]
 ...
 [226.73730678   0.67300004   0.80370009   0.04730001   0.78060001
    3.42341208]
 [ -8.45768573   0.87709999   0.95520002   0.88459998   0.94729996
    2.95474887]
 [-20.30188849   0.25209999   0.60680002   0.1947       0.50680006
    3.07079935]][0m
[37m[1m[2023-07-11 04:05:32,918][233954] Max Reward on eval: 333.5233473610133[0m
[37m[1m[2023-07-11 04:05:32,918][233954] Min Reward on eval: -132.86839910447597[0m
[37m[1m[2023-07-11 04:05:32,918][233954] Mean Reward across all agents: 4.52703200677694[0m
[37m[1m[2023-07-11 04:05:32,919][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:05:32,927][233954] mean_value=79.27586333883177, max_value=693.6954170312547[0m
[37m[1m[2023-07-11 04:05:32,930][233954] New mean coefficients: [[ 7.9292693  5.6394753 10.248852   9.915689   4.413546   2.015364 ]][0m
[37m[1m[2023-07-11 04:05:32,931][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:05:42,066][233954] train() took 9.13 seconds to complete[0m
[36m[2023-07-11 04:05:42,066][233954] FPS: 420451.43[0m
[36m[2023-07-11 04:05:42,068][233954] itr=318, itrs=2000, Progress: 15.90%[0m
[36m[2023-07-11 04:05:53,739][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 04:05:53,739][233954] FPS: 331307.42[0m
[36m[2023-07-11 04:05:58,009][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:05:58,009][233954] Reward + Measures: [[87.61526988  0.022876    0.99254435  0.68462533  0.99108696  3.15399337]][0m
[37m[1m[2023-07-11 04:05:58,009][233954] Max Reward on eval: 87.61526988030651[0m
[37m[1m[2023-07-11 04:05:58,010][233954] Min Reward on eval: 87.61526988030651[0m
[37m[1m[2023-07-11 04:05:58,010][233954] Mean Reward across all agents: 87.61526988030651[0m
[37m[1m[2023-07-11 04:05:58,010][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:06:02,973][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:06:02,974][233954] Reward + Measures: [[272.88897158   0.0206       0.74030006   0.70669997   0.70719999
    3.24268651]
 [  8.29640983   0.081        0.773        0.59569997   0.79560006
    3.45843053]
 [462.35332487   0.0379       0.87650007   0.76660001   0.84990007
    3.37327385]
 ...
 [367.30363893   0.0101       0.89489996   0.87220001   0.89180005
    3.48922801]
 [116.21187783   0.29930001   0.6613       0.51020002   0.76550001
    3.62751365]
 [ 34.61811285   0.18440001   0.66259998   0.29949999   0.64029998
    3.36139369]][0m
[37m[1m[2023-07-11 04:06:02,974][233954] Max Reward on eval: 839.1554717989405[0m
[37m[1m[2023-07-11 04:06:02,974][233954] Min Reward on eval: -204.65661813691258[0m
[37m[1m[2023-07-11 04:06:02,975][233954] Mean Reward across all agents: 223.78896929299174[0m
[37m[1m[2023-07-11 04:06:02,975][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:06:02,987][233954] mean_value=228.59333330228085, max_value=1030.3679166395814[0m
[37m[1m[2023-07-11 04:06:02,989][233954] New mean coefficients: [[ 8.30162    6.3263364 10.735827  10.187129   5.052264   2.8808637]][0m
[37m[1m[2023-07-11 04:06:02,990][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:06:11,995][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 04:06:11,995][233954] FPS: 426524.54[0m
[36m[2023-07-11 04:06:11,998][233954] itr=319, itrs=2000, Progress: 15.95%[0m
[36m[2023-07-11 04:06:23,514][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 04:06:23,515][233954] FPS: 335717.66[0m
[36m[2023-07-11 04:06:27,785][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:06:27,785][233954] Reward + Measures: [[127.30221867   0.02504733   0.97280973   0.68799698   0.97334832
    3.35523558]][0m
[37m[1m[2023-07-11 04:06:27,785][233954] Max Reward on eval: 127.30221867372329[0m
[37m[1m[2023-07-11 04:06:27,786][233954] Min Reward on eval: 127.30221867372329[0m
[37m[1m[2023-07-11 04:06:27,786][233954] Mean Reward across all agents: 127.30221867372329[0m
[37m[1m[2023-07-11 04:06:27,786][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:06:32,935][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:06:32,936][233954] Reward + Measures: [[  6.79644045   0.3962       0.88220006   0.46599999   0.90310001
    3.3002305 ]
 [172.02480674   0.54940003   0.51710004   0.38619998   0.26439998
    3.61444855]
 [218.99072645   0.206        0.96189994   0.6074       0.97130007
    3.5633893 ]
 ...
 [319.69013019   0.86199999   0.86230004   0.38820001   0.87449998
    3.39262009]
 [538.68305782   0.43460003   0.75980008   0.2626       0.76969999
    3.43424964]
 [145.86188809   0.41710001   0.479        0.3276       0.3556
    3.49342775]][0m
[37m[1m[2023-07-11 04:06:32,936][233954] Max Reward on eval: 767.1607055615634[0m
[37m[1m[2023-07-11 04:06:32,936][233954] Min Reward on eval: -235.4484644094482[0m
[37m[1m[2023-07-11 04:06:32,936][233954] Mean Reward across all agents: 121.61156139556091[0m
[37m[1m[2023-07-11 04:06:32,937][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:06:32,955][233954] mean_value=324.03564609713777, max_value=1057.4189939303324[0m
[37m[1m[2023-07-11 04:06:32,958][233954] New mean coefficients: [[ 8.477167   6.4265385 11.943983   9.525689   4.648804   2.47379  ]][0m
[37m[1m[2023-07-11 04:06:32,959][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:06:42,047][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 04:06:42,047][233954] FPS: 422602.88[0m
[36m[2023-07-11 04:06:42,050][233954] itr=320, itrs=2000, Progress: 16.00%[0m
[37m[1m[2023-07-11 04:09:51,631][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000300[0m
[36m[2023-07-11 04:10:03,589][233954] train() took 11.37 seconds to complete[0m
[36m[2023-07-11 04:10:03,589][233954] FPS: 337593.07[0m
[36m[2023-07-11 04:10:07,775][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:10:07,775][233954] Reward + Measures: [[160.25401047   0.02888133   0.96456528   0.68493098   0.96675473
    3.62214899]][0m
[37m[1m[2023-07-11 04:10:07,775][233954] Max Reward on eval: 160.25401046504314[0m
[37m[1m[2023-07-11 04:10:07,776][233954] Min Reward on eval: 160.25401046504314[0m
[37m[1m[2023-07-11 04:10:07,776][233954] Mean Reward across all agents: 160.25401046504314[0m
[37m[1m[2023-07-11 04:10:07,776][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:10:12,658][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:10:12,658][233954] Reward + Measures: [[ 12.03484011   0.0081       0.99220002   0.70690006   0.98970002
    3.86423111]
 [ 20.92523685   0.0061       0.98999995   0.71360004   0.98629999
    3.61063051]
 [-50.64519846   0.0049       0.99370003   0.69709998   0.99410003
    3.93925261]
 ...
 [ 96.70054883   0.0046       0.99559993   0.7622       0.9957
    3.99930048]
 [ 32.33522908   0.0055       0.99280006   0.7173       0.99290001
    3.92076182]
 [-20.4858489    0.0046       0.99539995   0.74120003   0.99560004
    3.99960017]][0m
[37m[1m[2023-07-11 04:10:12,659][233954] Max Reward on eval: 280.1318672228605[0m
[37m[1m[2023-07-11 04:10:12,659][233954] Min Reward on eval: -142.30736448769457[0m
[37m[1m[2023-07-11 04:10:12,659][233954] Mean Reward across all agents: 25.079554614152315[0m
[37m[1m[2023-07-11 04:10:12,659][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:10:12,662][233954] mean_value=-347.4641726308444, max_value=671.9852774174628[0m
[37m[1m[2023-07-11 04:10:12,665][233954] New mean coefficients: [[ 8.285876   6.111226  11.044915  10.044944   4.273071   1.7001526]][0m
[37m[1m[2023-07-11 04:10:12,666][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:10:21,690][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 04:10:21,690][233954] FPS: 425580.63[0m
[36m[2023-07-11 04:10:21,693][233954] itr=321, itrs=2000, Progress: 16.05%[0m
[36m[2023-07-11 04:10:33,282][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 04:10:33,282][233954] FPS: 333567.27[0m
[36m[2023-07-11 04:10:37,510][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:10:37,510][233954] Reward + Measures: [[55.72107043  0.021838    0.98015094  0.64158863  0.97592032  3.5335474 ]][0m
[37m[1m[2023-07-11 04:10:37,510][233954] Max Reward on eval: 55.72107042920609[0m
[37m[1m[2023-07-11 04:10:37,511][233954] Min Reward on eval: 55.72107042920609[0m
[37m[1m[2023-07-11 04:10:37,511][233954] Mean Reward across all agents: 55.72107042920609[0m
[37m[1m[2023-07-11 04:10:37,511][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:10:42,442][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:10:42,443][233954] Reward + Measures: [[ 172.85737694    0.16829999    0.43439999    0.2841        0.40240002
     3.72249961]
 [ 114.60726143    0.1815        0.29280001    0.3522        0.36579999
     3.46689868]
 [  61.08047103    0.10089999    0.45720002    0.36180001    0.42339998
     3.70909619]
 ...
 [ 373.5722742     0.41219997    0.82559997    0.37590003    0.80499995
     3.85934448]
 [-126.44697141    0.44779998    0.46409997    0.60690004    0.30360001
     3.71079254]
 [ 236.64452459    0.38869998    0.74920005    0.42580006    0.77740002
     3.82261133]][0m
[37m[1m[2023-07-11 04:10:42,443][233954] Max Reward on eval: 406.7457897330634[0m
[37m[1m[2023-07-11 04:10:42,443][233954] Min Reward on eval: -240.262917892728[0m
[37m[1m[2023-07-11 04:10:42,444][233954] Mean Reward across all agents: 96.1137746241072[0m
[37m[1m[2023-07-11 04:10:42,444][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:10:42,455][233954] mean_value=248.7792393659142, max_value=906.7457897330635[0m
[37m[1m[2023-07-11 04:10:42,457][233954] New mean coefficients: [[7.0351458  5.202942   7.729575   8.927572   3.2331932  0.86352384]][0m
[37m[1m[2023-07-11 04:10:42,458][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:10:51,392][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 04:10:51,392][233954] FPS: 429919.87[0m
[36m[2023-07-11 04:10:51,394][233954] itr=322, itrs=2000, Progress: 16.10%[0m
[36m[2023-07-11 04:11:02,883][233954] train() took 11.41 seconds to complete[0m
[36m[2023-07-11 04:11:02,883][233954] FPS: 336426.57[0m
[36m[2023-07-11 04:11:07,185][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:11:07,186][233954] Reward + Measures: [[77.35721393  0.02336533  0.97582406  0.65367234  0.97337002  3.55817509]][0m
[37m[1m[2023-07-11 04:11:07,186][233954] Max Reward on eval: 77.35721392720687[0m
[37m[1m[2023-07-11 04:11:07,186][233954] Min Reward on eval: 77.35721392720687[0m
[37m[1m[2023-07-11 04:11:07,186][233954] Mean Reward across all agents: 77.35721392720687[0m
[37m[1m[2023-07-11 04:11:07,186][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:11:12,386][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:11:12,387][233954] Reward + Measures: [[140.51389887   0.0093       0.963        0.67740005   0.96969998
    3.97372484]
 [ 94.52586942   0.8915       0.1005       0.93669999   0.91309994
    3.86124969]
 [-23.26650749   0.0131       0.98639995   0.72790003   0.9903
    3.99024773]
 ...
 [-10.97960722   0.17130002   0.82079995   0.76530004   0.96310008
    3.95424128]
 [ 15.32958349   0.13150002   0.82779998   0.72520006   0.93029994
    3.93621683]
 [-40.73831964   0.2036       0.79539996   0.77949995   0.98270005
    3.99690795]][0m
[37m[1m[2023-07-11 04:11:12,387][233954] Max Reward on eval: 417.55130670629444[0m
[37m[1m[2023-07-11 04:11:12,388][233954] Min Reward on eval: -163.73984386092053[0m
[37m[1m[2023-07-11 04:11:12,388][233954] Mean Reward across all agents: 67.4567642916142[0m
[37m[1m[2023-07-11 04:11:12,388][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:11:12,396][233954] mean_value=-66.71610169800893, max_value=903.2303614682518[0m
[37m[1m[2023-07-11 04:11:12,399][233954] New mean coefficients: [[ 8.234259   6.326579  10.8918495  9.755285   4.5628657  1.9190336]][0m
[37m[1m[2023-07-11 04:11:12,400][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:11:21,326][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 04:11:21,327][233954] FPS: 430244.23[0m
[36m[2023-07-11 04:11:21,329][233954] itr=323, itrs=2000, Progress: 16.15%[0m
[36m[2023-07-11 04:11:32,799][233954] train() took 11.39 seconds to complete[0m
[36m[2023-07-11 04:11:32,800][233954] FPS: 337033.08[0m
[36m[2023-07-11 04:11:37,042][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:11:37,042][233954] Reward + Measures: [[118.78744995   0.02521533   0.98144871   0.64298964   0.97616798
    3.66840911]][0m
[37m[1m[2023-07-11 04:11:37,042][233954] Max Reward on eval: 118.78744994519046[0m
[37m[1m[2023-07-11 04:11:37,043][233954] Min Reward on eval: 118.78744994519046[0m
[37m[1m[2023-07-11 04:11:37,043][233954] Mean Reward across all agents: 118.78744994519046[0m
[37m[1m[2023-07-11 04:11:37,043][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:11:42,100][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:11:42,100][233954] Reward + Measures: [[  3.35536755   0.3502       0.19160001   0.32589999   0.23020001
    3.21500468]
 [-46.45394303   0.24190001   0.1473       0.27740002   0.18260001
    3.25204539]
 [ 41.97517687   0.80940002   0.78750002   0.78100002   0.72930002
    3.30130553]
 ...
 [  8.72763396   0.38569999   0.18190001   0.3434       0.23290001
    3.25757599]
 [-27.35399056   0.27250001   0.15440001   0.2766       0.17999999
    3.33179927]
 [ 53.84029702   0.38870001   0.24129999   0.36639997   0.24489999
    3.19614387]][0m
[37m[1m[2023-07-11 04:11:42,101][233954] Max Reward on eval: 335.0782494822517[0m
[37m[1m[2023-07-11 04:11:42,101][233954] Min Reward on eval: -173.46194142645692[0m
[37m[1m[2023-07-11 04:11:42,101][233954] Mean Reward across all agents: 6.2767523269998104[0m
[37m[1m[2023-07-11 04:11:42,101][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:11:42,105][233954] mean_value=-104.37361937741161, max_value=562.0898362805135[0m
[37m[1m[2023-07-11 04:11:42,107][233954] New mean coefficients: [[6.736577   5.089719   8.211077   8.721118   3.3475962  0.29945362]][0m
[37m[1m[2023-07-11 04:11:42,108][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:11:51,030][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 04:11:51,030][233954] FPS: 430516.02[0m
[36m[2023-07-11 04:11:51,032][233954] itr=324, itrs=2000, Progress: 16.20%[0m
[36m[2023-07-11 04:12:02,645][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 04:12:02,645][233954] FPS: 332911.24[0m
[36m[2023-07-11 04:12:06,993][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:12:06,994][233954] Reward + Measures: [[72.35839981  0.045632    0.98138034  0.59335631  0.96513373  3.71312571]][0m
[37m[1m[2023-07-11 04:12:06,994][233954] Max Reward on eval: 72.358399814203[0m
[37m[1m[2023-07-11 04:12:06,994][233954] Min Reward on eval: 72.358399814203[0m
[37m[1m[2023-07-11 04:12:06,995][233954] Mean Reward across all agents: 72.358399814203[0m
[37m[1m[2023-07-11 04:12:06,995][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:12:12,063][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:12:12,064][233954] Reward + Measures: [[-358.20397568    0.81960005    0.27590001    0.84729999    0.58039999
     3.68881392]
 [-235.08704757    0.86730003    0.80550003    0.85430002    0.22600003
     3.4910264 ]
 [-111.14091922    0.26160002    0.9709        0.27280003    0.96160001
     3.05682969]
 ...
 [-118.72690153    0.72510004    0.54940003    0.72650003    0.37040001
     3.26224756]
 [-204.47801589    0.85589999    0.85760003    0.87330002    0.84210008
     3.91146326]
 [-240.54949093    0.84629995    0.85080004    0.87          0.82520002
     3.89388132]][0m
[37m[1m[2023-07-11 04:12:12,064][233954] Max Reward on eval: 336.38976861275734[0m
[37m[1m[2023-07-11 04:12:12,065][233954] Min Reward on eval: -652.4094848522916[0m
[37m[1m[2023-07-11 04:12:12,065][233954] Mean Reward across all agents: -180.4471241720515[0m
[37m[1m[2023-07-11 04:12:12,065][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:12:12,073][233954] mean_value=-55.09439247707435, max_value=591.639184904471[0m
[37m[1m[2023-07-11 04:12:12,076][233954] New mean coefficients: [[7.478341   4.6327906  9.402152   8.663773   2.9156218  0.10066438]][0m
[37m[1m[2023-07-11 04:12:12,077][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:12:21,212][233954] train() took 9.13 seconds to complete[0m
[36m[2023-07-11 04:12:21,213][233954] FPS: 420410.16[0m
[36m[2023-07-11 04:12:21,215][233954] itr=325, itrs=2000, Progress: 16.25%[0m
[36m[2023-07-11 04:12:32,976][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 04:12:32,976][233954] FPS: 328613.57[0m
[36m[2023-07-11 04:12:37,261][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:12:37,261][233954] Reward + Measures: [[85.2882014   0.04391433  0.98024172  0.60441899  0.96743292  3.7387948 ]][0m
[37m[1m[2023-07-11 04:12:37,262][233954] Max Reward on eval: 85.28820139800699[0m
[37m[1m[2023-07-11 04:12:37,262][233954] Min Reward on eval: 85.28820139800699[0m
[37m[1m[2023-07-11 04:12:37,262][233954] Mean Reward across all agents: 85.28820139800699[0m
[37m[1m[2023-07-11 04:12:37,262][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:12:42,273][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:12:42,274][233954] Reward + Measures: [[ 59.28957746   0.53610003   0.42730004   0.22230001   0.55489999
    3.77276158]
 [-32.89101695   0.24509998   0.35330003   0.3348       0.2053
    3.51748085]
 [ 41.19648793   0.16069999   0.10570001   0.15869999   0.12880002
    3.31827784]
 ...
 [-14.80845399   0.199        0.1804       0.1242       0.09000001
    3.45812035]
 [-54.84544463   0.64560002   0.48620006   0.3064       0.64359999
    3.53052711]
 [160.29158397   0.0362       0.29640001   0.30599999   0.2366
    3.56039429]][0m
[37m[1m[2023-07-11 04:12:42,274][233954] Max Reward on eval: 395.95860861931[0m
[37m[1m[2023-07-11 04:12:42,274][233954] Min Reward on eval: -227.75195989683272[0m
[37m[1m[2023-07-11 04:12:42,275][233954] Mean Reward across all agents: 33.41698837939774[0m
[37m[1m[2023-07-11 04:12:42,275][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:12:42,285][233954] mean_value=77.04465601598542, max_value=635.4411460854835[0m
[37m[1m[2023-07-11 04:12:42,288][233954] New mean coefficients: [[6.9162865  4.676259   7.687257   7.845941   2.945292   0.05356664]][0m
[37m[1m[2023-07-11 04:12:42,288][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:12:51,340][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 04:12:51,341][233954] FPS: 424302.48[0m
[36m[2023-07-11 04:12:51,343][233954] itr=326, itrs=2000, Progress: 16.30%[0m
[36m[2023-07-11 04:13:03,067][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 04:13:03,067][233954] FPS: 329708.72[0m
[36m[2023-07-11 04:13:07,263][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:13:07,263][233954] Reward + Measures: [[35.63290805  0.04387133  0.99104571  0.55655897  0.97505069  3.70535731]][0m
[37m[1m[2023-07-11 04:13:07,263][233954] Max Reward on eval: 35.632908053676985[0m
[37m[1m[2023-07-11 04:13:07,264][233954] Min Reward on eval: 35.632908053676985[0m
[37m[1m[2023-07-11 04:13:07,264][233954] Mean Reward across all agents: 35.632908053676985[0m
[37m[1m[2023-07-11 04:13:07,264][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:13:12,227][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:13:12,227][233954] Reward + Measures: [[-38.7798412    0.31380001   0.29410002   0.1779       0.32000002
    3.00568366]
 [-60.81868947   0.81910002   0.74200004   0.2139       0.6814
    3.63635325]
 [-81.348092     0.29840001   0.2507       0.17779998   0.30240002
    3.06100774]
 ...
 [-40.02693914   0.16419999   0.1266       0.1245       0.1621
    3.24246907]
 [-89.41514041   0.19829999   0.1786       0.14229999   0.21660002
    3.2827363 ]
 [-89.87617059   0.17190002   0.1841       0.16110002   0.17690001
    3.3674202 ]][0m
[37m[1m[2023-07-11 04:13:12,228][233954] Max Reward on eval: 232.2599687822163[0m
[37m[1m[2023-07-11 04:13:12,228][233954] Min Reward on eval: -299.05182050541043[0m
[37m[1m[2023-07-11 04:13:12,228][233954] Mean Reward across all agents: -63.328935879653294[0m
[37m[1m[2023-07-11 04:13:12,228][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:13:12,234][233954] mean_value=-60.29492154908381, max_value=652.6466906091011[0m
[37m[1m[2023-07-11 04:13:12,237][233954] New mean coefficients: [[ 5.104328   2.8113437  3.1105828  6.9237313  0.7942538 -1.8167505]][0m
[37m[1m[2023-07-11 04:13:12,238][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:13:21,178][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 04:13:21,179][233954] FPS: 429586.28[0m
[36m[2023-07-11 04:13:21,181][233954] itr=327, itrs=2000, Progress: 16.35%[0m
[36m[2023-07-11 04:13:32,703][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 04:13:32,703][233954] FPS: 335523.36[0m
[36m[2023-07-11 04:13:36,950][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:13:36,951][233954] Reward + Measures: [[15.08724423  0.04671467  0.99185133  0.58040833  0.97872329  3.72440124]][0m
[37m[1m[2023-07-11 04:13:36,951][233954] Max Reward on eval: 15.087244225492226[0m
[37m[1m[2023-07-11 04:13:36,951][233954] Min Reward on eval: 15.087244225492226[0m
[37m[1m[2023-07-11 04:13:36,951][233954] Mean Reward across all agents: 15.087244225492226[0m
[37m[1m[2023-07-11 04:13:36,952][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:13:42,127][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:13:42,128][233954] Reward + Measures: [[  34.59267934    0.22019999    0.35209998    0.1938        0.36560002
     3.29941916]
 [  84.38509941    0.16760001    0.88330001    0.4152        0.84860003
     3.74674773]
 [-107.59388837    0.48900005    0.62270004    0.63389999    0.93349999
     3.72967958]
 ...
 [  81.79810391    0.46029997    0.57440001    0.12490001    0.57120001
     3.66178894]
 [  50.32527472    0.11080001    0.87690002    0.70300001    0.94379997
     3.94642425]
 [ -52.39159512    0.62459999    0.86720002    0.1013        0.83059996
     3.70422292]][0m
[37m[1m[2023-07-11 04:13:42,128][233954] Max Reward on eval: 806.4973831251264[0m
[37m[1m[2023-07-11 04:13:42,128][233954] Min Reward on eval: -333.053738950938[0m
[37m[1m[2023-07-11 04:13:42,128][233954] Mean Reward across all agents: 14.912334172737301[0m
[37m[1m[2023-07-11 04:13:42,128][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:13:42,137][233954] mean_value=52.674538250948125, max_value=680.6559401937018[0m
[37m[1m[2023-07-11 04:13:42,140][233954] New mean coefficients: [[ 4.832953    2.3910496   2.6786904   6.44165    -0.15552497 -2.624117  ]][0m
[37m[1m[2023-07-11 04:13:42,141][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:13:51,158][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 04:13:51,158][233954] FPS: 425966.34[0m
[36m[2023-07-11 04:13:51,160][233954] itr=328, itrs=2000, Progress: 16.40%[0m
[36m[2023-07-11 04:14:03,069][233954] train() took 11.83 seconds to complete[0m
[36m[2023-07-11 04:14:03,069][233954] FPS: 324595.05[0m
[36m[2023-07-11 04:14:07,370][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:14:07,370][233954] Reward + Measures: [[91.24804808  0.034293    0.98626     0.62522495  0.97672135  3.63985801]][0m
[37m[1m[2023-07-11 04:14:07,371][233954] Max Reward on eval: 91.24804808302996[0m
[37m[1m[2023-07-11 04:14:07,371][233954] Min Reward on eval: 91.24804808302996[0m
[37m[1m[2023-07-11 04:14:07,371][233954] Mean Reward across all agents: 91.24804808302996[0m
[37m[1m[2023-07-11 04:14:07,371][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:14:12,332][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:14:12,332][233954] Reward + Measures: [[ -86.62778146    0.29500002    0.29990003    0.24270001    0.35699999
     2.82914996]
 [ 352.76079941    0.10389999    0.79160005    0.45429999    0.68489999
     2.96930766]
 [-146.31801651    0.38409996    0.35550001    0.22350001    0.42989999
     3.01623654]
 ...
 [ -26.87700168    0.42479998    0.51800001    0.35970002    0.41090003
     2.55994773]
 [ -52.27897869    0.4492        0.55590004    0.32930002    0.50840002
     2.7007978 ]
 [ -17.47665697    0.66280001    0.75430006    0.47069994    0.71079999
     2.77919936]][0m
[37m[1m[2023-07-11 04:14:12,333][233954] Max Reward on eval: 386.46156117878854[0m
[37m[1m[2023-07-11 04:14:12,333][233954] Min Reward on eval: -222.63509464403614[0m
[37m[1m[2023-07-11 04:14:12,333][233954] Mean Reward across all agents: -54.38133656909334[0m
[37m[1m[2023-07-11 04:14:12,333][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:14:12,340][233954] mean_value=-113.66108285311299, max_value=545.01848063449[0m
[37m[1m[2023-07-11 04:14:12,343][233954] New mean coefficients: [[ 4.039245   1.385049   1.4367243  5.634242  -1.2165914 -3.2050462]][0m
[37m[1m[2023-07-11 04:14:12,344][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:14:21,390][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 04:14:21,396][233954] FPS: 424553.00[0m
[36m[2023-07-11 04:14:21,399][233954] itr=329, itrs=2000, Progress: 16.45%[0m
[36m[2023-07-11 04:14:32,919][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 04:14:32,920][233954] FPS: 335611.00[0m
[36m[2023-07-11 04:14:37,256][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:14:37,256][233954] Reward + Measures: [[95.54602939  0.03404833  0.98653996  0.62838966  0.97665334  3.63202929]][0m
[37m[1m[2023-07-11 04:14:37,257][233954] Max Reward on eval: 95.54602939386908[0m
[37m[1m[2023-07-11 04:14:37,257][233954] Min Reward on eval: 95.54602939386908[0m
[37m[1m[2023-07-11 04:14:37,257][233954] Mean Reward across all agents: 95.54602939386908[0m
[37m[1m[2023-07-11 04:14:37,257][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:14:42,300][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:14:42,301][233954] Reward + Measures: [[ 56.70702819   0.60900003   0.34129998   0.40999994   0.53210002
    3.55943465]
 [ -1.84127171   0.98870003   0.9896       0.98810005   0.99220002
    3.74269843]
 [152.79804205   0.0102       0.98719996   0.7216       0.9853
    3.91802073]
 ...
 [ 76.4980694    0.1373       0.84780008   0.579        0.89960003
    3.50133681]
 [ -2.48831324   0.98750001   0.98970002   0.98680001   0.98890001
    3.64874506]
 [-60.42198099   0.71880007   0.33240005   0.49880001   0.63790005
    3.78616786]][0m
[37m[1m[2023-07-11 04:14:42,301][233954] Max Reward on eval: 219.33959818538278[0m
[37m[1m[2023-07-11 04:14:42,301][233954] Min Reward on eval: -206.7547364635393[0m
[37m[1m[2023-07-11 04:14:42,301][233954] Mean Reward across all agents: 12.038544857978861[0m
[37m[1m[2023-07-11 04:14:42,302][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:14:42,310][233954] mean_value=-34.91477466442957, max_value=568.7965370981022[0m
[37m[1m[2023-07-11 04:14:42,313][233954] New mean coefficients: [[ 2.9119973   0.06017041 -1.3503755   4.9836116  -3.1509426  -4.292669  ]][0m
[37m[1m[2023-07-11 04:14:42,314][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:14:51,320][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 04:14:51,325][233954] FPS: 426449.44[0m
[36m[2023-07-11 04:14:51,328][233954] itr=330, itrs=2000, Progress: 16.50%[0m
[37m[1m[2023-07-11 04:18:04,152][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000310[0m
[36m[2023-07-11 04:18:16,624][233954] train() took 11.74 seconds to complete[0m
[36m[2023-07-11 04:18:16,625][233954] FPS: 326968.81[0m
[36m[2023-07-11 04:18:20,861][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:18:20,861][233954] Reward + Measures: [[99.41371756  0.03813466  0.9823283   0.63660669  0.97284132  3.60840893]][0m
[37m[1m[2023-07-11 04:18:20,862][233954] Max Reward on eval: 99.41371756332802[0m
[37m[1m[2023-07-11 04:18:20,862][233954] Min Reward on eval: 99.41371756332802[0m
[37m[1m[2023-07-11 04:18:20,862][233954] Mean Reward across all agents: 99.41371756332802[0m
[37m[1m[2023-07-11 04:18:20,862][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:18:25,984][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:18:25,985][233954] Reward + Measures: [[-140.16105557    0.97749996    0.97620004    0.96889991    0.97609997
     3.64273119]
 [ -16.68778001    0.97830003    0.97560006    0.97060007    0.97859997
     2.94375467]
 [-104.83532107    0.0593        0.84079999    0.73019999    0.79900008
     3.25635123]
 ...
 [ 164.46628834    0.25009999    0.93790001    0.73369998    0.74409997
     3.51751828]
 [ 136.10416988    0.09190001    0.97240001    0.45919999    0.97170001
     3.70313644]
 [ 234.66263354    0.0125        0.88859999    0.86189997    0.91230005
     3.87215853]][0m
[37m[1m[2023-07-11 04:18:25,985][233954] Max Reward on eval: 776.3398284875788[0m
[37m[1m[2023-07-11 04:18:25,985][233954] Min Reward on eval: -212.1780910297297[0m
[37m[1m[2023-07-11 04:18:25,986][233954] Mean Reward across all agents: 128.21066080002862[0m
[37m[1m[2023-07-11 04:18:25,986][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:18:25,993][233954] mean_value=-126.06472547299806, max_value=789.1032547648622[0m
[37m[1m[2023-07-11 04:18:25,996][233954] New mean coefficients: [[ 2.7533321   0.03705086 -2.1459668   5.140241   -3.5835724  -4.221874  ]][0m
[37m[1m[2023-07-11 04:18:25,997][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:18:34,976][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 04:18:34,976][233954] FPS: 427723.32[0m
[36m[2023-07-11 04:18:34,979][233954] itr=331, itrs=2000, Progress: 16.55%[0m
[36m[2023-07-11 04:18:46,579][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 04:18:46,579][233954] FPS: 333274.79[0m
[36m[2023-07-11 04:18:50,844][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:18:50,844][233954] Reward + Measures: [[101.4505894    0.032133     0.98428667   0.63038868   0.97612596
    3.59432483]][0m
[37m[1m[2023-07-11 04:18:50,845][233954] Max Reward on eval: 101.45058940260193[0m
[37m[1m[2023-07-11 04:18:50,845][233954] Min Reward on eval: 101.45058940260193[0m
[37m[1m[2023-07-11 04:18:50,845][233954] Mean Reward across all agents: 101.45058940260193[0m
[37m[1m[2023-07-11 04:18:50,845][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:18:55,808][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:18:55,808][233954] Reward + Measures: [[ 211.70849285    0.0278        0.99069995    0.68050003    0.97970003
     3.82797933]
 [  27.72513641    0.018         0.99529994    0.63659996    0.98930007
     3.74192429]
 [  23.1917202     0.63379997    0.66940004    0.5794        0.6821
     2.87624216]
 ...
 [-103.97450616    0.9544        0.95879996    0.93450004    0.95830005
     3.21966934]
 [ -54.49049635    0.0465        0.98870003    0.58570004    0.97209996
     3.68819118]
 [  -5.70208541    0.97349995    0.9698        0.96940005    0.96390003
     3.27638483]][0m
[37m[1m[2023-07-11 04:18:55,808][233954] Max Reward on eval: 224.95812245109119[0m
[37m[1m[2023-07-11 04:18:55,809][233954] Min Reward on eval: -227.25640388894826[0m
[37m[1m[2023-07-11 04:18:55,809][233954] Mean Reward across all agents: -19.434211807128467[0m
[37m[1m[2023-07-11 04:18:55,809][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:18:55,813][233954] mean_value=-168.18176325423758, max_value=577.7197694591852[0m
[37m[1m[2023-07-11 04:18:55,816][233954] New mean coefficients: [[ 2.8663654  0.6280523 -2.1165547  5.6303177 -3.2271814 -3.4958882]][0m
[37m[1m[2023-07-11 04:18:55,817][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:19:04,866][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 04:19:04,867][233954] FPS: 424391.00[0m
[36m[2023-07-11 04:19:04,869][233954] itr=332, itrs=2000, Progress: 16.60%[0m
[36m[2023-07-11 04:19:16,598][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 04:19:16,599][233954] FPS: 329583.84[0m
[36m[2023-07-11 04:19:20,800][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:19:20,801][233954] Reward + Measures: [[112.94702724   0.03159667   0.98198903   0.64127499   0.97627234
    3.57563019]][0m
[37m[1m[2023-07-11 04:19:20,801][233954] Max Reward on eval: 112.947027240647[0m
[37m[1m[2023-07-11 04:19:20,801][233954] Min Reward on eval: 112.947027240647[0m
[37m[1m[2023-07-11 04:19:20,801][233954] Mean Reward across all agents: 112.947027240647[0m
[37m[1m[2023-07-11 04:19:20,802][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:19:25,749][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:19:25,750][233954] Reward + Measures: [[ 195.15573978    0.15390001    0.82929993    0.46070001    0.83999997
     3.38179064]
 [-164.67584516    0.39990002    0.41500002    0.1276        0.47950003
     3.11164474]
 [  54.2308699     0.58429998    0.9368        0.78119999    0.93190002
     3.15207839]
 ...
 [ 219.02051354    0.0872        0.86479998    0.48610005    0.87779999
     3.35528183]
 [ -82.41217721    0.74949998    0.74360007    0.0714        0.80190003
     3.33354235]
 [ 246.90459094    0.0363        0.97290003    0.57540005    0.95139998
     3.64194179]][0m
[37m[1m[2023-07-11 04:19:25,750][233954] Max Reward on eval: 283.4591536462307[0m
[37m[1m[2023-07-11 04:19:25,750][233954] Min Reward on eval: -255.06854630485176[0m
[37m[1m[2023-07-11 04:19:25,751][233954] Mean Reward across all agents: -15.369168203646087[0m
[37m[1m[2023-07-11 04:19:25,751][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:19:25,758][233954] mean_value=59.49269019931671, max_value=591.0306544047371[0m
[37m[1m[2023-07-11 04:19:25,761][233954] New mean coefficients: [[ 3.4428933  1.3072696 -0.6176038  6.3199644 -2.417595  -2.9469378]][0m
[37m[1m[2023-07-11 04:19:25,762][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:19:34,707][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 04:19:34,708][233954] FPS: 429346.03[0m
[36m[2023-07-11 04:19:34,710][233954] itr=333, itrs=2000, Progress: 16.65%[0m
[36m[2023-07-11 04:19:46,316][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 04:19:46,317][233954] FPS: 333177.80[0m
[36m[2023-07-11 04:19:50,546][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:19:50,547][233954] Reward + Measures: [[117.83902953   0.030136     0.98560268   0.63507199   0.9763853
    3.5778327 ]][0m
[37m[1m[2023-07-11 04:19:50,547][233954] Max Reward on eval: 117.83902953092024[0m
[37m[1m[2023-07-11 04:19:50,547][233954] Min Reward on eval: 117.83902953092024[0m
[37m[1m[2023-07-11 04:19:50,547][233954] Mean Reward across all agents: 117.83902953092024[0m
[37m[1m[2023-07-11 04:19:50,548][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:19:55,521][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:19:55,521][233954] Reward + Measures: [[  22.36175729    0.142         0.9271        0.3283        0.86700004
     3.49472594]
 [  12.20710592    0.98619998    0.98929995    0.98940003    0.99010003
     3.75152755]
 [  14.5092927     0.73160005    0.68970007    0.1437        0.68640006
     3.31296277]
 ...
 [ -42.33399966    0.40180001    0.88029999    0.0843        0.89160007
     3.540097  ]
 [-189.17420086    0.61079997    0.58310002    0.0364        0.6027
     3.74410605]
 [  -1.07550442    0.76899999    0.79250002    0.0274        0.79969996
     3.28770614]][0m
[37m[1m[2023-07-11 04:19:55,521][233954] Max Reward on eval: 471.52278714654966[0m
[37m[1m[2023-07-11 04:19:55,522][233954] Min Reward on eval: -294.3254728294909[0m
[37m[1m[2023-07-11 04:19:55,522][233954] Mean Reward across all agents: 17.998128759295845[0m
[37m[1m[2023-07-11 04:19:55,522][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:19:55,530][233954] mean_value=-16.76436099100881, max_value=579.9213773763925[0m
[37m[1m[2023-07-11 04:19:55,533][233954] New mean coefficients: [[ 3.2759457  1.5299839 -1.0335703  6.367384  -2.110518  -2.5334148]][0m
[37m[1m[2023-07-11 04:19:55,533][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:20:04,498][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 04:20:04,498][233954] FPS: 428430.13[0m
[36m[2023-07-11 04:20:04,501][233954] itr=334, itrs=2000, Progress: 16.70%[0m
[36m[2023-07-11 04:20:16,011][233954] train() took 11.43 seconds to complete[0m
[36m[2023-07-11 04:20:16,011][233954] FPS: 335875.47[0m
[36m[2023-07-11 04:20:20,259][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:20:20,260][233954] Reward + Measures: [[119.23774519   0.02981      0.98413867   0.64370602   0.97646534
    3.57315302]][0m
[37m[1m[2023-07-11 04:20:20,260][233954] Max Reward on eval: 119.23774518564339[0m
[37m[1m[2023-07-11 04:20:20,260][233954] Min Reward on eval: 119.23774518564339[0m
[37m[1m[2023-07-11 04:20:20,260][233954] Mean Reward across all agents: 119.23774518564339[0m
[37m[1m[2023-07-11 04:20:20,261][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:20:25,208][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:20:25,208][233954] Reward + Measures: [[-56.60967823   0.114        0.1673       0.10460001   0.17999999
    2.78375411]
 [ 34.64257776   0.80060005   0.81829995   0.54360002   0.86510003
    3.04171348]
 [110.12708567   0.0787       0.88910002   0.43470001   0.87889999
    3.54629874]
 ...
 [252.78736471   0.0898       0.87059993   0.51349998   0.83470005
    3.47324371]
 [138.94719268   0.1139       0.80559999   0.498        0.80380005
    3.74855924]
 [-68.96729004   0.57300001   0.63120002   0.55229998   0.60580003
    2.93759513]][0m
[37m[1m[2023-07-11 04:20:25,209][233954] Max Reward on eval: 352.78131960937753[0m
[37m[1m[2023-07-11 04:20:25,209][233954] Min Reward on eval: -168.93470592000523[0m
[37m[1m[2023-07-11 04:20:25,209][233954] Mean Reward across all agents: 64.52129019506373[0m
[37m[1m[2023-07-11 04:20:25,210][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:20:25,219][233954] mean_value=56.21780160257383, max_value=768.9107222503051[0m
[37m[1m[2023-07-11 04:20:25,221][233954] New mean coefficients: [[ 2.914575   1.2024899 -2.5378036  5.933652  -2.3327308 -2.7794538]][0m
[37m[1m[2023-07-11 04:20:25,222][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:20:34,242][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 04:20:34,243][233954] FPS: 425786.93[0m
[36m[2023-07-11 04:20:34,245][233954] itr=335, itrs=2000, Progress: 16.75%[0m
[36m[2023-07-11 04:20:46,074][233954] train() took 11.75 seconds to complete[0m
[36m[2023-07-11 04:20:46,074][233954] FPS: 326884.66[0m
[36m[2023-07-11 04:20:50,326][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:20:50,332][233954] Reward + Measures: [[121.52094818   0.02780567   0.98463929   0.63913834   0.97758335
    3.54250026]][0m
[37m[1m[2023-07-11 04:20:50,332][233954] Max Reward on eval: 121.52094818150861[0m
[37m[1m[2023-07-11 04:20:50,332][233954] Min Reward on eval: 121.52094818150861[0m
[37m[1m[2023-07-11 04:20:50,333][233954] Mean Reward across all agents: 121.52094818150861[0m
[37m[1m[2023-07-11 04:20:50,333][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:20:55,518][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:20:55,524][233954] Reward + Measures: [[100.25313878   0.94440001   0.93689996   0.92550004   0.93160003
    3.90673637]
 [-47.09029406   0.16320001   0.1936       0.19220002   0.19310002
    2.95320678]
 [-24.92319238   0.98450005   0.98839998   0.98659992   0.99209994
    2.97016478]
 ...
 [-14.13743484   0.9788       0.97629994   0.96719998   0.97250003
    3.81709743]
 [ 13.3364863    0.12609999   0.28660002   0.1469       0.3222
    2.91449594]
 [-57.24996978   0.1825       0.1645       0.20200001   0.15440001
    2.85340309]][0m
[37m[1m[2023-07-11 04:20:55,524][233954] Max Reward on eval: 671.8017754520755[0m
[37m[1m[2023-07-11 04:20:55,525][233954] Min Reward on eval: -349.94865131489934[0m
[37m[1m[2023-07-11 04:20:55,525][233954] Mean Reward across all agents: 31.301548544832887[0m
[37m[1m[2023-07-11 04:20:55,525][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:20:55,532][233954] mean_value=-101.00681171232169, max_value=546.1073947701686[0m
[37m[1m[2023-07-11 04:20:55,535][233954] New mean coefficients: [[ 3.7477002  2.0279036 -0.7600776  6.363707  -1.6470098 -2.2565074]][0m
[37m[1m[2023-07-11 04:20:55,536][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:21:04,500][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 04:21:04,500][233954] FPS: 428451.45[0m
[36m[2023-07-11 04:21:04,502][233954] itr=336, itrs=2000, Progress: 16.80%[0m
[36m[2023-07-11 04:21:16,135][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 04:21:16,135][233954] FPS: 332415.04[0m
[36m[2023-07-11 04:21:20,429][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:21:20,429][233954] Reward + Measures: [[129.46441097   0.03409867   0.97205502   0.67424434   0.97296304
    3.56777477]][0m
[37m[1m[2023-07-11 04:21:20,430][233954] Max Reward on eval: 129.46441097377078[0m
[37m[1m[2023-07-11 04:21:20,430][233954] Min Reward on eval: 129.46441097377078[0m
[37m[1m[2023-07-11 04:21:20,430][233954] Mean Reward across all agents: 129.46441097377078[0m
[37m[1m[2023-07-11 04:21:20,430][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:21:25,432][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:21:25,433][233954] Reward + Measures: [[103.61013556   0.021        0.995        0.44400001   0.99340004
    3.61712623]
 [ 75.45979786   0.041        0.99519998   0.40629998   0.99489993
    3.74384117]
 [ 93.02511167   0.0089       0.99729997   0.50660008   0.99620003
    3.65314746]
 ...
 [ 44.08645414   0.53929996   0.91850007   0.64510006   0.92989999
    3.44660616]
 [ 47.17787654   0.12910001   0.15439999   0.13970001   0.2168
    2.93409228]
 [ 38.80648251   0.0315       0.96710008   0.37130001   0.95380002
    3.42505074]][0m
[37m[1m[2023-07-11 04:21:25,433][233954] Max Reward on eval: 243.5229644836858[0m
[37m[1m[2023-07-11 04:21:25,433][233954] Min Reward on eval: -424.04632376823577[0m
[37m[1m[2023-07-11 04:21:25,433][233954] Mean Reward across all agents: 11.733358770156071[0m
[37m[1m[2023-07-11 04:21:25,434][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:21:25,441][233954] mean_value=-52.04759133977568, max_value=629.3070874143392[0m
[37m[1m[2023-07-11 04:21:25,444][233954] New mean coefficients: [[ 4.4442544  2.7990327  0.5997957  6.9033356 -1.1445065 -1.4741971]][0m
[37m[1m[2023-07-11 04:21:25,445][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:21:34,481][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 04:21:34,482][233954] FPS: 424999.39[0m
[36m[2023-07-11 04:21:34,484][233954] itr=337, itrs=2000, Progress: 16.85%[0m
[36m[2023-07-11 04:21:46,313][233954] train() took 11.75 seconds to complete[0m
[36m[2023-07-11 04:21:46,313][233954] FPS: 326882.74[0m
[36m[2023-07-11 04:21:50,580][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:21:50,580][233954] Reward + Measures: [[117.63191132   0.47132868   0.9300667    0.75464225   0.93853968
    3.39381051]][0m
[37m[1m[2023-07-11 04:21:50,580][233954] Max Reward on eval: 117.63191132360495[0m
[37m[1m[2023-07-11 04:21:50,581][233954] Min Reward on eval: 117.63191132360495[0m
[37m[1m[2023-07-11 04:21:50,581][233954] Mean Reward across all agents: 117.63191132360495[0m
[37m[1m[2023-07-11 04:21:50,581][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:21:55,560][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:21:55,561][233954] Reward + Measures: [[  7.14572535   0.85949993   0.87010002   0.84809989   0.85350001
    3.56667948]
 [145.96967597   0.90990001   0.91320002   0.90499991   0.90290004
    3.62091231]
 [  6.30193317   0.97939998   0.97689992   0.97259998   0.97820008
    3.60155034]
 ...
 [-42.35225094   0.99470007   0.99300003   0.99290001   0.99259996
    3.68364787]
 [161.41001987   0.16790001   0.98859996   0.61750001   0.98689997
    3.33486152]
 [-29.3725813    0.995        0.99329996   0.99379998   0.99470007
    3.65803599]][0m
[37m[1m[2023-07-11 04:21:55,561][233954] Max Reward on eval: 213.2021760871634[0m
[37m[1m[2023-07-11 04:21:55,561][233954] Min Reward on eval: -119.42539859930984[0m
[37m[1m[2023-07-11 04:21:55,562][233954] Mean Reward across all agents: 42.45538658653654[0m
[37m[1m[2023-07-11 04:21:55,562][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:21:55,566][233954] mean_value=-35.87407540656434, max_value=605.0486623147502[0m
[37m[1m[2023-07-11 04:21:55,569][233954] New mean coefficients: [[ 3.9539647   2.6172469  -0.45970678  6.606289   -1.7387232  -1.7558427 ]][0m
[37m[1m[2023-07-11 04:21:55,570][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:22:04,577][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 04:22:04,578][233954] FPS: 426376.33[0m
[36m[2023-07-11 04:22:04,580][233954] itr=338, itrs=2000, Progress: 16.90%[0m
[36m[2023-07-11 04:22:16,096][233954] train() took 11.43 seconds to complete[0m
[36m[2023-07-11 04:22:16,096][233954] FPS: 335822.64[0m
[36m[2023-07-11 04:22:20,296][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:22:20,297][233954] Reward + Measures: [[104.0906173    0.55936402   0.94735795   0.81021303   0.95283037
    3.39194536]][0m
[37m[1m[2023-07-11 04:22:20,297][233954] Max Reward on eval: 104.09061730039508[0m
[37m[1m[2023-07-11 04:22:20,297][233954] Min Reward on eval: 104.09061730039508[0m
[37m[1m[2023-07-11 04:22:20,297][233954] Mean Reward across all agents: 104.09061730039508[0m
[37m[1m[2023-07-11 04:22:20,298][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:22:25,233][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:22:25,233][233954] Reward + Measures: [[ 15.47378219   0.70960003   0.97090006   0.82440007   0.9677
    3.39277625]
 [ 83.04305411   0.67629999   0.98900002   0.84089994   0.98680013
    3.36483693]
 [-34.41339471   0.60780001   0.96990007   0.81129998   0.96530002
    3.24457479]
 ...
 [ -6.20632747   0.65160006   0.97659999   0.85880005   0.97749996
    3.45812583]
 [ -0.0205543    0.3888       0.87229997   0.55980003   0.88999999
    3.3145206 ]
 [ 21.60872004   0.75139999   0.97160006   0.87860006   0.96049994
    3.34430814]][0m
[37m[1m[2023-07-11 04:22:25,234][233954] Max Reward on eval: 194.2809753378853[0m
[37m[1m[2023-07-11 04:22:25,234][233954] Min Reward on eval: -195.26388098299503[0m
[37m[1m[2023-07-11 04:22:25,234][233954] Mean Reward across all agents: 24.232489934647592[0m
[37m[1m[2023-07-11 04:22:25,234][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:22:25,239][233954] mean_value=43.020040611566465, max_value=408.54782893842014[0m
[37m[1m[2023-07-11 04:22:25,242][233954] New mean coefficients: [[ 3.9719958   2.5641396  -0.42596906  5.563809   -1.8046155  -1.859803  ]][0m
[37m[1m[2023-07-11 04:22:25,243][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:22:34,250][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 04:22:34,251][233954] FPS: 426394.73[0m
[36m[2023-07-11 04:22:34,253][233954] itr=339, itrs=2000, Progress: 16.95%[0m
[36m[2023-07-11 04:22:45,964][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 04:22:45,965][233954] FPS: 330200.75[0m
[36m[2023-07-11 04:22:50,193][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:22:50,194][233954] Reward + Measures: [[98.64697122  0.59502965  0.94671404  0.82861197  0.95226926  3.38915157]][0m
[37m[1m[2023-07-11 04:22:50,194][233954] Max Reward on eval: 98.64697121505179[0m
[37m[1m[2023-07-11 04:22:50,194][233954] Min Reward on eval: 98.64697121505179[0m
[37m[1m[2023-07-11 04:22:50,194][233954] Mean Reward across all agents: 98.64697121505179[0m
[37m[1m[2023-07-11 04:22:50,195][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:22:55,106][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:22:55,107][233954] Reward + Measures: [[-54.73370673   0.4684       0.92950004   0.58990002   0.92360002
    3.60658908]
 [ 75.51554551   0.58520001   0.97640002   0.80009997   0.97609997
    3.39528465]
 [ 58.13347088   0.565        0.87740004   0.6875       0.87199992
    3.01840258]
 ...
 [-32.65507462   0.31690001   0.64219999   0.22620001   0.51840001
    2.66376185]
 [ 28.19646173   0.63220006   0.98070002   0.83230001   0.98220009
    3.53155708]
 [ 71.88844326   0.65040004   0.9745       0.80849999   0.97399998
    3.32201838]][0m
[37m[1m[2023-07-11 04:22:55,107][233954] Max Reward on eval: 204.65332604646682[0m
[37m[1m[2023-07-11 04:22:55,108][233954] Min Reward on eval: -200.77098322324454[0m
[37m[1m[2023-07-11 04:22:55,108][233954] Mean Reward across all agents: 35.22885563526209[0m
[37m[1m[2023-07-11 04:22:55,108][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:22:55,114][233954] mean_value=3.798433838681655, max_value=573.1713393507525[0m
[37m[1m[2023-07-11 04:22:55,117][233954] New mean coefficients: [[ 4.2042284  2.8577895  0.780497   5.826535  -1.4165115 -1.6578807]][0m
[37m[1m[2023-07-11 04:22:55,118][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:23:04,055][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 04:23:04,056][233954] FPS: 429738.23[0m
[36m[2023-07-11 04:23:04,058][233954] itr=340, itrs=2000, Progress: 17.00%[0m
[37m[1m[2023-07-11 04:26:36,637][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000320[0m
[36m[2023-07-11 04:26:48,874][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 04:26:48,874][233954] FPS: 329949.45[0m
[36m[2023-07-11 04:26:53,133][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:26:53,133][233954] Reward + Measures: [[98.24244092  0.60679531  0.94262433  0.82971466  0.94505888  3.39673233]][0m
[37m[1m[2023-07-11 04:26:53,133][233954] Max Reward on eval: 98.24244091792734[0m
[37m[1m[2023-07-11 04:26:53,133][233954] Min Reward on eval: 98.24244091792734[0m
[37m[1m[2023-07-11 04:26:53,134][233954] Mean Reward across all agents: 98.24244091792734[0m
[37m[1m[2023-07-11 04:26:53,134][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:26:58,062][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:26:58,062][233954] Reward + Measures: [[-16.00099752   0.55039996   0.93699998   0.73750001   0.97139996
    3.49699259]
 [ 65.27826953   0.82919997   0.74370009   0.68500006   0.83920002
    3.06090927]
 [-34.23488908   0.95530003   0.95679998   0.79010004   0.96730006
    3.02518392]
 ...
 [ -3.35533582   0.95279998   0.9436       0.95259994   0.96060002
    3.09364438]
 [ 59.2859144    0.38229999   0.96660006   0.67160004   0.97889996
    3.58516765]
 [ 14.49760664   0.7899       0.70220006   0.55450004   0.79380006
    2.91349578]][0m
[37m[1m[2023-07-11 04:26:58,062][233954] Max Reward on eval: 292.99651143336666[0m
[37m[1m[2023-07-11 04:26:58,063][233954] Min Reward on eval: -144.38432410508395[0m
[37m[1m[2023-07-11 04:26:58,063][233954] Mean Reward across all agents: 53.48353555652755[0m
[37m[1m[2023-07-11 04:26:58,063][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:26:58,072][233954] mean_value=87.37722955270537, max_value=589.8955508577637[0m
[37m[1m[2023-07-11 04:26:58,075][233954] New mean coefficients: [[ 3.3448846  2.6320329 -1.2960021  5.8817906 -1.8651757 -2.2586057]][0m
[37m[1m[2023-07-11 04:26:58,076][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:27:06,997][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 04:27:06,997][233954] FPS: 430511.76[0m
[36m[2023-07-11 04:27:07,000][233954] itr=341, itrs=2000, Progress: 17.05%[0m
[36m[2023-07-11 04:27:18,445][233954] train() took 11.36 seconds to complete[0m
[36m[2023-07-11 04:27:18,445][233954] FPS: 337943.81[0m
[36m[2023-07-11 04:27:22,659][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:27:22,660][233954] Reward + Measures: [[88.6420006   0.62550527  0.94302702  0.83867067  0.94560796  3.38360548]][0m
[37m[1m[2023-07-11 04:27:22,660][233954] Max Reward on eval: 88.64200059567617[0m
[37m[1m[2023-07-11 04:27:22,660][233954] Min Reward on eval: 88.64200059567617[0m
[37m[1m[2023-07-11 04:27:22,661][233954] Mean Reward across all agents: 88.64200059567617[0m
[37m[1m[2023-07-11 04:27:22,661][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:27:27,646][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:27:27,647][233954] Reward + Measures: [[ -2.55297924   0.99260008   0.99220002   0.99169999   0.99230003
    3.99583292]
 [-83.92383004   0.66310006   0.5869       0.2333       0.66839999
    3.30881238]
 [121.48695467   0.27560002   0.16439998   0.28860003   0.30990002
    3.11449146]
 ...
 [-21.81761297   0.64630002   0.4382       0.36429998   0.6182
    3.2230382 ]
 [-66.59437728   0.82660002   0.94590008   0.0345       0.98569995
    3.44105959]
 [-17.77302268   0.23         0.96610004   0.47440001   0.97360003
    3.27698135]][0m
[37m[1m[2023-07-11 04:27:27,647][233954] Max Reward on eval: 162.36175940334797[0m
[37m[1m[2023-07-11 04:27:27,647][233954] Min Reward on eval: -166.89631318012252[0m
[37m[1m[2023-07-11 04:27:27,647][233954] Mean Reward across all agents: -9.472397777483684[0m
[37m[1m[2023-07-11 04:27:27,648][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:27:27,658][233954] mean_value=81.78039919329699, max_value=534.9682029267773[0m
[37m[1m[2023-07-11 04:27:27,666][233954] New mean coefficients: [[ 3.3774576  3.094422  -1.4641155  6.3692927 -1.7666771 -2.0960402]][0m
[37m[1m[2023-07-11 04:27:27,667][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:27:36,652][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 04:27:36,652][233954] FPS: 427459.41[0m
[36m[2023-07-11 04:27:36,655][233954] itr=342, itrs=2000, Progress: 17.10%[0m
[36m[2023-07-11 04:27:48,245][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 04:27:48,246][233954] FPS: 333626.19[0m
[36m[2023-07-11 04:27:52,493][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:27:52,494][233954] Reward + Measures: [[94.08853595  0.63612103  0.93876129  0.83985597  0.93957329  3.36287665]][0m
[37m[1m[2023-07-11 04:27:52,494][233954] Max Reward on eval: 94.08853594903388[0m
[37m[1m[2023-07-11 04:27:52,494][233954] Min Reward on eval: 94.08853594903388[0m
[37m[1m[2023-07-11 04:27:52,494][233954] Mean Reward across all agents: 94.08853594903388[0m
[37m[1m[2023-07-11 04:27:52,495][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:27:57,637][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:27:57,644][233954] Reward + Measures: [[115.756021     0.24980001   0.90780002   0.62090003   0.90240002
    3.3437736 ]
 [ 37.01583444   0.63430005   0.9113       0.77570003   0.95550007
    3.39188933]
 [ 93.78570695   0.51610005   0.90429991   0.69310004   0.89519995
    3.32804871]
 ...
 [171.86973193   0.3687       0.89169997   0.61470002   0.88910002
    3.34539032]
 [ 87.27067138   0.60780001   0.85280001   0.73470002   0.84840006
    3.2948041 ]
 [ 66.20663893   0.06820001   0.9314       0.52640003   0.92410004
    3.41579509]][0m
[37m[1m[2023-07-11 04:27:57,644][233954] Max Reward on eval: 273.41857430525124[0m
[37m[1m[2023-07-11 04:27:57,644][233954] Min Reward on eval: -93.90746568329632[0m
[37m[1m[2023-07-11 04:27:57,645][233954] Mean Reward across all agents: 92.27630216036131[0m
[37m[1m[2023-07-11 04:27:57,645][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:27:57,650][233954] mean_value=-16.27143786363214, max_value=474.65856048429873[0m
[37m[1m[2023-07-11 04:27:57,653][233954] New mean coefficients: [[ 3.7128468   3.075232   -0.93911844  6.7596807  -1.8507086  -2.0299513 ]][0m
[37m[1m[2023-07-11 04:27:57,654][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:28:06,591][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 04:28:06,591][233954] FPS: 429748.77[0m
[36m[2023-07-11 04:28:06,593][233954] itr=343, itrs=2000, Progress: 17.15%[0m
[36m[2023-07-11 04:28:18,195][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 04:28:18,195][233954] FPS: 333220.41[0m
[36m[2023-07-11 04:28:22,392][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:28:22,392][233954] Reward + Measures: [[99.99524193  0.65273136  0.93714535  0.84974933  0.9400593   3.34135318]][0m
[37m[1m[2023-07-11 04:28:22,392][233954] Max Reward on eval: 99.9952419262278[0m
[37m[1m[2023-07-11 04:28:22,393][233954] Min Reward on eval: 99.9952419262278[0m
[37m[1m[2023-07-11 04:28:22,393][233954] Mean Reward across all agents: 99.9952419262278[0m
[37m[1m[2023-07-11 04:28:22,393][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:28:27,345][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:28:27,345][233954] Reward + Measures: [[-22.51776324   0.67879999   0.92919999   0.83630002   0.91799992
    3.3281002 ]
 [ 30.74259763   0.64169997   0.95020002   0.80049992   0.95219994
    3.33830714]
 [122.96828986   0.0221       0.99119997   0.3707       0.9874
    3.15042877]
 ...
 [ 68.47993919   0.2253       0.97480005   0.58170003   0.96429998
    3.17880225]
 [-39.09442287   0.88799995   0.93479997   0.84460002   0.88789999
    3.31221366]
 [ 16.23401089   0.60280001   0.97469997   0.83120006   0.9691
    3.41673732]][0m
[37m[1m[2023-07-11 04:28:27,345][233954] Max Reward on eval: 636.3919296456501[0m
[37m[1m[2023-07-11 04:28:27,346][233954] Min Reward on eval: -260.34337029615415[0m
[37m[1m[2023-07-11 04:28:27,346][233954] Mean Reward across all agents: 3.7291199456652966[0m
[37m[1m[2023-07-11 04:28:27,346][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:28:27,349][233954] mean_value=-101.14185814192533, max_value=560.4511463054116[0m
[37m[1m[2023-07-11 04:28:27,352][233954] New mean coefficients: [[ 4.528367    3.8038156   1.1984997   7.2308655  -0.9877024  -0.63358235]][0m
[37m[1m[2023-07-11 04:28:27,353][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:28:36,279][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 04:28:36,285][233954] FPS: 430265.18[0m
[36m[2023-07-11 04:28:36,287][233954] itr=344, itrs=2000, Progress: 17.20%[0m
[36m[2023-07-11 04:28:48,055][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 04:28:48,055][233954] FPS: 328575.58[0m
[36m[2023-07-11 04:28:52,271][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:28:52,272][233954] Reward + Measures: [[101.44045118   0.67555737   0.93187696   0.85912597   0.93948931
    3.34079361]][0m
[37m[1m[2023-07-11 04:28:52,272][233954] Max Reward on eval: 101.4404511789014[0m
[37m[1m[2023-07-11 04:28:52,272][233954] Min Reward on eval: 101.4404511789014[0m
[37m[1m[2023-07-11 04:28:52,272][233954] Mean Reward across all agents: 101.4404511789014[0m
[37m[1m[2023-07-11 04:28:52,273][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:28:57,236][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:28:57,237][233954] Reward + Measures: [[ 35.11620489   0.0602       0.9655       0.41920003   0.93570006
    3.18179393]
 [ 54.44370208   0.113        0.96140003   0.44910002   0.93450004
    3.15446901]
 [ 80.65369798   0.38710001   0.95170003   0.61479998   0.93269998
    3.29316068]
 ...
 [ 44.51355407   0.21270001   0.8021       0.47860003   0.72250003
    3.22484016]
 [134.13565231   0.40669999   0.87880003   0.69180006   0.89720005
    3.32963729]
 [ -5.88278342   0.60330003   0.83160001   0.68950003   0.8860001
    3.15738082]][0m
[37m[1m[2023-07-11 04:28:57,237][233954] Max Reward on eval: 210.18494676556438[0m
[37m[1m[2023-07-11 04:28:57,237][233954] Min Reward on eval: -81.91144679207355[0m
[37m[1m[2023-07-11 04:28:57,238][233954] Mean Reward across all agents: 55.73654862599979[0m
[37m[1m[2023-07-11 04:28:57,238][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:28:57,242][233954] mean_value=-49.890834266745266, max_value=508.3018179733459[0m
[37m[1m[2023-07-11 04:28:57,245][233954] New mean coefficients: [[ 4.708896   3.2986422  1.4952977  7.3155203 -1.7895267 -1.2717335]][0m
[37m[1m[2023-07-11 04:28:57,246][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:29:06,171][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 04:29:06,171][233954] FPS: 430324.19[0m
[36m[2023-07-11 04:29:06,174][233954] itr=345, itrs=2000, Progress: 17.25%[0m
[36m[2023-07-11 04:29:17,690][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 04:29:17,690][233954] FPS: 335795.40[0m
[36m[2023-07-11 04:29:21,939][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:29:21,940][233954] Reward + Measures: [[101.2478364    0.69726104   0.9481883    0.87679607   0.95246166
    3.34559035]][0m
[37m[1m[2023-07-11 04:29:21,940][233954] Max Reward on eval: 101.24783640004188[0m
[37m[1m[2023-07-11 04:29:21,940][233954] Min Reward on eval: 101.24783640004188[0m
[37m[1m[2023-07-11 04:29:21,940][233954] Mean Reward across all agents: 101.24783640004188[0m
[37m[1m[2023-07-11 04:29:21,941][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:29:26,888][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:29:26,888][233954] Reward + Measures: [[100.89296925   0.30150002   0.99480003   0.73649997   0.99150002
    3.65897679]
 [ 78.81506522   0.34599996   0.9806       0.6455       0.98089999
    3.47031569]
 [ 92.70551399   0.38789997   0.98920006   0.79370004   0.98940003
    3.63110161]
 ...
 [ 76.13105584   0.0289       0.9903       0.40400001   0.98520005
    3.63724113]
 [ 11.5600214    0.39140001   0.94760001   0.56779999   0.96020001
    3.48140264]
 [-12.51751617   0.34180003   0.98590004   0.65430003   0.98610002
    3.53697014]][0m
[37m[1m[2023-07-11 04:29:26,888][233954] Max Reward on eval: 300.7835264198482[0m
[37m[1m[2023-07-11 04:29:26,889][233954] Min Reward on eval: -167.77209071042017[0m
[37m[1m[2023-07-11 04:29:26,889][233954] Mean Reward across all agents: 55.03706243526804[0m
[37m[1m[2023-07-11 04:29:26,889][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:29:26,892][233954] mean_value=-89.36728199298156, max_value=178.68392897113793[0m
[37m[1m[2023-07-11 04:29:26,894][233954] New mean coefficients: [[ 5.1070805   3.732413    2.3761153   7.469532   -1.5205057  -0.83486336]][0m
[37m[1m[2023-07-11 04:29:26,895][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:29:35,861][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 04:29:35,861][233954] FPS: 428369.98[0m
[36m[2023-07-11 04:29:35,863][233954] itr=346, itrs=2000, Progress: 17.30%[0m
[36m[2023-07-11 04:29:47,777][233954] train() took 11.84 seconds to complete[0m
[36m[2023-07-11 04:29:47,778][233954] FPS: 324443.05[0m
[36m[2023-07-11 04:29:52,067][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:29:52,067][233954] Reward + Measures: [[88.67769144  0.72017527  0.95213664  0.88221467  0.95557761  3.31164455]][0m
[37m[1m[2023-07-11 04:29:52,067][233954] Max Reward on eval: 88.67769143781567[0m
[37m[1m[2023-07-11 04:29:52,067][233954] Min Reward on eval: 88.67769143781567[0m
[37m[1m[2023-07-11 04:29:52,068][233954] Mean Reward across all agents: 88.67769143781567[0m
[37m[1m[2023-07-11 04:29:52,068][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:29:57,002][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:29:57,002][233954] Reward + Measures: [[  44.91608311    0.6135        0.96700001    0.81280005    0.96149999
     3.27344513]
 [   3.50643825    0.99419993    0.99440002    0.99040002    0.99300003
     3.44794154]
 [  -1.57377731    0.99239999    0.99290001    0.98710006    0.98990005
     3.11745691]
 ...
 [   6.04002122    0.99200004    0.99420005    0.98660004    0.99160004
     3.26724792]
 [-157.92037249    0.99249995    0.99370003    0.98619998    0.98939991
     3.10159087]
 [  29.09431028    0.59320003    0.97930002    0.82060003    0.97659999
     3.28309321]][0m
[37m[1m[2023-07-11 04:29:57,003][233954] Max Reward on eval: 180.93839311013[0m
[37m[1m[2023-07-11 04:29:57,003][233954] Min Reward on eval: -164.70018650218844[0m
[37m[1m[2023-07-11 04:29:57,003][233954] Mean Reward across all agents: 4.792524485351751[0m
[37m[1m[2023-07-11 04:29:57,003][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:29:57,005][233954] mean_value=-131.89778991296876, max_value=17.8329286400742[0m
[37m[1m[2023-07-11 04:29:57,007][233954] New mean coefficients: [[ 6.025697    4.3818665   3.8982167   8.241762   -1.062022   -0.03655601]][0m
[37m[1m[2023-07-11 04:29:57,008][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:30:05,933][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 04:30:05,933][233954] FPS: 430335.18[0m
[36m[2023-07-11 04:30:05,935][233954] itr=347, itrs=2000, Progress: 17.35%[0m
[36m[2023-07-11 04:30:17,436][233954] train() took 11.42 seconds to complete[0m
[36m[2023-07-11 04:30:17,437][233954] FPS: 336237.92[0m
[36m[2023-07-11 04:30:21,731][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:30:21,731][233954] Reward + Measures: [[93.89436241  0.71896434  0.96041697  0.88521767  0.96115202  3.29513788]][0m
[37m[1m[2023-07-11 04:30:21,731][233954] Max Reward on eval: 93.89436241097714[0m
[37m[1m[2023-07-11 04:30:21,732][233954] Min Reward on eval: 93.89436241097714[0m
[37m[1m[2023-07-11 04:30:21,732][233954] Mean Reward across all agents: 93.89436241097714[0m
[37m[1m[2023-07-11 04:30:21,732][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:30:26,733][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:30:26,733][233954] Reward + Measures: [[ 71.3940444    0.0001       0.98990005   0.95300007   0.9946
    3.27769327]
 [ 79.71883102   0.0026       0.97179997   0.9040001    0.9801001
    3.32622457]
 [251.36065292   0.           0.99349993   0.98330003   0.99650002
    3.52890587]
 ...
 [ 60.02764848   0.0446       0.88880008   0.79619998   0.88989991
    2.99530768]
 [ 44.09901059   0.0143       0.98009998   0.95170003   0.98260003
    2.95967364]
 [103.1072055    0.0139       0.8585       0.83339995   0.85670006
    3.44044614]][0m
[37m[1m[2023-07-11 04:30:26,733][233954] Max Reward on eval: 478.10772321242837[0m
[37m[1m[2023-07-11 04:30:26,734][233954] Min Reward on eval: -131.42165958979166[0m
[37m[1m[2023-07-11 04:30:26,734][233954] Mean Reward across all agents: 115.7841237434253[0m
[37m[1m[2023-07-11 04:30:26,734][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:30:26,738][233954] mean_value=-183.24245895298262, max_value=509.358213492617[0m
[37m[1m[2023-07-11 04:30:26,741][233954] New mean coefficients: [[ 5.4216833  2.4745364  2.3436003  7.1146793 -3.1325438 -1.5270112]][0m
[37m[1m[2023-07-11 04:30:26,742][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:30:35,765][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 04:30:35,766][233954] FPS: 425635.32[0m
[36m[2023-07-11 04:30:35,768][233954] itr=348, itrs=2000, Progress: 17.40%[0m
[36m[2023-07-11 04:30:47,476][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 04:30:47,477][233954] FPS: 330203.46[0m
[36m[2023-07-11 04:30:51,784][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:30:51,784][233954] Reward + Measures: [[97.67747019  0.72758138  0.95739764  0.88623232  0.96022069  3.28309178]][0m
[37m[1m[2023-07-11 04:30:51,784][233954] Max Reward on eval: 97.67747018912404[0m
[37m[1m[2023-07-11 04:30:51,785][233954] Min Reward on eval: 97.67747018912404[0m
[37m[1m[2023-07-11 04:30:51,785][233954] Mean Reward across all agents: 97.67747018912404[0m
[37m[1m[2023-07-11 04:30:51,785][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:30:57,006][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:30:57,006][233954] Reward + Measures: [[ 35.38329878   0.60430002   0.9813       0.84300005   0.97860003
    3.28774118]
 [117.02377175   0.35780001   0.95760006   0.69460005   0.94119996
    3.2848103 ]
 [113.11158561   0.61430001   0.97759992   0.86400002   0.97249997
    3.37303329]
 ...
 [ 24.134355     0.43940002   0.94840002   0.73100007   0.92129993
    3.26096392]
 [ -8.73225405   0.51710004   0.9368       0.78410006   0.92000002
    3.270926  ]
 [  1.08658717   0.55369997   0.9077       0.7665       0.91160005
    3.20056987]][0m
[37m[1m[2023-07-11 04:30:57,006][233954] Max Reward on eval: 199.52461285442115[0m
[37m[1m[2023-07-11 04:30:57,007][233954] Min Reward on eval: -154.48810217389837[0m
[37m[1m[2023-07-11 04:30:57,007][233954] Mean Reward across all agents: 51.30282867658835[0m
[37m[1m[2023-07-11 04:30:57,007][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:30:57,010][233954] mean_value=-81.09836619427678, max_value=226.51381287432307[0m
[37m[1m[2023-07-11 04:30:57,012][233954] New mean coefficients: [[ 5.779778   2.637563   3.5455656  7.222935  -2.9842105 -1.742987 ]][0m
[37m[1m[2023-07-11 04:30:57,013][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:31:05,947][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 04:31:05,948][233954] FPS: 429887.18[0m
[36m[2023-07-11 04:31:05,950][233954] itr=349, itrs=2000, Progress: 17.45%[0m
[36m[2023-07-11 04:31:17,506][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 04:31:17,506][233954] FPS: 334668.73[0m
[36m[2023-07-11 04:31:21,776][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:31:21,776][233954] Reward + Measures: [[107.37791897   0.73404831   0.95769238   0.88834327   0.96096665
    3.27441955]][0m
[37m[1m[2023-07-11 04:31:21,777][233954] Max Reward on eval: 107.37791896663335[0m
[37m[1m[2023-07-11 04:31:21,777][233954] Min Reward on eval: 107.37791896663335[0m
[37m[1m[2023-07-11 04:31:21,777][233954] Mean Reward across all agents: 107.37791896663335[0m
[37m[1m[2023-07-11 04:31:21,777][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:31:26,756][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:31:26,756][233954] Reward + Measures: [[ 27.96123943   0.76499999   0.89680004   0.0397       0.93190002
    3.20464396]
 [ 40.41753917   0.42210004   0.81840003   0.40470001   0.88290006
    3.15721393]
 [ 59.53019176   0.31         0.94510001   0.29969999   0.91750002
    3.30192256]
 ...
 [ 19.22840664   0.85790008   0.98339999   0.90820009   0.98430008
    3.14772511]
 [ 53.96990916   0.07310001   0.92670006   0.43000004   0.91610003
    3.23719406]
 [-30.66536499   0.84240001   0.91610003   0.0109       0.94040006
    3.41440964]][0m
[37m[1m[2023-07-11 04:31:26,756][233954] Max Reward on eval: 212.2379050763324[0m
[37m[1m[2023-07-11 04:31:26,757][233954] Min Reward on eval: -146.93948529348708[0m
[37m[1m[2023-07-11 04:31:26,757][233954] Mean Reward across all agents: 26.487755209620016[0m
[37m[1m[2023-07-11 04:31:26,757][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:31:26,763][233954] mean_value=8.9430069406847, max_value=450.5573728286689[0m
[37m[1m[2023-07-11 04:31:26,765][233954] New mean coefficients: [[ 6.5498505  3.4893746  5.0019054  7.7677956 -2.3180656 -1.0425298]][0m
[37m[1m[2023-07-11 04:31:26,766][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:31:35,734][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 04:31:35,739][233954] FPS: 428281.76[0m
[36m[2023-07-11 04:31:35,742][233954] itr=350, itrs=2000, Progress: 17.50%[0m
[37m[1m[2023-07-11 04:34:52,996][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000330[0m
[36m[2023-07-11 04:35:05,360][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 04:35:05,360][233954] FPS: 327830.95[0m
[36m[2023-07-11 04:35:09,504][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:35:09,505][233954] Reward + Measures: [[108.81453568   0.74245298   0.96468568   0.89606005   0.96612835
    3.26129627]][0m
[37m[1m[2023-07-11 04:35:09,505][233954] Max Reward on eval: 108.81453568105465[0m
[37m[1m[2023-07-11 04:35:09,505][233954] Min Reward on eval: 108.81453568105465[0m
[37m[1m[2023-07-11 04:35:09,505][233954] Mean Reward across all agents: 108.81453568105465[0m
[37m[1m[2023-07-11 04:35:09,506][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:35:14,432][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:35:14,433][233954] Reward + Measures: [[179.05412104   0.93240005   0.88590002   0.92970002   0.0301
    3.80299234]
 [115.66594513   0.68550003   0.94069999   0.84250015   0.93610001
    3.41130233]
 [150.5821091    0.1225       0.96219999   0.67070001   0.94929999
    3.58376002]
 ...
 [ 91.57621227   0.9346       0.82100004   0.93349999   0.14619999
    3.75766063]
 [228.81614115   0.94440001   0.87250006   0.94499999   0.0639
    3.75036049]
 [ 77.55349971   0.87540007   0.83820003   0.88129997   0.0342
    3.73949671]][0m
[37m[1m[2023-07-11 04:35:14,433][233954] Max Reward on eval: 367.8153128724545[0m
[37m[1m[2023-07-11 04:35:14,433][233954] Min Reward on eval: -390.4454791460186[0m
[37m[1m[2023-07-11 04:35:14,433][233954] Mean Reward across all agents: 58.49281721717928[0m
[37m[1m[2023-07-11 04:35:14,434][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:35:14,440][233954] mean_value=-13.01025396298793, max_value=718.6996869452298[0m
[37m[1m[2023-07-11 04:35:14,443][233954] New mean coefficients: [[ 7.3108215  3.8317444  7.608719   8.103227  -1.8308445 -0.61197  ]][0m
[37m[1m[2023-07-11 04:35:14,444][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:35:23,443][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 04:35:23,443][233954] FPS: 426783.03[0m
[36m[2023-07-11 04:35:23,445][233954] itr=351, itrs=2000, Progress: 17.55%[0m
[36m[2023-07-11 04:35:35,068][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 04:35:35,068][233954] FPS: 332647.85[0m
[36m[2023-07-11 04:35:39,327][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:35:39,328][233954] Reward + Measures: [[114.46550411   0.74647564   0.96817636   0.89695036   0.96855932
    3.25446844]][0m
[37m[1m[2023-07-11 04:35:39,328][233954] Max Reward on eval: 114.46550411218529[0m
[37m[1m[2023-07-11 04:35:39,328][233954] Min Reward on eval: 114.46550411218529[0m
[37m[1m[2023-07-11 04:35:39,329][233954] Mean Reward across all agents: 114.46550411218529[0m
[37m[1m[2023-07-11 04:35:39,329][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:35:44,315][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:35:44,316][233954] Reward + Measures: [[154.00021771   0.10699999   0.85369998   0.583        0.85610002
    3.67376947]
 [ 61.07972217   0.0732       0.82870001   0.35479999   0.81490004
    3.57189751]
 [205.82383907   0.0448       0.95220006   0.56389999   0.94779998
    3.72535634]
 ...
 [ 88.67675879   0.0426       0.93810004   0.51719999   0.92559999
    3.67615247]
 [ 99.2486527    0.0846       0.85900003   0.3743       0.84400004
    3.58864713]
 [ 58.7587683    0.0889       0.98969996   0.46110001   0.99279994
    3.48416305]][0m
[37m[1m[2023-07-11 04:35:44,316][233954] Max Reward on eval: 320.58191449716685[0m
[37m[1m[2023-07-11 04:35:44,316][233954] Min Reward on eval: -65.61795928078936[0m
[37m[1m[2023-07-11 04:35:44,317][233954] Mean Reward across all agents: 108.86911124293977[0m
[37m[1m[2023-07-11 04:35:44,317][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:35:44,320][233954] mean_value=-101.03599298315756, max_value=703.3718796838075[0m
[37m[1m[2023-07-11 04:35:44,323][233954] New mean coefficients: [[ 8.574936    5.101087   10.899478    9.095197   -0.60910976  0.6435866 ]][0m
[37m[1m[2023-07-11 04:35:44,324][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:35:53,311][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 04:35:53,311][233954] FPS: 427360.74[0m
[36m[2023-07-11 04:35:53,313][233954] itr=352, itrs=2000, Progress: 17.60%[0m
[36m[2023-07-11 04:36:05,070][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 04:36:05,070][233954] FPS: 328943.78[0m
[36m[2023-07-11 04:36:09,382][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:36:09,383][233954] Reward + Measures: [[108.92494639   0.77601367   0.97963166   0.9144063    0.97959
    3.22393417]][0m
[37m[1m[2023-07-11 04:36:09,383][233954] Max Reward on eval: 108.92494638524313[0m
[37m[1m[2023-07-11 04:36:09,383][233954] Min Reward on eval: 108.92494638524313[0m
[37m[1m[2023-07-11 04:36:09,383][233954] Mean Reward across all agents: 108.92494638524313[0m
[37m[1m[2023-07-11 04:36:09,384][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:36:14,376][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:36:14,377][233954] Reward + Measures: [[ 69.30189525   0.1069       0.75749999   0.4269       0.72819996
    3.30568242]
 [107.46550314   0.0871       0.8427       0.43139997   0.80159998
    3.37840509]
 [ 84.59755482   0.0939       0.88749999   0.42669997   0.85550004
    3.37085462]
 ...
 [ 88.39751388   0.27290002   0.95440006   0.6261       0.94260007
    3.35603595]
 [ 81.5899329    0.18280001   0.93990004   0.53260005   0.91589993
    3.51796079]
 [185.30906016   0.087        0.8721       0.50880003   0.85399991
    3.46041465]][0m
[37m[1m[2023-07-11 04:36:14,377][233954] Max Reward on eval: 342.60267924983054[0m
[37m[1m[2023-07-11 04:36:14,377][233954] Min Reward on eval: -25.286040066881107[0m
[37m[1m[2023-07-11 04:36:14,378][233954] Mean Reward across all agents: 134.92797571725623[0m
[37m[1m[2023-07-11 04:36:14,378][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:36:14,381][233954] mean_value=-67.8745003315594, max_value=685.4201481537968[0m
[37m[1m[2023-07-11 04:36:14,383][233954] New mean coefficients: [[ 9.473145    6.34653    12.786258    9.523963    0.95254457  2.1363983 ]][0m
[37m[1m[2023-07-11 04:36:14,384][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:36:23,375][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 04:36:23,376][233954] FPS: 427166.11[0m
[36m[2023-07-11 04:36:23,378][233954] itr=353, itrs=2000, Progress: 17.65%[0m
[36m[2023-07-11 04:36:35,087][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 04:36:35,087][233954] FPS: 330165.04[0m
[36m[2023-07-11 04:36:39,318][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:36:39,318][233954] Reward + Measures: [[117.03871929   0.78269637   0.9809863    0.91428798   0.97992367
    3.21464992]][0m
[37m[1m[2023-07-11 04:36:39,319][233954] Max Reward on eval: 117.03871929029997[0m
[37m[1m[2023-07-11 04:36:39,319][233954] Min Reward on eval: 117.03871929029997[0m
[37m[1m[2023-07-11 04:36:39,319][233954] Mean Reward across all agents: 117.03871929029997[0m
[37m[1m[2023-07-11 04:36:39,319][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:36:44,244][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:36:44,245][233954] Reward + Measures: [[-25.32589345   0.96930009   0.96420002   0.96270001   0.97430003
    3.43320656]
 [  6.82722116   0.98400003   0.97909993   0.98400003   0.9842
    3.38187408]
 [-32.20458133   0.65640002   0.60230005   0.61989999   0.63700002
    3.30519414]
 ...
 [-33.08131246   0.97329998   0.963        0.97310001   0.97040004
    3.39605021]
 [-25.55244826   0.92950004   0.92649996   0.91790003   0.92390007
    3.26184654]
 [-13.57307727   0.98810005   0.98429996   0.98630011   0.9867
    3.32718396]][0m
[37m[1m[2023-07-11 04:36:44,245][233954] Max Reward on eval: 149.90945626050234[0m
[37m[1m[2023-07-11 04:36:44,245][233954] Min Reward on eval: -159.3720476783812[0m
[37m[1m[2023-07-11 04:36:44,246][233954] Mean Reward across all agents: -21.73663592236621[0m
[37m[1m[2023-07-11 04:36:44,246][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:36:44,248][233954] mean_value=-132.17583202207956, max_value=426.25612339128014[0m
[37m[1m[2023-07-11 04:36:44,250][233954] New mean coefficients: [[10.168574   7.8104873 15.062327  10.234634   2.8910103  3.986341 ]][0m
[37m[1m[2023-07-11 04:36:44,251][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:36:53,156][233954] train() took 8.90 seconds to complete[0m
[36m[2023-07-11 04:36:53,156][233954] FPS: 431284.01[0m
[36m[2023-07-11 04:36:53,158][233954] itr=354, itrs=2000, Progress: 17.70%[0m
[36m[2023-07-11 04:37:04,669][233954] train() took 11.43 seconds to complete[0m
[36m[2023-07-11 04:37:04,669][233954] FPS: 335916.71[0m
[36m[2023-07-11 04:37:08,844][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:37:08,844][233954] Reward + Measures: [[109.14710732   0.78770065   0.98847669   0.91895133   0.98534268
    3.19901133]][0m
[37m[1m[2023-07-11 04:37:08,844][233954] Max Reward on eval: 109.14710732383793[0m
[37m[1m[2023-07-11 04:37:08,845][233954] Min Reward on eval: 109.14710732383793[0m
[37m[1m[2023-07-11 04:37:08,845][233954] Mean Reward across all agents: 109.14710732383793[0m
[37m[1m[2023-07-11 04:37:08,845][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:37:13,812][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:37:13,813][233954] Reward + Measures: [[ 36.46544414   0.1648       0.18180001   0.18929999   0.1433
    2.82073689]
 [ 53.66446567   0.10880001   0.2464       0.14029999   0.25050002
    2.95850825]
 [226.78095902   0.1974       0.28809997   0.42360002   0.41560003
    3.31842041]
 ...
 [ 35.45070083   0.1225       0.12249999   0.11619999   0.1181
    3.05983472]
 [ 85.54464884   0.1436       0.13880001   0.14         0.1296
    3.1629529 ]
 [ 21.56226129   0.11489999   0.111        0.0974       0.11730001
    3.1305027 ]][0m
[37m[1m[2023-07-11 04:37:13,813][233954] Max Reward on eval: 677.8398399382364[0m
[37m[1m[2023-07-11 04:37:13,814][233954] Min Reward on eval: -59.79653734266758[0m
[37m[1m[2023-07-11 04:37:13,814][233954] Mean Reward across all agents: 61.80247473204395[0m
[37m[1m[2023-07-11 04:37:13,814][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:37:13,816][233954] mean_value=-173.51974058678258, max_value=459.10945037837735[0m
[37m[1m[2023-07-11 04:37:13,818][233954] New mean coefficients: [[ 8.588547   6.2076674 11.361516   9.976404   1.191114   1.9519773]][0m
[37m[1m[2023-07-11 04:37:13,819][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:37:22,744][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 04:37:22,744][233954] FPS: 430336.85[0m
[36m[2023-07-11 04:37:22,747][233954] itr=355, itrs=2000, Progress: 17.75%[0m
[36m[2023-07-11 04:37:34,214][233954] train() took 11.39 seconds to complete[0m
[36m[2023-07-11 04:37:34,214][233954] FPS: 337185.12[0m
[36m[2023-07-11 04:37:38,451][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:37:38,451][233954] Reward + Measures: [[113.84808573   0.780316     0.98852068   0.91613996   0.98569691
    3.22258878]][0m
[37m[1m[2023-07-11 04:37:38,451][233954] Max Reward on eval: 113.84808573029231[0m
[37m[1m[2023-07-11 04:37:38,451][233954] Min Reward on eval: 113.84808573029231[0m
[37m[1m[2023-07-11 04:37:38,452][233954] Mean Reward across all agents: 113.84808573029231[0m
[37m[1m[2023-07-11 04:37:38,452][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:37:43,371][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:37:43,371][233954] Reward + Measures: [[-41.02187874   0.1926       0.20619999   0.14030001   0.1585
    3.01330733]
 [ -3.71271975   0.97999996   0.97489995   0.96919996   0.97939998
    3.26449585]
 [-14.10926275   0.99020004   0.99120009   0.98069996   0.98979998
    3.70146728]
 ...
 [ 24.78367253   0.93050003   0.96350002   0.89790004   0.93040001
    3.27409482]
 [ 52.60138148   0.07830001   0.0877       0.0732       0.0588
    3.3519547 ]
 [ 38.03889852   0.0957       0.097        0.0687       0.0677
    3.22565007]][0m
[37m[1m[2023-07-11 04:37:43,372][233954] Max Reward on eval: 88.81141989156603[0m
[37m[1m[2023-07-11 04:37:43,372][233954] Min Reward on eval: -145.92224599423352[0m
[37m[1m[2023-07-11 04:37:43,372][233954] Mean Reward across all agents: 17.66729658016842[0m
[37m[1m[2023-07-11 04:37:43,372][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:37:43,375][233954] mean_value=-168.5221240915078, max_value=451.50842253181526[0m
[37m[1m[2023-07-11 04:37:43,377][233954] New mean coefficients: [[ 6.9436517   4.488336    7.534001    8.9465685  -0.7618841  -0.16010237]][0m
[37m[1m[2023-07-11 04:37:43,378][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:37:52,348][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 04:37:52,348][233954] FPS: 428171.62[0m
[36m[2023-07-11 04:37:52,351][233954] itr=356, itrs=2000, Progress: 17.80%[0m
[36m[2023-07-11 04:38:04,002][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 04:38:04,002][233954] FPS: 331831.07[0m
[36m[2023-07-11 04:38:08,298][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:38:08,298][233954] Reward + Measures: [[118.24715272   0.78520525   0.98856968   0.91741729   0.98647064
    3.2234056 ]][0m
[37m[1m[2023-07-11 04:38:08,299][233954] Max Reward on eval: 118.24715271651442[0m
[37m[1m[2023-07-11 04:38:08,299][233954] Min Reward on eval: 118.24715271651442[0m
[37m[1m[2023-07-11 04:38:08,299][233954] Mean Reward across all agents: 118.24715271651442[0m
[37m[1m[2023-07-11 04:38:08,299][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:38:13,352][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:38:13,353][233954] Reward + Measures: [[ 52.2989206    0.74830002   0.98509997   0.89510006   0.97189999
    3.06350732]
 [ -1.08330151   0.87330002   0.96539992   0.85879993   0.94709998
    2.66344333]
 [  8.75701514   0.56580001   0.81680006   0.60640001   0.84769994
    3.58494186]
 ...
 [ 22.86845096   0.1401       0.1648       0.1876       0.20290001
    2.3421185 ]
 [  0.38010174   0.2951       0.88770002   0.43210003   0.89939994
    3.46552086]
 [-11.09382066   0.58850002   0.93129998   0.64830005   0.92449999
    3.3256402 ]][0m
[37m[1m[2023-07-11 04:38:13,353][233954] Max Reward on eval: 167.73516341727228[0m
[37m[1m[2023-07-11 04:38:13,353][233954] Min Reward on eval: -91.65143874622882[0m
[37m[1m[2023-07-11 04:38:13,354][233954] Mean Reward across all agents: 28.115462907398566[0m
[37m[1m[2023-07-11 04:38:13,354][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:38:13,359][233954] mean_value=3.253737640200622, max_value=579.8412937949412[0m
[37m[1m[2023-07-11 04:38:13,362][233954] New mean coefficients: [[ 7.1654215  4.0427628  6.994976   8.269954  -1.5789244 -0.2551673]][0m
[37m[1m[2023-07-11 04:38:13,363][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:38:22,375][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 04:38:22,375][233954] FPS: 426186.53[0m
[36m[2023-07-11 04:38:22,378][233954] itr=357, itrs=2000, Progress: 17.85%[0m
[36m[2023-07-11 04:38:33,932][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 04:38:33,932][233954] FPS: 334790.16[0m
[36m[2023-07-11 04:38:38,191][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:38:38,192][233954] Reward + Measures: [[126.30313004   0.77944165   0.98793697   0.91179359   0.98526168
    3.23540783]][0m
[37m[1m[2023-07-11 04:38:38,192][233954] Max Reward on eval: 126.30313004309886[0m
[37m[1m[2023-07-11 04:38:38,192][233954] Min Reward on eval: 126.30313004309886[0m
[37m[1m[2023-07-11 04:38:38,192][233954] Mean Reward across all agents: 126.30313004309886[0m
[37m[1m[2023-07-11 04:38:38,193][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:38:43,447][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:38:43,448][233954] Reward + Measures: [[ -7.4514209    0.77450001   0.94400007   0.87600005   0.9698
    3.24116969]
 [ 16.71290501   0.24679999   0.90579998   0.43859997   0.866
    3.07481122]
 [-12.56045489   0.84540004   0.97049999   0.88789999   0.95529997
    3.20897746]
 ...
 [ 89.98314105   0.49990001   0.90340006   0.65289992   0.8707
    3.13581157]
 [138.98737289   0.48079997   0.98180002   0.80150002   0.9684
    3.3478303 ]
 [ 22.46404032   0.0379       0.96390003   0.48030004   0.94130003
    3.16911149]][0m
[37m[1m[2023-07-11 04:38:43,448][233954] Max Reward on eval: 292.00325905177743[0m
[37m[1m[2023-07-11 04:38:43,449][233954] Min Reward on eval: -293.6114082279149[0m
[37m[1m[2023-07-11 04:38:43,449][233954] Mean Reward across all agents: 46.518723555677376[0m
[37m[1m[2023-07-11 04:38:43,449][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:38:43,454][233954] mean_value=-56.15739247179774, max_value=318.2188266064019[0m
[37m[1m[2023-07-11 04:38:43,456][233954] New mean coefficients: [[ 7.906043    4.8033166   8.430698    8.892415   -0.6667992   0.52935785]][0m
[37m[1m[2023-07-11 04:38:43,457][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:38:52,538][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 04:38:52,538][233954] FPS: 422975.14[0m
[36m[2023-07-11 04:38:52,540][233954] itr=358, itrs=2000, Progress: 17.90%[0m
[36m[2023-07-11 04:39:04,581][233954] train() took 11.96 seconds to complete[0m
[36m[2023-07-11 04:39:04,586][233954] FPS: 321091.82[0m
[36m[2023-07-11 04:39:08,888][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:39:08,889][233954] Reward + Measures: [[-36.60641851   0.93970001   0.97282773   0.91080135   0.96033335
    2.78999281]][0m
[37m[1m[2023-07-11 04:39:08,889][233954] Max Reward on eval: -36.606418507795546[0m
[37m[1m[2023-07-11 04:39:08,889][233954] Min Reward on eval: -36.606418507795546[0m
[37m[1m[2023-07-11 04:39:08,889][233954] Mean Reward across all agents: -36.606418507795546[0m
[37m[1m[2023-07-11 04:39:08,890][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:39:13,916][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:39:13,917][233954] Reward + Measures: [[-57.79473567   0.64469999   0.991        0.83669996   0.98470002
    3.12262011]
 [-62.08005512   0.97659999   0.98740005   0.95580006   0.97869998
    2.75123954]
 [-55.48061675   0.95060009   0.98290008   0.92799997   0.9691
    2.77450204]
 ...
 [-25.41545164   0.97440004   0.98839998   0.95030004   0.97999996
    2.65307927]
 [ 99.91356785   0.29050002   0.96429998   0.66430002   0.96820003
    3.1687417 ]
 [-92.68419931   0.95620006   0.96950001   0.92940009   0.95459998
    2.99027753]][0m
[37m[1m[2023-07-11 04:39:13,917][233954] Max Reward on eval: 99.91356785427779[0m
[37m[1m[2023-07-11 04:39:13,917][233954] Min Reward on eval: -256.45492981579156[0m
[37m[1m[2023-07-11 04:39:13,917][233954] Mean Reward across all agents: -65.61130249425175[0m
[37m[1m[2023-07-11 04:39:13,918][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:39:13,920][233954] mean_value=-148.60139162833875, max_value=334.8673269124936[0m
[37m[1m[2023-07-11 04:39:13,922][233954] New mean coefficients: [[ 9.004075   6.5439253  9.721278  10.390395   1.3026663  3.035482 ]][0m
[37m[1m[2023-07-11 04:39:13,923][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:39:22,981][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 04:39:22,981][233954] FPS: 424035.84[0m
[36m[2023-07-11 04:39:22,983][233954] itr=359, itrs=2000, Progress: 17.95%[0m
[36m[2023-07-11 04:39:34,555][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 04:39:34,555][233954] FPS: 334219.72[0m
[36m[2023-07-11 04:39:38,827][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:39:38,828][233954] Reward + Measures: [[-23.77567632   0.95286167   0.98033267   0.92833859   0.97029001
    2.84946942]][0m
[37m[1m[2023-07-11 04:39:38,828][233954] Max Reward on eval: -23.77567631771887[0m
[37m[1m[2023-07-11 04:39:38,828][233954] Min Reward on eval: -23.77567631771887[0m
[37m[1m[2023-07-11 04:39:38,829][233954] Mean Reward across all agents: -23.77567631771887[0m
[37m[1m[2023-07-11 04:39:38,829][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:39:43,836][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:39:43,837][233954] Reward + Measures: [[-83.75173317   0.98250002   0.99260008   0.9740001    0.98700011
    3.10515833]
 [-55.74244621   0.99319994   0.9946       0.9867       0.99230003
    3.19803333]
 [-55.54334619   0.95279998   0.89039993   0.91909999   0.92010003
    3.01935172]
 ...
 [-72.81751578   0.97100002   0.98970002   0.9623       0.97760004
    2.95613813]
 [-89.33089319   0.9849       0.98820001   0.97690004   0.98320001
    3.18685174]
 [-65.1157246    0.98120004   0.96719998   0.96950001   0.97570002
    3.09277844]][0m
[37m[1m[2023-07-11 04:39:43,837][233954] Max Reward on eval: 22.983273445256053[0m
[37m[1m[2023-07-11 04:39:43,837][233954] Min Reward on eval: -237.30185175258666[0m
[37m[1m[2023-07-11 04:39:43,838][233954] Mean Reward across all agents: -92.64694176243876[0m
[37m[1m[2023-07-11 04:39:43,838][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:39:43,839][233954] mean_value=-234.8193317458666, max_value=453.0549342656508[0m
[37m[1m[2023-07-11 04:39:43,842][233954] New mean coefficients: [[ 9.427439   6.137545  10.538688   9.183278   0.7353373  2.442774 ]][0m
[37m[1m[2023-07-11 04:39:43,843][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:39:52,769][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 04:39:52,769][233954] FPS: 430258.65[0m
[36m[2023-07-11 04:39:52,772][233954] itr=360, itrs=2000, Progress: 18.00%[0m
[37m[1m[2023-07-11 04:43:00,050][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000340[0m
[36m[2023-07-11 04:43:12,279][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 04:43:12,279][233954] FPS: 332061.70[0m
[36m[2023-07-11 04:43:16,450][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:43:16,451][233954] Reward + Measures: [[-11.41943819   0.97622198   0.98838836   0.95802593   0.98391998
    2.87743425]][0m
[37m[1m[2023-07-11 04:43:16,451][233954] Max Reward on eval: -11.419438190787702[0m
[37m[1m[2023-07-11 04:43:16,451][233954] Min Reward on eval: -11.419438190787702[0m
[37m[1m[2023-07-11 04:43:16,451][233954] Mean Reward across all agents: -11.419438190787702[0m
[37m[1m[2023-07-11 04:43:16,452][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:43:21,301][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:43:21,302][233954] Reward + Measures: [[-87.39175834   0.82419997   0.9788       0.91240007   0.98000002
    3.23657584]
 [  3.94721975   0.6911       0.9745       0.86870003   0.97479993
    3.19176269]
 [-47.46427625   0.94309998   0.98640007   0.95139998   0.9842
    3.16908813]
 ...
 [  4.06788957   0.51800001   0.8969       0.66250002   0.90860003
    3.0618012 ]
 [-93.23040579   0.8524       0.98390007   0.94         0.9813
    3.23436022]
 [-84.90379765   0.89580005   0.93370008   0.92490005   0.96130002
    3.25504923]][0m
[37m[1m[2023-07-11 04:43:21,302][233954] Max Reward on eval: 116.92016647569835[0m
[37m[1m[2023-07-11 04:43:21,303][233954] Min Reward on eval: -242.57562068160624[0m
[37m[1m[2023-07-11 04:43:21,303][233954] Mean Reward across all agents: -21.169677648061867[0m
[37m[1m[2023-07-11 04:43:21,303][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:43:21,305][233954] mean_value=-139.03196589928677, max_value=418.2821040700362[0m
[37m[1m[2023-07-11 04:43:21,308][233954] New mean coefficients: [[11.821592   8.504731  15.590394   9.772425   3.2851706  5.3295574]][0m
[37m[1m[2023-07-11 04:43:21,309][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:43:30,244][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 04:43:30,245][233954] FPS: 429804.95[0m
[36m[2023-07-11 04:43:30,247][233954] itr=361, itrs=2000, Progress: 18.05%[0m
[36m[2023-07-11 04:43:41,825][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 04:43:41,825][233954] FPS: 333967.08[0m
[36m[2023-07-11 04:43:46,196][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:43:46,201][233954] Reward + Measures: [[-7.49494019  0.98320693  0.98961002  0.96838033  0.98654228  2.90753579]][0m
[37m[1m[2023-07-11 04:43:46,202][233954] Max Reward on eval: -7.49494019054147[0m
[37m[1m[2023-07-11 04:43:46,202][233954] Min Reward on eval: -7.49494019054147[0m
[37m[1m[2023-07-11 04:43:46,202][233954] Mean Reward across all agents: -7.49494019054147[0m
[37m[1m[2023-07-11 04:43:46,202][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:43:51,202][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:43:51,208][233954] Reward + Measures: [[ 112.4684719     0.40830001    0.94099998    0.65509999    0.93430007
     3.30457997]
 [ -63.1964515     0.54299998    0.8179        0.514         0.83490002
     3.16167712]
 [ -12.66222094    0.73979998    0.76599997    0.71689999    0.73510009
     2.9916985 ]
 ...
 [ -56.25946484    0.77410001    0.87709999    0.74550003    0.88929999
     3.23797536]
 [  20.96853093    0.7748        0.83719999    0.75669998    0.82859993
     3.21900034]
 [-150.73702405    0.377         0.87190002    0.45749998    0.89020008
     3.29524851]][0m
[37m[1m[2023-07-11 04:43:51,208][233954] Max Reward on eval: 163.44089702917262[0m
[37m[1m[2023-07-11 04:43:51,208][233954] Min Reward on eval: -210.74634576421232[0m
[37m[1m[2023-07-11 04:43:51,209][233954] Mean Reward across all agents: -33.201850840425706[0m
[37m[1m[2023-07-11 04:43:51,209][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:43:51,212][233954] mean_value=-70.16372453172194, max_value=520.180073301059[0m
[37m[1m[2023-07-11 04:43:51,214][233954] New mean coefficients: [[12.369359   9.137865  16.864187  10.804049   4.1124334  6.1017036]][0m
[37m[1m[2023-07-11 04:43:51,215][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:44:00,211][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 04:44:00,211][233954] FPS: 426948.34[0m
[36m[2023-07-11 04:44:00,213][233954] itr=362, itrs=2000, Progress: 18.10%[0m
[36m[2023-07-11 04:44:11,921][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 04:44:11,921][233954] FPS: 330226.93[0m
[36m[2023-07-11 04:44:16,269][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:44:16,270][233954] Reward + Measures: [[30.65112818  0.98468637  0.98838627  0.9711206   0.98553765  2.95931792]][0m
[37m[1m[2023-07-11 04:44:16,270][233954] Max Reward on eval: 30.651128176250786[0m
[37m[1m[2023-07-11 04:44:16,270][233954] Min Reward on eval: 30.651128176250786[0m
[37m[1m[2023-07-11 04:44:16,270][233954] Mean Reward across all agents: 30.651128176250786[0m
[37m[1m[2023-07-11 04:44:16,271][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:44:21,244][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:44:21,245][233954] Reward + Measures: [[  6.05775364   0.0972       0.85039997   0.48839998   0.84600002
    3.02706885]
 [ 33.50613329   0.17879999   0.41950002   0.1884       0.44829997
    2.43644905]
 [-62.91859246   0.87849998   0.94630003   0.83750004   0.90670007
    2.84877467]
 ...
 [158.15951543   0.1146       0.66400003   0.3321       0.65970004
    2.73743987]
 [108.93518178   0.14740001   0.50150001   0.2462       0.52470005
    2.59942889]
 [-39.03410552   0.91060001   0.95550007   0.87340003   0.92469996
    2.91786933]][0m
[37m[1m[2023-07-11 04:44:21,245][233954] Max Reward on eval: 478.6794340667082[0m
[37m[1m[2023-07-11 04:44:21,245][233954] Min Reward on eval: -97.05728048523888[0m
[37m[1m[2023-07-11 04:44:21,246][233954] Mean Reward across all agents: 62.712719589212135[0m
[37m[1m[2023-07-11 04:44:21,246][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:44:21,252][233954] mean_value=64.90513465509122, max_value=583.1777981638518[0m
[37m[1m[2023-07-11 04:44:21,255][233954] New mean coefficients: [[11.892222   8.527203  16.42125   10.165664   3.539941   5.0383286]][0m
[37m[1m[2023-07-11 04:44:21,256][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:44:30,312][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 04:44:30,313][233954] FPS: 424077.43[0m
[36m[2023-07-11 04:44:30,315][233954] itr=363, itrs=2000, Progress: 18.15%[0m
[36m[2023-07-11 04:44:41,999][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 04:44:41,999][233954] FPS: 330850.44[0m
[36m[2023-07-11 04:44:46,256][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:44:46,257][233954] Reward + Measures: [[59.56021613  0.98831296  0.99007827  0.97685432  0.98802733  2.99311352]][0m
[37m[1m[2023-07-11 04:44:46,257][233954] Max Reward on eval: 59.56021612777811[0m
[37m[1m[2023-07-11 04:44:46,257][233954] Min Reward on eval: 59.56021612777811[0m
[37m[1m[2023-07-11 04:44:46,257][233954] Mean Reward across all agents: 59.56021612777811[0m
[37m[1m[2023-07-11 04:44:46,258][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:44:51,201][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:44:51,202][233954] Reward + Measures: [[-10.6831821    0.80599993   0.90000004   0.76170003   0.84990007
    3.12684608]
 [-32.18123045   0.79900002   0.86739999   0.75549996   0.83389997
    3.07614803]
 [ 50.58262722   0.8035       0.93239993   0.7403       0.87760001
    3.13996816]
 ...
 [-60.75939839   0.90879995   0.95830005   0.8757       0.93959999
    3.11153984]
 [  4.85642725   0.80740005   0.87989998   0.77030003   0.84630007
    3.00137019]
 [ 14.87490946   0.82560009   0.89099997   0.78750002   0.86230004
    3.07601285]][0m
[37m[1m[2023-07-11 04:44:51,202][233954] Max Reward on eval: 102.1507563673891[0m
[37m[1m[2023-07-11 04:44:51,202][233954] Min Reward on eval: -173.37278952840717[0m
[37m[1m[2023-07-11 04:44:51,202][233954] Mean Reward across all agents: -12.640216361162311[0m
[37m[1m[2023-07-11 04:44:51,203][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:44:51,205][233954] mean_value=-103.17810204836978, max_value=299.2144622217122[0m
[37m[1m[2023-07-11 04:44:51,207][233954] New mean coefficients: [[12.749894   9.10256   18.412237  10.284577   4.2950683  6.0220976]][0m
[37m[1m[2023-07-11 04:44:51,208][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:45:00,286][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 04:45:00,286][233954] FPS: 423054.89[0m
[36m[2023-07-11 04:45:00,289][233954] itr=364, itrs=2000, Progress: 18.20%[0m
[36m[2023-07-11 04:45:11,952][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 04:45:11,952][233954] FPS: 331477.63[0m
[36m[2023-07-11 04:45:16,282][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:45:16,282][233954] Reward + Measures: [[67.45164728  0.97896165  0.98971027  0.96728969  0.98850131  2.99263191]][0m
[37m[1m[2023-07-11 04:45:16,283][233954] Max Reward on eval: 67.4516472753064[0m
[37m[1m[2023-07-11 04:45:16,283][233954] Min Reward on eval: 67.4516472753064[0m
[37m[1m[2023-07-11 04:45:16,283][233954] Mean Reward across all agents: 67.4516472753064[0m
[37m[1m[2023-07-11 04:45:16,283][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:45:21,198][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:45:21,198][233954] Reward + Measures: [[-58.99127639   0.85179996   0.89029998   0.80820006   0.88760006
    2.8176682 ]
 [-98.39945152   0.40489998   0.45699999   0.3073       0.48839998
    2.53990364]
 [-57.50305272   0.47790003   0.523        0.39449999   0.53380007
    2.47839212]
 ...
 [-52.19732219   0.94029999   0.94480002   0.90949994   0.93710005
    2.91973424]
 [-43.51192314   0.87980002   0.90809995   0.85769999   0.8818
    2.81624389]
 [-44.04220427   0.7762       0.81229991   0.71449995   0.8035
    2.70238471]][0m
[37m[1m[2023-07-11 04:45:21,199][233954] Max Reward on eval: 24.797049917047843[0m
[37m[1m[2023-07-11 04:45:21,199][233954] Min Reward on eval: -136.07256591613404[0m
[37m[1m[2023-07-11 04:45:21,199][233954] Mean Reward across all agents: -51.412914191535535[0m
[37m[1m[2023-07-11 04:45:21,199][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:45:21,202][233954] mean_value=-98.19063151807104, max_value=246.5147257160811[0m
[37m[1m[2023-07-11 04:45:21,204][233954] New mean coefficients: [[13.71236    9.244538  20.91912   10.277682   4.3428135  5.8783617]][0m
[37m[1m[2023-07-11 04:45:21,205][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:45:30,081][233954] train() took 8.87 seconds to complete[0m
[36m[2023-07-11 04:45:30,081][233954] FPS: 432721.76[0m
[36m[2023-07-11 04:45:30,083][233954] itr=365, itrs=2000, Progress: 18.25%[0m
[36m[2023-07-11 04:45:41,610][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 04:45:41,610][233954] FPS: 335571.49[0m
[36m[2023-07-11 04:45:45,825][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:45:45,826][233954] Reward + Measures: [[11.83086617  0.82699072  0.93713933  0.78736097  0.92539126  2.79696059]][0m
[37m[1m[2023-07-11 04:45:45,826][233954] Max Reward on eval: 11.83086616622952[0m
[37m[1m[2023-07-11 04:45:45,826][233954] Min Reward on eval: 11.83086616622952[0m
[37m[1m[2023-07-11 04:45:45,827][233954] Mean Reward across all agents: 11.83086616622952[0m
[37m[1m[2023-07-11 04:45:45,827][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:45:50,803][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:45:50,804][233954] Reward + Measures: [[-23.87473538   0.81170005   0.83859998   0.722        0.85499996
    2.79648352]
 [ 21.92249842   0.54210001   0.60009998   0.46989998   0.55430001
    2.69546628]
 [-29.44516621   0.7974       0.82870001   0.71630001   0.82460004
    2.80252004]
 ...
 [-17.56398277   0.62029999   0.70550007   0.54980004   0.67670006
    2.73564029]
 [ 16.09145853   0.61370003   0.64850008   0.54949999   0.6505
    2.67694449]
 [-23.13303936   0.74040002   0.83920002   0.67179996   0.80000001
    2.83599639]][0m
[37m[1m[2023-07-11 04:45:50,804][233954] Max Reward on eval: 108.49230432303156[0m
[37m[1m[2023-07-11 04:45:50,804][233954] Min Reward on eval: -85.52698849630542[0m
[37m[1m[2023-07-11 04:45:50,805][233954] Mean Reward across all agents: -1.209057432347479[0m
[37m[1m[2023-07-11 04:45:50,805][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:45:50,809][233954] mean_value=-28.19803547771315, max_value=500.5828692157394[0m
[37m[1m[2023-07-11 04:45:50,811][233954] New mean coefficients: [[14.275763   8.650167  22.10479    9.647357   3.3458862  4.1552687]][0m
[37m[1m[2023-07-11 04:45:50,812][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:45:59,748][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 04:45:59,748][233954] FPS: 429782.91[0m
[36m[2023-07-11 04:45:59,751][233954] itr=366, itrs=2000, Progress: 18.30%[0m
[36m[2023-07-11 04:46:11,305][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 04:46:11,306][233954] FPS: 334609.80[0m
[36m[2023-07-11 04:46:15,615][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:46:15,616][233954] Reward + Measures: [[-5.04282264  0.91312534  0.9669227   0.88243699  0.96262175  2.81488729]][0m
[37m[1m[2023-07-11 04:46:15,616][233954] Max Reward on eval: -5.042822642272035[0m
[37m[1m[2023-07-11 04:46:15,616][233954] Min Reward on eval: -5.042822642272035[0m
[37m[1m[2023-07-11 04:46:15,616][233954] Mean Reward across all agents: -5.042822642272035[0m
[37m[1m[2023-07-11 04:46:15,616][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:46:20,581][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:46:20,582][233954] Reward + Measures: [[-12.41565203   0.3035       0.38220003   0.27940002   0.39120004
    2.58468413]
 [ -9.77563958   0.90259999   0.96499997   0.87019998   0.9436
    2.50568461]
 [-18.00170904   0.97460002   0.99239999   0.96750003   0.98789996
    2.6535244 ]
 ...
 [-23.86138587   0.98199999   0.99519998   0.96610004   0.99110001
    2.57707691]
 [-21.7394957    0.89210004   0.94390005   0.8545       0.92269993
    2.53788924]
 [-23.37475029   0.97850001   0.99349993   0.96359998   0.9896
    2.63627791]][0m
[37m[1m[2023-07-11 04:46:20,582][233954] Max Reward on eval: 31.660918219247833[0m
[37m[1m[2023-07-11 04:46:20,582][233954] Min Reward on eval: -46.480321523314345[0m
[37m[1m[2023-07-11 04:46:20,583][233954] Mean Reward across all agents: -6.41540001017015[0m
[37m[1m[2023-07-11 04:46:20,583][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:46:20,585][233954] mean_value=-69.77430289633057, max_value=394.4181853554919[0m
[37m[1m[2023-07-11 04:46:20,588][233954] New mean coefficients: [[15.390657  10.405717  24.048426  11.399009   5.4513855  6.33079  ]][0m
[37m[1m[2023-07-11 04:46:20,589][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:46:29,594][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 04:46:29,595][233954] FPS: 426473.09[0m
[36m[2023-07-11 04:46:29,597][233954] itr=367, itrs=2000, Progress: 18.35%[0m
[36m[2023-07-11 04:46:41,239][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 04:46:41,239][233954] FPS: 332132.55[0m
[36m[2023-07-11 04:46:45,572][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:46:45,573][233954] Reward + Measures: [[-26.15951627   0.95058966   0.984465     0.92854607   0.97834361
    2.88837171]][0m
[37m[1m[2023-07-11 04:46:45,573][233954] Max Reward on eval: -26.159516268475038[0m
[37m[1m[2023-07-11 04:46:45,573][233954] Min Reward on eval: -26.159516268475038[0m
[37m[1m[2023-07-11 04:46:45,573][233954] Mean Reward across all agents: -26.159516268475038[0m
[37m[1m[2023-07-11 04:46:45,574][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:46:50,811][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:46:50,812][233954] Reward + Measures: [[-51.21768415   0.90700001   0.97150004   0.87050003   0.95710003
    2.8347621 ]
 [-39.9736401    0.96700001   0.98949999   0.94460005   0.9842
    2.85041928]
 [-27.67715989   0.91379994   0.98229998   0.87370008   0.97410005
    2.87218046]
 ...
 [ 59.85669576   0.36040002   0.47580004   0.32600001   0.54369998
    2.61021352]
 [ 60.55355143   0.3125       0.43020001   0.2658       0.4797
    2.65142369]
 [ 19.34564578   0.79219997   0.93829995   0.75769997   0.90910006
    2.945297  ]][0m
[37m[1m[2023-07-11 04:46:50,812][233954] Max Reward on eval: 113.38123129983433[0m
[37m[1m[2023-07-11 04:46:50,812][233954] Min Reward on eval: -65.7004992986098[0m
[37m[1m[2023-07-11 04:46:50,812][233954] Mean Reward across all agents: 13.466385245617909[0m
[37m[1m[2023-07-11 04:46:50,813][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:46:50,816][233954] mean_value=-67.01126381323836, max_value=169.51319306802853[0m
[37m[1m[2023-07-11 04:46:50,819][233954] New mean coefficients: [[16.408848  12.034578  26.035814  12.217899   7.3233423  8.465174 ]][0m
[37m[1m[2023-07-11 04:46:50,820][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:46:59,911][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 04:46:59,912][233954] FPS: 422433.53[0m
[36m[2023-07-11 04:46:59,914][233954] itr=368, itrs=2000, Progress: 18.40%[0m
[36m[2023-07-11 04:47:11,663][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 04:47:11,664][233954] FPS: 329024.88[0m
[36m[2023-07-11 04:47:15,884][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:47:15,884][233954] Reward + Measures: [[-14.03026988   0.97806597   0.98099267   0.96322262   0.98361832
    2.83561516]][0m
[37m[1m[2023-07-11 04:47:15,884][233954] Max Reward on eval: -14.03026988256868[0m
[37m[1m[2023-07-11 04:47:15,884][233954] Min Reward on eval: -14.03026988256868[0m
[37m[1m[2023-07-11 04:47:15,885][233954] Mean Reward across all agents: -14.03026988256868[0m
[37m[1m[2023-07-11 04:47:15,885][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:47:20,919][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:47:20,919][233954] Reward + Measures: [[ -3.94677154   0.97850001   0.98760003   0.97460002   0.97390002
    3.48481679]
 [  4.38118008   0.74660009   0.82440007   0.70289999   0.76840001
    3.18472409]
 [  8.73384613   0.90599996   0.95880002   0.88410008   0.91350001
    3.25550842]
 ...
 [ -0.27803204   0.72729999   0.82790005   0.67609996   0.76900005
    3.09754634]
 [-23.08602709   0.99110001   0.98610002   0.98699999   0.98619998
    3.2940681 ]
 [-10.37112285   0.98239994   0.98929995   0.97749996   0.9806
    3.40387011]][0m
[37m[1m[2023-07-11 04:47:20,920][233954] Max Reward on eval: 39.046554346755144[0m
[37m[1m[2023-07-11 04:47:20,920][233954] Min Reward on eval: -60.984652278060096[0m
[37m[1m[2023-07-11 04:47:20,920][233954] Mean Reward across all agents: -9.849871790451884[0m
[37m[1m[2023-07-11 04:47:20,920][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:47:20,922][233954] mean_value=-119.62035142367152, max_value=250.97495431789042[0m
[37m[1m[2023-07-11 04:47:20,924][233954] New mean coefficients: [[18.80191  14.422459 31.042711 14.41561   9.79908  11.705834]][0m
[37m[1m[2023-07-11 04:47:20,925][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:47:29,995][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 04:47:29,995][233954] FPS: 423463.19[0m
[36m[2023-07-11 04:47:29,998][233954] itr=369, itrs=2000, Progress: 18.45%[0m
[36m[2023-07-11 04:47:41,691][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 04:47:41,691][233954] FPS: 330651.12[0m
[36m[2023-07-11 04:47:46,011][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:47:46,011][233954] Reward + Measures: [[3.77310047 0.97695261 0.97184396 0.962457   0.98076034 2.82104754]][0m
[37m[1m[2023-07-11 04:47:46,012][233954] Max Reward on eval: 3.7731004666701313[0m
[37m[1m[2023-07-11 04:47:46,012][233954] Min Reward on eval: 3.7731004666701313[0m
[37m[1m[2023-07-11 04:47:46,012][233954] Mean Reward across all agents: 3.7731004666701313[0m
[37m[1m[2023-07-11 04:47:46,012][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:47:51,018][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:47:51,018][233954] Reward + Measures: [[ 23.51621751   0.24530001   0.70649999   0.26929998   0.71830004
    2.80022025]
 [-16.62199056   0.18169999   0.19070001   0.16240001   0.1803
    2.4758985 ]
 [ 12.79209252   0.1402       0.142        0.12789999   0.142
    2.61090899]
 ...
 [-25.1071908    0.17830001   0.1717       0.167        0.17220001
    2.59324026]
 [ 18.69880674   0.15629999   0.17739999   0.15530001   0.15549999
    2.51267982]
 [ 30.30880257   0.26520002   0.7184       0.30130002   0.74369997
    2.82079697]][0m
[37m[1m[2023-07-11 04:47:51,019][233954] Max Reward on eval: 89.80516680274158[0m
[37m[1m[2023-07-11 04:47:51,019][233954] Min Reward on eval: -64.89737641885877[0m
[37m[1m[2023-07-11 04:47:51,019][233954] Mean Reward across all agents: 1.7291075374510447[0m
[37m[1m[2023-07-11 04:47:51,019][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:47:51,022][233954] mean_value=-377.0748645251081, max_value=374.285262539298[0m
[37m[1m[2023-07-11 04:47:51,024][233954] New mean coefficients: [[16.634502  12.014562  25.573536  12.3407135  7.0655856  8.934183 ]][0m
[37m[1m[2023-07-11 04:47:51,025][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:48:00,056][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 04:48:00,056][233954] FPS: 425292.42[0m
[36m[2023-07-11 04:48:00,058][233954] itr=370, itrs=2000, Progress: 18.50%[0m
[37m[1m[2023-07-11 04:51:12,992][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000350[0m
[36m[2023-07-11 04:51:25,265][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 04:51:25,265][233954] FPS: 328391.00[0m
[36m[2023-07-11 04:51:29,401][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:51:29,401][233954] Reward + Measures: [[23.33264944  0.9774183   0.96645069  0.96224838  0.98119724  2.8128345 ]][0m
[37m[1m[2023-07-11 04:51:29,402][233954] Max Reward on eval: 23.3326494406343[0m
[37m[1m[2023-07-11 04:51:29,402][233954] Min Reward on eval: 23.3326494406343[0m
[37m[1m[2023-07-11 04:51:29,402][233954] Mean Reward across all agents: 23.3326494406343[0m
[37m[1m[2023-07-11 04:51:29,402][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:51:34,329][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:51:34,329][233954] Reward + Measures: [[ -49.19693948    0.64899999    0.74599999    0.64569998    0.81960005
     3.20989919]
 [ -90.31142626    0.47420001    0.56510001    0.48909998    0.67659998
     3.10959959]
 [  -0.90268581    0.90039998    0.93230003    0.90780002    0.95959997
     3.23890877]
 ...
 [ -11.76753837    0.82029992    0.88160002    0.8488        0.95819998
     3.35611129]
 [-128.37875878    0.38910002    0.48319998    0.3646        0.57170004
     2.95119166]
 [ -17.90536127    0.79770005    0.86650002    0.81140006    0.92960006
     3.3040483 ]][0m
[37m[1m[2023-07-11 04:51:34,330][233954] Max Reward on eval: 16.100541655439883[0m
[37m[1m[2023-07-11 04:51:34,330][233954] Min Reward on eval: -150.10347102172673[0m
[37m[1m[2023-07-11 04:51:34,330][233954] Mean Reward across all agents: -39.30236508532628[0m
[37m[1m[2023-07-11 04:51:34,330][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:51:34,334][233954] mean_value=-81.64412025405397, max_value=451.41314838440155[0m
[37m[1m[2023-07-11 04:51:34,337][233954] New mean coefficients: [[16.89622   12.0797    26.558043  12.9103565  6.9099574  8.527896 ]][0m
[37m[1m[2023-07-11 04:51:34,338][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:51:43,310][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 04:51:43,310][233954] FPS: 428073.68[0m
[36m[2023-07-11 04:51:43,312][233954] itr=371, itrs=2000, Progress: 18.55%[0m
[36m[2023-07-11 04:51:55,080][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 04:51:55,080][233954] FPS: 328541.67[0m
[36m[2023-07-11 04:51:59,356][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:51:59,357][233954] Reward + Measures: [[39.22452597  0.9714824   0.97197169  0.9622373   0.9712413   3.33291149]][0m
[37m[1m[2023-07-11 04:51:59,357][233954] Max Reward on eval: 39.224525974600525[0m
[37m[1m[2023-07-11 04:51:59,357][233954] Min Reward on eval: 39.224525974600525[0m
[37m[1m[2023-07-11 04:51:59,357][233954] Mean Reward across all agents: 39.224525974600525[0m
[37m[1m[2023-07-11 04:51:59,358][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:52:04,275][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:52:04,276][233954] Reward + Measures: [[ -68.85556067    0.94960004    0.96939993    0.92930001    0.95809996
     3.53738189]
 [-133.09655621    0.69419998    0.71719998    0.64020002    0.73789996
     3.32320476]
 [   2.51861072    0.4752        0.52340001    0.39990002    0.5632
     2.94210482]
 ...
 [-191.9143921     0.93790001    0.9526        0.92390007    0.95609999
     3.57439041]
 [  14.65483461    0.87369996    0.9005        0.82889998    0.88910002
     3.42666292]
 [ -86.48579545    0.66600001    0.70110005    0.61770004    0.70170003
     3.19385958]][0m
[37m[1m[2023-07-11 04:52:04,276][233954] Max Reward on eval: 47.454751150123776[0m
[37m[1m[2023-07-11 04:52:04,276][233954] Min Reward on eval: -233.49620741968974[0m
[37m[1m[2023-07-11 04:52:04,277][233954] Mean Reward across all agents: -51.0233932094683[0m
[37m[1m[2023-07-11 04:52:04,277][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:52:04,279][233954] mean_value=-177.20729318414215, max_value=216.44825286158047[0m
[37m[1m[2023-07-11 04:52:04,281][233954] New mean coefficients: [[17.057812 11.998558 27.187984 13.289598  6.948895  7.869921]][0m
[37m[1m[2023-07-11 04:52:04,282][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:52:13,272][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 04:52:13,272][233954] FPS: 427233.54[0m
[36m[2023-07-11 04:52:13,274][233954] itr=372, itrs=2000, Progress: 18.60%[0m
[36m[2023-07-11 04:52:24,913][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 04:52:24,913][233954] FPS: 332210.23[0m
[36m[2023-07-11 04:52:29,119][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:52:29,119][233954] Reward + Measures: [[41.14521604  0.97452474  0.97491407  0.96391505  0.973104    3.33326292]][0m
[37m[1m[2023-07-11 04:52:29,120][233954] Max Reward on eval: 41.14521604450272[0m
[37m[1m[2023-07-11 04:52:29,120][233954] Min Reward on eval: 41.14521604450272[0m
[37m[1m[2023-07-11 04:52:29,120][233954] Mean Reward across all agents: 41.14521604450272[0m
[37m[1m[2023-07-11 04:52:29,120][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:52:34,111][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:52:34,111][233954] Reward + Measures: [[ 32.24043056   0.33930001   0.45439997   0.30899999   0.44230005
    2.88254333]
 [-48.40316101   0.83759993   0.91870004   0.79689997   0.88440001
    3.23849654]
 [  9.23538577   0.63239998   0.7841       0.58140004   0.75769997
    3.06973362]
 ...
 [ 13.66875567   0.51750004   0.7141       0.46080002   0.68839997
    3.0734272 ]
 [ 76.93774596   0.25960001   0.34530002   0.24170001   0.32179999
    2.75473571]
 [ 63.44188737   0.5984       0.79290003   0.53640002   0.72240001
    3.07234931]][0m
[37m[1m[2023-07-11 04:52:34,111][233954] Max Reward on eval: 230.57503081061878[0m
[37m[1m[2023-07-11 04:52:34,112][233954] Min Reward on eval: -142.5144033389166[0m
[37m[1m[2023-07-11 04:52:34,112][233954] Mean Reward across all agents: 13.55340765681447[0m
[37m[1m[2023-07-11 04:52:34,112][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:52:34,116][233954] mean_value=-74.42262714243816, max_value=370.1275658793488[0m
[37m[1m[2023-07-11 04:52:34,119][233954] New mean coefficients: [[16.334473  11.773137  25.384739  12.86949    6.6999207  8.314701 ]][0m
[37m[1m[2023-07-11 04:52:34,119][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:52:43,077][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 04:52:43,077][233954] FPS: 428765.44[0m
[36m[2023-07-11 04:52:43,080][233954] itr=373, itrs=2000, Progress: 18.65%[0m
[36m[2023-07-11 04:52:54,780][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 04:52:54,781][233954] FPS: 330442.85[0m
[36m[2023-07-11 04:52:59,004][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:52:59,004][233954] Reward + Measures: [[37.90698175  0.98069268  0.98110271  0.97218508  0.97977763  3.32308006]][0m
[37m[1m[2023-07-11 04:52:59,005][233954] Max Reward on eval: 37.90698174775535[0m
[37m[1m[2023-07-11 04:52:59,005][233954] Min Reward on eval: 37.90698174775535[0m
[37m[1m[2023-07-11 04:52:59,005][233954] Mean Reward across all agents: 37.90698174775535[0m
[37m[1m[2023-07-11 04:52:59,006][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:53:03,918][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:53:03,919][233954] Reward + Measures: [[-119.26736446    0.9903        0.99290001    0.97060007    0.99039996
     3.34056902]
 [-136.16387177    0.98640007    0.99069995    0.97119999    0.98900002
     3.18538451]
 [ -58.47889598    0.91680002    0.94570011    0.88040012    0.95230007
     3.09945989]
 ...
 [-147.1016283     0.99040002    0.99180001    0.97719997    0.99139994
     3.30265856]
 [ -84.68726875    0.9801001     0.98680001    0.9623        0.9867
     3.15534782]
 [ -99.74140238    0.95880002    0.96999997    0.93450004    0.96579999
     3.23679209]][0m
[37m[1m[2023-07-11 04:53:03,919][233954] Max Reward on eval: 72.9244683496654[0m
[37m[1m[2023-07-11 04:53:03,919][233954] Min Reward on eval: -183.5428342604777[0m
[37m[1m[2023-07-11 04:53:03,920][233954] Mean Reward across all agents: -72.47159721364746[0m
[37m[1m[2023-07-11 04:53:03,920][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:53:03,921][233954] mean_value=-197.92831805754207, max_value=264.31290721939183[0m
[37m[1m[2023-07-11 04:53:03,924][233954] New mean coefficients: [[17.900793 13.078196 29.116457 14.360773  8.065833  9.761466]][0m
[37m[1m[2023-07-11 04:53:03,925][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:53:12,918][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 04:53:12,918][233954] FPS: 427046.52[0m
[36m[2023-07-11 04:53:12,921][233954] itr=374, itrs=2000, Progress: 18.70%[0m
[36m[2023-07-11 04:53:24,788][233954] train() took 11.78 seconds to complete[0m
[36m[2023-07-11 04:53:24,789][233954] FPS: 325853.40[0m
[36m[2023-07-11 04:53:29,106][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:53:29,106][233954] Reward + Measures: [[36.59703451  0.98071939  0.98128736  0.9722724   0.97955793  3.31756186]][0m
[37m[1m[2023-07-11 04:53:29,106][233954] Max Reward on eval: 36.597034508735234[0m
[37m[1m[2023-07-11 04:53:29,107][233954] Min Reward on eval: 36.597034508735234[0m
[37m[1m[2023-07-11 04:53:29,107][233954] Mean Reward across all agents: 36.597034508735234[0m
[37m[1m[2023-07-11 04:53:29,107][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:53:34,087][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:53:34,088][233954] Reward + Measures: [[-25.90903874   0.37730002   0.42220002   0.34310001   0.4032
    2.71236968]
 [-14.97262851   0.47269997   0.5061       0.398        0.4883
    2.78464389]
 [-15.20294657   0.58540004   0.6045       0.49779996   0.59050006
    2.89314818]
 ...
 [-51.88803522   0.61970001   0.63330001   0.52069998   0.60150003
    2.91712308]
 [ -0.65386051   0.74329996   0.74649996   0.67370003   0.78200001
    2.90070033]
 [-39.68208097   0.63770002   0.67720002   0.58199996   0.69910002
    2.99706125]][0m
[37m[1m[2023-07-11 04:53:34,088][233954] Max Reward on eval: 30.06942774876952[0m
[37m[1m[2023-07-11 04:53:34,088][233954] Min Reward on eval: -213.25876195449382[0m
[37m[1m[2023-07-11 04:53:34,088][233954] Mean Reward across all agents: -41.0623541220652[0m
[37m[1m[2023-07-11 04:53:34,088][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:53:34,091][233954] mean_value=-211.2404319686809, max_value=256.3312513985143[0m
[37m[1m[2023-07-11 04:53:34,093][233954] New mean coefficients: [[16.05      11.150301  24.844917  11.909829   6.0586042  8.114835 ]][0m
[37m[1m[2023-07-11 04:53:34,094][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:53:43,094][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 04:53:43,094][233954] FPS: 426755.47[0m
[36m[2023-07-11 04:53:43,096][233954] itr=375, itrs=2000, Progress: 18.75%[0m
[36m[2023-07-11 04:53:54,946][233954] train() took 11.77 seconds to complete[0m
[36m[2023-07-11 04:53:54,946][233954] FPS: 326331.68[0m
[36m[2023-07-11 04:53:59,159][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:53:59,160][233954] Reward + Measures: [[41.49751731  0.98249167  0.98278606  0.97384268  0.98092139  3.32623792]][0m
[37m[1m[2023-07-11 04:53:59,160][233954] Max Reward on eval: 41.49751730937452[0m
[37m[1m[2023-07-11 04:53:59,160][233954] Min Reward on eval: 41.49751730937452[0m
[37m[1m[2023-07-11 04:53:59,160][233954] Mean Reward across all agents: 41.49751730937452[0m
[37m[1m[2023-07-11 04:53:59,160][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:54:04,062][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:54:04,063][233954] Reward + Measures: [[-70.14287455   0.91640007   0.88740009   0.89890003   0.90809995
    3.34384084]
 [-33.67078465   0.45619997   0.4341       0.42649999   0.45479998
    2.76893783]
 [-68.67091919   0.64510006   0.61449999   0.60350001   0.65140003
    2.93376708]
 ...
 [-59.58527921   0.89600003   0.88870001   0.88050002   0.87729996
    3.2790277 ]
 [-52.64944366   0.77819997   0.76109999   0.75270003   0.76730007
    3.15520453]
 [-59.42778498   0.84619999   0.81409997   0.81259996   0.82460004
    3.2413938 ]][0m
[37m[1m[2023-07-11 04:54:04,063][233954] Max Reward on eval: 63.86812913782196[0m
[37m[1m[2023-07-11 04:54:04,063][233954] Min Reward on eval: -114.141393196024[0m
[37m[1m[2023-07-11 04:54:04,064][233954] Mean Reward across all agents: -55.29769831717547[0m
[37m[1m[2023-07-11 04:54:04,064][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:54:04,066][233954] mean_value=-141.0510659937179, max_value=156.66624567178914[0m
[37m[1m[2023-07-11 04:54:04,068][233954] New mean coefficients: [[15.826051  11.408668  24.113638  11.945427   6.4056935  8.083227 ]][0m
[37m[1m[2023-07-11 04:54:04,069][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:54:12,989][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 04:54:12,989][233954] FPS: 430576.02[0m
[36m[2023-07-11 04:54:12,992][233954] itr=376, itrs=2000, Progress: 18.80%[0m
[36m[2023-07-11 04:54:24,702][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 04:54:24,703][233954] FPS: 330187.93[0m
[36m[2023-07-11 04:54:29,011][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:54:29,012][233954] Reward + Measures: [[47.31633765  0.98512697  0.98518229  0.97649074  0.98318237  3.31760097]][0m
[37m[1m[2023-07-11 04:54:29,012][233954] Max Reward on eval: 47.316337652234516[0m
[37m[1m[2023-07-11 04:54:29,012][233954] Min Reward on eval: 47.316337652234516[0m
[37m[1m[2023-07-11 04:54:29,012][233954] Mean Reward across all agents: 47.316337652234516[0m
[37m[1m[2023-07-11 04:54:29,013][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:54:34,148][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:54:34,149][233954] Reward + Measures: [[-100.36088812    0.9867        0.98789996    0.96960014    0.98380005
     3.62527156]
 [-122.02619481    0.96239996    0.96569997    0.9386999     0.96490002
     3.49988341]
 [ -78.40863728    0.97279996    0.97729999    0.95069999    0.97240001
     3.56570029]
 ...
 [ -72.71334774    0.60530001    0.63970006    0.58149999    0.61479998
     3.17129397]
 [-112.93435157    0.95740002    0.96490002    0.93710005    0.94740003
     3.29135561]
 [-112.94154097    0.97630006    0.97769994    0.95559996    0.97970003
     3.555758  ]][0m
[37m[1m[2023-07-11 04:54:34,149][233954] Max Reward on eval: 38.13773296512663[0m
[37m[1m[2023-07-11 04:54:34,149][233954] Min Reward on eval: -260.86722135283054[0m
[37m[1m[2023-07-11 04:54:34,150][233954] Mean Reward across all agents: -90.41248295515797[0m
[37m[1m[2023-07-11 04:54:34,150][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:54:34,151][233954] mean_value=-198.55010786180065, max_value=175.4864141104492[0m
[37m[1m[2023-07-11 04:54:34,154][233954] New mean coefficients: [[17.287794 12.818525 27.704166 13.057916  7.871816  9.411514]][0m
[37m[1m[2023-07-11 04:54:34,155][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:54:43,047][233954] train() took 8.89 seconds to complete[0m
[36m[2023-07-11 04:54:43,048][233954] FPS: 431893.55[0m
[36m[2023-07-11 04:54:43,050][233954] itr=377, itrs=2000, Progress: 18.85%[0m
[36m[2023-07-11 04:54:54,598][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 04:54:54,598][233954] FPS: 334795.86[0m
[36m[2023-07-11 04:54:58,912][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:54:58,912][233954] Reward + Measures: [[-24.54349493   0.98765838   0.9876883    0.98045135   0.98670131
    3.27018952]][0m
[37m[1m[2023-07-11 04:54:58,913][233954] Max Reward on eval: -24.54349493267578[0m
[37m[1m[2023-07-11 04:54:58,913][233954] Min Reward on eval: -24.54349493267578[0m
[37m[1m[2023-07-11 04:54:58,913][233954] Mean Reward across all agents: -24.54349493267578[0m
[37m[1m[2023-07-11 04:54:58,913][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:55:03,862][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:55:03,862][233954] Reward + Measures: [[114.41942394   0.24519999   0.3409       0.25240001   0.34489998
    2.71500444]
 [ 57.79580486   0.21040002   0.4118       0.2234       0.4161
    2.70813465]
 [146.16867046   0.34200001   0.3432       0.3389       0.35419998
    2.79270911]
 ...
 [-35.17788809   0.25900003   0.68169993   0.25650001   0.68099999
    2.83669162]
 [-10.60101126   0.3026       0.59170002   0.30270001   0.60350007
    2.75416732]
 [ 89.47915306   0.2299       0.2414       0.24920002   0.2712
    2.66219711]][0m
[37m[1m[2023-07-11 04:55:03,862][233954] Max Reward on eval: 214.44348530811257[0m
[37m[1m[2023-07-11 04:55:03,863][233954] Min Reward on eval: -84.6697998070158[0m
[37m[1m[2023-07-11 04:55:03,863][233954] Mean Reward across all agents: 72.15724515965586[0m
[37m[1m[2023-07-11 04:55:03,863][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:55:03,866][233954] mean_value=-76.65139538493268, max_value=490.6106553884223[0m
[37m[1m[2023-07-11 04:55:03,869][233954] New mean coefficients: [[16.335552  11.120604  25.269135  12.330026   5.6879463  7.515819 ]][0m
[37m[1m[2023-07-11 04:55:03,870][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:55:12,831][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 04:55:12,831][233954] FPS: 428600.79[0m
[36m[2023-07-11 04:55:12,834][233954] itr=378, itrs=2000, Progress: 18.90%[0m
[36m[2023-07-11 04:55:24,444][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 04:55:24,444][233954] FPS: 333107.62[0m
[36m[2023-07-11 04:55:28,627][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:55:28,628][233954] Reward + Measures: [[-4.04853528  0.98706394  0.98681235  0.97968799  0.98610431  3.27191663]][0m
[37m[1m[2023-07-11 04:55:28,628][233954] Max Reward on eval: -4.0485352820636376[0m
[37m[1m[2023-07-11 04:55:28,628][233954] Min Reward on eval: -4.0485352820636376[0m
[37m[1m[2023-07-11 04:55:28,629][233954] Mean Reward across all agents: -4.0485352820636376[0m
[37m[1m[2023-07-11 04:55:28,629][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:55:33,570][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:55:33,571][233954] Reward + Measures: [[-56.19144727   0.99089998   0.96649992   0.98909998   0.98640007
    3.45953989]
 [-48.85557938   0.95220006   0.94709998   0.94410002   0.9702
    3.41901183]
 [-11.04343894   0.70839995   0.70099998   0.68370003   0.7414
    3.27059054]
 ...
 [-43.2076306    0.9127       0.90679997   0.91660005   0.9436
    3.44217086]
 [-52.9001677    0.98660004   0.96269989   0.98509997   0.98600006
    3.41004872]
 [-17.40759344   0.90900004   0.89880002   0.90749997   0.93580002
    3.41002727]][0m
[37m[1m[2023-07-11 04:55:33,571][233954] Max Reward on eval: 10.865096686035395[0m
[37m[1m[2023-07-11 04:55:33,571][233954] Min Reward on eval: -171.41974616404622[0m
[37m[1m[2023-07-11 04:55:33,571][233954] Mean Reward across all agents: -44.10101018462971[0m
[37m[1m[2023-07-11 04:55:33,572][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:55:33,573][233954] mean_value=-136.60026982670757, max_value=331.70938194982534[0m
[37m[1m[2023-07-11 04:55:33,576][233954] New mean coefficients: [[18.133667  12.966169  30.055313  14.396698   7.8734145  9.277386 ]][0m
[37m[1m[2023-07-11 04:55:33,577][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:55:42,446][233954] train() took 8.87 seconds to complete[0m
[36m[2023-07-11 04:55:42,446][233954] FPS: 433044.00[0m
[36m[2023-07-11 04:55:42,448][233954] itr=379, itrs=2000, Progress: 18.95%[0m
[36m[2023-07-11 04:55:54,095][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 04:55:54,096][233954] FPS: 331984.31[0m
[36m[2023-07-11 04:55:58,363][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:55:58,364][233954] Reward + Measures: [[-47.951539     0.91698068   0.972139     0.89803565   0.97615331
    2.62077999]][0m
[37m[1m[2023-07-11 04:55:58,364][233954] Max Reward on eval: -47.95153900313104[0m
[37m[1m[2023-07-11 04:55:58,364][233954] Min Reward on eval: -47.95153900313104[0m
[37m[1m[2023-07-11 04:55:58,364][233954] Mean Reward across all agents: -47.95153900313104[0m
[37m[1m[2023-07-11 04:55:58,365][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:56:03,337][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:56:03,337][233954] Reward + Measures: [[55.98803836  0.82910007  0.87960005  0.82480001  0.86920005  3.11684561]
 [85.03430936  0.6971001   0.76100004  0.68839997  0.7353      3.07634735]
 [68.40063264  0.6257      0.76960003  0.61860001  0.70329994  3.11073184]
 ...
 [67.54883144  0.75        0.8427      0.73509997  0.79229999  3.14745903]
 [20.65459828  0.34209999  0.31710002  0.294       0.3249      2.78022933]
 [39.54335497  0.69410002  0.76020002  0.62599999  0.70880002  3.05043221]][0m
[37m[1m[2023-07-11 04:56:03,338][233954] Max Reward on eval: 126.93323703277856[0m
[37m[1m[2023-07-11 04:56:03,338][233954] Min Reward on eval: -15.026077613141387[0m
[37m[1m[2023-07-11 04:56:03,338][233954] Mean Reward across all agents: 54.158532838023305[0m
[37m[1m[2023-07-11 04:56:03,338][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:56:03,342][233954] mean_value=-34.989782124966524, max_value=252.4159899648567[0m
[37m[1m[2023-07-11 04:56:03,345][233954] New mean coefficients: [[18.06332   12.842881  29.971039  14.360162   7.7795396  9.201191 ]][0m
[37m[1m[2023-07-11 04:56:03,346][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:56:12,298][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 04:56:12,298][233954] FPS: 429022.48[0m
[36m[2023-07-11 04:56:12,300][233954] itr=380, itrs=2000, Progress: 19.00%[0m
[37m[1m[2023-07-11 04:59:26,357][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000360[0m
[36m[2023-07-11 04:59:38,661][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 04:59:38,661][233954] FPS: 330058.53[0m
[36m[2023-07-11 04:59:42,731][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:59:42,731][233954] Reward + Measures: [[-42.9244735    0.941576     0.98210937   0.93118805   0.98332763
    2.6952486 ]][0m
[37m[1m[2023-07-11 04:59:42,732][233954] Max Reward on eval: -42.92447350397715[0m
[37m[1m[2023-07-11 04:59:42,732][233954] Min Reward on eval: -42.92447350397715[0m
[37m[1m[2023-07-11 04:59:42,732][233954] Mean Reward across all agents: -42.92447350397715[0m
[37m[1m[2023-07-11 04:59:42,732][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:59:47,611][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 04:59:47,612][233954] Reward + Measures: [[-26.78221203   0.20130001   0.44260001   0.2          0.5029
    2.81291628]
 [-29.44883575   0.25030002   0.50480002   0.23000002   0.56720001
    2.82393193]
 [  7.16207998   0.2325       0.40060002   0.228        0.44099998
    2.74243617]
 ...
 [  9.11481163   0.2359       0.37240002   0.22070001   0.40180001
    2.74630046]
 [-18.76082393   0.18789999   0.4224       0.2191       0.4436
    2.78187919]
 [ -0.93198683   0.21030001   0.30740002   0.1869       0.33050004
    2.72172737]][0m
[37m[1m[2023-07-11 04:59:47,612][233954] Max Reward on eval: 45.22763792392798[0m
[37m[1m[2023-07-11 04:59:47,612][233954] Min Reward on eval: -70.45364259015768[0m
[37m[1m[2023-07-11 04:59:47,613][233954] Mean Reward across all agents: -8.95804851104152[0m
[37m[1m[2023-07-11 04:59:47,613][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 04:59:47,615][233954] mean_value=-192.82558659332352, max_value=496.1117175824009[0m
[37m[1m[2023-07-11 04:59:47,617][233954] New mean coefficients: [[16.388552  10.989506  26.316584  12.25413    5.5804186  6.860211 ]][0m
[37m[1m[2023-07-11 04:59:47,618][233954] Moving the mean solution point...[0m
[36m[2023-07-11 04:59:56,496][233954] train() took 8.88 seconds to complete[0m
[36m[2023-07-11 04:59:56,497][233954] FPS: 432591.36[0m
[36m[2023-07-11 04:59:56,499][233954] itr=381, itrs=2000, Progress: 19.05%[0m
[36m[2023-07-11 05:00:08,058][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 05:00:08,058][233954] FPS: 334501.94[0m
[36m[2023-07-11 05:00:12,277][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:00:12,278][233954] Reward + Measures: [[-32.26691499   0.94920099   0.98551732   0.94222105   0.985937
    2.77614546]][0m
[37m[1m[2023-07-11 05:00:12,278][233954] Max Reward on eval: -32.266914987703146[0m
[37m[1m[2023-07-11 05:00:12,278][233954] Min Reward on eval: -32.266914987703146[0m
[37m[1m[2023-07-11 05:00:12,278][233954] Mean Reward across all agents: -32.266914987703146[0m
[37m[1m[2023-07-11 05:00:12,279][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:00:17,174][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:00:17,175][233954] Reward + Measures: [[-9.39666384  0.23550001  0.24159999  0.21610001  0.28990003  2.32688022]
 [ 9.7342022   0.2203      0.34870002  0.2103      0.3734      2.28834987]
 [48.96842155  0.25419998  0.33970001  0.22129999  0.3766      2.31550407]
 ...
 [-6.92138416  0.226       0.22219999  0.22019999  0.25049999  2.30056763]
 [26.72137266  0.20700002  0.31130001  0.2069      0.33590004  2.30263567]
 [ 5.31474313  0.24640003  0.22090001  0.22810002  0.26140001  2.24845886]][0m
[37m[1m[2023-07-11 05:00:17,175][233954] Max Reward on eval: 203.42478753020987[0m
[37m[1m[2023-07-11 05:00:17,175][233954] Min Reward on eval: -145.29823590018788[0m
[37m[1m[2023-07-11 05:00:17,176][233954] Mean Reward across all agents: 24.12918327536564[0m
[37m[1m[2023-07-11 05:00:17,176][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:00:17,180][233954] mean_value=-156.28815482735146, max_value=503.0537369415164[0m
[37m[1m[2023-07-11 05:00:17,182][233954] New mean coefficients: [[13.999037   9.193012  21.708546  10.4522915  3.5670037  4.22358  ]][0m
[37m[1m[2023-07-11 05:00:17,183][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:00:26,113][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 05:00:26,113][233954] FPS: 430107.20[0m
[36m[2023-07-11 05:00:26,115][233954] itr=382, itrs=2000, Progress: 19.10%[0m
[36m[2023-07-11 05:00:37,685][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 05:00:37,690][233954] FPS: 334313.18[0m
[36m[2023-07-11 05:00:41,898][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:00:41,899][233954] Reward + Measures: [[-24.3476271    0.955603     0.98755866   0.95024806   0.98809332
    2.79535842]][0m
[37m[1m[2023-07-11 05:00:41,899][233954] Max Reward on eval: -24.347627098240114[0m
[37m[1m[2023-07-11 05:00:41,899][233954] Min Reward on eval: -24.347627098240114[0m
[37m[1m[2023-07-11 05:00:41,899][233954] Mean Reward across all agents: -24.347627098240114[0m
[37m[1m[2023-07-11 05:00:41,900][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:00:46,859][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:00:46,859][233954] Reward + Measures: [[-124.65968609    0.93829995    0.96260005    0.90480006    0.94029999
     3.30802011]
 [ -95.10647585    0.96569997    0.97209996    0.94450009    0.95990002
     3.06011176]
 [ -91.94272325    0.93940002    0.95069999    0.90459996    0.93180007
     3.20160413]
 ...
 [ -88.48748542    0.94229996    0.9709        0.89860004    0.9471001
     3.17417431]
 [-129.32147622    0.9648        0.98140001    0.93489999    0.96540004
     3.25061989]
 [ -78.85918737    0.95629996    0.96760005    0.92910004    0.95370007
     3.12359118]][0m
[37m[1m[2023-07-11 05:00:46,859][233954] Max Reward on eval: 28.363816014304756[0m
[37m[1m[2023-07-11 05:00:46,860][233954] Min Reward on eval: -168.17654704870657[0m
[37m[1m[2023-07-11 05:00:46,860][233954] Mean Reward across all agents: -80.3763852567502[0m
[37m[1m[2023-07-11 05:00:46,860][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:00:46,862][233954] mean_value=-207.04753101361592, max_value=424.56200936784967[0m
[37m[1m[2023-07-11 05:00:46,864][233954] New mean coefficients: [[12.129218   8.106116  17.28008    9.469339   2.4251466  2.5848658]][0m
[37m[1m[2023-07-11 05:00:46,865][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:00:55,886][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 05:00:55,886][233954] FPS: 425763.45[0m
[36m[2023-07-11 05:00:55,888][233954] itr=383, itrs=2000, Progress: 19.15%[0m
[36m[2023-07-11 05:01:07,706][233954] train() took 11.74 seconds to complete[0m
[36m[2023-07-11 05:01:07,706][233954] FPS: 327171.63[0m
[36m[2023-07-11 05:01:11,987][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:01:11,988][233954] Reward + Measures: [[-15.51298912   0.96491957   0.989851     0.9607473    0.98938501
    2.78285408]][0m
[37m[1m[2023-07-11 05:01:11,988][233954] Max Reward on eval: -15.512989119773323[0m
[37m[1m[2023-07-11 05:01:11,988][233954] Min Reward on eval: -15.512989119773323[0m
[37m[1m[2023-07-11 05:01:11,988][233954] Mean Reward across all agents: -15.512989119773323[0m
[37m[1m[2023-07-11 05:01:11,989][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:01:16,931][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:01:16,932][233954] Reward + Measures: [[ 8.17247958  0.47889996  0.63809997  0.46059999  0.60640001  2.6135757 ]
 [16.20990454  0.34189999  0.53979999  0.32210001  0.51669997  2.62416148]
 [22.75464894  0.58039999  0.82240009  0.52039999  0.8186      2.74811482]
 ...
 [23.31580328  0.57240003  0.78149998  0.52509999  0.76450008  2.75249529]
 [52.0393951   0.57429999  0.89089996  0.55439997  0.8337      2.75547028]
 [10.20014931  0.81660002  0.95020008  0.77999997  0.92070007  2.59601831]][0m
[37m[1m[2023-07-11 05:01:16,932][233954] Max Reward on eval: 98.32651446146774[0m
[37m[1m[2023-07-11 05:01:16,932][233954] Min Reward on eval: -97.18107344778255[0m
[37m[1m[2023-07-11 05:01:16,932][233954] Mean Reward across all agents: 24.180163855511548[0m
[37m[1m[2023-07-11 05:01:16,933][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:01:16,937][233954] mean_value=-9.127591781579207, max_value=326.12191639993034[0m
[37m[1m[2023-07-11 05:01:16,940][233954] New mean coefficients: [[11.985201   9.168396  16.555523  10.0115185  3.5550487  3.582697 ]][0m
[37m[1m[2023-07-11 05:01:16,941][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:01:25,938][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 05:01:25,939][233954] FPS: 426867.56[0m
[36m[2023-07-11 05:01:25,941][233954] itr=384, itrs=2000, Progress: 19.20%[0m
[36m[2023-07-11 05:01:37,523][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 05:01:37,523][233954] FPS: 333804.72[0m
[36m[2023-07-11 05:01:41,805][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:01:41,805][233954] Reward + Measures: [[-10.69709537   0.96870995   0.99150461   0.96577638   0.98997706
    2.80689311]][0m
[37m[1m[2023-07-11 05:01:41,806][233954] Max Reward on eval: -10.697095371560863[0m
[37m[1m[2023-07-11 05:01:41,806][233954] Min Reward on eval: -10.697095371560863[0m
[37m[1m[2023-07-11 05:01:41,806][233954] Mean Reward across all agents: -10.697095371560863[0m
[37m[1m[2023-07-11 05:01:41,806][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:01:46,921][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:01:46,921][233954] Reward + Measures: [[-58.60232726   0.19050001   0.29260001   0.2172       0.30630001
    2.30402303]
 [-78.9330169    0.2131       0.25640002   0.2251       0.27719998
    2.22237015]
 [-39.68873136   0.22830001   0.6875       0.2861       0.73119998
    2.56483531]
 ...
 [-99.35628661   0.17990001   0.54229999   0.32209998   0.51789999
    2.45657873]
 [-61.60972696   0.1858       0.51450002   0.23469999   0.51130003
    2.44933629]
 [  9.77705129   0.26199999   0.69480008   0.25710002   0.71820009
    2.57579017]][0m
[37m[1m[2023-07-11 05:01:46,922][233954] Max Reward on eval: 45.07862337809056[0m
[37m[1m[2023-07-11 05:01:46,922][233954] Min Reward on eval: -137.15855600517244[0m
[37m[1m[2023-07-11 05:01:46,922][233954] Mean Reward across all agents: -46.6137386972007[0m
[37m[1m[2023-07-11 05:01:46,922][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:01:46,925][233954] mean_value=-179.47847528595187, max_value=423.46909376582687[0m
[37m[1m[2023-07-11 05:01:46,927][233954] New mean coefficients: [[10.115805   7.0808363 11.43516    8.262781   1.1678746  1.6975048]][0m
[37m[1m[2023-07-11 05:01:46,928][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:01:55,841][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 05:01:55,841][233954] FPS: 430927.54[0m
[36m[2023-07-11 05:01:55,843][233954] itr=385, itrs=2000, Progress: 19.25%[0m
[36m[2023-07-11 05:02:07,351][233954] train() took 11.43 seconds to complete[0m
[36m[2023-07-11 05:02:07,351][233954] FPS: 336055.37[0m
[36m[2023-07-11 05:02:11,563][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:02:11,564][233954] Reward + Measures: [[-11.5243277    0.95673031   0.9912346    0.9571054    0.99079424
    2.79624605]][0m
[37m[1m[2023-07-11 05:02:11,564][233954] Max Reward on eval: -11.524327696211015[0m
[37m[1m[2023-07-11 05:02:11,564][233954] Min Reward on eval: -11.524327696211015[0m
[37m[1m[2023-07-11 05:02:11,564][233954] Mean Reward across all agents: -11.524327696211015[0m
[37m[1m[2023-07-11 05:02:11,565][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:02:16,516][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:02:16,516][233954] Reward + Measures: [[  44.28701707    0.92140007    0.94349998    0.91000003    0.93959999
     2.77546096]
 [-118.3992322     0.59850007    0.58720005    0.59780002    0.58640003
     3.08372641]
 [  48.88053099    0.72830003    0.79729998    0.69960004    0.76280004
     2.92154694]
 ...
 [  -6.68344787    0.54150003    0.60650003    0.54829997    0.60869998
     2.8828485 ]
 [-204.73368978    0.5851        0.58470005    0.57380003    0.56620002
     3.12373304]
 [-142.05455257    0.73180002    0.75580007    0.69390005    0.75
     3.35699964]][0m
[37m[1m[2023-07-11 05:02:16,517][233954] Max Reward on eval: 212.18659951444715[0m
[37m[1m[2023-07-11 05:02:16,517][233954] Min Reward on eval: -258.0921654652804[0m
[37m[1m[2023-07-11 05:02:16,517][233954] Mean Reward across all agents: -35.60332004886497[0m
[37m[1m[2023-07-11 05:02:16,517][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:02:16,520][233954] mean_value=-105.08746166272444, max_value=237.82527126094098[0m
[37m[1m[2023-07-11 05:02:16,523][233954] New mean coefficients: [[ 8.908842    5.8776402   7.896681    6.683894   -0.4206643   0.41894686]][0m
[37m[1m[2023-07-11 05:02:16,523][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:02:25,525][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 05:02:25,525][233954] FPS: 426685.12[0m
[36m[2023-07-11 05:02:25,527][233954] itr=386, itrs=2000, Progress: 19.30%[0m
[36m[2023-07-11 05:02:37,205][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 05:02:37,205][233954] FPS: 331071.92[0m
[36m[2023-07-11 05:02:41,499][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:02:41,500][233954] Reward + Measures: [[-9.58660695  0.96636605  0.99275428  0.96628731  0.99145198  2.77204561]][0m
[37m[1m[2023-07-11 05:02:41,500][233954] Max Reward on eval: -9.586606953086797[0m
[37m[1m[2023-07-11 05:02:41,500][233954] Min Reward on eval: -9.586606953086797[0m
[37m[1m[2023-07-11 05:02:41,501][233954] Mean Reward across all agents: -9.586606953086797[0m
[37m[1m[2023-07-11 05:02:41,501][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:02:46,481][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:02:46,482][233954] Reward + Measures: [[ 26.95274191   0.10820001   0.86830008   0.33580002   0.80000001
    3.05913711]
 [ 52.88780661   0.1445       0.33129999   0.22059999   0.30939999
    2.98314548]
 [139.52250814   0.0501       0.95879996   0.40619999   0.89340001
    3.2820797 ]
 ...
 [  4.45459517   0.35050002   0.97430003   0.65700001   0.94200003
    2.80044436]
 [ 59.47412878   0.15769999   0.3125       0.22239999   0.29880002
    2.93528247]
 [171.07348729   0.0537       0.90949994   0.41570002   0.86799997
    3.30226135]][0m
[37m[1m[2023-07-11 05:02:46,482][233954] Max Reward on eval: 572.6759920142591[0m
[37m[1m[2023-07-11 05:02:46,482][233954] Min Reward on eval: -72.2233540961286[0m
[37m[1m[2023-07-11 05:02:46,482][233954] Mean Reward across all agents: 105.7222927156646[0m
[37m[1m[2023-07-11 05:02:46,482][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:02:46,487][233954] mean_value=-21.81109366626372, max_value=329.7823884593178[0m
[37m[1m[2023-07-11 05:02:46,490][233954] New mean coefficients: [[ 9.524445    6.1202703   9.522238    7.3657303  -0.07567048  0.8071166 ]][0m
[37m[1m[2023-07-11 05:02:46,491][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:02:55,459][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 05:02:55,459][233954] FPS: 428254.76[0m
[36m[2023-07-11 05:02:55,461][233954] itr=387, itrs=2000, Progress: 19.35%[0m
[36m[2023-07-11 05:03:07,266][233954] train() took 11.72 seconds to complete[0m
[36m[2023-07-11 05:03:07,266][233954] FPS: 327557.80[0m
[36m[2023-07-11 05:03:11,587][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:03:11,588][233954] Reward + Measures: [[-6.04872443  0.9773643   0.99390298  0.97649562  0.99191129  2.78203416]][0m
[37m[1m[2023-07-11 05:03:11,588][233954] Max Reward on eval: -6.048724425919295[0m
[37m[1m[2023-07-11 05:03:11,588][233954] Min Reward on eval: -6.048724425919295[0m
[37m[1m[2023-07-11 05:03:11,589][233954] Mean Reward across all agents: -6.048724425919295[0m
[37m[1m[2023-07-11 05:03:11,589][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:03:16,610][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:03:16,616][233954] Reward + Measures: [[-29.35457988   0.53249997   0.88070005   0.52530003   0.9176001
    2.88074803]
 [-16.92416965   0.71010011   0.92449999   0.72200006   0.95090002
    2.96346354]
 [-18.70587907   0.79980004   0.96070004   0.80910009   0.97010005
    2.74207354]
 ...
 [  4.2377967    0.17060001   0.71320003   0.21879999   0.70280004
    2.67866302]
 [-11.80392323   0.49490005   0.85299999   0.5          0.87639999
    2.8751328 ]
 [ -3.86208916   0.90830004   0.9799       0.92189997   0.99239999
    2.95985937]][0m
[37m[1m[2023-07-11 05:03:16,616][233954] Max Reward on eval: 53.90493422728032[0m
[37m[1m[2023-07-11 05:03:16,616][233954] Min Reward on eval: -56.76488065810408[0m
[37m[1m[2023-07-11 05:03:16,617][233954] Mean Reward across all agents: -17.9207708263332[0m
[37m[1m[2023-07-11 05:03:16,617][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:03:16,619][233954] mean_value=-72.20061428313879, max_value=488.5420659848489[0m
[37m[1m[2023-07-11 05:03:16,622][233954] New mean coefficients: [[11.952673   6.9541726 15.272763   8.195566   1.5403596  2.6279018]][0m
[37m[1m[2023-07-11 05:03:16,622][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:03:25,669][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 05:03:25,669][233954] FPS: 424556.43[0m
[36m[2023-07-11 05:03:25,671][233954] itr=388, itrs=2000, Progress: 19.40%[0m
[36m[2023-07-11 05:03:37,642][233954] train() took 11.89 seconds to complete[0m
[36m[2023-07-11 05:03:37,643][233954] FPS: 322920.56[0m
[36m[2023-07-11 05:03:41,999][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:03:42,000][233954] Reward + Measures: [[-6.14518103  0.98177165  0.99450493  0.98085207  0.99177897  2.78885412]][0m
[37m[1m[2023-07-11 05:03:42,000][233954] Max Reward on eval: -6.145181031161259[0m
[37m[1m[2023-07-11 05:03:42,000][233954] Min Reward on eval: -6.145181031161259[0m
[37m[1m[2023-07-11 05:03:42,000][233954] Mean Reward across all agents: -6.145181031161259[0m
[37m[1m[2023-07-11 05:03:42,001][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:03:47,039][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:03:47,044][233954] Reward + Measures: [[-17.24552995   0.9224       0.96320003   0.90990001   0.94580001
    2.76563525]
 [ -2.03981814   0.91040003   0.94379997   0.88020003   0.93120003
    2.49312186]
 [-83.14579344   0.89639997   0.94510001   0.85830003   0.95349997
    3.12012434]
 ...
 [-57.98049753   0.92539996   0.95779991   0.90020007   0.94530004
    2.87742591]
 [  5.54889732   0.95410007   0.98640007   0.92589998   0.98289996
    2.66773748]
 [-63.20345149   0.85470003   0.92150003   0.82080001   0.89250004
    2.84182572]][0m
[37m[1m[2023-07-11 05:03:47,045][233954] Max Reward on eval: 53.11196902096272[0m
[37m[1m[2023-07-11 05:03:47,045][233954] Min Reward on eval: -133.69340183185415[0m
[37m[1m[2023-07-11 05:03:47,045][233954] Mean Reward across all agents: -19.69625618974147[0m
[37m[1m[2023-07-11 05:03:47,046][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:03:47,047][233954] mean_value=-98.975872296054, max_value=118.60401931809879[0m
[37m[1m[2023-07-11 05:03:47,050][233954] New mean coefficients: [[12.495092   6.5103846 16.60289    6.879794   1.4439266  1.5771124]][0m
[37m[1m[2023-07-11 05:03:47,051][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:03:56,081][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 05:03:56,082][233954] FPS: 425283.62[0m
[36m[2023-07-11 05:03:56,084][233954] itr=389, itrs=2000, Progress: 19.45%[0m
[36m[2023-07-11 05:04:07,714][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 05:04:07,714][233954] FPS: 332401.74[0m
[36m[2023-07-11 05:04:12,000][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:04:12,005][233954] Reward + Measures: [[-0.00603335  0.98324656  0.99494702  0.98116297  0.99196303  2.79573464]][0m
[37m[1m[2023-07-11 05:04:12,005][233954] Max Reward on eval: -0.006033347226970363[0m
[37m[1m[2023-07-11 05:04:12,006][233954] Min Reward on eval: -0.006033347226970363[0m
[37m[1m[2023-07-11 05:04:12,006][233954] Mean Reward across all agents: -0.006033347226970363[0m
[37m[1m[2023-07-11 05:04:12,006][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:04:17,161][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:04:17,167][233954] Reward + Measures: [[-33.89753389   0.7238       0.93839997   0.67610008   0.89449996
    2.79285026]
 [-10.96073478   0.98820001   0.99080002   0.98650008   0.98990005
    2.89327383]
 [  6.8158103    0.96590006   0.99319994   0.95529997   0.98549998
    2.73744535]
 ...
 [-16.93615764   0.2836       0.61739999   0.2296       0.56599998
    2.57622099]
 [-13.0966985    0.92659998   0.98540002   0.91900009   0.96880001
    2.90751529]
 [ 39.82765485   0.2234       0.37649998   0.182        0.33239999
    2.49770451]][0m
[37m[1m[2023-07-11 05:04:17,168][233954] Max Reward on eval: 96.48507884438149[0m
[37m[1m[2023-07-11 05:04:17,168][233954] Min Reward on eval: -48.05400536444504[0m
[37m[1m[2023-07-11 05:04:17,168][233954] Mean Reward across all agents: -9.791577330554224[0m
[37m[1m[2023-07-11 05:04:17,168][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:04:17,172][233954] mean_value=-64.93687484126131, max_value=390.6648789600128[0m
[37m[1m[2023-07-11 05:04:17,174][233954] New mean coefficients: [[13.825373   8.354122  19.75363    7.7166314  3.882858   4.4606996]][0m
[37m[1m[2023-07-11 05:04:17,175][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:04:26,160][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 05:04:26,161][233954] FPS: 427444.76[0m
[36m[2023-07-11 05:04:26,163][233954] itr=390, itrs=2000, Progress: 19.50%[0m
[37m[1m[2023-07-11 05:07:43,376][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000370[0m
[36m[2023-07-11 05:07:55,486][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 05:07:55,486][233954] FPS: 331323.47[0m
[36m[2023-07-11 05:07:59,634][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:07:59,635][233954] Reward + Measures: [[0.0626332  0.9801296  0.99470329 0.97704995 0.99185431 2.82435274]][0m
[37m[1m[2023-07-11 05:07:59,635][233954] Max Reward on eval: 0.06263320148012523[0m
[37m[1m[2023-07-11 05:07:59,635][233954] Min Reward on eval: 0.06263320148012523[0m
[37m[1m[2023-07-11 05:07:59,635][233954] Mean Reward across all agents: 0.06263320148012523[0m
[37m[1m[2023-07-11 05:07:59,636][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:08:04,570][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:08:04,570][233954] Reward + Measures: [[ -66.21111013    0.39369997    0.91119999    0.44580004    0.93969995
     3.06474876]
 [-144.6724305     0.94080013    0.97200006    0.91569996    0.97679996
     3.12226462]
 [-114.81680774    0.90690005    0.96450007    0.8853001     0.97570002
     2.94886756]
 ...
 [-129.95057963    0.95590001    0.97060007    0.92959994    0.97320002
     3.16396189]
 [  46.89342084    0.3145        0.97439998    0.54719996    0.9813
     2.95810151]
 [ -54.56315804    0.3874        0.8488        0.41289997    0.88120002
     3.06121969]][0m
[37m[1m[2023-07-11 05:08:04,570][233954] Max Reward on eval: 298.9268050212413[0m
[37m[1m[2023-07-11 05:08:04,571][233954] Min Reward on eval: -206.9785909488797[0m
[37m[1m[2023-07-11 05:08:04,571][233954] Mean Reward across all agents: -66.2323766808105[0m
[37m[1m[2023-07-11 05:08:04,571][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:08:04,573][233954] mean_value=-131.5595819687891, max_value=435.92621960579413[0m
[37m[1m[2023-07-11 05:08:04,576][233954] New mean coefficients: [[14.644985   7.077894  21.907858   6.016734   2.8431964  3.3597753]][0m
[37m[1m[2023-07-11 05:08:04,577][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:08:13,474][233954] train() took 8.90 seconds to complete[0m
[36m[2023-07-11 05:08:13,475][233954] FPS: 431641.11[0m
[36m[2023-07-11 05:08:13,477][233954] itr=391, itrs=2000, Progress: 19.55%[0m
[36m[2023-07-11 05:08:24,974][233954] train() took 11.42 seconds to complete[0m
[36m[2023-07-11 05:08:24,974][233954] FPS: 336288.94[0m
[36m[2023-07-11 05:08:29,270][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:08:29,270][233954] Reward + Measures: [[13.44344026  0.98194134  0.99477398  0.97641063  0.99183899  2.84538603]][0m
[37m[1m[2023-07-11 05:08:29,271][233954] Max Reward on eval: 13.443440262598102[0m
[37m[1m[2023-07-11 05:08:29,271][233954] Min Reward on eval: 13.443440262598102[0m
[37m[1m[2023-07-11 05:08:29,271][233954] Mean Reward across all agents: 13.443440262598102[0m
[37m[1m[2023-07-11 05:08:29,272][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:08:34,462][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:08:34,462][233954] Reward + Measures: [[-45.024451     0.72520006   0.86279994   0.71869993   0.949
    2.90318537]
 [ -6.6243092    0.88199997   0.93629998   0.90290004   0.96900004
    3.07542849]
 [-60.54972148   0.57090002   0.7177       0.59820002   0.80940002
    2.97720957]
 ...
 [-25.79110457   0.79269999   0.89480001   0.81630003   0.96439999
    3.02469635]
 [ -7.52885773   0.72040004   0.81950009   0.74720001   0.85799998
    3.10197878]
 [-24.19777238   0.73159999   0.8592       0.76960003   0.94410002
    3.08548975]][0m
[37m[1m[2023-07-11 05:08:34,463][233954] Max Reward on eval: 12.890131203830242[0m
[37m[1m[2023-07-11 05:08:34,463][233954] Min Reward on eval: -84.65819363191258[0m
[37m[1m[2023-07-11 05:08:34,463][233954] Mean Reward across all agents: -32.51462362950163[0m
[37m[1m[2023-07-11 05:08:34,463][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:08:34,466][233954] mean_value=-58.49172802436799, max_value=467.80638528070415[0m
[37m[1m[2023-07-11 05:08:34,468][233954] New mean coefficients: [[14.80605    7.043305  22.419436   5.000983   3.3288846  3.9541123]][0m
[37m[1m[2023-07-11 05:08:34,469][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:08:43,445][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 05:08:43,445][233954] FPS: 427902.64[0m
[36m[2023-07-11 05:08:43,447][233954] itr=392, itrs=2000, Progress: 19.60%[0m
[36m[2023-07-11 05:08:55,089][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 05:08:55,089][233954] FPS: 332091.34[0m
[36m[2023-07-11 05:08:59,337][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:08:59,338][233954] Reward + Measures: [[24.65911361  0.97850132  0.99489999  0.97330534  0.99149793  2.84887004]][0m
[37m[1m[2023-07-11 05:08:59,338][233954] Max Reward on eval: 24.659113612578825[0m
[37m[1m[2023-07-11 05:08:59,338][233954] Min Reward on eval: 24.659113612578825[0m
[37m[1m[2023-07-11 05:08:59,338][233954] Mean Reward across all agents: 24.659113612578825[0m
[37m[1m[2023-07-11 05:08:59,339][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:09:04,321][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:09:04,322][233954] Reward + Measures: [[145.38579034   0.09299999   0.94589996   0.41080004   0.8854
    2.88538551]
 [280.51437223   0.0434       0.94869995   0.5072       0.89099997
    3.01128054]
 [163.858304     0.0893       0.80649996   0.36690003   0.72439998
    2.98536491]
 ...
 [341.68564796   0.1275       0.92440003   0.44969997   0.86140007
    3.18768358]
 [ 44.23624944   0.0478       0.95279998   0.50769997   0.90310001
    2.97692728]
 [137.41100289   0.0857       0.91779995   0.41050002   0.82769996
    3.00551486]][0m
[37m[1m[2023-07-11 05:09:04,322][233954] Max Reward on eval: 724.3711853105575[0m
[37m[1m[2023-07-11 05:09:04,323][233954] Min Reward on eval: -272.8247098978609[0m
[37m[1m[2023-07-11 05:09:04,323][233954] Mean Reward across all agents: 184.29059273937247[0m
[37m[1m[2023-07-11 05:09:04,323][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:09:04,329][233954] mean_value=57.32260385903629, max_value=707.4760830622166[0m
[37m[1m[2023-07-11 05:09:04,332][233954] New mean coefficients: [[14.878506   6.3441076 21.985018   4.382961   2.5541935  2.9127483]][0m
[37m[1m[2023-07-11 05:09:04,333][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:09:13,349][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 05:09:13,349][233954] FPS: 425972.38[0m
[36m[2023-07-11 05:09:13,351][233954] itr=393, itrs=2000, Progress: 19.65%[0m
[36m[2023-07-11 05:09:25,128][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 05:09:25,128][233954] FPS: 328296.79[0m
[36m[2023-07-11 05:09:29,493][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:09:29,493][233954] Reward + Measures: [[36.04067213  0.98387337  0.99528533  0.97964597  0.99209225  2.85554552]][0m
[37m[1m[2023-07-11 05:09:29,494][233954] Max Reward on eval: 36.04067212796646[0m
[37m[1m[2023-07-11 05:09:29,494][233954] Min Reward on eval: 36.04067212796646[0m
[37m[1m[2023-07-11 05:09:29,494][233954] Mean Reward across all agents: 36.04067212796646[0m
[37m[1m[2023-07-11 05:09:29,494][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:09:34,467][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:09:34,468][233954] Reward + Measures: [[ 42.1710984    0.88420004   0.92950004   0.87250006   0.91140002
    2.733284  ]
 [ 22.22734609   0.83169997   0.89639997   0.80150002   0.89429998
    2.87278533]
 [ 44.88038274   0.54619998   0.62050003   0.51929998   0.51380002
    2.84518123]
 ...
 [ 17.36700735   0.80530006   0.88079995   0.77289999   0.86899996
    2.77354646]
 [ 59.46041708   0.87700003   0.91189998   0.84259999   0.89099997
    2.87624168]
 [131.87686184   0.88560009   0.91470003   0.85639995   0.86739999
    3.20847774]][0m
[37m[1m[2023-07-11 05:09:34,468][233954] Max Reward on eval: 208.68678081771358[0m
[37m[1m[2023-07-11 05:09:34,469][233954] Min Reward on eval: -18.822249348741025[0m
[37m[1m[2023-07-11 05:09:34,469][233954] Mean Reward across all agents: 70.63210234724023[0m
[37m[1m[2023-07-11 05:09:34,469][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:09:34,473][233954] mean_value=-14.657500823240564, max_value=562.0568247135728[0m
[37m[1m[2023-07-11 05:09:34,476][233954] New mean coefficients: [[15.653624   6.7210126 24.328863   5.111391   2.9339478  2.849072 ]][0m
[37m[1m[2023-07-11 05:09:34,477][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:09:43,514][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 05:09:43,515][233954] FPS: 424986.96[0m
[36m[2023-07-11 05:09:43,517][233954] itr=394, itrs=2000, Progress: 19.70%[0m
[36m[2023-07-11 05:09:55,299][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 05:09:55,299][233954] FPS: 328228.68[0m
[36m[2023-07-11 05:09:59,649][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:09:59,649][233954] Reward + Measures: [[42.49547204  0.98673028  0.99454325  0.98281264  0.99152958  2.85755229]][0m
[37m[1m[2023-07-11 05:09:59,650][233954] Max Reward on eval: 42.495472038681406[0m
[37m[1m[2023-07-11 05:09:59,650][233954] Min Reward on eval: 42.495472038681406[0m
[37m[1m[2023-07-11 05:09:59,650][233954] Mean Reward across all agents: 42.495472038681406[0m
[37m[1m[2023-07-11 05:09:59,650][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:10:04,665][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:10:04,670][233954] Reward + Measures: [[-17.84178553   0.49210006   0.71710008   0.45229998   0.70290005
    2.69919443]
 [-94.07171705   0.51599997   0.88450003   0.4093       0.78689998
    2.89175797]
 [-18.87151272   0.80340004   0.94530004   0.77239996   0.9163
    2.83984876]
 ...
 [-21.89749984   0.65690005   0.80660003   0.65009993   0.8257001
    2.71589351]
 [ 38.57414004   0.55620003   0.90290004   0.54189998   0.83199996
    2.90848708]
 [-99.67967032   0.57889998   0.93440002   0.39649999   0.85499996
    2.98456764]][0m
[37m[1m[2023-07-11 05:10:04,671][233954] Max Reward on eval: 67.2440532501787[0m
[37m[1m[2023-07-11 05:10:04,671][233954] Min Reward on eval: -481.34158325316383[0m
[37m[1m[2023-07-11 05:10:04,671][233954] Mean Reward across all agents: -48.01537663009257[0m
[37m[1m[2023-07-11 05:10:04,671][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:10:04,674][233954] mean_value=-50.991176346791086, max_value=368.8917036932521[0m
[37m[1m[2023-07-11 05:10:04,677][233954] New mean coefficients: [[15.619166   5.116252  24.298056   3.075143   1.4011183  0.9887177]][0m
[37m[1m[2023-07-11 05:10:04,678][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:10:13,737][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 05:10:13,737][233954] FPS: 423963.55[0m
[36m[2023-07-11 05:10:13,740][233954] itr=395, itrs=2000, Progress: 19.75%[0m
[36m[2023-07-11 05:10:25,495][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 05:10:25,495][233954] FPS: 328860.87[0m
[36m[2023-07-11 05:10:29,721][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:10:29,722][233954] Reward + Measures: [[50.34405415  0.98626065  0.99486625  0.98229402  0.99158365  2.84900641]][0m
[37m[1m[2023-07-11 05:10:29,722][233954] Max Reward on eval: 50.34405415462101[0m
[37m[1m[2023-07-11 05:10:29,722][233954] Min Reward on eval: 50.34405415462101[0m
[37m[1m[2023-07-11 05:10:29,723][233954] Mean Reward across all agents: 50.34405415462101[0m
[37m[1m[2023-07-11 05:10:29,723][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:10:34,636][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:10:34,637][233954] Reward + Measures: [[ -6.61427921   0.94259995   0.9799       0.93839997   0.96989995
    2.70881319]
 [ -6.07096969   0.89510006   0.95679998   0.88330001   0.93839991
    2.7041328 ]
 [-22.78928357   0.81270009   0.91990006   0.80740005   0.89680004
    2.69774508]
 ...
 [-22.61749982   0.61020005   0.82250005   0.58599997   0.77190006
    2.78451514]
 [-17.85610487   0.87089998   0.94480002   0.84320003   0.96649998
    2.7202394 ]
 [ -9.88514503   0.93850005   0.97659999   0.92960006   0.96400005
    2.71647286]][0m
[37m[1m[2023-07-11 05:10:34,637][233954] Max Reward on eval: 77.2555943761021[0m
[37m[1m[2023-07-11 05:10:34,637][233954] Min Reward on eval: -71.39321471001021[0m
[37m[1m[2023-07-11 05:10:34,638][233954] Mean Reward across all agents: -14.085872218860167[0m
[37m[1m[2023-07-11 05:10:34,638][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:10:34,640][233954] mean_value=-79.99885560951854, max_value=383.1910554778623[0m
[37m[1m[2023-07-11 05:10:34,642][233954] New mean coefficients: [[16.306412    4.84558    26.315205    3.0819678   1.1854348   0.81836814]][0m
[37m[1m[2023-07-11 05:10:34,643][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:10:43,578][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 05:10:43,578][233954] FPS: 429860.92[0m
[36m[2023-07-11 05:10:43,580][233954] itr=396, itrs=2000, Progress: 19.80%[0m
[36m[2023-07-11 05:10:55,144][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 05:10:55,149][233954] FPS: 334484.98[0m
[36m[2023-07-11 05:10:59,383][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:10:59,383][233954] Reward + Measures: [[59.29688259  0.98602402  0.99462134  0.98132199  0.99181569  2.83852887]][0m
[37m[1m[2023-07-11 05:10:59,383][233954] Max Reward on eval: 59.29688259429172[0m
[37m[1m[2023-07-11 05:10:59,384][233954] Min Reward on eval: 59.29688259429172[0m
[37m[1m[2023-07-11 05:10:59,384][233954] Mean Reward across all agents: 59.29688259429172[0m
[37m[1m[2023-07-11 05:10:59,384][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:11:04,394][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:11:04,395][233954] Reward + Measures: [[ 80.15466217   0.23940001   0.52990007   0.18709999   0.53909999
    2.57415771]
 [ 27.22037767   0.31189999   0.61669999   0.2095       0.61040002
    2.61433148]
 [ 49.91939871   0.22330001   0.49980003   0.20009999   0.48790002
    2.57407403]
 ...
 [-10.55228186   0.92700005   0.99080002   0.91919994   0.97430003
    2.66259003]
 [ 80.40408031   0.21140002   0.48330003   0.16730002   0.50260001
    2.56284213]
 [ 48.74533475   0.21170001   0.48000002   0.18660001   0.46720001
    2.5072751 ]][0m
[37m[1m[2023-07-11 05:11:04,395][233954] Max Reward on eval: 132.25273815589026[0m
[37m[1m[2023-07-11 05:11:04,395][233954] Min Reward on eval: -44.40173150328919[0m
[37m[1m[2023-07-11 05:11:04,396][233954] Mean Reward across all agents: 30.06931044899154[0m
[37m[1m[2023-07-11 05:11:04,396][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:11:04,399][233954] mean_value=-119.54907567811186, max_value=480.15676044099035[0m
[37m[1m[2023-07-11 05:11:04,401][233954] New mean coefficients: [[14.341596    3.1915941  21.708523    1.4874365  -0.6393242  -0.99726015]][0m
[37m[1m[2023-07-11 05:11:04,402][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:11:13,517][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 05:11:13,517][233954] FPS: 421369.90[0m
[36m[2023-07-11 05:11:13,520][233954] itr=397, itrs=2000, Progress: 19.85%[0m
[36m[2023-07-11 05:11:25,201][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 05:11:25,201][233954] FPS: 330994.40[0m
[36m[2023-07-11 05:11:29,461][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:11:29,461][233954] Reward + Measures: [[62.73663901  0.98548198  0.99525601  0.98114598  0.99165165  2.8262291 ]][0m
[37m[1m[2023-07-11 05:11:29,461][233954] Max Reward on eval: 62.73663901193215[0m
[37m[1m[2023-07-11 05:11:29,462][233954] Min Reward on eval: 62.73663901193215[0m
[37m[1m[2023-07-11 05:11:29,462][233954] Mean Reward across all agents: 62.73663901193215[0m
[37m[1m[2023-07-11 05:11:29,462][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:11:34,629][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:11:34,630][233954] Reward + Measures: [[-117.98307403    0.235         0.32220003    0.0824        0.30770001
     2.63644195]
 [ -22.35585876    0.32360002    0.47459999    0.1522        0.42489997
     2.51671004]
 [  65.02407148    0.16880001    0.4481        0.19219999    0.3953
     2.57621765]
 ...
 [ 722.61047366    0.0065        0.99189997    0.80109996    0.98850006
     2.93331909]
 [  47.99665596    0.18340002    0.53380007    0.20899999    0.46300003
     2.58695674]
 [  30.34341       0.24620001    0.67049998    0.2043        0.61159998
     2.78121638]][0m
[37m[1m[2023-07-11 05:11:34,630][233954] Max Reward on eval: 790.071548431227[0m
[37m[1m[2023-07-11 05:11:34,630][233954] Min Reward on eval: -269.9400296440348[0m
[37m[1m[2023-07-11 05:11:34,631][233954] Mean Reward across all agents: 82.4865145976559[0m
[37m[1m[2023-07-11 05:11:34,631][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:11:34,636][233954] mean_value=-103.95802132242736, max_value=507.1456383750103[0m
[37m[1m[2023-07-11 05:11:34,638][233954] New mean coefficients: [[11.881194    1.7785782  15.849678   -0.25317848 -2.1356125  -2.8879564 ]][0m
[37m[1m[2023-07-11 05:11:34,639][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:11:43,637][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 05:11:43,637][233954] FPS: 426857.39[0m
[36m[2023-07-11 05:11:43,639][233954] itr=398, itrs=2000, Progress: 19.90%[0m
[36m[2023-07-11 05:11:55,248][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 05:11:55,248][233954] FPS: 333185.34[0m
[36m[2023-07-11 05:11:59,571][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:11:59,572][233954] Reward + Measures: [[72.38765406  0.98501062  0.99519026  0.98024201  0.99103069  2.79928803]][0m
[37m[1m[2023-07-11 05:11:59,572][233954] Max Reward on eval: 72.38765406494386[0m
[37m[1m[2023-07-11 05:11:59,572][233954] Min Reward on eval: 72.38765406494386[0m
[37m[1m[2023-07-11 05:11:59,573][233954] Mean Reward across all agents: 72.38765406494386[0m
[37m[1m[2023-07-11 05:11:59,573][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:12:04,570][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:12:04,576][233954] Reward + Measures: [[ 32.30834612   0.65189999   0.92430001   0.62260002   0.88640004
    2.82820582]
 [ 10.43953211   0.8016001    0.95749998   0.7895       0.93260002
    2.69863653]
 [331.42372421   0.1336       0.94670004   0.45050001   0.91759998
    3.09926295]
 ...
 [ 17.79967856   0.94080001   0.97850001   0.94010001   0.97799999
    2.68499565]
 [-78.38771762   0.26930004   0.66119999   0.17869999   0.5794
    2.74255943]
 [-16.56887677   0.9127       0.96810001   0.90280002   0.98589993
    2.71246743]][0m
[37m[1m[2023-07-11 05:12:04,576][233954] Max Reward on eval: 477.92998264525085[0m
[37m[1m[2023-07-11 05:12:04,576][233954] Min Reward on eval: -342.80249595204367[0m
[37m[1m[2023-07-11 05:12:04,577][233954] Mean Reward across all agents: 21.987938945873637[0m
[37m[1m[2023-07-11 05:12:04,577][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:12:04,582][233954] mean_value=-2.662873647912729, max_value=525.876801833883[0m
[37m[1m[2023-07-11 05:12:04,585][233954] New mean coefficients: [[12.783707    0.01036489 17.952423   -1.6244382  -4.0580926  -4.7999525 ]][0m
[37m[1m[2023-07-11 05:12:04,586][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:12:13,558][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 05:12:13,559][233954] FPS: 428043.44[0m
[36m[2023-07-11 05:12:13,561][233954] itr=399, itrs=2000, Progress: 19.95%[0m
[36m[2023-07-11 05:12:25,223][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 05:12:25,223][233954] FPS: 331522.00[0m
[36m[2023-07-11 05:12:29,513][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:12:29,513][233954] Reward + Measures: [[94.84716501  0.98003465  0.99452096  0.97403854  0.9895674   2.71622252]][0m
[37m[1m[2023-07-11 05:12:29,514][233954] Max Reward on eval: 94.84716500587497[0m
[37m[1m[2023-07-11 05:12:29,514][233954] Min Reward on eval: 94.84716500587497[0m
[37m[1m[2023-07-11 05:12:29,514][233954] Mean Reward across all agents: 94.84716500587497[0m
[37m[1m[2023-07-11 05:12:29,514][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:12:34,539][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:12:34,539][233954] Reward + Measures: [[ 39.41367156   0.47230002   0.73190004   0.42879996   0.69690001
    2.67854643]
 [ 31.39772284   0.91020006   0.97030002   0.88520002   0.94870007
    2.66650891]
 [ 11.04101822   0.95249999   0.98000002   0.92750007   0.96149999
    2.69299102]
 ...
 [  9.75779294   0.62690002   0.82910007   0.56520003   0.85039997
    2.87261486]
 [-18.45267452   0.59930003   0.97789997   0.73980004   0.97280008
    2.80914545]
 [  8.30228226   0.78060001   0.9156       0.7518       0.85659999
    2.69853163]][0m
[37m[1m[2023-07-11 05:12:34,540][233954] Max Reward on eval: 308.4542789293453[0m
[37m[1m[2023-07-11 05:12:34,540][233954] Min Reward on eval: -35.71984084527939[0m
[37m[1m[2023-07-11 05:12:34,540][233954] Mean Reward across all agents: 30.77211465761513[0m
[37m[1m[2023-07-11 05:12:34,540][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:12:34,544][233954] mean_value=-18.208526902439996, max_value=286.4662071127516[0m
[37m[1m[2023-07-11 05:12:34,547][233954] New mean coefficients: [[12.286812  -2.5118346 18.1765    -3.5365062 -6.205167  -7.3420763]][0m
[37m[1m[2023-07-11 05:12:34,548][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:12:43,628][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 05:12:43,629][233954] FPS: 422948.67[0m
[36m[2023-07-11 05:12:43,631][233954] itr=400, itrs=2000, Progress: 20.00%[0m
[37m[1m[2023-07-11 05:15:50,847][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000380[0m
[36m[2023-07-11 05:16:03,132][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 05:16:03,132][233954] FPS: 328051.81[0m
[36m[2023-07-11 05:16:07,294][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:16:07,295][233954] Reward + Measures: [[123.37178299   0.97471029   0.98665935   0.96563262   0.98374528
    2.56482196]][0m
[37m[1m[2023-07-11 05:16:07,295][233954] Max Reward on eval: 123.37178298589768[0m
[37m[1m[2023-07-11 05:16:07,295][233954] Min Reward on eval: 123.37178298589768[0m
[37m[1m[2023-07-11 05:16:07,295][233954] Mean Reward across all agents: 123.37178298589768[0m
[37m[1m[2023-07-11 05:16:07,296][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:16:12,124][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:16:12,124][233954] Reward + Measures: [[ 65.39148381   0.1204       0.69550002   0.2404       0.61370003
    2.7576828 ]
 [170.35500824   0.09300001   0.71799999   0.25420001   0.62080002
    2.83230233]
 [ 83.30796454   0.43290001   0.9249       0.29190001   0.90189993
    2.94416952]
 ...
 [ 46.21319552   0.1708       0.3272       0.21859999   0.33180001
    2.49235058]
 [ 14.66727292   0.27519998   0.92819995   0.21959999   0.82539999
    2.89619446]
 [101.94807173   0.18430001   0.43400002   0.26949999   0.4059
    2.48164606]][0m
[37m[1m[2023-07-11 05:16:12,124][233954] Max Reward on eval: 583.191112563014[0m
[37m[1m[2023-07-11 05:16:12,125][233954] Min Reward on eval: -200.6913112649694[0m
[37m[1m[2023-07-11 05:16:12,125][233954] Mean Reward across all agents: 96.02790391911185[0m
[37m[1m[2023-07-11 05:16:12,125][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:16:12,131][233954] mean_value=70.73941047383343, max_value=760.5122194309952[0m
[37m[1m[2023-07-11 05:16:12,134][233954] New mean coefficients: [[11.662088  -2.6094804 17.292082  -3.5800323 -6.339719  -8.068792 ]][0m
[37m[1m[2023-07-11 05:16:12,135][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:16:21,094][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 05:16:21,094][233954] FPS: 428676.81[0m
[36m[2023-07-11 05:16:21,097][233954] itr=401, itrs=2000, Progress: 20.05%[0m
[36m[2023-07-11 05:16:32,714][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 05:16:32,714][233954] FPS: 332860.13[0m
[36m[2023-07-11 05:16:36,956][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:16:36,962][233954] Reward + Measures: [[139.31034815   0.97120994   0.98168194   0.95906103   0.97944808
    2.39561272]][0m
[37m[1m[2023-07-11 05:16:36,962][233954] Max Reward on eval: 139.31034815258835[0m
[37m[1m[2023-07-11 05:16:36,962][233954] Min Reward on eval: 139.31034815258835[0m
[37m[1m[2023-07-11 05:16:36,963][233954] Mean Reward across all agents: 139.31034815258835[0m
[37m[1m[2023-07-11 05:16:36,963][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:16:41,976][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:16:41,977][233954] Reward + Measures: [[ 11.69976685   0.95520002   0.98149997   0.92469996   0.98199999
    2.79599428]
 [ 23.28327273   0.94010001   0.97030002   0.92030001   0.96480006
    2.68514061]
 [-13.43337347   0.89790004   0.9472       0.84549999   0.95699996
    2.76489043]
 ...
 [ 29.88420342   0.89499998   0.97090006   0.87220001   0.94539994
    2.66649699]
 [  2.72948644   0.94230002   0.9884001    0.93230003   0.96830004
    2.73915029]
 [  7.52888425   0.92749995   0.95230001   0.89880002   0.95590001
    2.617939  ]][0m
[37m[1m[2023-07-11 05:16:41,977][233954] Max Reward on eval: 100.61442578546703[0m
[37m[1m[2023-07-11 05:16:41,977][233954] Min Reward on eval: -59.90273329023039[0m
[37m[1m[2023-07-11 05:16:41,978][233954] Mean Reward across all agents: 1.5175228491849755[0m
[37m[1m[2023-07-11 05:16:41,978][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:16:41,980][233954] mean_value=-80.92029301063904, max_value=77.6138645685445[0m
[37m[1m[2023-07-11 05:16:41,982][233954] New mean coefficients: [[12.269471  -3.6983917 18.271317  -4.917264  -7.0907946 -9.007694 ]][0m
[37m[1m[2023-07-11 05:16:41,983][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:16:50,938][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 05:16:50,938][233954] FPS: 428879.86[0m
[36m[2023-07-11 05:16:50,940][233954] itr=402, itrs=2000, Progress: 20.10%[0m
[36m[2023-07-11 05:17:02,583][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 05:17:02,583][233954] FPS: 332043.89[0m
[36m[2023-07-11 05:17:06,901][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:17:06,907][233954] Reward + Measures: [[130.35497054   0.97077864   0.97701162   0.95683903   0.97614568
    2.21565127]][0m
[37m[1m[2023-07-11 05:17:06,907][233954] Max Reward on eval: 130.35497053596538[0m
[37m[1m[2023-07-11 05:17:06,908][233954] Min Reward on eval: 130.35497053596538[0m
[37m[1m[2023-07-11 05:17:06,908][233954] Mean Reward across all agents: 130.35497053596538[0m
[37m[1m[2023-07-11 05:17:06,908][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:17:11,913][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:17:11,919][233954] Reward + Measures: [[ 44.73828543   0.56910002   0.79650003   0.55369997   0.80360001
    2.69190645]
 [ 77.0813088    0.49269995   0.63780004   0.45960003   0.66650003
    2.68076205]
 [-40.1976429    0.29580003   0.61569995   0.2174       0.51240003
    2.75636864]
 ...
 [ 76.47200596   0.37760001   0.58249998   0.34489998   0.61589998
    2.74816108]
 [-31.56137847   0.29190001   0.55620003   0.26040003   0.45170003
    2.73409343]
 [ -3.34708875   0.50130004   0.78210002   0.49590001   0.76470006
    2.63467407]][0m
[37m[1m[2023-07-11 05:17:11,919][233954] Max Reward on eval: 654.1208019390702[0m
[37m[1m[2023-07-11 05:17:11,920][233954] Min Reward on eval: -51.241827273927626[0m
[37m[1m[2023-07-11 05:17:11,920][233954] Mean Reward across all agents: 71.12740236956955[0m
[37m[1m[2023-07-11 05:17:11,920][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:17:11,927][233954] mean_value=28.00309186846746, max_value=712.8586272153073[0m
[37m[1m[2023-07-11 05:17:11,930][233954] New mean coefficients: [[12.017495  -2.3196054 17.21696   -4.690381  -5.559067  -6.877342 ]][0m
[37m[1m[2023-07-11 05:17:11,931][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:17:21,077][233954] train() took 9.14 seconds to complete[0m
[36m[2023-07-11 05:17:21,077][233954] FPS: 419906.38[0m
[36m[2023-07-11 05:17:21,080][233954] itr=403, itrs=2000, Progress: 20.15%[0m
[36m[2023-07-11 05:17:32,678][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 05:17:32,678][233954] FPS: 333472.41[0m
[36m[2023-07-11 05:17:36,931][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:17:36,932][233954] Reward + Measures: [[124.50164549   0.97009194   0.97646004   0.95112664   0.97256964
    2.03841829]][0m
[37m[1m[2023-07-11 05:17:36,932][233954] Max Reward on eval: 124.50164549380162[0m
[37m[1m[2023-07-11 05:17:36,932][233954] Min Reward on eval: 124.50164549380162[0m
[37m[1m[2023-07-11 05:17:36,932][233954] Mean Reward across all agents: 124.50164549380162[0m
[37m[1m[2023-07-11 05:17:36,933][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:17:41,912][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:17:41,918][233954] Reward + Measures: [[42.64651923  0.34199998  0.55680007  0.26949999  0.46720001  2.71839237]
 [60.15315142  0.58639997  0.75310004  0.54619998  0.66130006  2.72507405]
 [85.99519319  0.48890001  0.73139995  0.45439997  0.59609997  2.84130263]
 ...
 [53.91753291  0.74369997  0.92989999  0.70460004  0.84090006  2.89600444]
 [69.89045     0.69869995  0.94079989  0.6451      0.85000002  2.93084693]
 [61.54953778  0.72760004  0.92740005  0.69889998  0.8592      2.9395237 ]][0m
[37m[1m[2023-07-11 05:17:41,918][233954] Max Reward on eval: 162.82013512933628[0m
[37m[1m[2023-07-11 05:17:41,918][233954] Min Reward on eval: -100.16969667784869[0m
[37m[1m[2023-07-11 05:17:41,918][233954] Mean Reward across all agents: 73.55226999962605[0m
[37m[1m[2023-07-11 05:17:41,919][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:17:41,925][233954] mean_value=26.582275315362825, max_value=529.7895107157901[0m
[37m[1m[2023-07-11 05:17:41,928][233954] New mean coefficients: [[11.632826  -3.5378802 16.596756  -5.7594857 -6.8245974 -8.178174 ]][0m
[37m[1m[2023-07-11 05:17:41,929][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:17:51,002][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 05:17:51,002][233954] FPS: 423330.72[0m
[36m[2023-07-11 05:17:51,004][233954] itr=404, itrs=2000, Progress: 20.20%[0m
[36m[2023-07-11 05:18:03,089][233954] train() took 12.00 seconds to complete[0m
[36m[2023-07-11 05:18:03,089][233954] FPS: 319952.86[0m
[36m[2023-07-11 05:18:07,319][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:18:07,320][233954] Reward + Measures: [[118.09596863   0.96956301   0.98151064   0.94017464   0.973656
    1.90586722]][0m
[37m[1m[2023-07-11 05:18:07,320][233954] Max Reward on eval: 118.09596863196218[0m
[37m[1m[2023-07-11 05:18:07,320][233954] Min Reward on eval: 118.09596863196218[0m
[37m[1m[2023-07-11 05:18:07,321][233954] Mean Reward across all agents: 118.09596863196218[0m
[37m[1m[2023-07-11 05:18:07,321][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:18:12,209][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:18:12,209][233954] Reward + Measures: [[119.83296016   0.31290001   0.80660003   0.45919999   0.80370009
    2.76344681]
 [-13.86229356   0.51140004   0.77350003   0.48050004   0.77009994
    2.51127505]
 [-58.84173989   0.54310006   0.75599998   0.45099998   0.86240005
    2.85068846]
 ...
 [  1.38666432   0.45229998   0.74300003   0.4048       0.77710003
    2.76313066]
 [-39.46781968   0.54210001   0.81059998   0.48520002   0.86049998
    2.83961678]
 [258.71034955   0.20829999   0.9235       0.52990001   0.92260009
    2.98375201]][0m
[37m[1m[2023-07-11 05:18:12,209][233954] Max Reward on eval: 697.4874420113862[0m
[37m[1m[2023-07-11 05:18:12,210][233954] Min Reward on eval: -80.55465315263719[0m
[37m[1m[2023-07-11 05:18:12,210][233954] Mean Reward across all agents: 92.82447586256092[0m
[37m[1m[2023-07-11 05:18:12,210][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:18:12,216][233954] mean_value=-19.15497779390315, max_value=595.2640612009466[0m
[37m[1m[2023-07-11 05:18:12,219][233954] New mean coefficients: [[11.022331  -3.0111742 13.564659  -6.559996  -5.9000874 -6.9442205]][0m
[37m[1m[2023-07-11 05:18:12,220][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:18:21,133][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 05:18:21,134][233954] FPS: 430873.73[0m
[36m[2023-07-11 05:18:21,136][233954] itr=405, itrs=2000, Progress: 20.25%[0m
[36m[2023-07-11 05:18:32,864][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 05:18:32,865][233954] FPS: 329718.46[0m
[36m[2023-07-11 05:18:37,114][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:18:37,115][233954] Reward + Measures: [[117.41015412   0.95723963   0.959768     0.9237963    0.95344096
    1.86992383]][0m
[37m[1m[2023-07-11 05:18:37,115][233954] Max Reward on eval: 117.4101541166897[0m
[37m[1m[2023-07-11 05:18:37,115][233954] Min Reward on eval: 117.4101541166897[0m
[37m[1m[2023-07-11 05:18:37,115][233954] Mean Reward across all agents: 117.4101541166897[0m
[37m[1m[2023-07-11 05:18:37,116][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:18:42,344][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:18:42,344][233954] Reward + Measures: [[  88.36151242    0.3601        0.3673        0.32089999    0.46170002
     3.07949114]
 [ -92.06854184    0.39279997    0.46330005    0.15589999    0.47610003
     3.01672292]
 [  98.9662348     0.26159999    0.30119997    0.25929999    0.36700001
     2.91761947]
 ...
 [-103.50040332    0.32819998    0.46300003    0.148         0.43180004
     3.02629089]
 [ -14.26206422    0.62540001    0.68980002    0.46110001    0.74440002
     2.96958661]
 [   6.07733838    0.22190002    0.32510003    0.22060001    0.32519999
     2.86325073]][0m
[37m[1m[2023-07-11 05:18:42,344][233954] Max Reward on eval: 200.84103787234054[0m
[37m[1m[2023-07-11 05:18:42,345][233954] Min Reward on eval: -478.6731493643485[0m
[37m[1m[2023-07-11 05:18:42,345][233954] Mean Reward across all agents: -22.04789348209112[0m
[37m[1m[2023-07-11 05:18:42,345][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:18:42,348][233954] mean_value=-156.08198373187204, max_value=419.78917210921645[0m
[37m[1m[2023-07-11 05:18:42,351][233954] New mean coefficients: [[11.123885  -1.1199265 13.374884  -5.434478  -4.0708113 -4.8160167]][0m
[37m[1m[2023-07-11 05:18:42,351][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:18:51,349][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 05:18:51,350][233954] FPS: 426844.95[0m
[36m[2023-07-11 05:18:51,352][233954] itr=406, itrs=2000, Progress: 20.30%[0m
[36m[2023-07-11 05:19:03,030][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 05:19:03,030][233954] FPS: 331177.30[0m
[36m[2023-07-11 05:19:07,316][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:19:07,316][233954] Reward + Measures: [[123.97936801   0.95771629   0.96903694   0.92050731   0.94863737
    1.7767396 ]][0m
[37m[1m[2023-07-11 05:19:07,316][233954] Max Reward on eval: 123.97936801033153[0m
[37m[1m[2023-07-11 05:19:07,317][233954] Min Reward on eval: 123.97936801033153[0m
[37m[1m[2023-07-11 05:19:07,317][233954] Mean Reward across all agents: 123.97936801033153[0m
[37m[1m[2023-07-11 05:19:07,317][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:19:12,397][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:19:12,397][233954] Reward + Measures: [[ 47.05187367   0.53300005   0.90460008   0.48590001   0.81940001
    2.71693206]
 [ 23.05863436   0.4851       0.85320008   0.4434       0.76949996
    2.74176502]
 [ 14.02687879   0.3705       0.85589999   0.28830001   0.72280002
    2.70604205]
 ...
 [ 13.88491738   0.72710001   0.90150005   0.75120002   0.85900003
    2.63170385]
 [-58.40616855   0.30530003   0.87830001   0.27849999   0.78999996
    2.69211626]
 [ -2.8754374    0.30459997   0.78579998   0.3204       0.74149996
    2.65993738]][0m
[37m[1m[2023-07-11 05:19:12,397][233954] Max Reward on eval: 313.8791154095903[0m
[37m[1m[2023-07-11 05:19:12,398][233954] Min Reward on eval: -212.79682803517207[0m
[37m[1m[2023-07-11 05:19:12,398][233954] Mean Reward across all agents: 23.597694606813565[0m
[37m[1m[2023-07-11 05:19:12,398][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:19:12,404][233954] mean_value=34.672898105893196, max_value=567.3938669831109[0m
[37m[1m[2023-07-11 05:19:12,407][233954] New mean coefficients: [[ 9.685856  -0.7942071  9.478337  -6.304736  -3.9458513 -3.9091396]][0m
[37m[1m[2023-07-11 05:19:12,408][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:19:21,517][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 05:19:21,522][233954] FPS: 421623.29[0m
[36m[2023-07-11 05:19:21,525][233954] itr=407, itrs=2000, Progress: 20.35%[0m
[36m[2023-07-11 05:19:33,351][233954] train() took 11.75 seconds to complete[0m
[36m[2023-07-11 05:19:33,351][233954] FPS: 326887.06[0m
[36m[2023-07-11 05:19:37,682][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:19:37,682][233954] Reward + Measures: [[145.26649941   0.96261895   0.98167968   0.91219938   0.95144236
    1.73416913]][0m
[37m[1m[2023-07-11 05:19:37,683][233954] Max Reward on eval: 145.2664994116357[0m
[37m[1m[2023-07-11 05:19:37,683][233954] Min Reward on eval: 145.2664994116357[0m
[37m[1m[2023-07-11 05:19:37,683][233954] Mean Reward across all agents: 145.2664994116357[0m
[37m[1m[2023-07-11 05:19:37,683][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:19:42,660][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:19:42,661][233954] Reward + Measures: [[ 97.55759667   0.62800002   0.9465       0.58939999   0.83290005
    2.64320445]
 [  8.21570702   0.4797       0.79049999   0.39739999   0.66800004
    2.76638508]
 [ 67.27109314   0.73629999   0.94810009   0.69850004   0.88410008
    2.65936685]
 ...
 [134.12348747   0.50370002   0.91099995   0.45630002   0.78109998
    2.7655704 ]
 [-18.80653001   0.36240003   0.72410005   0.17470001   0.58039999
    2.72454119]
 [ 88.87282254   0.32069999   0.60470003   0.3019       0.48810002
    2.60424042]][0m
[37m[1m[2023-07-11 05:19:42,661][233954] Max Reward on eval: 164.66389368064702[0m
[37m[1m[2023-07-11 05:19:42,661][233954] Min Reward on eval: -215.40518424976617[0m
[37m[1m[2023-07-11 05:19:42,662][233954] Mean Reward across all agents: 85.56909475398521[0m
[37m[1m[2023-07-11 05:19:42,662][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:19:42,669][233954] mean_value=49.7370599103019, max_value=381.2606252404398[0m
[37m[1m[2023-07-11 05:19:42,672][233954] New mean coefficients: [[ 8.454897  -1.6156347  7.026374  -6.8077254 -5.0261364 -5.5827703]][0m
[37m[1m[2023-07-11 05:19:42,673][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:19:51,677][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 05:19:51,678][233954] FPS: 426536.62[0m
[36m[2023-07-11 05:19:51,680][233954] itr=408, itrs=2000, Progress: 20.40%[0m
[36m[2023-07-11 05:20:03,372][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 05:20:03,372][233954] FPS: 330702.65[0m
[36m[2023-07-11 05:20:07,610][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:20:07,610][233954] Reward + Measures: [[153.21030404   0.96261531   0.98360068   0.9082796    0.94607699
    1.68567514]][0m
[37m[1m[2023-07-11 05:20:07,611][233954] Max Reward on eval: 153.210304038828[0m
[37m[1m[2023-07-11 05:20:07,611][233954] Min Reward on eval: 153.210304038828[0m
[37m[1m[2023-07-11 05:20:07,611][233954] Mean Reward across all agents: 153.210304038828[0m
[37m[1m[2023-07-11 05:20:07,611][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:20:12,691][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:20:12,691][233954] Reward + Measures: [[-88.56869743   0.0091       0.99470007   0.68880004   0.98720008
    3.12429595]
 [ -3.39584199   0.0031       0.99720001   0.73110002   0.99449998
    3.21102381]
 [ 86.81670997   0.0025       0.99410003   0.72850001   0.99529999
    3.10362601]
 ...
 [126.9424804    0.0711       0.92870009   0.45770001   0.91219997
    3.37821507]
 [249.81048392   0.0029       0.99550003   0.72110003   0.99559993
    3.09716463]
 [-45.6382906    0.52739996   0.88180012   0.45910001   0.7723
    3.04447412]][0m
[37m[1m[2023-07-11 05:20:12,691][233954] Max Reward on eval: 350.62786293318493[0m
[37m[1m[2023-07-11 05:20:12,692][233954] Min Reward on eval: -231.970400842838[0m
[37m[1m[2023-07-11 05:20:12,692][233954] Mean Reward across all agents: 11.091112981415238[0m
[37m[1m[2023-07-11 05:20:12,692][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:20:12,694][233954] mean_value=-210.65601091994702, max_value=418.7127895763668[0m
[37m[1m[2023-07-11 05:20:12,697][233954] New mean coefficients: [[ 8.552872   -0.38653052  7.0155425  -5.389824   -3.8454485  -4.343796  ]][0m
[37m[1m[2023-07-11 05:20:12,698][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:20:21,797][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 05:20:21,797][233954] FPS: 422100.24[0m
[36m[2023-07-11 05:20:21,799][233954] itr=409, itrs=2000, Progress: 20.45%[0m
[36m[2023-07-11 05:20:33,535][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 05:20:33,535][233954] FPS: 329501.77[0m
[36m[2023-07-11 05:20:37,803][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:20:37,804][233954] Reward + Measures: [[159.70563055   0.96129966   0.98547769   0.90074104   0.93962103
    1.65884233]][0m
[37m[1m[2023-07-11 05:20:37,804][233954] Max Reward on eval: 159.7056305543067[0m
[37m[1m[2023-07-11 05:20:37,804][233954] Min Reward on eval: 159.7056305543067[0m
[37m[1m[2023-07-11 05:20:37,804][233954] Mean Reward across all agents: 159.7056305543067[0m
[37m[1m[2023-07-11 05:20:37,805][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:20:42,748][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:20:42,749][233954] Reward + Measures: [[53.41607798  0.97239989  0.995       0.95950001  0.98699999  2.94455433]
 [53.06007268  0.96279997  0.99340004  0.95419997  0.98030007  2.97028995]
 [53.11917877  0.94410002  0.99529999  0.93029994  0.97679996  2.8900516 ]
 ...
 [ 2.5348318   0.94890004  0.96180004  0.91779995  0.96019995  2.81405902]
 [14.44676724  0.93120003  0.99420005  0.91390002  0.97299999  2.91178489]
 [33.71329709  0.9052      0.95660001  0.88679999  0.95090008  2.95379424]][0m
[37m[1m[2023-07-11 05:20:42,749][233954] Max Reward on eval: 216.70895003732295[0m
[37m[1m[2023-07-11 05:20:42,749][233954] Min Reward on eval: -51.83117093620822[0m
[37m[1m[2023-07-11 05:20:42,749][233954] Mean Reward across all agents: 37.85368367524368[0m
[37m[1m[2023-07-11 05:20:42,750][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:20:42,752][233954] mean_value=-63.91199298140499, max_value=209.37616778350645[0m
[37m[1m[2023-07-11 05:20:42,755][233954] New mean coefficients: [[ 6.9361954  1.6308392  3.5676394 -3.206314  -2.1774132 -2.8456435]][0m
[37m[1m[2023-07-11 05:20:42,755][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:20:51,685][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 05:20:51,685][233954] FPS: 430119.14[0m
[36m[2023-07-11 05:20:51,687][233954] itr=410, itrs=2000, Progress: 20.50%[0m
[37m[1m[2023-07-11 05:23:59,994][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000390[0m
[36m[2023-07-11 05:24:12,291][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 05:24:12,316][233954] FPS: 330206.88[0m
[36m[2023-07-11 05:24:16,429][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:24:16,430][233954] Reward + Measures: [[173.62593874   0.96228659   0.98716199   0.89177895   0.93459266
    1.60909569]][0m
[37m[1m[2023-07-11 05:24:16,430][233954] Max Reward on eval: 173.62593874392778[0m
[37m[1m[2023-07-11 05:24:16,430][233954] Min Reward on eval: 173.62593874392778[0m
[37m[1m[2023-07-11 05:24:16,430][233954] Mean Reward across all agents: 173.62593874392778[0m
[37m[1m[2023-07-11 05:24:16,431][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:24:21,296][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:24:21,297][233954] Reward + Measures: [[-59.01675606   0.29480001   0.73229998   0.31810001   0.82270002
    3.07050896]
 [ 69.3379159    0.70790005   0.9016       0.67360002   0.86090004
    2.51711106]
 [137.77986049   0.0289       0.97059995   0.39810002   0.96110004
    3.35554433]
 ...
 [ 53.46938089   0.83630002   0.90939999   0.80089992   0.88999999
    2.33785534]
 [ -8.88607507   0.35010001   0.91110003   0.4237       0.93669999
    3.04831338]
 [147.54550434   0.7008       0.95720005   0.75650001   0.94880003
    2.81055188]][0m
[37m[1m[2023-07-11 05:24:21,297][233954] Max Reward on eval: 400.2076401698403[0m
[37m[1m[2023-07-11 05:24:21,298][233954] Min Reward on eval: -69.17035222314298[0m
[37m[1m[2023-07-11 05:24:21,298][233954] Mean Reward across all agents: 76.08651153345569[0m
[37m[1m[2023-07-11 05:24:21,298][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:24:21,303][233954] mean_value=-10.641388795993004, max_value=552.5951716785702[0m
[37m[1m[2023-07-11 05:24:21,305][233954] New mean coefficients: [[ 5.4573064   1.4022422   0.61632466 -3.9116716  -2.2699003  -3.4335194 ]][0m
[37m[1m[2023-07-11 05:24:21,306][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:24:30,239][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 05:24:30,239][233954] FPS: 429962.76[0m
[36m[2023-07-11 05:24:30,241][233954] itr=411, itrs=2000, Progress: 20.55%[0m
[36m[2023-07-11 05:24:42,133][233954] train() took 11.81 seconds to complete[0m
[36m[2023-07-11 05:24:42,133][233954] FPS: 325166.12[0m
[36m[2023-07-11 05:24:46,388][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:24:46,389][233954] Reward + Measures: [[180.77713885   0.96246803   0.98617429   0.89259034   0.92540634
    1.53645504]][0m
[37m[1m[2023-07-11 05:24:46,389][233954] Max Reward on eval: 180.77713884876607[0m
[37m[1m[2023-07-11 05:24:46,389][233954] Min Reward on eval: 180.77713884876607[0m
[37m[1m[2023-07-11 05:24:46,390][233954] Mean Reward across all agents: 180.77713884876607[0m
[37m[1m[2023-07-11 05:24:46,390][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:24:51,387][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:24:51,388][233954] Reward + Measures: [[135.23277999   0.63490003   0.91130012   0.62230003   0.83280003
    2.46466541]
 [ 63.72801652   0.86470002   0.83999997   0.80000001   0.85710001
    2.51601315]
 [326.56070422   0.10829999   0.93209994   0.41359997   0.82050002
    2.77514076]
 ...
 [118.18594502   0.94820005   0.98600006   0.89300007   0.96280003
    1.83139837]
 [ 18.57888315   0.32900003   0.65490001   0.25650001   0.54929996
    2.61736751]
 [110.10315689   0.83249998   0.78890002   0.80170006   0.87680006
    2.41927767]][0m
[37m[1m[2023-07-11 05:24:51,388][233954] Max Reward on eval: 648.3129196368158[0m
[37m[1m[2023-07-11 05:24:51,388][233954] Min Reward on eval: -67.11620356673374[0m
[37m[1m[2023-07-11 05:24:51,389][233954] Mean Reward across all agents: 88.25770275915428[0m
[37m[1m[2023-07-11 05:24:51,389][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:24:51,395][233954] mean_value=48.153823179893884, max_value=785.2093133699149[0m
[37m[1m[2023-07-11 05:24:51,398][233954] New mean coefficients: [[ 5.6810455   0.66155875  0.7027381  -4.2075496  -3.28457    -4.049219  ]][0m
[37m[1m[2023-07-11 05:24:51,399][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:25:00,387][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 05:25:00,387][233954] FPS: 427328.18[0m
[36m[2023-07-11 05:25:00,390][233954] itr=412, itrs=2000, Progress: 20.60%[0m
[36m[2023-07-11 05:25:11,963][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 05:25:11,968][233954] FPS: 334153.24[0m
[36m[2023-07-11 05:25:16,246][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:25:16,247][233954] Reward + Measures: [[193.1850991    0.96116298   0.98407      0.88199329   0.91246802
    1.52123463]][0m
[37m[1m[2023-07-11 05:25:16,247][233954] Max Reward on eval: 193.18509909740777[0m
[37m[1m[2023-07-11 05:25:16,247][233954] Min Reward on eval: 193.18509909740777[0m
[37m[1m[2023-07-11 05:25:16,248][233954] Mean Reward across all agents: 193.18509909740777[0m
[37m[1m[2023-07-11 05:25:16,248][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:25:21,238][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:25:21,239][233954] Reward + Measures: [[178.72391938   0.0692       0.95900005   0.52640003   0.89489996
    3.53237462]
 [ -9.66894446   0.0567       0.97830009   0.44619998   0.91350001
    3.28812718]
 [ 33.58613743   0.0862       0.84580004   0.40270001   0.80030006
    3.60621047]
 ...
 [-21.91109232   0.0546       0.9799       0.4375       0.91860002
    3.22156596]
 [ 89.08415508   0.101        0.82320005   0.4276       0.77200001
    3.55877352]
 [129.12211896   0.0621       0.95520002   0.46200004   0.86400002
    3.51322341]][0m
[37m[1m[2023-07-11 05:25:21,239][233954] Max Reward on eval: 316.87827723398806[0m
[37m[1m[2023-07-11 05:25:21,239][233954] Min Reward on eval: -151.84068873159123[0m
[37m[1m[2023-07-11 05:25:21,240][233954] Mean Reward across all agents: 89.06776909050177[0m
[37m[1m[2023-07-11 05:25:21,240][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:25:21,243][233954] mean_value=-116.83280835974959, max_value=409.27331851494364[0m
[37m[1m[2023-07-11 05:25:21,245][233954] New mean coefficients: [[ 6.5302167  1.0156468  2.2843845 -3.461914  -2.8285933 -3.3410819]][0m
[37m[1m[2023-07-11 05:25:21,246][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:25:30,335][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 05:25:30,336][233954] FPS: 422568.16[0m
[36m[2023-07-11 05:25:30,338][233954] itr=413, itrs=2000, Progress: 20.65%[0m
[36m[2023-07-11 05:25:42,005][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 05:25:42,005][233954] FPS: 331476.43[0m
[36m[2023-07-11 05:25:46,281][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:25:46,281][233954] Reward + Measures: [[203.59975286   0.95923895   0.98132235   0.86754      0.89020097
    1.48788476]][0m
[37m[1m[2023-07-11 05:25:46,281][233954] Max Reward on eval: 203.59975285864405[0m
[37m[1m[2023-07-11 05:25:46,282][233954] Min Reward on eval: 203.59975285864405[0m
[37m[1m[2023-07-11 05:25:46,282][233954] Mean Reward across all agents: 203.59975285864405[0m
[37m[1m[2023-07-11 05:25:46,282][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:25:51,243][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:25:51,244][233954] Reward + Measures: [[ 6.93067058  0.96340001  0.9903      0.96050006  0.96540004  3.82149124]
 [ 1.31620182  0.98000002  0.99289989  0.9813      0.97780001  3.89030576]
 [-3.43563368  0.98909998  0.99379998  0.98929995  0.98859996  3.93315244]
 ...
 [29.17717849  0.93059999  0.98050004  0.91180003  0.95120001  2.97224522]
 [ 0.95822704  0.98260003  0.99109995  0.98009998  0.98450005  3.89263701]
 [ 1.48257712  0.98559999  0.99300003  0.98480004  0.98400003  3.94150782]][0m
[37m[1m[2023-07-11 05:25:51,244][233954] Max Reward on eval: 281.21916246535255[0m
[37m[1m[2023-07-11 05:25:51,244][233954] Min Reward on eval: -71.74705366361886[0m
[37m[1m[2023-07-11 05:25:51,245][233954] Mean Reward across all agents: 9.784268533681777[0m
[37m[1m[2023-07-11 05:25:51,245][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:25:51,247][233954] mean_value=-49.84626468687666, max_value=737.7728329797683[0m
[37m[1m[2023-07-11 05:25:51,249][233954] New mean coefficients: [[ 9.217202   2.1077108  7.036605  -2.46527   -1.7323477 -1.3102124]][0m
[37m[1m[2023-07-11 05:25:51,250][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:26:00,310][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 05:26:00,310][233954] FPS: 423935.81[0m
[36m[2023-07-11 05:26:00,312][233954] itr=414, itrs=2000, Progress: 20.70%[0m
[36m[2023-07-11 05:26:12,007][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 05:26:12,008][233954] FPS: 330558.04[0m
[36m[2023-07-11 05:26:16,313][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:26:16,319][233954] Reward + Measures: [[186.88297346   0.96374762   0.98808634   0.89298272   0.92025536
    1.49852538]][0m
[37m[1m[2023-07-11 05:26:16,319][233954] Max Reward on eval: 186.8829734589331[0m
[37m[1m[2023-07-11 05:26:16,319][233954] Min Reward on eval: 186.8829734589331[0m
[37m[1m[2023-07-11 05:26:16,320][233954] Mean Reward across all agents: 186.8829734589331[0m
[37m[1m[2023-07-11 05:26:16,320][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:26:21,316][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:26:21,322][233954] Reward + Measures: [[ 84.43128196   0.1201       0.98159999   0.60059994   0.9648
    2.89992499]
 [ 68.32421541   0.51090002   0.92220002   0.4894       0.84130001
    2.86316037]
 [ 34.2620767    0.52770001   0.88339996   0.50270003   0.85500002
    2.85073233]
 ...
 [ 85.28339721   0.71349996   0.9806       0.72390002   0.95850003
    2.55624938]
 [ 97.86109853   0.0468       0.97390002   0.56910002   0.91940004
    2.89600492]
 [649.87912367   0.01         0.99139994   0.74519998   0.98610002
    3.00969267]][0m
[37m[1m[2023-07-11 05:26:21,323][233954] Max Reward on eval: 798.0036239562556[0m
[37m[1m[2023-07-11 05:26:21,323][233954] Min Reward on eval: -57.88394435569644[0m
[37m[1m[2023-07-11 05:26:21,323][233954] Mean Reward across all agents: 91.39707803470063[0m
[37m[1m[2023-07-11 05:26:21,323][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:26:21,327][233954] mean_value=-123.29783414686554, max_value=356.76966478844514[0m
[37m[1m[2023-07-11 05:26:21,330][233954] New mean coefficients: [[ 8.148571   2.0920577  4.7878284 -2.5560021 -1.8209968 -1.5134914]][0m
[37m[1m[2023-07-11 05:26:21,331][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:26:30,257][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 05:26:30,257][233954] FPS: 430282.05[0m
[36m[2023-07-11 05:26:30,259][233954] itr=415, itrs=2000, Progress: 20.75%[0m
[36m[2023-07-11 05:26:42,217][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 05:26:42,217][233954] FPS: 329357.09[0m
[36m[2023-07-11 05:26:46,431][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:26:46,432][233954] Reward + Measures: [[234.95377878   0.9534086    0.98290968   0.85999805   0.88957763
    1.53216958]][0m
[37m[1m[2023-07-11 05:26:46,432][233954] Max Reward on eval: 234.95377877922797[0m
[37m[1m[2023-07-11 05:26:46,432][233954] Min Reward on eval: 234.95377877922797[0m
[37m[1m[2023-07-11 05:26:46,432][233954] Mean Reward across all agents: 234.95377877922797[0m
[37m[1m[2023-07-11 05:26:46,433][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:26:51,455][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:26:51,456][233954] Reward + Measures: [[ 26.43025661   0.98850006   0.99349993   0.98309994   0.99260008
    2.82163024]
 [-78.61815674   0.98709995   0.99260008   0.97819996   0.99250001
    3.00094199]
 [ 82.11108087   0.93310004   0.98159999   0.90610009   0.94300002
    2.14765644]
 ...
 [-49.78830576   0.99009991   0.99329996   0.98239994   0.99370003
    2.87704325]
 [146.35895824   0.94890004   0.98700011   0.91890001   0.95559996
    1.85341537]
 [ 15.08887328   0.96740001   0.95370007   0.95979995   0.97690004
    2.57288051]][0m
[37m[1m[2023-07-11 05:26:51,456][233954] Max Reward on eval: 234.95654393006117[0m
[37m[1m[2023-07-11 05:26:51,456][233954] Min Reward on eval: -125.01127819903195[0m
[37m[1m[2023-07-11 05:26:51,456][233954] Mean Reward across all agents: 51.22497318263589[0m
[37m[1m[2023-07-11 05:26:51,457][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:26:51,460][233954] mean_value=-68.58486934170767, max_value=185.37730740120062[0m
[37m[1m[2023-07-11 05:26:51,462][233954] New mean coefficients: [[ 4.875928   1.4386525 -1.3631482 -3.8119764 -2.6180573 -3.4553313]][0m
[37m[1m[2023-07-11 05:26:51,463][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:27:00,441][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 05:27:00,441][233954] FPS: 427798.13[0m
[36m[2023-07-11 05:27:00,444][233954] itr=416, itrs=2000, Progress: 20.80%[0m
[36m[2023-07-11 05:27:12,110][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 05:27:12,110][233954] FPS: 331441.07[0m
[36m[2023-07-11 05:27:16,364][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:27:16,370][233954] Reward + Measures: [[101.52111576   0.90116334   0.97727698   0.85905957   0.91651827
    2.33002186]][0m
[37m[1m[2023-07-11 05:27:16,370][233954] Max Reward on eval: 101.52111575771276[0m
[37m[1m[2023-07-11 05:27:16,371][233954] Min Reward on eval: 101.52111575771276[0m
[37m[1m[2023-07-11 05:27:16,371][233954] Mean Reward across all agents: 101.52111575771276[0m
[37m[1m[2023-07-11 05:27:16,371][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:27:21,285][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:27:21,286][233954] Reward + Measures: [[  13.71029872    0.94929999    0.99220002    0.94690001    0.97720003
     2.81772065]
 [   7.09320861    0.8179        0.97039998    0.77840006    0.90420008
     2.70501447]
 [ -53.0991614     0.74800003    0.94440001    0.68650001    0.84779996
     2.83500934]
 ...
 [  92.88445961    0.76719999    0.89499998    0.74620003    0.85050005
     2.54720211]
 [-108.97649264    0.43199998    0.81960005    0.26719999    0.64890003
     2.71247458]
 [  14.38414645    0.74020004    0.95090002    0.67970008    0.86240005
     2.47799706]][0m
[37m[1m[2023-07-11 05:27:21,286][233954] Max Reward on eval: 179.93346308022737[0m
[37m[1m[2023-07-11 05:27:21,286][233954] Min Reward on eval: -351.5222778371535[0m
[37m[1m[2023-07-11 05:27:21,287][233954] Mean Reward across all agents: -7.325687232704465[0m
[37m[1m[2023-07-11 05:27:21,287][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:27:21,291][233954] mean_value=-97.83526146980077, max_value=504.81076649921016[0m
[37m[1m[2023-07-11 05:27:21,293][233954] New mean coefficients: [[ 5.8592515  1.1777757  0.750927  -3.5979462 -2.7375875 -3.3413386]][0m
[37m[1m[2023-07-11 05:27:21,294][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:27:30,286][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 05:27:30,286][233954] FPS: 427150.65[0m
[36m[2023-07-11 05:27:30,288][233954] itr=417, itrs=2000, Progress: 20.85%[0m
[36m[2023-07-11 05:27:41,904][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 05:27:41,905][233954] FPS: 332810.67[0m
[36m[2023-07-11 05:27:46,134][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:27:46,134][233954] Reward + Measures: [[528.89477822   0.034903     0.97487664   0.73038661   0.96257406
    3.13269925]][0m
[37m[1m[2023-07-11 05:27:46,134][233954] Max Reward on eval: 528.89477822474[0m
[37m[1m[2023-07-11 05:27:46,135][233954] Min Reward on eval: 528.89477822474[0m
[37m[1m[2023-07-11 05:27:46,135][233954] Mean Reward across all agents: 528.89477822474[0m
[37m[1m[2023-07-11 05:27:46,135][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:27:51,057][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:27:51,058][233954] Reward + Measures: [[ 319.60007312    0.0196        0.83850002    0.53070003    0.82929993
     3.15939832]
 [-129.25416756    0.3145        0.954         0.42519999    0.98159999
     3.4995141 ]
 [ -31.07786582    0.82520002    0.88870001    0.77100003    0.90230006
     2.81312728]
 ...
 [ -87.51898092    0.63809997    0.88480008    0.63800001    0.90329999
     3.37216735]
 [ -32.5895418     0.74769998    0.82190001    0.71170002    0.82720006
     3.21216822]
 [ -28.15956012    0.6354        0.787         0.63250005    0.83550006
     3.18105817]][0m
[37m[1m[2023-07-11 05:27:51,058][233954] Max Reward on eval: 520.1591682656668[0m
[37m[1m[2023-07-11 05:27:51,058][233954] Min Reward on eval: -381.8452661102172[0m
[37m[1m[2023-07-11 05:27:51,059][233954] Mean Reward across all agents: 85.41094234275195[0m
[37m[1m[2023-07-11 05:27:51,059][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:27:51,064][233954] mean_value=-42.40293988515487, max_value=695.454620288536[0m
[37m[1m[2023-07-11 05:27:51,066][233954] New mean coefficients: [[ 5.06522    -0.52064943 -0.8281938  -4.751626   -4.369992   -4.9908752 ]][0m
[37m[1m[2023-07-11 05:27:51,067][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:28:00,006][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 05:28:00,006][233954] FPS: 429678.05[0m
[36m[2023-07-11 05:28:00,008][233954] itr=418, itrs=2000, Progress: 20.90%[0m
[36m[2023-07-11 05:28:11,603][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 05:28:11,603][233954] FPS: 333487.21[0m
[36m[2023-07-11 05:28:15,908][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:28:15,909][233954] Reward + Measures: [[528.98698976   0.02916533   0.96756768   0.70484334   0.95331073
    2.93099618]][0m
[37m[1m[2023-07-11 05:28:15,909][233954] Max Reward on eval: 528.9869897556388[0m
[37m[1m[2023-07-11 05:28:15,909][233954] Min Reward on eval: 528.9869897556388[0m
[37m[1m[2023-07-11 05:28:15,909][233954] Mean Reward across all agents: 528.9869897556388[0m
[37m[1m[2023-07-11 05:28:15,910][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:28:20,921][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:28:20,921][233954] Reward + Measures: [[407.98056649   0.0396       0.96710008   0.61790001   0.89960003
    2.69501114]
 [169.06780095   0.09780001   0.92740005   0.40190002   0.80979997
    2.64336944]
 [ 50.44494798   0.176        0.59850001   0.226        0.48639998
    2.36199617]
 ...
 [251.40720604   0.0796       0.95480007   0.46799999   0.85690004
    2.66903377]
 [382.73465218   0.29970002   0.8976       0.67989999   0.90350002
    2.76005602]
 [300.6884098    0.0368       0.88840002   0.56200004   0.87300009
    2.83978319]][0m
[37m[1m[2023-07-11 05:28:20,921][233954] Max Reward on eval: 657.1012611516751[0m
[37m[1m[2023-07-11 05:28:20,922][233954] Min Reward on eval: -92.75855515846051[0m
[37m[1m[2023-07-11 05:28:20,922][233954] Mean Reward across all agents: 313.5438599130767[0m
[37m[1m[2023-07-11 05:28:20,922][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:28:20,928][233954] mean_value=36.626576694705676, max_value=759.914173127763[0m
[37m[1m[2023-07-11 05:28:20,931][233954] New mean coefficients: [[ 4.99803   -1.064225  -1.1122642 -4.373872  -5.040381  -6.149035 ]][0m
[37m[1m[2023-07-11 05:28:20,932][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:28:29,968][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 05:28:29,968][233954] FPS: 425020.63[0m
[36m[2023-07-11 05:28:29,971][233954] itr=419, itrs=2000, Progress: 20.95%[0m
[36m[2023-07-11 05:28:41,696][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 05:28:41,696][233954] FPS: 329794.38[0m
[36m[2023-07-11 05:28:45,950][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:28:45,955][233954] Reward + Measures: [[560.6679855    0.02399733   0.96666163   0.71902806   0.95205271
    2.92685509]][0m
[37m[1m[2023-07-11 05:28:45,955][233954] Max Reward on eval: 560.6679855035297[0m
[37m[1m[2023-07-11 05:28:45,956][233954] Min Reward on eval: 560.6679855035297[0m
[37m[1m[2023-07-11 05:28:45,956][233954] Mean Reward across all agents: 560.6679855035297[0m
[37m[1m[2023-07-11 05:28:45,956][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:28:50,981][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:28:50,987][233954] Reward + Measures: [[589.48370363   0.0065       0.98869991   0.78260005   0.97720003
    2.76337934]
 [303.95097899   0.27649999   0.94580001   0.63620007   0.9188
    2.65247035]
 [ 16.70179824   0.63009995   0.91390002   0.65219998   0.884
    2.54096985]
 ...
 [310.99443435   0.0042       0.99560004   0.70300001   0.99279994
    3.1039741 ]
 [ 73.94782421   0.1551       0.53780001   0.1952       0.4429
    2.2780273 ]
 [423.77363805   0.0436       0.86660004   0.58450001   0.82660002
    2.69466519]][0m
[37m[1m[2023-07-11 05:28:50,987][233954] Max Reward on eval: 672.6019515991211[0m
[37m[1m[2023-07-11 05:28:50,987][233954] Min Reward on eval: -25.59504910870455[0m
[37m[1m[2023-07-11 05:28:50,987][233954] Mean Reward across all agents: 335.4491346353549[0m
[37m[1m[2023-07-11 05:28:50,988][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:28:50,992][233954] mean_value=13.458027023324053, max_value=619.308317577561[0m
[37m[1m[2023-07-11 05:28:50,995][233954] New mean coefficients: [[ 4.6786914 -1.6327002 -1.9936832 -5.3327384 -5.7600136 -7.273162 ]][0m
[37m[1m[2023-07-11 05:28:50,996][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:29:00,082][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 05:29:00,083][233954] FPS: 422682.16[0m
[36m[2023-07-11 05:29:00,085][233954] itr=420, itrs=2000, Progress: 21.00%[0m
[37m[1m[2023-07-11 05:32:14,852][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000400[0m
[36m[2023-07-11 05:32:27,069][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 05:32:27,069][233954] FPS: 331912.05[0m
[36m[2023-07-11 05:32:31,236][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:32:31,237][233954] Reward + Measures: [[516.56420991   0.026137     0.96051335   0.69150376   0.94195396
    2.91993165]][0m
[37m[1m[2023-07-11 05:32:31,237][233954] Max Reward on eval: 516.5642099086962[0m
[37m[1m[2023-07-11 05:32:31,237][233954] Min Reward on eval: 516.5642099086962[0m
[37m[1m[2023-07-11 05:32:31,238][233954] Mean Reward across all agents: 516.5642099086962[0m
[37m[1m[2023-07-11 05:32:31,238][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:32:36,120][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:32:36,120][233954] Reward + Measures: [[117.39549208   0.1158       0.80409998   0.30039999   0.67950004
    2.79484725]
 [452.58238557   0.0525       0.86210006   0.58560002   0.81850004
    2.9089222 ]
 [479.11641335   0.0457       0.87869996   0.60699999   0.83809996
    2.88522005]
 ...
 [644.58199123   0.0185       0.97650003   0.75420004   0.96400005
    3.11358809]
 [459.81804658   0.0309       0.97679996   0.64850008   0.9526
    2.90368342]
 [547.5269115    0.0376       0.97109997   0.64720005   0.94019997
    2.96540999]][0m
[37m[1m[2023-07-11 05:32:36,121][233954] Max Reward on eval: 778.8245696820319[0m
[37m[1m[2023-07-11 05:32:36,121][233954] Min Reward on eval: -17.62284968085587[0m
[37m[1m[2023-07-11 05:32:36,121][233954] Mean Reward across all agents: 411.51826368875413[0m
[37m[1m[2023-07-11 05:32:36,121][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:32:36,126][233954] mean_value=9.30862590981771, max_value=632.5958670725062[0m
[37m[1m[2023-07-11 05:32:36,129][233954] New mean coefficients: [[ 2.4323862 -2.7841394 -6.567863  -7.4969397 -6.7672153 -8.808175 ]][0m
[37m[1m[2023-07-11 05:32:36,130][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:32:45,153][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 05:32:45,153][233954] FPS: 425681.57[0m
[36m[2023-07-11 05:32:45,155][233954] itr=421, itrs=2000, Progress: 21.05%[0m
[36m[2023-07-11 05:32:57,110][233954] train() took 11.88 seconds to complete[0m
[36m[2023-07-11 05:32:57,110][233954] FPS: 323337.70[0m
[36m[2023-07-11 05:33:01,376][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:33:01,376][233954] Reward + Measures: [[340.89164404   0.05241233   0.95077097   0.57330865   0.911093
    2.90064716]][0m
[37m[1m[2023-07-11 05:33:01,376][233954] Max Reward on eval: 340.8916440375608[0m
[37m[1m[2023-07-11 05:33:01,377][233954] Min Reward on eval: 340.8916440375608[0m
[37m[1m[2023-07-11 05:33:01,377][233954] Mean Reward across all agents: 340.8916440375608[0m
[37m[1m[2023-07-11 05:33:01,377][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:33:06,320][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:33:06,320][233954] Reward + Measures: [[302.02515603   0.0686       0.7525       0.41840002   0.71150005
    2.84198356]
 [479.63826133   0.025        0.84240001   0.59969997   0.81739998
    3.15567899]
 [ 73.12393435   0.0706       0.90380001   0.3448       0.78680003
    2.83882713]
 ...
 [ 67.12261391   0.0767       0.85690004   0.30590001   0.72320002
    2.72533941]
 [390.06188733   0.0572       0.85890001   0.49689999   0.80360001
    2.99835682]
 [136.10085163   0.0345       0.95629996   0.50209999   0.88810009
    2.83686829]][0m
[37m[1m[2023-07-11 05:33:06,320][233954] Max Reward on eval: 681.9038643870502[0m
[37m[1m[2023-07-11 05:33:06,321][233954] Min Reward on eval: -36.74462074255571[0m
[37m[1m[2023-07-11 05:33:06,321][233954] Mean Reward across all agents: 242.4234835797556[0m
[37m[1m[2023-07-11 05:33:06,321][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:33:06,325][233954] mean_value=-75.94082276060043, max_value=357.4281077462557[0m
[37m[1m[2023-07-11 05:33:06,328][233954] New mean coefficients: [[  3.5382018  -3.977684   -3.5502734  -7.7360716  -8.230943  -10.075897 ]][0m
[37m[1m[2023-07-11 05:33:06,329][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:33:15,262][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 05:33:15,262][233954] FPS: 429955.13[0m
[36m[2023-07-11 05:33:15,265][233954] itr=422, itrs=2000, Progress: 21.10%[0m
[36m[2023-07-11 05:33:27,007][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 05:33:27,007][233954] FPS: 329392.59[0m
[36m[2023-07-11 05:33:31,229][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:33:31,229][233954] Reward + Measures: [[304.76420078   0.08711267   0.93125367   0.48018631   0.86297393
    2.88720417]][0m
[37m[1m[2023-07-11 05:33:31,229][233954] Max Reward on eval: 304.76420078001973[0m
[37m[1m[2023-07-11 05:33:31,230][233954] Min Reward on eval: 304.76420078001973[0m
[37m[1m[2023-07-11 05:33:31,230][233954] Mean Reward across all agents: 304.76420078001973[0m
[37m[1m[2023-07-11 05:33:31,230][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:33:36,410][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:33:36,415][233954] Reward + Measures: [[377.78073397   0.0538       0.8757       0.55129999   0.8427
    2.91175151]
 [356.04957627   0.24919999   0.89050001   0.63230002   0.88099998
    3.01791573]
 [239.28010371   0.24430001   0.91860002   0.52810001   0.89250004
    3.07290912]
 ...
 [149.22206234   0.25299999   0.93260002   0.43150002   0.85389996
    3.02443552]
 [506.13443567   0.1227       0.9587       0.70170003   0.94490004
    3.01743603]
 [407.00141836   0.1041       0.94190007   0.53470004   0.87480003
    3.02611732]][0m
[37m[1m[2023-07-11 05:33:36,416][233954] Max Reward on eval: 701.9649391170591[0m
[37m[1m[2023-07-11 05:33:36,416][233954] Min Reward on eval: 13.935542013542726[0m
[37m[1m[2023-07-11 05:33:36,416][233954] Mean Reward across all agents: 378.44820650102105[0m
[37m[1m[2023-07-11 05:33:36,416][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:33:36,419][233954] mean_value=-103.60398589335414, max_value=445.9421454273596[0m
[37m[1m[2023-07-11 05:33:36,422][233954] New mean coefficients: [[  3.5434732  -4.535867   -3.2075894  -8.076363   -8.736304  -10.937412 ]][0m
[37m[1m[2023-07-11 05:33:36,423][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:33:45,403][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 05:33:45,403][233954] FPS: 427690.44[0m
[36m[2023-07-11 05:33:45,405][233954] itr=423, itrs=2000, Progress: 21.15%[0m
[36m[2023-07-11 05:33:56,924][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 05:33:56,925][233954] FPS: 335670.61[0m
[36m[2023-07-11 05:34:01,209][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:34:01,209][233954] Reward + Measures: [[151.10979092   0.14569199   0.88799232   0.33909333   0.77697265
    2.83303523]][0m
[37m[1m[2023-07-11 05:34:01,209][233954] Max Reward on eval: 151.10979092397284[0m
[37m[1m[2023-07-11 05:34:01,210][233954] Min Reward on eval: 151.10979092397284[0m
[37m[1m[2023-07-11 05:34:01,210][233954] Mean Reward across all agents: 151.10979092397284[0m
[37m[1m[2023-07-11 05:34:01,210][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:34:06,156][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:34:06,156][233954] Reward + Measures: [[156.22660208   0.08200001   0.87489998   0.29899999   0.71820003
    2.81877017]
 [ 84.20650221   0.116        0.85249996   0.2309       0.68500006
    2.78216052]
 [263.64863251   0.08880001   0.84699994   0.48909998   0.7924
    2.77981639]
 ...
 [149.68105536   0.09440001   0.69730002   0.2983       0.60720003
    2.7029233 ]
 [ 59.81370775   0.1134       0.50520003   0.17690001   0.40459996
    2.54940104]
 [411.61950207   0.1005       0.93790001   0.53430003   0.85710001
    2.85481334]][0m
[37m[1m[2023-07-11 05:34:06,157][233954] Max Reward on eval: 552.3579368535429[0m
[37m[1m[2023-07-11 05:34:06,157][233954] Min Reward on eval: -164.8800114389509[0m
[37m[1m[2023-07-11 05:34:06,157][233954] Mean Reward across all agents: 108.72953100825438[0m
[37m[1m[2023-07-11 05:34:06,157][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:34:06,162][233954] mean_value=-31.76238774053129, max_value=617.3863349050283[0m
[37m[1m[2023-07-11 05:34:06,165][233954] New mean coefficients: [[  2.1994562  -5.219565   -5.7602663  -8.896572   -9.565723  -11.907215 ]][0m
[37m[1m[2023-07-11 05:34:06,166][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:34:15,149][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 05:34:15,149][233954] FPS: 427546.71[0m
[36m[2023-07-11 05:34:15,152][233954] itr=424, itrs=2000, Progress: 21.20%[0m
[36m[2023-07-11 05:34:26,965][233954] train() took 11.73 seconds to complete[0m
[36m[2023-07-11 05:34:26,966][233954] FPS: 327365.02[0m
[36m[2023-07-11 05:34:31,334][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:34:31,335][233954] Reward + Measures: [[87.25372328  0.17793134  0.86966193  0.28552935  0.73875701  2.78883004]][0m
[37m[1m[2023-07-11 05:34:31,335][233954] Max Reward on eval: 87.25372328269671[0m
[37m[1m[2023-07-11 05:34:31,335][233954] Min Reward on eval: 87.25372328269671[0m
[37m[1m[2023-07-11 05:34:31,335][233954] Mean Reward across all agents: 87.25372328269671[0m
[37m[1m[2023-07-11 05:34:31,336][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:34:36,300][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:34:36,301][233954] Reward + Measures: [[154.89102174   0.05070001   0.92460006   0.30309999   0.78459996
    2.99360323]
 [455.43722629   0.0409       0.94490004   0.63560003   0.87770003
    3.00507593]
 [347.66043328   0.0533       0.96589994   0.46510002   0.91289997
    3.17697072]
 ...
 [157.17799284   0.048        0.9285       0.32049999   0.81059998
    2.99611664]
 [335.4397999    0.0442       0.91640007   0.46739998   0.84930003
    2.87479186]
 [247.64759397   0.0435       0.93489999   0.38330001   0.82579994
    2.96919394]][0m
[37m[1m[2023-07-11 05:34:36,301][233954] Max Reward on eval: 734.690849327948[0m
[37m[1m[2023-07-11 05:34:36,301][233954] Min Reward on eval: 43.8364599683322[0m
[37m[1m[2023-07-11 05:34:36,301][233954] Mean Reward across all agents: 334.8168971188713[0m
[37m[1m[2023-07-11 05:34:36,302][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:34:36,305][233954] mean_value=-73.30526053054031, max_value=474.49802497120993[0m
[37m[1m[2023-07-11 05:34:36,307][233954] New mean coefficients: [[  1.9470694  -5.215343   -6.771287   -8.415548   -9.723029  -12.305436 ]][0m
[37m[1m[2023-07-11 05:34:36,308][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:34:45,220][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 05:34:45,220][233954] FPS: 430976.65[0m
[36m[2023-07-11 05:34:45,222][233954] itr=425, itrs=2000, Progress: 21.25%[0m
[36m[2023-07-11 05:34:56,985][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 05:34:56,985][233954] FPS: 328791.89[0m
[36m[2023-07-11 05:35:01,201][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:35:01,201][233954] Reward + Measures: [[15.87469356  0.20003733  0.83156103  0.22038735  0.67654866  2.7595892 ]][0m
[37m[1m[2023-07-11 05:35:01,201][233954] Max Reward on eval: 15.874693559286108[0m
[37m[1m[2023-07-11 05:35:01,202][233954] Min Reward on eval: 15.874693559286108[0m
[37m[1m[2023-07-11 05:35:01,202][233954] Mean Reward across all agents: 15.874693559286108[0m
[37m[1m[2023-07-11 05:35:01,202][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:35:06,219][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:35:06,219][233954] Reward + Measures: [[ 94.83064868   0.12819999   0.91000003   0.41580001   0.82550001
    3.15728354]
 [323.64729404   0.0326       0.95250005   0.4718       0.87220001
    2.89642191]
 [257.8209858    0.0216       0.98019999   0.46599999   0.92290002
    2.78285789]
 ...
 [199.84916211   0.0379       0.94800007   0.41590005   0.87589997
    2.82225442]
 [ 71.4889481    0.0857       0.76920003   0.24200001   0.67989999
    2.55280185]
 [343.82521676   0.0248       0.96189994   0.51069999   0.88440001
    2.81011558]][0m
[37m[1m[2023-07-11 05:35:06,220][233954] Max Reward on eval: 702.0681705474854[0m
[37m[1m[2023-07-11 05:35:06,220][233954] Min Reward on eval: -40.748553950199856[0m
[37m[1m[2023-07-11 05:35:06,220][233954] Mean Reward across all agents: 245.83938837165658[0m
[37m[1m[2023-07-11 05:35:06,221][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:35:06,223][233954] mean_value=-124.80779669668622, max_value=111.70937385770094[0m
[37m[1m[2023-07-11 05:35:06,225][233954] New mean coefficients: [[  2.317261   -4.551213   -6.4244637  -7.956312   -8.866024  -11.010387 ]][0m
[37m[1m[2023-07-11 05:35:06,226][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:35:15,317][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 05:35:15,317][233954] FPS: 422481.46[0m
[36m[2023-07-11 05:35:15,320][233954] itr=426, itrs=2000, Progress: 21.30%[0m
[36m[2023-07-11 05:35:27,017][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 05:35:27,017][233954] FPS: 330587.17[0m
[36m[2023-07-11 05:35:31,318][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:35:31,318][233954] Reward + Measures: [[-8.98435455  0.20265532  0.79283398  0.201519    0.63131434  2.72570491]][0m
[37m[1m[2023-07-11 05:35:31,319][233954] Max Reward on eval: -8.984354550188757[0m
[37m[1m[2023-07-11 05:35:31,319][233954] Min Reward on eval: -8.984354550188757[0m
[37m[1m[2023-07-11 05:35:31,319][233954] Mean Reward across all agents: -8.984354550188757[0m
[37m[1m[2023-07-11 05:35:31,319][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:35:36,345][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:35:36,397][233954] Reward + Measures: [[ -57.38361097    0.16870001    0.51809996    0.12090001    0.39899999
     2.41546535]
 [ -26.98732542    0.22160001    0.64890003    0.1033        0.4815
     2.47709775]
 [ -23.93019304    0.1793        0.59680003    0.1226        0.4395
     2.51171374]
 ...
 [ -64.31794548    0.41309997    0.86680001    0.09410001    0.74360001
     2.69197536]
 [   1.6237211     0.15930001    0.45159999    0.1408        0.36690003
     2.34290242]
 [-106.33292887    0.30990002    0.75160003    0.1056        0.58450001
     2.61333466]][0m
[37m[1m[2023-07-11 05:35:36,398][233954] Max Reward on eval: 407.0931949365884[0m
[37m[1m[2023-07-11 05:35:36,398][233954] Min Reward on eval: -204.30489393770694[0m
[37m[1m[2023-07-11 05:35:36,398][233954] Mean Reward across all agents: -22.39209745872366[0m
[37m[1m[2023-07-11 05:35:36,398][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:35:36,406][233954] mean_value=143.19637706252368, max_value=570.2440430328803[0m
[37m[1m[2023-07-11 05:35:36,409][233954] New mean coefficients: [[  1.5698216  -5.7350254  -7.4416018  -9.191393  -10.318344  -12.274141 ]][0m
[37m[1m[2023-07-11 05:35:36,410][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:35:45,410][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 05:35:45,411][233954] FPS: 426722.33[0m
[36m[2023-07-11 05:35:45,413][233954] itr=427, itrs=2000, Progress: 21.35%[0m
[36m[2023-07-11 05:35:57,071][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 05:35:57,071][233954] FPS: 331777.48[0m
[36m[2023-07-11 05:36:01,316][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:36:01,317][233954] Reward + Measures: [[-33.66733682   0.18970968   0.71051121   0.17637601   0.54650599
    2.67599392]][0m
[37m[1m[2023-07-11 05:36:01,317][233954] Max Reward on eval: -33.667336823494054[0m
[37m[1m[2023-07-11 05:36:01,317][233954] Min Reward on eval: -33.667336823494054[0m
[37m[1m[2023-07-11 05:36:01,317][233954] Mean Reward across all agents: -33.667336823494054[0m
[37m[1m[2023-07-11 05:36:01,318][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:36:06,373][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:36:06,378][233954] Reward + Measures: [[-73.33055207   0.2455       0.73009998   0.13869999   0.5399
    2.64745593]
 [ 52.29798603   0.3215       0.77590007   0.34990001   0.68040001
    2.7588861 ]
 [ 60.047964     0.29350001   0.88399994   0.32620001   0.7572
    2.78899384]
 ...
 [ 53.89619041   0.20660003   0.42529997   0.21610001   0.35069999
    2.63272667]
 [198.16472005   0.1806       0.87400001   0.38980004   0.7791
    2.89772868]
 [ 47.83201535   0.2595       0.86440003   0.3193       0.72119999
    2.84627342]][0m
[37m[1m[2023-07-11 05:36:06,379][233954] Max Reward on eval: 672.3478126598523[0m
[37m[1m[2023-07-11 05:36:06,379][233954] Min Reward on eval: -98.12207601089031[0m
[37m[1m[2023-07-11 05:36:06,379][233954] Mean Reward across all agents: 97.5535046812859[0m
[37m[1m[2023-07-11 05:36:06,380][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:36:06,383][233954] mean_value=-57.485100541119365, max_value=552.1462168958969[0m
[37m[1m[2023-07-11 05:36:06,386][233954] New mean coefficients: [[  1.8248669  -5.081431   -6.704769   -8.030163   -9.809547  -11.377767 ]][0m
[37m[1m[2023-07-11 05:36:06,387][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:36:15,315][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 05:36:15,315][233954] FPS: 430171.56[0m
[36m[2023-07-11 05:36:15,317][233954] itr=428, itrs=2000, Progress: 21.40%[0m
[36m[2023-07-11 05:36:27,039][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 05:36:27,040][233954] FPS: 329838.03[0m
[36m[2023-07-11 05:36:31,392][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:36:31,392][233954] Reward + Measures: [[-58.8302365    0.19312567   0.67110801   0.15817833   0.49907601
    2.64684749]][0m
[37m[1m[2023-07-11 05:36:31,392][233954] Max Reward on eval: -58.83023650035553[0m
[37m[1m[2023-07-11 05:36:31,393][233954] Min Reward on eval: -58.83023650035553[0m
[37m[1m[2023-07-11 05:36:31,393][233954] Mean Reward across all agents: -58.83023650035553[0m
[37m[1m[2023-07-11 05:36:31,393][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:36:36,641][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:36:36,647][233954] Reward + Measures: [[ -46.69378754    0.11790001    0.31150001    0.0962        0.31710002
     2.64161253]
 [  33.08445694    0.14780001    0.42669997    0.13090001    0.37140003
     2.60465097]
 [ -77.61005937    0.17260002    0.58280003    0.14600001    0.45830002
     2.79567313]
 ...
 [ -90.63158723    0.20910001    0.56460005    0.1247        0.43739995
     2.79197621]
 [-118.37508641    0.21440001    0.72299999    0.1549        0.48480001
     2.77822185]
 [ -19.20374945    0.18419999    0.59939998    0.19770001    0.47550002
     2.60984445]][0m
[37m[1m[2023-07-11 05:36:36,647][233954] Max Reward on eval: 437.356923086755[0m
[37m[1m[2023-07-11 05:36:36,647][233954] Min Reward on eval: -207.18671563956886[0m
[37m[1m[2023-07-11 05:36:36,648][233954] Mean Reward across all agents: -10.354430124684713[0m
[37m[1m[2023-07-11 05:36:36,648][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:36:36,651][233954] mean_value=-98.60768683083859, max_value=185.68934591778176[0m
[37m[1m[2023-07-11 05:36:36,653][233954] New mean coefficients: [[  2.8138986  -4.5005054  -5.4541373  -7.155314   -9.439257  -10.56068  ]][0m
[37m[1m[2023-07-11 05:36:36,654][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:36:45,636][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 05:36:45,636][233954] FPS: 427601.72[0m
[36m[2023-07-11 05:36:45,639][233954] itr=429, itrs=2000, Progress: 21.45%[0m
[36m[2023-07-11 05:36:57,322][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 05:36:57,322][233954] FPS: 330965.18[0m
[36m[2023-07-11 05:37:01,698][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:37:01,699][233954] Reward + Measures: [[-73.13074369   0.18405101   0.61065698   0.148434     0.44354662
    2.60009956]][0m
[37m[1m[2023-07-11 05:37:01,699][233954] Max Reward on eval: -73.13074369050695[0m
[37m[1m[2023-07-11 05:37:01,699][233954] Min Reward on eval: -73.13074369050695[0m
[37m[1m[2023-07-11 05:37:01,699][233954] Mean Reward across all agents: -73.13074369050695[0m
[37m[1m[2023-07-11 05:37:01,700][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:37:06,726][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:37:06,727][233954] Reward + Measures: [[ 82.76353414   0.23510002   0.71889991   0.20410001   0.53070003
    2.75368929]
 [-95.71116475   0.19760001   0.64530003   0.13200001   0.44650003
    2.58479738]
 [107.30626679   0.1919       0.83799994   0.2395       0.71469998
    2.87761354]
 ...
 [-18.3090569    0.18350001   0.67519999   0.17369999   0.51250005
    2.73472476]
 [ 31.13137675   0.30669999   0.59280008   0.25330001   0.47279999
    2.60105157]
 [-31.33931496   0.2139       0.61860001   0.1516       0.45269999
    2.72397852]][0m
[37m[1m[2023-07-11 05:37:06,727][233954] Max Reward on eval: 144.54544257102535[0m
[37m[1m[2023-07-11 05:37:06,728][233954] Min Reward on eval: -172.60061550159008[0m
[37m[1m[2023-07-11 05:37:06,728][233954] Mean Reward across all agents: 30.298770921898758[0m
[37m[1m[2023-07-11 05:37:06,728][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:37:06,732][233954] mean_value=-45.04879157468157, max_value=119.98070556892269[0m
[37m[1m[2023-07-11 05:37:06,734][233954] New mean coefficients: [[  2.3115518  -4.7562027  -6.607634   -6.9964523  -9.474725  -10.37988  ]][0m
[37m[1m[2023-07-11 05:37:06,735][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:37:15,859][233954] train() took 9.12 seconds to complete[0m
[36m[2023-07-11 05:37:15,859][233954] FPS: 420949.82[0m
[36m[2023-07-11 05:37:15,862][233954] itr=430, itrs=2000, Progress: 21.50%[0m
[37m[1m[2023-07-11 05:40:33,996][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000410[0m
[36m[2023-07-11 05:40:46,316][233954] train() took 11.77 seconds to complete[0m
[36m[2023-07-11 05:40:46,316][233954] FPS: 326277.42[0m
[36m[2023-07-11 05:40:50,491][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:40:50,491][233954] Reward + Measures: [[-74.31488085   0.16728599   0.54489064   0.13891333   0.38244367
    2.54642081]][0m
[37m[1m[2023-07-11 05:40:50,492][233954] Max Reward on eval: -74.3148808544031[0m
[37m[1m[2023-07-11 05:40:50,492][233954] Min Reward on eval: -74.3148808544031[0m
[37m[1m[2023-07-11 05:40:50,492][233954] Mean Reward across all agents: -74.3148808544031[0m
[37m[1m[2023-07-11 05:40:50,492][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:40:55,505][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:40:55,511][233954] Reward + Measures: [[119.30631412   0.1693       0.66149992   0.24099998   0.55600005
    2.76758909]
 [-59.56665467   0.13520001   0.45930004   0.12750001   0.34020004
    2.55465388]
 [463.49241877   0.0605       0.92430001   0.53549999   0.82240003
    2.98758268]
 ...
 [-49.54435066   0.16180001   0.67340004   0.1653       0.45749998
    2.74871135]
 [297.79016496   0.0846       0.79360002   0.37979999   0.68090004
    2.84706593]
 [171.74675264   0.10210001   0.59620005   0.22740002   0.50220001
    2.64449716]][0m
[37m[1m[2023-07-11 05:40:55,511][233954] Max Reward on eval: 676.2654113519936[0m
[37m[1m[2023-07-11 05:40:55,512][233954] Min Reward on eval: -94.12436186857522[0m
[37m[1m[2023-07-11 05:40:55,512][233954] Mean Reward across all agents: 116.88919475377135[0m
[37m[1m[2023-07-11 05:40:55,512][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:40:55,516][233954] mean_value=-42.15226494617344, max_value=487.30041248160416[0m
[37m[1m[2023-07-11 05:40:55,519][233954] New mean coefficients: [[  1.7767924  -5.211749   -8.382227   -7.9524283  -9.849405  -11.096504 ]][0m
[37m[1m[2023-07-11 05:40:55,520][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:41:04,539][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 05:41:04,539][233954] FPS: 425847.15[0m
[36m[2023-07-11 05:41:04,541][233954] itr=431, itrs=2000, Progress: 21.55%[0m
[36m[2023-07-11 05:41:16,253][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 05:41:16,253][233954] FPS: 330285.44[0m
[36m[2023-07-11 05:41:20,467][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:41:20,472][233954] Reward + Measures: [[-77.08015504   0.16454667   0.514947     0.13283934   0.35712698
    2.52672338]][0m
[37m[1m[2023-07-11 05:41:20,473][233954] Max Reward on eval: -77.0801550388604[0m
[37m[1m[2023-07-11 05:41:20,473][233954] Min Reward on eval: -77.0801550388604[0m
[37m[1m[2023-07-11 05:41:20,473][233954] Mean Reward across all agents: -77.0801550388604[0m
[37m[1m[2023-07-11 05:41:20,473][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:41:25,406][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:41:25,407][233954] Reward + Measures: [[ 212.5507493     0.14170001    0.70089996    0.27770001    0.59140003
     2.90235925]
 [  39.81085523    0.114         0.59610003    0.21469998    0.5018
     2.8660171 ]
 [ 208.51313113    0.072         0.67819995    0.29390001    0.62099999
     3.00854349]
 ...
 [-325.02236366    0.49169999    0.89670002    0.0815        0.75360006
     2.90147567]
 [ -66.11721542    0.1234        0.48839998    0.15030001    0.39610001
     2.73688078]
 [ 141.08859538    0.11310001    0.85170001    0.28220001    0.69370002
     3.04680228]][0m
[37m[1m[2023-07-11 05:41:25,407][233954] Max Reward on eval: 689.8725776528008[0m
[37m[1m[2023-07-11 05:41:25,407][233954] Min Reward on eval: -417.3333749704063[0m
[37m[1m[2023-07-11 05:41:25,408][233954] Mean Reward across all agents: 39.89599028960842[0m
[37m[1m[2023-07-11 05:41:25,408][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:41:25,412][233954] mean_value=-27.56148554935653, max_value=478.839485930837[0m
[37m[1m[2023-07-11 05:41:25,415][233954] New mean coefficients: [[  1.9719532  -6.8069086  -7.628281   -9.637984  -11.549975  -12.704922 ]][0m
[37m[1m[2023-07-11 05:41:25,416][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:41:34,364][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 05:41:34,364][233954] FPS: 429234.45[0m
[36m[2023-07-11 05:41:34,366][233954] itr=432, itrs=2000, Progress: 21.60%[0m
[36m[2023-07-11 05:41:46,086][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 05:41:46,086][233954] FPS: 329977.13[0m
[36m[2023-07-11 05:41:50,262][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:41:50,263][233954] Reward + Measures: [[-73.35078142   0.14714067   0.44466332   0.12542732   0.311856
    2.48691869]][0m
[37m[1m[2023-07-11 05:41:50,263][233954] Max Reward on eval: -73.35078142169131[0m
[37m[1m[2023-07-11 05:41:50,263][233954] Min Reward on eval: -73.35078142169131[0m
[37m[1m[2023-07-11 05:41:50,264][233954] Mean Reward across all agents: -73.35078142169131[0m
[37m[1m[2023-07-11 05:41:50,264][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:41:55,221][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:41:55,221][233954] Reward + Measures: [[ 70.99486247   0.1737       0.36770001   0.1178       0.28299999
    2.36894655]
 [ 40.62221719   0.1559       0.35280001   0.1201       0.27070001
    2.3667767 ]
 [120.18701771   0.2397       0.50650001   0.09859999   0.42589998
    2.46883821]
 ...
 [-39.52589119   0.1558       0.4285       0.1141       0.34039998
    2.38585711]
 [ 61.10641528   0.27959999   0.46169996   0.0902       0.40950003
    2.46195865]
 [127.96310185   0.2481       0.49650002   0.1176       0.3915
    2.43904853]][0m
[37m[1m[2023-07-11 05:41:55,222][233954] Max Reward on eval: 392.64478350561114[0m
[37m[1m[2023-07-11 05:41:55,222][233954] Min Reward on eval: -96.53291831761598[0m
[37m[1m[2023-07-11 05:41:55,222][233954] Mean Reward across all agents: 67.00394960939937[0m
[37m[1m[2023-07-11 05:41:55,222][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:41:55,227][233954] mean_value=-20.724868534783102, max_value=707.983787858989[0m
[37m[1m[2023-07-11 05:41:55,230][233954] New mean coefficients: [[  3.0755386  -6.2185583  -4.887844   -8.336828  -11.136701  -11.451839 ]][0m
[37m[1m[2023-07-11 05:41:55,231][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:42:04,213][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 05:42:04,213][233954] FPS: 427596.75[0m
[36m[2023-07-11 05:42:04,216][233954] itr=433, itrs=2000, Progress: 21.65%[0m
[36m[2023-07-11 05:42:15,769][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 05:42:15,769][233954] FPS: 334682.66[0m
[36m[2023-07-11 05:42:19,987][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:42:19,992][233954] Reward + Measures: [[-72.02773452   0.144687     0.42242733   0.12205867   0.29404798
    2.44869971]][0m
[37m[1m[2023-07-11 05:42:19,993][233954] Max Reward on eval: -72.02773452191803[0m
[37m[1m[2023-07-11 05:42:19,994][233954] Min Reward on eval: -72.02773452191803[0m
[37m[1m[2023-07-11 05:42:19,994][233954] Mean Reward across all agents: -72.02773452191803[0m
[37m[1m[2023-07-11 05:42:19,995][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:42:24,963][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:42:24,963][233954] Reward + Measures: [[ 99.36259137   0.30309999   0.65630001   0.31279999   0.60140002
    2.48961234]
 [120.81825409   0.26200002   0.68339998   0.2888       0.57890004
    2.45211339]
 [-53.91142242   0.21229999   0.5873       0.16140001   0.46339998
    2.63860917]
 ...
 [ 60.53284767   0.19910002   0.59870005   0.2191       0.48890001
    2.46243978]
 [102.89607405   0.33040005   0.67980003   0.3335       0.62270004
    2.48872066]
 [ 81.82945311   0.45580003   0.72030002   0.4244       0.69609994
    2.5856545 ]][0m
[37m[1m[2023-07-11 05:42:24,964][233954] Max Reward on eval: 183.66702698785812[0m
[37m[1m[2023-07-11 05:42:24,964][233954] Min Reward on eval: -102.64971723174676[0m
[37m[1m[2023-07-11 05:42:24,964][233954] Mean Reward across all agents: 68.41016999207407[0m
[37m[1m[2023-07-11 05:42:24,964][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:42:24,967][233954] mean_value=-43.32370909315264, max_value=418.8244133704927[0m
[37m[1m[2023-07-11 05:42:24,970][233954] New mean coefficients: [[  3.1301327  -7.045151   -3.70437    -8.849731  -12.042306  -12.710952 ]][0m
[37m[1m[2023-07-11 05:42:24,971][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:42:34,056][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 05:42:34,057][233954] FPS: 422723.22[0m
[36m[2023-07-11 05:42:34,059][233954] itr=434, itrs=2000, Progress: 21.70%[0m
[36m[2023-07-11 05:42:45,781][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 05:42:45,781][233954] FPS: 329888.24[0m
[36m[2023-07-11 05:42:50,066][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:42:50,066][233954] Reward + Measures: [[-66.03711656   0.13949099   0.40273935   0.11992334   0.27871799
    2.41890168]][0m
[37m[1m[2023-07-11 05:42:50,066][233954] Max Reward on eval: -66.03711656107467[0m
[37m[1m[2023-07-11 05:42:50,067][233954] Min Reward on eval: -66.03711656107467[0m
[37m[1m[2023-07-11 05:42:50,067][233954] Mean Reward across all agents: -66.03711656107467[0m
[37m[1m[2023-07-11 05:42:50,067][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:42:55,045][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:42:55,049][233954] Reward + Measures: [[122.32432604   0.27519998   0.69440001   0.30640003   0.616
    2.6290586 ]
 [129.85687829   0.27850002   0.66020006   0.30229998   0.61190003
    2.61269689]
 [123.66843785   0.20040002   0.57359999   0.22810002   0.48909998
    2.49273872]
 ...
 [133.56193448   0.2247       0.71170002   0.25140002   0.62530005
    2.60417628]
 [157.98117352   0.38009998   0.7525       0.39650002   0.66000003
    2.61287379]
 [139.02193977   0.26800001   0.6584       0.31690001   0.58929998
    2.5791204 ]][0m
[37m[1m[2023-07-11 05:42:55,050][233954] Max Reward on eval: 248.20485402084887[0m
[37m[1m[2023-07-11 05:42:55,050][233954] Min Reward on eval: -96.31384333870373[0m
[37m[1m[2023-07-11 05:42:55,050][233954] Mean Reward across all agents: 124.88213534039882[0m
[37m[1m[2023-07-11 05:42:55,051][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:42:55,055][233954] mean_value=-13.012942986347625, max_value=596.0368595621856[0m
[37m[1m[2023-07-11 05:42:55,057][233954] New mean coefficients: [[  2.078794   -6.8134894  -5.4773664  -7.398249  -12.054507  -12.438614 ]][0m
[37m[1m[2023-07-11 05:42:55,058][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:43:04,064][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 05:43:04,065][233954] FPS: 426462.33[0m
[36m[2023-07-11 05:43:04,067][233954] itr=435, itrs=2000, Progress: 21.75%[0m
[36m[2023-07-11 05:43:15,620][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 05:43:15,621][233954] FPS: 334795.30[0m
[36m[2023-07-11 05:43:19,847][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:43:19,848][233954] Reward + Measures: [[-60.04836605   0.12898333   0.36225435   0.11548501   0.25330836
    2.38574958]][0m
[37m[1m[2023-07-11 05:43:19,848][233954] Max Reward on eval: -60.04836604995036[0m
[37m[1m[2023-07-11 05:43:19,848][233954] Min Reward on eval: -60.04836604995036[0m
[37m[1m[2023-07-11 05:43:19,849][233954] Mean Reward across all agents: -60.04836604995036[0m
[37m[1m[2023-07-11 05:43:19,849][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:43:24,821][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:43:24,822][233954] Reward + Measures: [[ 25.73595729   0.23020001   0.53740001   0.1876       0.5151
    2.49943686]
 [100.3227684    0.29620001   0.55409998   0.18439999   0.53529996
    2.54531622]
 [ 54.67880597   0.23360001   0.5363       0.1451       0.537
    2.66118026]
 ...
 [-34.01952807   0.22230001   0.3964       0.0964       0.41300002
    2.54723048]
 [ 51.43556779   0.1723       0.33020002   0.11849999   0.3161
    2.551826  ]
 [-46.05469834   0.19310001   0.45690003   0.1258       0.42989999
    2.55100131]][0m
[37m[1m[2023-07-11 05:43:24,822][233954] Max Reward on eval: 242.71672390028834[0m
[37m[1m[2023-07-11 05:43:24,822][233954] Min Reward on eval: -268.3666728478391[0m
[37m[1m[2023-07-11 05:43:24,822][233954] Mean Reward across all agents: -2.383085076860894[0m
[37m[1m[2023-07-11 05:43:24,823][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:43:24,825][233954] mean_value=-145.53204944453597, max_value=335.59351057454637[0m
[37m[1m[2023-07-11 05:43:24,828][233954] New mean coefficients: [[  3.1023204  -7.0054746  -3.5684652  -5.593875  -12.711853  -12.893965 ]][0m
[37m[1m[2023-07-11 05:43:24,829][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:43:33,744][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 05:43:33,744][233954] FPS: 430796.56[0m
[36m[2023-07-11 05:43:33,747][233954] itr=436, itrs=2000, Progress: 21.80%[0m
[36m[2023-07-11 05:43:45,592][233954] train() took 11.76 seconds to complete[0m
[36m[2023-07-11 05:43:45,592][233954] FPS: 326515.21[0m
[36m[2023-07-11 05:43:49,836][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:43:49,836][233954] Reward + Measures: [[-58.66601528   0.12422634   0.344818     0.11204334   0.24172334
    2.36245084]][0m
[37m[1m[2023-07-11 05:43:49,836][233954] Max Reward on eval: -58.666015277267356[0m
[37m[1m[2023-07-11 05:43:49,837][233954] Min Reward on eval: -58.666015277267356[0m
[37m[1m[2023-07-11 05:43:49,837][233954] Mean Reward across all agents: -58.666015277267356[0m
[37m[1m[2023-07-11 05:43:49,837][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:43:55,006][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:43:55,012][233954] Reward + Measures: [[ 13.0594401    0.1763       0.5966       0.18499999   0.44449997
    2.37327266]
 [-62.45157263   0.16830002   0.52680004   0.13770001   0.36490002
    2.46626711]
 [ -3.72792827   0.1337       0.48559999   0.1805       0.38240001
    2.52915716]
 ...
 [ 46.9508589    0.1974       0.64440006   0.20940001   0.50559998
    2.41610026]
 [-13.77635303   0.15440001   0.61230004   0.16870001   0.47020003
    2.43477511]
 [ 26.11067137   0.15659998   0.48880002   0.14600001   0.35340002
    2.33851266]][0m
[37m[1m[2023-07-11 05:43:55,012][233954] Max Reward on eval: 262.34481479115783[0m
[37m[1m[2023-07-11 05:43:55,013][233954] Min Reward on eval: -106.01654330901802[0m
[37m[1m[2023-07-11 05:43:55,013][233954] Mean Reward across all agents: 0.7894488017232891[0m
[37m[1m[2023-07-11 05:43:55,013][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:43:55,015][233954] mean_value=-106.92152469014273, max_value=542.1973808320239[0m
[37m[1m[2023-07-11 05:43:55,018][233954] New mean coefficients: [[  3.181504   -6.681184   -3.7208815  -6.142722  -12.210757  -12.300845 ]][0m
[37m[1m[2023-07-11 05:43:55,019][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:44:03,945][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 05:44:03,946][233954] FPS: 430258.15[0m
[36m[2023-07-11 05:44:03,948][233954] itr=437, itrs=2000, Progress: 21.85%[0m
[36m[2023-07-11 05:44:15,605][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 05:44:15,605][233954] FPS: 331727.89[0m
[36m[2023-07-11 05:44:19,863][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:44:19,863][233954] Reward + Measures: [[-49.92565177   0.11554799   0.31375799   0.104964     0.22450966
    2.31466079]][0m
[37m[1m[2023-07-11 05:44:19,863][233954] Max Reward on eval: -49.925651770710985[0m
[37m[1m[2023-07-11 05:44:19,864][233954] Min Reward on eval: -49.925651770710985[0m
[37m[1m[2023-07-11 05:44:19,864][233954] Mean Reward across all agents: -49.925651770710985[0m
[37m[1m[2023-07-11 05:44:19,864][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:44:24,844][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:44:24,845][233954] Reward + Measures: [[ 51.05696796   0.21689999   0.5151       0.19930001   0.43829998
    2.55534983]
 [ -5.32695281   0.11080001   0.30940002   0.11059999   0.2168
    2.29418826]
 [109.6011183    0.31599998   0.75490004   0.31830001   0.61920005
    2.72214842]
 ...
 [-31.08311837   0.13         0.43579999   0.1275       0.29140002
    2.49842238]
 [102.25862621   0.18910001   0.47709998   0.15720001   0.35450003
    2.38856888]
 [ 37.97110426   0.15009999   0.37080002   0.11550001   0.25430003
    2.32205009]][0m
[37m[1m[2023-07-11 05:44:24,845][233954] Max Reward on eval: 218.9717446638737[0m
[37m[1m[2023-07-11 05:44:24,845][233954] Min Reward on eval: -47.863974758516996[0m
[37m[1m[2023-07-11 05:44:24,846][233954] Mean Reward across all agents: 43.11536018905133[0m
[37m[1m[2023-07-11 05:44:24,846][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:44:24,848][233954] mean_value=-269.9099247812358, max_value=500.657712172335[0m
[37m[1m[2023-07-11 05:44:24,850][233954] New mean coefficients: [[  2.6161087  -6.655032   -5.2623734  -6.637007  -12.013991  -12.186738 ]][0m
[37m[1m[2023-07-11 05:44:24,851][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:44:33,825][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 05:44:33,825][233954] FPS: 427999.88[0m
[36m[2023-07-11 05:44:33,827][233954] itr=438, itrs=2000, Progress: 21.90%[0m
[36m[2023-07-11 05:44:45,578][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 05:44:45,578][233954] FPS: 329169.58[0m
[36m[2023-07-11 05:44:49,873][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:44:49,873][233954] Reward + Measures: [[-45.19538007   0.110744     0.28471431   0.10406633   0.21159466
    2.30388665]][0m
[37m[1m[2023-07-11 05:44:49,873][233954] Max Reward on eval: -45.19538007243918[0m
[37m[1m[2023-07-11 05:44:49,874][233954] Min Reward on eval: -45.19538007243918[0m
[37m[1m[2023-07-11 05:44:49,874][233954] Mean Reward across all agents: -45.19538007243918[0m
[37m[1m[2023-07-11 05:44:49,874][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:44:54,896][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:44:54,902][233954] Reward + Measures: [[588.9662242    0.0148       0.97119999   0.68359995   0.93260002
    2.9353931 ]
 [225.31489222   0.0634       0.89209998   0.3538       0.76440006
    2.72855926]
 [709.12697218   0.0049       0.98800004   0.7281       0.9752
    3.08378386]
 ...
 [170.60443975   0.034        0.95510006   0.34290001   0.86090004
    2.84753394]
 [659.47942352   0.0033       0.9939       0.7511       0.9877001
    2.96470928]
 [113.19912823   0.0781       0.77950001   0.27290002   0.61729997
    2.71376109]][0m
[37m[1m[2023-07-11 05:44:54,902][233954] Max Reward on eval: 783.124725358095[0m
[37m[1m[2023-07-11 05:44:54,903][233954] Min Reward on eval: -80.78839241908864[0m
[37m[1m[2023-07-11 05:44:54,903][233954] Mean Reward across all agents: 330.12981685570463[0m
[37m[1m[2023-07-11 05:44:54,903][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:44:54,907][233954] mean_value=-45.03776259765021, max_value=616.9650382823311[0m
[37m[1m[2023-07-11 05:44:54,909][233954] New mean coefficients: [[  2.7484717  -6.0096054  -5.3985467  -5.9367166 -11.282204  -11.271855 ]][0m
[37m[1m[2023-07-11 05:44:54,910][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:45:03,924][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 05:45:03,925][233954] FPS: 426080.51[0m
[36m[2023-07-11 05:45:03,927][233954] itr=439, itrs=2000, Progress: 21.95%[0m
[36m[2023-07-11 05:45:15,652][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 05:45:15,652][233954] FPS: 329832.92[0m
[36m[2023-07-11 05:45:19,893][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:45:19,893][233954] Reward + Measures: [[-42.12528991   0.10811768   0.26748198   0.101835     0.20221834
    2.28334951]][0m
[37m[1m[2023-07-11 05:45:19,894][233954] Max Reward on eval: -42.12528991146163[0m
[37m[1m[2023-07-11 05:45:19,894][233954] Min Reward on eval: -42.12528991146163[0m
[37m[1m[2023-07-11 05:45:19,894][233954] Mean Reward across all agents: -42.12528991146163[0m
[37m[1m[2023-07-11 05:45:19,894][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:45:24,892][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:45:24,893][233954] Reward + Measures: [[ 84.23373975   0.083        0.78950006   0.4488       0.7942
    3.62485433]
 [-88.40632126   0.0121       0.97060007   0.90179998   0.98369998
    3.82359099]
 [152.42012883   0.0016       0.98680001   0.96180004   0.99490005
    3.88338733]
 ...
 [ 70.27867341   0.0056       0.98120004   0.93129998   0.98979998
    3.8715992 ]
 [ 39.42966804   0.09029999   0.81410009   0.39890003   0.79940003
    3.63531685]
 [ 38.81373357   0.0152       0.95290005   0.87070006   0.96569997
    3.86041641]][0m
[37m[1m[2023-07-11 05:45:24,893][233954] Max Reward on eval: 274.3247417178005[0m
[37m[1m[2023-07-11 05:45:24,894][233954] Min Reward on eval: -88.40632125586271[0m
[37m[1m[2023-07-11 05:45:24,894][233954] Mean Reward across all agents: 97.64550462810689[0m
[37m[1m[2023-07-11 05:45:24,894][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:45:24,896][233954] mean_value=-239.71673807862805, max_value=407.7943033593578[0m
[37m[1m[2023-07-11 05:45:24,899][233954] New mean coefficients: [[ 2.8036942 -4.5179734 -5.004326  -5.28059   -9.357341  -9.875984 ]][0m
[37m[1m[2023-07-11 05:45:24,900][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:45:33,904][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 05:45:33,904][233954] FPS: 426564.39[0m
[36m[2023-07-11 05:45:33,906][233954] itr=440, itrs=2000, Progress: 22.00%[0m
[37m[1m[2023-07-11 05:48:54,122][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000420[0m
[36m[2023-07-11 05:49:06,379][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 05:49:06,379][233954] FPS: 330894.74[0m
[36m[2023-07-11 05:49:10,565][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:49:10,565][233954] Reward + Measures: [[-42.01179815   0.10381401   0.25421301   0.098887     0.19158132
    2.26897502]][0m
[37m[1m[2023-07-11 05:49:10,565][233954] Max Reward on eval: -42.01179815075496[0m
[37m[1m[2023-07-11 05:49:10,566][233954] Min Reward on eval: -42.01179815075496[0m
[37m[1m[2023-07-11 05:49:10,566][233954] Mean Reward across all agents: -42.01179815075496[0m
[37m[1m[2023-07-11 05:49:10,566][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:49:15,454][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:49:15,455][233954] Reward + Measures: [[115.12281039   0.164        0.52220005   0.22849999   0.49340001
    2.36764216]
 [ 23.71562542   0.1331       0.2304       0.1286       0.28220001
    2.44863439]
 [ 27.32627926   0.14390002   0.2273       0.12630001   0.24319999
    2.32480931]
 ...
 [ 87.63679237   0.0927       0.49090001   0.18480001   0.47119999
    2.42140126]
 [ 60.61307739   0.12899999   0.3409       0.15820001   0.36840001
    2.43200994]
 [ 29.83058589   0.1693       0.4165       0.18279999   0.40889999
    2.35801315]][0m
[37m[1m[2023-07-11 05:49:15,455][233954] Max Reward on eval: 178.47773955899757[0m
[37m[1m[2023-07-11 05:49:15,455][233954] Min Reward on eval: -104.49370527174324[0m
[37m[1m[2023-07-11 05:49:15,455][233954] Mean Reward across all agents: 55.94974021821606[0m
[37m[1m[2023-07-11 05:49:15,456][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:49:15,458][233954] mean_value=-178.15083547360797, max_value=409.8002408109775[0m
[37m[1m[2023-07-11 05:49:15,460][233954] New mean coefficients: [[  3.859035   -4.939354   -2.698475   -5.589846  -10.0022545 -10.232719 ]][0m
[37m[1m[2023-07-11 05:49:15,461][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:49:24,362][233954] train() took 8.90 seconds to complete[0m
[36m[2023-07-11 05:49:24,362][233954] FPS: 431483.63[0m
[36m[2023-07-11 05:49:24,364][233954] itr=441, itrs=2000, Progress: 22.05%[0m
[36m[2023-07-11 05:49:36,007][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 05:49:36,007][233954] FPS: 332145.65[0m
[36m[2023-07-11 05:49:40,318][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:49:40,318][233954] Reward + Measures: [[-39.12012769   0.10547666   0.25039867   0.09809867   0.18757133
    2.24008226]][0m
[37m[1m[2023-07-11 05:49:40,318][233954] Max Reward on eval: -39.12012768759392[0m
[37m[1m[2023-07-11 05:49:40,319][233954] Min Reward on eval: -39.12012768759392[0m
[37m[1m[2023-07-11 05:49:40,319][233954] Mean Reward across all agents: -39.12012768759392[0m
[37m[1m[2023-07-11 05:49:40,319][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:49:45,379][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:49:45,380][233954] Reward + Measures: [[466.71130181   0.0235       0.92970002   0.87290001   0.94200003
    3.81729507]
 [148.29587362   0.0074       0.99050009   0.53280002   0.97749996
    3.30004048]
 [ 20.04874823   0.12630001   0.7978       0.28670001   0.77699995
    2.93975925]
 ...
 [ 92.71543557   0.0392       0.97720003   0.33800003   0.95510006
    3.11792994]
 [129.41154273   0.14389999   0.68260002   0.40549999   0.6863001
    3.17377806]
 [104.38846157   0.0234       0.98519993   0.44140002   0.95570004
    3.01185083]][0m
[37m[1m[2023-07-11 05:49:45,380][233954] Max Reward on eval: 512.8043651698157[0m
[37m[1m[2023-07-11 05:49:45,380][233954] Min Reward on eval: -88.347031040024[0m
[37m[1m[2023-07-11 05:49:45,381][233954] Mean Reward across all agents: 145.4073771195813[0m
[37m[1m[2023-07-11 05:49:45,381][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:49:45,383][233954] mean_value=-141.89488626384818, max_value=413.3128726256918[0m
[37m[1m[2023-07-11 05:49:45,386][233954] New mean coefficients: [[  3.789331  -5.964903  -2.950293  -6.741399 -11.116865 -11.176966]][0m
[37m[1m[2023-07-11 05:49:45,387][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:49:54,376][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 05:49:54,376][233954] FPS: 427259.88[0m
[36m[2023-07-11 05:49:54,379][233954] itr=442, itrs=2000, Progress: 22.10%[0m
[36m[2023-07-11 05:50:05,973][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 05:50:05,973][233954] FPS: 333499.32[0m
[36m[2023-07-11 05:50:10,233][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:50:10,234][233954] Reward + Measures: [[-35.01871066   0.10058633   0.229426     0.096828     0.18238899
    2.22431087]][0m
[37m[1m[2023-07-11 05:50:10,234][233954] Max Reward on eval: -35.018710656451944[0m
[37m[1m[2023-07-11 05:50:10,234][233954] Min Reward on eval: -35.018710656451944[0m
[37m[1m[2023-07-11 05:50:10,234][233954] Mean Reward across all agents: -35.018710656451944[0m
[37m[1m[2023-07-11 05:50:10,235][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:50:15,214][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:50:15,215][233954] Reward + Measures: [[  18.99558129    0.32090002    0.48809996    0.29159999    0.39760002
     2.62884116]
 [-129.16304538    0.28830001    0.73379999    0.15970001    0.50190002
     2.6417017 ]
 [ -46.64529232    0.27340001    0.54259998    0.21180001    0.39669999
     2.58624721]
 ...
 [ -58.35297271    0.2289        0.55140001    0.18240002    0.43460003
     2.59357715]
 [  11.50045942    0.2687        0.54680002    0.2           0.44989997
     2.55306029]
 [ -79.30376291    0.23150001    0.4876        0.15979999    0.4082
     2.54563451]][0m
[37m[1m[2023-07-11 05:50:15,215][233954] Max Reward on eval: 306.9627331668511[0m
[37m[1m[2023-07-11 05:50:15,215][233954] Min Reward on eval: -272.97055327165873[0m
[37m[1m[2023-07-11 05:50:15,215][233954] Mean Reward across all agents: -12.069196013383207[0m
[37m[1m[2023-07-11 05:50:15,216][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:50:15,218][233954] mean_value=-156.34109697667722, max_value=276.11819882117703[0m
[37m[1m[2023-07-11 05:50:15,220][233954] New mean coefficients: [[  3.3507147  -6.322028   -4.425703   -7.5760417 -11.54691   -11.217218 ]][0m
[37m[1m[2023-07-11 05:50:15,221][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:50:24,245][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 05:50:24,246][233954] FPS: 425590.83[0m
[36m[2023-07-11 05:50:24,248][233954] itr=443, itrs=2000, Progress: 22.15%[0m
[36m[2023-07-11 05:50:35,968][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 05:50:35,968][233954] FPS: 329904.99[0m
[36m[2023-07-11 05:50:40,284][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:50:40,285][233954] Reward + Measures: [[-33.82772598   0.09974968   0.22859967   0.09539367   0.17738765
    2.20836186]][0m
[37m[1m[2023-07-11 05:50:40,285][233954] Max Reward on eval: -33.82772598180526[0m
[37m[1m[2023-07-11 05:50:40,285][233954] Min Reward on eval: -33.82772598180526[0m
[37m[1m[2023-07-11 05:50:40,286][233954] Mean Reward across all agents: -33.82772598180526[0m
[37m[1m[2023-07-11 05:50:40,286][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:50:45,454][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:50:45,454][233954] Reward + Measures: [[242.51098802   0.1611       0.91909999   0.43979999   0.86159992
    3.05748558]
 [ 61.87090729   0.32370001   0.87750006   0.36930001   0.84650004
    3.03215432]
 [ 22.54694924   0.4632       0.8028       0.4499       0.74370003
    2.6794076 ]
 ...
 [-43.70356592   0.60269994   0.77879995   0.51010001   0.78750002
    2.89845419]
 [ 29.09646389   0.53770006   0.8344       0.52179998   0.80310005
    2.75686526]
 [ 79.22458459   0.373        0.85589999   0.41370001   0.79420006
    2.86927533]][0m
[37m[1m[2023-07-11 05:50:45,455][233954] Max Reward on eval: 461.8939433044987[0m
[37m[1m[2023-07-11 05:50:45,455][233954] Min Reward on eval: -90.18688014671207[0m
[37m[1m[2023-07-11 05:50:45,455][233954] Mean Reward across all agents: 70.25690780731495[0m
[37m[1m[2023-07-11 05:50:45,455][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:50:45,458][233954] mean_value=-69.88628151896035, max_value=402.822043093713[0m
[37m[1m[2023-07-11 05:50:45,461][233954] New mean coefficients: [[  4.3066616  -6.0559063  -2.926483   -6.7208033 -11.664608  -10.713413 ]][0m
[37m[1m[2023-07-11 05:50:45,462][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:50:54,475][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 05:50:54,475][233954] FPS: 426122.54[0m
[36m[2023-07-11 05:50:54,477][233954] itr=444, itrs=2000, Progress: 22.20%[0m
[36m[2023-07-11 05:51:06,096][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 05:51:06,096][233954] FPS: 332768.94[0m
[36m[2023-07-11 05:51:10,373][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:51:10,373][233954] Reward + Measures: [[-32.62845499   0.10016166   0.22698301   0.09449801   0.17608432
    2.19732761]][0m
[37m[1m[2023-07-11 05:51:10,374][233954] Max Reward on eval: -32.62845499302533[0m
[37m[1m[2023-07-11 05:51:10,374][233954] Min Reward on eval: -32.62845499302533[0m
[37m[1m[2023-07-11 05:51:10,374][233954] Mean Reward across all agents: -32.62845499302533[0m
[37m[1m[2023-07-11 05:51:10,374][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:51:15,371][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:51:15,371][233954] Reward + Measures: [[-27.25217876   0.1435       0.38940001   0.1256       0.24790001
    2.35110164]
 [ -8.93546214   0.13309999   0.345        0.11440001   0.23989999
    2.4921658 ]
 [-16.59536284   0.19620001   0.45480004   0.1543       0.36100003
    2.48268056]
 ...
 [-10.13836419   0.13060001   0.32950002   0.1048       0.2246
    2.4065907 ]
 [ 23.86475627   0.10829999   0.1885       0.10270001   0.14060001
    2.44609189]
 [-75.84974449   0.1851       0.50450003   0.1813       0.38140002
    2.47959185]][0m
[37m[1m[2023-07-11 05:51:15,371][233954] Max Reward on eval: 101.02400989569723[0m
[37m[1m[2023-07-11 05:51:15,372][233954] Min Reward on eval: -90.31531077865512[0m
[37m[1m[2023-07-11 05:51:15,372][233954] Mean Reward across all agents: -2.207765645051424[0m
[37m[1m[2023-07-11 05:51:15,372][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:51:15,374][233954] mean_value=-393.0996280079957, max_value=352.0416031374686[0m
[37m[1m[2023-07-11 05:51:15,376][233954] New mean coefficients: [[  5.525094    -5.39827     -0.21493697  -5.9021564  -10.735851
   -9.595701  ]][0m
[37m[1m[2023-07-11 05:51:15,377][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:51:24,280][233954] train() took 8.90 seconds to complete[0m
[36m[2023-07-11 05:51:24,280][233954] FPS: 431395.51[0m
[36m[2023-07-11 05:51:24,283][233954] itr=445, itrs=2000, Progress: 22.25%[0m
[36m[2023-07-11 05:51:35,950][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 05:51:35,950][233954] FPS: 331528.71[0m
[36m[2023-07-11 05:51:40,145][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:51:40,146][233954] Reward + Measures: [[-27.98167753   0.09386601   0.20999034   0.09047833   0.16502367
    2.16787291]][0m
[37m[1m[2023-07-11 05:51:40,146][233954] Max Reward on eval: -27.981677530091392[0m
[37m[1m[2023-07-11 05:51:40,146][233954] Min Reward on eval: -27.981677530091392[0m
[37m[1m[2023-07-11 05:51:40,147][233954] Mean Reward across all agents: -27.981677530091392[0m
[37m[1m[2023-07-11 05:51:40,147][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:51:45,151][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:51:45,152][233954] Reward + Measures: [[-21.71922866   0.17110001   0.77280009   0.1698       0.58090001
    2.62836385]
 [ 47.39677264   0.10580001   0.79360002   0.16340001   0.56160003
    2.67750573]
 [139.24632837   0.11009999   0.81000006   0.1655       0.60249996
    2.65387511]
 ...
 [ 67.97038995   0.14350002   0.7525       0.19679999   0.56810004
    2.58609748]
 [166.92140557   0.10220001   0.85769999   0.30580002   0.67770004
    2.72501206]
 [289.63344669   0.0566       0.91640007   0.41640002   0.80370009
    2.85051417]][0m
[37m[1m[2023-07-11 05:51:45,152][233954] Max Reward on eval: 632.9589596001431[0m
[37m[1m[2023-07-11 05:51:45,152][233954] Min Reward on eval: -181.4512932526879[0m
[37m[1m[2023-07-11 05:51:45,152][233954] Mean Reward across all agents: 109.93111482734206[0m
[37m[1m[2023-07-11 05:51:45,153][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:51:45,156][233954] mean_value=-30.214606013369927, max_value=451.79772794375464[0m
[37m[1m[2023-07-11 05:51:45,158][233954] New mean coefficients: [[  7.872901   -4.8299494   4.524974   -4.826214  -10.189272   -8.436327 ]][0m
[37m[1m[2023-07-11 05:51:45,159][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:51:54,214][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 05:51:54,214][233954] FPS: 424181.31[0m
[36m[2023-07-11 05:51:54,216][233954] itr=446, itrs=2000, Progress: 22.30%[0m
[36m[2023-07-11 05:52:06,078][233954] train() took 11.78 seconds to complete[0m
[36m[2023-07-11 05:52:06,079][233954] FPS: 325910.82[0m
[36m[2023-07-11 05:52:10,354][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:52:10,354][233954] Reward + Measures: [[-24.25652562   0.092816     0.20640866   0.092559     0.16241834
    2.16189265]][0m
[37m[1m[2023-07-11 05:52:10,354][233954] Max Reward on eval: -24.25652561836201[0m
[37m[1m[2023-07-11 05:52:10,355][233954] Min Reward on eval: -24.25652561836201[0m
[37m[1m[2023-07-11 05:52:10,355][233954] Mean Reward across all agents: -24.25652561836201[0m
[37m[1m[2023-07-11 05:52:10,355][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:52:15,282][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:52:15,283][233954] Reward + Measures: [[-71.39334297   0.1353       0.41669998   0.12400001   0.2665
    2.45298576]
 [ 85.23591363   0.23940001   0.36399999   0.23870002   0.31370002
    2.34472919]
 [ 10.13097044   0.2393       0.4844       0.22189999   0.3955
    2.34767532]
 ...
 [ 49.87997966   0.24879999   0.3319       0.2045       0.29730001
    2.32951665]
 [ 92.81475957   0.32210001   0.47659999   0.29499999   0.43470001
    2.35021901]
 [  4.23376178   0.20299999   0.43810001   0.2007       0.33680001
    2.3452518 ]][0m
[37m[1m[2023-07-11 05:52:15,283][233954] Max Reward on eval: 146.85128449415788[0m
[37m[1m[2023-07-11 05:52:15,283][233954] Min Reward on eval: -90.19813090693205[0m
[37m[1m[2023-07-11 05:52:15,284][233954] Mean Reward across all agents: 16.43770844475093[0m
[37m[1m[2023-07-11 05:52:15,284][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:52:15,285][233954] mean_value=-250.35308475375544, max_value=297.9574991415624[0m
[37m[1m[2023-07-11 05:52:15,288][233954] New mean coefficients: [[  8.002254   -5.828607    4.7618494  -5.0234356 -11.53467    -9.947878 ]][0m
[37m[1m[2023-07-11 05:52:15,289][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:52:24,214][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 05:52:24,214][233954] FPS: 430323.87[0m
[36m[2023-07-11 05:52:24,216][233954] itr=447, itrs=2000, Progress: 22.35%[0m
[36m[2023-07-11 05:52:35,890][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 05:52:35,890][233954] FPS: 331199.85[0m
[36m[2023-07-11 05:52:40,212][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:52:40,212][233954] Reward + Measures: [[-19.34437509   0.09000266   0.19288434   0.08887      0.15009733
    2.13578248]][0m
[37m[1m[2023-07-11 05:52:40,213][233954] Max Reward on eval: -19.34437508949133[0m
[37m[1m[2023-07-11 05:52:40,213][233954] Min Reward on eval: -19.34437508949133[0m
[37m[1m[2023-07-11 05:52:40,213][233954] Mean Reward across all agents: -19.34437508949133[0m
[37m[1m[2023-07-11 05:52:40,213][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:52:45,207][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:52:45,208][233954] Reward + Measures: [[ 34.97936135   0.2017       0.54100001   0.1893       0.41750002
    2.41444898]
 [ -9.79534218   0.1741       0.67740005   0.19220002   0.53290004
    2.67483902]
 [ 34.02725474   0.20729999   0.50749999   0.1591       0.42580006
    2.60058904]
 ...
 [-41.09981259   0.14580001   0.39980003   0.1265       0.31709999
    2.42304683]
 [ 15.21113622   0.24260001   0.53890002   0.18779999   0.44650003
    2.44631457]
 [-23.74268364   0.13079999   0.59530002   0.1426       0.44379997
    2.59365749]][0m
[37m[1m[2023-07-11 05:52:45,208][233954] Max Reward on eval: 181.35887958034874[0m
[37m[1m[2023-07-11 05:52:45,208][233954] Min Reward on eval: -157.26244301991537[0m
[37m[1m[2023-07-11 05:52:45,209][233954] Mean Reward across all agents: 18.508618067989616[0m
[37m[1m[2023-07-11 05:52:45,209][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:52:45,211][233954] mean_value=-164.54960774516547, max_value=289.03339964148256[0m
[37m[1m[2023-07-11 05:52:45,213][233954] New mean coefficients: [[  8.352391   -5.8977537   6.0114355  -5.552613  -11.353404   -9.720007 ]][0m
[37m[1m[2023-07-11 05:52:45,214][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:52:54,256][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 05:52:54,257][233954] FPS: 424758.40[0m
[36m[2023-07-11 05:52:54,259][233954] itr=448, itrs=2000, Progress: 22.40%[0m
[36m[2023-07-11 05:53:05,955][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 05:53:05,956][233954] FPS: 330546.26[0m
[36m[2023-07-11 05:53:10,266][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:53:10,266][233954] Reward + Measures: [[-14.95470543   0.08473333   0.17774066   0.087512     0.14440399
    2.11282969]][0m
[37m[1m[2023-07-11 05:53:10,266][233954] Max Reward on eval: -14.954705433008359[0m
[37m[1m[2023-07-11 05:53:10,266][233954] Min Reward on eval: -14.954705433008359[0m
[37m[1m[2023-07-11 05:53:10,267][233954] Mean Reward across all agents: -14.954705433008359[0m
[37m[1m[2023-07-11 05:53:10,267][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:53:15,304][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:53:15,305][233954] Reward + Measures: [[ 56.90708495   0.1437       0.61040002   0.25300002   0.50929999
    2.60943794]
 [-72.19629069   0.15619999   0.61399996   0.15120001   0.44280002
    2.60061097]
 [ 31.78302997   0.1139       0.23210001   0.12100001   0.20650001
    2.51923418]
 ...
 [-18.81505094   0.0807       0.24229999   0.11110001   0.21610001
    2.464046  ]
 [176.28661445   0.1049       0.65499997   0.30899999   0.55489999
    2.6976552 ]
 [-75.2857137    0.14659999   0.53899997   0.1578       0.39719999
    2.55771375]][0m
[37m[1m[2023-07-11 05:53:15,305][233954] Max Reward on eval: 538.3376674726605[0m
[37m[1m[2023-07-11 05:53:15,306][233954] Min Reward on eval: -145.56314419405535[0m
[37m[1m[2023-07-11 05:53:15,306][233954] Mean Reward across all agents: 32.73479567954421[0m
[37m[1m[2023-07-11 05:53:15,306][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:53:15,308][233954] mean_value=-169.15302777905322, max_value=98.95570565245595[0m
[37m[1m[2023-07-11 05:53:15,311][233954] New mean coefficients: [[  6.9356413  -7.2901917   3.5240526  -7.4759073 -13.082352  -11.228522 ]][0m
[37m[1m[2023-07-11 05:53:15,312][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:53:24,337][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 05:53:24,337][233954] FPS: 425548.97[0m
[36m[2023-07-11 05:53:24,339][233954] itr=449, itrs=2000, Progress: 22.45%[0m
[36m[2023-07-11 05:53:36,005][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 05:53:36,006][233954] FPS: 331441.69[0m
[36m[2023-07-11 05:53:40,233][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:53:40,233][233954] Reward + Measures: [[-13.01952133   0.08474333   0.17502534   0.08868968   0.14464401
    2.0943799 ]][0m
[37m[1m[2023-07-11 05:53:40,234][233954] Max Reward on eval: -13.019521330339387[0m
[37m[1m[2023-07-11 05:53:40,234][233954] Min Reward on eval: -13.019521330339387[0m
[37m[1m[2023-07-11 05:53:40,234][233954] Mean Reward across all agents: -13.019521330339387[0m
[37m[1m[2023-07-11 05:53:40,234][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:53:45,383][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:53:45,383][233954] Reward + Measures: [[16.41748701  0.20630001  0.36110002  0.19509999  0.3177      2.37099242]
 [71.53080102  0.22420001  0.59190005  0.205       0.49939999  2.46680975]
 [87.10985251  0.26010001  0.45000002  0.25009999  0.42660004  2.40759897]
 ...
 [50.96049118  0.1955      0.63510001  0.17999999  0.49349999  2.46889567]
 [98.93331042  0.2342      0.3547      0.25330001  0.3908      2.29824257]
 [ 5.88387656  0.18099999  0.3434      0.1689      0.3021      2.38571811]][0m
[37m[1m[2023-07-11 05:53:45,383][233954] Max Reward on eval: 147.9868011098355[0m
[37m[1m[2023-07-11 05:53:45,384][233954] Min Reward on eval: -77.15109759508633[0m
[37m[1m[2023-07-11 05:53:45,384][233954] Mean Reward across all agents: 46.09659606339491[0m
[37m[1m[2023-07-11 05:53:45,384][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:53:45,386][233954] mean_value=-218.73854657224885, max_value=119.57053995968154[0m
[37m[1m[2023-07-11 05:53:45,388][233954] New mean coefficients: [[  5.1650743   -6.8593597   -0.22344685  -8.245793   -12.630431
  -11.737889  ]][0m
[37m[1m[2023-07-11 05:53:45,389][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:53:54,378][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 05:53:54,378][233954] FPS: 427268.09[0m
[36m[2023-07-11 05:53:54,381][233954] itr=450, itrs=2000, Progress: 22.50%[0m
[37m[1m[2023-07-11 05:57:19,321][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000430[0m
[36m[2023-07-11 05:57:31,347][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 05:57:31,348][233954] FPS: 334103.47[0m
[36m[2023-07-11 05:57:35,479][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:57:35,479][233954] Reward + Measures: [[-13.26881177   0.082126     0.17188001   0.08523034   0.13844199
    2.06195092]][0m
[37m[1m[2023-07-11 05:57:35,479][233954] Max Reward on eval: -13.26881177484184[0m
[37m[1m[2023-07-11 05:57:35,479][233954] Min Reward on eval: -13.26881177484184[0m
[37m[1m[2023-07-11 05:57:35,480][233954] Mean Reward across all agents: -13.26881177484184[0m
[37m[1m[2023-07-11 05:57:35,480][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:57:40,621][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:57:40,622][233954] Reward + Measures: [[ 92.56929801   0.1037       0.57880002   0.30669999   0.61760002
    3.51878047]
 [ 75.38671439   0.1267       0.47460005   0.24059999   0.46820003
    3.46020484]
 [ 40.82811863   0.07520001   0.44720003   0.2089       0.42389998
    3.35617042]
 ...
 [ 80.37405593   0.0587       0.73619998   0.53940004   0.77939999
    3.674896  ]
 [ 88.72299316   0.09230001   0.58000004   0.28510001   0.62720001
    3.50859308]
 [188.7518511    0.0908       0.71950001   0.37760001   0.71060002
    3.56645131]][0m
[37m[1m[2023-07-11 05:57:40,622][233954] Max Reward on eval: 332.3295478546992[0m
[37m[1m[2023-07-11 05:57:40,623][233954] Min Reward on eval: 25.951510153897107[0m
[37m[1m[2023-07-11 05:57:40,623][233954] Mean Reward across all agents: 106.8431680864044[0m
[37m[1m[2023-07-11 05:57:40,623][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:57:40,628][233954] mean_value=20.45873119096451, max_value=422.73723607646747[0m
[37m[1m[2023-07-11 05:57:40,630][233954] New mean coefficients: [[  5.9932613  -5.842955    2.2275696  -6.6569004 -11.755965  -10.84582  ]][0m
[37m[1m[2023-07-11 05:57:40,631][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:57:49,557][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 05:57:49,558][233954] FPS: 430285.22[0m
[36m[2023-07-11 05:57:49,560][233954] itr=451, itrs=2000, Progress: 22.55%[0m
[36m[2023-07-11 05:58:01,216][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 05:58:01,216][233954] FPS: 331829.55[0m
[36m[2023-07-11 05:58:05,486][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:58:05,487][233954] Reward + Measures: [[-10.5147738    0.08012599   0.16354099   0.08448099   0.135913
    2.04565239]][0m
[37m[1m[2023-07-11 05:58:05,487][233954] Max Reward on eval: -10.514773803182916[0m
[37m[1m[2023-07-11 05:58:05,487][233954] Min Reward on eval: -10.514773803182916[0m
[37m[1m[2023-07-11 05:58:05,487][233954] Mean Reward across all agents: -10.514773803182916[0m
[37m[1m[2023-07-11 05:58:05,488][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:58:10,479][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:58:10,480][233954] Reward + Measures: [[-17.94852412   0.1479       0.2422       0.17279999   0.2465
    2.80364656]
 [-46.50548146   0.13130002   0.73100007   0.25400001   0.72339994
    2.92393398]
 [-22.99925745   0.17639999   0.40879998   0.2033       0.40959999
    2.80013132]
 ...
 [-11.08835169   0.09         0.8229       0.31430003   0.81770003
    3.07935309]
 [-41.25476014   0.11830001   0.67850006   0.27330002   0.67009997
    2.88120699]
 [-21.15012438   0.114        0.68089998   0.245        0.66609997
    2.99251986]][0m
[37m[1m[2023-07-11 05:58:10,480][233954] Max Reward on eval: 267.88711834959685[0m
[37m[1m[2023-07-11 05:58:10,480][233954] Min Reward on eval: -107.43741992199793[0m
[37m[1m[2023-07-11 05:58:10,481][233954] Mean Reward across all agents: 1.1335765837483727[0m
[37m[1m[2023-07-11 05:58:10,481][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:58:10,483][233954] mean_value=-180.01212031178787, max_value=537.492480599694[0m
[37m[1m[2023-07-11 05:58:10,485][233954] New mean coefficients: [[  7.7876883  -5.3573995   6.175462   -4.7104735 -11.3741865  -9.5644455]][0m
[37m[1m[2023-07-11 05:58:10,486][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:58:19,428][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 05:58:19,433][233954] FPS: 429511.92[0m
[36m[2023-07-11 05:58:19,440][233954] itr=452, itrs=2000, Progress: 22.60%[0m
[36m[2023-07-11 05:58:31,113][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 05:58:31,113][233954] FPS: 331266.89[0m
[36m[2023-07-11 05:58:35,360][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:58:35,361][233954] Reward + Measures: [[-10.32085125   0.08008133   0.16660333   0.085441     0.136425
    2.04037023]][0m
[37m[1m[2023-07-11 05:58:35,361][233954] Max Reward on eval: -10.320851253245085[0m
[37m[1m[2023-07-11 05:58:35,361][233954] Min Reward on eval: -10.320851253245085[0m
[37m[1m[2023-07-11 05:58:35,362][233954] Mean Reward across all agents: -10.320851253245085[0m
[37m[1m[2023-07-11 05:58:35,362][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:58:40,297][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:58:40,297][233954] Reward + Measures: [[ 57.93104672   0.28260002   0.39590001   0.26100001   0.33960003
    2.59435129]
 [-29.24660033   0.0789       0.2264       0.0961       0.17300001
    2.36063457]
 [ 12.25019978   0.20149998   0.19030002   0.178        0.1701
    2.7651639 ]
 ...
 [ 94.71414247   0.20900002   0.2148       0.19940001   0.17300001
    2.67642021]
 [-40.8494912    0.12820001   0.18550001   0.16399999   0.21630001
    2.59519458]
 [ 47.67718495   0.22950001   0.2588       0.1807       0.24879999
    2.94672823]][0m
[37m[1m[2023-07-11 05:58:40,297][233954] Max Reward on eval: 137.3255046869628[0m
[37m[1m[2023-07-11 05:58:40,298][233954] Min Reward on eval: -110.55890609258786[0m
[37m[1m[2023-07-11 05:58:40,298][233954] Mean Reward across all agents: 20.236024874483657[0m
[37m[1m[2023-07-11 05:58:40,298][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:58:40,300][233954] mean_value=-321.6822796606052, max_value=399.99322868738324[0m
[37m[1m[2023-07-11 05:58:40,302][233954] New mean coefficients: [[  9.754809   -5.618169    9.895176   -4.451237  -11.4708805  -9.139139 ]][0m
[37m[1m[2023-07-11 05:58:40,303][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:58:49,246][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 05:58:49,246][233954] FPS: 429455.81[0m
[36m[2023-07-11 05:58:49,248][233954] itr=453, itrs=2000, Progress: 22.65%[0m
[36m[2023-07-11 05:59:00,905][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 05:59:00,911][233954] FPS: 331681.61[0m
[36m[2023-07-11 05:59:05,247][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:59:05,248][233954] Reward + Measures: [[-7.11376362  0.08012533  0.16397467  0.084537    0.133002    2.01953816]][0m
[37m[1m[2023-07-11 05:59:05,248][233954] Max Reward on eval: -7.113763617129179[0m
[37m[1m[2023-07-11 05:59:05,248][233954] Min Reward on eval: -7.113763617129179[0m
[37m[1m[2023-07-11 05:59:05,248][233954] Mean Reward across all agents: -7.113763617129179[0m
[37m[1m[2023-07-11 05:59:05,249][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:59:10,214][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:59:10,214][233954] Reward + Measures: [[ 47.37500256   0.147        0.32070002   0.12810002   0.30990002
    2.76776505]
 [ 49.03147004   0.22420001   0.51060003   0.11080001   0.56239998
    3.02800751]
 [ 34.48140538   0.2278       0.42160001   0.122        0.34919998
    2.57778025]
 ...
 [-24.01362507   0.31020001   0.40170002   0.19520001   0.39309999
    2.6165731 ]
 [101.70497118   0.23360002   0.4224       0.11290001   0.37149999
    2.73947382]
 [ 19.91490402   0.16849999   0.50450003   0.1151       0.56600004
    2.90136027]][0m
[37m[1m[2023-07-11 05:59:10,215][233954] Max Reward on eval: 269.29448022227734[0m
[37m[1m[2023-07-11 05:59:10,215][233954] Min Reward on eval: -78.00710542006418[0m
[37m[1m[2023-07-11 05:59:10,215][233954] Mean Reward across all agents: 44.489494455074855[0m
[37m[1m[2023-07-11 05:59:10,215][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:59:10,219][233954] mean_value=-89.5061219375321, max_value=453.8124002807239[0m
[37m[1m[2023-07-11 05:59:10,222][233954] New mean coefficients: [[  8.144636   -5.4456224   7.068135   -4.4343247 -10.990444   -9.330663 ]][0m
[37m[1m[2023-07-11 05:59:10,223][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:59:19,296][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 05:59:19,296][233954] FPS: 423283.04[0m
[36m[2023-07-11 05:59:19,299][233954] itr=454, itrs=2000, Progress: 22.70%[0m
[36m[2023-07-11 05:59:31,015][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 05:59:31,016][233954] FPS: 330029.86[0m
[36m[2023-07-11 05:59:35,302][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:59:35,302][233954] Reward + Measures: [[-7.66624406  0.081618    0.16747467  0.08568567  0.13922967  2.02633977]][0m
[37m[1m[2023-07-11 05:59:35,303][233954] Max Reward on eval: -7.666244055838596[0m
[37m[1m[2023-07-11 05:59:35,303][233954] Min Reward on eval: -7.666244055838596[0m
[37m[1m[2023-07-11 05:59:35,303][233954] Mean Reward across all agents: -7.666244055838596[0m
[37m[1m[2023-07-11 05:59:35,303][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:59:40,236][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 05:59:40,237][233954] Reward + Measures: [[114.63444008   0.12449999   0.56770003   0.22230001   0.54750007
    2.30925727]
 [116.04809283   0.1097       0.565        0.19780001   0.51410002
    2.37461853]
 [158.58011721   0.12750001   0.63880002   0.23020001   0.5589
    2.4406538 ]
 ...
 [119.11245481   0.13410001   0.6494       0.24510001   0.5808
    2.47582173]
 [123.1236286    0.16400002   0.56510001   0.2581       0.52680004
    2.25956011]
 [115.32840655   0.13540001   0.58790004   0.26440001   0.59390002
    2.45430732]][0m
[37m[1m[2023-07-11 05:59:40,237][233954] Max Reward on eval: 459.92558856494725[0m
[37m[1m[2023-07-11 05:59:40,237][233954] Min Reward on eval: -52.68057573363185[0m
[37m[1m[2023-07-11 05:59:40,237][233954] Mean Reward across all agents: 117.21588484365707[0m
[37m[1m[2023-07-11 05:59:40,238][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 05:59:40,241][233954] mean_value=-56.672496970160026, max_value=370.49350353536755[0m
[37m[1m[2023-07-11 05:59:40,243][233954] New mean coefficients: [[  9.19813    -4.9877834   9.523399   -4.0432153 -10.182957   -8.132715 ]][0m
[37m[1m[2023-07-11 05:59:40,244][233954] Moving the mean solution point...[0m
[36m[2023-07-11 05:59:49,150][233954] train() took 8.90 seconds to complete[0m
[36m[2023-07-11 05:59:49,151][233954] FPS: 431240.40[0m
[36m[2023-07-11 05:59:49,153][233954] itr=455, itrs=2000, Progress: 22.75%[0m
[36m[2023-07-11 06:00:00,839][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 06:00:00,845][233954] FPS: 330890.78[0m
[36m[2023-07-11 06:00:05,148][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:00:05,148][233954] Reward + Measures: [[-4.49176641  0.080434    0.16465667  0.08525466  0.13758166  1.99715459]][0m
[37m[1m[2023-07-11 06:00:05,148][233954] Max Reward on eval: -4.491766408801409[0m
[37m[1m[2023-07-11 06:00:05,148][233954] Min Reward on eval: -4.491766408801409[0m
[37m[1m[2023-07-11 06:00:05,149][233954] Mean Reward across all agents: -4.491766408801409[0m
[37m[1m[2023-07-11 06:00:05,149][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:00:10,125][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:00:10,126][233954] Reward + Measures: [[106.19475203   0.23109999   0.4673       0.25050002   0.433
    2.3373735 ]
 [101.24496381   0.29070002   0.54689997   0.31659999   0.49380001
    2.42972922]
 [ 61.79643834   0.21299998   0.53249997   0.2139       0.45539999
    2.3480258 ]
 ...
 [ 66.65807095   0.45769998   0.8064       0.46580002   0.68500006
    2.67978096]
 [119.91778683   0.3434       0.64420003   0.33390003   0.58350003
    2.43263698]
 [ 83.40038601   0.36320001   0.57980007   0.30050001   0.5431
    2.30838656]][0m
[37m[1m[2023-07-11 06:00:10,126][233954] Max Reward on eval: 171.1246580796782[0m
[37m[1m[2023-07-11 06:00:10,127][233954] Min Reward on eval: -76.65419458029791[0m
[37m[1m[2023-07-11 06:00:10,127][233954] Mean Reward across all agents: 48.50267681733813[0m
[37m[1m[2023-07-11 06:00:10,127][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:00:10,130][233954] mean_value=-79.13042230383844, max_value=617.8494114783883[0m
[37m[1m[2023-07-11 06:00:10,132][233954] New mean coefficients: [[ 9.255307  -4.040736   9.778531  -4.404208  -9.006441  -6.8717713]][0m
[37m[1m[2023-07-11 06:00:10,133][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:00:19,125][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 06:00:19,126][233954] FPS: 427116.43[0m
[36m[2023-07-11 06:00:19,128][233954] itr=456, itrs=2000, Progress: 22.80%[0m
[36m[2023-07-11 06:00:30,889][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 06:00:30,889][233954] FPS: 329193.67[0m
[36m[2023-07-11 06:00:35,156][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:00:35,157][233954] Reward + Measures: [[-5.29636629  0.07976133  0.16675133  0.08338434  0.13675967  1.99025953]][0m
[37m[1m[2023-07-11 06:00:35,157][233954] Max Reward on eval: -5.296366290876312[0m
[37m[1m[2023-07-11 06:00:35,157][233954] Min Reward on eval: -5.296366290876312[0m
[37m[1m[2023-07-11 06:00:35,157][233954] Mean Reward across all agents: -5.296366290876312[0m
[37m[1m[2023-07-11 06:00:35,158][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:00:40,332][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:00:40,333][233954] Reward + Measures: [[73.98401942  0.12409999  0.7604      0.19850001  0.61170006  2.74803448]
 [62.93277038  0.1629      0.61920005  0.18139999  0.49359998  2.61909795]
 [77.49097529  0.14390001  0.84149998  0.24230002  0.66949999  2.7810111 ]
 ...
 [51.9805715   0.1718      0.77499998  0.1682      0.58610004  2.64668965]
 [96.71824287  0.0832      0.75660008  0.2289      0.65249997  2.78178263]
 [74.59593256  0.12030001  0.72130001  0.22830001  0.56770003  2.67494774]][0m
[37m[1m[2023-07-11 06:00:40,333][233954] Max Reward on eval: 458.2086591995321[0m
[37m[1m[2023-07-11 06:00:40,333][233954] Min Reward on eval: -74.98869815580547[0m
[37m[1m[2023-07-11 06:00:40,334][233954] Mean Reward across all agents: 75.24587499339144[0m
[37m[1m[2023-07-11 06:00:40,334][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:00:40,336][233954] mean_value=-93.89320174628135, max_value=81.5131759268738[0m
[37m[1m[2023-07-11 06:00:40,338][233954] New mean coefficients: [[ 9.554619  -4.5549    10.268472  -4.7580028 -9.732417  -7.288394 ]][0m
[37m[1m[2023-07-11 06:00:40,339][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:00:49,433][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 06:00:49,433][233954] FPS: 422339.42[0m
[36m[2023-07-11 06:00:49,436][233954] itr=457, itrs=2000, Progress: 22.85%[0m
[36m[2023-07-11 06:01:01,026][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 06:01:01,026][233954] FPS: 333750.80[0m
[36m[2023-07-11 06:01:05,261][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:01:05,262][233954] Reward + Measures: [[1.37433929 0.075964   0.153595   0.08298567 0.13158266 1.96609557]][0m
[37m[1m[2023-07-11 06:01:05,262][233954] Max Reward on eval: 1.3743392902336589[0m
[37m[1m[2023-07-11 06:01:05,262][233954] Min Reward on eval: 1.3743392902336589[0m
[37m[1m[2023-07-11 06:01:05,263][233954] Mean Reward across all agents: 1.3743392902336589[0m
[37m[1m[2023-07-11 06:01:05,263][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:01:10,236][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:01:10,236][233954] Reward + Measures: [[317.62899474   0.08310001   0.78640002   0.38360003   0.71180004
    3.18017077]
 [258.48934736   0.09230001   0.67930007   0.3847       0.62250006
    3.13051343]
 [ 12.71372993   0.2264       0.66180009   0.25040001   0.57919997
    3.21456027]
 ...
 [508.90062222   0.0601       0.86680001   0.57910007   0.81140006
    3.10595632]
 [ 88.11888888   0.1804       0.74439996   0.29860002   0.64870006
    3.22986579]
 [304.82628103   0.0972       0.74820006   0.41780001   0.6728
    3.1342392 ]][0m
[37m[1m[2023-07-11 06:01:10,236][233954] Max Reward on eval: 705.0813770089298[0m
[37m[1m[2023-07-11 06:01:10,237][233954] Min Reward on eval: -85.10172707168822[0m
[37m[1m[2023-07-11 06:01:10,237][233954] Mean Reward across all agents: 213.49045610231792[0m
[37m[1m[2023-07-11 06:01:10,237][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:01:10,242][233954] mean_value=4.97504194972018, max_value=553.9407664998138[0m
[37m[1m[2023-07-11 06:01:10,245][233954] New mean coefficients: [[  8.155435   -4.7882047   7.4347725  -5.3591547 -10.032876   -7.838841 ]][0m
[37m[1m[2023-07-11 06:01:10,246][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:01:19,213][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 06:01:19,214][233954] FPS: 428299.38[0m
[36m[2023-07-11 06:01:19,216][233954] itr=458, itrs=2000, Progress: 22.90%[0m
[36m[2023-07-11 06:01:30,819][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 06:01:30,819][233954] FPS: 333312.89[0m
[36m[2023-07-11 06:01:35,159][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:01:35,159][233954] Reward + Measures: [[2.82421679 0.07422267 0.15196133 0.082245   0.129187   1.95045793]][0m
[37m[1m[2023-07-11 06:01:35,160][233954] Max Reward on eval: 2.8242167871878094[0m
[37m[1m[2023-07-11 06:01:35,160][233954] Min Reward on eval: 2.8242167871878094[0m
[37m[1m[2023-07-11 06:01:35,160][233954] Mean Reward across all agents: 2.8242167871878094[0m
[37m[1m[2023-07-11 06:01:35,160][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:01:40,171][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:01:40,177][233954] Reward + Measures: [[-315.25638051    0.49460003    0.84170002    0.25250003    0.8405
     3.83224225]
 [ -13.24578574    0.38729998    0.79659998    0.29260001    0.80550003
     3.62172627]
 [-314.6119421     0.62510002    0.88190001    0.17349999    0.8757
     3.88630033]
 ...
 [-371.65696837    0.63819999    0.88570005    0.16260001    0.86129999
     3.83703852]
 [ 147.31596184    0.49200001    0.87630004    0.23709999    0.87079996
     3.75076103]
 [ -79.94125796    0.46750003    0.79139996    0.22239999    0.77399999
     3.53129697]][0m
[37m[1m[2023-07-11 06:01:40,177][233954] Max Reward on eval: 430.9503750920761[0m
[37m[1m[2023-07-11 06:01:40,177][233954] Min Reward on eval: -513.3083206380718[0m
[37m[1m[2023-07-11 06:01:40,178][233954] Mean Reward across all agents: -59.707141529713454[0m
[37m[1m[2023-07-11 06:01:40,178][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:01:40,186][233954] mean_value=163.82971231178433, max_value=746.082876247724[0m
[37m[1m[2023-07-11 06:01:40,189][233954] New mean coefficients: [[  8.447802   -4.7443113   8.5054     -5.3819366 -10.013232   -7.758686 ]][0m
[37m[1m[2023-07-11 06:01:40,190][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:01:49,307][233954] train() took 9.12 seconds to complete[0m
[36m[2023-07-11 06:01:49,307][233954] FPS: 421253.27[0m
[36m[2023-07-11 06:01:49,310][233954] itr=459, itrs=2000, Progress: 22.95%[0m
[36m[2023-07-11 06:02:00,936][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 06:02:00,936][233954] FPS: 333200.71[0m
[36m[2023-07-11 06:02:05,189][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:02:05,189][233954] Reward + Measures: [[4.54387981 0.07681566 0.15836801 0.08380134 0.13310933 1.94490302]][0m
[37m[1m[2023-07-11 06:02:05,189][233954] Max Reward on eval: 4.543879813316317[0m
[37m[1m[2023-07-11 06:02:05,190][233954] Min Reward on eval: 4.543879813316317[0m
[37m[1m[2023-07-11 06:02:05,190][233954] Mean Reward across all agents: 4.543879813316317[0m
[37m[1m[2023-07-11 06:02:05,190][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:02:10,218][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:02:10,224][233954] Reward + Measures: [[106.53435182   0.20200002   0.35300002   0.1837       0.28589997
    2.87839484]
 [151.4085052    0.20650001   0.56490004   0.1504       0.40200001
    2.95630717]
 [ 40.65312612   0.38960001   0.47339997   0.37390003   0.42510006
    2.9361794 ]
 ...
 [103.62353563   0.2379       0.48590001   0.19469999   0.37350002
    2.83991432]
 [132.1251241    0.24229999   0.43450004   0.21480003   0.34200001
    2.984231  ]
 [ 75.54347193   0.1612       0.3538       0.1257       0.28889999
    2.78878951]][0m
[37m[1m[2023-07-11 06:02:10,224][233954] Max Reward on eval: 183.30059861075134[0m
[37m[1m[2023-07-11 06:02:10,224][233954] Min Reward on eval: -80.71133677642793[0m
[37m[1m[2023-07-11 06:02:10,225][233954] Mean Reward across all agents: 89.0065568417438[0m
[37m[1m[2023-07-11 06:02:10,225][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:02:10,229][233954] mean_value=-76.93539560706813, max_value=672.7109607312829[0m
[37m[1m[2023-07-11 06:02:10,231][233954] New mean coefficients: [[  7.965772   -5.8040037   6.3337865  -6.59363   -10.844941   -8.672007 ]][0m
[37m[1m[2023-07-11 06:02:10,232][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:02:19,303][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 06:02:19,303][233954] FPS: 423417.13[0m
[36m[2023-07-11 06:02:19,305][233954] itr=460, itrs=2000, Progress: 23.00%[0m
[37m[1m[2023-07-11 06:05:34,517][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000440[0m
[36m[2023-07-11 06:05:46,717][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 06:05:46,717][233954] FPS: 328548.93[0m
[36m[2023-07-11 06:05:50,939][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:05:50,939][233954] Reward + Measures: [[3.41273211 0.07378633 0.15783699 0.08206066 0.12744966 1.95702338]][0m
[37m[1m[2023-07-11 06:05:50,940][233954] Max Reward on eval: 3.412732111462508[0m
[37m[1m[2023-07-11 06:05:50,940][233954] Min Reward on eval: 3.412732111462508[0m
[37m[1m[2023-07-11 06:05:50,940][233954] Mean Reward across all agents: 3.412732111462508[0m
[37m[1m[2023-07-11 06:05:50,941][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:05:55,854][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:05:55,855][233954] Reward + Measures: [[17.42928747  0.25470001  0.51700002  0.21440001  0.4289      2.40493655]
 [44.82451938  0.36480004  0.53870004  0.31750003  0.47480002  2.65924501]
 [50.08702759  0.18460001  0.27160001  0.14380001  0.2545      2.31085181]
 ...
 [17.02012187  0.30710003  0.47980005  0.25780001  0.44850001  2.39938879]
 [33.83724034  0.3197      0.44440004  0.26970002  0.42129999  2.64507079]
 [29.73199567  0.1892      0.59380001  0.1934      0.46199998  2.43096519]][0m
[37m[1m[2023-07-11 06:05:55,855][233954] Max Reward on eval: 314.8591318661347[0m
[37m[1m[2023-07-11 06:05:55,855][233954] Min Reward on eval: -34.144365350529554[0m
[37m[1m[2023-07-11 06:05:55,856][233954] Mean Reward across all agents: 53.118367168530625[0m
[37m[1m[2023-07-11 06:05:55,856][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:05:55,858][233954] mean_value=-130.00494839551263, max_value=291.8818838456842[0m
[37m[1m[2023-07-11 06:05:55,861][233954] New mean coefficients: [[ 9.676741 -4.853963  8.320762 -5.281114 -9.769973 -6.75716 ]][0m
[37m[1m[2023-07-11 06:05:55,862][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:06:04,794][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 06:06:04,794][233954] FPS: 430009.11[0m
[36m[2023-07-11 06:06:04,796][233954] itr=461, itrs=2000, Progress: 23.05%[0m
[36m[2023-07-11 06:06:16,397][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 06:06:16,397][233954] FPS: 333401.04[0m
[36m[2023-07-11 06:06:20,726][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:06:20,727][233954] Reward + Measures: [[7.13343966 0.07654166 0.16054933 0.08344067 0.12671833 1.93794525]][0m
[37m[1m[2023-07-11 06:06:20,727][233954] Max Reward on eval: 7.133439662605117[0m
[37m[1m[2023-07-11 06:06:20,727][233954] Min Reward on eval: 7.133439662605117[0m
[37m[1m[2023-07-11 06:06:20,728][233954] Mean Reward across all agents: 7.133439662605117[0m
[37m[1m[2023-07-11 06:06:20,728][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:06:25,636][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:06:25,637][233954] Reward + Measures: [[-20.83315605   0.28529999   0.52599996   0.24770001   0.42530003
    2.4539535 ]
 [ 18.48097454   0.26910001   0.60420001   0.1194       0.51920003
    2.55824471]
 [-35.13554844   0.22669999   0.64560002   0.12990001   0.54140002
    2.51933074]
 ...
 [-59.66513419   0.152        0.46380001   0.13070001   0.35249996
    2.49291277]
 [ 60.67159063   0.3026       0.52809995   0.12630001   0.4955
    2.57171392]
 [ 18.70268657   0.1531       0.50789994   0.17020001   0.4021
    2.61004233]][0m
[37m[1m[2023-07-11 06:06:25,637][233954] Max Reward on eval: 412.96573065794075[0m
[37m[1m[2023-07-11 06:06:25,637][233954] Min Reward on eval: -111.38425445146859[0m
[37m[1m[2023-07-11 06:06:25,638][233954] Mean Reward across all agents: 16.0053356338893[0m
[37m[1m[2023-07-11 06:06:25,638][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:06:25,640][233954] mean_value=-182.8516253811085, max_value=682.989253159443[0m
[37m[1m[2023-07-11 06:06:25,643][233954] New mean coefficients: [[  7.6739736  -5.0037017   5.8733864  -4.7997694 -10.176861   -8.1942005]][0m
[37m[1m[2023-07-11 06:06:25,644][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:06:34,601][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 06:06:34,601][233954] FPS: 428765.05[0m
[36m[2023-07-11 06:06:34,604][233954] itr=462, itrs=2000, Progress: 23.10%[0m
[36m[2023-07-11 06:06:46,477][233954] train() took 11.79 seconds to complete[0m
[36m[2023-07-11 06:06:46,477][233954] FPS: 325652.35[0m
[36m[2023-07-11 06:06:50,789][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:06:50,789][233954] Reward + Measures: [[6.64342382 0.07381133 0.15789734 0.082469   0.12092267 1.92247462]][0m
[37m[1m[2023-07-11 06:06:50,789][233954] Max Reward on eval: 6.6434238172482[0m
[37m[1m[2023-07-11 06:06:50,790][233954] Min Reward on eval: 6.6434238172482[0m
[37m[1m[2023-07-11 06:06:50,790][233954] Mean Reward across all agents: 6.6434238172482[0m
[37m[1m[2023-07-11 06:06:50,790][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:06:56,001][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:06:56,002][233954] Reward + Measures: [[ 34.18045556   0.15860002   0.47490001   0.1418       0.35780001
    2.23087406]
 [-36.966836     0.27320001   0.59000003   0.2076       0.48270002
    2.53342557]
 [ 55.0077073    0.17659999   0.73119992   0.207        0.62260002
    2.7190423 ]
 ...
 [  2.39975211   0.20179999   0.61089998   0.12959999   0.48390004
    2.48743296]
 [-17.65390258   0.29580003   0.62190002   0.24000001   0.49449998
    2.56579471]
 [105.12398462   0.13600001   0.6868       0.1909       0.57389998
    2.55593371]][0m
[37m[1m[2023-07-11 06:06:56,002][233954] Max Reward on eval: 170.2880012858659[0m
[37m[1m[2023-07-11 06:06:56,003][233954] Min Reward on eval: -60.26031328900717[0m
[37m[1m[2023-07-11 06:06:56,003][233954] Mean Reward across all agents: 39.94341270988379[0m
[37m[1m[2023-07-11 06:06:56,003][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:06:56,005][233954] mean_value=-140.39007459408862, max_value=233.05306670132978[0m
[37m[1m[2023-07-11 06:06:56,007][233954] New mean coefficients: [[  4.9201365  -5.554686    1.2205949  -7.097712  -10.395576   -9.389586 ]][0m
[37m[1m[2023-07-11 06:06:56,008][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:07:05,033][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 06:07:05,033][233954] FPS: 425577.45[0m
[36m[2023-07-11 06:07:05,035][233954] itr=463, itrs=2000, Progress: 23.15%[0m
[36m[2023-07-11 06:07:16,724][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 06:07:16,725][233954] FPS: 330798.17[0m
[36m[2023-07-11 06:07:20,991][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:07:20,991][233954] Reward + Measures: [[10.8934977   0.07334767  0.15347166  0.081724    0.11766434  1.90304196]][0m
[37m[1m[2023-07-11 06:07:20,991][233954] Max Reward on eval: 10.89349769768631[0m
[37m[1m[2023-07-11 06:07:20,992][233954] Min Reward on eval: 10.89349769768631[0m
[37m[1m[2023-07-11 06:07:20,992][233954] Mean Reward across all agents: 10.89349769768631[0m
[37m[1m[2023-07-11 06:07:20,992][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:07:25,972][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:07:25,973][233954] Reward + Measures: [[ -4.01610592   0.23090003   0.42570001   0.21009998   0.40120003
    2.70436263]
 [ 78.29025026   0.39379999   0.52470005   0.0565       0.49659997
    2.50910378]
 [ 44.65832584   0.15210001   0.54809999   0.18560001   0.46280003
    2.46537089]
 ...
 [-12.00441719   0.35639998   0.57300007   0.0951       0.57639998
    2.54805231]
 [-36.62321065   0.2378       0.31479999   0.13590001   0.33189997
    2.44682693]
 [  1.29829879   0.2089       0.39359999   0.2404       0.3723
    2.64573836]][0m
[37m[1m[2023-07-11 06:07:25,973][233954] Max Reward on eval: 312.9286091946997[0m
[37m[1m[2023-07-11 06:07:25,974][233954] Min Reward on eval: -104.68801759146154[0m
[37m[1m[2023-07-11 06:07:25,974][233954] Mean Reward across all agents: 29.342900265832522[0m
[37m[1m[2023-07-11 06:07:25,974][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:07:25,977][233954] mean_value=-160.93425615370919, max_value=534.111451580223[0m
[37m[1m[2023-07-11 06:07:25,979][233954] New mean coefficients: [[  3.9833813  -5.927384   -1.5356395  -7.9510393 -10.854975   -9.70168  ]][0m
[37m[1m[2023-07-11 06:07:25,980][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:07:34,955][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 06:07:34,955][233954] FPS: 427929.90[0m
[36m[2023-07-11 06:07:34,958][233954] itr=464, itrs=2000, Progress: 23.20%[0m
[36m[2023-07-11 06:07:46,748][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 06:07:46,749][233954] FPS: 328017.23[0m
[36m[2023-07-11 06:07:51,147][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:07:51,148][233954] Reward + Measures: [[10.33934672  0.07339934  0.15541366  0.08293967  0.11956567  1.88740218]][0m
[37m[1m[2023-07-11 06:07:51,148][233954] Max Reward on eval: 10.339346723857123[0m
[37m[1m[2023-07-11 06:07:51,148][233954] Min Reward on eval: 10.339346723857123[0m
[37m[1m[2023-07-11 06:07:51,148][233954] Mean Reward across all agents: 10.339346723857123[0m
[37m[1m[2023-07-11 06:07:51,149][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:07:56,148][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:07:56,149][233954] Reward + Measures: [[ 57.28571906   0.1139       0.13259999   0.12         0.13700001
    2.65127921]
 [114.38715748   0.23920003   0.1904       0.2175       0.25140002
    2.70427847]
 [ 43.17761213   0.0879       0.1258       0.13170001   0.12230001
    2.8850553 ]
 ...
 [ 60.2314127    0.09370001   0.1267       0.1081       0.1069
    2.82830167]
 [ 83.14271581   0.0704       0.09870001   0.1158       0.1006
    2.9795711 ]
 [160.53246809   0.1045       0.125        0.147        0.12540002
    2.88878894]][0m
[37m[1m[2023-07-11 06:07:56,149][233954] Max Reward on eval: 283.73051059469583[0m
[37m[1m[2023-07-11 06:07:56,150][233954] Min Reward on eval: -24.25421032262966[0m
[37m[1m[2023-07-11 06:07:56,150][233954] Mean Reward across all agents: 89.83055902036055[0m
[37m[1m[2023-07-11 06:07:56,150][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:07:56,152][233954] mean_value=-218.12283682814282, max_value=248.23841870147194[0m
[37m[1m[2023-07-11 06:07:56,154][233954] New mean coefficients: [[  2.8110623  -5.4003673  -3.2556758  -7.405785  -10.309058   -9.968039 ]][0m
[37m[1m[2023-07-11 06:07:56,155][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:08:05,212][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 06:08:05,212][233954] FPS: 424068.66[0m
[36m[2023-07-11 06:08:05,214][233954] itr=465, itrs=2000, Progress: 23.25%[0m
[36m[2023-07-11 06:08:16,852][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 06:08:16,852][233954] FPS: 332270.54[0m
[36m[2023-07-11 06:08:21,117][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:08:21,118][233954] Reward + Measures: [[11.46858531  0.07255767  0.14975199  0.08106034  0.11738999  1.87787819]][0m
[37m[1m[2023-07-11 06:08:21,118][233954] Max Reward on eval: 11.46858530918433[0m
[37m[1m[2023-07-11 06:08:21,118][233954] Min Reward on eval: 11.46858530918433[0m
[37m[1m[2023-07-11 06:08:21,118][233954] Mean Reward across all agents: 11.46858530918433[0m
[37m[1m[2023-07-11 06:08:21,119][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:08:26,082][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:08:26,082][233954] Reward + Measures: [[ 16.3585035    0.1296       0.40330002   0.1349       0.31150001
    2.1693716 ]
 [193.6608495    0.31799999   0.23         0.32150003   0.34459996
    2.72818232]
 [ 38.07714688   0.1918       0.207        0.18179999   0.1944
    2.76076126]
 ...
 [ 90.51041177   0.24190001   0.2999       0.1997       0.26550001
    2.66258693]
 [168.62434398   0.41169998   0.1908       0.43670002   0.48280001
    2.66011572]
 [180.07302799   0.2613       0.308        0.20430003   0.28959998
    2.74254036]][0m
[37m[1m[2023-07-11 06:08:26,083][233954] Max Reward on eval: 542.0026626519859[0m
[37m[1m[2023-07-11 06:08:26,083][233954] Min Reward on eval: -73.8555817561224[0m
[37m[1m[2023-07-11 06:08:26,083][233954] Mean Reward across all agents: 86.36027729478897[0m
[37m[1m[2023-07-11 06:08:26,083][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:08:26,086][233954] mean_value=-166.24886410292652, max_value=485.71218624299763[0m
[37m[1m[2023-07-11 06:08:26,089][233954] New mean coefficients: [[ 3.5562809 -4.4387984 -1.8256419 -5.853982  -9.405402  -9.0299   ]][0m
[37m[1m[2023-07-11 06:08:26,090][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:08:35,060][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 06:08:35,060][233954] FPS: 428164.73[0m
[36m[2023-07-11 06:08:35,062][233954] itr=466, itrs=2000, Progress: 23.30%[0m
[36m[2023-07-11 06:08:46,662][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 06:08:46,662][233954] FPS: 333400.75[0m
[36m[2023-07-11 06:08:50,886][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:08:50,887][233954] Reward + Measures: [[11.19556408  0.07169334  0.14634232  0.08084767  0.11715334  1.87262464]][0m
[37m[1m[2023-07-11 06:08:50,887][233954] Max Reward on eval: 11.195564075697796[0m
[37m[1m[2023-07-11 06:08:50,887][233954] Min Reward on eval: 11.195564075697796[0m
[37m[1m[2023-07-11 06:08:50,888][233954] Mean Reward across all agents: 11.195564075697796[0m
[37m[1m[2023-07-11 06:08:50,888][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:08:55,923][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:08:55,924][233954] Reward + Measures: [[-10.91224244   0.81160003   0.81840003   0.78739995   0.79449999
    3.40995193]
 [ -5.6946476    0.23509999   0.20469999   0.23040001   0.1945
    2.74633336]
 [ 18.618521     0.31020001   0.29949999   0.2357       0.29369998
    2.90362406]
 ...
 [ 38.02091588   0.32749999   0.33239999   0.32469997   0.30110002
    3.02582145]
 [ -5.31718868   0.14870001   0.14720002   0.1348       0.1339
    3.09041095]
 [ -3.72841211   0.1805       0.15510002   0.19719999   0.15980001
    2.86781025]][0m
[37m[1m[2023-07-11 06:08:55,924][233954] Max Reward on eval: 278.1142361165956[0m
[37m[1m[2023-07-11 06:08:55,924][233954] Min Reward on eval: -80.63920249380172[0m
[37m[1m[2023-07-11 06:08:55,924][233954] Mean Reward across all agents: 16.071369224309038[0m
[37m[1m[2023-07-11 06:08:55,925][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:08:55,927][233954] mean_value=-174.90988841302394, max_value=133.1596229338577[0m
[37m[1m[2023-07-11 06:08:55,930][233954] New mean coefficients: [[ 5.191661  -2.9774537  0.9463005 -4.3787003 -7.9714646 -7.389066 ]][0m
[37m[1m[2023-07-11 06:08:55,931][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:09:05,025][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 06:09:05,025][233954] FPS: 422339.74[0m
[36m[2023-07-11 06:09:05,027][233954] itr=467, itrs=2000, Progress: 23.35%[0m
[36m[2023-07-11 06:09:16,711][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 06:09:16,711][233954] FPS: 331028.46[0m
[36m[2023-07-11 06:09:20,942][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:09:20,948][233954] Reward + Measures: [[13.96278183  0.07204     0.141895    0.08142867  0.11430433  1.84979749]][0m
[37m[1m[2023-07-11 06:09:20,948][233954] Max Reward on eval: 13.962781826313158[0m
[37m[1m[2023-07-11 06:09:20,948][233954] Min Reward on eval: 13.962781826313158[0m
[37m[1m[2023-07-11 06:09:20,948][233954] Mean Reward across all agents: 13.962781826313158[0m
[37m[1m[2023-07-11 06:09:20,949][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:09:26,181][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:09:26,182][233954] Reward + Measures: [[ 27.65829352   0.1989       0.47710004   0.1798       0.39510003
    2.32995868]
 [ 63.31081775   0.11210001   0.70909995   0.23630002   0.67690003
    2.67063904]
 [212.11585786   0.09760001   0.73679996   0.36310002   0.66930002
    2.57667232]
 ...
 [213.99322854   0.11670001   0.89700001   0.38410002   0.80720007
    2.77776217]
 [-40.13189779   0.15019999   0.39970002   0.1372       0.3576
    2.42785001]
 [124.51340853   0.1028       0.6322       0.2762       0.59420007
    2.63478446]][0m
[37m[1m[2023-07-11 06:09:26,182][233954] Max Reward on eval: 293.73236989285795[0m
[37m[1m[2023-07-11 06:09:26,182][233954] Min Reward on eval: -81.50820745734964[0m
[37m[1m[2023-07-11 06:09:26,183][233954] Mean Reward across all agents: 44.52906048768808[0m
[37m[1m[2023-07-11 06:09:26,183][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:09:26,185][233954] mean_value=-154.36640484168814, max_value=228.7759001473382[0m
[37m[1m[2023-07-11 06:09:26,187][233954] New mean coefficients: [[ 4.7640653 -2.4055877 -0.4473821 -3.0542302 -7.2517614 -7.2827244]][0m
[37m[1m[2023-07-11 06:09:26,188][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:09:35,211][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 06:09:35,212][233954] FPS: 425640.62[0m
[36m[2023-07-11 06:09:35,214][233954] itr=468, itrs=2000, Progress: 23.40%[0m
[36m[2023-07-11 06:09:46,811][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 06:09:46,811][233954] FPS: 333427.33[0m
[36m[2023-07-11 06:09:51,037][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:09:51,037][233954] Reward + Measures: [[15.90600963  0.07302967  0.14029801  0.08263766  0.11698032  1.84384573]][0m
[37m[1m[2023-07-11 06:09:51,038][233954] Max Reward on eval: 15.906009634627825[0m
[37m[1m[2023-07-11 06:09:51,038][233954] Min Reward on eval: 15.906009634627825[0m
[37m[1m[2023-07-11 06:09:51,038][233954] Mean Reward across all agents: 15.906009634627825[0m
[37m[1m[2023-07-11 06:09:51,038][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:09:56,026][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:09:56,027][233954] Reward + Measures: [[125.61427922   0.2141       0.40229997   0.2335       0.368
    2.50107765]
 [133.9295435    0.16760001   0.32309997   0.20379999   0.27790001
    2.91589403]
 [200.95077896   0.0027       0.99650002   0.52450007   0.99000007
    3.64238405]
 ...
 [122.89848899   0.17550002   0.32860002   0.2062       0.2956
    2.83896303]
 [ 72.31714201   0.25940001   0.5266       0.29159999   0.50850004
    2.68638682]
 [141.38966272   0.1542       0.29190001   0.18350001   0.26830003
    2.71228123]][0m
[37m[1m[2023-07-11 06:09:56,027][233954] Max Reward on eval: 628.5647468363866[0m
[37m[1m[2023-07-11 06:09:56,028][233954] Min Reward on eval: -92.81718395263889[0m
[37m[1m[2023-07-11 06:09:56,028][233954] Mean Reward across all agents: 112.05241354997575[0m
[37m[1m[2023-07-11 06:09:56,028][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:09:56,033][233954] mean_value=-73.21824113825568, max_value=556.3169278985448[0m
[37m[1m[2023-07-11 06:09:56,035][233954] New mean coefficients: [[ 4.2891164 -2.500101  -1.1864501 -3.234741  -7.278079  -7.7861753]][0m
[37m[1m[2023-07-11 06:09:56,036][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:10:05,043][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 06:10:05,044][233954] FPS: 426412.00[0m
[36m[2023-07-11 06:10:05,046][233954] itr=469, itrs=2000, Progress: 23.45%[0m
[36m[2023-07-11 06:10:16,636][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 06:10:16,636][233954] FPS: 333731.33[0m
[36m[2023-07-11 06:10:20,943][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:10:20,949][233954] Reward + Measures: [[14.92923725  0.07181267  0.14148466  0.07959233  0.11594333  1.84636998]][0m
[37m[1m[2023-07-11 06:10:20,949][233954] Max Reward on eval: 14.929237252343368[0m
[37m[1m[2023-07-11 06:10:20,949][233954] Min Reward on eval: 14.929237252343368[0m
[37m[1m[2023-07-11 06:10:20,950][233954] Mean Reward across all agents: 14.929237252343368[0m
[37m[1m[2023-07-11 06:10:20,950][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:10:25,958][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:10:25,963][233954] Reward + Measures: [[151.21614646   0.18670002   0.39800003   0.21030001   0.40990001
    2.74757957]
 [ 44.76984204   0.19460002   0.44649997   0.17510001   0.42600003
    2.72488093]
 [ -6.81438911   0.20420001   0.47960001   0.212        0.4118
    2.82730341]
 ...
 [ 87.87538703   0.1619       0.39049998   0.18880001   0.37779999
    2.79720306]
 [ 84.61563565   0.43800002   0.85699999   0.48890004   0.787
    2.73030901]
 [131.10131863   0.2588       0.30159998   0.193        0.36630002
    2.81320453]][0m
[37m[1m[2023-07-11 06:10:25,964][233954] Max Reward on eval: 245.71841633296572[0m
[37m[1m[2023-07-11 06:10:25,964][233954] Min Reward on eval: -73.32665973855183[0m
[37m[1m[2023-07-11 06:10:25,964][233954] Mean Reward across all agents: 72.9781019594991[0m
[37m[1m[2023-07-11 06:10:25,964][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:10:25,967][233954] mean_value=-117.87630183300568, max_value=547.0455071119592[0m
[37m[1m[2023-07-11 06:10:25,970][233954] New mean coefficients: [[ 5.7712483 -3.3298457  1.0621098 -2.547713  -8.391047  -7.6973467]][0m
[37m[1m[2023-07-11 06:10:25,971][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:10:35,010][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 06:10:35,010][233954] FPS: 424913.40[0m
[36m[2023-07-11 06:10:35,012][233954] itr=470, itrs=2000, Progress: 23.50%[0m
[37m[1m[2023-07-11 06:13:53,576][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000450[0m
[36m[2023-07-11 06:14:05,867][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 06:14:05,867][233954] FPS: 333360.27[0m
[36m[2023-07-11 06:14:10,016][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:14:10,017][233954] Reward + Measures: [[17.70145928  0.07075333  0.13413566  0.08010033  0.113432    1.81802046]][0m
[37m[1m[2023-07-11 06:14:10,017][233954] Max Reward on eval: 17.701459279054436[0m
[37m[1m[2023-07-11 06:14:10,017][233954] Min Reward on eval: 17.701459279054436[0m
[37m[1m[2023-07-11 06:14:10,018][233954] Mean Reward across all agents: 17.701459279054436[0m
[37m[1m[2023-07-11 06:14:10,018][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:14:14,959][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:14:14,959][233954] Reward + Measures: [[-56.35485868   0.12740001   0.51550001   0.11980001   0.36059999
    2.34875607]
 [-43.90984647   0.11900001   0.47139999   0.13950001   0.37040001
    2.32314372]
 [-33.03300287   0.0977       0.2922       0.0948       0.21630001
    2.2495954 ]
 ...
 [-29.34467471   0.07430001   0.2978       0.11730001   0.24360001
    2.25496745]
 [ 19.12196913   0.2321       0.574        0.12910001   0.51550001
    2.36570883]
 [-85.15381672   0.1578       0.6232       0.1239       0.45240003
    2.44694877]][0m
[37m[1m[2023-07-11 06:14:14,959][233954] Max Reward on eval: 160.82812001444398[0m
[37m[1m[2023-07-11 06:14:14,960][233954] Min Reward on eval: -93.21586559740827[0m
[37m[1m[2023-07-11 06:14:14,960][233954] Mean Reward across all agents: 14.71579007287855[0m
[37m[1m[2023-07-11 06:14:14,960][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:14:14,963][233954] mean_value=-221.94001643016327, max_value=226.3098927371634[0m
[37m[1m[2023-07-11 06:14:14,965][233954] New mean coefficients: [[ 4.182548  -3.892609  -2.0683875 -3.71841   -8.744923  -8.376568 ]][0m
[37m[1m[2023-07-11 06:14:14,966][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:14:24,011][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 06:14:24,012][233954] FPS: 424616.58[0m
[36m[2023-07-11 06:14:24,014][233954] itr=471, itrs=2000, Progress: 23.55%[0m
[36m[2023-07-11 06:14:35,729][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 06:14:35,729][233954] FPS: 330197.78[0m
[36m[2023-07-11 06:14:39,967][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:14:39,967][233954] Reward + Measures: [[17.59987993  0.07446966  0.13690867  0.08234667  0.11788133  1.80768979]][0m
[37m[1m[2023-07-11 06:14:39,967][233954] Max Reward on eval: 17.599879933345598[0m
[37m[1m[2023-07-11 06:14:39,968][233954] Min Reward on eval: 17.599879933345598[0m
[37m[1m[2023-07-11 06:14:39,968][233954] Mean Reward across all agents: 17.599879933345598[0m
[37m[1m[2023-07-11 06:14:39,968][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:14:44,969][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:14:44,970][233954] Reward + Measures: [[26.24832161  0.10309999  0.22919999  0.12730001  0.18520001  2.81820464]
 [94.80813625  0.0825      0.1847      0.1181      0.12110001  2.87287593]
 [32.13194272  0.76840007  0.82629997  0.7525      0.82260001  3.55877948]
 ...
 [82.48322346  0.24959998  0.5341      0.1675      0.46870002  3.14817858]
 [24.94176823  0.1089      0.18660001  0.1296      0.16239999  2.81391406]
 [24.86229675  0.26789999  0.34439999  0.13349999  0.35010001  3.22129297]][0m
[37m[1m[2023-07-11 06:14:44,970][233954] Max Reward on eval: 433.81261104889853[0m
[37m[1m[2023-07-11 06:14:44,971][233954] Min Reward on eval: -303.3769340788946[0m
[37m[1m[2023-07-11 06:14:44,971][233954] Mean Reward across all agents: 35.58988361881477[0m
[37m[1m[2023-07-11 06:14:44,971][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:14:44,976][233954] mean_value=-108.42490433636003, max_value=503.82391772335393[0m
[37m[1m[2023-07-11 06:14:44,978][233954] New mean coefficients: [[ 6.63947   -2.7285047  2.3118286 -1.5710297 -7.664861  -6.234324 ]][0m
[37m[1m[2023-07-11 06:14:44,979][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:14:54,105][233954] train() took 9.12 seconds to complete[0m
[36m[2023-07-11 06:14:54,105][233954] FPS: 420870.17[0m
[36m[2023-07-11 06:14:54,107][233954] itr=472, itrs=2000, Progress: 23.60%[0m
[36m[2023-07-11 06:15:05,866][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 06:15:05,866][233954] FPS: 328826.34[0m
[36m[2023-07-11 06:15:10,149][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:15:10,149][233954] Reward + Measures: [[16.04453983  0.07230966  0.13964367  0.08036266  0.11695866  1.80496943]][0m
[37m[1m[2023-07-11 06:15:10,149][233954] Max Reward on eval: 16.044539830992814[0m
[37m[1m[2023-07-11 06:15:10,150][233954] Min Reward on eval: 16.044539830992814[0m
[37m[1m[2023-07-11 06:15:10,150][233954] Mean Reward across all agents: 16.044539830992814[0m
[37m[1m[2023-07-11 06:15:10,150][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:15:15,133][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:15:15,133][233954] Reward + Measures: [[441.67014283   0.57669997   0.63459998   0.0517       0.61129999
    2.84219217]
 [168.19683079   0.0861       0.52990001   0.28879997   0.47499999
    2.70244622]
 [ 42.24833773   0.0861       0.0965       0.1089       0.12740001
    2.94961143]
 ...
 [180.38721901   0.32040003   0.53780001   0.16150001   0.55730003
    2.92685485]
 [111.08862144   0.1408       0.13670002   0.1007       0.16469999
    3.07694054]
 [ 62.20034549   0.2379       0.2165       0.13630001   0.26030001
    2.88939023]][0m
[37m[1m[2023-07-11 06:15:15,133][233954] Max Reward on eval: 441.67014282960446[0m
[37m[1m[2023-07-11 06:15:15,134][233954] Min Reward on eval: -38.66089752092957[0m
[37m[1m[2023-07-11 06:15:15,134][233954] Mean Reward across all agents: 97.89352600173477[0m
[37m[1m[2023-07-11 06:15:15,134][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:15:15,137][233954] mean_value=-118.75824491235963, max_value=659.1163268130273[0m
[37m[1m[2023-07-11 06:15:15,140][233954] New mean coefficients: [[ 5.2273664  -1.9034797  -0.27862883 -1.796995   -6.5672092  -6.1377153 ]][0m
[37m[1m[2023-07-11 06:15:15,141][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:15:24,121][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 06:15:24,122][233954] FPS: 427680.51[0m
[36m[2023-07-11 06:15:24,124][233954] itr=473, itrs=2000, Progress: 23.65%[0m
[36m[2023-07-11 06:15:35,846][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 06:15:35,846][233954] FPS: 329853.93[0m
[36m[2023-07-11 06:15:40,134][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:15:40,134][233954] Reward + Measures: [[14.99262472  0.074262    0.13870433  0.08129466  0.12028566  1.80931926]][0m
[37m[1m[2023-07-11 06:15:40,135][233954] Max Reward on eval: 14.992624723951584[0m
[37m[1m[2023-07-11 06:15:40,135][233954] Min Reward on eval: 14.992624723951584[0m
[37m[1m[2023-07-11 06:15:40,135][233954] Mean Reward across all agents: 14.992624723951584[0m
[37m[1m[2023-07-11 06:15:40,135][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:15:45,073][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:15:45,074][233954] Reward + Measures: [[114.77823734   0.1357       0.69870007   0.20960002   0.58390003
    3.08075881]
 [175.37286282   0.19170001   0.94709998   0.53060001   0.92179996
    3.19780827]
 [147.43281913   0.33910003   0.23530002   0.35189998   0.44329998
    2.98680091]
 ...
 [ 61.28461501   0.0952       0.35599998   0.13680001   0.33790001
    2.79010177]
 [130.54656189   0.1552       0.33430001   0.21689999   0.38680002
    2.79519463]
 [194.30281839   0.2167       0.68279999   0.2538       0.62330002
    2.78569198]][0m
[37m[1m[2023-07-11 06:15:45,074][233954] Max Reward on eval: 415.94837761430534[0m
[37m[1m[2023-07-11 06:15:45,074][233954] Min Reward on eval: -46.37741186674684[0m
[37m[1m[2023-07-11 06:15:45,074][233954] Mean Reward across all agents: 156.4213798164995[0m
[37m[1m[2023-07-11 06:15:45,075][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:15:45,081][233954] mean_value=12.794029123345293, max_value=815.1245099556363[0m
[37m[1m[2023-07-11 06:15:45,084][233954] New mean coefficients: [[ 5.9555526 -1.008241   0.9190116 -2.52807   -5.379713  -4.9094715]][0m
[37m[1m[2023-07-11 06:15:45,085][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:15:54,047][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 06:15:54,047][233954] FPS: 428559.98[0m
[36m[2023-07-11 06:15:54,049][233954] itr=474, itrs=2000, Progress: 23.70%[0m
[36m[2023-07-11 06:16:05,658][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 06:16:05,658][233954] FPS: 333061.35[0m
[36m[2023-07-11 06:16:09,896][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:16:09,897][233954] Reward + Measures: [[17.77012812  0.073083    0.13407201  0.081173    0.115551    1.78281283]][0m
[37m[1m[2023-07-11 06:16:09,897][233954] Max Reward on eval: 17.770128115322983[0m
[37m[1m[2023-07-11 06:16:09,897][233954] Min Reward on eval: 17.770128115322983[0m
[37m[1m[2023-07-11 06:16:09,897][233954] Mean Reward across all agents: 17.770128115322983[0m
[37m[1m[2023-07-11 06:16:09,898][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:16:14,879][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:16:14,880][233954] Reward + Measures: [[ 16.60122009   0.4515       0.77689999   0.42570001   0.70160002
    2.64266968]
 [ 49.24199206   0.3107       0.69699997   0.29180002   0.58640003
    2.55317545]
 [-43.22638193   0.2378       0.53240001   0.1833       0.39470002
    2.42090774]
 ...
 [ 22.92505715   0.33980003   0.7306       0.3197       0.64820004
    2.59448886]
 [ 29.27843785   0.27739999   0.75309992   0.22920001   0.59450001
    2.63254428]
 [ 58.96080571   0.31119999   0.79820001   0.29539999   0.61390001
    2.70742726]][0m
[37m[1m[2023-07-11 06:16:14,880][233954] Max Reward on eval: 142.37114758109675[0m
[37m[1m[2023-07-11 06:16:14,881][233954] Min Reward on eval: -91.01681830985471[0m
[37m[1m[2023-07-11 06:16:14,881][233954] Mean Reward across all agents: 4.16979934084336[0m
[37m[1m[2023-07-11 06:16:14,881][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:16:14,883][233954] mean_value=-230.35923760812628, max_value=104.02137250538199[0m
[37m[1m[2023-07-11 06:16:14,886][233954] New mean coefficients: [[ 7.9871902   0.21531355  4.662852   -0.9349508  -3.9770474  -2.8559878 ]][0m
[37m[1m[2023-07-11 06:16:14,886][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:16:23,900][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 06:16:23,900][233954] FPS: 426122.31[0m
[36m[2023-07-11 06:16:23,902][233954] itr=475, itrs=2000, Progress: 23.75%[0m
[36m[2023-07-11 06:16:35,686][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 06:16:35,686][233954] FPS: 328117.90[0m
[36m[2023-07-11 06:16:40,008][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:16:40,009][233954] Reward + Measures: [[15.72245875  0.076406    0.14112766  0.08252334  0.117414    1.76766014]][0m
[37m[1m[2023-07-11 06:16:40,009][233954] Max Reward on eval: 15.72245875223239[0m
[37m[1m[2023-07-11 06:16:40,009][233954] Min Reward on eval: 15.72245875223239[0m
[37m[1m[2023-07-11 06:16:40,009][233954] Mean Reward across all agents: 15.72245875223239[0m
[37m[1m[2023-07-11 06:16:40,010][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:16:44,975][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:16:44,976][233954] Reward + Measures: [[ 20.09174999   0.2069       0.37360001   0.126        0.37619999
    2.63831067]
 [ 35.58063497   0.13460001   0.13410001   0.1323       0.17839999
    2.26551938]
 [  3.17301789   0.1463       0.3495       0.1251       0.31510001
    2.53512478]
 ...
 [195.53729172   0.23460002   0.68160003   0.23849998   0.65149999
    3.35528803]
 [ 94.26576223   0.2163       0.41770002   0.07749999   0.38719997
    2.89969087]
 [141.59034824   0.2198       0.28150001   0.043        0.24200001
    3.034868  ]][0m
[37m[1m[2023-07-11 06:16:44,976][233954] Max Reward on eval: 429.9463806068525[0m
[37m[1m[2023-07-11 06:16:44,976][233954] Min Reward on eval: -49.967644300684334[0m
[37m[1m[2023-07-11 06:16:44,977][233954] Mean Reward across all agents: 112.41360093603953[0m
[37m[1m[2023-07-11 06:16:44,977][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:16:44,981][233954] mean_value=-112.5040848432474, max_value=566.8130283655898[0m
[37m[1m[2023-07-11 06:16:44,984][233954] New mean coefficients: [[ 7.565998   -0.59856856  3.7322364  -1.2184665  -4.9301524  -3.9259927 ]][0m
[37m[1m[2023-07-11 06:16:44,985][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:16:53,981][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 06:16:53,981][233954] FPS: 426930.98[0m
[36m[2023-07-11 06:16:53,983][233954] itr=476, itrs=2000, Progress: 23.80%[0m
[36m[2023-07-11 06:17:05,607][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 06:17:05,607][233954] FPS: 332683.12[0m
[36m[2023-07-11 06:17:09,935][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:17:09,935][233954] Reward + Measures: [[20.36892942  0.07372167  0.13413134  0.08136734  0.11470567  1.76684165]][0m
[37m[1m[2023-07-11 06:17:09,935][233954] Max Reward on eval: 20.368929421930787[0m
[37m[1m[2023-07-11 06:17:09,936][233954] Min Reward on eval: 20.368929421930787[0m
[37m[1m[2023-07-11 06:17:09,936][233954] Mean Reward across all agents: 20.368929421930787[0m
[37m[1m[2023-07-11 06:17:09,936][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:17:15,171][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:17:15,172][233954] Reward + Measures: [[ 83.27744058   0.3946       0.57179999   0.39560002   0.49530002
    2.44078708]
 [ 53.14357502   0.1096       0.2277       0.1224       0.19770001
    2.33887458]
 [ 69.1401582    0.0776       0.2007       0.1181       0.176
    2.46949935]
 ...
 [158.17095638   0.05         0.28600001   0.1954       0.26150003
    3.16354465]
 [ 63.92354319   0.093        0.21700001   0.1151       0.19859998
    2.88048339]
 [ 68.91912502   0.66470003   0.90129995   0.64910001   0.84100002
    2.79030991]][0m
[37m[1m[2023-07-11 06:17:15,172][233954] Max Reward on eval: 350.77545037195085[0m
[37m[1m[2023-07-11 06:17:15,172][233954] Min Reward on eval: -67.79328463356941[0m
[37m[1m[2023-07-11 06:17:15,172][233954] Mean Reward across all agents: 87.82782399890046[0m
[37m[1m[2023-07-11 06:17:15,173][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:17:15,177][233954] mean_value=-177.24286728450667, max_value=494.33686587746627[0m
[37m[1m[2023-07-11 06:17:15,180][233954] New mean coefficients: [[ 6.9877152  -0.48789972  2.9329848  -1.739116   -4.664947   -3.9340222 ]][0m
[37m[1m[2023-07-11 06:17:15,180][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:17:24,214][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 06:17:24,214][233954] FPS: 425149.84[0m
[36m[2023-07-11 06:17:24,217][233954] itr=477, itrs=2000, Progress: 23.85%[0m
[36m[2023-07-11 06:17:35,911][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 06:17:35,912][233954] FPS: 330752.62[0m
[36m[2023-07-11 06:17:40,269][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:17:40,270][233954] Reward + Measures: [[20.31359939  0.07603533  0.13976033  0.083971    0.11708999  1.75804436]][0m
[37m[1m[2023-07-11 06:17:40,270][233954] Max Reward on eval: 20.31359938731842[0m
[37m[1m[2023-07-11 06:17:40,270][233954] Min Reward on eval: 20.31359938731842[0m
[37m[1m[2023-07-11 06:17:40,271][233954] Mean Reward across all agents: 20.31359938731842[0m
[37m[1m[2023-07-11 06:17:40,271][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:17:45,324][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:17:45,325][233954] Reward + Measures: [[ 67.0360606    0.079        0.84349996   0.30040002   0.71380007
    3.41345334]
 [ 96.15536117   0.0153       0.98930007   0.48480001   0.98359996
    3.66383028]
 [ 94.83197268   0.36980003   0.53350002   0.37310001   0.4914
    2.58332109]
 ...
 [-65.09231337   0.34920004   0.60340005   0.24270001   0.46350002
    2.61592746]
 [ 28.01762892   0.1912       0.2471       0.19649999   0.24630001
    2.12557173]
 [102.9360876    0.0119       0.99570006   0.49530002   0.99120009
    3.78015184]][0m
[37m[1m[2023-07-11 06:17:45,325][233954] Max Reward on eval: 734.9206314101815[0m
[37m[1m[2023-07-11 06:17:45,325][233954] Min Reward on eval: -204.0380192638375[0m
[37m[1m[2023-07-11 06:17:45,325][233954] Mean Reward across all agents: 106.34802367127786[0m
[37m[1m[2023-07-11 06:17:45,326][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:17:45,329][233954] mean_value=-137.16730812863042, max_value=524.5607438274658[0m
[37m[1m[2023-07-11 06:17:45,332][233954] New mean coefficients: [[ 7.3336864  0.8276028  3.4465606 -1.0102696 -3.3675034 -2.7560966]][0m
[37m[1m[2023-07-11 06:17:45,333][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:17:54,387][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 06:17:54,393][233954] FPS: 424183.11[0m
[36m[2023-07-11 06:17:54,396][233954] itr=478, itrs=2000, Progress: 23.90%[0m
[36m[2023-07-11 06:18:06,111][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 06:18:06,111][233954] FPS: 330045.60[0m
[36m[2023-07-11 06:18:10,353][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:18:10,354][233954] Reward + Measures: [[23.18749041  0.07521667  0.14031866  0.08464933  0.118482    1.74756253]][0m
[37m[1m[2023-07-11 06:18:10,354][233954] Max Reward on eval: 23.187490407714098[0m
[37m[1m[2023-07-11 06:18:10,354][233954] Min Reward on eval: 23.187490407714098[0m
[37m[1m[2023-07-11 06:18:10,355][233954] Mean Reward across all agents: 23.187490407714098[0m
[37m[1m[2023-07-11 06:18:10,355][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:18:15,301][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:18:15,302][233954] Reward + Measures: [[ 24.77220071   0.06420001   0.1147       0.0956       0.15480001
    2.17672396]
 [ 53.66851332   0.76019996   0.94400007   0.73349994   0.87970001
    2.67202449]
 [ 25.01514711   0.78140002   0.87859994   0.70920002   0.85320008
    2.64759517]
 ...
 [ 87.95281315   0.8016001    0.93110001   0.76570004   0.8653
    2.06632137]
 [-25.29356505   0.0816       0.2287       0.0873       0.1928
    2.44988942]
 [ 50.23339629   0.47489998   0.63759995   0.43979999   0.55649996
    2.07449794]][0m
[37m[1m[2023-07-11 06:18:15,302][233954] Max Reward on eval: 262.2945344972424[0m
[37m[1m[2023-07-11 06:18:15,302][233954] Min Reward on eval: -33.690729784709404[0m
[37m[1m[2023-07-11 06:18:15,303][233954] Mean Reward across all agents: 60.15377010862411[0m
[37m[1m[2023-07-11 06:18:15,303][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:18:15,306][233954] mean_value=-113.63385633201763, max_value=578.4561722967774[0m
[37m[1m[2023-07-11 06:18:15,309][233954] New mean coefficients: [[ 7.870418    1.1048877   4.020598   -0.52664685 -3.0393047  -2.099351  ]][0m
[37m[1m[2023-07-11 06:18:15,310][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:18:24,280][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 06:18:24,281][233954] FPS: 428153.54[0m
[36m[2023-07-11 06:18:24,283][233954] itr=479, itrs=2000, Progress: 23.95%[0m
[36m[2023-07-11 06:18:35,864][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 06:18:35,865][233954] FPS: 333887.24[0m
[36m[2023-07-11 06:18:40,183][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:18:40,183][233954] Reward + Measures: [[26.75465763  0.07350833  0.13569     0.081794    0.11338733  1.73815978]][0m
[37m[1m[2023-07-11 06:18:40,184][233954] Max Reward on eval: 26.754657633881976[0m
[37m[1m[2023-07-11 06:18:40,184][233954] Min Reward on eval: 26.754657633881976[0m
[37m[1m[2023-07-11 06:18:40,184][233954] Mean Reward across all agents: 26.754657633881976[0m
[37m[1m[2023-07-11 06:18:40,184][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:18:45,217][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:18:45,217][233954] Reward + Measures: [[-79.3360781    0.1693       0.49230003   0.1332       0.31500003
    2.2640655 ]
 [-23.75025644   0.18400002   0.43409997   0.1152       0.29809999
    2.33049107]
 [  4.52242781   0.1459       0.37149999   0.14189999   0.30160001
    2.21369934]
 ...
 [  3.74169293   0.1645       0.3757       0.10830001   0.2811
    2.25530934]
 [-35.39761742   0.18689999   0.36240003   0.1107       0.2552
    2.45973563]
 [-40.62886157   0.2493       0.53949994   0.1186       0.40229997
    2.36461449]][0m
[37m[1m[2023-07-11 06:18:45,218][233954] Max Reward on eval: 196.62727513927967[0m
[37m[1m[2023-07-11 06:18:45,218][233954] Min Reward on eval: -111.85095301363617[0m
[37m[1m[2023-07-11 06:18:45,218][233954] Mean Reward across all agents: 17.550782874238177[0m
[37m[1m[2023-07-11 06:18:45,218][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:18:45,221][233954] mean_value=-226.5563550658197, max_value=482.4580112903239[0m
[37m[1m[2023-07-11 06:18:45,223][233954] New mean coefficients: [[ 8.076259   2.227423   5.245493  -1.4587896 -1.4772434 -1.0184864]][0m
[37m[1m[2023-07-11 06:18:45,224][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:18:54,176][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 06:18:54,177][233954] FPS: 429032.89[0m
[36m[2023-07-11 06:18:54,179][233954] itr=480, itrs=2000, Progress: 24.00%[0m
[37m[1m[2023-07-11 06:22:16,902][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000460[0m
[36m[2023-07-11 06:22:28,970][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 06:22:28,970][233954] FPS: 334062.45[0m
[36m[2023-07-11 06:22:33,208][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:22:33,209][233954] Reward + Measures: [[31.74964234  0.07772233  0.13644633  0.084751    0.11625067  1.72421706]][0m
[37m[1m[2023-07-11 06:22:33,209][233954] Max Reward on eval: 31.749642341980177[0m
[37m[1m[2023-07-11 06:22:33,209][233954] Min Reward on eval: 31.749642341980177[0m
[37m[1m[2023-07-11 06:22:33,209][233954] Mean Reward across all agents: 31.749642341980177[0m
[37m[1m[2023-07-11 06:22:33,210][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:22:38,033][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:22:38,033][233954] Reward + Measures: [[140.6101898    0.17489998   0.21510001   0.093        0.2177
    3.41250992]
 [  6.3366593    0.07179999   0.10780001   0.1013       0.1184
    3.20027089]
 [ 93.01711906   0.0532       0.0781       0.1159       0.16470002
    3.34179378]
 ...
 [  3.30777376   0.0913       0.12169999   0.0644       0.0838
    3.30441022]
 [226.52085357   0.32380003   0.4515       0.15880001   0.46040002
    3.65039754]
 [ 96.08328493   0.1366       0.33990002   0.2647       0.37369999
    3.52999663]][0m
[37m[1m[2023-07-11 06:22:38,034][233954] Max Reward on eval: 444.8297987129539[0m
[37m[1m[2023-07-11 06:22:38,034][233954] Min Reward on eval: -210.077169922553[0m
[37m[1m[2023-07-11 06:22:38,034][233954] Mean Reward across all agents: 76.41616972672539[0m
[37m[1m[2023-07-11 06:22:38,034][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:22:38,039][233954] mean_value=-120.6025845517453, max_value=725.2685094609112[0m
[37m[1m[2023-07-11 06:22:38,041][233954] New mean coefficients: [[ 5.666897   1.9038817  1.6466227 -2.461275  -1.741298  -2.5323923]][0m
[37m[1m[2023-07-11 06:22:38,042][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:22:47,056][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 06:22:47,056][233954] FPS: 426086.82[0m
[36m[2023-07-11 06:22:47,059][233954] itr=481, itrs=2000, Progress: 24.05%[0m
[36m[2023-07-11 06:22:58,653][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 06:22:58,653][233954] FPS: 333499.26[0m
[36m[2023-07-11 06:23:02,967][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:23:02,967][233954] Reward + Measures: [[36.14402521  0.079697    0.13102032  0.084124    0.11640967  1.70552504]][0m
[37m[1m[2023-07-11 06:23:02,967][233954] Max Reward on eval: 36.14402521292838[0m
[37m[1m[2023-07-11 06:23:02,968][233954] Min Reward on eval: 36.14402521292838[0m
[37m[1m[2023-07-11 06:23:02,968][233954] Mean Reward across all agents: 36.14402521292838[0m
[37m[1m[2023-07-11 06:23:02,968][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:23:07,980][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:23:07,981][233954] Reward + Measures: [[ 95.09296889   0.71890002   0.90039998   0.67910004   0.84160006
    2.93328571]
 [ 64.18648176   0.80200005   0.85509998   0.78479999   0.82989997
    3.3122673 ]
 [113.93516603   0.41910002   0.79500002   0.3777       0.65970004
    2.86467195]
 ...
 [112.19125787   0.56370002   0.87970012   0.53619999   0.79709995
    2.63480163]
 [ 53.16215181   0.76540005   0.903        0.71709996   0.85000002
    3.02911067]
 [ 35.20263697   0.0774       0.30310002   0.1513       0.30700001
    2.48774338]][0m
[37m[1m[2023-07-11 06:23:07,981][233954] Max Reward on eval: 344.02707028072325[0m
[37m[1m[2023-07-11 06:23:07,982][233954] Min Reward on eval: -69.7923394870013[0m
[37m[1m[2023-07-11 06:23:07,982][233954] Mean Reward across all agents: 67.23572889140118[0m
[37m[1m[2023-07-11 06:23:07,982][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:23:07,986][233954] mean_value=-88.45583010401587, max_value=481.5785774519836[0m
[37m[1m[2023-07-11 06:23:07,989][233954] New mean coefficients: [[ 5.9197655  1.9748446  1.2685573 -1.6471325 -1.7598962 -2.4784381]][0m
[37m[1m[2023-07-11 06:23:07,990][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:23:17,074][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 06:23:17,074][233954] FPS: 422799.83[0m
[36m[2023-07-11 06:23:17,076][233954] itr=482, itrs=2000, Progress: 24.10%[0m
[36m[2023-07-11 06:23:28,712][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 06:23:28,712][233954] FPS: 332461.74[0m
[36m[2023-07-11 06:23:33,036][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:23:33,036][233954] Reward + Measures: [[35.98287997  0.08136266  0.13595967  0.08524933  0.11817233  1.6938982 ]][0m
[37m[1m[2023-07-11 06:23:33,036][233954] Max Reward on eval: 35.982879969709046[0m
[37m[1m[2023-07-11 06:23:33,037][233954] Min Reward on eval: 35.982879969709046[0m
[37m[1m[2023-07-11 06:23:33,037][233954] Mean Reward across all agents: 35.982879969709046[0m
[37m[1m[2023-07-11 06:23:33,037][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:23:37,962][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:23:37,968][233954] Reward + Measures: [[ 36.05021285   0.19669999   0.37320003   0.19600001   0.32320002
    2.28366542]
 [ -5.49016215   0.17710002   0.3788       0.1561       0.28449997
    2.24694681]
 [  3.32659426   0.1957       0.35520002   0.19710001   0.33629999
    2.17694855]
 ...
 [ 16.94258322   0.20580001   0.52419996   0.17120001   0.43240005
    2.28084278]
 [142.32749844   0.26269999   0.83830005   0.2588       0.68160003
    2.5933187 ]
 [ 63.29375457   0.27940002   0.36960003   0.2368       0.33630002
    2.30007267]][0m
[37m[1m[2023-07-11 06:23:37,969][233954] Max Reward on eval: 164.61226435927674[0m
[37m[1m[2023-07-11 06:23:37,969][233954] Min Reward on eval: -160.648425917048[0m
[37m[1m[2023-07-11 06:23:37,969][233954] Mean Reward across all agents: 35.49606736700949[0m
[37m[1m[2023-07-11 06:23:37,969][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:23:37,972][233954] mean_value=-221.0911105771815, max_value=595.339224280119[0m
[37m[1m[2023-07-11 06:23:37,975][233954] New mean coefficients: [[ 4.7699537   0.64050484 -0.30830503 -3.7119198  -2.99797    -4.722349  ]][0m
[37m[1m[2023-07-11 06:23:37,976][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:23:46,956][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 06:23:46,956][233954] FPS: 427693.53[0m
[36m[2023-07-11 06:23:46,958][233954] itr=483, itrs=2000, Progress: 24.15%[0m
[36m[2023-07-11 06:23:58,604][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 06:23:58,604][233954] FPS: 332045.94[0m
[36m[2023-07-11 06:24:02,811][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:24:02,811][233954] Reward + Measures: [[38.60760453  0.07723833  0.12905633  0.08552866  0.11347433  1.68839121]][0m
[37m[1m[2023-07-11 06:24:02,811][233954] Max Reward on eval: 38.607604531732235[0m
[37m[1m[2023-07-11 06:24:02,812][233954] Min Reward on eval: 38.607604531732235[0m
[37m[1m[2023-07-11 06:24:02,812][233954] Mean Reward across all agents: 38.607604531732235[0m
[37m[1m[2023-07-11 06:24:02,812][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:24:07,748][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:24:07,748][233954] Reward + Measures: [[ 14.22146289   0.2026       0.38330001   0.1532       0.34040001
    2.16888475]
 [ 80.30876462   0.22260001   0.46450001   0.182        0.42060003
    2.27191997]
 [165.63656504   0.12529999   0.30080003   0.133        0.2638
    2.66409349]
 ...
 [106.42647405   0.15740001   0.47870001   0.1596       0.38990003
    2.47141337]
 [ 28.64599476   0.0778       0.21680002   0.0902       0.17230001
    2.08135581]
 [ 31.20523016   0.198        0.49429998   0.18720001   0.41529998
    2.25472617]][0m
[37m[1m[2023-07-11 06:24:07,749][233954] Max Reward on eval: 283.4449490668252[0m
[37m[1m[2023-07-11 06:24:07,749][233954] Min Reward on eval: -42.99975790604949[0m
[37m[1m[2023-07-11 06:24:07,749][233954] Mean Reward across all agents: 79.98596487205667[0m
[37m[1m[2023-07-11 06:24:07,749][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:24:07,752][233954] mean_value=-146.33389006743874, max_value=233.91152462639633[0m
[37m[1m[2023-07-11 06:24:07,755][233954] New mean coefficients: [[ 4.5135984   0.71396655 -0.4032423  -3.1754014  -3.0022945  -4.8915405 ]][0m
[37m[1m[2023-07-11 06:24:07,756][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:24:16,756][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 06:24:16,756][233954] FPS: 426755.50[0m
[36m[2023-07-11 06:24:16,758][233954] itr=484, itrs=2000, Progress: 24.20%[0m
[36m[2023-07-11 06:24:28,768][233954] train() took 11.92 seconds to complete[0m
[36m[2023-07-11 06:24:28,769][233954] FPS: 322048.07[0m
[36m[2023-07-11 06:24:33,090][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:24:33,090][233954] Reward + Measures: [[39.15653037  0.07835733  0.13273932  0.085762    0.11605733  1.67885482]][0m
[37m[1m[2023-07-11 06:24:33,090][233954] Max Reward on eval: 39.1565303716393[0m
[37m[1m[2023-07-11 06:24:33,091][233954] Min Reward on eval: 39.1565303716393[0m
[37m[1m[2023-07-11 06:24:33,091][233954] Mean Reward across all agents: 39.1565303716393[0m
[37m[1m[2023-07-11 06:24:33,091][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:24:38,325][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:24:38,325][233954] Reward + Measures: [[80.08088693  0.14329998  0.1969      0.0717      0.16500001  1.81944275]
 [33.12052273  0.14619999  0.30500001  0.13450001  0.27649999  2.44797969]
 [36.30907328  0.0602      0.1657      0.0775      0.12159999  2.09643102]
 ...
 [36.41989356  0.0547      0.11240001  0.0696      0.1032      1.87703073]
 [23.78598646  0.0414      0.14920001  0.0808      0.0762      1.98749542]
 [35.42799789  0.0612      0.123       0.06820001  0.0807      1.76738   ]][0m
[37m[1m[2023-07-11 06:24:38,326][233954] Max Reward on eval: 338.80724111781456[0m
[37m[1m[2023-07-11 06:24:38,326][233954] Min Reward on eval: -26.12092559142038[0m
[37m[1m[2023-07-11 06:24:38,326][233954] Mean Reward across all agents: 36.424917326490416[0m
[37m[1m[2023-07-11 06:24:38,327][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:24:38,328][233954] mean_value=-422.4732453319073, max_value=167.23341111644555[0m
[37m[1m[2023-07-11 06:24:38,330][233954] New mean coefficients: [[ 4.190224   -0.16147822 -1.0731411  -3.8487575  -3.9065597  -6.0960703 ]][0m
[37m[1m[2023-07-11 06:24:38,331][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:24:47,429][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 06:24:47,430][233954] FPS: 422144.80[0m
[36m[2023-07-11 06:24:47,432][233954] itr=485, itrs=2000, Progress: 24.25%[0m
[36m[2023-07-11 06:24:59,044][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 06:24:59,045][233954] FPS: 333010.32[0m
[36m[2023-07-11 06:25:03,286][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:25:03,287][233954] Reward + Measures: [[42.59024842  0.07940967  0.132153    0.08575133  0.11529767  1.65836322]][0m
[37m[1m[2023-07-11 06:25:03,287][233954] Max Reward on eval: 42.590248420065095[0m
[37m[1m[2023-07-11 06:25:03,287][233954] Min Reward on eval: 42.590248420065095[0m
[37m[1m[2023-07-11 06:25:03,287][233954] Mean Reward across all agents: 42.590248420065095[0m
[37m[1m[2023-07-11 06:25:03,288][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:25:08,303][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:25:08,304][233954] Reward + Measures: [[140.82114554   0.20130001   0.21430002   0.2572       0.3238
    3.05742192]
 [108.80741739   0.0161       0.98890001   0.49349999   0.98149997
    3.69142914]
 [119.35794925   0.0116       0.99370003   0.49429998   0.98199999
    3.7021699 ]
 ...
 [ 94.21209097   0.016        0.98699999   0.4632       0.96929997
    3.74253893]
 [128.71970079   0.2218       0.27070004   0.32440001   0.37110001
    2.82884383]
 [ 83.72535943   0.0267       0.97539997   0.51109999   0.97329998
    3.31925368]][0m
[37m[1m[2023-07-11 06:25:08,304][233954] Max Reward on eval: 640.076089861989[0m
[37m[1m[2023-07-11 06:25:08,304][233954] Min Reward on eval: -105.52127027604729[0m
[37m[1m[2023-07-11 06:25:08,305][233954] Mean Reward across all agents: 110.12871506759072[0m
[37m[1m[2023-07-11 06:25:08,305][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:25:08,309][233954] mean_value=-127.51501765481163, max_value=812.9554004264064[0m
[37m[1m[2023-07-11 06:25:08,311][233954] New mean coefficients: [[ 5.0738945  -0.21834283 -0.17794704 -2.8597236  -4.085546   -5.892384  ]][0m
[37m[1m[2023-07-11 06:25:08,312][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:25:17,399][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 06:25:17,399][233954] FPS: 422674.97[0m
[36m[2023-07-11 06:25:17,402][233954] itr=486, itrs=2000, Progress: 24.30%[0m
[36m[2023-07-11 06:25:29,151][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 06:25:29,151][233954] FPS: 329204.60[0m
[36m[2023-07-11 06:25:33,478][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:25:33,478][233954] Reward + Measures: [[41.55905759  0.07791767  0.13301733  0.08666933  0.113455    1.65595865]][0m
[37m[1m[2023-07-11 06:25:33,478][233954] Max Reward on eval: 41.559057587594296[0m
[37m[1m[2023-07-11 06:25:33,479][233954] Min Reward on eval: 41.559057587594296[0m
[37m[1m[2023-07-11 06:25:33,479][233954] Mean Reward across all agents: 41.559057587594296[0m
[37m[1m[2023-07-11 06:25:33,479][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:25:38,509][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:25:38,509][233954] Reward + Measures: [[-36.02986903   0.1894       0.23239999   0.20150001   0.25170001
    2.47266626]
 [-16.66159845   0.2545       0.23029999   0.23029999   0.30469999
    1.92222691]
 [ 23.31730211   0.21520002   0.26630002   0.17809999   0.29179999
    2.34061408]
 ...
 [-44.902599     0.1701       0.19460002   0.1807       0.21630001
    2.39307237]
 [114.19685584   0.1028       0.685        0.1771       0.57020003
    2.37200141]
 [ 39.09680504   0.26770002   0.35589999   0.2263       0.34310001
    2.33118773]][0m
[37m[1m[2023-07-11 06:25:38,510][233954] Max Reward on eval: 199.5298052046448[0m
[37m[1m[2023-07-11 06:25:38,510][233954] Min Reward on eval: -120.45595126841218[0m
[37m[1m[2023-07-11 06:25:38,510][233954] Mean Reward across all agents: 19.70500311120207[0m
[37m[1m[2023-07-11 06:25:38,510][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:25:38,513][233954] mean_value=-263.33833853436516, max_value=546.5054774286505[0m
[37m[1m[2023-07-11 06:25:38,515][233954] New mean coefficients: [[ 5.7634554  1.061898   1.2826983 -3.4656427 -2.3303165 -3.9256144]][0m
[37m[1m[2023-07-11 06:25:38,516][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:25:47,575][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 06:25:47,575][233954] FPS: 424007.54[0m
[36m[2023-07-11 06:25:47,577][233954] itr=487, itrs=2000, Progress: 24.35%[0m
[36m[2023-07-11 06:25:59,316][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 06:25:59,321][233954] FPS: 329406.23[0m
[36m[2023-07-11 06:26:03,549][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:26:03,549][233954] Reward + Measures: [[38.98188607  0.07749499  0.13387534  0.08896466  0.11538967  1.64519954]][0m
[37m[1m[2023-07-11 06:26:03,550][233954] Max Reward on eval: 38.98188607222626[0m
[37m[1m[2023-07-11 06:26:03,550][233954] Min Reward on eval: 38.98188607222626[0m
[37m[1m[2023-07-11 06:26:03,550][233954] Mean Reward across all agents: 38.98188607222626[0m
[37m[1m[2023-07-11 06:26:03,550][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:26:08,522][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:26:08,522][233954] Reward + Measures: [[-29.46657351   0.1478       0.18700001   0.15830001   0.18599999
    2.71508026]
 [ 95.2866288    0.22979999   0.1947       0.21210001   0.2516
    2.64563727]
 [  6.44747482   0.1362       0.17900001   0.164        0.18020001
    2.56645846]
 ...
 [-68.7425837    0.1321       0.20829999   0.14890002   0.17839999
    2.62721896]
 [162.35616683   0.11180001   0.53899997   0.2167       0.43940002
    2.30193377]
 [-31.62874127   0.15339999   0.17979999   0.15440001   0.1463
    2.85128951]][0m
[37m[1m[2023-07-11 06:26:08,523][233954] Max Reward on eval: 196.64115619435907[0m
[37m[1m[2023-07-11 06:26:08,523][233954] Min Reward on eval: -214.7110766839236[0m
[37m[1m[2023-07-11 06:26:08,523][233954] Mean Reward across all agents: 24.95749008283703[0m
[37m[1m[2023-07-11 06:26:08,523][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:26:08,525][233954] mean_value=-390.821603404212, max_value=182.6828621648738[0m
[37m[1m[2023-07-11 06:26:08,527][233954] New mean coefficients: [[ 3.960498   1.4089957 -1.9489766 -1.8288007 -2.2740586 -4.395968 ]][0m
[37m[1m[2023-07-11 06:26:08,528][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:26:17,528][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 06:26:17,528][233954] FPS: 426755.16[0m
[36m[2023-07-11 06:26:17,531][233954] itr=488, itrs=2000, Progress: 24.40%[0m
[36m[2023-07-11 06:26:29,248][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 06:26:29,249][233954] FPS: 330063.90[0m
[36m[2023-07-11 06:26:33,485][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:26:33,485][233954] Reward + Measures: [[40.9072566   0.07574367  0.12982666  0.08751133  0.10510898  1.6088196 ]][0m
[37m[1m[2023-07-11 06:26:33,485][233954] Max Reward on eval: 40.907256603649316[0m
[37m[1m[2023-07-11 06:26:33,486][233954] Min Reward on eval: 40.907256603649316[0m
[37m[1m[2023-07-11 06:26:33,486][233954] Mean Reward across all agents: 40.907256603649316[0m
[37m[1m[2023-07-11 06:26:33,486][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:26:38,459][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:26:38,460][233954] Reward + Measures: [[  -2.58051811    0.52090001    0.77540004    0.44630003    0.6766001
     3.28020978]
 [  36.28417088    0.3028        0.50690001    0.19780001    0.53740001
     2.88746023]
 [-125.63978577    0.5618        0.5977        0.10950001    0.65329999
     2.90847397]
 ...
 [  50.80170751    0.26229998    0.45959997    0.21510001    0.48410001
     2.66489553]
 [  29.8808932     0.67450005    0.88950008    0.63980001    0.83200008
     3.16864014]
 [   7.85384167    0.76940006    0.84730005    0.7324        0.83390009
     3.05795789]][0m
[37m[1m[2023-07-11 06:26:38,460][233954] Max Reward on eval: 678.7887535275892[0m
[37m[1m[2023-07-11 06:26:38,460][233954] Min Reward on eval: -460.6832724009408[0m
[37m[1m[2023-07-11 06:26:38,460][233954] Mean Reward across all agents: 29.183507787078742[0m
[37m[1m[2023-07-11 06:26:38,461][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:26:38,465][233954] mean_value=-155.9159614940053, max_value=784.6825659986586[0m
[37m[1m[2023-07-11 06:26:38,468][233954] New mean coefficients: [[ 4.347478   2.8762612 -1.7016277 -1.5002685 -0.8737172 -2.4796908]][0m
[37m[1m[2023-07-11 06:26:38,469][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:26:47,466][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 06:26:47,467][233954] FPS: 426851.14[0m
[36m[2023-07-11 06:26:47,469][233954] itr=489, itrs=2000, Progress: 24.45%[0m
[36m[2023-07-11 06:26:59,103][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 06:26:59,103][233954] FPS: 332361.19[0m
[36m[2023-07-11 06:27:03,416][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:27:03,416][233954] Reward + Measures: [[43.23113226  0.07584567  0.12816434  0.08674066  0.10615601  1.60645735]][0m
[37m[1m[2023-07-11 06:27:03,416][233954] Max Reward on eval: 43.231132259748975[0m
[37m[1m[2023-07-11 06:27:03,417][233954] Min Reward on eval: 43.231132259748975[0m
[37m[1m[2023-07-11 06:27:03,417][233954] Mean Reward across all agents: 43.231132259748975[0m
[37m[1m[2023-07-11 06:27:03,417][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:27:08,632][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:27:08,633][233954] Reward + Measures: [[ 95.49866902   0.18120001   0.27710003   0.1987       0.2818
    2.44816852]
 [-45.92755774   0.37549999   0.3917       0.33979997   0.40439996
    3.27129531]
 [152.90488864   0.0874       0.77630001   0.31810001   0.68729997
    2.86581111]
 ...
 [ 55.88689277   0.0512       0.91329998   0.33320001   0.88029999
    3.25134659]
 [ 85.67231693   0.199        0.50319999   0.20970002   0.44039997
    2.88763666]
 [127.71535921   0.0665       0.84460002   0.30380002   0.73699999
    3.09682727]][0m
[37m[1m[2023-07-11 06:27:08,633][233954] Max Reward on eval: 400.67019654847684[0m
[37m[1m[2023-07-11 06:27:08,633][233954] Min Reward on eval: -111.93995226779953[0m
[37m[1m[2023-07-11 06:27:08,634][233954] Mean Reward across all agents: 33.61705002497917[0m
[37m[1m[2023-07-11 06:27:08,634][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:27:08,637][233954] mean_value=-117.55590383502805, max_value=430.1356225583898[0m
[37m[1m[2023-07-11 06:27:08,639][233954] New mean coefficients: [[ 5.2043204   3.785794    0.06293249 -1.9882632  -0.0425064  -1.2378467 ]][0m
[37m[1m[2023-07-11 06:27:08,640][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:27:17,578][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 06:27:17,584][233954] FPS: 429721.96[0m
[36m[2023-07-11 06:27:17,586][233954] itr=490, itrs=2000, Progress: 24.50%[0m
[37m[1m[2023-07-11 06:30:31,018][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000470[0m
[36m[2023-07-11 06:30:43,279][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 06:30:43,279][233954] FPS: 328922.92[0m
[36m[2023-07-11 06:30:47,231][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:30:47,231][233954] Reward + Measures: [[44.71246712  0.07627733  0.13026066  0.084879    0.10351133  1.59994543]][0m
[37m[1m[2023-07-11 06:30:47,231][233954] Max Reward on eval: 44.71246712251593[0m
[37m[1m[2023-07-11 06:30:47,232][233954] Min Reward on eval: 44.71246712251593[0m
[37m[1m[2023-07-11 06:30:47,232][233954] Mean Reward across all agents: 44.71246712251593[0m
[37m[1m[2023-07-11 06:30:47,232][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:30:52,195][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:30:52,196][233954] Reward + Measures: [[-18.8133474    0.88140005   0.87639999   0.86879998   0.89960003
    3.80550742]
 [ 65.53538447   0.68159997   0.75650007   0.64840001   0.73080003
    3.16257167]
 [  3.18222902   0.90030003   0.91160005   0.8818       0.90580004
    3.43496513]
 ...
 [ -6.73138882   0.94160002   0.9483       0.9224       0.94139999
    3.47042322]
 [-29.31500411   0.9382       0.93689996   0.92570001   0.93629998
    3.77503705]
 [ 64.69988935   0.1512       0.68989998   0.35260001   0.63050002
    2.96672821]][0m
[37m[1m[2023-07-11 06:30:52,196][233954] Max Reward on eval: 237.14230346838013[0m
[37m[1m[2023-07-11 06:30:52,196][233954] Min Reward on eval: -205.76752127292565[0m
[37m[1m[2023-07-11 06:30:52,197][233954] Mean Reward across all agents: -8.300619346819474[0m
[37m[1m[2023-07-11 06:30:52,197][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:30:52,199][233954] mean_value=-115.63600573853182, max_value=319.9422879984408[0m
[37m[1m[2023-07-11 06:30:52,201][233954] New mean coefficients: [[ 5.2623677   3.4621463   0.08340603 -0.8514414  -0.7976391  -1.5069224 ]][0m
[37m[1m[2023-07-11 06:30:52,202][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:31:01,186][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 06:31:01,186][233954] FPS: 427534.79[0m
[36m[2023-07-11 06:31:01,188][233954] itr=491, itrs=2000, Progress: 24.55%[0m
[36m[2023-07-11 06:31:12,838][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 06:31:12,839][233954] FPS: 332018.09[0m
[36m[2023-07-11 06:31:17,060][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:31:17,061][233954] Reward + Measures: [[47.288821    0.07973033  0.12918967  0.08738734  0.10908233  1.59373415]][0m
[37m[1m[2023-07-11 06:31:17,061][233954] Max Reward on eval: 47.28882100371931[0m
[37m[1m[2023-07-11 06:31:17,061][233954] Min Reward on eval: 47.28882100371931[0m
[37m[1m[2023-07-11 06:31:17,062][233954] Mean Reward across all agents: 47.28882100371931[0m
[37m[1m[2023-07-11 06:31:17,062][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:31:21,962][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:31:21,963][233954] Reward + Measures: [[ 57.50858519   0.20650001   0.48400003   0.27000004   0.45559999
    3.62824988]
 [328.61897277   0.3962       0.7353       0.19499999   0.68959999
    3.65829158]
 [  9.53439755   0.09500001   0.07160001   0.0927       0.111
    2.54144263]
 ...
 [ 95.46601177   0.2102       0.28509998   0.1464       0.29640001
    3.41239905]
 [ 25.76995711   0.0958       0.24369998   0.17440002   0.26040003
    3.54866099]
 [115.39625897   0.15359999   0.2622       0.1288       0.25100002
    3.2287662 ]][0m
[37m[1m[2023-07-11 06:31:21,963][233954] Max Reward on eval: 439.42365924827754[0m
[37m[1m[2023-07-11 06:31:21,963][233954] Min Reward on eval: -261.1183389386162[0m
[37m[1m[2023-07-11 06:31:21,964][233954] Mean Reward across all agents: 116.28021883899045[0m
[37m[1m[2023-07-11 06:31:21,964][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:31:21,970][233954] mean_value=-61.42911100440993, max_value=604.2169681217522[0m
[37m[1m[2023-07-11 06:31:21,973][233954] New mean coefficients: [[ 6.5690756   4.1568375   1.611443    2.1154783  -0.17728418 -0.35444295]][0m
[37m[1m[2023-07-11 06:31:21,974][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:31:30,908][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 06:31:30,908][233954] FPS: 429909.34[0m
[36m[2023-07-11 06:31:30,910][233954] itr=492, itrs=2000, Progress: 24.60%[0m
[36m[2023-07-11 06:31:42,449][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 06:31:42,450][233954] FPS: 335157.81[0m
[36m[2023-07-11 06:31:46,652][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:31:46,652][233954] Reward + Measures: [[47.10303765  0.07497966  0.124501    0.08069233  0.10210834  1.60114837]][0m
[37m[1m[2023-07-11 06:31:46,653][233954] Max Reward on eval: 47.10303764792089[0m
[37m[1m[2023-07-11 06:31:46,653][233954] Min Reward on eval: 47.10303764792089[0m
[37m[1m[2023-07-11 06:31:46,653][233954] Mean Reward across all agents: 47.10303764792089[0m
[37m[1m[2023-07-11 06:31:46,653][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:31:51,608][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:31:51,609][233954] Reward + Measures: [[ 35.67222047   0.79769993   0.96259993   0.78930008   0.87769997
    2.95491767]
 [131.89395769   0.10640001   0.361        0.24859999   0.37900001
    3.16092157]
 [ -2.89795737   0.98280001   0.986        0.98730004   0.99080002
    3.90445876]
 ...
 [198.85028887   0.0921       0.65090007   0.46679997   0.63850003
    2.85662103]
 [ 45.56742455   0.8125       0.95880002   0.79809999   0.88160002
    3.18069458]
 [ -4.96006207   0.97530001   0.97909993   0.98239994   0.98629999
    3.88944221]][0m
[37m[1m[2023-07-11 06:31:51,609][233954] Max Reward on eval: 339.08607623912394[0m
[37m[1m[2023-07-11 06:31:51,609][233954] Min Reward on eval: -55.48838405823335[0m
[37m[1m[2023-07-11 06:31:51,609][233954] Mean Reward across all agents: 59.18998277786718[0m
[37m[1m[2023-07-11 06:31:51,610][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:31:51,613][233954] mean_value=-85.04075914980193, max_value=679.2934574251476[0m
[37m[1m[2023-07-11 06:31:51,615][233954] New mean coefficients: [[ 5.108241    4.043496   -1.2365298   1.7247157  -0.08344413 -0.8146564 ]][0m
[37m[1m[2023-07-11 06:31:51,616][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:32:00,540][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 06:32:00,540][233954] FPS: 430383.78[0m
[36m[2023-07-11 06:32:00,542][233954] itr=493, itrs=2000, Progress: 24.65%[0m
[36m[2023-07-11 06:32:12,046][233954] train() took 11.42 seconds to complete[0m
[36m[2023-07-11 06:32:12,047][233954] FPS: 336274.22[0m
[36m[2023-07-11 06:32:16,330][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:32:16,330][233954] Reward + Measures: [[52.05707377  0.083417    0.122486    0.08631099  0.11367434  1.61353576]][0m
[37m[1m[2023-07-11 06:32:16,331][233954] Max Reward on eval: 52.05707376722165[0m
[37m[1m[2023-07-11 06:32:16,331][233954] Min Reward on eval: 52.05707376722165[0m
[37m[1m[2023-07-11 06:32:16,331][233954] Mean Reward across all agents: 52.05707376722165[0m
[37m[1m[2023-07-11 06:32:16,331][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:32:21,348][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:32:21,349][233954] Reward + Measures: [[89.64271394  0.3795      0.76609993  0.3989      0.65860003  3.0353148 ]
 [64.00198332  0.4368      0.80179995  0.45840001  0.71059996  3.08264804]
 [ 8.24615779  0.08889999  0.20920001  0.0974      0.16159999  2.3530426 ]
 ...
 [16.49331011  0.0831      0.1965      0.11509999  0.1699      2.33863521]
 [20.51723494  0.0639      0.08660001  0.0741      0.14670001  2.24245811]
 [46.22915481  0.373       0.84200001  0.46219999  0.8096      3.19912362]][0m
[37m[1m[2023-07-11 06:32:21,349][233954] Max Reward on eval: 369.32912090132015[0m
[37m[1m[2023-07-11 06:32:21,349][233954] Min Reward on eval: -35.32699920479208[0m
[37m[1m[2023-07-11 06:32:21,349][233954] Mean Reward across all agents: 75.25185764698708[0m
[37m[1m[2023-07-11 06:32:21,350][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:32:21,356][233954] mean_value=-11.132344090366981, max_value=600.4328039502259[0m
[37m[1m[2023-07-11 06:32:21,358][233954] New mean coefficients: [[ 7.1589804   4.2148275   2.3673458   3.3252597  -0.09932736 -0.38654652]][0m
[37m[1m[2023-07-11 06:32:21,359][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:32:30,472][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 06:32:30,472][233954] FPS: 421466.71[0m
[36m[2023-07-11 06:32:30,474][233954] itr=494, itrs=2000, Progress: 24.70%[0m
[36m[2023-07-11 06:32:42,223][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 06:32:42,223][233954] FPS: 329203.57[0m
[36m[2023-07-11 06:32:46,504][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:32:46,504][233954] Reward + Measures: [[55.91847308  0.08615232  0.12116267  0.08503734  0.11446466  1.62579525]][0m
[37m[1m[2023-07-11 06:32:46,504][233954] Max Reward on eval: 55.918473075046904[0m
[37m[1m[2023-07-11 06:32:46,504][233954] Min Reward on eval: 55.918473075046904[0m
[37m[1m[2023-07-11 06:32:46,505][233954] Mean Reward across all agents: 55.918473075046904[0m
[37m[1m[2023-07-11 06:32:46,505][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:32:51,430][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:32:51,431][233954] Reward + Measures: [[ 80.52313623   0.0271       0.98219997   0.4384       0.9665001
    3.51201105]
 [ 68.75864002   0.087        0.47890002   0.17999999   0.40599999
    2.77288365]
 [100.15063382   0.027        0.99279994   0.48129997   0.97440004
    3.37493682]
 ...
 [403.88926863   0.0454       0.92080003   0.5693       0.82910007
    2.70001602]
 [ 72.21275711   0.0376       0.93950003   0.41030002   0.91330004
    3.6361661 ]
 [-27.73519532   0.0625       0.1182       0.08090001   0.0953
    2.58822417]][0m
[37m[1m[2023-07-11 06:32:51,431][233954] Max Reward on eval: 523.7636528015137[0m
[37m[1m[2023-07-11 06:32:51,431][233954] Min Reward on eval: -96.32110084714368[0m
[37m[1m[2023-07-11 06:32:51,431][233954] Mean Reward across all agents: 76.40682570294693[0m
[37m[1m[2023-07-11 06:32:51,432][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:32:51,434][233954] mean_value=-170.4890581528904, max_value=307.2988352812108[0m
[37m[1m[2023-07-11 06:32:51,436][233954] New mean coefficients: [[ 6.1838713  3.1408577  1.4544148  3.777507  -1.646067  -2.4300313]][0m
[37m[1m[2023-07-11 06:32:51,437][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:33:00,316][233954] train() took 8.88 seconds to complete[0m
[36m[2023-07-11 06:33:00,316][233954] FPS: 432560.69[0m
[36m[2023-07-11 06:33:00,318][233954] itr=495, itrs=2000, Progress: 24.75%[0m
[36m[2023-07-11 06:33:11,896][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 06:33:11,897][233954] FPS: 334008.01[0m
[36m[2023-07-11 06:33:16,120][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:33:16,121][233954] Reward + Measures: [[55.77351797  0.086586    0.12740399  0.08608433  0.10979766  1.61569202]][0m
[37m[1m[2023-07-11 06:33:16,121][233954] Max Reward on eval: 55.773517968326885[0m
[37m[1m[2023-07-11 06:33:16,121][233954] Min Reward on eval: 55.773517968326885[0m
[37m[1m[2023-07-11 06:33:16,121][233954] Mean Reward across all agents: 55.773517968326885[0m
[37m[1m[2023-07-11 06:33:16,122][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:33:21,077][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:33:21,077][233954] Reward + Measures: [[164.89113619   0.21259999   0.4364       0.14229999   0.39969999
    2.39371729]
 [ 93.96728126   0.1224       0.41130003   0.12660001   0.32400003
    2.09084702]
 [-67.64360221   0.0764       0.069        0.0848       0.11669999
    3.19660258]
 ...
 [ 42.21528704   0.21929999   0.38320002   0.2102       0.39980003
    2.57998133]
 [  0.62769346   0.069        0.0826       0.07120001   0.0978
    2.87853909]
 [ 36.82610374   0.17050001   0.2525       0.1495       0.28220001
    2.18675327]][0m
[37m[1m[2023-07-11 06:33:21,077][233954] Max Reward on eval: 219.07097349464894[0m
[37m[1m[2023-07-11 06:33:21,078][233954] Min Reward on eval: -173.46779904384167[0m
[37m[1m[2023-07-11 06:33:21,078][233954] Mean Reward across all agents: 53.50502607881196[0m
[37m[1m[2023-07-11 06:33:21,078][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:33:21,080][233954] mean_value=-256.3571579374368, max_value=486.2154990821052[0m
[37m[1m[2023-07-11 06:33:21,083][233954] New mean coefficients: [[ 5.861557   4.1574354  1.0572562  3.1701603 -0.4495567 -1.6213106]][0m
[37m[1m[2023-07-11 06:33:21,084][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:33:30,058][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 06:33:30,059][233954] FPS: 427945.82[0m
[36m[2023-07-11 06:33:30,061][233954] itr=496, itrs=2000, Progress: 24.80%[0m
[36m[2023-07-11 06:33:41,578][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 06:33:41,579][233954] FPS: 335749.31[0m
[36m[2023-07-11 06:33:45,812][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:33:45,813][233954] Reward + Measures: [[65.35466561  0.09331866  0.12790667  0.08926534  0.117598    1.61762714]][0m
[37m[1m[2023-07-11 06:33:45,813][233954] Max Reward on eval: 65.35466561462935[0m
[37m[1m[2023-07-11 06:33:45,813][233954] Min Reward on eval: 65.35466561462935[0m
[37m[1m[2023-07-11 06:33:45,814][233954] Mean Reward across all agents: 65.35466561462935[0m
[37m[1m[2023-07-11 06:33:45,814][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:33:50,983][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:33:50,984][233954] Reward + Measures: [[ 86.42425476   0.11099999   0.1305       0.1141       0.13069999
    3.16692519]
 [ 42.05664685   0.191        0.75279999   0.0963       0.76249999
    3.14270449]
 [-84.00801307   0.27870002   0.74370003   0.1964       0.71609998
    3.55130363]
 ...
 [ 76.2785411    0.13239999   0.26229998   0.11310001   0.24730001
    3.31217384]
 [ 55.13043819   0.10950001   0.16580001   0.10640001   0.1346
    3.14000249]
 [411.66346075   0.27860001   0.85200006   0.44220001   0.83120006
    3.34287047]][0m
[37m[1m[2023-07-11 06:33:50,984][233954] Max Reward on eval: 571.6730232009664[0m
[37m[1m[2023-07-11 06:33:50,984][233954] Min Reward on eval: -505.48943540398034[0m
[37m[1m[2023-07-11 06:33:50,985][233954] Mean Reward across all agents: 80.95876949721759[0m
[37m[1m[2023-07-11 06:33:50,985][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:33:50,993][233954] mean_value=9.734398268303291, max_value=758.4361791372299[0m
[37m[1m[2023-07-11 06:33:50,996][233954] New mean coefficients: [[ 8.8310375  3.4795134  5.1256967  4.025523  -1.3107908 -0.996235 ]][0m
[37m[1m[2023-07-11 06:33:50,997][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:34:00,022][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 06:34:00,028][233954] FPS: 425554.61[0m
[36m[2023-07-11 06:34:00,030][233954] itr=497, itrs=2000, Progress: 24.85%[0m
[36m[2023-07-11 06:34:12,115][233954] train() took 12.00 seconds to complete[0m
[36m[2023-07-11 06:34:12,115][233954] FPS: 320030.62[0m
[36m[2023-07-11 06:34:16,434][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:34:16,434][233954] Reward + Measures: [[62.29399588  0.09407967  0.13100998  0.09351733  0.11775867  1.62003076]][0m
[37m[1m[2023-07-11 06:34:16,435][233954] Max Reward on eval: 62.29399588197491[0m
[37m[1m[2023-07-11 06:34:16,435][233954] Min Reward on eval: 62.29399588197491[0m
[37m[1m[2023-07-11 06:34:16,435][233954] Mean Reward across all agents: 62.29399588197491[0m
[37m[1m[2023-07-11 06:34:16,435][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:34:21,390][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:34:21,391][233954] Reward + Measures: [[ 99.6405908    0.96569997   0.97620004   0.94760001   0.97010005
    3.39465022]
 [ 25.75102487   0.80159998   0.91230005   0.79180002   0.85610002
    3.40408587]
 [183.52725442   0.23459999   0.54010004   0.41850001   0.62960005
    3.10151482]
 ...
 [113.49019625   0.47770005   0.66619998   0.49689999   0.5948
    3.37221837]
 [-48.89514961   0.84899998   0.87849998   0.84829998   0.87190002
    3.48065162]
 [124.66009028   0.21430002   0.34640002   0.19550002   0.3527
    2.4480226 ]][0m
[37m[1m[2023-07-11 06:34:21,391][233954] Max Reward on eval: 508.57453587278724[0m
[37m[1m[2023-07-11 06:34:21,391][233954] Min Reward on eval: -89.05546284932643[0m
[37m[1m[2023-07-11 06:34:21,391][233954] Mean Reward across all agents: 53.73013995509732[0m
[37m[1m[2023-07-11 06:34:21,391][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:34:21,395][233954] mean_value=-55.33389639623694, max_value=973.1172329217568[0m
[37m[1m[2023-07-11 06:34:21,398][233954] New mean coefficients: [[ 9.209675    4.320448    5.6176405   4.1503625  -0.21778941  0.34773076]][0m
[37m[1m[2023-07-11 06:34:21,399][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:34:30,417][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 06:34:30,417][233954] FPS: 425895.43[0m
[36m[2023-07-11 06:34:30,419][233954] itr=498, itrs=2000, Progress: 24.90%[0m
[36m[2023-07-11 06:34:42,387][233954] train() took 11.88 seconds to complete[0m
[36m[2023-07-11 06:34:42,388][233954] FPS: 323178.42[0m
[36m[2023-07-11 06:34:46,621][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:34:46,621][233954] Reward + Measures: [[65.18765617  0.09514134  0.13593633  0.09544666  0.11714567  1.60913348]][0m
[37m[1m[2023-07-11 06:34:46,622][233954] Max Reward on eval: 65.18765616603159[0m
[37m[1m[2023-07-11 06:34:46,622][233954] Min Reward on eval: 65.18765616603159[0m
[37m[1m[2023-07-11 06:34:46,622][233954] Mean Reward across all agents: 65.18765616603159[0m
[37m[1m[2023-07-11 06:34:46,622][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:34:51,562][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:34:51,563][233954] Reward + Measures: [[ 73.26189632   0.67550004   0.66049999   0.62079996   0.63749999
    3.26681495]
 [-16.32521047   0.2911       0.34119999   0.22119999   0.36310002
    2.51113534]
 [ 34.98713965   0.2494       0.31990001   0.22989999   0.33580002
    3.06740069]
 ...
 [506.81099039   0.30779999   0.77419996   0.37270001   0.81159991
    3.62520766]
 [ 16.69153974   0.18610001   0.25430003   0.18719999   0.28739998
    3.15004611]
 [ 34.3228876    0.34999999   0.42799997   0.32839999   0.46430001
    2.97347069]][0m
[37m[1m[2023-07-11 06:34:51,563][233954] Max Reward on eval: 703.6564370006788[0m
[37m[1m[2023-07-11 06:34:51,564][233954] Min Reward on eval: -84.3221937247552[0m
[37m[1m[2023-07-11 06:34:51,564][233954] Mean Reward across all agents: 98.5760263672615[0m
[37m[1m[2023-07-11 06:34:51,564][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:34:51,569][233954] mean_value=-59.09786040804897, max_value=796.795299933572[0m
[37m[1m[2023-07-11 06:34:51,572][233954] New mean coefficients: [[8.031987  5.2930703 3.2901342 0.9104979 1.6900256 1.8484279]][0m
[37m[1m[2023-07-11 06:34:51,573][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:35:00,483][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 06:35:00,483][233954] FPS: 431045.94[0m
[36m[2023-07-11 06:35:00,486][233954] itr=499, itrs=2000, Progress: 24.95%[0m
[36m[2023-07-11 06:35:12,349][233954] train() took 11.78 seconds to complete[0m
[36m[2023-07-11 06:35:12,349][233954] FPS: 325999.49[0m
[36m[2023-07-11 06:35:16,619][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:35:16,619][233954] Reward + Measures: [[64.24682     0.09231433  0.13509166  0.09257033  0.11485233  1.62471676]][0m
[37m[1m[2023-07-11 06:35:16,620][233954] Max Reward on eval: 64.24682000339625[0m
[37m[1m[2023-07-11 06:35:16,620][233954] Min Reward on eval: 64.24682000339625[0m
[37m[1m[2023-07-11 06:35:16,620][233954] Mean Reward across all agents: 64.24682000339625[0m
[37m[1m[2023-07-11 06:35:16,620][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:35:21,604][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:35:21,604][233954] Reward + Measures: [[-24.96012641   0.66100007   0.68260002   0.68180001   0.57789999
    2.87388492]
 [ 82.69813889   0.12220001   0.90939999   0.30779999   0.87390006
    3.33987737]
 [ 13.39862954   0.41849995   0.34540001   0.25209996   0.4677
    3.42987514]
 ...
 [ 67.15056238   0.30110002   0.43010002   0.2445       0.4113
    3.09346652]
 [-15.21099924   0.45919999   0.36139998   0.39840004   0.50690001
    2.97279406]
 [ 85.01448166   0.39970002   0.34660003   0.39410001   0.21700001
    2.80476022]][0m
[37m[1m[2023-07-11 06:35:21,604][233954] Max Reward on eval: 672.4370651414617[0m
[37m[1m[2023-07-11 06:35:21,605][233954] Min Reward on eval: -128.62049618586897[0m
[37m[1m[2023-07-11 06:35:21,605][233954] Mean Reward across all agents: 62.21705834852531[0m
[37m[1m[2023-07-11 06:35:21,605][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:35:21,610][233954] mean_value=-115.2504753211518, max_value=579.0123928562738[0m
[37m[1m[2023-07-11 06:35:21,612][233954] New mean coefficients: [[8.721304  4.892225  5.091713  0.6654849 1.1212859 1.7994643]][0m
[37m[1m[2023-07-11 06:35:21,613][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:35:30,615][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 06:35:30,616][233954] FPS: 426646.39[0m
[36m[2023-07-11 06:35:30,618][233954] itr=500, itrs=2000, Progress: 25.00%[0m
[37m[1m[2023-07-11 06:38:48,735][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000480[0m
[36m[2023-07-11 06:39:00,992][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 06:39:00,992][233954] FPS: 330048.98[0m
[36m[2023-07-11 06:39:05,251][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:39:05,251][233954] Reward + Measures: [[67.43180925  0.09932467  0.13953967  0.09861333  0.12372701  1.65407407]][0m
[37m[1m[2023-07-11 06:39:05,252][233954] Max Reward on eval: 67.43180924744244[0m
[37m[1m[2023-07-11 06:39:05,252][233954] Min Reward on eval: 67.43180924744244[0m
[37m[1m[2023-07-11 06:39:05,252][233954] Mean Reward across all agents: 67.43180924744244[0m
[37m[1m[2023-07-11 06:39:05,252][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:39:10,256][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:39:10,257][233954] Reward + Measures: [[ 54.60658886   0.1768       0.83190006   0.23010002   0.6997
    3.23963594]
 [197.53043889   0.1164       0.74760002   0.31870002   0.66740006
    2.94879603]
 [112.65773534   0.25370002   0.83200008   0.28270003   0.76870006
    2.88971519]
 ...
 [ 76.62955289   0.14130001   0.68520004   0.2098       0.57370001
    3.03957081]
 [152.15147066   0.09850001   0.82660007   0.3382       0.73519999
    3.38754392]
 [123.04949709   0.15070002   0.77750009   0.2404       0.6631
    2.95421648]][0m
[37m[1m[2023-07-11 06:39:10,257][233954] Max Reward on eval: 554.1491260372102[0m
[37m[1m[2023-07-11 06:39:10,257][233954] Min Reward on eval: -250.21268701758237[0m
[37m[1m[2023-07-11 06:39:10,257][233954] Mean Reward across all agents: 113.1538183757071[0m
[37m[1m[2023-07-11 06:39:10,258][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:39:10,261][233954] mean_value=-93.55172136105875, max_value=547.7569764390448[0m
[37m[1m[2023-07-11 06:39:10,263][233954] New mean coefficients: [[8.406033   5.26617    4.7593274  0.15765041 1.6649048  1.9475522 ]][0m
[37m[1m[2023-07-11 06:39:10,264][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:39:19,242][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 06:39:19,242][233954] FPS: 427786.09[0m
[36m[2023-07-11 06:39:19,245][233954] itr=501, itrs=2000, Progress: 25.05%[0m
[36m[2023-07-11 06:39:30,774][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 06:39:30,774][233954] FPS: 335507.45[0m
[36m[2023-07-11 06:39:35,032][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:39:35,032][233954] Reward + Measures: [[72.82860091  0.10396501  0.14378634  0.09404833  0.13062666  1.70212579]][0m
[37m[1m[2023-07-11 06:39:35,033][233954] Max Reward on eval: 72.828600906421[0m
[37m[1m[2023-07-11 06:39:35,033][233954] Min Reward on eval: 72.828600906421[0m
[37m[1m[2023-07-11 06:39:35,033][233954] Mean Reward across all agents: 72.828600906421[0m
[37m[1m[2023-07-11 06:39:35,034][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:39:40,017][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:39:40,018][233954] Reward + Measures: [[ -5.48167886   0.96550006   0.97500002   0.97109997   0.96600002
    3.92922473]
 [ 16.88744688   0.52070004   0.73290002   0.52249998   0.64350003
    3.40701985]
 [-10.91947921   0.67549998   0.75640005   0.74220002   0.69690007
    3.68808866]
 ...
 [ 59.49057676   0.94230002   0.94489998   0.92189997   0.94139999
    3.74780965]
 [ 63.86509648   0.78289998   0.78399998   0.76279998   0.77460003
    3.8279922 ]
 [-77.65922118   0.93129998   0.92339993   0.93530005   0.92399997
    3.72191429]][0m
[37m[1m[2023-07-11 06:39:40,018][233954] Max Reward on eval: 201.2429618556518[0m
[37m[1m[2023-07-11 06:39:40,018][233954] Min Reward on eval: -211.2756713625975[0m
[37m[1m[2023-07-11 06:39:40,018][233954] Mean Reward across all agents: -12.100542892383489[0m
[37m[1m[2023-07-11 06:39:40,019][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:39:40,022][233954] mean_value=-44.04695024262934, max_value=518.2374414280057[0m
[37m[1m[2023-07-11 06:39:40,024][233954] New mean coefficients: [[ 8.178165    3.6734946   4.99739    -1.0770168  -0.06814623 -0.19095516]][0m
[37m[1m[2023-07-11 06:39:40,025][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:39:48,976][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 06:39:48,976][233954] FPS: 429095.45[0m
[36m[2023-07-11 06:39:48,979][233954] itr=502, itrs=2000, Progress: 25.10%[0m
[36m[2023-07-11 06:40:00,558][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 06:40:00,558][233954] FPS: 334082.12[0m
[36m[2023-07-11 06:40:04,848][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:40:04,853][233954] Reward + Measures: [[71.09263419  0.10369066  0.16000433  0.09077933  0.13028733  1.70677924]][0m
[37m[1m[2023-07-11 06:40:04,854][233954] Max Reward on eval: 71.09263418868088[0m
[37m[1m[2023-07-11 06:40:04,854][233954] Min Reward on eval: 71.09263418868088[0m
[37m[1m[2023-07-11 06:40:04,854][233954] Mean Reward across all agents: 71.09263418868088[0m
[37m[1m[2023-07-11 06:40:04,854][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:40:10,017][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:40:10,018][233954] Reward + Measures: [[ 60.49322212   0.91919994   0.91820002   0.87779999   0.93419999
    3.44270444]
 [158.13125262   0.95690006   0.96740001   0.94319993   0.96040004
    3.26750946]
 [-15.97167648   0.92679995   0.92469996   0.85319996   0.93769997
    3.44473648]
 ...
 [ 25.54691247   0.8314001    0.83449996   0.86219996   0.83770001
    3.37303972]
 [118.37384259   0.95489997   0.95999998   0.92200005   0.94919997
    3.34323931]
 [ 52.12298827   0.94130003   0.94349998   0.9052       0.95500004
    3.46983004]][0m
[37m[1m[2023-07-11 06:40:10,018][233954] Max Reward on eval: 282.44385170950557[0m
[37m[1m[2023-07-11 06:40:10,018][233954] Min Reward on eval: -49.21053528562188[0m
[37m[1m[2023-07-11 06:40:10,019][233954] Mean Reward across all agents: 62.65887324609592[0m
[37m[1m[2023-07-11 06:40:10,019][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:40:10,021][233954] mean_value=-89.34880218562685, max_value=154.05786137878385[0m
[37m[1m[2023-07-11 06:40:10,024][233954] New mean coefficients: [[ 9.35822     2.861714    6.324006    0.12533021 -1.173678   -0.54665637]][0m
[37m[1m[2023-07-11 06:40:10,025][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:40:18,960][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 06:40:18,960][233954] FPS: 429827.15[0m
[36m[2023-07-11 06:40:18,963][233954] itr=503, itrs=2000, Progress: 25.15%[0m
[36m[2023-07-11 06:40:30,560][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 06:40:30,561][233954] FPS: 333591.52[0m
[36m[2023-07-11 06:40:34,758][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:40:34,763][233954] Reward + Measures: [[76.54013291  0.11422566  0.17363533  0.09513434  0.14407867  1.71960187]][0m
[37m[1m[2023-07-11 06:40:34,764][233954] Max Reward on eval: 76.54013291150366[0m
[37m[1m[2023-07-11 06:40:34,764][233954] Min Reward on eval: 76.54013291150366[0m
[37m[1m[2023-07-11 06:40:34,764][233954] Mean Reward across all agents: 76.54013291150366[0m
[37m[1m[2023-07-11 06:40:34,765][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:40:39,699][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:40:39,699][233954] Reward + Measures: [[ 96.80575086   0.033        0.98480004   0.4059       0.95959997
    3.45873618]
 [285.64835546   0.0775       0.88709992   0.50929999   0.83170003
    3.31679535]
 [313.98247936   0.06879999   0.70710003   0.47140002   0.68529999
    3.21904922]
 ...
 [-23.72440536   0.082        0.11170001   0.0984       0.11180001
    2.75521731]
 [ 52.80346007   0.17170002   0.74840003   0.38000003   0.62169999
    3.4495995 ]
 [111.37661023   0.14229999   0.7651       0.39449999   0.68720001
    3.25217795]][0m
[37m[1m[2023-07-11 06:40:39,699][233954] Max Reward on eval: 737.2762908928096[0m
[37m[1m[2023-07-11 06:40:39,700][233954] Min Reward on eval: -93.51486650872975[0m
[37m[1m[2023-07-11 06:40:39,700][233954] Mean Reward across all agents: 133.37416740075233[0m
[37m[1m[2023-07-11 06:40:39,700][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:40:39,703][233954] mean_value=-171.18448590198824, max_value=260.5878182675325[0m
[37m[1m[2023-07-11 06:40:39,705][233954] New mean coefficients: [[ 9.429602    3.3194308   6.5174227  -0.32846853 -0.54510146 -0.01213741]][0m
[37m[1m[2023-07-11 06:40:39,706][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:40:48,687][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 06:40:48,687][233954] FPS: 427670.71[0m
[36m[2023-07-11 06:40:48,689][233954] itr=504, itrs=2000, Progress: 25.20%[0m
[36m[2023-07-11 06:41:00,295][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 06:41:00,295][233954] FPS: 333189.59[0m
[36m[2023-07-11 06:41:04,644][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:41:04,644][233954] Reward + Measures: [[85.12054678  0.12941967  0.19327299  0.101459    0.16737801  1.74560583]][0m
[37m[1m[2023-07-11 06:41:04,644][233954] Max Reward on eval: 85.12054677622064[0m
[37m[1m[2023-07-11 06:41:04,645][233954] Min Reward on eval: 85.12054677622064[0m
[37m[1m[2023-07-11 06:41:04,645][233954] Mean Reward across all agents: 85.12054677622064[0m
[37m[1m[2023-07-11 06:41:04,645][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:41:09,701][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:41:09,702][233954] Reward + Measures: [[ 95.66107068   0.20280002   0.63850003   0.2201       0.54320002
    2.6762569 ]
 [ 95.18596765   0.24489999   0.61700004   0.25209999   0.52360004
    2.6920929 ]
 [ 29.40540246   0.22009997   0.68940002   0.20579998   0.56930006
    2.63936472]
 ...
 [-11.00663136   0.16229999   0.66580003   0.20250002   0.52319998
    2.64907956]
 [ 49.64407625   0.0709       0.1568       0.0714       0.11610001
    1.93600309]
 [ 74.64110491   0.31710002   0.71330005   0.354        0.67110002
    2.54342866]][0m
[37m[1m[2023-07-11 06:41:09,702][233954] Max Reward on eval: 167.9571456808597[0m
[37m[1m[2023-07-11 06:41:09,702][233954] Min Reward on eval: -476.605629445333[0m
[37m[1m[2023-07-11 06:41:09,703][233954] Mean Reward across all agents: 20.911976694864805[0m
[37m[1m[2023-07-11 06:41:09,703][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:41:09,705][233954] mean_value=-154.24356829097758, max_value=323.0254379200271[0m
[37m[1m[2023-07-11 06:41:09,708][233954] New mean coefficients: [[ 8.977636   2.8544102  5.3392615 -0.7342501 -1.112226  -0.5369027]][0m
[37m[1m[2023-07-11 06:41:09,709][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:41:18,741][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 06:41:18,742][233954] FPS: 425214.41[0m
[36m[2023-07-11 06:41:18,744][233954] itr=505, itrs=2000, Progress: 25.25%[0m
[36m[2023-07-11 06:41:30,466][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 06:41:30,466][233954] FPS: 329884.56[0m
[36m[2023-07-11 06:41:34,786][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:41:34,787][233954] Reward + Measures: [[96.17187458  0.14031199  0.21341333  0.106106    0.18358468  1.77748561]][0m
[37m[1m[2023-07-11 06:41:34,787][233954] Max Reward on eval: 96.17187457549537[0m
[37m[1m[2023-07-11 06:41:34,787][233954] Min Reward on eval: 96.17187457549537[0m
[37m[1m[2023-07-11 06:41:34,788][233954] Mean Reward across all agents: 96.17187457549537[0m
[37m[1m[2023-07-11 06:41:34,788][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:41:39,821][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:41:39,821][233954] Reward + Measures: [[-140.76013135    0.2676        0.27169999    0.06          0.2827
     3.26838541]
 [ 168.85304459    0.36149999    0.7859        0.2507        0.76490003
     3.4190979 ]
 [ -22.47654639    0.16949999    0.1684        0.0572        0.18599999
     3.0292244 ]
 ...
 [ -10.50030389    0.08560001    0.0852        0.085         0.0905
     3.11151385]
 [ 141.43184638    0.36240003    0.8785001     0.69410002    0.90259999
     3.61080337]
 [ -54.23660652    0.1019        0.14650001    0.08310001    0.12450001
     3.48883867]][0m
[37m[1m[2023-07-11 06:41:39,822][233954] Max Reward on eval: 773.7393684314563[0m
[37m[1m[2023-07-11 06:41:39,822][233954] Min Reward on eval: -308.5277225569822[0m
[37m[1m[2023-07-11 06:41:39,822][233954] Mean Reward across all agents: 39.43031766843178[0m
[37m[1m[2023-07-11 06:41:39,822][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:41:39,826][233954] mean_value=-149.55221306587987, max_value=517.7476061477398[0m
[37m[1m[2023-07-11 06:41:39,829][233954] New mean coefficients: [[ 8.984571   2.251548   6.1559725 -0.3249781 -1.9572079 -1.2493498]][0m
[37m[1m[2023-07-11 06:41:39,830][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:41:48,942][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 06:41:48,943][233954] FPS: 421464.85[0m
[36m[2023-07-11 06:41:48,945][233954] itr=506, itrs=2000, Progress: 25.30%[0m
[36m[2023-07-11 06:42:00,721][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 06:42:00,721][233954] FPS: 328345.48[0m
[36m[2023-07-11 06:42:05,017][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:42:05,017][233954] Reward + Measures: [[100.41678103   0.14477533   0.24130367   0.10854501   0.19472967
    1.77285397]][0m
[37m[1m[2023-07-11 06:42:05,017][233954] Max Reward on eval: 100.41678103416157[0m
[37m[1m[2023-07-11 06:42:05,018][233954] Min Reward on eval: 100.41678103416157[0m
[37m[1m[2023-07-11 06:42:05,018][233954] Mean Reward across all agents: 100.41678103416157[0m
[37m[1m[2023-07-11 06:42:05,018][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:42:10,017][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:42:10,071][233954] Reward + Measures: [[100.5366406    0.34380004   0.50739998   0.3671       0.3211
    2.47164702]
 [257.68989739   0.3037       0.90170002   0.61880004   0.90430003
    2.90694213]
 [270.51598815   0.27250001   0.90210003   0.55500001   0.83939999
    2.75909233]
 ...
 [429.24463654   0.1592       0.9738       0.65179998   0.93409997
    2.80557227]
 [  9.90799143   0.40519997   0.88510001   0.48660001   0.8707
    2.99713683]
 [231.17278052   0.31689999   0.94139999   0.58359998   0.89740002
    2.94045138]][0m
[37m[1m[2023-07-11 06:42:10,072][233954] Max Reward on eval: 706.5574760313145[0m
[37m[1m[2023-07-11 06:42:10,073][233954] Min Reward on eval: -70.14924909551628[0m
[37m[1m[2023-07-11 06:42:10,074][233954] Mean Reward across all agents: 125.59356532633474[0m
[37m[1m[2023-07-11 06:42:10,075][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:42:10,094][233954] mean_value=-45.397386180136124, max_value=531.659135260852[0m
[37m[1m[2023-07-11 06:42:10,102][233954] New mean coefficients: [[ 9.309846    1.8942447   7.017618    0.13980806 -2.6054788  -1.56645   ]][0m
[37m[1m[2023-07-11 06:42:10,104][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:42:19,248][233954] train() took 9.14 seconds to complete[0m
[36m[2023-07-11 06:42:19,249][233954] FPS: 420080.79[0m
[36m[2023-07-11 06:42:19,251][233954] itr=507, itrs=2000, Progress: 25.35%[0m
[36m[2023-07-11 06:42:30,817][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 06:42:30,817][233954] FPS: 334345.62[0m
[36m[2023-07-11 06:42:35,074][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:42:35,074][233954] Reward + Measures: [[102.25677309   0.15132533   0.27540502   0.116509     0.21193866
    1.8056308 ]][0m
[37m[1m[2023-07-11 06:42:35,074][233954] Max Reward on eval: 102.25677309383836[0m
[37m[1m[2023-07-11 06:42:35,075][233954] Min Reward on eval: 102.25677309383836[0m
[37m[1m[2023-07-11 06:42:35,075][233954] Mean Reward across all agents: 102.25677309383836[0m
[37m[1m[2023-07-11 06:42:35,075][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:42:40,067][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:42:40,068][233954] Reward + Measures: [[-10.12522948   0.48709998   0.70480007   0.42179999   0.65640002
    2.71872044]
 [ 38.42542874   0.3608       0.55949998   0.28080001   0.48009998
    2.84043312]
 [  7.14466312   0.6864       0.71620005   0.66569996   0.72369999
    3.47330832]
 ...
 [ 46.80642723   0.79680002   0.90500003   0.79900002   0.87050003
    3.36944962]
 [-20.98591743   0.2445       0.86650002   0.2431       0.84510005
    3.09383607]
 [ 84.28632306   0.0269       0.96160001   0.4093       0.91979998
    3.72799611]][0m
[37m[1m[2023-07-11 06:42:40,068][233954] Max Reward on eval: 468.0410700128414[0m
[37m[1m[2023-07-11 06:42:40,068][233954] Min Reward on eval: -126.57237577143533[0m
[37m[1m[2023-07-11 06:42:40,069][233954] Mean Reward across all agents: 48.133030157322025[0m
[37m[1m[2023-07-11 06:42:40,069][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:42:40,073][233954] mean_value=-94.51801657116145, max_value=608.536891565223[0m
[37m[1m[2023-07-11 06:42:40,075][233954] New mean coefficients: [[ 8.963427    2.6261926   6.6632204  -0.59662986 -1.4690468  -0.6162584 ]][0m
[37m[1m[2023-07-11 06:42:40,076][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:42:49,032][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 06:42:49,032][233954] FPS: 428827.79[0m
[36m[2023-07-11 06:42:49,034][233954] itr=508, itrs=2000, Progress: 25.40%[0m
[36m[2023-07-11 06:43:00,823][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 06:43:00,823][233954] FPS: 327987.97[0m
[36m[2023-07-11 06:43:05,154][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:43:05,155][233954] Reward + Measures: [[111.32690586   0.16079934   0.32160234   0.12018933   0.23926266
    1.84999049]][0m
[37m[1m[2023-07-11 06:43:05,155][233954] Max Reward on eval: 111.32690586049641[0m
[37m[1m[2023-07-11 06:43:05,155][233954] Min Reward on eval: 111.32690586049641[0m
[37m[1m[2023-07-11 06:43:05,155][233954] Mean Reward across all agents: 111.32690586049641[0m
[37m[1m[2023-07-11 06:43:05,156][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:43:10,400][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:43:10,406][233954] Reward + Measures: [[ 33.02932726   0.22129999   0.71400005   0.29679999   0.69099998
    3.05755043]
 [  2.94508434   0.32969999   0.49960002   0.2067       0.53969997
    3.27336621]
 [ 46.74774964   0.1468       0.29210004   0.16150001   0.28299999
    3.03847837]
 ...
 [128.3573135    0.1822       0.43840003   0.29249999   0.45000002
    3.0409894 ]
 [ 74.1456404    0.089        0.26999998   0.1574       0.24660002
    2.82488561]
 [141.91495768   0.24299999   0.2474       0.28910002   0.35330001
    3.07291794]][0m
[37m[1m[2023-07-11 06:43:10,406][233954] Max Reward on eval: 388.6191369742155[0m
[37m[1m[2023-07-11 06:43:10,406][233954] Min Reward on eval: -505.3666012855712[0m
[37m[1m[2023-07-11 06:43:10,406][233954] Mean Reward across all agents: 59.63015185343965[0m
[37m[1m[2023-07-11 06:43:10,407][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:43:10,410][233954] mean_value=-133.34762309894074, max_value=498.8921833907816[0m
[37m[1m[2023-07-11 06:43:10,412][233954] New mean coefficients: [[ 8.527569    2.8530505   5.7966743  -0.20554924 -1.3300984  -0.77900624]][0m
[37m[1m[2023-07-11 06:43:10,413][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:43:19,454][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 06:43:19,454][233954] FPS: 424831.92[0m
[36m[2023-07-11 06:43:19,456][233954] itr=509, itrs=2000, Progress: 25.45%[0m
[36m[2023-07-11 06:43:31,220][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 06:43:31,221][233954] FPS: 328812.08[0m
[36m[2023-07-11 06:43:35,515][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:43:35,521][233954] Reward + Measures: [[115.0641938    0.16918099   0.39592829   0.12882167   0.27858633
    1.92669988]][0m
[37m[1m[2023-07-11 06:43:35,521][233954] Max Reward on eval: 115.06419379743227[0m
[37m[1m[2023-07-11 06:43:35,521][233954] Min Reward on eval: 115.06419379743227[0m
[37m[1m[2023-07-11 06:43:35,521][233954] Mean Reward across all agents: 115.06419379743227[0m
[37m[1m[2023-07-11 06:43:35,522][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:43:40,600][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:43:40,605][233954] Reward + Measures: [[ 56.67957767   0.14669999   0.1637       0.1358       0.1761
    3.08887839]
 [ 71.1350327    0.0253       0.97679996   0.40629998   0.9429
    3.29257894]
 [ 81.38492993   0.18730001   0.39739999   0.20030001   0.33610001
    2.34576964]
 ...
 [289.54373956   0.10350001   0.64290005   0.2789       0.60839999
    2.99182725]
 [106.06912804   0.0373       0.94309998   0.46190006   0.92640001
    3.67200208]
 [102.43187339   0.0341       0.9702       0.40740004   0.94130003
    3.2716291 ]][0m
[37m[1m[2023-07-11 06:43:40,606][233954] Max Reward on eval: 664.4534683285281[0m
[37m[1m[2023-07-11 06:43:40,606][233954] Min Reward on eval: -66.10345953609794[0m
[37m[1m[2023-07-11 06:43:40,606][233954] Mean Reward across all agents: 117.03367672244087[0m
[37m[1m[2023-07-11 06:43:40,607][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:43:40,610][233954] mean_value=-125.62393020922595, max_value=477.2025814797222[0m
[37m[1m[2023-07-11 06:43:40,613][233954] New mean coefficients: [[ 8.453327   1.9423078  5.321705  -2.9019861 -1.8075452 -1.722758 ]][0m
[37m[1m[2023-07-11 06:43:40,614][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:43:49,688][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 06:43:49,688][233954] FPS: 423255.06[0m
[36m[2023-07-11 06:43:49,691][233954] itr=510, itrs=2000, Progress: 25.50%[0m
[37m[1m[2023-07-11 06:47:11,196][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000490[0m
[36m[2023-07-11 06:47:23,345][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 06:47:23,346][233954] FPS: 332234.86[0m
[36m[2023-07-11 06:47:27,589][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:47:27,589][233954] Reward + Measures: [[116.47330656   0.17199634   0.43413296   0.12933233   0.29980135
    1.96764827]][0m
[37m[1m[2023-07-11 06:47:27,590][233954] Max Reward on eval: 116.47330656236339[0m
[37m[1m[2023-07-11 06:47:27,590][233954] Min Reward on eval: 116.47330656236339[0m
[37m[1m[2023-07-11 06:47:27,590][233954] Mean Reward across all agents: 116.47330656236339[0m
[37m[1m[2023-07-11 06:47:27,590][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:47:32,467][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:47:32,468][233954] Reward + Measures: [[ 129.11930084    0.38310003    0.86680001    0.34280005    0.71950001
     2.58424687]
 [  24.06128537    0.68809998    0.8215        0.65240002    0.78360003
     2.85275555]
 [-110.11672188    0.85979998    0.92340004    0.83420002    0.89040005
     3.30757022]
 ...
 [ 612.4319725     0.0095        0.99300003    0.78380007    0.98549998
     2.7819345 ]
 [  80.48126936    0.44970003    0.86809999    0.41750002    0.75430006
     2.65806055]
 [ 579.70340923    0.0287        0.9716        0.65009993    0.94320005
     2.91086268]][0m
[37m[1m[2023-07-11 06:47:32,468][233954] Max Reward on eval: 665.6112136955838[0m
[37m[1m[2023-07-11 06:47:32,468][233954] Min Reward on eval: -110.11672188295051[0m
[37m[1m[2023-07-11 06:47:32,469][233954] Mean Reward across all agents: 119.40309225227165[0m
[37m[1m[2023-07-11 06:47:32,469][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:47:32,472][233954] mean_value=-72.0392737767545, max_value=479.27634974643973[0m
[37m[1m[2023-07-11 06:47:32,475][233954] New mean coefficients: [[ 7.9119596  2.6584938  4.1591196 -2.7663507 -0.8610712 -1.3114892]][0m
[37m[1m[2023-07-11 06:47:32,476][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:47:41,542][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 06:47:41,542][233954] FPS: 423643.26[0m
[36m[2023-07-11 06:47:41,544][233954] itr=511, itrs=2000, Progress: 25.55%[0m
[36m[2023-07-11 06:47:53,200][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 06:47:53,200][233954] FPS: 331773.82[0m
[36m[2023-07-11 06:47:57,544][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:47:57,544][233954] Reward + Measures: [[143.85043613   0.16183366   0.42923534   0.15111266   0.30955431
    1.9569869 ]][0m
[37m[1m[2023-07-11 06:47:57,545][233954] Max Reward on eval: 143.85043613346122[0m
[37m[1m[2023-07-11 06:47:57,545][233954] Min Reward on eval: 143.85043613346122[0m
[37m[1m[2023-07-11 06:47:57,545][233954] Mean Reward across all agents: 143.85043613346122[0m
[37m[1m[2023-07-11 06:47:57,546][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:48:02,526][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:48:02,526][233954] Reward + Measures: [[141.54731032   0.28420001   0.39580002   0.23889999   0.51560003
    2.50991845]
 [ 92.3922588    0.13950001   0.2122       0.14649999   0.22649999
    3.22784996]
 [ 96.07302422   0.21329999   0.1321       0.19690001   0.22230001
    3.19233179]
 ...
 [275.26324657   0.0359       0.70730001   0.61640006   0.68370003
    3.47663426]
 [ 44.03957059   0.0875       0.44010001   0.16870001   0.38219997
    2.86301351]
 [ 53.04136149   0.1714       0.2251       0.17900001   0.25510001
    3.02013898]][0m
[37m[1m[2023-07-11 06:48:02,526][233954] Max Reward on eval: 786.93558502011[0m
[37m[1m[2023-07-11 06:48:02,527][233954] Min Reward on eval: -120.75604028489325[0m
[37m[1m[2023-07-11 06:48:02,527][233954] Mean Reward across all agents: 88.16338614945971[0m
[37m[1m[2023-07-11 06:48:02,527][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:48:02,531][233954] mean_value=-125.75463766567154, max_value=540.4432288774904[0m
[37m[1m[2023-07-11 06:48:02,534][233954] New mean coefficients: [[ 6.4553413  1.8214986  1.5178344 -3.3642244 -1.8826832 -3.345759 ]][0m
[37m[1m[2023-07-11 06:48:02,535][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:48:11,474][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 06:48:11,474][233954] FPS: 429666.60[0m
[36m[2023-07-11 06:48:11,476][233954] itr=512, itrs=2000, Progress: 25.60%[0m
[36m[2023-07-11 06:48:23,152][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 06:48:23,152][233954] FPS: 331264.34[0m
[36m[2023-07-11 06:48:27,378][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:48:27,378][233954] Reward + Measures: [[139.08999363   0.16054033   0.41079769   0.13746567   0.28455999
    1.92265701]][0m
[37m[1m[2023-07-11 06:48:27,378][233954] Max Reward on eval: 139.08999363295925[0m
[37m[1m[2023-07-11 06:48:27,379][233954] Min Reward on eval: 139.08999363295925[0m
[37m[1m[2023-07-11 06:48:27,379][233954] Mean Reward across all agents: 139.08999363295925[0m
[37m[1m[2023-07-11 06:48:27,379][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:48:32,331][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:48:32,331][233954] Reward + Measures: [[ 115.644516      0.0715        0.54630005    0.2405        0.51700002
     3.09281397]
 [ 134.8315781     0.1314        0.45380002    0.2313        0.46749997
     3.21908545]
 [-132.8539052     0.1837        0.49760005    0.19400001    0.45809999
     2.88456702]
 ...
 [ 152.99951796    0.0658        0.68220007    0.27740002    0.62459999
     3.22026181]
 [ -16.57902096    0.38139999    0.4765        0.24890001    0.52540004
     3.04914665]
 [  35.67410658    0.1097        0.36129999    0.15100001    0.3522
     2.91136146]][0m
[37m[1m[2023-07-11 06:48:32,331][233954] Max Reward on eval: 621.2027287501842[0m
[37m[1m[2023-07-11 06:48:32,332][233954] Min Reward on eval: -345.48326858393847[0m
[37m[1m[2023-07-11 06:48:32,332][233954] Mean Reward across all agents: 48.466782179683285[0m
[37m[1m[2023-07-11 06:48:32,332][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:48:32,335][233954] mean_value=-146.04551182828297, max_value=306.7696936131931[0m
[37m[1m[2023-07-11 06:48:32,337][233954] New mean coefficients: [[ 6.076764    0.75154865  1.3908606  -3.012605   -3.0057936  -4.8890224 ]][0m
[37m[1m[2023-07-11 06:48:32,338][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:48:41,270][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 06:48:41,270][233954] FPS: 429995.16[0m
[36m[2023-07-11 06:48:41,273][233954] itr=513, itrs=2000, Progress: 25.65%[0m
[36m[2023-07-11 06:48:52,938][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 06:48:52,938][233954] FPS: 331521.27[0m
[36m[2023-07-11 06:48:57,307][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:48:57,308][233954] Reward + Measures: [[141.95886336   0.15085134   0.33934733   0.13620432   0.24630965
    1.86966467]][0m
[37m[1m[2023-07-11 06:48:57,308][233954] Max Reward on eval: 141.9588633622325[0m
[37m[1m[2023-07-11 06:48:57,308][233954] Min Reward on eval: 141.9588633622325[0m
[37m[1m[2023-07-11 06:48:57,308][233954] Mean Reward across all agents: 141.9588633622325[0m
[37m[1m[2023-07-11 06:48:57,309][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:49:02,335][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:49:02,335][233954] Reward + Measures: [[ 58.85474742   0.0657       0.2771       0.1217       0.2158
    2.43793035]
 [ 82.30722496   0.1221       0.1188       0.13170001   0.1523
    3.27652049]
 [ 36.26154663   0.11079999   0.20050001   0.108        0.18179999
    2.53783464]
 ...
 [138.03130506   0.51470006   0.81599998   0.6103       0.8524
    3.31149912]
 [101.73568699   0.0581       0.27239999   0.15020001   0.21440001
    2.78619647]
 [267.42660948   0.0787       0.43330002   0.2902       0.43140003
    2.95603633]][0m
[37m[1m[2023-07-11 06:49:02,335][233954] Max Reward on eval: 272.289212686161[0m
[37m[1m[2023-07-11 06:49:02,336][233954] Min Reward on eval: -82.46389579996466[0m
[37m[1m[2023-07-11 06:49:02,336][233954] Mean Reward across all agents: 68.0177834949268[0m
[37m[1m[2023-07-11 06:49:02,336][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:49:02,339][233954] mean_value=-178.2229955507392, max_value=380.3371560420113[0m
[37m[1m[2023-07-11 06:49:02,342][233954] New mean coefficients: [[ 6.095868    0.05717468  2.4717028  -2.5888445  -3.9177032  -5.524181  ]][0m
[37m[1m[2023-07-11 06:49:02,343][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:49:11,422][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 06:49:11,422][233954] FPS: 423050.39[0m
[36m[2023-07-11 06:49:11,425][233954] itr=514, itrs=2000, Progress: 25.70%[0m
[36m[2023-07-11 06:49:23,101][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 06:49:23,101][233954] FPS: 331197.21[0m
[36m[2023-07-11 06:49:27,393][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:49:27,394][233954] Reward + Measures: [[131.34820132   0.14408433   0.32711133   0.12801833   0.226399
    1.82470131]][0m
[37m[1m[2023-07-11 06:49:27,394][233954] Max Reward on eval: 131.34820131699823[0m
[37m[1m[2023-07-11 06:49:27,394][233954] Min Reward on eval: 131.34820131699823[0m
[37m[1m[2023-07-11 06:49:27,395][233954] Mean Reward across all agents: 131.34820131699823[0m
[37m[1m[2023-07-11 06:49:27,395][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:49:32,406][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:49:32,407][233954] Reward + Measures: [[ 47.3670144    0.40060002   0.28750002   0.43089995   0.30560002
    2.71687746]
 [ 15.33205487   0.1274       0.65920001   0.17950001   0.62849998
    2.88628197]
 [  0.53549277   0.22540002   0.44720003   0.24780002   0.41270003
    3.23353958]
 ...
 [ 45.41165792   0.12880002   0.45830002   0.18789999   0.41949996
    3.18089342]
 [ 20.03325176   0.08880001   0.86329997   0.2362       0.71630001
    3.48824692]
 [-13.56993857   0.1664       0.66300005   0.23550001   0.59689999
    3.32261848]][0m
[37m[1m[2023-07-11 06:49:32,407][233954] Max Reward on eval: 378.8106091124937[0m
[37m[1m[2023-07-11 06:49:32,407][233954] Min Reward on eval: -66.34150183438324[0m
[37m[1m[2023-07-11 06:49:32,408][233954] Mean Reward across all agents: 43.877865818368385[0m
[37m[1m[2023-07-11 06:49:32,408][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:49:32,411][233954] mean_value=-108.58578902354496, max_value=587.9409729950596[0m
[37m[1m[2023-07-11 06:49:32,413][233954] New mean coefficients: [[ 5.0457134   1.1878738   0.56885576 -2.7889366  -2.7656875  -4.775711  ]][0m
[37m[1m[2023-07-11 06:49:32,414][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:49:41,368][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 06:49:41,369][233954] FPS: 428907.13[0m
[36m[2023-07-11 06:49:41,371][233954] itr=515, itrs=2000, Progress: 25.75%[0m
[36m[2023-07-11 06:49:53,390][233954] train() took 11.94 seconds to complete[0m
[36m[2023-07-11 06:49:53,390][233954] FPS: 321687.21[0m
[36m[2023-07-11 06:49:57,628][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:49:57,629][233954] Reward + Measures: [[122.45236101   0.13381667   0.27080667   0.12152567   0.19294867
    1.74316823]][0m
[37m[1m[2023-07-11 06:49:57,629][233954] Max Reward on eval: 122.45236100913708[0m
[37m[1m[2023-07-11 06:49:57,629][233954] Min Reward on eval: 122.45236100913708[0m
[37m[1m[2023-07-11 06:49:57,629][233954] Mean Reward across all agents: 122.45236100913708[0m
[37m[1m[2023-07-11 06:49:57,630][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:50:02,676][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:50:02,677][233954] Reward + Measures: [[124.08977364   0.15940002   0.53839999   0.16450001   0.56840003
    2.46704841]
 [101.90658142   0.2076       0.46079999   0.17539999   0.5244
    2.64221454]
 [ 74.81379144   0.167        0.4508       0.161        0.51280004
    2.64370346]
 ...
 [ 72.41817867   0.21519999   0.31240001   0.24220002   0.38639998
    3.12875605]
 [130.33753155   0.13950001   0.56459999   0.147        0.59939998
    2.68670535]
 [182.53348471   0.1098       0.59030002   0.31600001   0.57990003
    3.09425855]][0m
[37m[1m[2023-07-11 06:50:02,677][233954] Max Reward on eval: 260.59147360799835[0m
[37m[1m[2023-07-11 06:50:02,677][233954] Min Reward on eval: -79.52632655486232[0m
[37m[1m[2023-07-11 06:50:02,678][233954] Mean Reward across all agents: 73.55762923080731[0m
[37m[1m[2023-07-11 06:50:02,678][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:50:02,681][233954] mean_value=-89.07550718537681, max_value=374.9248511059121[0m
[37m[1m[2023-07-11 06:50:02,684][233954] New mean coefficients: [[ 4.678497   2.6806936 -1.1337066 -3.6563034 -0.793828  -3.0453427]][0m
[37m[1m[2023-07-11 06:50:02,685][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:50:11,696][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 06:50:11,697][233954] FPS: 426182.28[0m
[36m[2023-07-11 06:50:11,699][233954] itr=516, itrs=2000, Progress: 25.80%[0m
[36m[2023-07-11 06:50:23,681][233954] train() took 11.90 seconds to complete[0m
[36m[2023-07-11 06:50:23,682][233954] FPS: 322744.48[0m
[36m[2023-07-11 06:50:27,915][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:50:27,915][233954] Reward + Measures: [[122.71886609   0.131023     0.228625     0.11712734   0.166225
    1.66663611]][0m
[37m[1m[2023-07-11 06:50:27,915][233954] Max Reward on eval: 122.71886609246656[0m
[37m[1m[2023-07-11 06:50:27,915][233954] Min Reward on eval: 122.71886609246656[0m
[37m[1m[2023-07-11 06:50:27,916][233954] Mean Reward across all agents: 122.71886609246656[0m
[37m[1m[2023-07-11 06:50:27,916][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:50:32,868][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:50:32,869][233954] Reward + Measures: [[243.27327489   0.1292       0.32669997   0.16869999   0.2554
    2.50725174]
 [116.90974282   0.22930002   0.36520001   0.24259999   0.31659999
    2.27576017]
 [ 11.30785765   0.37090001   0.70060003   0.33880001   0.57450002
    2.91411805]
 ...
 [117.30692598   0.16800001   0.31200001   0.18080001   0.27309999
    2.49535799]
 [ 36.1224709    0.1927       0.53850001   0.21429999   0.44790003
    2.86401105]
 [258.20607947   0.27780002   0.45860001   0.2832       0.40910003
    2.58534265]][0m
[37m[1m[2023-07-11 06:50:32,869][233954] Max Reward on eval: 317.1657075762749[0m
[37m[1m[2023-07-11 06:50:32,869][233954] Min Reward on eval: -130.6528626070358[0m
[37m[1m[2023-07-11 06:50:32,870][233954] Mean Reward across all agents: 96.8821394164931[0m
[37m[1m[2023-07-11 06:50:32,870][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:50:32,873][233954] mean_value=-197.850463694514, max_value=726.3750319481641[0m
[37m[1m[2023-07-11 06:50:32,875][233954] New mean coefficients: [[ 3.602014   3.866082  -2.386051  -2.0978756  0.5239494 -2.1753411]][0m
[37m[1m[2023-07-11 06:50:32,876][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:50:41,913][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 06:50:41,913][233954] FPS: 425023.20[0m
[36m[2023-07-11 06:50:41,915][233954] itr=517, itrs=2000, Progress: 25.85%[0m
[36m[2023-07-11 06:50:53,652][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 06:50:53,652][233954] FPS: 329422.91[0m
[36m[2023-07-11 06:50:57,864][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:50:57,864][233954] Reward + Measures: [[134.96702715   0.14406799   0.22064766   0.115591     0.17294665
    1.67484927]][0m
[37m[1m[2023-07-11 06:50:57,865][233954] Max Reward on eval: 134.96702714504988[0m
[37m[1m[2023-07-11 06:50:57,865][233954] Min Reward on eval: 134.96702714504988[0m
[37m[1m[2023-07-11 06:50:57,865][233954] Mean Reward across all agents: 134.96702714504988[0m
[37m[1m[2023-07-11 06:50:57,865][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:51:03,090][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:51:03,090][233954] Reward + Measures: [[ -6.39849233   0.89660007   0.96880001   0.88830006   0.95130008
    2.73221231]
 [ 50.31598968   0.2043       0.5068       0.14600001   0.46580002
    2.76721501]
 [ 78.76478827   0.1602       0.50389999   0.24340001   0.47839999
    3.03286028]
 ...
 [117.3461733    0.51200002   0.87880003   0.47889996   0.75040001
    2.61089396]
 [175.59817403   0.28749999   0.56650001   0.16680001   0.50840002
    2.72725415]
 [ 31.93592791   0.10650001   0.4522       0.13810001   0.3664
    2.28515124]][0m
[37m[1m[2023-07-11 06:51:03,090][233954] Max Reward on eval: 438.5155944820493[0m
[37m[1m[2023-07-11 06:51:03,091][233954] Min Reward on eval: -69.74628926345613[0m
[37m[1m[2023-07-11 06:51:03,091][233954] Mean Reward across all agents: 73.89391348683566[0m
[37m[1m[2023-07-11 06:51:03,091][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:51:03,095][233954] mean_value=-74.03152550936322, max_value=553.4931545285358[0m
[37m[1m[2023-07-11 06:51:03,098][233954] New mean coefficients: [[ 3.5443199   4.7971253  -2.7812495  -2.017481    1.7869427  -0.70538425]][0m
[37m[1m[2023-07-11 06:51:03,099][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:51:12,172][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 06:51:12,172][233954] FPS: 423319.95[0m
[36m[2023-07-11 06:51:12,174][233954] itr=518, itrs=2000, Progress: 25.90%[0m
[36m[2023-07-11 06:51:23,974][233954] train() took 11.72 seconds to complete[0m
[36m[2023-07-11 06:51:23,974][233954] FPS: 327747.67[0m
[36m[2023-07-11 06:51:28,265][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:51:28,265][233954] Reward + Measures: [[134.11462433   0.14324133   0.20079732   0.11590999   0.16816133
    1.66825914]][0m
[37m[1m[2023-07-11 06:51:28,266][233954] Max Reward on eval: 134.11462432615784[0m
[37m[1m[2023-07-11 06:51:28,266][233954] Min Reward on eval: 134.11462432615784[0m
[37m[1m[2023-07-11 06:51:28,266][233954] Mean Reward across all agents: 134.11462432615784[0m
[37m[1m[2023-07-11 06:51:28,266][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:51:33,223][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:51:33,224][233954] Reward + Measures: [[167.10771417   0.58450001   0.1251       0.57600003   0.48020002
    2.70814133]
 [ 16.1488926    0.75580001   0.75769997   0.71310002   0.70710003
    3.35781288]
 [102.04860965   0.22650002   0.47440001   0.1749       0.40650001
    2.47858143]
 ...
 [215.54874608   0.26989999   0.68730003   0.62970001   0.82419997
    2.97268987]
 [172.12240605   0.4894       0.2163       0.4962       0.45110002
    2.63441467]
 [ 26.84267824   0.30790004   0.91850007   0.25680003   0.90529996
    2.86058021]][0m
[37m[1m[2023-07-11 06:51:33,224][233954] Max Reward on eval: 641.8085708584753[0m
[37m[1m[2023-07-11 06:51:33,224][233954] Min Reward on eval: -239.1603295808658[0m
[37m[1m[2023-07-11 06:51:33,224][233954] Mean Reward across all agents: 72.84415411646508[0m
[37m[1m[2023-07-11 06:51:33,225][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:51:33,230][233954] mean_value=-66.58309565599438, max_value=644.4148926435412[0m
[37m[1m[2023-07-11 06:51:33,233][233954] New mean coefficients: [[ 3.2999492  4.095598  -2.8466096 -2.523538   1.4855586 -1.372801 ]][0m
[37m[1m[2023-07-11 06:51:33,234][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:51:42,275][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 06:51:42,275][233954] FPS: 424800.00[0m
[36m[2023-07-11 06:51:42,278][233954] itr=519, itrs=2000, Progress: 25.95%[0m
[36m[2023-07-11 06:51:54,107][233954] train() took 11.74 seconds to complete[0m
[36m[2023-07-11 06:51:54,108][233954] FPS: 326962.93[0m
[36m[2023-07-11 06:51:58,486][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:51:58,486][233954] Reward + Measures: [[138.18220494   0.13946566   0.17568368   0.10267899   0.15733166
    1.62465882]][0m
[37m[1m[2023-07-11 06:51:58,487][233954] Max Reward on eval: 138.18220494132908[0m
[37m[1m[2023-07-11 06:51:58,487][233954] Min Reward on eval: 138.18220494132908[0m
[37m[1m[2023-07-11 06:51:58,487][233954] Mean Reward across all agents: 138.18220494132908[0m
[37m[1m[2023-07-11 06:51:58,488][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:52:03,531][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:52:03,532][233954] Reward + Measures: [[ 10.59666067   0.36070001   0.43010005   0.32210001   0.41170001
    3.03310895]
 [105.44405507   0.34989998   0.15449999   0.32650003   0.27920002
    3.3497014 ]
 [ 66.95921897   0.25549999   0.31500003   0.26390001   0.35009998
    2.89587903]
 ...
 [ 94.4231458    0.27180001   0.46400005   0.23550001   0.4454
    2.67244935]
 [ 33.22026514   0.20720001   0.54699993   0.30090004   0.479
    2.77059436]
 [147.83775045   0.1549       0.62         0.16860001   0.48899999
    2.61514688]][0m
[37m[1m[2023-07-11 06:52:03,532][233954] Max Reward on eval: 342.04818534217776[0m
[37m[1m[2023-07-11 06:52:03,532][233954] Min Reward on eval: -74.92918805368245[0m
[37m[1m[2023-07-11 06:52:03,533][233954] Mean Reward across all agents: 56.830495032765675[0m
[37m[1m[2023-07-11 06:52:03,533][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:52:03,535][233954] mean_value=-153.41568669958176, max_value=365.2907100933383[0m
[37m[1m[2023-07-11 06:52:03,538][233954] New mean coefficients: [[ 3.9900746   2.0714216  -0.38938308 -1.305447   -0.8945173  -3.264089  ]][0m
[37m[1m[2023-07-11 06:52:03,539][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:52:12,620][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 06:52:12,620][233954] FPS: 422947.02[0m
[36m[2023-07-11 06:52:12,622][233954] itr=520, itrs=2000, Progress: 26.00%[0m
[37m[1m[2023-07-11 06:55:26,715][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000500[0m
[36m[2023-07-11 06:55:38,976][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 06:55:38,976][233954] FPS: 330504.82[0m
[36m[2023-07-11 06:55:43,143][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:55:43,143][233954] Reward + Measures: [[145.99316672   0.14218567   0.16882101   0.098201     0.15015666
    1.61633527]][0m
[37m[1m[2023-07-11 06:55:43,143][233954] Max Reward on eval: 145.99316671713515[0m
[37m[1m[2023-07-11 06:55:43,144][233954] Min Reward on eval: 145.99316671713515[0m
[37m[1m[2023-07-11 06:55:43,144][233954] Mean Reward across all agents: 145.99316671713515[0m
[37m[1m[2023-07-11 06:55:43,144][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:55:47,990][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:55:47,990][233954] Reward + Measures: [[ 34.8726157    0.27669999   0.40459999   0.2872       0.3725
    2.91691279]
 [-16.69114456   0.93050003   0.95249999   0.94679993   0.94379997
    3.44031382]
 [ 89.84719469   0.36950001   0.57530004   0.38730001   0.52629995
    2.88924766]
 ...
 [ -9.5912775    0.90600008   0.95500004   0.92180008   0.93169993
    3.47431064]
 [-17.36960885   0.71180004   0.81220001   0.75750005   0.84509993
    2.98586464]
 [ -8.42087771   0.89890003   0.93710005   0.92169994   0.97430003
    3.08732867]][0m
[37m[1m[2023-07-11 06:55:47,990][233954] Max Reward on eval: 697.1398048333824[0m
[37m[1m[2023-07-11 06:55:47,991][233954] Min Reward on eval: -305.9778823900968[0m
[37m[1m[2023-07-11 06:55:47,991][233954] Mean Reward across all agents: -1.6687101760939067[0m
[37m[1m[2023-07-11 06:55:47,991][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:55:47,994][233954] mean_value=-140.36342275511728, max_value=562.5377977389045[0m
[37m[1m[2023-07-11 06:55:47,996][233954] New mean coefficients: [[ 2.735951   1.8995898 -1.401545  -0.174716  -1.2374446 -4.601358 ]][0m
[37m[1m[2023-07-11 06:55:47,997][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:55:56,873][233954] train() took 8.87 seconds to complete[0m
[36m[2023-07-11 06:55:56,873][233954] FPS: 432735.47[0m
[36m[2023-07-11 06:55:56,875][233954] itr=521, itrs=2000, Progress: 26.05%[0m
[36m[2023-07-11 06:56:08,423][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 06:56:08,423][233954] FPS: 334998.32[0m
[36m[2023-07-11 06:56:12,634][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:56:12,635][233954] Reward + Measures: [[135.45066793   0.13539067   0.16445766   0.10114933   0.140453
    1.57798612]][0m
[37m[1m[2023-07-11 06:56:12,635][233954] Max Reward on eval: 135.45066792931533[0m
[37m[1m[2023-07-11 06:56:12,635][233954] Min Reward on eval: 135.45066792931533[0m
[37m[1m[2023-07-11 06:56:12,636][233954] Mean Reward across all agents: 135.45066792931533[0m
[37m[1m[2023-07-11 06:56:12,636][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:56:17,577][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:56:17,577][233954] Reward + Measures: [[ -1.18226911   0.1753       0.17760001   0.1736       0.24070001
    3.23484039]
 [ 41.94292965   0.82889998   0.89920008   0.79430002   0.87360001
    3.28632784]
 [  3.21057443   0.14900002   0.20480001   0.1515       0.14400001
    3.02836847]
 ...
 [ 99.23743979   0.1019       0.25009999   0.0994       0.1804
    2.74958873]
 [105.32443904   0.20150001   0.19560002   0.1086       0.20220001
    2.91676784]
 [ 49.52537828   0.0892       0.1152       0.0982       0.1097
    3.25878406]][0m
[37m[1m[2023-07-11 06:56:17,577][233954] Max Reward on eval: 318.84888197667897[0m
[37m[1m[2023-07-11 06:56:17,578][233954] Min Reward on eval: -112.91627360393758[0m
[37m[1m[2023-07-11 06:56:17,578][233954] Mean Reward across all agents: 58.54335383617924[0m
[37m[1m[2023-07-11 06:56:17,578][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:56:17,580][233954] mean_value=-172.95384878294752, max_value=197.61248281252907[0m
[37m[1m[2023-07-11 06:56:17,582][233954] New mean coefficients: [[ 1.5144558   2.644396   -3.0698082  -0.316258   -0.18246436 -4.690456  ]][0m
[37m[1m[2023-07-11 06:56:17,583][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:56:26,628][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 06:56:26,628][233954] FPS: 424653.57[0m
[36m[2023-07-11 06:56:26,630][233954] itr=522, itrs=2000, Progress: 26.10%[0m
[36m[2023-07-11 06:56:38,441][233954] train() took 11.73 seconds to complete[0m
[36m[2023-07-11 06:56:38,446][233954] FPS: 327493.86[0m
[36m[2023-07-11 06:56:42,790][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:56:42,791][233954] Reward + Measures: [[133.29499154   0.13419299   0.15098032   0.10250401   0.134709
    1.56425393]][0m
[37m[1m[2023-07-11 06:56:42,791][233954] Max Reward on eval: 133.2949915378053[0m
[37m[1m[2023-07-11 06:56:42,791][233954] Min Reward on eval: 133.2949915378053[0m
[37m[1m[2023-07-11 06:56:42,792][233954] Mean Reward across all agents: 133.2949915378053[0m
[37m[1m[2023-07-11 06:56:42,792][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:56:47,801][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:56:47,801][233954] Reward + Measures: [[  6.0632275    0.21200001   0.92539996   0.25420001   0.86809999
    3.19158459]
 [  6.59613914   0.67469996   0.76930004   0.62600005   0.65420002
    3.02158332]
 [157.88940238   0.0093       0.99290001   0.56619996   0.98050004
    3.21496892]
 ...
 [ 63.6243944    0.72459996   0.92979997   0.704        0.81599998
    3.42863703]
 [ 70.19817196   0.66420001   0.83120006   0.51669997   0.79510003
    3.28983879]
 [-27.5263384    0.73879999   0.88980001   0.70180005   0.78600001
    3.09737468]][0m
[37m[1m[2023-07-11 06:56:47,801][233954] Max Reward on eval: 473.71743390634657[0m
[37m[1m[2023-07-11 06:56:47,802][233954] Min Reward on eval: -377.0809879472246[0m
[37m[1m[2023-07-11 06:56:47,802][233954] Mean Reward across all agents: 64.80679185048231[0m
[37m[1m[2023-07-11 06:56:47,802][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:56:47,807][233954] mean_value=-88.94161015573405, max_value=589.1909564361536[0m
[37m[1m[2023-07-11 06:56:47,810][233954] New mean coefficients: [[ 2.1511104  3.7681563 -1.7879831 -0.5666225  1.278315  -3.0484037]][0m
[37m[1m[2023-07-11 06:56:47,811][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:56:56,869][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 06:56:56,869][233954] FPS: 424018.70[0m
[36m[2023-07-11 06:56:56,872][233954] itr=523, itrs=2000, Progress: 26.15%[0m
[36m[2023-07-11 06:57:08,450][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 06:57:08,450][233954] FPS: 334014.09[0m
[36m[2023-07-11 06:57:12,744][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:57:12,744][233954] Reward + Measures: [[142.48272856   0.13926999   0.14159134   0.09241834   0.135233
    1.55199718]][0m
[37m[1m[2023-07-11 06:57:12,744][233954] Max Reward on eval: 142.48272856232518[0m
[37m[1m[2023-07-11 06:57:12,745][233954] Min Reward on eval: 142.48272856232518[0m
[37m[1m[2023-07-11 06:57:12,745][233954] Mean Reward across all agents: 142.48272856232518[0m
[37m[1m[2023-07-11 06:57:12,745][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:57:17,719][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:57:17,719][233954] Reward + Measures: [[ 12.41096175   0.0518       0.96099997   0.39309999   0.88640004
    3.14725471]
 [ 26.44999112   0.0468       0.96579999   0.50700003   0.94520009
    3.10616255]
 [151.32439942   0.0075       0.99760002   0.58680004   0.99599999
    3.36508989]
 ...
 [ 12.45748087   0.1698       0.61570001   0.26050001   0.56999993
    2.94677711]
 [131.24460721   0.0895       0.49759999   0.278        0.53549999
    2.83932447]
 [ 80.9864836    0.35620001   0.60250002   0.2184       0.63340002
    2.62302065]][0m
[37m[1m[2023-07-11 06:57:17,719][233954] Max Reward on eval: 579.3175282491953[0m
[37m[1m[2023-07-11 06:57:17,720][233954] Min Reward on eval: -82.36997891934588[0m
[37m[1m[2023-07-11 06:57:17,720][233954] Mean Reward across all agents: 161.08501465959557[0m
[37m[1m[2023-07-11 06:57:17,720][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:57:17,725][233954] mean_value=-91.983467558352, max_value=750.3102261037509[0m
[37m[1m[2023-07-11 06:57:17,728][233954] New mean coefficients: [[ 2.3593779  5.0672364 -2.2222857 -1.2043898  2.7651482 -1.2314217]][0m
[37m[1m[2023-07-11 06:57:17,729][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:57:26,796][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 06:57:26,796][233954] FPS: 423599.82[0m
[36m[2023-07-11 06:57:26,798][233954] itr=524, itrs=2000, Progress: 26.20%[0m
[36m[2023-07-11 06:57:38,486][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 06:57:38,486][233954] FPS: 330867.32[0m
[36m[2023-07-11 06:57:42,796][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:57:42,797][233954] Reward + Measures: [[142.27445215   0.14326566   0.139614     0.09621199   0.138413
    1.55591416]][0m
[37m[1m[2023-07-11 06:57:42,797][233954] Max Reward on eval: 142.27445214710423[0m
[37m[1m[2023-07-11 06:57:42,797][233954] Min Reward on eval: 142.27445214710423[0m
[37m[1m[2023-07-11 06:57:42,797][233954] Mean Reward across all agents: 142.27445214710423[0m
[37m[1m[2023-07-11 06:57:42,797][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:57:48,017][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:57:48,018][233954] Reward + Measures: [[-43.11731953   0.16419999   0.52649999   0.29170004   0.54190004
    3.21129584]
 [127.29066033   0.28819999   0.32519999   0.2899       0.39000002
    2.84808683]
 [157.41372088   0.21280001   0.39659998   0.1303       0.39470002
    2.66051173]
 ...
 [250.62017571   0.22130001   0.54350007   0.1858       0.51130003
    2.95744967]
 [ 59.86512062   0.0672       0.89120007   0.3723       0.87980002
    3.17461061]
 [219.07228553   0.0895       0.42610002   0.18800001   0.4316
    2.41846871]][0m
[37m[1m[2023-07-11 06:57:48,018][233954] Max Reward on eval: 522.7801704433747[0m
[37m[1m[2023-07-11 06:57:48,018][233954] Min Reward on eval: -70.84827944659628[0m
[37m[1m[2023-07-11 06:57:48,019][233954] Mean Reward across all agents: 115.57882724631867[0m
[37m[1m[2023-07-11 06:57:48,019][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:57:48,022][233954] mean_value=-167.77079441819663, max_value=179.5820360993523[0m
[37m[1m[2023-07-11 06:57:48,024][233954] New mean coefficients: [[ 2.4133167   3.5472631  -1.7959852   0.7237543   0.78371763 -2.9174953 ]][0m
[37m[1m[2023-07-11 06:57:48,025][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:57:57,027][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 06:57:57,027][233954] FPS: 426650.02[0m
[36m[2023-07-11 06:57:57,029][233954] itr=525, itrs=2000, Progress: 26.25%[0m
[36m[2023-07-11 06:58:08,558][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 06:58:08,558][233954] FPS: 335443.34[0m
[36m[2023-07-11 06:58:12,803][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:58:12,804][233954] Reward + Measures: [[139.78851012   0.13991266   0.13196599   0.09306966   0.13521934
    1.54237592]][0m
[37m[1m[2023-07-11 06:58:12,804][233954] Max Reward on eval: 139.78851011553223[0m
[37m[1m[2023-07-11 06:58:12,804][233954] Min Reward on eval: 139.78851011553223[0m
[37m[1m[2023-07-11 06:58:12,805][233954] Mean Reward across all agents: 139.78851011553223[0m
[37m[1m[2023-07-11 06:58:12,805][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:58:17,747][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:58:17,747][233954] Reward + Measures: [[ 47.33435947   0.14479999   0.1662       0.19100001   0.1569
    2.73032618]
 [195.13991311   0.0643       0.52770007   0.3427       0.48090002
    2.63197637]
 [ 47.29084423   0.48760006   0.5952       0.42829999   0.57050002
    2.69798207]
 ...
 [-53.82796235   0.30760002   0.32480001   0.32220003   0.40620002
    2.63018656]
 [  1.30840272   0.96390003   0.97080004   0.94449997   0.96749991
    3.62162662]
 [ -8.5344217    0.75010002   0.79749995   0.71220005   0.79300004
    2.77440715]][0m
[37m[1m[2023-07-11 06:58:17,748][233954] Max Reward on eval: 574.2788924893364[0m
[37m[1m[2023-07-11 06:58:17,748][233954] Min Reward on eval: -148.36162447761745[0m
[37m[1m[2023-07-11 06:58:17,748][233954] Mean Reward across all agents: 33.815783475478156[0m
[37m[1m[2023-07-11 06:58:17,748][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:58:17,750][233954] mean_value=-235.1523904724471, max_value=212.0405190188481[0m
[37m[1m[2023-07-11 06:58:17,753][233954] New mean coefficients: [[ 2.5317323   1.9652653  -0.77539635  1.232146   -0.82583547 -4.5729303 ]][0m
[37m[1m[2023-07-11 06:58:17,754][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:58:26,700][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 06:58:26,700][233954] FPS: 429321.76[0m
[36m[2023-07-11 06:58:26,702][233954] itr=526, itrs=2000, Progress: 26.30%[0m
[36m[2023-07-11 06:58:38,276][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 06:58:38,276][233954] FPS: 334151.65[0m
[36m[2023-07-11 06:58:42,578][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:58:42,578][233954] Reward + Measures: [[131.36291444   0.136829     0.13262834   0.09921533   0.13279566
    1.51060331]][0m
[37m[1m[2023-07-11 06:58:42,579][233954] Max Reward on eval: 131.36291443709382[0m
[37m[1m[2023-07-11 06:58:42,579][233954] Min Reward on eval: 131.36291443709382[0m
[37m[1m[2023-07-11 06:58:42,579][233954] Mean Reward across all agents: 131.36291443709382[0m
[37m[1m[2023-07-11 06:58:42,579][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:58:47,575][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:58:47,576][233954] Reward + Measures: [[ 35.21791509   0.32020003   0.85109997   0.3026       0.72320002
    2.57373881]
 [116.26764298   0.30360004   0.65939999   0.31219998   0.55089998
    2.43089342]
 [116.6325321    0.2093       0.83280003   0.21680002   0.69539994
    2.53748393]
 ...
 [ 67.7488915    0.1258       0.2465       0.098        0.22879998
    2.10025978]
 [ 72.94987582   0.2182       0.72369999   0.22489999   0.61110002
    2.47577667]
 [ 65.62370622   0.1135       0.24340001   0.078        0.1666
    1.88707244]][0m
[37m[1m[2023-07-11 06:58:47,576][233954] Max Reward on eval: 394.4999219208024[0m
[37m[1m[2023-07-11 06:58:47,576][233954] Min Reward on eval: -75.90545095782727[0m
[37m[1m[2023-07-11 06:58:47,577][233954] Mean Reward across all agents: 81.90203436810373[0m
[37m[1m[2023-07-11 06:58:47,577][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:58:47,581][233954] mean_value=-205.047431337436, max_value=775.8366667836905[0m
[37m[1m[2023-07-11 06:58:47,583][233954] New mean coefficients: [[ 3.5096931   1.9006318   1.0886817   1.3953007  -0.85459304 -3.9850142 ]][0m
[37m[1m[2023-07-11 06:58:47,584][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:58:56,556][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 06:58:56,556][233954] FPS: 428108.32[0m
[36m[2023-07-11 06:58:56,558][233954] itr=527, itrs=2000, Progress: 26.35%[0m
[36m[2023-07-11 06:59:08,340][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 06:59:08,341][233954] FPS: 328176.46[0m
[36m[2023-07-11 06:59:12,671][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:59:12,671][233954] Reward + Measures: [[120.33489014   0.13095033   0.13363366   0.10112467   0.12759432
    1.48636377]][0m
[37m[1m[2023-07-11 06:59:12,672][233954] Max Reward on eval: 120.3348901422168[0m
[37m[1m[2023-07-11 06:59:12,672][233954] Min Reward on eval: 120.3348901422168[0m
[37m[1m[2023-07-11 06:59:12,672][233954] Mean Reward across all agents: 120.3348901422168[0m
[37m[1m[2023-07-11 06:59:12,672][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:59:17,732][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:59:17,733][233954] Reward + Measures: [[455.60672186   0.0495       0.91580009   0.55660003   0.85960001
    2.96573377]
 [135.8725004    0.0476       0.92770004   0.36210001   0.88219994
    3.05467081]
 [162.67350385   0.17379999   0.2493       0.1829       0.2701
    2.14545798]
 ...
 [108.56424571   0.29940003   0.66530001   0.2791       0.55550003
    2.34522986]
 [116.22650836   0.0737       0.78220004   0.3229       0.74000001
    3.15043283]
 [ 93.91149282   0.12330001   0.73210001   0.33470002   0.69080001
    2.99719357]][0m
[37m[1m[2023-07-11 06:59:17,733][233954] Max Reward on eval: 758.0715103044174[0m
[37m[1m[2023-07-11 06:59:17,733][233954] Min Reward on eval: -24.941781835630536[0m
[37m[1m[2023-07-11 06:59:17,733][233954] Mean Reward across all agents: 136.6016550156317[0m
[37m[1m[2023-07-11 06:59:17,734][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:59:17,736][233954] mean_value=-176.29952190678674, max_value=202.56908757828106[0m
[37m[1m[2023-07-11 06:59:17,739][233954] New mean coefficients: [[ 3.1040375   1.7659078   0.70281726  1.2795776  -1.090812   -4.430089  ]][0m
[37m[1m[2023-07-11 06:59:17,740][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:59:26,813][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 06:59:26,814][233954] FPS: 423300.85[0m
[36m[2023-07-11 06:59:26,816][233954] itr=528, itrs=2000, Progress: 26.40%[0m
[36m[2023-07-11 06:59:38,579][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 06:59:38,579][233954] FPS: 328838.13[0m
[36m[2023-07-11 06:59:42,829][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:59:42,830][233954] Reward + Measures: [[110.95931826   0.12426333   0.13910031   0.10559166   0.126766
    1.47520471]][0m
[37m[1m[2023-07-11 06:59:42,830][233954] Max Reward on eval: 110.95931826475848[0m
[37m[1m[2023-07-11 06:59:42,830][233954] Min Reward on eval: 110.95931826475848[0m
[37m[1m[2023-07-11 06:59:42,830][233954] Mean Reward across all agents: 110.95931826475848[0m
[37m[1m[2023-07-11 06:59:42,831][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:59:47,812][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 06:59:47,813][233954] Reward + Measures: [[ 59.11427189   0.3405       0.43710002   0.30830002   0.42280003
    3.04869461]
 [278.09515334   0.0599       0.81690007   0.59009999   0.79640001
    3.51729822]
 [ -2.73818032   0.98439997   0.9884001    0.98639995   0.98809999
    3.87537694]
 ...
 [175.65053772   0.0867       0.47070003   0.3536       0.47920004
    3.04639888]
 [228.93821998   0.26460001   0.58140004   0.49499997   0.6257
    3.13229156]
 [-16.67766775   0.0915       0.14500001   0.09550001   0.1401
    3.02243805]][0m
[37m[1m[2023-07-11 06:59:47,813][233954] Max Reward on eval: 393.54707336984575[0m
[37m[1m[2023-07-11 06:59:47,813][233954] Min Reward on eval: -75.4766587776132[0m
[37m[1m[2023-07-11 06:59:47,814][233954] Mean Reward across all agents: 76.84828684239454[0m
[37m[1m[2023-07-11 06:59:47,814][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 06:59:47,817][233954] mean_value=-149.24157292127566, max_value=222.92450939270572[0m
[37m[1m[2023-07-11 06:59:47,820][233954] New mean coefficients: [[ 2.6569211   2.308701    0.03729701 -0.91893494 -0.5086447  -3.941627  ]][0m
[37m[1m[2023-07-11 06:59:47,821][233954] Moving the mean solution point...[0m
[36m[2023-07-11 06:59:56,767][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 06:59:56,768][233954] FPS: 429302.50[0m
[36m[2023-07-11 06:59:56,770][233954] itr=529, itrs=2000, Progress: 26.45%[0m
[36m[2023-07-11 07:00:08,548][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 07:00:08,548][233954] FPS: 328419.61[0m
[36m[2023-07-11 07:00:12,889][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:00:12,890][233954] Reward + Measures: [[98.66838099  0.11693133  0.15413299  0.10843667  0.12424801  1.44685304]][0m
[37m[1m[2023-07-11 07:00:12,890][233954] Max Reward on eval: 98.66838098565978[0m
[37m[1m[2023-07-11 07:00:12,890][233954] Min Reward on eval: 98.66838098565978[0m
[37m[1m[2023-07-11 07:00:12,891][233954] Mean Reward across all agents: 98.66838098565978[0m
[37m[1m[2023-07-11 07:00:12,891][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:00:18,106][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:00:18,107][233954] Reward + Measures: [[65.37095238  0.55330002  0.84219992  0.48880002  0.76750004  2.42597175]
 [64.34895442  0.31819999  0.69980001  0.3168      0.59359998  2.29636741]
 [61.34982179  0.6347      0.86620009  0.55240005  0.81169999  2.58001256]
 ...
 [25.94625736  0.88090003  0.97430003  0.85289997  0.94959992  2.95679474]
 [50.82220958  0.13500001  0.398       0.13680001  0.30899999  2.31990027]
 [30.81107874  0.249       0.47960001  0.20019999  0.3664      2.37286115]][0m
[37m[1m[2023-07-11 07:00:18,107][233954] Max Reward on eval: 438.18084240332246[0m
[37m[1m[2023-07-11 07:00:18,107][233954] Min Reward on eval: -46.96759996972978[0m
[37m[1m[2023-07-11 07:00:18,108][233954] Mean Reward across all agents: 96.42172267435257[0m
[37m[1m[2023-07-11 07:00:18,108][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:00:18,111][233954] mean_value=-181.5702165858629, max_value=248.0479106328454[0m
[37m[1m[2023-07-11 07:00:18,114][233954] New mean coefficients: [[ 2.5721748   1.6916168  -0.08090045 -0.52690613 -1.5699133  -4.688948  ]][0m
[37m[1m[2023-07-11 07:00:18,115][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:00:27,105][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 07:00:27,105][233954] FPS: 427215.63[0m
[36m[2023-07-11 07:00:27,108][233954] itr=530, itrs=2000, Progress: 26.50%[0m
[37m[1m[2023-07-11 07:03:53,956][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000510[0m
[36m[2023-07-11 07:04:06,551][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 07:04:06,551][233954] FPS: 331266.08[0m
[36m[2023-07-11 07:04:10,695][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:04:10,695][233954] Reward + Measures: [[91.41384462  0.10918199  0.14625134  0.11010834  0.11847866  1.40931296]][0m
[37m[1m[2023-07-11 07:04:10,696][233954] Max Reward on eval: 91.41384461607785[0m
[37m[1m[2023-07-11 07:04:10,696][233954] Min Reward on eval: 91.41384461607785[0m
[37m[1m[2023-07-11 07:04:10,696][233954] Mean Reward across all agents: 91.41384461607785[0m
[37m[1m[2023-07-11 07:04:10,696][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:04:15,797][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:04:15,797][233954] Reward + Measures: [[ 32.04483852   0.15980001   0.53009999   0.17650001   0.4395
    3.05361366]
 [ 68.39116348   0.13950001   0.1964       0.0689       0.18090001
    2.37872577]
 [-54.2965145    0.4032       0.68710005   0.3775       0.75199997
    2.83827209]
 ...
 [ 64.34834076   0.0588       0.94529992   0.38249999   0.86250001
    2.97479296]
 [ -3.41549447   0.63450003   0.67140001   0.60750002   0.70359999
    2.66689873]
 [-29.11528956   0.8648001    0.95550007   0.8488       0.93349999
    2.95124006]][0m
[37m[1m[2023-07-11 07:04:15,798][233954] Max Reward on eval: 531.609094595164[0m
[37m[1m[2023-07-11 07:04:15,798][233954] Min Reward on eval: -159.263137316145[0m
[37m[1m[2023-07-11 07:04:15,798][233954] Mean Reward across all agents: 33.61163060324674[0m
[37m[1m[2023-07-11 07:04:15,798][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:04:15,801][233954] mean_value=-166.9578704686326, max_value=445.55845636237416[0m
[37m[1m[2023-07-11 07:04:15,804][233954] New mean coefficients: [[ 2.2622738   0.49998116 -0.19224057 -0.02263677 -2.8827724  -6.107652  ]][0m
[37m[1m[2023-07-11 07:04:15,805][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:04:24,799][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 07:04:24,800][233954] FPS: 427007.29[0m
[36m[2023-07-11 07:04:24,802][233954] itr=531, itrs=2000, Progress: 26.55%[0m
[36m[2023-07-11 07:04:36,527][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 07:04:36,527][233954] FPS: 329865.78[0m
[36m[2023-07-11 07:04:40,729][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:04:40,729][233954] Reward + Measures: [[84.34582329  0.10179567  0.14382534  0.10181101  0.11545866  1.4265902 ]][0m
[37m[1m[2023-07-11 07:04:40,729][233954] Max Reward on eval: 84.34582329217316[0m
[37m[1m[2023-07-11 07:04:40,730][233954] Min Reward on eval: 84.34582329217316[0m
[37m[1m[2023-07-11 07:04:40,730][233954] Mean Reward across all agents: 84.34582329217316[0m
[37m[1m[2023-07-11 07:04:40,730][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:04:45,657][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:04:45,658][233954] Reward + Measures: [[ 23.13587374   0.40650001   0.50809997   0.41039997   0.48610002
    2.64550519]
 [ 93.43503904   0.0884       0.2719       0.11619999   0.22059999
    1.92333543]
 [ 48.22093118   0.12029999   0.29240003   0.1038       0.21529999
    1.92199063]
 ...
 [ 76.22769275   0.32589999   0.58700007   0.30679998   0.51010001
    2.43975282]
 [ 25.68429933   0.23819999   0.46180001   0.21530001   0.38589999
    2.35570264]
 [164.60293296   0.36630002   0.59170002   0.3513       0.51999998
    2.44732213]][0m
[37m[1m[2023-07-11 07:04:45,658][233954] Max Reward on eval: 211.99250267418103[0m
[37m[1m[2023-07-11 07:04:45,658][233954] Min Reward on eval: -174.82289376640693[0m
[37m[1m[2023-07-11 07:04:45,658][233954] Mean Reward across all agents: 49.38612632605725[0m
[37m[1m[2023-07-11 07:04:45,659][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:04:45,661][233954] mean_value=-343.3779502938876, max_value=436.28977548970164[0m
[37m[1m[2023-07-11 07:04:45,663][233954] New mean coefficients: [[ 2.3573866  1.0530703  0.5984572 -0.630822  -2.0219588 -5.4788003]][0m
[37m[1m[2023-07-11 07:04:45,664][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:04:54,619][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 07:04:54,620][233954] FPS: 428860.32[0m
[36m[2023-07-11 07:04:54,622][233954] itr=532, itrs=2000, Progress: 26.60%[0m
[36m[2023-07-11 07:05:06,233][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 07:05:06,234][233954] FPS: 333066.01[0m
[36m[2023-07-11 07:05:10,469][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:05:10,470][233954] Reward + Measures: [[76.52258039  0.09522866  0.15526     0.10705867  0.11369099  1.39109695]][0m
[37m[1m[2023-07-11 07:05:10,470][233954] Max Reward on eval: 76.52258039399621[0m
[37m[1m[2023-07-11 07:05:10,470][233954] Min Reward on eval: 76.52258039399621[0m
[37m[1m[2023-07-11 07:05:10,470][233954] Mean Reward across all agents: 76.52258039399621[0m
[37m[1m[2023-07-11 07:05:10,471][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:05:15,391][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:05:15,392][233954] Reward + Measures: [[ 50.34286641   0.0697       0.11619999   0.0981       0.0865
    2.36924052]
 [-78.59650764   0.20469999   0.92889994   0.27959999   0.95769995
    3.27903724]
 [ 57.53022307   0.14860001   0.15390001   0.14880002   0.17379999
    1.81025851]
 ...
 [143.43301689   0.06510001   0.28550002   0.178        0.25060004
    1.8970747 ]
 [-75.16173647   0.35760003   0.90000004   0.37610003   0.91670001
    3.06337142]
 [ 18.36028484   0.0804       0.10550001   0.0896       0.1129
    2.62385082]][0m
[37m[1m[2023-07-11 07:05:15,392][233954] Max Reward on eval: 509.0174198169261[0m
[37m[1m[2023-07-11 07:05:15,392][233954] Min Reward on eval: -175.7727069753455[0m
[37m[1m[2023-07-11 07:05:15,392][233954] Mean Reward across all agents: 92.98139716664285[0m
[37m[1m[2023-07-11 07:05:15,393][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:05:15,397][233954] mean_value=-356.2481772213069, max_value=572.2029292516321[0m
[37m[1m[2023-07-11 07:05:15,399][233954] New mean coefficients: [[ 2.397915    0.97112703  1.2609417  -0.4219376  -2.2263026  -5.5496774 ]][0m
[37m[1m[2023-07-11 07:05:15,400][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:05:24,318][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 07:05:24,318][233954] FPS: 430677.23[0m
[36m[2023-07-11 07:05:24,320][233954] itr=533, itrs=2000, Progress: 26.65%[0m
[36m[2023-07-11 07:05:35,865][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 07:05:35,865][233954] FPS: 335106.25[0m
[36m[2023-07-11 07:05:40,088][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:05:40,088][233954] Reward + Measures: [[80.70649094  0.09939066  0.15354532  0.110866    0.11601733  1.38533819]][0m
[37m[1m[2023-07-11 07:05:40,088][233954] Max Reward on eval: 80.70649093828995[0m
[37m[1m[2023-07-11 07:05:40,089][233954] Min Reward on eval: 80.70649093828995[0m
[37m[1m[2023-07-11 07:05:40,089][233954] Mean Reward across all agents: 80.70649093828995[0m
[37m[1m[2023-07-11 07:05:40,089][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:05:45,056][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:05:45,057][233954] Reward + Measures: [[313.12768365   0.0317       0.92150003   0.58310002   0.89340001
    2.7077558 ]
 [170.05934958   0.1892       0.86850005   0.30320001   0.69999999
    3.01453352]
 [  9.8818814    0.97539997   0.97560006   0.96289998   0.97659999
    3.58631682]
 ...
 [-70.68746643   0.95190012   0.96700001   0.91350001   0.94439995
    3.31193781]
 [ 62.8946191    0.53610003   0.63889998   0.49909997   0.6322
    2.95014167]
 [-61.03912323   0.85000002   0.84720004   0.80949992   0.87550002
    3.33127952]][0m
[37m[1m[2023-07-11 07:05:45,057][233954] Max Reward on eval: 602.1198544356972[0m
[37m[1m[2023-07-11 07:05:45,057][233954] Min Reward on eval: -227.9785851661116[0m
[37m[1m[2023-07-11 07:05:45,058][233954] Mean Reward across all agents: 72.2542924091151[0m
[37m[1m[2023-07-11 07:05:45,058][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:05:45,061][233954] mean_value=-104.31293677606453, max_value=394.506264282052[0m
[37m[1m[2023-07-11 07:05:45,064][233954] New mean coefficients: [[ 2.5718117   0.77552414  2.162099   -0.4455899  -2.5624375  -5.907046  ]][0m
[37m[1m[2023-07-11 07:05:45,065][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:05:54,043][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 07:05:54,043][233954] FPS: 427781.24[0m
[36m[2023-07-11 07:05:54,046][233954] itr=534, itrs=2000, Progress: 26.70%[0m
[36m[2023-07-11 07:06:05,681][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 07:06:05,681][233954] FPS: 332385.57[0m
[36m[2023-07-11 07:06:10,056][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:06:10,056][233954] Reward + Measures: [[78.78716698  0.09584967  0.14774434  0.11316966  0.11102334  1.3450439 ]][0m
[37m[1m[2023-07-11 07:06:10,056][233954] Max Reward on eval: 78.78716697563317[0m
[37m[1m[2023-07-11 07:06:10,057][233954] Min Reward on eval: 78.78716697563317[0m
[37m[1m[2023-07-11 07:06:10,057][233954] Mean Reward across all agents: 78.78716697563317[0m
[37m[1m[2023-07-11 07:06:10,057][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:06:15,048][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:06:15,048][233954] Reward + Measures: [[ 15.98008742   0.11930001   0.18880001   0.0904       0.14130001
    2.80073738]
 [ 75.73988587   0.1688       0.1682       0.16770001   0.20100001
    2.67382812]
 [130.23763661   0.2251       0.3795       0.19070001   0.37129998
    2.30621958]
 ...
 [ 10.91557507   0.0964       0.1214       0.11359999   0.1565
    2.95368648]
 [-16.99354273   0.31029999   0.88959998   0.42659998   0.85650009
    2.75649381]
 [ 67.92054358   0.1027       0.1053       0.0975       0.1258
    3.07814074]][0m
[37m[1m[2023-07-11 07:06:15,048][233954] Max Reward on eval: 438.77826828584074[0m
[37m[1m[2023-07-11 07:06:15,049][233954] Min Reward on eval: -86.4998212115839[0m
[37m[1m[2023-07-11 07:06:15,049][233954] Mean Reward across all agents: 65.89376785239394[0m
[37m[1m[2023-07-11 07:06:15,049][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:06:15,051][233954] mean_value=-238.63000859885176, max_value=482.07144814422355[0m
[37m[1m[2023-07-11 07:06:15,054][233954] New mean coefficients: [[ 0.8041165   0.90610945 -0.14684439 -1.1709747  -2.0929155  -6.4179444 ]][0m
[37m[1m[2023-07-11 07:06:15,059][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:06:24,099][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 07:06:24,099][233954] FPS: 424894.88[0m
[36m[2023-07-11 07:06:24,101][233954] itr=535, itrs=2000, Progress: 26.75%[0m
[36m[2023-07-11 07:06:35,908][233954] train() took 11.72 seconds to complete[0m
[36m[2023-07-11 07:06:35,908][233954] FPS: 327619.25[0m
[36m[2023-07-11 07:06:40,192][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:06:40,193][233954] Reward + Measures: [[74.22930848  0.09075566  0.15364933  0.11266101  0.10541367  1.31234109]][0m
[37m[1m[2023-07-11 07:06:40,193][233954] Max Reward on eval: 74.22930848296231[0m
[37m[1m[2023-07-11 07:06:40,193][233954] Min Reward on eval: 74.22930848296231[0m
[37m[1m[2023-07-11 07:06:40,194][233954] Mean Reward across all agents: 74.22930848296231[0m
[37m[1m[2023-07-11 07:06:40,194][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:06:45,151][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:06:45,152][233954] Reward + Measures: [[-72.98480992   0.40470001   0.25840002   0.31539997   0.25580001
    2.68785667]
 [ 51.94909918   0.0995       0.5711       0.15719998   0.47890002
    2.6345253 ]
 [ 73.98101038   0.19159999   0.61650002   0.26700002   0.58950001
    2.41722202]
 ...
 [ 55.12145829   0.1763       0.5722       0.2057       0.51170003
    2.82589364]
 [157.81750817   0.2182       0.29620004   0.2361       0.27320001
    2.83033919]
 [ 32.53110392   0.19200002   0.59039998   0.2052       0.54630005
    2.57437778]][0m
[37m[1m[2023-07-11 07:06:45,152][233954] Max Reward on eval: 629.3877754199318[0m
[37m[1m[2023-07-11 07:06:45,153][233954] Min Reward on eval: -119.26078796554357[0m
[37m[1m[2023-07-11 07:06:45,153][233954] Mean Reward across all agents: 108.13705595507625[0m
[37m[1m[2023-07-11 07:06:45,153][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:06:45,157][233954] mean_value=-144.70800741813827, max_value=767.4104692294397[0m
[37m[1m[2023-07-11 07:06:45,159][233954] New mean coefficients: [[ 0.89903426  1.4070244   0.28433052 -1.8555193  -1.2267957  -5.8939023 ]][0m
[37m[1m[2023-07-11 07:06:45,160][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:06:54,142][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 07:06:54,143][233954] FPS: 427580.37[0m
[36m[2023-07-11 07:06:54,145][233954] itr=536, itrs=2000, Progress: 26.80%[0m
[36m[2023-07-11 07:07:05,859][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 07:07:05,859][233954] FPS: 330212.14[0m
[36m[2023-07-11 07:07:10,177][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:07:10,177][233954] Reward + Measures: [[71.48973164  0.08601233  0.15706199  0.11468433  0.10074566  1.28856921]][0m
[37m[1m[2023-07-11 07:07:10,177][233954] Max Reward on eval: 71.4897316397662[0m
[37m[1m[2023-07-11 07:07:10,178][233954] Min Reward on eval: 71.4897316397662[0m
[37m[1m[2023-07-11 07:07:10,178][233954] Mean Reward across all agents: 71.4897316397662[0m
[37m[1m[2023-07-11 07:07:10,178][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:07:15,426][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:07:15,432][233954] Reward + Measures: [[ 55.77760167   0.17770003   0.37870002   0.19170001   0.45100003
    2.67329073]
 [ 99.91683882   0.16229999   0.35999998   0.1759       0.28560001
    2.29718781]
 [-38.27769481   0.44800001   0.76189995   0.4619       0.79710001
    3.35106587]
 ...
 [ 40.47487548   0.1222       0.22830001   0.11849999   0.21010001
    1.99904275]
 [169.3542938    0.1626       0.4337       0.18500002   0.35090002
    2.04665947]
 [111.43313219   0.14260001   0.64869994   0.13800001   0.56490004
    2.35701656]][0m
[37m[1m[2023-07-11 07:07:15,432][233954] Max Reward on eval: 701.9486732828431[0m
[37m[1m[2023-07-11 07:07:15,432][233954] Min Reward on eval: -197.8414051199332[0m
[37m[1m[2023-07-11 07:07:15,433][233954] Mean Reward across all agents: 92.3931929731116[0m
[37m[1m[2023-07-11 07:07:15,433][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:07:15,437][233954] mean_value=-206.20901521298163, max_value=504.6082501076535[0m
[37m[1m[2023-07-11 07:07:15,440][233954] New mean coefficients: [[ 0.29112983  1.5882545   0.18897498 -0.01357663 -1.0452882  -6.563195  ]][0m
[37m[1m[2023-07-11 07:07:15,441][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:07:24,553][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 07:07:24,553][233954] FPS: 421473.36[0m
[36m[2023-07-11 07:07:24,556][233954] itr=537, itrs=2000, Progress: 26.85%[0m
[36m[2023-07-11 07:07:36,309][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 07:07:36,309][233954] FPS: 329018.65[0m
[36m[2023-07-11 07:07:40,605][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:07:40,605][233954] Reward + Measures: [[66.34133355  0.088155    0.16706     0.11951333  0.10112967  1.27947545]][0m
[37m[1m[2023-07-11 07:07:40,605][233954] Max Reward on eval: 66.34133354570278[0m
[37m[1m[2023-07-11 07:07:40,606][233954] Min Reward on eval: 66.34133354570278[0m
[37m[1m[2023-07-11 07:07:40,606][233954] Mean Reward across all agents: 66.34133354570278[0m
[37m[1m[2023-07-11 07:07:40,606][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:07:45,574][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:07:45,575][233954] Reward + Measures: [[158.26351068   0.184        0.43610001   0.24249999   0.42720005
    2.59211969]
 [ 35.6740508    0.06940001   0.28240001   0.1066       0.19620001
    2.59024596]
 [ 89.22127862   0.26910001   0.43260002   0.0875       0.41560003
    3.08636713]
 ...
 [ 96.51357649   0.39400002   0.63350004   0.33610001   0.55450004
    2.47446632]
 [-27.10993475   0.79660004   0.88249999   0.73700011   0.87709999
    3.0578568 ]
 [ 28.19932196   0.37439999   0.6085       0.0758       0.60630006
    2.8167944 ]][0m
[37m[1m[2023-07-11 07:07:45,575][233954] Max Reward on eval: 650.819612502493[0m
[37m[1m[2023-07-11 07:07:45,575][233954] Min Reward on eval: -44.75380897959694[0m
[37m[1m[2023-07-11 07:07:45,576][233954] Mean Reward across all agents: 125.4908929608535[0m
[37m[1m[2023-07-11 07:07:45,576][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:07:45,581][233954] mean_value=-88.22063885336497, max_value=773.7114711239934[0m
[37m[1m[2023-07-11 07:07:45,584][233954] New mean coefficients: [[ 0.30647728  2.5642047   0.7387023  -1.0318443   0.19443    -5.2617917 ]][0m
[37m[1m[2023-07-11 07:07:45,585][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:07:54,544][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 07:07:54,544][233954] FPS: 428712.47[0m
[36m[2023-07-11 07:07:54,546][233954] itr=538, itrs=2000, Progress: 26.90%[0m
[36m[2023-07-11 07:08:06,085][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 07:08:06,086][233954] FPS: 335244.35[0m
[36m[2023-07-11 07:08:10,349][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:08:10,354][233954] Reward + Measures: [[66.56805853  0.08757467  0.16527532  0.12594333  0.10086633  1.26190042]][0m
[37m[1m[2023-07-11 07:08:10,355][233954] Max Reward on eval: 66.56805853131338[0m
[37m[1m[2023-07-11 07:08:10,355][233954] Min Reward on eval: 66.56805853131338[0m
[37m[1m[2023-07-11 07:08:10,355][233954] Mean Reward across all agents: 66.56805853131338[0m
[37m[1m[2023-07-11 07:08:10,355][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:08:15,300][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:08:15,300][233954] Reward + Measures: [[  3.91877075   0.12560001   0.39359999   0.207        0.35879999
    2.88279223]
 [146.70125768   0.0834       0.20900002   0.14909999   0.1864
    2.10921645]
 [-25.47097264   0.2041       0.28049999   0.2052       0.29879999
    2.66954136]
 ...
 [ 36.40311761   0.13990001   0.17850001   0.1314       0.22360002
    2.54592776]
 [ 69.73751547   0.185        0.30789998   0.1816       0.31060001
    2.50862288]
 [226.90085028   0.0596       0.73089999   0.49560004   0.71200001
    2.96153831]][0m
[37m[1m[2023-07-11 07:08:15,300][233954] Max Reward on eval: 622.2177886988269[0m
[37m[1m[2023-07-11 07:08:15,301][233954] Min Reward on eval: -311.9602842875523[0m
[37m[1m[2023-07-11 07:08:15,301][233954] Mean Reward across all agents: 75.35929606891291[0m
[37m[1m[2023-07-11 07:08:15,301][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:08:15,305][233954] mean_value=-193.65664629277958, max_value=512.7366398519284[0m
[37m[1m[2023-07-11 07:08:15,307][233954] New mean coefficients: [[-0.2952452   1.9225059  -0.19448334 -0.27075255 -0.52907705 -6.5487947 ]][0m
[37m[1m[2023-07-11 07:08:15,308][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:08:24,256][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 07:08:24,256][233954] FPS: 429248.13[0m
[36m[2023-07-11 07:08:24,258][233954] itr=539, itrs=2000, Progress: 26.95%[0m
[36m[2023-07-11 07:08:35,802][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 07:08:35,802][233954] FPS: 335024.97[0m
[36m[2023-07-11 07:08:40,041][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:08:40,041][233954] Reward + Measures: [[68.90736273  0.08650801  0.15957999  0.125965    0.09777568  1.23328698]][0m
[37m[1m[2023-07-11 07:08:40,041][233954] Max Reward on eval: 68.90736272941669[0m
[37m[1m[2023-07-11 07:08:40,041][233954] Min Reward on eval: 68.90736272941669[0m
[37m[1m[2023-07-11 07:08:40,042][233954] Mean Reward across all agents: 68.90736272941669[0m
[37m[1m[2023-07-11 07:08:40,042][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:08:44,987][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:08:44,987][233954] Reward + Measures: [[ 40.75649423   0.21170001   0.4506       0.20990001   0.44510004
    2.19803929]
 [114.08497381   0.19270001   0.73960006   0.184        0.71270001
    2.60185432]
 [ 82.87860591   0.169        0.50240004   0.1912       0.56700003
    2.35675621]
 ...
 [ 89.73699283   0.26840001   0.56199998   0.2719       0.58390003
    2.36948657]
 [102.68442391   0.1867       0.55800003   0.18359999   0.59680003
    2.37723422]
 [-39.54388448   0.33940002   0.36840001   0.30650002   0.38069999
    2.64434075]][0m
[37m[1m[2023-07-11 07:08:44,988][233954] Max Reward on eval: 652.9175600701943[0m
[37m[1m[2023-07-11 07:08:44,988][233954] Min Reward on eval: -130.18415805650875[0m
[37m[1m[2023-07-11 07:08:44,988][233954] Mean Reward across all agents: 51.79307882473186[0m
[37m[1m[2023-07-11 07:08:44,988][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:08:44,991][233954] mean_value=-158.93376555320182, max_value=325.75948069586747[0m
[37m[1m[2023-07-11 07:08:44,993][233954] New mean coefficients: [[-0.46555302  2.0603595  -1.3012462  -0.559042   -0.2043944  -5.8476257 ]][0m
[37m[1m[2023-07-11 07:08:44,994][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:08:53,976][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 07:08:53,976][233954] FPS: 427605.65[0m
[36m[2023-07-11 07:08:53,978][233954] itr=540, itrs=2000, Progress: 27.00%[0m
[37m[1m[2023-07-11 07:12:16,429][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000520[0m
[36m[2023-07-11 07:12:29,870][233954] train() took 11.73 seconds to complete[0m
[36m[2023-07-11 07:12:29,871][233954] FPS: 327279.49[0m
[36m[2023-07-11 07:12:34,180][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:12:34,181][233954] Reward + Measures: [[67.54396975  0.08470433  0.16875167  0.13026333  0.097102    1.23506975]][0m
[37m[1m[2023-07-11 07:12:34,181][233954] Max Reward on eval: 67.54396974890003[0m
[37m[1m[2023-07-11 07:12:34,181][233954] Min Reward on eval: 67.54396974890003[0m
[37m[1m[2023-07-11 07:12:34,181][233954] Mean Reward across all agents: 67.54396974890003[0m
[37m[1m[2023-07-11 07:12:34,181][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:12:39,072][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:12:39,073][233954] Reward + Measures: [[143.33595106   0.0733       0.89650005   0.4619       0.82999992
    3.19811988]
 [ 86.40867872   0.71270001   0.792        0.65760005   0.8495
    3.12934971]
 [111.96096325   0.08400001   0.73199999   0.3362       0.65640002
    3.0185287 ]
 ...
 [235.86092565   0.0654       0.87819999   0.56730002   0.84499997
    3.37183809]
 [-25.85767913   0.21270001   0.51870006   0.2793       0.54290003
    2.98639941]
 [134.31335061   0.072        0.25009999   0.14080001   0.2789
    2.73620963]][0m
[37m[1m[2023-07-11 07:12:39,073][233954] Max Reward on eval: 479.19095227401704[0m
[37m[1m[2023-07-11 07:12:39,074][233954] Min Reward on eval: -139.28096072813497[0m
[37m[1m[2023-07-11 07:12:39,074][233954] Mean Reward across all agents: 80.60758421285473[0m
[37m[1m[2023-07-11 07:12:39,074][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:12:39,077][233954] mean_value=-185.0604550729983, max_value=673.3157718110199[0m
[37m[1m[2023-07-11 07:12:39,080][233954] New mean coefficients: [[ 0.44384685  1.5915074  -0.38499463 -1.3308096  -0.58482504 -5.860827  ]][0m
[37m[1m[2023-07-11 07:12:39,081][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:12:48,042][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 07:12:48,043][233954] FPS: 428569.42[0m
[36m[2023-07-11 07:12:48,045][233954] itr=541, itrs=2000, Progress: 27.05%[0m
[36m[2023-07-11 07:12:59,665][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 07:12:59,666][233954] FPS: 332857.69[0m
[36m[2023-07-11 07:13:03,893][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:13:03,894][233954] Reward + Measures: [[64.99454289  0.07994567  0.16559501  0.13168466  0.09316166  1.21577358]][0m
[37m[1m[2023-07-11 07:13:03,894][233954] Max Reward on eval: 64.99454288927798[0m
[37m[1m[2023-07-11 07:13:03,894][233954] Min Reward on eval: 64.99454288927798[0m
[37m[1m[2023-07-11 07:13:03,895][233954] Mean Reward across all agents: 64.99454288927798[0m
[37m[1m[2023-07-11 07:13:03,895][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:13:08,808][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:13:08,814][233954] Reward + Measures: [[  4.52732069   0.26730001   0.46330005   0.24779999   0.47189999
    2.25831842]
 [-15.88868923   0.31190002   0.51599997   0.31479999   0.53170007
    2.26179481]
 [543.01358577   0.1008       0.91850007   0.64930004   0.90900004
    2.98257828]
 ...
 [524.42697787   0.08880001   0.88640004   0.6085       0.87650007
    3.02188802]
 [  8.9535406    0.28099999   0.36280003   0.27270001   0.31690001
    2.6186533 ]
 [-53.5475578    0.50380003   0.59300005   0.52430004   0.56349999
    2.29367733]][0m
[37m[1m[2023-07-11 07:13:08,815][233954] Max Reward on eval: 548.6580028630793[0m
[37m[1m[2023-07-11 07:13:08,815][233954] Min Reward on eval: -461.6238348707557[0m
[37m[1m[2023-07-11 07:13:08,816][233954] Mean Reward across all agents: 63.740392827223985[0m
[37m[1m[2023-07-11 07:13:08,816][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:13:08,824][233954] mean_value=-208.71541378634728, max_value=425.0912043296855[0m
[37m[1m[2023-07-11 07:13:08,828][233954] New mean coefficients: [[ 0.08438614  0.6527964  -0.2042661   0.5511042  -1.7894709  -7.1062684 ]][0m
[37m[1m[2023-07-11 07:13:08,830][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:13:17,784][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 07:13:17,784][233954] FPS: 428956.67[0m
[36m[2023-07-11 07:13:17,787][233954] itr=542, itrs=2000, Progress: 27.10%[0m
[36m[2023-07-11 07:13:29,564][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 07:13:29,564][233954] FPS: 328343.65[0m
[36m[2023-07-11 07:13:33,865][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:13:33,866][233954] Reward + Measures: [[65.24532351  0.08026833  0.17292732  0.136472    0.09347233  1.19634652]][0m
[37m[1m[2023-07-11 07:13:33,866][233954] Max Reward on eval: 65.24532350909298[0m
[37m[1m[2023-07-11 07:13:33,866][233954] Min Reward on eval: 65.24532350909298[0m
[37m[1m[2023-07-11 07:13:33,866][233954] Mean Reward across all agents: 65.24532350909298[0m
[37m[1m[2023-07-11 07:13:33,867][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:13:38,768][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:13:38,769][233954] Reward + Measures: [[ 39.37196945   0.30099997   0.34670001   0.28980002   0.2897
    2.13544774]
 [162.73214438   0.13010001   0.63609999   0.20780002   0.54329997
    2.45375514]
 [-61.18853729   0.33290002   0.17640002   0.35759997   0.28439999
    2.43851662]
 ...
 [ 60.36327625   0.26520002   0.40380001   0.36570001   0.49990001
    2.41902995]
 [190.23420524   0.1473       0.74999994   0.1917       0.58649999
    2.49172568]
 [-75.55809155   0.42290002   0.73200005   0.2306       0.68589991
    2.63007045]][0m
[37m[1m[2023-07-11 07:13:38,769][233954] Max Reward on eval: 410.53126478865744[0m
[37m[1m[2023-07-11 07:13:38,769][233954] Min Reward on eval: -289.19103147499266[0m
[37m[1m[2023-07-11 07:13:38,770][233954] Mean Reward across all agents: 76.03390536515036[0m
[37m[1m[2023-07-11 07:13:38,770][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:13:38,775][233954] mean_value=-222.49556372790965, max_value=624.3195625441904[0m
[37m[1m[2023-07-11 07:13:38,777][233954] New mean coefficients: [[-0.56373787  0.88296956 -0.7583405  -1.4203929  -1.3875251  -6.812229  ]][0m
[37m[1m[2023-07-11 07:13:38,778][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:13:47,689][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 07:13:47,690][233954] FPS: 430996.44[0m
[36m[2023-07-11 07:13:47,692][233954] itr=543, itrs=2000, Progress: 27.15%[0m
[36m[2023-07-11 07:13:59,234][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 07:13:59,234][233954] FPS: 335199.20[0m
[36m[2023-07-11 07:14:03,464][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:14:03,464][233954] Reward + Measures: [[62.76782805  0.078316    0.16975068  0.135923    0.09301766  1.19682109]][0m
[37m[1m[2023-07-11 07:14:03,465][233954] Max Reward on eval: 62.76782804908417[0m
[37m[1m[2023-07-11 07:14:03,465][233954] Min Reward on eval: 62.76782804908417[0m
[37m[1m[2023-07-11 07:14:03,465][233954] Mean Reward across all agents: 62.76782804908417[0m
[37m[1m[2023-07-11 07:14:03,465][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:14:08,695][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:14:08,696][233954] Reward + Measures: [[178.26951279   0.37630001   0.78220004   0.31459999   0.74949998
    2.91584539]
 [250.06771997   0.0989       0.69830006   0.41640002   0.66970003
    3.00724721]
 [ 43.32855924   0.71200001   0.74270004   0.63980001   0.73189992
    2.87897229]
 ...
 [108.39206435   0.26500002   0.3536       0.2825       0.47070003
    2.61253929]
 [ 36.63535097   0.89429998   0.91580003   0.86079997   0.92010003
    3.13000941]
 [-21.342594     0.70750004   0.80510008   0.6943       0.84499997
    2.65416789]][0m
[37m[1m[2023-07-11 07:14:08,696][233954] Max Reward on eval: 346.3988818780519[0m
[37m[1m[2023-07-11 07:14:08,696][233954] Min Reward on eval: -74.63923738114536[0m
[37m[1m[2023-07-11 07:14:08,696][233954] Mean Reward across all agents: 71.74466491521827[0m
[37m[1m[2023-07-11 07:14:08,697][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:14:08,700][233954] mean_value=-207.78655845494873, max_value=522.4395117643043[0m
[37m[1m[2023-07-11 07:14:08,703][233954] New mean coefficients: [[-0.12100005  0.8963132   0.01485324 -0.9969038  -1.5435567  -6.4880414 ]][0m
[37m[1m[2023-07-11 07:14:08,704][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:14:17,774][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 07:14:17,774][233954] FPS: 423451.44[0m
[36m[2023-07-11 07:14:17,776][233954] itr=544, itrs=2000, Progress: 27.20%[0m
[36m[2023-07-11 07:14:29,445][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 07:14:29,446][233954] FPS: 331568.41[0m
[36m[2023-07-11 07:14:33,705][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:14:33,706][233954] Reward + Measures: [[62.67680892  0.07929966  0.16366266  0.13490066  0.093434    1.18084991]][0m
[37m[1m[2023-07-11 07:14:33,706][233954] Max Reward on eval: 62.67680891785452[0m
[37m[1m[2023-07-11 07:14:33,706][233954] Min Reward on eval: 62.67680891785452[0m
[37m[1m[2023-07-11 07:14:33,706][233954] Mean Reward across all agents: 62.67680891785452[0m
[37m[1m[2023-07-11 07:14:33,707][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:14:38,622][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:14:38,623][233954] Reward + Measures: [[ 24.05637523   0.10090001   0.1524       0.0776       0.12330001
    1.95940483]
 [ 24.06281671   0.1173       0.21500002   0.09299999   0.1761
    2.07748127]
 [ 57.53070544   0.20110002   0.16780001   0.17420001   0.2405
    1.84019661]
 ...
 [ 54.33090677   0.0814       0.14129999   0.0931       0.1127
    2.35912585]
 [104.30437358   0.08630001   0.65600008   0.17920001   0.56049997
    2.68363762]
 [192.77605606   0.0717       0.6752001    0.25869998   0.58029997
    2.86310577]][0m
[37m[1m[2023-07-11 07:14:38,623][233954] Max Reward on eval: 490.34264564439655[0m
[37m[1m[2023-07-11 07:14:38,623][233954] Min Reward on eval: -72.09564732750877[0m
[37m[1m[2023-07-11 07:14:38,623][233954] Mean Reward across all agents: 70.84198636660021[0m
[37m[1m[2023-07-11 07:14:38,624][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:14:38,626][233954] mean_value=-289.95234363479557, max_value=377.33616345876334[0m
[37m[1m[2023-07-11 07:14:38,628][233954] New mean coefficients: [[-0.43806452  2.2004652  -0.5665396  -1.3482256   0.0383532  -4.762554  ]][0m
[37m[1m[2023-07-11 07:14:38,629][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:14:47,603][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 07:14:47,603][233954] FPS: 428002.66[0m
[36m[2023-07-11 07:14:47,606][233954] itr=545, itrs=2000, Progress: 27.25%[0m
[36m[2023-07-11 07:14:59,301][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 07:14:59,301][233954] FPS: 330664.50[0m
[36m[2023-07-11 07:15:03,576][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:15:03,576][233954] Reward + Measures: [[58.04322478  0.08578634  0.15464567  0.11831634  0.10390333  1.23846602]][0m
[37m[1m[2023-07-11 07:15:03,576][233954] Max Reward on eval: 58.04322477581631[0m
[37m[1m[2023-07-11 07:15:03,577][233954] Min Reward on eval: 58.04322477581631[0m
[37m[1m[2023-07-11 07:15:03,577][233954] Mean Reward across all agents: 58.04322477581631[0m
[37m[1m[2023-07-11 07:15:03,577][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:15:08,577][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:15:08,583][233954] Reward + Measures: [[ 36.81455187   0.18429999   0.66119999   0.1911       0.52380008
    2.39792061]
 [ 39.56404924   0.55540001   0.81080002   0.45370004   0.7899
    2.76034331]
 [280.18420886   0.2112       0.40690002   0.2148       0.28569999
    2.14514208]
 ...
 [121.73690652   0.435        0.85089999   0.38960001   0.7058
    2.43570924]
 [119.05584872   0.09679999   0.31860003   0.162        0.27970001
    1.99999654]
 [ 40.49263514   0.07160001   0.10390001   0.06130001   0.08880001
    1.46987879]][0m
[37m[1m[2023-07-11 07:15:08,583][233954] Max Reward on eval: 403.0876977056265[0m
[37m[1m[2023-07-11 07:15:08,584][233954] Min Reward on eval: -15.344454833585768[0m
[37m[1m[2023-07-11 07:15:08,584][233954] Mean Reward across all agents: 99.53950869938566[0m
[37m[1m[2023-07-11 07:15:08,584][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:15:08,588][233954] mean_value=-336.1124424136005, max_value=777.859660387896[0m
[37m[1m[2023-07-11 07:15:08,591][233954] New mean coefficients: [[-0.7036364   2.0093277  -0.48125327 -2.1456819  -0.2453768  -4.6765723 ]][0m
[37m[1m[2023-07-11 07:15:08,592][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:15:17,585][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 07:15:17,585][233954] FPS: 427043.69[0m
[36m[2023-07-11 07:15:17,588][233954] itr=546, itrs=2000, Progress: 27.30%[0m
[36m[2023-07-11 07:15:29,187][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 07:15:29,187][233954] FPS: 333394.17[0m
[36m[2023-07-11 07:15:33,389][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:15:33,389][233954] Reward + Measures: [[60.10345212  0.083304    0.15913367  0.12136533  0.09787166  1.20834339]][0m
[37m[1m[2023-07-11 07:15:33,389][233954] Max Reward on eval: 60.10345211928889[0m
[37m[1m[2023-07-11 07:15:33,390][233954] Min Reward on eval: 60.10345211928889[0m
[37m[1m[2023-07-11 07:15:33,390][233954] Mean Reward across all agents: 60.10345211928889[0m
[37m[1m[2023-07-11 07:15:33,390][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:15:38,393][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:15:38,393][233954] Reward + Measures: [[230.20097099   0.16440001   0.43409997   0.20990001   0.3529
    2.10690284]
 [ 63.60988142   0.33660001   0.57709998   0.3179       0.4982
    2.45722628]
 [216.86788367   0.1211       0.80389994   0.40290004   0.68529999
    2.63522935]
 ...
 [ 42.74107551   0.0809       0.30080003   0.126        0.1666
    1.40133846]
 [187.31727938   0.22930001   0.62279999   0.25209999   0.4303
    2.14928627]
 [ 63.38939981   0.1227       0.81590003   0.23910001   0.63929999
    2.95836735]][0m
[37m[1m[2023-07-11 07:15:38,394][233954] Max Reward on eval: 646.9054374415427[0m
[37m[1m[2023-07-11 07:15:38,394][233954] Min Reward on eval: -165.84715120457113[0m
[37m[1m[2023-07-11 07:15:38,394][233954] Mean Reward across all agents: 169.9292286774898[0m
[37m[1m[2023-07-11 07:15:38,394][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:15:38,402][233954] mean_value=-104.84822460999017, max_value=788.9604992420412[0m
[37m[1m[2023-07-11 07:15:38,404][233954] New mean coefficients: [[-1.2035851  1.381442  -0.9927824 -3.8266075 -0.8228042 -5.0151477]][0m
[37m[1m[2023-07-11 07:15:38,405][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:15:47,339][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 07:15:47,339][233954] FPS: 429903.93[0m
[36m[2023-07-11 07:15:47,342][233954] itr=547, itrs=2000, Progress: 27.35%[0m
[36m[2023-07-11 07:15:58,966][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 07:15:58,966][233954] FPS: 332707.49[0m
[36m[2023-07-11 07:16:03,229][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:16:03,229][233954] Reward + Measures: [[58.76879903  0.08041466  0.16579266  0.12456866  0.094058    1.18109846]][0m
[37m[1m[2023-07-11 07:16:03,229][233954] Max Reward on eval: 58.76879903286723[0m
[37m[1m[2023-07-11 07:16:03,230][233954] Min Reward on eval: 58.76879903286723[0m
[37m[1m[2023-07-11 07:16:03,230][233954] Mean Reward across all agents: 58.76879903286723[0m
[37m[1m[2023-07-11 07:16:03,230][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:16:08,212][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:16:08,212][233954] Reward + Measures: [[ 30.91375239   0.1128       0.81329995   0.43109998   0.77340001
    2.68219852]
 [ 86.95671928   0.1894       0.3955       0.24039999   0.36929998
    2.40696216]
 [126.20803737   0.0075       0.99440002   0.51259995   0.98680001
    3.38093829]
 ...
 [-76.81274183   0.5327       0.7396       0.58350003   0.80360001
    3.1035459 ]
 [-26.86420178   0.206        0.48289999   0.2271       0.45050001
    2.84649587]
 [-17.30504933   0.18839999   0.46740004   0.20770001   0.46879998
    2.51065707]][0m
[37m[1m[2023-07-11 07:16:08,212][233954] Max Reward on eval: 592.6874046177138[0m
[37m[1m[2023-07-11 07:16:08,213][233954] Min Reward on eval: -118.94870602322044[0m
[37m[1m[2023-07-11 07:16:08,213][233954] Mean Reward across all agents: 50.4846289125115[0m
[37m[1m[2023-07-11 07:16:08,213][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:16:08,216][233954] mean_value=-210.54108993406976, max_value=539.3122285223546[0m
[37m[1m[2023-07-11 07:16:08,219][233954] New mean coefficients: [[-0.6053812   1.4995843  -0.949478   -2.7663865  -0.60495776 -4.698052  ]][0m
[37m[1m[2023-07-11 07:16:08,220][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:16:17,209][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 07:16:17,209][233954] FPS: 427272.94[0m
[36m[2023-07-11 07:16:17,211][233954] itr=548, itrs=2000, Progress: 27.40%[0m
[36m[2023-07-11 07:16:28,780][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 07:16:28,780][233954] FPS: 334284.36[0m
[36m[2023-07-11 07:16:33,133][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:16:33,134][233954] Reward + Measures: [[56.18506866  0.08034533  0.158085    0.11790434  0.09624501  1.18136191]][0m
[37m[1m[2023-07-11 07:16:33,134][233954] Max Reward on eval: 56.18506866488885[0m
[37m[1m[2023-07-11 07:16:33,134][233954] Min Reward on eval: 56.18506866488885[0m
[37m[1m[2023-07-11 07:16:33,134][233954] Mean Reward across all agents: 56.18506866488885[0m
[37m[1m[2023-07-11 07:16:33,135][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:16:38,116][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:16:38,117][233954] Reward + Measures: [[ -27.44886892    0.43449998    0.67449999    0.41940004    0.70770001
     2.97726893]
 [ -28.40624337    0.14819999    0.4131        0.18880001    0.3188
     2.80609751]
 [-107.60185003    0.17209999    0.78589994    0.2079        0.76360005
     2.97499204]
 ...
 [  31.85701894    0.11059999    0.18859999    0.12809999    0.22049999
     2.77544808]
 [ 286.8664618     0.21280001    0.4522        0.30849999    0.5255
     2.72505569]
 [  85.87781882    0.1461        0.82740003    0.1372        0.85270005
     2.52046609]][0m
[37m[1m[2023-07-11 07:16:38,117][233954] Max Reward on eval: 406.44211913347243[0m
[37m[1m[2023-07-11 07:16:38,117][233954] Min Reward on eval: -161.97333859568462[0m
[37m[1m[2023-07-11 07:16:38,118][233954] Mean Reward across all agents: 38.80081431039949[0m
[37m[1m[2023-07-11 07:16:38,118][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:16:38,121][233954] mean_value=-193.05583944878416, max_value=565.413157421099[0m
[37m[1m[2023-07-11 07:16:38,123][233954] New mean coefficients: [[ 0.255382    1.6403606  -0.06919426 -2.2717385  -0.67952836 -4.344513  ]][0m
[37m[1m[2023-07-11 07:16:38,124][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:16:47,136][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 07:16:47,136][233954] FPS: 426145.53[0m
[36m[2023-07-11 07:16:47,139][233954] itr=549, itrs=2000, Progress: 27.45%[0m
[36m[2023-07-11 07:16:59,011][233954] train() took 11.79 seconds to complete[0m
[36m[2023-07-11 07:16:59,011][233954] FPS: 325808.17[0m
[36m[2023-07-11 07:17:03,335][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:17:03,336][233954] Reward + Measures: [[55.21063803  0.08650533  0.14264534  0.10210934  0.10268734  1.20731223]][0m
[37m[1m[2023-07-11 07:17:03,336][233954] Max Reward on eval: 55.21063802607103[0m
[37m[1m[2023-07-11 07:17:03,336][233954] Min Reward on eval: 55.21063802607103[0m
[37m[1m[2023-07-11 07:17:03,337][233954] Mean Reward across all agents: 55.21063802607103[0m
[37m[1m[2023-07-11 07:17:03,337][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:17:08,584][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:17:08,584][233954] Reward + Measures: [[ 134.05871153    0.176         0.42939997    0.15710001    0.27700001
     2.50953555]
 [ 299.35854006    0.1238        0.67079997    0.3089        0.63569993
     2.28979373]
 [-233.47066973    0.52109998    0.83590001    0.18520001    0.82139999
     3.17573786]
 ...
 [ 189.75496818    0.0535        0.8811        0.33810002    0.75850004
     3.27141356]
 [ 255.03284338    0.12          0.6778        0.48669997    0.61680001
     2.19732118]
 [ 283.35977698    0.0417        0.92410004    0.41990003    0.83459997
     3.27000594]][0m
[37m[1m[2023-07-11 07:17:08,585][233954] Max Reward on eval: 681.6584167152644[0m
[37m[1m[2023-07-11 07:17:08,585][233954] Min Reward on eval: -717.8997287517413[0m
[37m[1m[2023-07-11 07:17:08,585][233954] Mean Reward across all agents: 72.26518348994189[0m
[37m[1m[2023-07-11 07:17:08,585][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:17:08,589][233954] mean_value=-184.41247673695614, max_value=610.7618071084433[0m
[37m[1m[2023-07-11 07:17:08,592][233954] New mean coefficients: [[ 0.07017979  1.4936395  -0.38203195 -2.009785   -0.6735462  -4.8771296 ]][0m
[37m[1m[2023-07-11 07:17:08,593][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:17:17,658][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 07:17:17,658][233954] FPS: 423700.37[0m
[36m[2023-07-11 07:17:17,660][233954] itr=550, itrs=2000, Progress: 27.50%[0m
[37m[1m[2023-07-11 07:20:30,058][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000530[0m
[36m[2023-07-11 07:20:43,505][233954] train() took 11.76 seconds to complete[0m
[36m[2023-07-11 07:20:43,506][233954] FPS: 326395.19[0m
[36m[2023-07-11 07:20:47,538][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:20:47,538][233954] Reward + Measures: [[54.42327635  0.087052    0.140714    0.10189433  0.105141    1.2067399 ]][0m
[37m[1m[2023-07-11 07:20:47,539][233954] Max Reward on eval: 54.42327634801813[0m
[37m[1m[2023-07-11 07:20:47,539][233954] Min Reward on eval: 54.42327634801813[0m
[37m[1m[2023-07-11 07:20:47,539][233954] Mean Reward across all agents: 54.42327634801813[0m
[37m[1m[2023-07-11 07:20:47,540][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:20:52,499][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:20:52,500][233954] Reward + Measures: [[-40.70020204   0.2254       0.4244       0.22680001   0.38699999
    2.61533618]
 [-17.09685468   0.86910003   0.9285       0.84610003   0.92129993
    2.60926747]
 [ 30.18344932   0.86919993   0.9149       0.81680006   0.94390005
    2.78154302]
 ...
 [ 17.42730809   0.71640003   0.85780001   0.67989999   0.85760003
    2.96708941]
 [-21.72059808   0.6074       0.80760002   0.58710003   0.79479998
    2.61664176]
 [110.80800105   0.0807       0.26910001   0.16680001   0.2897
    3.03817415]][0m
[37m[1m[2023-07-11 07:20:52,500][233954] Max Reward on eval: 548.1673431290313[0m
[37m[1m[2023-07-11 07:20:52,500][233954] Min Reward on eval: -79.92370367124677[0m
[37m[1m[2023-07-11 07:20:52,501][233954] Mean Reward across all agents: 61.52736763530566[0m
[37m[1m[2023-07-11 07:20:52,501][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:20:52,510][233954] mean_value=-139.40007296521787, max_value=518.6357545565442[0m
[37m[1m[2023-07-11 07:20:52,513][233954] New mean coefficients: [[-0.4675618   1.7355626  -2.079688   -1.9885966  -0.12014771 -4.980425  ]][0m
[37m[1m[2023-07-11 07:20:52,514][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:21:01,442][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 07:21:01,443][233954] FPS: 430157.49[0m
[36m[2023-07-11 07:21:01,445][233954] itr=551, itrs=2000, Progress: 27.55%[0m
[36m[2023-07-11 07:21:12,971][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 07:21:12,971][233954] FPS: 335523.45[0m
[36m[2023-07-11 07:21:17,255][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:21:17,256][233954] Reward + Measures: [[54.11224323  0.09189034  0.13383633  0.09720033  0.11277334  1.24536705]][0m
[37m[1m[2023-07-11 07:21:17,256][233954] Max Reward on eval: 54.112243233384255[0m
[37m[1m[2023-07-11 07:21:17,256][233954] Min Reward on eval: 54.112243233384255[0m
[37m[1m[2023-07-11 07:21:17,257][233954] Mean Reward across all agents: 54.112243233384255[0m
[37m[1m[2023-07-11 07:21:17,257][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:21:22,187][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:21:22,188][233954] Reward + Measures: [[-64.5376632    0.3163       0.66409999   0.18460001   0.65450001
    2.75030875]
 [ 17.17174148   0.83920002   0.96870005   0.82790005   0.88029999
    3.31521678]
 [-20.99767007   0.68919998   0.72390002   0.67109996   0.71579999
    2.59019709]
 ...
 [-28.99348046   0.96589994   0.95819998   0.98210001   0.97589999
    3.59031463]
 [ 52.84652057   0.29620001   0.6024       0.29750001   0.60600001
    2.88874269]
 [ -8.05962692   0.71029997   0.79879999   0.67160004   0.78689998
    2.13197303]][0m
[37m[1m[2023-07-11 07:21:22,188][233954] Max Reward on eval: 529.0880365230144[0m
[37m[1m[2023-07-11 07:21:22,188][233954] Min Reward on eval: -147.9284601341933[0m
[37m[1m[2023-07-11 07:21:22,189][233954] Mean Reward across all agents: 43.89582513412094[0m
[37m[1m[2023-07-11 07:21:22,189][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:21:22,193][233954] mean_value=-123.33323486404254, max_value=515.1234340864689[0m
[37m[1m[2023-07-11 07:21:22,196][233954] New mean coefficients: [[-0.7820092   1.2939594  -2.04308    -2.2857044  -0.50483805 -5.669462  ]][0m
[37m[1m[2023-07-11 07:21:22,197][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:21:31,296][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 07:21:31,296][233954] FPS: 422083.91[0m
[36m[2023-07-11 07:21:31,298][233954] itr=552, itrs=2000, Progress: 27.60%[0m
[36m[2023-07-11 07:21:43,094][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 07:21:43,095][233954] FPS: 327804.82[0m
[36m[2023-07-11 07:21:47,438][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:21:47,438][233954] Reward + Measures: [[52.32496708  0.08761833  0.13440233  0.096079    0.11052866  1.23316073]][0m
[37m[1m[2023-07-11 07:21:47,438][233954] Max Reward on eval: 52.32496708411774[0m
[37m[1m[2023-07-11 07:21:47,439][233954] Min Reward on eval: 52.32496708411774[0m
[37m[1m[2023-07-11 07:21:47,439][233954] Mean Reward across all agents: 52.32496708411774[0m
[37m[1m[2023-07-11 07:21:47,439][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:21:52,376][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:21:52,377][233954] Reward + Measures: [[228.28263379   0.13450001   0.59849995   0.43290001   0.58770001
    2.53175235]
 [ 57.65795605   0.12180001   0.48319998   0.17300001   0.34570003
    2.5189321 ]
 [ 45.67654737   0.3213       0.79540002   0.3856       0.7604
    2.69978189]
 ...
 [ 70.70620963   0.25740001   0.26010001   0.21470001   0.1866
    2.46197557]
 [ 79.02885591   0.06280001   0.228        0.14579999   0.24989998
    2.89859748]
 [-27.14319883   0.53549999   0.3581       0.47489998   0.18560001
    2.43534636]][0m
[37m[1m[2023-07-11 07:21:52,377][233954] Max Reward on eval: 567.4809684984386[0m
[37m[1m[2023-07-11 07:21:52,377][233954] Min Reward on eval: -184.9231381483376[0m
[37m[1m[2023-07-11 07:21:52,377][233954] Mean Reward across all agents: 37.32154960374017[0m
[37m[1m[2023-07-11 07:21:52,378][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:21:52,381][233954] mean_value=-182.6509478261859, max_value=533.173121687211[0m
[37m[1m[2023-07-11 07:21:52,384][233954] New mean coefficients: [[-0.08601797  1.6392856  -1.1585054  -2.1256733  -0.12853357 -5.21705   ]][0m
[37m[1m[2023-07-11 07:21:52,385][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:22:01,330][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 07:22:01,330][233954] FPS: 429350.25[0m
[36m[2023-07-11 07:22:01,333][233954] itr=553, itrs=2000, Progress: 27.65%[0m
[36m[2023-07-11 07:22:12,988][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 07:22:12,989][233954] FPS: 331893.88[0m
[36m[2023-07-11 07:22:17,263][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:22:17,264][233954] Reward + Measures: [[52.38531837  0.08912767  0.12129566  0.09050067  0.11550433  1.26309597]][0m
[37m[1m[2023-07-11 07:22:17,264][233954] Max Reward on eval: 52.3853183721536[0m
[37m[1m[2023-07-11 07:22:17,264][233954] Min Reward on eval: 52.3853183721536[0m
[37m[1m[2023-07-11 07:22:17,265][233954] Mean Reward across all agents: 52.3853183721536[0m
[37m[1m[2023-07-11 07:22:17,265][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:22:22,362][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:22:22,363][233954] Reward + Measures: [[168.8602617    0.1104       0.39230001   0.2256       0.32780001
    2.39770412]
 [ 92.96831232   0.1001       0.54479998   0.19840001   0.47130004
    3.00597978]
 [ 69.68904249   0.84200001   0.88100004   0.8229       0.86020005
    3.26801848]
 ...
 [230.15250585   0.0843       0.88249999   0.42319998   0.7942
    3.06663489]
 [ 83.27788259   0.57310003   0.78859997   0.52769995   0.70950001
    3.01070642]
 [ 82.43592377   0.21939997   0.48709998   0.2184       0.419
    2.41751862]][0m
[37m[1m[2023-07-11 07:22:22,363][233954] Max Reward on eval: 722.6000976583921[0m
[37m[1m[2023-07-11 07:22:22,363][233954] Min Reward on eval: -106.44223740254529[0m
[37m[1m[2023-07-11 07:22:22,363][233954] Mean Reward across all agents: 152.08712370191444[0m
[37m[1m[2023-07-11 07:22:22,364][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:22:22,369][233954] mean_value=-92.16034465784291, max_value=746.01937103346[0m
[37m[1m[2023-07-11 07:22:22,371][233954] New mean coefficients: [[-1.4184588   1.4215144  -2.4920197  -2.7544231  -0.37935144 -6.1796074 ]][0m
[37m[1m[2023-07-11 07:22:22,372][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:22:31,409][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 07:22:31,409][233954] FPS: 425024.90[0m
[36m[2023-07-11 07:22:31,411][233954] itr=554, itrs=2000, Progress: 27.70%[0m
[36m[2023-07-11 07:22:43,320][233954] train() took 11.82 seconds to complete[0m
[36m[2023-07-11 07:22:43,320][233954] FPS: 324785.13[0m
[36m[2023-07-11 07:22:47,655][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:22:47,655][233954] Reward + Measures: [[51.35453635  0.088229    0.12195766  0.09231933  0.11254767  1.23358989]][0m
[37m[1m[2023-07-11 07:22:47,655][233954] Max Reward on eval: 51.35453634877373[0m
[37m[1m[2023-07-11 07:22:47,656][233954] Min Reward on eval: 51.35453634877373[0m
[37m[1m[2023-07-11 07:22:47,656][233954] Mean Reward across all agents: 51.35453634877373[0m
[37m[1m[2023-07-11 07:22:47,656][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:22:52,613][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:22:52,613][233954] Reward + Measures: [[-48.75276018   0.13880001   0.60190004   0.20970002   0.54870003
    2.3427422 ]
 [ 95.77649654   0.13579999   0.76730001   0.21090002   0.66450006
    2.83594012]
 [-10.35103042   0.88819999   0.98089999   0.88590002   0.94539994
    2.7082262 ]
 ...
 [ 90.91399972   0.36019999   0.71969998   0.24949999   0.72320002
    3.42218375]
 [-22.64612117   0.74980003   0.92500001   0.7676       0.96399993
    2.95023084]
 [162.84621766   0.1002       0.42809996   0.25299999   0.43740001
    3.26780319]][0m
[37m[1m[2023-07-11 07:22:52,614][233954] Max Reward on eval: 537.1506004342809[0m
[37m[1m[2023-07-11 07:22:52,614][233954] Min Reward on eval: -160.9650533080101[0m
[37m[1m[2023-07-11 07:22:52,614][233954] Mean Reward across all agents: 30.166342181451874[0m
[37m[1m[2023-07-11 07:22:52,614][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:22:52,618][233954] mean_value=-165.50914285345232, max_value=471.1965309957042[0m
[37m[1m[2023-07-11 07:22:52,620][233954] New mean coefficients: [[-1.7406934   1.71541    -2.1360383  -2.5949337   0.15797156 -6.090983  ]][0m
[37m[1m[2023-07-11 07:22:52,621][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:23:01,640][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 07:23:01,640][233954] FPS: 425860.64[0m
[36m[2023-07-11 07:23:01,642][233954] itr=555, itrs=2000, Progress: 27.75%[0m
[36m[2023-07-11 07:23:13,386][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 07:23:13,386][233954] FPS: 329267.37[0m
[36m[2023-07-11 07:23:17,747][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:23:17,747][233954] Reward + Measures: [[51.92178966  0.08824467  0.12096033  0.09377899  0.11345766  1.20636725]][0m
[37m[1m[2023-07-11 07:23:17,747][233954] Max Reward on eval: 51.92178966182616[0m
[37m[1m[2023-07-11 07:23:17,748][233954] Min Reward on eval: 51.92178966182616[0m
[37m[1m[2023-07-11 07:23:17,748][233954] Mean Reward across all agents: 51.92178966182616[0m
[37m[1m[2023-07-11 07:23:17,748][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:23:22,703][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:23:22,704][233954] Reward + Measures: [[  8.08116832   0.32230002   0.51820004   0.31350002   0.53180003
    2.79060435]
 [-32.48443075   0.17739999   0.38690001   0.22550002   0.27420002
    2.37449622]
 [196.43012834   0.28630003   0.38249999   0.2965       0.31420001
    2.2864635 ]
 ...
 [  7.94950205   0.94049996   0.97139996   0.95810002   0.98089999
    3.53056264]
 [-72.5124644    0.33039999   0.51969999   0.3125       0.5557
    2.40793157]
 [ -6.04506255   0.0904       0.24770001   0.12770002   0.17990001
    2.46341038]][0m
[37m[1m[2023-07-11 07:23:22,704][233954] Max Reward on eval: 687.8899307400919[0m
[37m[1m[2023-07-11 07:23:22,705][233954] Min Reward on eval: -72.51246439693496[0m
[37m[1m[2023-07-11 07:23:22,705][233954] Mean Reward across all agents: 69.04479493320059[0m
[37m[1m[2023-07-11 07:23:22,705][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:23:22,708][233954] mean_value=-215.56546634573078, max_value=456.0874729483413[0m
[37m[1m[2023-07-11 07:23:22,710][233954] New mean coefficients: [[-1.214698   2.0253448 -1.35779   -1.8631483  0.3657875 -5.5122952]][0m
[37m[1m[2023-07-11 07:23:22,711][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:23:31,657][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 07:23:31,658][233954] FPS: 429282.72[0m
[36m[2023-07-11 07:23:31,660][233954] itr=556, itrs=2000, Progress: 27.80%[0m
[36m[2023-07-11 07:23:43,254][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 07:23:43,254][233954] FPS: 333640.78[0m
[36m[2023-07-11 07:23:47,488][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:23:47,494][233954] Reward + Measures: [[51.14971789  0.08193066  0.11514166  0.08919067  0.10679568  1.17875051]][0m
[37m[1m[2023-07-11 07:23:47,494][233954] Max Reward on eval: 51.14971788972294[0m
[37m[1m[2023-07-11 07:23:47,495][233954] Min Reward on eval: 51.14971788972294[0m
[37m[1m[2023-07-11 07:23:47,495][233954] Mean Reward across all agents: 51.14971788972294[0m
[37m[1m[2023-07-11 07:23:47,495][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:23:52,440][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:23:52,440][233954] Reward + Measures: [[ 61.71717318   0.2098       0.52109998   0.23410001   0.57420003
    2.71824145]
 [ 20.81079293   0.68080002   0.78080004   0.6735       0.74819994
    2.34588957]
 [141.88352871   0.3026       0.81389999   0.329        0.69440001
    2.64365363]
 ...
 [ 53.62541683   0.42340001   0.39540002   0.40920001   0.43779999
    2.6551857 ]
 [142.27618169   0.1521       0.92819995   0.2861       0.78370005
    2.55808806]
 [ 24.70365302   0.71680003   0.86500007   0.6724       0.84860003
    2.59919715]][0m
[37m[1m[2023-07-11 07:23:52,440][233954] Max Reward on eval: 229.7907066327054[0m
[37m[1m[2023-07-11 07:23:52,441][233954] Min Reward on eval: -131.94254685253836[0m
[37m[1m[2023-07-11 07:23:52,441][233954] Mean Reward across all agents: 65.00024659333047[0m
[37m[1m[2023-07-11 07:23:52,441][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:23:52,445][233954] mean_value=-127.92645663792506, max_value=527.768632743461[0m
[37m[1m[2023-07-11 07:23:52,447][233954] New mean coefficients: [[-0.15290523  1.5032206  -0.3418423  -1.6674821  -0.6130754  -5.3126636 ]][0m
[37m[1m[2023-07-11 07:23:52,448][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:24:01,425][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 07:24:01,425][233954] FPS: 427845.37[0m
[36m[2023-07-11 07:24:01,428][233954] itr=557, itrs=2000, Progress: 27.85%[0m
[36m[2023-07-11 07:24:13,070][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 07:24:13,070][233954] FPS: 332216.85[0m
[36m[2023-07-11 07:24:17,415][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:24:17,415][233954] Reward + Measures: [[48.77357167  0.08392933  0.12539233  0.09424766  0.10886467  1.16751182]][0m
[37m[1m[2023-07-11 07:24:17,415][233954] Max Reward on eval: 48.773571668737006[0m
[37m[1m[2023-07-11 07:24:17,416][233954] Min Reward on eval: 48.773571668737006[0m
[37m[1m[2023-07-11 07:24:17,416][233954] Mean Reward across all agents: 48.773571668737006[0m
[37m[1m[2023-07-11 07:24:17,416][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:24:22,632][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:24:22,633][233954] Reward + Measures: [[168.17882871   0.09020001   0.45140001   0.2191       0.43170005
    2.74009013]
 [200.05944372   0.0929       0.2737       0.19170001   0.35139999
    2.89874053]
 [ 89.79030514   0.96139997   0.97360003   0.94330007   0.97080004
    2.74167299]
 ...
 [ 62.43164016   0.0385       0.98479998   0.37800002   0.97699994
    3.5095284 ]
 [ 55.10056591   0.49530002   0.8696       0.64820004   0.86140007
    2.96180511]
 [ 49.90431309   0.95310003   0.98699999   0.95209998   0.98670006
    3.09172869]][0m
[37m[1m[2023-07-11 07:24:22,634][233954] Max Reward on eval: 608.7743189982605[0m
[37m[1m[2023-07-11 07:24:22,634][233954] Min Reward on eval: -180.63372706510125[0m
[37m[1m[2023-07-11 07:24:22,634][233954] Mean Reward across all agents: 35.12115371138584[0m
[37m[1m[2023-07-11 07:24:22,634][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:24:22,637][233954] mean_value=-146.56767655286248, max_value=392.08802804980974[0m
[37m[1m[2023-07-11 07:24:22,640][233954] New mean coefficients: [[ 0.22217816  1.3616294  -0.21620402 -0.9771147  -0.5899879  -5.113425  ]][0m
[37m[1m[2023-07-11 07:24:22,641][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:24:31,686][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 07:24:31,687][233954] FPS: 424588.64[0m
[36m[2023-07-11 07:24:31,689][233954] itr=558, itrs=2000, Progress: 27.90%[0m
[36m[2023-07-11 07:24:43,404][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 07:24:43,405][233954] FPS: 330062.65[0m
[36m[2023-07-11 07:24:47,716][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:24:47,716][233954] Reward + Measures: [[47.84026684  0.077694    0.12895399  0.09480367  0.10042333  1.12754488]][0m
[37m[1m[2023-07-11 07:24:47,716][233954] Max Reward on eval: 47.8402668387222[0m
[37m[1m[2023-07-11 07:24:47,717][233954] Min Reward on eval: 47.8402668387222[0m
[37m[1m[2023-07-11 07:24:47,717][233954] Mean Reward across all agents: 47.8402668387222[0m
[37m[1m[2023-07-11 07:24:47,717][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:24:52,662][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:24:52,663][233954] Reward + Measures: [[ 76.34473904   0.31420001   0.41220003   0.09150001   0.40970001
    2.71333432]
 [ 93.43972722   0.113        0.06620001   0.0677       0.1426
    3.28101039]
 [ 91.30607523   0.2203       0.16060001   0.13690001   0.26460001
    2.71583462]
 ...
 [  0.95744383   0.0784       0.98409998   0.2509       0.9752
    3.10651946]
 [576.18920852   0.1069       0.88819999   0.77249998   0.8854
    3.83248019]
 [124.46097495   0.26300001   0.1883       0.15320002   0.30650002
    3.06301665]][0m
[37m[1m[2023-07-11 07:24:52,663][233954] Max Reward on eval: 576.1892085203435[0m
[37m[1m[2023-07-11 07:24:52,663][233954] Min Reward on eval: -156.30279442537577[0m
[37m[1m[2023-07-11 07:24:52,663][233954] Mean Reward across all agents: 60.04010232421863[0m
[37m[1m[2023-07-11 07:24:52,664][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:24:52,668][233954] mean_value=-95.79157465529946, max_value=649.3615925462916[0m
[37m[1m[2023-07-11 07:24:52,671][233954] New mean coefficients: [[ 0.14764197  1.304708   -0.52232957 -0.43970692 -0.31009996 -5.4064107 ]][0m
[37m[1m[2023-07-11 07:24:52,672][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:25:01,756][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 07:25:01,757][233954] FPS: 422787.39[0m
[36m[2023-07-11 07:25:01,759][233954] itr=559, itrs=2000, Progress: 27.95%[0m
[36m[2023-07-11 07:25:13,521][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 07:25:13,521][233954] FPS: 328758.53[0m
[36m[2023-07-11 07:25:17,827][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:25:17,827][233954] Reward + Measures: [[47.89555507  0.07919267  0.129307    0.09582434  0.10011899  1.1277982 ]][0m
[37m[1m[2023-07-11 07:25:17,827][233954] Max Reward on eval: 47.89555506677982[0m
[37m[1m[2023-07-11 07:25:17,828][233954] Min Reward on eval: 47.89555506677982[0m
[37m[1m[2023-07-11 07:25:17,828][233954] Mean Reward across all agents: 47.89555506677982[0m
[37m[1m[2023-07-11 07:25:17,828][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:25:22,870][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:25:22,871][233954] Reward + Measures: [[ 37.97208397   0.7245       0.8646       0.72439998   0.79180002
    3.11165857]
 [ 71.8341782    0.08350001   0.7209       0.36330003   0.65360004
    2.62433839]
 [181.13433532   0.0815       0.61560005   0.46339998   0.59000003
    3.5734117 ]
 ...
 [ 36.60616566   0.63880008   0.77629995   0.58389997   0.70789999
    2.63096786]
 [227.44162538   0.0768       0.66769999   0.52810001   0.64460009
    2.84643602]
 [147.88052271   0.0701       0.61559999   0.43619999   0.53619999
    2.49131823]][0m
[37m[1m[2023-07-11 07:25:22,871][233954] Max Reward on eval: 609.6415347836912[0m
[37m[1m[2023-07-11 07:25:22,871][233954] Min Reward on eval: -63.65022377846763[0m
[37m[1m[2023-07-11 07:25:22,872][233954] Mean Reward across all agents: 83.81486531089854[0m
[37m[1m[2023-07-11 07:25:22,872][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:25:22,875][233954] mean_value=-155.61280092938304, max_value=439.1303036246728[0m
[37m[1m[2023-07-11 07:25:22,877][233954] New mean coefficients: [[-0.32929432  1.6203656  -0.52001685 -0.18520606 -0.17481901 -5.738032  ]][0m
[37m[1m[2023-07-11 07:25:22,878][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:25:31,855][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 07:25:31,856][233954] FPS: 427819.39[0m
[36m[2023-07-11 07:25:31,858][233954] itr=560, itrs=2000, Progress: 28.00%[0m
[37m[1m[2023-07-11 07:28:48,014][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000540[0m
[36m[2023-07-11 07:29:00,846][233954] train() took 11.79 seconds to complete[0m
[36m[2023-07-11 07:29:00,846][233954] FPS: 325824.07[0m
[36m[2023-07-11 07:29:04,996][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:29:04,996][233954] Reward + Measures: [[46.30121309  0.07830933  0.12702633  0.09441933  0.10377999  1.13554621]][0m
[37m[1m[2023-07-11 07:29:04,996][233954] Max Reward on eval: 46.30121308700269[0m
[37m[1m[2023-07-11 07:29:04,997][233954] Min Reward on eval: 46.30121308700269[0m
[37m[1m[2023-07-11 07:29:04,997][233954] Mean Reward across all agents: 46.30121308700269[0m
[37m[1m[2023-07-11 07:29:04,997][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:29:09,946][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:29:09,946][233954] Reward + Measures: [[  87.23380452    0.1767        0.48990002    0.17550002    0.57980001
     2.8714335 ]
 [ 314.07892256    0.32949999    0.91930008    0.7313        0.88759995
     2.91143394]
 [ -38.73673579    0.59009999    0.6462        0.58500004    0.6656
     3.00224185]
 ...
 [ 125.99884803    0.19660001    0.8599        0.41120002    0.86730003
     3.48385549]
 [-129.39986275    0.86390001    0.89200002    0.84250003    0.86920005
     3.15360951]
 [ 109.62866546    0.88110012    0.91250002    0.82370007    0.88410008
     2.35114145]][0m
[37m[1m[2023-07-11 07:29:09,946][233954] Max Reward on eval: 582.0331383438315[0m
[37m[1m[2023-07-11 07:29:09,947][233954] Min Reward on eval: -374.3670415783301[0m
[37m[1m[2023-07-11 07:29:09,947][233954] Mean Reward across all agents: 61.72498691296396[0m
[37m[1m[2023-07-11 07:29:09,947][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:29:09,961][233954] mean_value=-143.72952813326708, max_value=619.6181991383434[0m
[37m[1m[2023-07-11 07:29:09,965][233954] New mean coefficients: [[-1.2130392  1.0252492 -1.4255471 -0.6216362 -0.8272325 -6.7133512]][0m
[37m[1m[2023-07-11 07:29:09,967][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:29:18,920][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 07:29:18,920][233954] FPS: 429026.21[0m
[36m[2023-07-11 07:29:18,922][233954] itr=561, itrs=2000, Progress: 28.05%[0m
[36m[2023-07-11 07:29:30,413][233954] train() took 11.41 seconds to complete[0m
[36m[2023-07-11 07:29:30,413][233954] FPS: 336573.36[0m
[36m[2023-07-11 07:29:34,632][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:29:34,632][233954] Reward + Measures: [[43.65537582  0.07561133  0.14084001  0.10141601  0.098755    1.10938227]][0m
[37m[1m[2023-07-11 07:29:34,633][233954] Max Reward on eval: 43.6553758246059[0m
[37m[1m[2023-07-11 07:29:34,633][233954] Min Reward on eval: 43.6553758246059[0m
[37m[1m[2023-07-11 07:29:34,633][233954] Mean Reward across all agents: 43.6553758246059[0m
[37m[1m[2023-07-11 07:29:34,634][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:29:39,573][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:29:39,574][233954] Reward + Measures: [[19.92837508  0.1301      0.32689998  0.15200001  0.32379997  2.07443595]
 [30.35013249  0.1657      0.447       0.20720001  0.42280003  2.18519855]
 [75.56470511  0.1314      0.48420006  0.17150001  0.44850001  2.02329612]
 ...
 [10.53218268  0.45040002  0.65619999  0.41689998  0.60589999  2.58751369]
 [-5.00165767  0.62400001  0.75590003  0.6024      0.69460005  2.90631175]
 [ 9.21885587  0.1338      0.28819999  0.1269      0.23629999  1.96004319]][0m
[37m[1m[2023-07-11 07:29:39,574][233954] Max Reward on eval: 383.5612392234616[0m
[37m[1m[2023-07-11 07:29:39,575][233954] Min Reward on eval: -433.40465114200487[0m
[37m[1m[2023-07-11 07:29:39,575][233954] Mean Reward across all agents: 89.07311451986193[0m
[37m[1m[2023-07-11 07:29:39,575][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:29:39,579][233954] mean_value=-198.95336789736461, max_value=872.6179351688363[0m
[37m[1m[2023-07-11 07:29:39,581][233954] New mean coefficients: [[-1.4124479   1.4122725  -2.0849316  -0.53864634 -0.028319   -6.378081  ]][0m
[37m[1m[2023-07-11 07:29:39,583][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:29:48,500][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 07:29:48,501][233954] FPS: 430679.14[0m
[36m[2023-07-11 07:29:48,503][233954] itr=562, itrs=2000, Progress: 28.10%[0m
[36m[2023-07-11 07:30:00,352][233954] train() took 11.76 seconds to complete[0m
[36m[2023-07-11 07:30:00,353][233954] FPS: 326397.20[0m
[36m[2023-07-11 07:30:04,578][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:30:04,578][233954] Reward + Measures: [[42.03368148  0.07348166  0.14604066  0.11103234  0.09774633  1.06682372]][0m
[37m[1m[2023-07-11 07:30:04,579][233954] Max Reward on eval: 42.03368147629573[0m
[37m[1m[2023-07-11 07:30:04,579][233954] Min Reward on eval: 42.03368147629573[0m
[37m[1m[2023-07-11 07:30:04,579][233954] Mean Reward across all agents: 42.03368147629573[0m
[37m[1m[2023-07-11 07:30:04,579][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:30:09,569][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:30:09,570][233954] Reward + Measures: [[ 53.19960507   0.25479999   0.80949992   0.40019998   0.83290005
    3.30481696]
 [-28.09992969   0.41630003   0.51120001   0.4179       0.53610003
    2.92824912]
 [ 72.94072926   0.16150001   0.16199999   0.20549999   0.30100003
    2.40721679]
 ...
 [-14.66133805   0.0612       0.15340002   0.1012       0.12159999
    2.62783813]
 [107.34822082   0.1366       0.2825       0.1513       0.3163
    2.83358383]
 [107.5818963    0.1107       0.74659997   0.3662       0.75330001
    3.26334167]][0m
[37m[1m[2023-07-11 07:30:09,570][233954] Max Reward on eval: 647.5506419899873[0m
[37m[1m[2023-07-11 07:30:09,570][233954] Min Reward on eval: -212.2107963943854[0m
[37m[1m[2023-07-11 07:30:09,570][233954] Mean Reward across all agents: 79.0830507000122[0m
[37m[1m[2023-07-11 07:30:09,571][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:30:09,573][233954] mean_value=-199.22007347170475, max_value=243.23761666268575[0m
[37m[1m[2023-07-11 07:30:09,576][233954] New mean coefficients: [[-0.42778224  2.0082123  -0.7187431   0.09887588  0.38584545 -5.378442  ]][0m
[37m[1m[2023-07-11 07:30:09,577][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:30:18,609][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 07:30:18,609][233954] FPS: 425216.67[0m
[36m[2023-07-11 07:30:18,612][233954] itr=563, itrs=2000, Progress: 28.15%[0m
[36m[2023-07-11 07:30:30,179][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 07:30:30,179][233954] FPS: 334437.78[0m
[36m[2023-07-11 07:30:34,476][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:30:34,476][233954] Reward + Measures: [[43.92296875  0.07259233  0.14487965  0.11145366  0.09637367  1.06321418]][0m
[37m[1m[2023-07-11 07:30:34,476][233954] Max Reward on eval: 43.92296874592298[0m
[37m[1m[2023-07-11 07:30:34,477][233954] Min Reward on eval: 43.92296874592298[0m
[37m[1m[2023-07-11 07:30:34,477][233954] Mean Reward across all agents: 43.92296874592298[0m
[37m[1m[2023-07-11 07:30:34,477][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:30:39,436][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:30:39,436][233954] Reward + Measures: [[  88.34967322    0.10469999    0.13259999    0.089         0.1652
     2.54424024]
 [-141.98896446    0.29450002    0.60899997    0.28889999    0.73079997
     2.89223075]
 [-114.50451061    0.85960001    0.89540005    0.83090001    0.89560002
     3.34734511]
 ...
 [  33.04517945    0.17820001    0.2493        0.11080001    0.33019999
     2.82890701]
 [ -10.64336297    0.0715        0.0578        0.0873        0.1344
     3.0189476 ]
 [ -89.85841992    0.95769995    0.96750003    0.93559992    0.97369999
     3.41363335]][0m
[37m[1m[2023-07-11 07:30:39,436][233954] Max Reward on eval: 621.7236356656998[0m
[37m[1m[2023-07-11 07:30:39,437][233954] Min Reward on eval: -141.98896446041763[0m
[37m[1m[2023-07-11 07:30:39,437][233954] Mean Reward across all agents: 30.899360048527452[0m
[37m[1m[2023-07-11 07:30:39,437][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:30:39,440][233954] mean_value=-241.3983013493562, max_value=684.638419679217[0m
[37m[1m[2023-07-11 07:30:39,442][233954] New mean coefficients: [[ 0.04401749  1.9767854   0.19712818  0.4129122   0.2551735  -5.461463  ]][0m
[37m[1m[2023-07-11 07:30:39,443][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:30:48,346][233954] train() took 8.90 seconds to complete[0m
[36m[2023-07-11 07:30:48,346][233954] FPS: 431413.23[0m
[36m[2023-07-11 07:30:48,348][233954] itr=564, itrs=2000, Progress: 28.20%[0m
[36m[2023-07-11 07:30:59,962][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 07:30:59,962][233954] FPS: 332986.82[0m
[36m[2023-07-11 07:31:04,249][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:31:04,250][233954] Reward + Measures: [[42.79324494  0.070916    0.148406    0.11465533  0.09339766  1.04101908]][0m
[37m[1m[2023-07-11 07:31:04,250][233954] Max Reward on eval: 42.7932449387558[0m
[37m[1m[2023-07-11 07:31:04,250][233954] Min Reward on eval: 42.7932449387558[0m
[37m[1m[2023-07-11 07:31:04,251][233954] Mean Reward across all agents: 42.7932449387558[0m
[37m[1m[2023-07-11 07:31:04,251][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:31:09,240][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:31:09,241][233954] Reward + Measures: [[126.05931899   0.40279999   0.74530005   0.2978       0.75270003
    3.10204911]
 [130.03739955   0.10439999   0.82440007   0.39630002   0.76429999
    3.24721146]
 [118.65095996   0.3696       0.36919999   0.347        0.48359999
    3.11947989]
 ...
 [ 21.59637921   0.249        0.74640006   0.33380005   0.7044
    2.90997028]
 [ 60.58695098   0.10090001   0.8624       0.33220002   0.75550002
    3.30527115]
 [125.67418178   0.0874       0.39910004   0.2436       0.42799997
    3.12645149]][0m
[37m[1m[2023-07-11 07:31:09,241][233954] Max Reward on eval: 689.2275667490088[0m
[37m[1m[2023-07-11 07:31:09,241][233954] Min Reward on eval: -333.80418203752487[0m
[37m[1m[2023-07-11 07:31:09,242][233954] Mean Reward across all agents: 72.38997275096345[0m
[37m[1m[2023-07-11 07:31:09,242][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:31:09,245][233954] mean_value=-203.19086218639842, max_value=710.7886947185918[0m
[37m[1m[2023-07-11 07:31:09,247][233954] New mean coefficients: [[-1.4742093   1.1188632  -0.8769735   0.20973451 -0.5570374  -7.391919  ]][0m
[37m[1m[2023-07-11 07:31:09,248][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:31:18,324][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 07:31:18,324][233954] FPS: 423163.51[0m
[36m[2023-07-11 07:31:18,327][233954] itr=565, itrs=2000, Progress: 28.25%[0m
[36m[2023-07-11 07:31:30,167][233954] train() took 11.75 seconds to complete[0m
[36m[2023-07-11 07:31:30,167][233954] FPS: 326720.92[0m
[36m[2023-07-11 07:31:34,538][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:31:34,539][233954] Reward + Measures: [[42.29619674  0.06995066  0.14495432  0.11303433  0.09257001  1.0101639 ]][0m
[37m[1m[2023-07-11 07:31:34,539][233954] Max Reward on eval: 42.29619674445064[0m
[37m[1m[2023-07-11 07:31:34,539][233954] Min Reward on eval: 42.29619674445064[0m
[37m[1m[2023-07-11 07:31:34,539][233954] Mean Reward across all agents: 42.29619674445064[0m
[37m[1m[2023-07-11 07:31:34,540][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:31:39,906][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:31:39,906][233954] Reward + Measures: [[ -4.69103106   0.9849       0.9903       0.98400003   0.98579997
    3.27514577]
 [132.00481164   0.2599       0.71249998   0.16160001   0.7475
    2.51917243]
 [ 61.35733805   0.1138       0.2208       0.0767       0.19240001
    2.74971223]
 ...
 [ 54.91575026   0.94099998   0.97199994   0.91940004   0.95220006
    3.30704093]
 [  8.53502725   0.3924       0.81030005   0.1189       0.87360001
    3.12429357]
 [254.43081759   0.0223       0.9349001    0.4131       0.92469996
    2.69533539]][0m
[37m[1m[2023-07-11 07:31:39,907][233954] Max Reward on eval: 528.1928128499537[0m
[37m[1m[2023-07-11 07:31:39,907][233954] Min Reward on eval: -242.5765404706821[0m
[37m[1m[2023-07-11 07:31:39,907][233954] Mean Reward across all agents: 57.5245614360027[0m
[37m[1m[2023-07-11 07:31:39,907][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:31:39,912][233954] mean_value=-85.25950965513175, max_value=421.0524473805074[0m
[37m[1m[2023-07-11 07:31:39,915][233954] New mean coefficients: [[-2.3294895  0.9253907 -1.1682281  0.1141303 -0.8425168 -8.228455 ]][0m
[37m[1m[2023-07-11 07:31:39,916][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:31:48,976][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 07:31:48,976][233954] FPS: 423917.69[0m
[36m[2023-07-11 07:31:48,979][233954] itr=566, itrs=2000, Progress: 28.30%[0m
[36m[2023-07-11 07:32:00,706][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 07:32:00,706][233954] FPS: 329755.66[0m
[36m[2023-07-11 07:32:04,996][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:32:04,996][233954] Reward + Measures: [[41.01669056  0.07104634  0.14263068  0.10803967  0.09619633  1.04428983]][0m
[37m[1m[2023-07-11 07:32:04,997][233954] Max Reward on eval: 41.01669055886586[0m
[37m[1m[2023-07-11 07:32:04,997][233954] Min Reward on eval: 41.01669055886586[0m
[37m[1m[2023-07-11 07:32:04,997][233954] Mean Reward across all agents: 41.01669055886586[0m
[37m[1m[2023-07-11 07:32:04,997][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:32:10,011][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:32:10,012][233954] Reward + Measures: [[ -40.82072736    0.80670005    0.92579997    0.7658        0.86070007
     2.78090978]
 [  45.99815023    0.1178        0.76830006    0.27859998    0.662
     2.84251761]
 [  68.15899234    0.0976        0.5735001     0.18980001    0.49950001
     2.84720349]
 ...
 [  -3.9431167     0.1499        0.28370002    0.1258        0.26139998
     2.0931046 ]
 [ 598.05317973    0.0401        0.92309999    0.61289996    0.86680001
     2.90114546]
 [-129.83365654    0.17629999    0.74540007    0.21100001    0.80270004
     3.01107883]][0m
[37m[1m[2023-07-11 07:32:10,012][233954] Max Reward on eval: 598.0531797317788[0m
[37m[1m[2023-07-11 07:32:10,012][233954] Min Reward on eval: -325.53243038374933[0m
[37m[1m[2023-07-11 07:32:10,013][233954] Mean Reward across all agents: 78.08868678431722[0m
[37m[1m[2023-07-11 07:32:10,013][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:32:10,016][233954] mean_value=-173.25172299736636, max_value=492.7746388714463[0m
[37m[1m[2023-07-11 07:32:10,018][233954] New mean coefficients: [[-0.90397286  1.2227824  -0.0383817   0.10059129 -0.59905815 -7.363909  ]][0m
[37m[1m[2023-07-11 07:32:10,019][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:32:19,112][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 07:32:19,112][233954] FPS: 422395.06[0m
[36m[2023-07-11 07:32:19,114][233954] itr=567, itrs=2000, Progress: 28.35%[0m
[36m[2023-07-11 07:32:30,722][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 07:32:30,722][233954] FPS: 333182.30[0m
[36m[2023-07-11 07:32:34,941][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:32:34,941][233954] Reward + Measures: [[41.78102816  0.07083067  0.15155201  0.11632     0.09343968  1.02547693]][0m
[37m[1m[2023-07-11 07:32:34,941][233954] Max Reward on eval: 41.781028158286084[0m
[37m[1m[2023-07-11 07:32:34,941][233954] Min Reward on eval: 41.781028158286084[0m
[37m[1m[2023-07-11 07:32:34,942][233954] Mean Reward across all agents: 41.781028158286084[0m
[37m[1m[2023-07-11 07:32:34,942][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:32:40,027][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:32:40,028][233954] Reward + Measures: [[69.17483068  0.80319995  0.88449997  0.76340002  0.83640003  2.83509254]
 [-5.17520095  0.21710001  0.27340004  0.21010001  0.21970001  2.7175622 ]
 [ 4.24747728  0.98390001  0.99309999  0.98240006  0.98689997  3.15711975]
 ...
 [55.90317224  0.11830001  0.59210002  0.20719998  0.53619999  3.03190756]
 [-6.00188765  0.26480004  0.52679998  0.24919999  0.52890003  2.76549435]
 [ 8.221548    0.76270002  0.22239999  0.76060003  0.21710001  2.65119362]][0m
[37m[1m[2023-07-11 07:32:40,028][233954] Max Reward on eval: 650.7172279210761[0m
[37m[1m[2023-07-11 07:32:40,028][233954] Min Reward on eval: -134.81134366774933[0m
[37m[1m[2023-07-11 07:32:40,029][233954] Mean Reward across all agents: 50.1150990266273[0m
[37m[1m[2023-07-11 07:32:40,029][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:32:40,032][233954] mean_value=-202.33826895235896, max_value=573.867354535386[0m
[37m[1m[2023-07-11 07:32:40,035][233954] New mean coefficients: [[ 0.21721816  1.3836462   1.1291145   0.4392153  -0.43442032 -6.6367292 ]][0m
[37m[1m[2023-07-11 07:32:40,036][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:32:49,096][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 07:32:49,096][233954] FPS: 423879.38[0m
[36m[2023-07-11 07:32:49,099][233954] itr=568, itrs=2000, Progress: 28.40%[0m
[36m[2023-07-11 07:33:00,788][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 07:33:00,788][233954] FPS: 330940.73[0m
[36m[2023-07-11 07:33:05,165][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:33:05,166][233954] Reward + Measures: [[38.29546686  0.071779    0.154468    0.11537001  0.09652501  1.0166142 ]][0m
[37m[1m[2023-07-11 07:33:05,166][233954] Max Reward on eval: 38.29546686054318[0m
[37m[1m[2023-07-11 07:33:05,166][233954] Min Reward on eval: 38.29546686054318[0m
[37m[1m[2023-07-11 07:33:05,166][233954] Mean Reward across all agents: 38.29546686054318[0m
[37m[1m[2023-07-11 07:33:05,166][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:33:10,156][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:33:10,157][233954] Reward + Measures: [[ 91.56647011   0.1637       0.24239998   0.17330001   0.29610002
    2.50870967]
 [136.07588242   0.09119999   0.73439997   0.23819999   0.62600005
    2.61291766]
 [ 42.36084275   0.25830001   0.39520001   0.22610001   0.34580001
    2.72307086]
 ...
 [-24.8171771    0.33520004   0.46919999   0.3177       0.41009998
    3.17967486]
 [ 44.06617271   0.17050001   0.266        0.1612       0.24609999
    2.30635428]
 [-26.53965654   0.46700001   0.76099998   0.44350001   0.67269993
    2.49971557]][0m
[37m[1m[2023-07-11 07:33:10,157][233954] Max Reward on eval: 777.0952682731673[0m
[37m[1m[2023-07-11 07:33:10,158][233954] Min Reward on eval: -175.8336553384026[0m
[37m[1m[2023-07-11 07:33:10,158][233954] Mean Reward across all agents: 115.08745555400323[0m
[37m[1m[2023-07-11 07:33:10,158][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:33:10,161][233954] mean_value=-234.74062099351994, max_value=407.4287440304424[0m
[37m[1m[2023-07-11 07:33:10,163][233954] New mean coefficients: [[ 0.48466706  1.267312    1.9153565   0.40980697 -0.6902313  -6.822196  ]][0m
[37m[1m[2023-07-11 07:33:10,164][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:33:19,231][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 07:33:19,231][233954] FPS: 423599.45[0m
[36m[2023-07-11 07:33:19,233][233954] itr=569, itrs=2000, Progress: 28.45%[0m
[36m[2023-07-11 07:33:31,009][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 07:33:31,009][233954] FPS: 328486.69[0m
[36m[2023-07-11 07:33:35,243][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:33:35,243][233954] Reward + Measures: [[38.91824694  0.07293234  0.154799    0.12015399  0.09413699  0.99688327]][0m
[37m[1m[2023-07-11 07:33:35,243][233954] Max Reward on eval: 38.91824693839205[0m
[37m[1m[2023-07-11 07:33:35,244][233954] Min Reward on eval: 38.91824693839205[0m
[37m[1m[2023-07-11 07:33:35,244][233954] Mean Reward across all agents: 38.91824693839205[0m
[37m[1m[2023-07-11 07:33:35,244][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:33:40,216][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:33:40,216][233954] Reward + Measures: [[197.63583888   0.0623       0.52509999   0.28099999   0.51749998
    2.74682283]
 [  2.23508254   0.56960005   0.78120005   0.55739999   0.78250009
    2.23815727]
 [ 38.64794411   0.1846       0.38949999   0.189        0.37470004
    2.08695006]
 ...
 [ 81.314235     0.11309999   0.3407       0.16410001   0.32449999
    2.65379834]
 [189.02404976   0.0126       0.9781       0.67470002   0.97560006
    3.369452  ]
 [ 28.53089721   0.1176       0.83090001   0.41910002   0.82010001
    3.21671605]][0m
[37m[1m[2023-07-11 07:33:40,216][233954] Max Reward on eval: 744.428581227921[0m
[37m[1m[2023-07-11 07:33:40,217][233954] Min Reward on eval: -187.91952502913773[0m
[37m[1m[2023-07-11 07:33:40,217][233954] Mean Reward across all agents: 125.63662583747386[0m
[37m[1m[2023-07-11 07:33:40,217][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:33:40,220][233954] mean_value=-173.72868925539058, max_value=608.9673633534461[0m
[37m[1m[2023-07-11 07:33:40,223][233954] New mean coefficients: [[ 1.3063449   1.2496495   2.6134148  -0.27392977 -0.50447536 -6.6952906 ]][0m
[37m[1m[2023-07-11 07:33:40,224][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:33:49,195][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 07:33:49,195][233954] FPS: 428138.57[0m
[36m[2023-07-11 07:33:49,197][233954] itr=570, itrs=2000, Progress: 28.50%[0m
[37m[1m[2023-07-11 07:37:09,477][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000550[0m
[36m[2023-07-11 07:37:21,980][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 07:37:21,980][233954] FPS: 329761.34[0m
[36m[2023-07-11 07:37:26,109][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:37:26,110][233954] Reward + Measures: [[37.47574568  0.067242    0.16760567  0.12604032  0.09104633  0.97122723]][0m
[37m[1m[2023-07-11 07:37:26,110][233954] Max Reward on eval: 37.47574567840363[0m
[37m[1m[2023-07-11 07:37:26,110][233954] Min Reward on eval: 37.47574567840363[0m
[37m[1m[2023-07-11 07:37:26,111][233954] Mean Reward across all agents: 37.47574567840363[0m
[37m[1m[2023-07-11 07:37:26,111][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:37:31,080][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:37:31,081][233954] Reward + Measures: [[ -1.26021625   0.88510001   0.93000001   0.88330001   0.90850002
    2.79611731]
 [218.27258981   0.12590002   0.87330002   0.35339999   0.81899995
    2.6172266 ]
 [-13.9512481    0.27669999   0.55760002   0.2552       0.59009999
    2.85149741]
 ...
 [ 82.98188874   0.1982       0.4851       0.17850001   0.3592
    1.95297849]
 [157.38476848   0.0036       0.99760002   0.60430002   0.99700004
    3.47024083]
 [ 22.37909191   0.87959999   0.91250002   0.90669996   0.89930004
    3.38105464]][0m
[37m[1m[2023-07-11 07:37:31,081][233954] Max Reward on eval: 706.9971121258102[0m
[37m[1m[2023-07-11 07:37:31,081][233954] Min Reward on eval: -133.58982754303142[0m
[37m[1m[2023-07-11 07:37:31,082][233954] Mean Reward across all agents: 61.16635650910359[0m
[37m[1m[2023-07-11 07:37:31,082][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:37:31,085][233954] mean_value=-186.4905574753665, max_value=490.03934697619206[0m
[37m[1m[2023-07-11 07:37:31,087][233954] New mean coefficients: [[ 0.5601268  0.8549589  1.9574738 -1.0870879 -0.6766213 -7.7254558]][0m
[37m[1m[2023-07-11 07:37:31,088][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:37:40,025][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 07:37:40,026][233954] FPS: 429752.54[0m
[36m[2023-07-11 07:37:40,028][233954] itr=571, itrs=2000, Progress: 28.55%[0m
[36m[2023-07-11 07:37:51,519][233954] train() took 11.39 seconds to complete[0m
[36m[2023-07-11 07:37:51,520][233954] FPS: 337061.46[0m
[36m[2023-07-11 07:37:55,794][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:37:55,794][233954] Reward + Measures: [[36.23697339  0.06970533  0.16045     0.12576167  0.092531    0.96571058]][0m
[37m[1m[2023-07-11 07:37:55,795][233954] Max Reward on eval: 36.236973386605015[0m
[37m[1m[2023-07-11 07:37:55,795][233954] Min Reward on eval: 36.236973386605015[0m
[37m[1m[2023-07-11 07:37:55,795][233954] Mean Reward across all agents: 36.236973386605015[0m
[37m[1m[2023-07-11 07:37:55,795][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:38:00,830][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:38:00,831][233954] Reward + Measures: [[ 19.42780176   0.53860009   0.83840001   0.51130003   0.72860003
    2.28678513]
 [-23.11170242   0.75740004   0.81170005   0.74420005   0.84569997
    2.87681627]
 [-34.16958573   0.15269999   0.84759998   0.25709999   0.78759998
    2.8827219 ]
 ...
 [ 90.25246907   0.1661       0.80430001   0.12279999   0.74189997
    3.08782697]
 [529.38681031   0.0263       0.95419997   0.59690005   0.89639997
    2.8056109 ]
 [121.73309396   0.2545       0.59920001   0.2261       0.66940004
    2.63331723]][0m
[37m[1m[2023-07-11 07:38:00,831][233954] Max Reward on eval: 571.5871124256403[0m
[37m[1m[2023-07-11 07:38:00,831][233954] Min Reward on eval: -88.17475171869447[0m
[37m[1m[2023-07-11 07:38:00,832][233954] Mean Reward across all agents: 67.95389149181133[0m
[37m[1m[2023-07-11 07:38:00,832][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:38:00,835][233954] mean_value=-103.15059803567448, max_value=542.651362176109[0m
[37m[1m[2023-07-11 07:38:00,838][233954] New mean coefficients: [[ 0.23262185  1.8452969   0.91009235 -1.3574041   0.5388495  -6.64449   ]][0m
[37m[1m[2023-07-11 07:38:00,839][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:38:09,911][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 07:38:09,911][233954] FPS: 423367.54[0m
[36m[2023-07-11 07:38:09,913][233954] itr=572, itrs=2000, Progress: 28.60%[0m
[36m[2023-07-11 07:38:22,031][233954] train() took 12.03 seconds to complete[0m
[36m[2023-07-11 07:38:22,031][233954] FPS: 319164.90[0m
[36m[2023-07-11 07:38:26,341][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:38:26,341][233954] Reward + Measures: [[35.36975813  0.06598067  0.166042    0.13132566  0.09003367  0.95434856]][0m
[37m[1m[2023-07-11 07:38:26,342][233954] Max Reward on eval: 35.36975813307813[0m
[37m[1m[2023-07-11 07:38:26,342][233954] Min Reward on eval: 35.36975813307813[0m
[37m[1m[2023-07-11 07:38:26,342][233954] Mean Reward across all agents: 35.36975813307813[0m
[37m[1m[2023-07-11 07:38:26,342][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:38:31,545][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:38:31,545][233954] Reward + Measures: [[-180.77017684    0.29700002    0.3766        0.0612        0.35349998
     3.12863922]
 [  -9.13180683    0.47730002    0.57380003    0.45240003    0.55410004
     2.39934707]
 [ -37.09665495    0.93809998    0.94869995    0.92080003    0.93239993
     3.06934881]
 ...
 [ 108.15144562    0.05269999    0.59230006    0.39070001    0.56009996
     2.4822216 ]
 [   1.79306795    0.90340006    0.97520012    0.88990003    0.92360002
     3.20770335]
 [ 182.593709      0.14380001    0.30200002    0.3044        0.4197
     2.49023509]][0m
[37m[1m[2023-07-11 07:38:31,546][233954] Max Reward on eval: 540.5933997679501[0m
[37m[1m[2023-07-11 07:38:31,546][233954] Min Reward on eval: -430.67944952566177[0m
[37m[1m[2023-07-11 07:38:31,546][233954] Mean Reward across all agents: 46.73482150997537[0m
[37m[1m[2023-07-11 07:38:31,546][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:38:31,550][233954] mean_value=-141.5179211722961, max_value=868.0729971929268[0m
[37m[1m[2023-07-11 07:38:31,553][233954] New mean coefficients: [[ 1.7525227   1.5857698   2.6656456  -0.7291356   0.38335186 -6.0739317 ]][0m
[37m[1m[2023-07-11 07:38:31,553][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:38:40,539][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 07:38:40,539][233954] FPS: 427441.81[0m
[36m[2023-07-11 07:38:40,541][233954] itr=573, itrs=2000, Progress: 28.65%[0m
[36m[2023-07-11 07:38:52,113][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 07:38:52,113][233954] FPS: 334352.01[0m
[36m[2023-07-11 07:38:56,337][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:38:56,337][233954] Reward + Measures: [[35.23094003  0.062435    0.18497334  0.13833767  0.08469266  0.92293113]][0m
[37m[1m[2023-07-11 07:38:56,338][233954] Max Reward on eval: 35.23094002647966[0m
[37m[1m[2023-07-11 07:38:56,338][233954] Min Reward on eval: 35.23094002647966[0m
[37m[1m[2023-07-11 07:38:56,338][233954] Mean Reward across all agents: 35.23094002647966[0m
[37m[1m[2023-07-11 07:38:56,339][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:39:01,278][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:39:01,279][233954] Reward + Measures: [[ 132.64838362    0.036         0.96950006    0.42209998    0.90860003
     3.16774249]
 [-148.98364805    0.36680004    0.34039998    0.34769997    0.4093
     2.90990424]
 [  82.86351798    0.33259997    0.3186        0.27719998    0.29839998
     2.68381119]
 ...
 [  12.49656853    0.0927        0.1416        0.08460001    0.1585
     2.05045819]
 [ 254.10356258    0.0815        0.80800003    0.40299997    0.72230005
     2.90100431]
 [  91.11604975    0.94070005    0.86199999    0.85680002    0.94880003
     3.40339923]][0m
[37m[1m[2023-07-11 07:39:01,279][233954] Max Reward on eval: 737.6595535391941[0m
[37m[1m[2023-07-11 07:39:01,279][233954] Min Reward on eval: -312.3746929557994[0m
[37m[1m[2023-07-11 07:39:01,279][233954] Mean Reward across all agents: 43.969822102345354[0m
[37m[1m[2023-07-11 07:39:01,280][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:39:01,283][233954] mean_value=-191.25788186145135, max_value=440.1873964750022[0m
[37m[1m[2023-07-11 07:39:01,286][233954] New mean coefficients: [[ 1.6520214  2.4534993  2.2245777 -1.078082   1.5493793 -4.8010116]][0m
[37m[1m[2023-07-11 07:39:01,287][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:39:10,238][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 07:39:10,239][233954] FPS: 429051.55[0m
[36m[2023-07-11 07:39:10,241][233954] itr=574, itrs=2000, Progress: 28.70%[0m
[36m[2023-07-11 07:39:22,070][233954] train() took 11.75 seconds to complete[0m
[36m[2023-07-11 07:39:22,070][233954] FPS: 326876.57[0m
[36m[2023-07-11 07:39:26,390][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:39:26,390][233954] Reward + Measures: [[35.82612516  0.05895633  0.20186399  0.15236299  0.07845434  0.87846547]][0m
[37m[1m[2023-07-11 07:39:26,391][233954] Max Reward on eval: 35.826125159755236[0m
[37m[1m[2023-07-11 07:39:26,391][233954] Min Reward on eval: 35.826125159755236[0m
[37m[1m[2023-07-11 07:39:26,391][233954] Mean Reward across all agents: 35.826125159755236[0m
[37m[1m[2023-07-11 07:39:26,391][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:39:31,406][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:39:31,406][233954] Reward + Measures: [[162.32549856   0.3335       0.70200002   0.32710001   0.57599998
    2.66033125]
 [656.16220092   0.0019       0.99250001   0.77689999   0.99770004
    3.15976834]
 [186.31116138   0.296        0.48390004   0.53069997   0.64580005
    2.42361474]
 ...
 [  9.26313974   0.9285       0.98519993   0.91930002   0.94239998
    3.28564525]
 [  9.96225688   0.10270001   0.1743       0.11259999   0.20640002
    2.46072078]
 [ 76.81539801   0.1974       0.39290002   0.17910001   0.4357
    2.54704046]][0m
[37m[1m[2023-07-11 07:39:31,407][233954] Max Reward on eval: 726.1649971079081[0m
[37m[1m[2023-07-11 07:39:31,407][233954] Min Reward on eval: -257.3423376507126[0m
[37m[1m[2023-07-11 07:39:31,407][233954] Mean Reward across all agents: 117.29075082324246[0m
[37m[1m[2023-07-11 07:39:31,407][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:39:31,412][233954] mean_value=-132.75934256164467, max_value=681.5426500251517[0m
[37m[1m[2023-07-11 07:39:31,415][233954] New mean coefficients: [[ 1.7972807  2.0485535  1.8706172 -1.3056722  1.1613748 -5.042199 ]][0m
[37m[1m[2023-07-11 07:39:31,416][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:39:40,415][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 07:39:40,415][233954] FPS: 426797.13[0m
[36m[2023-07-11 07:39:40,418][233954] itr=575, itrs=2000, Progress: 28.75%[0m
[36m[2023-07-11 07:39:51,941][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 07:39:51,941][233954] FPS: 335733.43[0m
[36m[2023-07-11 07:39:56,143][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:39:56,143][233954] Reward + Measures: [[34.73629218  0.062365    0.23267066  0.17353299  0.07861367  0.86677557]][0m
[37m[1m[2023-07-11 07:39:56,143][233954] Max Reward on eval: 34.736292179248075[0m
[37m[1m[2023-07-11 07:39:56,144][233954] Min Reward on eval: 34.736292179248075[0m
[37m[1m[2023-07-11 07:39:56,144][233954] Mean Reward across all agents: 34.736292179248075[0m
[37m[1m[2023-07-11 07:39:56,144][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:40:01,107][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:40:01,108][233954] Reward + Measures: [[90.81442804  0.17739999  0.40169999  0.2595      0.38299999  2.58411646]
 [15.94728064  0.20749998  0.30580002  0.178       0.24590002  3.02341938]
 [12.55902636  0.2841      0.35460001  0.27649999  0.31580001  2.38774347]
 ...
 [-2.38110851  0.28349999  0.41410002  0.23349999  0.31040004  3.04344606]
 [17.13526991  0.71040004  0.81450003  0.67500001  0.79370004  3.2540741 ]
 [-3.35162077  0.70320004  0.76710004  0.6656      0.73079997  3.2861526 ]][0m
[37m[1m[2023-07-11 07:40:01,108][233954] Max Reward on eval: 673.709743504785[0m
[37m[1m[2023-07-11 07:40:01,108][233954] Min Reward on eval: -196.8090381554328[0m
[37m[1m[2023-07-11 07:40:01,108][233954] Mean Reward across all agents: 24.179671013588496[0m
[37m[1m[2023-07-11 07:40:01,108][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:40:01,111][233954] mean_value=-225.37974677364983, max_value=303.1909618445672[0m
[37m[1m[2023-07-11 07:40:01,113][233954] New mean coefficients: [[ 2.2138495  1.3976638  1.500366  -1.1294562  1.020664  -5.4211216]][0m
[37m[1m[2023-07-11 07:40:01,114][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:40:10,071][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 07:40:10,071][233954] FPS: 428780.83[0m
[36m[2023-07-11 07:40:10,074][233954] itr=576, itrs=2000, Progress: 28.80%[0m
[36m[2023-07-11 07:40:21,730][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 07:40:21,730][233954] FPS: 331809.98[0m
[36m[2023-07-11 07:40:26,114][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:40:26,115][233954] Reward + Measures: [[31.6843823   0.061454    0.24131     0.181234    0.08221433  0.84856802]][0m
[37m[1m[2023-07-11 07:40:26,115][233954] Max Reward on eval: 31.684382300580832[0m
[37m[1m[2023-07-11 07:40:26,115][233954] Min Reward on eval: 31.684382300580832[0m
[37m[1m[2023-07-11 07:40:26,116][233954] Mean Reward across all agents: 31.684382300580832[0m
[37m[1m[2023-07-11 07:40:26,116][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:40:31,242][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:40:31,243][233954] Reward + Measures: [[ 43.09981809   0.1725       0.5244       0.2112       0.56800002
    2.62797999]
 [123.95771014   0.80979997   0.83670008   0.76350003   0.83950007
    3.27026558]
 [ 93.17073383   0.80200005   0.82040006   0.75159997   0.78290004
    3.44609332]
 ...
 [ 77.55208969   0.94840002   0.95950001   0.92629999   0.93990004
    3.29915857]
 [-12.66755174   0.51970005   0.667        0.47510001   0.7094
    2.63247657]
 [ 11.33949518   0.73950005   0.76090002   0.48700005   0.74700004
    2.8504591 ]][0m
[37m[1m[2023-07-11 07:40:31,243][233954] Max Reward on eval: 639.9376106209122[0m
[37m[1m[2023-07-11 07:40:31,243][233954] Min Reward on eval: -326.8701696464792[0m
[37m[1m[2023-07-11 07:40:31,243][233954] Mean Reward across all agents: 46.934087783328835[0m
[37m[1m[2023-07-11 07:40:31,244][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:40:31,247][233954] mean_value=-181.53567309885184, max_value=458.458470006967[0m
[37m[1m[2023-07-11 07:40:31,250][233954] New mean coefficients: [[ 2.577502    1.1003666   2.0441766  -0.268291    0.11997259 -6.021386  ]][0m
[37m[1m[2023-07-11 07:40:31,251][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:40:40,321][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 07:40:40,322][233954] FPS: 423405.33[0m
[36m[2023-07-11 07:40:40,324][233954] itr=577, itrs=2000, Progress: 28.85%[0m
[36m[2023-07-11 07:40:52,045][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 07:40:52,045][233954] FPS: 330008.02[0m
[36m[2023-07-11 07:40:56,367][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:40:56,368][233954] Reward + Measures: [[30.20693406  0.058908    0.25223365  0.18411833  0.079409    0.81866318]][0m
[37m[1m[2023-07-11 07:40:56,368][233954] Max Reward on eval: 30.206934064106704[0m
[37m[1m[2023-07-11 07:40:56,368][233954] Min Reward on eval: 30.206934064106704[0m
[37m[1m[2023-07-11 07:40:56,369][233954] Mean Reward across all agents: 30.206934064106704[0m
[37m[1m[2023-07-11 07:40:56,369][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:41:01,582][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:41:01,588][233954] Reward + Measures: [[175.78209451   0.46709999   0.25040001   0.48900005   0.19090001
    2.30419111]
 [-37.68652103   0.75620002   0.76120007   0.7335       0.82910007
    2.78244853]
 [397.20808076   0.49359998   0.8278001    0.16580001   0.77130002
    3.24051285]
 ...
 [ 89.19440785   0.51230001   0.88810009   0.61759996   0.88759995
    2.69128871]
 [ 91.14743805   0.42180005   0.6688       0.48339996   0.60970002
    2.30461383]
 [ 69.60567969   0.2247       0.60289997   0.2313       0.59649998
    2.637573  ]][0m
[37m[1m[2023-07-11 07:41:01,588][233954] Max Reward on eval: 621.742831510352[0m
[37m[1m[2023-07-11 07:41:01,588][233954] Min Reward on eval: -110.77046510204673[0m
[37m[1m[2023-07-11 07:41:01,589][233954] Mean Reward across all agents: 131.91632239594688[0m
[37m[1m[2023-07-11 07:41:01,589][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:41:01,594][233954] mean_value=-135.1106524208304, max_value=471.7394870976249[0m
[37m[1m[2023-07-11 07:41:01,597][233954] New mean coefficients: [[ 2.750802    1.4561514   1.600763   -0.49643844  0.5277808  -5.2320714 ]][0m
[37m[1m[2023-07-11 07:41:01,598][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:41:10,585][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 07:41:10,585][233954] FPS: 427374.43[0m
[36m[2023-07-11 07:41:10,587][233954] itr=578, itrs=2000, Progress: 28.90%[0m
[36m[2023-07-11 07:41:22,140][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 07:41:22,140][233954] FPS: 334758.72[0m
[36m[2023-07-11 07:41:26,514][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:41:26,515][233954] Reward + Measures: [[32.59504762  0.05895933  0.26310965  0.194428    0.07638     0.79876006]][0m
[37m[1m[2023-07-11 07:41:26,515][233954] Max Reward on eval: 32.595047623995185[0m
[37m[1m[2023-07-11 07:41:26,515][233954] Min Reward on eval: 32.595047623995185[0m
[37m[1m[2023-07-11 07:41:26,515][233954] Mean Reward across all agents: 32.595047623995185[0m
[37m[1m[2023-07-11 07:41:26,516][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:41:31,556][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:41:31,556][233954] Reward + Measures: [[177.86066531   0.2789       0.92140007   0.33950001   0.89320004
    3.11979342]
 [300.59956362   0.25309998   0.93559998   0.55000001   0.90770006
    3.4522202 ]
 [ 80.06931193   0.18539999   0.33920002   0.191        0.39000002
    2.85385776]
 ...
 [ 20.66626822   0.20009999   0.22850001   0.1736       0.25049999
    2.99636006]
 [153.60721055   0.16180001   0.35660002   0.17490001   0.28169999
    2.53307176]
 [-19.63031567   0.41940004   0.51109999   0.40889999   0.498
    2.39297771]][0m
[37m[1m[2023-07-11 07:41:31,557][233954] Max Reward on eval: 539.4949435941875[0m
[37m[1m[2023-07-11 07:41:31,557][233954] Min Reward on eval: -191.49948888358193[0m
[37m[1m[2023-07-11 07:41:31,557][233954] Mean Reward across all agents: 65.92156664470758[0m
[37m[1m[2023-07-11 07:41:31,558][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:41:31,561][233954] mean_value=-148.17883792577584, max_value=407.47724901879747[0m
[37m[1m[2023-07-11 07:41:31,563][233954] New mean coefficients: [[ 2.1294537  1.3055389  1.2244686 -1.2367363  0.5623531 -5.9669585]][0m
[37m[1m[2023-07-11 07:41:31,564][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:41:40,669][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 07:41:40,669][233954] FPS: 421825.14[0m
[36m[2023-07-11 07:41:40,671][233954] itr=579, itrs=2000, Progress: 28.95%[0m
[36m[2023-07-11 07:41:52,358][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 07:41:52,358][233954] FPS: 330969.43[0m
[36m[2023-07-11 07:41:56,604][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:41:56,605][233954] Reward + Measures: [[33.17829679  0.05878033  0.26962867  0.19761033  0.07401367  0.78015167]][0m
[37m[1m[2023-07-11 07:41:56,605][233954] Max Reward on eval: 33.178296791931224[0m
[37m[1m[2023-07-11 07:41:56,605][233954] Min Reward on eval: 33.178296791931224[0m
[37m[1m[2023-07-11 07:41:56,606][233954] Mean Reward across all agents: 33.178296791931224[0m
[37m[1m[2023-07-11 07:41:56,606][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:42:01,576][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:42:01,577][233954] Reward + Measures: [[-50.2605816    0.0695       0.89870006   0.46140003   0.89469999
    2.58889699]
 [268.75109483   0.13630001   0.87270004   0.40549999   0.84899998
    3.2792592 ]
 [-14.85027675   0.38620004   0.72589999   0.17260002   0.7069
    3.03312159]
 ...
 [ 49.75376858   0.257        0.22400001   0.23699999   0.27169999
    2.70900846]
 [399.97743515   0.0409       0.80340004   0.58320004   0.79080003
    2.89274478]
 [119.14553274   0.20640002   0.47469997   0.17230001   0.49270001
    2.7595613 ]][0m
[37m[1m[2023-07-11 07:42:01,577][233954] Max Reward on eval: 414.9566745899618[0m
[37m[1m[2023-07-11 07:42:01,577][233954] Min Reward on eval: -194.93566704764962[0m
[37m[1m[2023-07-11 07:42:01,577][233954] Mean Reward across all agents: 87.0696631333392[0m
[37m[1m[2023-07-11 07:42:01,578][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:42:01,581][233954] mean_value=-151.32365458236774, max_value=621.9902375958511[0m
[37m[1m[2023-07-11 07:42:01,584][233954] New mean coefficients: [[ 2.7646356  1.7964482  1.5599899 -1.5272822  1.2358919 -4.828059 ]][0m
[37m[1m[2023-07-11 07:42:01,585][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:42:10,577][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 07:42:10,577][233954] FPS: 427127.98[0m
[36m[2023-07-11 07:42:10,579][233954] itr=580, itrs=2000, Progress: 29.00%[0m
[37m[1m[2023-07-11 07:45:36,425][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000560[0m
[36m[2023-07-11 07:45:48,783][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 07:45:48,783][233954] FPS: 330125.59[0m
[36m[2023-07-11 07:45:52,858][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:45:52,858][233954] Reward + Measures: [[34.53293587  0.05452034  0.27177835  0.196073    0.07203966  0.76856768]][0m
[37m[1m[2023-07-11 07:45:52,858][233954] Max Reward on eval: 34.532935865708296[0m
[37m[1m[2023-07-11 07:45:52,858][233954] Min Reward on eval: 34.532935865708296[0m
[37m[1m[2023-07-11 07:45:52,859][233954] Mean Reward across all agents: 34.532935865708296[0m
[37m[1m[2023-07-11 07:45:52,859][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:45:57,694][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:45:57,695][233954] Reward + Measures: [[140.69429196   0.18540001   0.59569997   0.32510003   0.5801
    2.42822814]
 [ 65.11140659   0.90150005   0.91240007   0.88490003   0.91380006
    3.68862534]
 [ 72.73145367   0.22299998   0.74189997   0.26980001   0.69399995
    2.71486211]
 ...
 [161.25755315   0.0987       0.1381       0.1153       0.19420001
    3.46897578]
 [108.3148823    0.0972       0.102        0.08459999   0.103
    2.86808896]
 [-23.59082581   0.6433       0.70139998   0.60400003   0.31869999
    2.88134289]][0m
[37m[1m[2023-07-11 07:45:57,695][233954] Max Reward on eval: 330.03523541104516[0m
[37m[1m[2023-07-11 07:45:57,695][233954] Min Reward on eval: -113.79762271670624[0m
[37m[1m[2023-07-11 07:45:57,696][233954] Mean Reward across all agents: 79.82278498599065[0m
[37m[1m[2023-07-11 07:45:57,696][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:45:57,699][233954] mean_value=-224.16126864915617, max_value=468.51177988222366[0m
[37m[1m[2023-07-11 07:45:57,702][233954] New mean coefficients: [[ 2.0770216   1.5153954   0.79548085 -1.6481827   1.1199992  -5.4977436 ]][0m
[37m[1m[2023-07-11 07:45:57,703][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:46:05,995][233954] train() took 8.29 seconds to complete[0m
[36m[2023-07-11 07:46:05,995][233954] FPS: 463156.51[0m
[36m[2023-07-11 07:46:05,998][233954] itr=581, itrs=2000, Progress: 29.05%[0m
[36m[2023-07-11 07:46:17,578][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 07:46:17,578][233954] FPS: 334096.40[0m
[36m[2023-07-11 07:46:21,654][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:46:21,655][233954] Reward + Measures: [[34.43232955  0.05443     0.275336    0.19809365  0.07188834  0.765499  ]][0m
[37m[1m[2023-07-11 07:46:21,655][233954] Max Reward on eval: 34.43232955430421[0m
[37m[1m[2023-07-11 07:46:21,655][233954] Min Reward on eval: 34.43232955430421[0m
[37m[1m[2023-07-11 07:46:21,655][233954] Mean Reward across all agents: 34.43232955430421[0m
[37m[1m[2023-07-11 07:46:21,656][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:46:26,662][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:46:26,667][233954] Reward + Measures: [[-31.01553178   0.81450003   0.83190006   0.78740007   0.83519995
    3.54304743]
 [ 54.05928852   0.12980001   0.1962       0.07660001   0.15890001
    2.93039393]
 [  7.00151204   0.79640001   0.80520004   0.75290006   0.82010001
    3.38719225]
 ...
 [ 73.79508166   0.2507       0.40400001   0.2278       0.43810001
    2.88191676]
 [-97.94780688   0.3926       0.41280004   0.3294       0.44380003
    3.25437713]
 [-18.02932365   0.0702       0.2024       0.10890001   0.22049999
    3.18625951]][0m
[37m[1m[2023-07-11 07:46:26,668][233954] Max Reward on eval: 519.7841243749484[0m
[37m[1m[2023-07-11 07:46:26,668][233954] Min Reward on eval: -162.78609924111515[0m
[37m[1m[2023-07-11 07:46:26,668][233954] Mean Reward across all agents: 29.693776658724722[0m
[37m[1m[2023-07-11 07:46:26,669][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:46:26,674][233954] mean_value=-95.72417827656051, max_value=656.4299649656281[0m
[37m[1m[2023-07-11 07:46:26,676][233954] New mean coefficients: [[ 1.8766794   1.1620905   0.04770732 -1.3484807   0.8562994  -6.119039  ]][0m
[37m[1m[2023-07-11 07:46:26,677][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:46:35,674][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 07:46:35,674][233954] FPS: 426918.62[0m
[36m[2023-07-11 07:46:35,676][233954] itr=582, itrs=2000, Progress: 29.10%[0m
[36m[2023-07-11 07:46:47,485][233954] train() took 11.73 seconds to complete[0m
[36m[2023-07-11 07:46:47,485][233954] FPS: 327487.53[0m
[36m[2023-07-11 07:46:51,709][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:46:51,709][233954] Reward + Measures: [[32.76520344  0.05425433  0.28280133  0.20905733  0.071309    0.72970116]][0m
[37m[1m[2023-07-11 07:46:51,709][233954] Max Reward on eval: 32.76520343795607[0m
[37m[1m[2023-07-11 07:46:51,710][233954] Min Reward on eval: 32.76520343795607[0m
[37m[1m[2023-07-11 07:46:51,710][233954] Mean Reward across all agents: 32.76520343795607[0m
[37m[1m[2023-07-11 07:46:51,710][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:46:56,631][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:46:56,637][233954] Reward + Measures: [[ -2.34009669   0.074        0.84600002   0.17480001   0.82250005
    2.82801032]
 [ 29.27990076   0.22389999   0.6473       0.21499999   0.60949999
    2.8119781 ]
 [101.19530823   0.039        0.93250006   0.41549999   0.93059999
    3.56098294]
 ...
 [ 38.3165912    0.037        0.13469999   0.1053       0.102
    1.3569361 ]
 [202.26090549   0.29679999   0.55619997   0.1585       0.58210003
    2.95327306]
 [ 24.10045099   0.1856       0.43449998   0.10869999   0.39630002
    2.7027874 ]][0m
[37m[1m[2023-07-11 07:46:56,638][233954] Max Reward on eval: 721.0108909668401[0m
[37m[1m[2023-07-11 07:46:56,638][233954] Min Reward on eval: -120.55965085607022[0m
[37m[1m[2023-07-11 07:46:56,638][233954] Mean Reward across all agents: 69.41664319999725[0m
[37m[1m[2023-07-11 07:46:56,638][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:46:56,641][233954] mean_value=-187.71312919971692, max_value=398.2064282885394[0m
[37m[1m[2023-07-11 07:46:56,644][233954] New mean coefficients: [[ 0.6456449  1.1223644  0.0021136 -1.9326004  0.8205843 -6.7130556]][0m
[37m[1m[2023-07-11 07:46:56,645][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:47:05,541][233954] train() took 8.89 seconds to complete[0m
[36m[2023-07-11 07:47:05,541][233954] FPS: 431723.84[0m
[36m[2023-07-11 07:47:05,544][233954] itr=583, itrs=2000, Progress: 29.15%[0m
[36m[2023-07-11 07:47:17,142][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 07:47:17,143][233954] FPS: 333550.38[0m
[36m[2023-07-11 07:47:21,377][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:47:21,378][233954] Reward + Measures: [[34.46737303  0.05869066  0.25350568  0.17967533  0.07447967  0.77703768]][0m
[37m[1m[2023-07-11 07:47:21,378][233954] Max Reward on eval: 34.46737303166579[0m
[37m[1m[2023-07-11 07:47:21,378][233954] Min Reward on eval: 34.46737303166579[0m
[37m[1m[2023-07-11 07:47:21,379][233954] Mean Reward across all agents: 34.46737303166579[0m
[37m[1m[2023-07-11 07:47:21,379][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:47:26,320][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:47:26,321][233954] Reward + Measures: [[  5.85728002   0.70679998   0.80680007   0.67090005   0.77429998
    2.26723337]
 [ 76.84753043   0.1693       0.2181       0.15970001   0.20750001
    1.86234725]
 [ 20.05158389   0.95370001   0.9781       0.96019995   0.98110002
    3.25536776]
 ...
 [ 88.65145214   0.0985       0.14649999   0.1025       0.13860001
    2.14508843]
 [-47.34874225   0.38750002   0.83810008   0.37310001   0.84910005
    2.67458844]
 [-49.40471721   0.73550004   0.88720006   0.71150005   0.89059991
    2.44295239]][0m
[37m[1m[2023-07-11 07:47:26,321][233954] Max Reward on eval: 470.87582986622584[0m
[37m[1m[2023-07-11 07:47:26,322][233954] Min Reward on eval: -151.36689668204636[0m
[37m[1m[2023-07-11 07:47:26,322][233954] Mean Reward across all agents: 21.099104696843767[0m
[37m[1m[2023-07-11 07:47:26,322][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:47:26,325][233954] mean_value=-174.19745186038054, max_value=376.8150052885285[0m
[37m[1m[2023-07-11 07:47:26,328][233954] New mean coefficients: [[-0.24662071  0.7843195  -0.7722456  -1.7154369   0.5099788  -7.5512657 ]][0m
[37m[1m[2023-07-11 07:47:26,329][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:47:35,368][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 07:47:35,368][233954] FPS: 424886.84[0m
[36m[2023-07-11 07:47:35,370][233954] itr=584, itrs=2000, Progress: 29.20%[0m
[36m[2023-07-11 07:47:47,262][233954] train() took 11.81 seconds to complete[0m
[36m[2023-07-11 07:47:47,263][233954] FPS: 325130.24[0m
[36m[2023-07-11 07:47:51,573][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:47:51,573][233954] Reward + Measures: [[33.359157    0.059515    0.25477466  0.17551367  0.07737766  0.78708941]][0m
[37m[1m[2023-07-11 07:47:51,573][233954] Max Reward on eval: 33.35915700335333[0m
[37m[1m[2023-07-11 07:47:51,574][233954] Min Reward on eval: 33.35915700335333[0m
[37m[1m[2023-07-11 07:47:51,574][233954] Mean Reward across all agents: 33.35915700335333[0m
[37m[1m[2023-07-11 07:47:51,574][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:47:56,593][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:47:56,599][233954] Reward + Measures: [[ -2.444007     0.99340004   0.99469995   0.99510002   0.99360001
    3.76704478]
 [-25.93841166   0.96630001   0.97879994   0.97109997   0.98180002
    3.47605181]
 [ 70.89522549   0.24330001   0.73210001   0.3177       0.71880001
    2.66692495]
 ...
 [ 25.1939441    0.98850006   0.99480003   0.99329996   0.99469995
    3.28661966]
 [ -2.7353045    0.991        0.99239999   0.99150002   0.99250001
    3.69123006]
 [ 96.96605765   0.08290001   0.90630001   0.50229996   0.81480008
    3.10511303]][0m
[37m[1m[2023-07-11 07:47:56,600][233954] Max Reward on eval: 559.9084661019035[0m
[37m[1m[2023-07-11 07:47:56,601][233954] Min Reward on eval: -189.17437171959318[0m
[37m[1m[2023-07-11 07:47:56,601][233954] Mean Reward across all agents: 62.53088148534667[0m
[37m[1m[2023-07-11 07:47:56,602][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:47:56,610][233954] mean_value=-108.02413347974326, max_value=677.8316272979773[0m
[37m[1m[2023-07-11 07:47:56,614][233954] New mean coefficients: [[-1.4738667   0.08358932 -1.6395249  -1.1133573  -0.06825846 -9.400071  ]][0m
[37m[1m[2023-07-11 07:47:56,616][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:48:05,644][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 07:48:05,644][233954] FPS: 425440.71[0m
[36m[2023-07-11 07:48:05,646][233954] itr=585, itrs=2000, Progress: 29.25%[0m
[36m[2023-07-11 07:48:17,309][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 07:48:17,310][233954] FPS: 331671.90[0m
[36m[2023-07-11 07:48:21,561][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:48:21,562][233954] Reward + Measures: [[31.95141023  0.06081999  0.259426    0.17580999  0.08123666  0.79667705]][0m
[37m[1m[2023-07-11 07:48:21,562][233954] Max Reward on eval: 31.95141023262179[0m
[37m[1m[2023-07-11 07:48:21,562][233954] Min Reward on eval: 31.95141023262179[0m
[37m[1m[2023-07-11 07:48:21,563][233954] Mean Reward across all agents: 31.95141023262179[0m
[37m[1m[2023-07-11 07:48:21,563][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:48:26,565][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:48:26,566][233954] Reward + Measures: [[254.17086214   0.0652       0.86920005   0.47820002   0.84439993
    3.23042107]
 [ 91.40145539   0.1608       0.45559999   0.22480002   0.43099999
    2.49615955]
 [127.25079079   0.1176       0.40159997   0.22590001   0.39449999
    2.99002028]
 ...
 [157.21930841   0.1112       0.85610002   0.32710001   0.75229996
    3.20889354]
 [ 42.455161     0.024        0.97839993   0.50779998   0.96079999
    3.09796786]
 [-15.85267316   0.1033       0.85750002   0.43540001   0.84779996
    2.78273201]][0m
[37m[1m[2023-07-11 07:48:26,566][233954] Max Reward on eval: 754.2183646971359[0m
[37m[1m[2023-07-11 07:48:26,566][233954] Min Reward on eval: -205.26798203531652[0m
[37m[1m[2023-07-11 07:48:26,566][233954] Mean Reward across all agents: 119.98509680596548[0m
[37m[1m[2023-07-11 07:48:26,567][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:48:26,570][233954] mean_value=-194.07144898513988, max_value=516.0078116615966[0m
[37m[1m[2023-07-11 07:48:26,573][233954] New mean coefficients: [[-1.4674501   0.403659   -1.2842805  -0.72402966 -0.05522804 -9.129876  ]][0m
[37m[1m[2023-07-11 07:48:26,574][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:48:35,616][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 07:48:35,616][233954] FPS: 424778.37[0m
[36m[2023-07-11 07:48:35,619][233954] itr=586, itrs=2000, Progress: 29.30%[0m
[36m[2023-07-11 07:48:47,321][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 07:48:47,321][233954] FPS: 330488.97[0m
[36m[2023-07-11 07:48:51,574][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:48:51,574][233954] Reward + Measures: [[29.62094896  0.05817033  0.27832133  0.19457364  0.07851133  0.76023859]][0m
[37m[1m[2023-07-11 07:48:51,574][233954] Max Reward on eval: 29.620948959918653[0m
[37m[1m[2023-07-11 07:48:51,575][233954] Min Reward on eval: 29.620948959918653[0m
[37m[1m[2023-07-11 07:48:51,575][233954] Mean Reward across all agents: 29.620948959918653[0m
[37m[1m[2023-07-11 07:48:51,575][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:48:56,821][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:48:56,827][233954] Reward + Measures: [[352.95001319   0.0673       0.88799995   0.4427       0.77160007
    2.63246036]
 [ 13.16515616   0.06130001   0.0577       0.074        0.0862
    2.8451097 ]
 [232.12747339   0.0574       0.65010005   0.34200001   0.60330003
    2.45448542]
 ...
 [ 77.15049706   0.2079       0.24689999   0.13800001   0.24569999
    2.2913568 ]
 [ 49.13271557   0.126        0.1013       0.1053       0.1105
    2.82856226]
 [ 13.03155542   0.36800003   0.94259995   0.0882       0.94410002
    3.4364922 ]][0m
[37m[1m[2023-07-11 07:48:56,827][233954] Max Reward on eval: 689.720581012778[0m
[37m[1m[2023-07-11 07:48:56,828][233954] Min Reward on eval: -217.3826761558652[0m
[37m[1m[2023-07-11 07:48:56,828][233954] Mean Reward across all agents: 104.37399210825455[0m
[37m[1m[2023-07-11 07:48:56,828][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:48:56,833][233954] mean_value=-192.92340061421493, max_value=435.89647673820383[0m
[37m[1m[2023-07-11 07:48:56,835][233954] New mean coefficients: [[-1.0622194   1.1199532  -1.4022727  -0.65320563  0.75369215 -7.611624  ]][0m
[37m[1m[2023-07-11 07:48:56,836][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:49:05,827][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 07:49:05,828][233954] FPS: 427172.30[0m
[36m[2023-07-11 07:49:05,830][233954] itr=587, itrs=2000, Progress: 29.35%[0m
[36m[2023-07-11 07:49:17,447][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 07:49:17,447][233954] FPS: 332931.80[0m
[36m[2023-07-11 07:49:21,739][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:49:21,739][233954] Reward + Measures: [[31.50560404  0.055995    0.27529398  0.19389333  0.07675534  0.74457097]][0m
[37m[1m[2023-07-11 07:49:21,739][233954] Max Reward on eval: 31.50560404107184[0m
[37m[1m[2023-07-11 07:49:21,740][233954] Min Reward on eval: 31.50560404107184[0m
[37m[1m[2023-07-11 07:49:21,740][233954] Mean Reward across all agents: 31.50560404107184[0m
[37m[1m[2023-07-11 07:49:21,740][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:49:26,729][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:49:26,783][233954] Reward + Measures: [[ 224.56404209    0.0297        0.97360003    0.43929997    0.93540001
     3.09800529]
 [ -72.38510626    0.30050001    0.47690001    0.3321        0.51590002
     2.18984461]
 [  40.00039896    0.81310004    0.91189998    0.76800007    0.84979993
     3.52008891]
 ...
 [-224.90673193    0.38960001    0.51840001    0.06950001    0.50470001
     2.71464038]
 [ 115.24437024    0.53500003    0.71870005    0.49540004    0.65430003
     3.36204338]
 [  65.48383576    0.13340001    0.59540004    0.17299999    0.50850004
     2.51371932]][0m
[37m[1m[2023-07-11 07:49:26,784][233954] Max Reward on eval: 622.9461288589985[0m
[37m[1m[2023-07-11 07:49:26,784][233954] Min Reward on eval: -379.6211123178713[0m
[37m[1m[2023-07-11 07:49:26,785][233954] Mean Reward across all agents: 106.6286887204551[0m
[37m[1m[2023-07-11 07:49:26,786][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:49:26,801][233954] mean_value=-144.50740646593536, max_value=699.7015181208671[0m
[37m[1m[2023-07-11 07:49:26,811][233954] New mean coefficients: [[-1.0112402   1.1881232  -1.775095   -0.15788057  1.2301798  -7.2274804 ]][0m
[37m[1m[2023-07-11 07:49:26,815][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:49:35,841][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 07:49:35,841][233954] FPS: 425580.92[0m
[36m[2023-07-11 07:49:35,844][233954] itr=588, itrs=2000, Progress: 29.40%[0m
[36m[2023-07-11 07:49:47,571][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 07:49:47,571][233954] FPS: 329917.44[0m
[36m[2023-07-11 07:49:51,850][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:49:51,850][233954] Reward + Measures: [[28.22463153  0.05551333  0.27738598  0.191392    0.07724366  0.74493945]][0m
[37m[1m[2023-07-11 07:49:51,851][233954] Max Reward on eval: 28.224631525777358[0m
[37m[1m[2023-07-11 07:49:51,851][233954] Min Reward on eval: 28.224631525777358[0m
[37m[1m[2023-07-11 07:49:51,851][233954] Mean Reward across all agents: 28.224631525777358[0m
[37m[1m[2023-07-11 07:49:51,851][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:49:56,834][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:49:56,835][233954] Reward + Measures: [[ 187.98114577    0.0965        0.31100002    0.17439999    0.33059999
     2.58090973]
 [ 143.93659262    0.40700004    0.4481        0.0478        0.45030004
     2.78777862]
 [  58.95009411    0.09240001    0.34780002    0.11550001    0.3339
     2.48565149]
 ...
 [ 125.11659812    0.0044        0.99919999    0.64859998    0.99720001
     3.16888881]
 [-133.61077966    0.17839999    0.51220006    0.21040002    0.57109994
     2.85228944]
 [-130.53209402    0.86560005    0.83820003    0.84440005    0.8858
     3.3727169 ]][0m
[37m[1m[2023-07-11 07:49:56,835][233954] Max Reward on eval: 597.3827934287489[0m
[37m[1m[2023-07-11 07:49:56,835][233954] Min Reward on eval: -207.5165853492086[0m
[37m[1m[2023-07-11 07:49:56,836][233954] Mean Reward across all agents: 60.57706858343758[0m
[37m[1m[2023-07-11 07:49:56,836][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:49:56,839][233954] mean_value=-208.0956680221486, max_value=778.0561760024075[0m
[37m[1m[2023-07-11 07:49:56,841][233954] New mean coefficients: [[-0.15380383  1.7786365  -0.3051442  -0.7243515   2.0247111  -6.04996   ]][0m
[37m[1m[2023-07-11 07:49:56,842][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:50:05,768][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 07:50:05,768][233954] FPS: 430306.52[0m
[36m[2023-07-11 07:50:05,770][233954] itr=589, itrs=2000, Progress: 29.45%[0m
[36m[2023-07-11 07:50:17,394][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 07:50:17,394][233954] FPS: 332727.86[0m
[36m[2023-07-11 07:50:21,622][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:50:21,622][233954] Reward + Measures: [[27.82422002  0.05115467  0.28546366  0.20803767  0.07224767  0.67811972]][0m
[37m[1m[2023-07-11 07:50:21,622][233954] Max Reward on eval: 27.8242200210748[0m
[37m[1m[2023-07-11 07:50:21,623][233954] Min Reward on eval: 27.8242200210748[0m
[37m[1m[2023-07-11 07:50:21,623][233954] Mean Reward across all agents: 27.8242200210748[0m
[37m[1m[2023-07-11 07:50:21,623][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:50:26,619][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:50:26,671][233954] Reward + Measures: [[ 19.31199483   0.97360003   0.97970003   0.97300005   0.9849
    3.44924855]
 [103.83407399   0.80730003   0.8057       0.80000001   0.79470003
    3.34767008]
 [ 31.49414781   0.1312       0.0591       0.0966       0.16220002
    2.34081697]
 ...
 [ -5.01622223   0.87740004   0.96829998   0.87870008   0.96640009
    2.37778831]
 [113.48112534   0.0861       0.88249999   0.48450002   0.86760008
    3.02870584]
 [ 17.96756949   0.78180003   0.81999999   0.7924       0.85519999
    3.1652298 ]][0m
[37m[1m[2023-07-11 07:50:26,671][233954] Max Reward on eval: 593.2962322384119[0m
[37m[1m[2023-07-11 07:50:26,671][233954] Min Reward on eval: -148.3093634755234[0m
[37m[1m[2023-07-11 07:50:26,672][233954] Mean Reward across all agents: 43.83022588915023[0m
[37m[1m[2023-07-11 07:50:26,672][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:50:26,675][233954] mean_value=-158.27329475390536, max_value=360.13019726327684[0m
[37m[1m[2023-07-11 07:50:26,678][233954] New mean coefficients: [[-0.13626635  2.3944664  -0.14773552 -1.0702045   3.006208   -5.153646  ]][0m
[37m[1m[2023-07-11 07:50:26,679][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:50:35,622][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 07:50:35,622][233954] FPS: 429438.58[0m
[36m[2023-07-11 07:50:35,625][233954] itr=590, itrs=2000, Progress: 29.50%[0m
[37m[1m[2023-07-11 07:54:06,474][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000570[0m
[36m[2023-07-11 07:54:18,732][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 07:54:18,733][233954] FPS: 328703.37[0m
[36m[2023-07-11 07:54:23,128][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:54:23,128][233954] Reward + Measures: [[28.33761161  0.05094433  0.28603867  0.20702234  0.07322567  0.66593832]][0m
[37m[1m[2023-07-11 07:54:23,129][233954] Max Reward on eval: 28.337611610618964[0m
[37m[1m[2023-07-11 07:54:23,129][233954] Min Reward on eval: 28.337611610618964[0m
[37m[1m[2023-07-11 07:54:23,129][233954] Mean Reward across all agents: 28.337611610618964[0m
[37m[1m[2023-07-11 07:54:23,129][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:54:28,148][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:54:28,148][233954] Reward + Measures: [[-131.56455945    0.95279998    0.963         0.93969995    0.97310013
     3.00358438]
 [ -15.17234023    0.72080004    0.69519997    0.67890006    0.73089999
     2.77308607]
 [  40.04507669    0.0521        0.11000001    0.0622        0.07309999
     2.84103489]
 ...
 [  67.53134395    0.35789999    0.84130001    0.37110001    0.69970006
     2.49568248]
 [  -0.42913569    0.12899999    0.70460004    0.37760001    0.7173
     2.79530311]
 [  47.58054292    0.78899997    0.79960006    0.79839998    0.81720001
     3.31370926]][0m
[37m[1m[2023-07-11 07:54:28,148][233954] Max Reward on eval: 772.8506622270681[0m
[37m[1m[2023-07-11 07:54:28,149][233954] Min Reward on eval: -183.38206400228665[0m
[37m[1m[2023-07-11 07:54:28,149][233954] Mean Reward across all agents: 52.01031116579671[0m
[37m[1m[2023-07-11 07:54:28,149][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:54:28,153][233954] mean_value=-117.01138944617738, max_value=657.2042090194659[0m
[37m[1m[2023-07-11 07:54:28,156][233954] New mean coefficients: [[-0.32734466  2.3399968  -0.5636956  -0.9901202   3.1643364  -4.9302015 ]][0m
[37m[1m[2023-07-11 07:54:28,157][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:54:37,171][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 07:54:37,172][233954] FPS: 426062.12[0m
[36m[2023-07-11 07:54:37,174][233954] itr=591, itrs=2000, Progress: 29.55%[0m
[36m[2023-07-11 07:54:48,903][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 07:54:48,903][233954] FPS: 329841.34[0m
[36m[2023-07-11 07:54:53,173][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:54:53,174][233954] Reward + Measures: [[28.88752153  0.04819933  0.26874134  0.18896668  0.069914    0.65508842]][0m
[37m[1m[2023-07-11 07:54:53,174][233954] Max Reward on eval: 28.88752153435658[0m
[37m[1m[2023-07-11 07:54:53,174][233954] Min Reward on eval: 28.88752153435658[0m
[37m[1m[2023-07-11 07:54:53,175][233954] Mean Reward across all agents: 28.88752153435658[0m
[37m[1m[2023-07-11 07:54:53,175][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:54:58,156][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:54:58,156][233954] Reward + Measures: [[-16.49481606   0.46620002   0.45320007   0.442        0.49640003
    3.24429774]
 [140.23996639   0.26120004   0.79689997   0.39750001   0.80930007
    2.65354633]
 [-13.36026001   0.9465       0.96089995   0.92189997   0.96060002
    3.05938888]
 ...
 [ 79.99279237   0.26089999   0.6846       0.37799999   0.62449998
    3.16630101]
 [ 99.86233397   0.92500001   0.92810005   0.91160005   0.92389995
    3.49504161]
 [502.9109488    0.0361       0.91800004   0.65670002   0.89939994
    3.19002438]][0m
[37m[1m[2023-07-11 07:54:58,156][233954] Max Reward on eval: 778.0310592701659[0m
[37m[1m[2023-07-11 07:54:58,157][233954] Min Reward on eval: -283.5619606886059[0m
[37m[1m[2023-07-11 07:54:58,157][233954] Mean Reward across all agents: 55.45810881047659[0m
[37m[1m[2023-07-11 07:54:58,157][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:54:58,161][233954] mean_value=-161.69966825171198, max_value=483.39212819929753[0m
[37m[1m[2023-07-11 07:54:58,163][233954] New mean coefficients: [[ 0.07728213  2.5067456   0.24690199 -0.762248    3.4770854  -4.529232  ]][0m
[37m[1m[2023-07-11 07:54:58,164][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:55:07,209][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 07:55:07,209][233954] FPS: 424634.90[0m
[36m[2023-07-11 07:55:07,212][233954] itr=592, itrs=2000, Progress: 29.60%[0m
[36m[2023-07-11 07:55:18,989][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 07:55:18,989][233954] FPS: 328364.82[0m
[36m[2023-07-11 07:55:23,193][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:55:23,193][233954] Reward + Measures: [[29.11045322  0.05175     0.28894132  0.20995833  0.07307167  0.62775409]][0m
[37m[1m[2023-07-11 07:55:23,193][233954] Max Reward on eval: 29.110453219611987[0m
[37m[1m[2023-07-11 07:55:23,194][233954] Min Reward on eval: 29.110453219611987[0m
[37m[1m[2023-07-11 07:55:23,194][233954] Mean Reward across all agents: 29.110453219611987[0m
[37m[1m[2023-07-11 07:55:23,194][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:55:28,146][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:55:28,147][233954] Reward + Measures: [[162.13239332   0.18080001   0.29429999   0.2113       0.3028
    2.84814429]
 [ 70.61999248   0.11979999   0.1224       0.10260002   0.1371
    2.95480204]
 [ 48.90109601   0.26040003   0.85329992   0.14670001   0.86269999
    3.38266802]
 ...
 [-31.69360347   0.40850002   0.9774       0.0592       0.97490007
    3.90586019]
 [133.42818639   0.1741       0.1432       0.14130001   0.18049999
    2.74333239]
 [232.10511017   0.0604       0.87849998   0.5086       0.87220001
    3.05547071]][0m
[37m[1m[2023-07-11 07:55:28,147][233954] Max Reward on eval: 686.7897529915906[0m
[37m[1m[2023-07-11 07:55:28,147][233954] Min Reward on eval: -178.82176685538144[0m
[37m[1m[2023-07-11 07:55:28,148][233954] Mean Reward across all agents: 51.62512888376901[0m
[37m[1m[2023-07-11 07:55:28,148][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:55:28,154][233954] mean_value=-12.000327216307445, max_value=594.7013039560989[0m
[37m[1m[2023-07-11 07:55:28,156][233954] New mean coefficients: [[ 2.2031481  3.1131668  2.0626426 -1.1433735  4.4368925 -2.5907154]][0m
[37m[1m[2023-07-11 07:55:28,157][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:55:37,133][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 07:55:37,133][233954] FPS: 427903.43[0m
[36m[2023-07-11 07:55:37,136][233954] itr=593, itrs=2000, Progress: 29.65%[0m
[36m[2023-07-11 07:55:48,743][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 07:55:48,744][233954] FPS: 333198.99[0m
[36m[2023-07-11 07:55:53,031][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:55:53,032][233954] Reward + Measures: [[27.33937533  0.04614267  0.31369799  0.21786334  0.067816    0.60044158]][0m
[37m[1m[2023-07-11 07:55:53,032][233954] Max Reward on eval: 27.339375326594705[0m
[37m[1m[2023-07-11 07:55:53,032][233954] Min Reward on eval: 27.339375326594705[0m
[37m[1m[2023-07-11 07:55:53,032][233954] Mean Reward across all agents: 27.339375326594705[0m
[37m[1m[2023-07-11 07:55:53,033][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:55:58,291][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:55:58,291][233954] Reward + Measures: [[350.46250557   0.0936       0.7802       0.4364       0.77870005
    2.4266355 ]
 [ -4.72115892   0.92159998   0.92700005   0.93310004   0.92640001
    3.58836102]
 [ -4.99751015   0.8682       0.89580005   0.8617       0.89540005
    2.59103131]
 ...
 [ 16.22723824   0.95879996   0.97769994   0.95230001   0.98530006
    2.62018728]
 [  0.47188241   0.98820001   0.98940003   0.99080002   0.99040002
    3.9792695 ]
 [  8.63514975   0.91920006   0.92290002   0.93000001   0.92019999
    3.78937387]][0m
[37m[1m[2023-07-11 07:55:58,292][233954] Max Reward on eval: 503.92791748871093[0m
[37m[1m[2023-07-11 07:55:58,292][233954] Min Reward on eval: -390.67668150663377[0m
[37m[1m[2023-07-11 07:55:58,292][233954] Mean Reward across all agents: 35.25310332399073[0m
[37m[1m[2023-07-11 07:55:58,292][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:55:58,294][233954] mean_value=-175.9868223198284, max_value=220.49331855144047[0m
[37m[1m[2023-07-11 07:55:58,297][233954] New mean coefficients: [[ 2.1550252  4.2824364  1.7532873 -1.8460991  5.9028172 -1.069793 ]][0m
[37m[1m[2023-07-11 07:55:58,298][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:56:07,286][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 07:56:07,286][233954] FPS: 427332.60[0m
[36m[2023-07-11 07:56:07,288][233954] itr=594, itrs=2000, Progress: 29.70%[0m
[36m[2023-07-11 07:56:19,147][233954] train() took 11.77 seconds to complete[0m
[36m[2023-07-11 07:56:19,148][233954] FPS: 326162.27[0m
[36m[2023-07-11 07:56:23,460][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:56:23,460][233954] Reward + Measures: [[29.11462914  0.04907566  0.32016167  0.20963767  0.070703    0.62157786]][0m
[37m[1m[2023-07-11 07:56:23,460][233954] Max Reward on eval: 29.11462914189997[0m
[37m[1m[2023-07-11 07:56:23,461][233954] Min Reward on eval: 29.11462914189997[0m
[37m[1m[2023-07-11 07:56:23,461][233954] Mean Reward across all agents: 29.11462914189997[0m
[37m[1m[2023-07-11 07:56:23,461][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:56:28,515][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:56:28,516][233954] Reward + Measures: [[ 58.48637509   0.1322       0.24509998   0.15440001   0.26429999
    2.96065068]
 [178.86055467   0.1533       0.37110001   0.22180001   0.36939999
    2.96145606]
 [ 65.03761902   0.09630001   0.40970001   0.26830003   0.40030003
    3.18384576]
 ...
 [ 32.68062361   0.13429999   0.42109999   0.23720001   0.41929999
    3.19361663]
 [ 83.27077604   0.0895       0.1238       0.12190001   0.12620001
    2.95620346]
 [ 83.73131086   0.08859999   0.222        0.1805       0.2335
    2.93407607]][0m
[37m[1m[2023-07-11 07:56:28,516][233954] Max Reward on eval: 577.3885889002122[0m
[37m[1m[2023-07-11 07:56:28,516][233954] Min Reward on eval: -101.12779950900003[0m
[37m[1m[2023-07-11 07:56:28,516][233954] Mean Reward across all agents: 80.54253833064875[0m
[37m[1m[2023-07-11 07:56:28,517][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:56:28,520][233954] mean_value=-179.19291846837888, max_value=589.8121690953271[0m
[37m[1m[2023-07-11 07:56:28,522][233954] New mean coefficients: [[ 0.03552127  3.157677   -0.443511   -1.6302398   4.6924706  -3.2727005 ]][0m
[37m[1m[2023-07-11 07:56:28,523][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:56:37,581][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 07:56:37,582][233954] FPS: 424007.41[0m
[36m[2023-07-11 07:56:37,584][233954] itr=595, itrs=2000, Progress: 29.75%[0m
[36m[2023-07-11 07:56:49,328][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 07:56:49,329][233954] FPS: 329356.54[0m
[36m[2023-07-11 07:56:53,656][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:56:53,656][233954] Reward + Measures: [[33.66928326  0.054292    0.29279032  0.18933631  0.080879    0.68522608]][0m
[37m[1m[2023-07-11 07:56:53,656][233954] Max Reward on eval: 33.669283257701416[0m
[37m[1m[2023-07-11 07:56:53,656][233954] Min Reward on eval: 33.669283257701416[0m
[37m[1m[2023-07-11 07:56:53,657][233954] Mean Reward across all agents: 33.669283257701416[0m
[37m[1m[2023-07-11 07:56:53,657][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:56:58,651][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:56:58,652][233954] Reward + Measures: [[175.66488553   0.1451       0.53459996   0.29080001   0.46300003
    2.64539528]
 [-29.51109714   0.23020001   0.25050002   0.23190001   0.2791
    2.78528285]
 [ 48.28378262   0.15210001   0.58650005   0.15260001   0.66420001
    2.75171494]
 ...
 [ 69.93981594   0.09760001   0.23049998   0.1419       0.24270001
    2.60236526]
 [-56.46200996   0.38700002   0.46830001   0.37020001   0.5007
    2.79665565]
 [ 47.55866243   0.14490001   0.1099       0.1925       0.18870001
    3.04465413]][0m
[37m[1m[2023-07-11 07:56:58,652][233954] Max Reward on eval: 347.4779504580423[0m
[37m[1m[2023-07-11 07:56:58,652][233954] Min Reward on eval: -178.93440413149074[0m
[37m[1m[2023-07-11 07:56:58,653][233954] Mean Reward across all agents: 63.003028229432964[0m
[37m[1m[2023-07-11 07:56:58,653][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:56:58,657][233954] mean_value=-163.72184436104862, max_value=446.6757751764637[0m
[37m[1m[2023-07-11 07:56:58,660][233954] New mean coefficients: [[ 1.5491284   2.966072    0.96742725 -0.9556397   4.457991   -2.8728158 ]][0m
[37m[1m[2023-07-11 07:56:58,660][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:57:07,633][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 07:57:07,633][233954] FPS: 428052.97[0m
[36m[2023-07-11 07:57:07,636][233954] itr=596, itrs=2000, Progress: 29.80%[0m
[36m[2023-07-11 07:57:19,181][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 07:57:19,181][233954] FPS: 335052.21[0m
[36m[2023-07-11 07:57:23,497][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:57:23,497][233954] Reward + Measures: [[33.29725188  0.05498534  0.301314    0.18801734  0.080579    0.68141335]][0m
[37m[1m[2023-07-11 07:57:23,498][233954] Max Reward on eval: 33.29725187520912[0m
[37m[1m[2023-07-11 07:57:23,498][233954] Min Reward on eval: 33.29725187520912[0m
[37m[1m[2023-07-11 07:57:23,498][233954] Mean Reward across all agents: 33.29725187520912[0m
[37m[1m[2023-07-11 07:57:23,498][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:57:28,445][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:57:28,445][233954] Reward + Measures: [[ -24.01740951    0.26280001    0.7277        0.30580002    0.83010006
     3.50891423]
 [-105.01832795    0.65799999    0.59390002    0.60420001    0.72920007
     3.4200573 ]
 [ 138.59472607    0.14490001    0.6925        0.3382        0.65600002
     3.15530396]
 ...
 [-160.57226275    0.22160001    0.5151        0.23439999    0.59600002
     2.84224391]
 [  29.91168424    0.17709999    0.5478        0.28860003    0.51029998
     2.81791496]
 [-108.52990341    0.40220004    0.7604        0.3908        0.87200004
     3.40956378]][0m
[37m[1m[2023-07-11 07:57:28,446][233954] Max Reward on eval: 616.4470748784021[0m
[37m[1m[2023-07-11 07:57:28,446][233954] Min Reward on eval: -176.40435838997365[0m
[37m[1m[2023-07-11 07:57:28,446][233954] Mean Reward across all agents: 25.076252334059554[0m
[37m[1m[2023-07-11 07:57:28,446][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:57:28,450][233954] mean_value=-174.1654098883844, max_value=525.6218187510967[0m
[37m[1m[2023-07-11 07:57:28,452][233954] New mean coefficients: [[ 1.9960841  2.523631   0.9159593 -0.2962395  4.2596035 -3.1584854]][0m
[37m[1m[2023-07-11 07:57:28,453][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:57:37,477][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 07:57:37,477][233954] FPS: 425608.00[0m
[36m[2023-07-11 07:57:37,479][233954] itr=597, itrs=2000, Progress: 29.85%[0m
[36m[2023-07-11 07:57:49,233][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 07:57:49,234][233954] FPS: 329025.18[0m
[36m[2023-07-11 07:57:53,525][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:57:53,526][233954] Reward + Measures: [[28.27695869  0.05588067  0.342273    0.22680633  0.084489    0.64754385]][0m
[37m[1m[2023-07-11 07:57:53,526][233954] Max Reward on eval: 28.276958693738038[0m
[37m[1m[2023-07-11 07:57:53,526][233954] Min Reward on eval: 28.276958693738038[0m
[37m[1m[2023-07-11 07:57:53,527][233954] Mean Reward across all agents: 28.276958693738038[0m
[37m[1m[2023-07-11 07:57:53,527][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:57:58,503][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:57:58,503][233954] Reward + Measures: [[ 59.84684846   0.15879999   0.2597       0.0737       0.22420001
    3.09773755]
 [ 25.10614931   0.18679999   0.42179999   0.18620001   0.30060002
    2.19259167]
 [197.80670618   0.33140001   0.84749997   0.54269999   0.79830003
    2.39049411]
 ...
 [206.01761772   0.1591       0.62379998   0.32940003   0.59429997
    2.81393766]
 [171.95131638   0.1886       0.6534       0.31619999   0.56080002
    2.43326116]
 [237.16930551   0.2588       0.68760002   0.36270002   0.6846
    2.38919997]][0m
[37m[1m[2023-07-11 07:57:58,504][233954] Max Reward on eval: 407.92294309120626[0m
[37m[1m[2023-07-11 07:57:58,504][233954] Min Reward on eval: -132.32512521741447[0m
[37m[1m[2023-07-11 07:57:58,504][233954] Mean Reward across all agents: 96.08388240368632[0m
[37m[1m[2023-07-11 07:57:58,505][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:57:58,508][233954] mean_value=-195.01268959873138, max_value=624.309402310228[0m
[37m[1m[2023-07-11 07:57:58,510][233954] New mean coefficients: [[ 2.0966802  2.2335758  1.0497186 -0.4135242  3.6690793 -3.4891117]][0m
[37m[1m[2023-07-11 07:57:58,511][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:58:07,504][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 07:58:07,504][233954] FPS: 427063.42[0m
[36m[2023-07-11 07:58:07,507][233954] itr=598, itrs=2000, Progress: 29.90%[0m
[36m[2023-07-11 07:58:19,109][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 07:58:19,110][233954] FPS: 333336.71[0m
[36m[2023-07-11 07:58:23,322][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:58:23,322][233954] Reward + Measures: [[28.26697487  0.05744667  0.34441763  0.22405933  0.08845166  0.64407068]][0m
[37m[1m[2023-07-11 07:58:23,323][233954] Max Reward on eval: 28.266974869418377[0m
[37m[1m[2023-07-11 07:58:23,323][233954] Min Reward on eval: 28.266974869418377[0m
[37m[1m[2023-07-11 07:58:23,323][233954] Mean Reward across all agents: 28.266974869418377[0m
[37m[1m[2023-07-11 07:58:23,323][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:58:28,517][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:58:28,518][233954] Reward + Measures: [[-105.09634207    0.63670003    0.67189997    0.61870003    0.70160002
     2.92808032]
 [ -10.49144658    0.18099999    0.23580001    0.19450001    0.20209999
     2.98924589]
 [  12.96181278    0.42109999    0.69229996    0.38480005    0.5539
     2.91444516]
 ...
 [ 155.84934262    0.13220002    0.6419        0.24299999    0.60430002
     2.41609192]
 [  45.34966723    0.51330006    0.78730005    0.47179994    0.74910003
     2.31599045]
 [ -15.80962384    0.56840003    0.73949999    0.59039998    0.72370005
     3.09970355]][0m
[37m[1m[2023-07-11 07:58:28,518][233954] Max Reward on eval: 755.6583175664767[0m
[37m[1m[2023-07-11 07:58:28,518][233954] Min Reward on eval: -475.88605020809916[0m
[37m[1m[2023-07-11 07:58:28,518][233954] Mean Reward across all agents: 28.69640509807262[0m
[37m[1m[2023-07-11 07:58:28,519][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:58:28,521][233954] mean_value=-176.43577858622012, max_value=212.86687313899773[0m
[37m[1m[2023-07-11 07:58:28,524][233954] New mean coefficients: [[ 1.3864603   1.5463464   0.5471414   0.09944481  2.8869767  -4.753955  ]][0m
[37m[1m[2023-07-11 07:58:28,524][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:58:37,526][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 07:58:37,526][233954] FPS: 426660.61[0m
[36m[2023-07-11 07:58:37,529][233954] itr=599, itrs=2000, Progress: 29.95%[0m
[36m[2023-07-11 07:58:49,168][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 07:58:49,168][233954] FPS: 332295.19[0m
[36m[2023-07-11 07:58:53,482][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:58:53,482][233954] Reward + Measures: [[30.36509574  0.04567867  0.35486197  0.224526    0.077014    0.61823553]][0m
[37m[1m[2023-07-11 07:58:53,483][233954] Max Reward on eval: 30.365095739766335[0m
[37m[1m[2023-07-11 07:58:53,483][233954] Min Reward on eval: 30.365095739766335[0m
[37m[1m[2023-07-11 07:58:53,483][233954] Mean Reward across all agents: 30.365095739766335[0m
[37m[1m[2023-07-11 07:58:53,483][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:58:58,513][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 07:58:58,514][233954] Reward + Measures: [[ 79.81636286   0.0916       0.2861       0.10520001   0.25760001
    2.53660369]
 [191.91023225   0.36970001   0.2712       0.25770003   0.48480007
    3.15128183]
 [  8.39503475   0.68040001   0.82709998   0.63850003   0.79840004
    2.30849361]
 ...
 [176.23895455   0.0165       0.991        0.54840004   0.99180001
    3.34260416]
 [  8.33382623   0.98710006   0.99080008   0.99039996   0.9932
    3.65291333]
 [-19.76181685   0.98900002   0.99370003   0.99200004   0.99299997
    3.31630445]][0m
[37m[1m[2023-07-11 07:58:58,514][233954] Max Reward on eval: 758.074073805986[0m
[37m[1m[2023-07-11 07:58:58,514][233954] Min Reward on eval: -391.3600225467235[0m
[37m[1m[2023-07-11 07:58:58,514][233954] Mean Reward across all agents: 23.25723349368769[0m
[37m[1m[2023-07-11 07:58:58,514][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 07:58:58,518][233954] mean_value=-140.62670097837437, max_value=289.63388681402307[0m
[37m[1m[2023-07-11 07:58:58,521][233954] New mean coefficients: [[-0.14070344  0.7718301  -1.5982594   0.53140074  1.8060783  -6.559757  ]][0m
[37m[1m[2023-07-11 07:58:58,522][233954] Moving the mean solution point...[0m
[36m[2023-07-11 07:59:07,637][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 07:59:07,638][233954] FPS: 421321.90[0m
[36m[2023-07-11 07:59:07,640][233954] itr=600, itrs=2000, Progress: 30.00%[0m
[37m[1m[2023-07-11 08:02:42,979][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000580[0m
[36m[2023-07-11 08:02:55,256][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 08:02:55,257][233954] FPS: 331034.70[0m
[36m[2023-07-11 08:02:59,363][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:02:59,364][233954] Reward + Measures: [[-1.59474377  0.31295133  0.61124569  0.27820167  0.46155998  1.503245  ]][0m
[37m[1m[2023-07-11 08:02:59,364][233954] Max Reward on eval: -1.5947437723143376[0m
[37m[1m[2023-07-11 08:02:59,364][233954] Min Reward on eval: -1.5947437723143376[0m
[37m[1m[2023-07-11 08:02:59,365][233954] Mean Reward across all agents: -1.5947437723143376[0m
[37m[1m[2023-07-11 08:02:59,365][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:03:04,333][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:03:04,334][233954] Reward + Measures: [[148.89305165   0.1069       0.861        0.40879998   0.87309998
    3.16295385]
 [192.00476625   0.1141       0.76239997   0.51459998   0.78500003
    3.15432191]
 [ 79.81437635   0.09670001   0.13880001   0.1129       0.16659999
    2.46607542]
 ...
 [ 29.54387696   0.42090002   0.90999997   0.075        0.91350001
    3.24309516]
 [-41.1207206    0.40690002   0.66009998   0.45949998   0.67980003
    2.71012807]
 [103.17700987   0.27800003   0.22979999   0.32329997   0.23190001
    2.81203985]][0m
[37m[1m[2023-07-11 08:03:04,334][233954] Max Reward on eval: 451.67184494119135[0m
[37m[1m[2023-07-11 08:03:04,334][233954] Min Reward on eval: -179.09896183966194[0m
[37m[1m[2023-07-11 08:03:04,334][233954] Mean Reward across all agents: 100.16798233079626[0m
[37m[1m[2023-07-11 08:03:04,335][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:03:04,340][233954] mean_value=-88.23946557455245, max_value=477.6447472508735[0m
[37m[1m[2023-07-11 08:03:04,343][233954] New mean coefficients: [[ 0.96150875  2.267905   -0.59365535 -0.23700762  3.7202153  -3.9195976 ]][0m
[37m[1m[2023-07-11 08:03:04,344][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:03:13,358][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 08:03:13,358][233954] FPS: 426066.60[0m
[36m[2023-07-11 08:03:13,360][233954] itr=601, itrs=2000, Progress: 30.05%[0m
[36m[2023-07-11 08:03:24,887][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 08:03:24,893][233954] FPS: 335666.92[0m
[36m[2023-07-11 08:03:29,190][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:03:29,190][233954] Reward + Measures: [[333.51289387   0.06709      0.520455     0.40285301   0.58666569
    2.37685394]][0m
[37m[1m[2023-07-11 08:03:29,191][233954] Max Reward on eval: 333.51289386975117[0m
[37m[1m[2023-07-11 08:03:29,191][233954] Min Reward on eval: 333.51289386975117[0m
[37m[1m[2023-07-11 08:03:29,191][233954] Mean Reward across all agents: 333.51289386975117[0m
[37m[1m[2023-07-11 08:03:29,192][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:03:34,176][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:03:34,176][233954] Reward + Measures: [[-23.52044516   0.102        0.266        0.2158       0.35429999
    2.73969865]
 [ 72.53816294   0.0568       0.2906       0.2309       0.3346
    3.30164576]
 [ 85.68525022   0.14749999   0.22589998   0.20550001   0.31349999
    2.4538703 ]
 ...
 [202.12830759   0.0485       0.6487       0.49370003   0.63679999
    3.05598497]
 [328.07350432   0.0975       0.73210001   0.47419998   0.72290003
    2.79411364]
 [  6.16971431   0.0729       0.1148       0.0868       0.11850001
    3.1561048 ]][0m
[37m[1m[2023-07-11 08:03:34,176][233954] Max Reward on eval: 778.9555282472168[0m
[37m[1m[2023-07-11 08:03:34,177][233954] Min Reward on eval: -281.75052373632786[0m
[37m[1m[2023-07-11 08:03:34,177][233954] Mean Reward across all agents: 130.5397483540627[0m
[37m[1m[2023-07-11 08:03:34,177][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:03:34,180][233954] mean_value=-150.12287289209115, max_value=505.88580194165934[0m
[37m[1m[2023-07-11 08:03:34,183][233954] New mean coefficients: [[ 0.27046013  1.468196   -1.1608937  -0.08644646  3.045577   -5.099877  ]][0m
[37m[1m[2023-07-11 08:03:34,184][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:03:43,175][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 08:03:43,176][233954] FPS: 427143.36[0m
[36m[2023-07-11 08:03:43,178][233954] itr=602, itrs=2000, Progress: 30.10%[0m
[36m[2023-07-11 08:03:54,975][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 08:03:54,976][233954] FPS: 327799.98[0m
[36m[2023-07-11 08:03:59,193][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:03:59,199][233954] Reward + Measures: [[305.63268962   0.07345633   0.48872399   0.37752363   0.557275
    2.32423401]][0m
[37m[1m[2023-07-11 08:03:59,199][233954] Max Reward on eval: 305.63268961668007[0m
[37m[1m[2023-07-11 08:03:59,199][233954] Min Reward on eval: 305.63268961668007[0m
[37m[1m[2023-07-11 08:03:59,200][233954] Mean Reward across all agents: 305.63268961668007[0m
[37m[1m[2023-07-11 08:03:59,200][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:04:04,152][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:04:04,153][233954] Reward + Measures: [[ 65.90904258   0.32070002   0.37600002   0.2177       0.45479998
    2.72806621]
 [263.30253983   0.10339999   0.98460001   0.67589998   0.98579997
    3.54184794]
 [ -4.09184037   0.76359999   0.80629998   0.74290001   0.79259998
    2.76292205]
 ...
 [ 86.29582459   0.8441       0.84860003   0.83810008   0.84009999
    3.18823171]
 [ 71.8560087    0.33750001   0.50220007   0.26210001   0.47620001
    2.25027537]
 [ -5.61092274   0.98379993   0.98460001   0.9842       0.99220002
    3.13620877]][0m
[37m[1m[2023-07-11 08:04:04,153][233954] Max Reward on eval: 505.40192928295585[0m
[37m[1m[2023-07-11 08:04:04,153][233954] Min Reward on eval: -132.6817226115614[0m
[37m[1m[2023-07-11 08:04:04,154][233954] Mean Reward across all agents: 45.96986402726407[0m
[37m[1m[2023-07-11 08:04:04,154][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:04:04,157][233954] mean_value=-136.97395072541156, max_value=326.41017029508816[0m
[37m[1m[2023-07-11 08:04:04,160][233954] New mean coefficients: [[-1.1519661   0.60517764 -1.74223     0.01935247  2.2034855  -6.9968104 ]][0m
[37m[1m[2023-07-11 08:04:04,161][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:04:13,150][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 08:04:13,150][233954] FPS: 427269.98[0m
[36m[2023-07-11 08:04:13,152][233954] itr=603, itrs=2000, Progress: 30.15%[0m
[36m[2023-07-11 08:04:24,804][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 08:04:24,804][233954] FPS: 331954.78[0m
[36m[2023-07-11 08:04:29,031][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:04:29,036][233954] Reward + Measures: [[276.52722494   0.08542001   0.43212068   0.33780965   0.50729263
    2.26579094]][0m
[37m[1m[2023-07-11 08:04:29,037][233954] Max Reward on eval: 276.5272249356936[0m
[37m[1m[2023-07-11 08:04:29,037][233954] Min Reward on eval: 276.5272249356936[0m
[37m[1m[2023-07-11 08:04:29,037][233954] Mean Reward across all agents: 276.5272249356936[0m
[37m[1m[2023-07-11 08:04:29,038][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:04:34,003][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:04:34,004][233954] Reward + Measures: [[-15.39408334   0.0173       0.96499997   0.85699999   0.96780008
    3.51792002]
 [121.04699087   0.12899999   0.72490001   0.37470004   0.76230001
    3.20060039]
 [127.28378285   0.10700001   0.80000001   0.38189998   0.81650001
    2.8932538 ]
 ...
 [ 28.9339219    0.25460002   0.48709998   0.28120002   0.5158
    2.62855029]
 [696.42319823   0.0185       0.89200002   0.64170003   0.89120001
    3.16368675]
 [ 31.96167523   0.14300002   0.21929999   0.0954       0.2256
    2.64013362]][0m
[37m[1m[2023-07-11 08:04:34,004][233954] Max Reward on eval: 788.9064064029604[0m
[37m[1m[2023-07-11 08:04:34,004][233954] Min Reward on eval: -181.50038882736118[0m
[37m[1m[2023-07-11 08:04:34,004][233954] Mean Reward across all agents: 102.23434681572537[0m
[37m[1m[2023-07-11 08:04:34,005][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:04:34,007][233954] mean_value=-240.52848522378713, max_value=901.854296147759[0m
[37m[1m[2023-07-11 08:04:34,010][233954] New mean coefficients: [[-0.70729977  0.41040796 -1.5871458  -0.33437213  1.7028357  -7.004819  ]][0m
[37m[1m[2023-07-11 08:04:34,011][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:04:42,933][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 08:04:42,933][233954] FPS: 430460.52[0m
[36m[2023-07-11 08:04:42,935][233954] itr=604, itrs=2000, Progress: 30.20%[0m
[36m[2023-07-11 08:04:54,555][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 08:04:54,556][233954] FPS: 332877.21[0m
[36m[2023-07-11 08:04:58,809][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:04:58,815][233954] Reward + Measures: [[260.72692457   0.08064134   0.42452368   0.32744867   0.50768906
    2.22070909]][0m
[37m[1m[2023-07-11 08:04:58,815][233954] Max Reward on eval: 260.726924573633[0m
[37m[1m[2023-07-11 08:04:58,815][233954] Min Reward on eval: 260.726924573633[0m
[37m[1m[2023-07-11 08:04:58,816][233954] Mean Reward across all agents: 260.726924573633[0m
[37m[1m[2023-07-11 08:04:58,816][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:05:03,819][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:05:03,825][233954] Reward + Measures: [[ 45.93810218   0.0929       0.3098       0.1232       0.34510002
    2.13271689]
 [145.45875009   0.34150001   0.74700004   0.43389997   0.65309995
    2.42597079]
 [ 61.24555328   0.4323       0.64440006   0.41149998   0.6476
    2.63864398]
 ...
 [ 43.40066745   0.61659998   0.5442       0.60400003   0.53009999
    2.65361643]
 [ 14.80075559   0.52570003   0.75880009   0.50620002   0.72469997
    2.75090671]
 [ -8.85081403   0.29629999   0.63940001   0.1908       0.54769999
    2.30466127]][0m
[37m[1m[2023-07-11 08:05:03,825][233954] Max Reward on eval: 671.0810623280704[0m
[37m[1m[2023-07-11 08:05:03,826][233954] Min Reward on eval: -52.791655024501964[0m
[37m[1m[2023-07-11 08:05:03,826][233954] Mean Reward across all agents: 123.25739048472377[0m
[37m[1m[2023-07-11 08:05:03,826][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:05:03,830][233954] mean_value=-115.00678023531029, max_value=578.6336969399533[0m
[37m[1m[2023-07-11 08:05:03,833][233954] New mean coefficients: [[ 0.2866419   1.013288   -0.5713855  -0.10373026  2.194381   -5.8731995 ]][0m
[37m[1m[2023-07-11 08:05:03,834][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:05:12,778][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 08:05:12,778][233954] FPS: 429438.07[0m
[36m[2023-07-11 08:05:12,780][233954] itr=605, itrs=2000, Progress: 30.25%[0m
[36m[2023-07-11 08:05:24,317][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 08:05:24,318][233954] FPS: 335233.32[0m
[36m[2023-07-11 08:05:28,576][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:05:28,577][233954] Reward + Measures: [[254.20482857   0.07924367   0.43000528   0.32363701   0.50456631
    2.21067142]][0m
[37m[1m[2023-07-11 08:05:28,577][233954] Max Reward on eval: 254.204828569666[0m
[37m[1m[2023-07-11 08:05:28,577][233954] Min Reward on eval: 254.204828569666[0m
[37m[1m[2023-07-11 08:05:28,577][233954] Mean Reward across all agents: 254.204828569666[0m
[37m[1m[2023-07-11 08:05:28,578][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:05:33,498][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:05:33,499][233954] Reward + Measures: [[100.77827126   0.1779       0.82689995   0.47939998   0.74419999
    2.51931429]
 [251.90719487   0.0819       0.4745       0.26570001   0.51209998
    2.68832326]
 [169.96326773   0.14920001   0.42969999   0.18629999   0.4815
    3.05957603]
 ...
 [220.97672901   0.23340002   0.7058       0.33070001   0.63679999
    2.40599513]
 [  3.65841076   0.0699       0.94310009   0.44960004   0.91119999
    2.74198103]
 [204.38825062   0.26100001   0.55790001   0.14400001   0.55300003
    2.69687343]][0m
[37m[1m[2023-07-11 08:05:33,499][233954] Max Reward on eval: 514.640780958347[0m
[37m[1m[2023-07-11 08:05:33,499][233954] Min Reward on eval: -168.4454946497455[0m
[37m[1m[2023-07-11 08:05:33,500][233954] Mean Reward across all agents: 109.12909091149864[0m
[37m[1m[2023-07-11 08:05:33,500][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:05:33,506][233954] mean_value=-69.99269809492104, max_value=617.6654760673741[0m
[37m[1m[2023-07-11 08:05:33,509][233954] New mean coefficients: [[ 1.620316   2.1127877  0.8720217 -0.3337801  3.5401652 -3.6455483]][0m
[37m[1m[2023-07-11 08:05:33,510][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:05:42,452][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 08:05:42,452][233954] FPS: 429495.44[0m
[36m[2023-07-11 08:05:42,455][233954] itr=606, itrs=2000, Progress: 30.30%[0m
[36m[2023-07-11 08:05:54,282][233954] train() took 11.75 seconds to complete[0m
[36m[2023-07-11 08:05:54,282][233954] FPS: 326926.22[0m
[36m[2023-07-11 08:05:58,529][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:05:58,534][233954] Reward + Measures: [[262.85157256   0.07671167   0.43842399   0.33199498   0.51506537
    2.20761728]][0m
[37m[1m[2023-07-11 08:05:58,534][233954] Max Reward on eval: 262.8515725599976[0m
[37m[1m[2023-07-11 08:05:58,535][233954] Min Reward on eval: 262.8515725599976[0m
[37m[1m[2023-07-11 08:05:58,535][233954] Mean Reward across all agents: 262.8515725599976[0m
[37m[1m[2023-07-11 08:05:58,535][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:06:03,773][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:06:03,779][233954] Reward + Measures: [[119.68017958   0.1402       0.30329999   0.13800001   0.23810001
    2.92057681]
 [ 11.93844282   0.1199       0.16430001   0.1017       0.1084
    3.3929913 ]
 [253.11928757   0.0397       0.48410001   0.37560001   0.51239997
    3.13495135]
 ...
 [ 10.48790096   0.09320001   0.1277       0.11189999   0.12330001
    3.01904082]
 [ 76.68751809   0.0965       0.2095       0.10999999   0.18230002
    1.86438739]
 [-32.09070781   0.0965       0.0968       0.1171       0.0933
    3.40811968]][0m
[37m[1m[2023-07-11 08:06:03,780][233954] Max Reward on eval: 568.9744682441466[0m
[37m[1m[2023-07-11 08:06:03,780][233954] Min Reward on eval: -78.82307315990329[0m
[37m[1m[2023-07-11 08:06:03,780][233954] Mean Reward across all agents: 117.65174588475939[0m
[37m[1m[2023-07-11 08:06:03,780][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:06:03,784][233954] mean_value=-135.32097155071136, max_value=521.6789418468159[0m
[37m[1m[2023-07-11 08:06:03,787][233954] New mean coefficients: [[ 0.7663725  1.639183   0.1487171  0.2695697  2.598743  -4.9916067]][0m
[37m[1m[2023-07-11 08:06:03,788][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:06:12,769][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 08:06:12,769][233954] FPS: 427632.78[0m
[36m[2023-07-11 08:06:12,772][233954] itr=607, itrs=2000, Progress: 30.35%[0m
[36m[2023-07-11 08:06:24,263][233954] train() took 11.41 seconds to complete[0m
[36m[2023-07-11 08:06:24,263][233954] FPS: 336582.62[0m
[36m[2023-07-11 08:06:28,552][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:06:28,558][233954] Reward + Measures: [[246.33063088   0.08276801   0.42571801   0.32172665   0.49998769
    2.16091537]][0m
[37m[1m[2023-07-11 08:06:28,559][233954] Max Reward on eval: 246.33063087543988[0m
[37m[1m[2023-07-11 08:06:28,560][233954] Min Reward on eval: 246.33063087543988[0m
[37m[1m[2023-07-11 08:06:28,560][233954] Mean Reward across all agents: 246.33063087543988[0m
[37m[1m[2023-07-11 08:06:28,561][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:06:33,510][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:06:33,511][233954] Reward + Measures: [[  2.37966298   0.68850005   0.36579999   0.70239997   0.15220001
    2.97261024]
 [-59.85205399   0.68790001   0.33310002   0.69520003   0.1584
    3.1588769 ]
 [173.89938641   0.1053       0.91800004   0.49779996   0.8466
    2.82029319]
 ...
 [ 63.42545422   0.61610001   0.2167       0.64780003   0.2624
    2.79438901]
 [-29.32730808   0.53570002   0.25920001   0.53910005   0.09730001
    2.85746574]
 [154.14734457   0.56640005   0.32520005   0.62819999   0.32370004
    2.66766548]][0m
[37m[1m[2023-07-11 08:06:33,511][233954] Max Reward on eval: 677.121500007622[0m
[37m[1m[2023-07-11 08:06:33,511][233954] Min Reward on eval: -89.61617119135335[0m
[37m[1m[2023-07-11 08:06:33,512][233954] Mean Reward across all agents: 172.22370797996555[0m
[37m[1m[2023-07-11 08:06:33,512][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:06:33,518][233954] mean_value=-62.65786541207177, max_value=727.7967455402016[0m
[37m[1m[2023-07-11 08:06:33,521][233954] New mean coefficients: [[ 0.6238264   1.9256653  -0.14859396  0.3941062   3.0288014  -4.481591  ]][0m
[37m[1m[2023-07-11 08:06:33,522][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:06:42,602][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 08:06:42,603][233954] FPS: 422955.28[0m
[36m[2023-07-11 08:06:42,605][233954] itr=608, itrs=2000, Progress: 30.40%[0m
[36m[2023-07-11 08:06:54,374][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 08:06:54,374][233954] FPS: 328686.37[0m
[36m[2023-07-11 08:06:58,626][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:06:58,626][233954] Reward + Measures: [[239.88700097   0.08487534   0.41139632   0.31744465   0.49168667
    2.12664342]][0m
[37m[1m[2023-07-11 08:06:58,626][233954] Max Reward on eval: 239.8870009678256[0m
[37m[1m[2023-07-11 08:06:58,627][233954] Min Reward on eval: 239.8870009678256[0m
[37m[1m[2023-07-11 08:06:58,627][233954] Mean Reward across all agents: 239.8870009678256[0m
[37m[1m[2023-07-11 08:06:58,627][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:07:03,587][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:07:03,588][233954] Reward + Measures: [[681.67978284   0.0095       0.99410003   0.72890007   0.98820001
    3.02085829]
 [347.82948305   0.2974       0.99200004   0.60030001   0.97389996
    2.92752886]
 [-32.29519797   0.20910001   0.2263       0.2024       0.26300001
    2.08534694]
 ...
 [ -6.64423751   0.4104       0.98750001   0.44080001   0.95999998
    3.20190215]
 [409.04283811   0.0184       0.98750001   0.7198       0.98509997
    2.83180737]
 [-71.56714541   0.31549999   0.3443       0.2455       0.44409999
    1.9720968 ]][0m
[37m[1m[2023-07-11 08:07:03,588][233954] Max Reward on eval: 749.1948852717876[0m
[37m[1m[2023-07-11 08:07:03,588][233954] Min Reward on eval: -422.8639742324129[0m
[37m[1m[2023-07-11 08:07:03,588][233954] Mean Reward across all agents: 192.84952902622177[0m
[37m[1m[2023-07-11 08:07:03,589][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:07:03,593][233954] mean_value=-165.8648531635916, max_value=572.4614188296139[0m
[37m[1m[2023-07-11 08:07:03,596][233954] New mean coefficients: [[ 0.89705503  1.4859165  -0.07217991  1.1487255   2.1601944  -5.1876183 ]][0m
[37m[1m[2023-07-11 08:07:03,597][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:07:12,558][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 08:07:12,558][233954] FPS: 428619.57[0m
[36m[2023-07-11 08:07:12,560][233954] itr=609, itrs=2000, Progress: 30.45%[0m
[36m[2023-07-11 08:07:24,288][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 08:07:24,288][233954] FPS: 329800.80[0m
[36m[2023-07-11 08:07:28,498][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:07:28,498][233954] Reward + Measures: [[228.63745775   0.08604632   0.39832735   0.30623966   0.47818103
    2.07209444]][0m
[37m[1m[2023-07-11 08:07:28,498][233954] Max Reward on eval: 228.63745774583356[0m
[37m[1m[2023-07-11 08:07:28,499][233954] Min Reward on eval: 228.63745774583356[0m
[37m[1m[2023-07-11 08:07:28,499][233954] Mean Reward across all agents: 228.63745774583356[0m
[37m[1m[2023-07-11 08:07:28,499][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:07:33,477][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:07:33,478][233954] Reward + Measures: [[  38.65488517    0.7762        0.82050002    0.74349999    0.80690002
     2.24215841]
 [  15.63337686    0.76419997    0.80289996    0.74130005    0.79520005
     2.20169139]
 [-108.77568459    0.83059996    0.84650004    0.7967        0.86650002
     3.0822854 ]
 ...
 [ -40.03969425    0.93260002    0.95780003    0.91409999    0.94740009
     2.9657836 ]
 [  21.46047134    0.20580001    0.37050003    0.18230002    0.34050003
     2.21197867]
 [  26.54639316    0.24489999    0.3294        0.25570002    0.32360002
     2.45866275]][0m
[37m[1m[2023-07-11 08:07:33,478][233954] Max Reward on eval: 532.2537479430437[0m
[37m[1m[2023-07-11 08:07:33,478][233954] Min Reward on eval: -146.83491494990886[0m
[37m[1m[2023-07-11 08:07:33,478][233954] Mean Reward across all agents: 20.822542092723406[0m
[37m[1m[2023-07-11 08:07:33,479][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:07:33,482][233954] mean_value=-139.00247058004058, max_value=571.892006041389[0m
[37m[1m[2023-07-11 08:07:33,484][233954] New mean coefficients: [[-0.04964316  0.3011018  -0.5854482   0.71940506  1.0068028  -7.0119247 ]][0m
[37m[1m[2023-07-11 08:07:33,485][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:07:42,458][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 08:07:42,458][233954] FPS: 428028.71[0m
[36m[2023-07-11 08:07:42,461][233954] itr=610, itrs=2000, Progress: 30.50%[0m
[37m[1m[2023-07-11 08:11:09,259][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000590[0m
[36m[2023-07-11 08:11:21,647][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 08:11:21,647][233954] FPS: 329606.28[0m
[36m[2023-07-11 08:11:25,702][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:11:25,702][233954] Reward + Measures: [[155.2947601    0.098992     0.34468132   0.26251334   0.44264835
    2.03887463]][0m
[37m[1m[2023-07-11 08:11:25,702][233954] Max Reward on eval: 155.29476009971333[0m
[37m[1m[2023-07-11 08:11:25,703][233954] Min Reward on eval: 155.29476009971333[0m
[37m[1m[2023-07-11 08:11:25,703][233954] Mean Reward across all agents: 155.29476009971333[0m
[37m[1m[2023-07-11 08:11:25,703][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:11:30,567][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:11:30,568][233954] Reward + Measures: [[ 71.48606841   0.0576       0.1798       0.13609998   0.19349998
    1.55748785]
 [ 95.96387414   0.36270005   0.2895       0.3908       0.48359999
    2.0551641 ]
 [ 28.80590259   0.2203       0.53570002   0.21020003   0.46949998
    2.10832691]
 ...
 [-57.08636381   0.21210001   0.35030004   0.1265       0.4355
    2.27340007]
 [ 24.3141795    0.1245       0.69930005   0.16680001   0.62239999
    2.22982764]
 [ -3.27994379   0.86779994   0.89010012   0.80000001   0.88690007
    2.44470382]][0m
[37m[1m[2023-07-11 08:11:30,568][233954] Max Reward on eval: 418.0437318790704[0m
[37m[1m[2023-07-11 08:11:30,568][233954] Min Reward on eval: -746.7298731380142[0m
[37m[1m[2023-07-11 08:11:30,569][233954] Mean Reward across all agents: 81.25808228619319[0m
[37m[1m[2023-07-11 08:11:30,569][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:11:30,574][233954] mean_value=-227.496800592506, max_value=511.6457088389527[0m
[37m[1m[2023-07-11 08:11:30,577][233954] New mean coefficients: [[ 0.45469683  0.44923407 -0.33030376  0.78064734  1.0447029  -6.663957  ]][0m
[37m[1m[2023-07-11 08:11:30,578][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:11:39,501][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 08:11:39,502][233954] FPS: 430398.63[0m
[36m[2023-07-11 08:11:39,504][233954] itr=611, itrs=2000, Progress: 30.55%[0m
[36m[2023-07-11 08:11:51,217][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 08:11:51,217][233954] FPS: 330217.82[0m
[36m[2023-07-11 08:11:55,628][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:11:55,634][233954] Reward + Measures: [[157.04800298   0.0929       0.35438865   0.26433268   0.44066367
    2.00547647]][0m
[37m[1m[2023-07-11 08:11:55,634][233954] Max Reward on eval: 157.0480029825266[0m
[37m[1m[2023-07-11 08:11:55,635][233954] Min Reward on eval: 157.0480029825266[0m
[37m[1m[2023-07-11 08:11:55,635][233954] Mean Reward across all agents: 157.0480029825266[0m
[37m[1m[2023-07-11 08:11:55,635][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:12:00,919][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:12:00,920][233954] Reward + Measures: [[  41.59453597    0.10820001    0.30019999    0.1051        0.29840001
     1.96931541]
 [-103.0015376     0.38610002    0.76359999    0.16790001    0.60810006
     2.13261294]
 [ 196.47773841    0.1147        0.37130001    0.31470001    0.47329998
     1.85915148]
 ...
 [   8.31633399    0.68479997    0.80059999    0.66480005    0.76319999
     2.14877295]
 [ 504.31973839    0.0101        0.99060005    0.69450003    0.99000007
     2.82873297]
 [ 212.25748824    0.2854        0.52250004    0.37560001    0.63959998
     2.33953643]][0m
[37m[1m[2023-07-11 08:12:00,920][233954] Max Reward on eval: 530.8300087489188[0m
[37m[1m[2023-07-11 08:12:00,920][233954] Min Reward on eval: -360.7058183366433[0m
[37m[1m[2023-07-11 08:12:00,921][233954] Mean Reward across all agents: 81.43418042066085[0m
[37m[1m[2023-07-11 08:12:00,921][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:12:00,925][233954] mean_value=-137.81068248706063, max_value=604.8397439755917[0m
[37m[1m[2023-07-11 08:12:00,928][233954] New mean coefficients: [[ 0.90668666  0.18984303 -0.47295535  0.8799578   0.95618665 -6.658452  ]][0m
[37m[1m[2023-07-11 08:12:00,929][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:12:09,975][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 08:12:09,976][233954] FPS: 424541.00[0m
[36m[2023-07-11 08:12:09,978][233954] itr=612, itrs=2000, Progress: 30.60%[0m
[36m[2023-07-11 08:12:21,485][233954] train() took 11.42 seconds to complete[0m
[36m[2023-07-11 08:12:21,486][233954] FPS: 336133.06[0m
[36m[2023-07-11 08:12:25,686][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:12:25,686][233954] Reward + Measures: [[154.46543281   0.10023767   0.3454397    0.26705733   0.43270203
    1.96935654]][0m
[37m[1m[2023-07-11 08:12:25,687][233954] Max Reward on eval: 154.46543280688786[0m
[37m[1m[2023-07-11 08:12:25,687][233954] Min Reward on eval: 154.46543280688786[0m
[37m[1m[2023-07-11 08:12:25,687][233954] Mean Reward across all agents: 154.46543280688786[0m
[37m[1m[2023-07-11 08:12:25,688][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:12:30,621][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:12:30,622][233954] Reward + Measures: [[279.75347684   0.0319       0.6099       0.39690003   0.66040003
    2.1883564 ]
 [ 47.64769999   0.28730002   0.52590001   0.22920001   0.48130003
    2.01470947]
 [155.28157779   0.0419       0.59920001   0.419        0.64870006
    2.47007513]
 ...
 [ -5.62493159   0.37600002   0.5948       0.31819996   0.49399996
    2.4155519 ]
 [279.17089366   0.06950001   0.5194       0.32540002   0.57630002
    2.25358415]
 [536.14195658   0.1869       0.79960001   0.59939998   0.88030005
    2.71171379]][0m
[37m[1m[2023-07-11 08:12:30,622][233954] Max Reward on eval: 549.9929409035482[0m
[37m[1m[2023-07-11 08:12:30,623][233954] Min Reward on eval: -116.47818959075957[0m
[37m[1m[2023-07-11 08:12:30,623][233954] Mean Reward across all agents: 135.03540055712597[0m
[37m[1m[2023-07-11 08:12:30,623][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:12:30,627][233954] mean_value=-190.06587852707347, max_value=663.6991709090952[0m
[37m[1m[2023-07-11 08:12:30,629][233954] New mean coefficients: [[ 1.5036743  -0.25442898  0.53993213  0.79748636  0.42661262 -7.1053286 ]][0m
[37m[1m[2023-07-11 08:12:30,630][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:12:39,595][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 08:12:39,596][233954] FPS: 428406.33[0m
[36m[2023-07-11 08:12:39,598][233954] itr=613, itrs=2000, Progress: 30.65%[0m
[36m[2023-07-11 08:12:51,239][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 08:12:51,239][233954] FPS: 332236.87[0m
[36m[2023-07-11 08:12:55,570][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:12:55,576][233954] Reward + Measures: [[145.69609222   0.10207967   0.33241433   0.25929365   0.41983667
    1.91377616]][0m
[37m[1m[2023-07-11 08:12:55,576][233954] Max Reward on eval: 145.6960922201964[0m
[37m[1m[2023-07-11 08:12:55,576][233954] Min Reward on eval: 145.6960922201964[0m
[37m[1m[2023-07-11 08:12:55,577][233954] Mean Reward across all agents: 145.6960922201964[0m
[37m[1m[2023-07-11 08:12:55,577][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:13:00,553][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:13:00,554][233954] Reward + Measures: [[-30.12988854   0.31529999   0.64869994   0.264        0.66029996
    2.38538814]
 [173.29622015   0.0579       0.5431       0.3107       0.54030001
    2.23931289]
 [ 78.2740409    0.0606       0.19890001   0.1461       0.32170001
    2.27960181]
 ...
 [-51.48926397   0.23740001   0.33379999   0.1355       0.41610003
    2.7252872 ]
 [  5.49682337   0.15450001   0.38010001   0.25890002   0.4395
    2.30797625]
 [ -3.35945651   0.1357       0.2852       0.1319       0.33510002
    2.21003222]][0m
[37m[1m[2023-07-11 08:13:00,554][233954] Max Reward on eval: 345.95143932178615[0m
[37m[1m[2023-07-11 08:13:00,554][233954] Min Reward on eval: -346.75737252533435[0m
[37m[1m[2023-07-11 08:13:00,555][233954] Mean Reward across all agents: 60.118913245958524[0m
[37m[1m[2023-07-11 08:13:00,555][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:13:00,558][233954] mean_value=-180.61484228963877, max_value=698.2016699193045[0m
[37m[1m[2023-07-11 08:13:00,561][233954] New mean coefficients: [[ 1.6159952  -0.19548234  0.46057177  0.9941015   0.41942492 -6.8171835 ]][0m
[37m[1m[2023-07-11 08:13:00,562][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:13:09,602][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 08:13:09,602][233954] FPS: 424846.59[0m
[36m[2023-07-11 08:13:09,605][233954] itr=614, itrs=2000, Progress: 30.70%[0m
[36m[2023-07-11 08:13:21,509][233954] train() took 11.82 seconds to complete[0m
[36m[2023-07-11 08:13:21,509][233954] FPS: 324951.86[0m
[36m[2023-07-11 08:13:25,843][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:13:25,843][233954] Reward + Measures: [[147.7234934    0.097445     0.34157202   0.26459602   0.4268133
    1.89172649]][0m
[37m[1m[2023-07-11 08:13:25,843][233954] Max Reward on eval: 147.72349340093757[0m
[37m[1m[2023-07-11 08:13:25,844][233954] Min Reward on eval: 147.72349340093757[0m
[37m[1m[2023-07-11 08:13:25,844][233954] Mean Reward across all agents: 147.72349340093757[0m
[37m[1m[2023-07-11 08:13:25,844][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:13:30,849][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:13:30,851][233954] Reward + Measures: [[  4.76155687   0.86739999   0.87120008   0.81160003   0.83960003
    2.45907521]
 [116.5311746    0.1142       0.53620005   0.31940001   0.54329997
    2.59273839]
 [289.83738274   0.0804       0.63370001   0.46800002   0.61140001
    2.41160131]
 ...
 [ 64.40152807   0.1051       0.3495       0.16059999   0.31570002
    2.13960791]
 [  2.7214023    0.77969998   0.78870004   0.70710003   0.73690003
    2.59850287]
 [300.6330924    0.0387       0.63310003   0.43880001   0.61879998
    2.64847064]][0m
[37m[1m[2023-07-11 08:13:30,852][233954] Max Reward on eval: 548.7475051578134[0m
[37m[1m[2023-07-11 08:13:30,852][233954] Min Reward on eval: -537.0585182804614[0m
[37m[1m[2023-07-11 08:13:30,852][233954] Mean Reward across all agents: 100.99880708473603[0m
[37m[1m[2023-07-11 08:13:30,852][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:13:30,856][233954] mean_value=-129.56945512592839, max_value=705.9547416558751[0m
[37m[1m[2023-07-11 08:13:30,859][233954] New mean coefficients: [[ 1.594211   -0.64076424  0.10582697  1.0771791   0.05673069 -7.2168245 ]][0m
[37m[1m[2023-07-11 08:13:30,860][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:13:39,914][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 08:13:39,914][233954] FPS: 424188.32[0m
[36m[2023-07-11 08:13:39,916][233954] itr=615, itrs=2000, Progress: 30.75%[0m
[36m[2023-07-11 08:13:51,476][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 08:13:51,476][233954] FPS: 334696.35[0m
[36m[2023-07-11 08:13:55,715][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:13:55,715][233954] Reward + Measures: [[143.16965951   0.09855168   0.31170833   0.24882768   0.39429268
    1.85435843]][0m
[37m[1m[2023-07-11 08:13:55,716][233954] Max Reward on eval: 143.16965950690866[0m
[37m[1m[2023-07-11 08:13:55,716][233954] Min Reward on eval: 143.16965950690866[0m
[37m[1m[2023-07-11 08:13:55,716][233954] Mean Reward across all agents: 143.16965950690866[0m
[37m[1m[2023-07-11 08:13:55,716][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:14:00,689][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:14:00,695][233954] Reward + Measures: [[  1.08027796   0.63520002   0.83640003   0.58240002   0.75950003
    2.75341702]
 [140.39532543   0.2314       0.72429997   0.2034       0.63380003
    2.17106605]
 [149.29693315   0.0904       0.82679999   0.42560002   0.76450008
    2.47389007]
 ...
 [ 57.96555966   0.07480001   0.25800002   0.1313       0.29010001
    2.14251661]
 [ -6.44117237   0.08810001   0.09249999   0.10320001   0.11610001
    2.50779033]
 [ 39.25359461   0.15500002   0.2122       0.14940001   0.2323
    2.41206622]][0m
[37m[1m[2023-07-11 08:14:00,695][233954] Max Reward on eval: 588.0256819835398[0m
[37m[1m[2023-07-11 08:14:00,695][233954] Min Reward on eval: -453.12314389290987[0m
[37m[1m[2023-07-11 08:14:00,696][233954] Mean Reward across all agents: 78.84007608338378[0m
[37m[1m[2023-07-11 08:14:00,696][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:14:00,699][233954] mean_value=-199.8310213764153, max_value=610.2978366301229[0m
[37m[1m[2023-07-11 08:14:00,701][233954] New mean coefficients: [[ 0.7907438  -1.2086703  -0.09985477  1.2906191  -0.918177   -8.596494  ]][0m
[37m[1m[2023-07-11 08:14:00,702][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:14:09,637][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 08:14:09,637][233954] FPS: 429862.25[0m
[36m[2023-07-11 08:14:09,640][233954] itr=616, itrs=2000, Progress: 30.80%[0m
[36m[2023-07-11 08:14:21,282][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 08:14:21,282][233954] FPS: 332241.30[0m
[36m[2023-07-11 08:14:25,580][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:14:25,586][233954] Reward + Measures: [[132.99397218   0.10312066   0.29343933   0.24162166   0.38328099
    1.80939615]][0m
[37m[1m[2023-07-11 08:14:25,586][233954] Max Reward on eval: 132.99397218222862[0m
[37m[1m[2023-07-11 08:14:25,586][233954] Min Reward on eval: 132.99397218222862[0m
[37m[1m[2023-07-11 08:14:25,586][233954] Mean Reward across all agents: 132.99397218222862[0m
[37m[1m[2023-07-11 08:14:25,586][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:14:30,570][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:14:30,571][233954] Reward + Measures: [[ -70.54843046    0.35170001    0.97300005    0.3283        0.94480002
     3.01043773]
 [ -65.92198695    0.22130001    0.43790004    0.13460001    0.33269998
     2.14904594]
 [ 119.61719665    0.1279        0.07120001    0.10320001    0.15750001
     2.60031199]
 ...
 [  12.44385669    0.26050001    0.84490007    0.3105        0.72530001
     2.17023349]
 [-290.56622314    0.54460001    0.9806        0.28290001    0.97139996
     3.13582587]
 [  91.90269049    0.1026        0.40570003    0.18699999    0.36719999
     2.36829305]][0m
[37m[1m[2023-07-11 08:14:30,571][233954] Max Reward on eval: 606.6506271425751[0m
[37m[1m[2023-07-11 08:14:30,571][233954] Min Reward on eval: -663.1105789996684[0m
[37m[1m[2023-07-11 08:14:30,571][233954] Mean Reward across all agents: 51.60873828390036[0m
[37m[1m[2023-07-11 08:14:30,572][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:14:30,575][233954] mean_value=-241.65453257493044, max_value=522.8972408082896[0m
[37m[1m[2023-07-11 08:14:30,577][233954] New mean coefficients: [[ 0.532552   -0.59636194  0.16885853  1.243201    0.1287986  -7.5717483 ]][0m
[37m[1m[2023-07-11 08:14:30,578][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:14:39,585][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 08:14:39,585][233954] FPS: 426416.96[0m
[36m[2023-07-11 08:14:39,588][233954] itr=617, itrs=2000, Progress: 30.85%[0m
[36m[2023-07-11 08:14:51,172][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 08:14:51,172][233954] FPS: 333997.42[0m
[36m[2023-07-11 08:14:55,455][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:14:55,456][233954] Reward + Measures: [[124.70617955   0.09750799   0.27425832   0.23031034   0.36377966
    1.73859406]][0m
[37m[1m[2023-07-11 08:14:55,456][233954] Max Reward on eval: 124.706179553172[0m
[37m[1m[2023-07-11 08:14:55,456][233954] Min Reward on eval: 124.706179553172[0m
[37m[1m[2023-07-11 08:14:55,456][233954] Mean Reward across all agents: 124.706179553172[0m
[37m[1m[2023-07-11 08:14:55,457][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:15:00,644][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:15:00,645][233954] Reward + Measures: [[144.34926461   0.1846       0.31100002   0.2278       0.42640001
    2.15677142]
 [-30.07232675   0.65720004   0.5539       0.66600007   0.15290001
    2.52125335]
 [ 66.54310252   0.65370005   0.80400002   0.65979999   0.58770001
    2.50433135]
 ...
 [282.48402666   0.0503       0.70590001   0.33629999   0.70410007
    2.17523932]
 [-54.09167077   0.58000004   0.5959       0.60140008   0.25209999
    2.46438646]
 [247.91408802   0.1618       0.5273       0.39160001   0.56280005
    2.37265635]][0m
[37m[1m[2023-07-11 08:15:00,645][233954] Max Reward on eval: 690.3566283991561[0m
[37m[1m[2023-07-11 08:15:00,645][233954] Min Reward on eval: -343.89579115873204[0m
[37m[1m[2023-07-11 08:15:00,646][233954] Mean Reward across all agents: 81.03960393596327[0m
[37m[1m[2023-07-11 08:15:00,646][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:15:00,651][233954] mean_value=-50.76394351398893, max_value=681.6067708001854[0m
[37m[1m[2023-07-11 08:15:00,654][233954] New mean coefficients: [[ 0.310703   -1.3110406   0.41996247  0.5797332  -0.58773255 -8.307466  ]][0m
[37m[1m[2023-07-11 08:15:00,655][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:15:09,569][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 08:15:09,570][233954] FPS: 430842.72[0m
[36m[2023-07-11 08:15:09,572][233954] itr=618, itrs=2000, Progress: 30.90%[0m
[36m[2023-07-11 08:15:21,176][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 08:15:21,176][233954] FPS: 333302.42[0m
[36m[2023-07-11 08:15:25,367][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:15:25,367][233954] Reward + Measures: [[108.97358739   0.09631033   0.26598933   0.21557499   0.34191966
    1.68874645]][0m
[37m[1m[2023-07-11 08:15:25,367][233954] Max Reward on eval: 108.97358738914613[0m
[37m[1m[2023-07-11 08:15:25,367][233954] Min Reward on eval: 108.97358738914613[0m
[37m[1m[2023-07-11 08:15:25,368][233954] Mean Reward across all agents: 108.97358738914613[0m
[37m[1m[2023-07-11 08:15:25,368][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:15:30,351][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:15:30,352][233954] Reward + Measures: [[187.47552939   0.0432       0.34650001   0.25079998   0.40570003
    2.01872206]
 [218.99346872   0.16110002   0.3863       0.36890003   0.46100003
    2.64472246]
 [337.61959961   0.0937       0.80060005   0.44569999   0.7683
    2.40323949]
 ...
 [240.4883413    0.0822       0.74150002   0.51630002   0.6965
    2.5533452 ]
 [100.05879586   0.08460001   0.2559       0.1881       0.26340002
    2.36687875]
 [ 68.75456588   0.06770001   0.14060001   0.0898       0.1301
    2.47185612]][0m
[37m[1m[2023-07-11 08:15:30,352][233954] Max Reward on eval: 578.1421575393528[0m
[37m[1m[2023-07-11 08:15:30,352][233954] Min Reward on eval: -76.76649189912715[0m
[37m[1m[2023-07-11 08:15:30,353][233954] Mean Reward across all agents: 131.15603896035375[0m
[37m[1m[2023-07-11 08:15:30,353][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:15:30,356][233954] mean_value=-263.223926086542, max_value=752.6608286681585[0m
[37m[1m[2023-07-11 08:15:30,359][233954] New mean coefficients: [[ 0.5381273  -0.8893353   0.8226029  -0.26480597 -0.13820276 -7.7467113 ]][0m
[37m[1m[2023-07-11 08:15:30,360][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:15:39,413][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 08:15:39,414][233954] FPS: 424224.01[0m
[36m[2023-07-11 08:15:39,416][233954] itr=619, itrs=2000, Progress: 30.95%[0m
[36m[2023-07-11 08:15:51,270][233954] train() took 11.77 seconds to complete[0m
[36m[2023-07-11 08:15:51,270][233954] FPS: 326298.61[0m
[36m[2023-07-11 08:15:55,513][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:15:55,513][233954] Reward + Measures: [[113.60325659   0.10136633   0.25096032   0.21134165   0.32926765
    1.62404323]][0m
[37m[1m[2023-07-11 08:15:55,513][233954] Max Reward on eval: 113.60325659182563[0m
[37m[1m[2023-07-11 08:15:55,514][233954] Min Reward on eval: 113.60325659182563[0m
[37m[1m[2023-07-11 08:15:55,514][233954] Mean Reward across all agents: 113.60325659182563[0m
[37m[1m[2023-07-11 08:15:55,514][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:16:00,503][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:16:00,504][233954] Reward + Measures: [[ 77.17839475   0.2138       0.38570002   0.25170001   0.41019997
    1.97716463]
 [319.09709427   0.0675       0.84679997   0.52340001   0.82720006
    2.14713907]
 [  8.67103669   0.28299999   0.79300004   0.2854       0.66339999
    2.21230268]
 ...
 [351.84085084   0.0577       0.97229999   0.4244       0.97560006
    2.3960073 ]
 [  3.8549752    0.3748       0.45859995   0.2129       0.58649999
    2.42131305]
 [261.47328126   0.05190001   0.59810001   0.368        0.58750004
    1.97059143]][0m
[37m[1m[2023-07-11 08:16:00,504][233954] Max Reward on eval: 503.65454050268744[0m
[37m[1m[2023-07-11 08:16:00,504][233954] Min Reward on eval: -386.32779016280546[0m
[37m[1m[2023-07-11 08:16:00,505][233954] Mean Reward across all agents: 49.84754650249137[0m
[37m[1m[2023-07-11 08:16:00,505][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:16:00,509][233954] mean_value=-210.96878846128203, max_value=494.6786993063311[0m
[37m[1m[2023-07-11 08:16:00,511][233954] New mean coefficients: [[ 0.76843524  0.04035187  0.6797376  -0.40575004  0.7409233  -6.13418   ]][0m
[37m[1m[2023-07-11 08:16:00,512][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:16:09,523][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 08:16:09,523][233954] FPS: 426257.76[0m
[36m[2023-07-11 08:16:09,525][233954] itr=620, itrs=2000, Progress: 31.00%[0m
[37m[1m[2023-07-11 08:19:28,608][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000600[0m
[36m[2023-07-11 08:19:40,669][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 08:19:40,670][233954] FPS: 333384.76[0m
[36m[2023-07-11 08:19:44,917][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:19:44,917][233954] Reward + Measures: [[107.23829818   0.10237634   0.25319567   0.20726965   0.32599834
    1.59764254]][0m
[37m[1m[2023-07-11 08:19:44,918][233954] Max Reward on eval: 107.23829817762734[0m
[37m[1m[2023-07-11 08:19:44,918][233954] Min Reward on eval: 107.23829817762734[0m
[37m[1m[2023-07-11 08:19:44,918][233954] Mean Reward across all agents: 107.23829817762734[0m
[37m[1m[2023-07-11 08:19:44,918][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:19:49,820][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:19:49,821][233954] Reward + Measures: [[ 64.95713029   0.12980001   0.2902       0.2033       0.30039999
    2.39272165]
 [ 69.50259627   0.1442       0.25209999   0.19660001   0.2744
    1.92008913]
 [135.8969719    0.2066       0.87550002   0.1893       0.9169001
    2.53195739]
 ...
 [138.43366957   0.0437       0.35429999   0.26719999   0.37890002
    1.75500453]
 [159.37736734   0.0417       0.2969       0.21339999   0.33570001
    1.8948822 ]
 [ 73.44628242   0.1054       0.3511       0.13120002   0.41370001
    1.98036313]][0m
[37m[1m[2023-07-11 08:19:49,821][233954] Max Reward on eval: 757.4825706363656[0m
[37m[1m[2023-07-11 08:19:49,821][233954] Min Reward on eval: -194.93025720426812[0m
[37m[1m[2023-07-11 08:19:49,821][233954] Mean Reward across all agents: 182.82839490788515[0m
[37m[1m[2023-07-11 08:19:49,822][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:19:49,827][233954] mean_value=-115.9063972330862, max_value=711.8166509618052[0m
[37m[1m[2023-07-11 08:19:49,830][233954] New mean coefficients: [[ 0.59266275  0.22687358  0.3510975  -0.3927264   0.9931514  -5.7956033 ]][0m
[37m[1m[2023-07-11 08:19:49,831][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:19:58,734][233954] train() took 8.90 seconds to complete[0m
[36m[2023-07-11 08:19:58,734][233954] FPS: 431392.05[0m
[36m[2023-07-11 08:19:58,736][233954] itr=621, itrs=2000, Progress: 31.05%[0m
[36m[2023-07-11 08:20:10,377][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 08:20:10,377][233954] FPS: 332366.10[0m
[36m[2023-07-11 08:20:14,636][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:20:14,637][233954] Reward + Measures: [[94.2207907   0.10117666  0.231436    0.19198035  0.30065432  1.51011586]][0m
[37m[1m[2023-07-11 08:20:14,637][233954] Max Reward on eval: 94.22079070378577[0m
[37m[1m[2023-07-11 08:20:14,637][233954] Min Reward on eval: 94.22079070378577[0m
[37m[1m[2023-07-11 08:20:14,638][233954] Mean Reward across all agents: 94.22079070378577[0m
[37m[1m[2023-07-11 08:20:14,638][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:20:19,611][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:20:19,612][233954] Reward + Measures: [[ 20.19145469   0.16400002   0.39020002   0.14170001   0.35999998
    1.83379781]
 [175.90289921   0.1567       0.58059996   0.41680002   0.57499999
    3.41644096]
 [114.28288621   0.23360002   0.16060001   0.22230001   0.25110003
    2.91317797]
 ...
 [-75.79986603   0.27740002   0.39250001   0.18809998   0.3448
    2.25296807]
 [172.57775008   0.17         0.25920001   0.2613       0.33160001
    2.5010345 ]
 [ 62.09958219   0.08000001   0.8023001    0.29449999   0.77849996
    2.89729929]][0m
[37m[1m[2023-07-11 08:20:19,612][233954] Max Reward on eval: 517.7336958546191[0m
[37m[1m[2023-07-11 08:20:19,612][233954] Min Reward on eval: -153.9203891633777[0m
[37m[1m[2023-07-11 08:20:19,613][233954] Mean Reward across all agents: 112.61990336858157[0m
[37m[1m[2023-07-11 08:20:19,613][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:20:19,617][233954] mean_value=-193.73299980321298, max_value=684.3569157991093[0m
[37m[1m[2023-07-11 08:20:19,620][233954] New mean coefficients: [[ 0.16481894  0.18847902  0.1329461  -0.39334565  0.70279115 -6.148355  ]][0m
[37m[1m[2023-07-11 08:20:19,620][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:20:28,553][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 08:20:28,554][233954] FPS: 429954.64[0m
[36m[2023-07-11 08:20:28,556][233954] itr=622, itrs=2000, Progress: 31.10%[0m
[36m[2023-07-11 08:20:40,315][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 08:20:40,315][233954] FPS: 328862.87[0m
[36m[2023-07-11 08:20:44,606][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:20:44,607][233954] Reward + Measures: [[87.39630867  0.10052466  0.23417699  0.189439    0.29984599  1.50716043]][0m
[37m[1m[2023-07-11 08:20:44,607][233954] Max Reward on eval: 87.39630867419784[0m
[37m[1m[2023-07-11 08:20:44,607][233954] Min Reward on eval: 87.39630867419784[0m
[37m[1m[2023-07-11 08:20:44,608][233954] Mean Reward across all agents: 87.39630867419784[0m
[37m[1m[2023-07-11 08:20:44,608][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:20:49,561][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:20:49,561][233954] Reward + Measures: [[ 72.87611039   0.69210005   0.84200001   0.65040004   0.78120005
    2.33820701]
 [ 48.18630041   0.95979995   0.9684       0.94239998   0.95069999
    2.69509101]
 [ 52.04987027   0.0478       0.35450003   0.28169999   0.50419998
    2.56969762]
 ...
 [466.67568493   0.0292       0.88959998   0.60100001   0.86590004
    2.73123026]
 [ 24.49794448   0.23530002   0.60440004   0.1384       0.50150001
    2.42711616]
 [152.27323949   0.14680001   0.68840003   0.44829997   0.63310003
    2.25400519]][0m
[37m[1m[2023-07-11 08:20:49,562][233954] Max Reward on eval: 582.6196385417134[0m
[37m[1m[2023-07-11 08:20:49,562][233954] Min Reward on eval: -548.4911270206794[0m
[37m[1m[2023-07-11 08:20:49,562][233954] Mean Reward across all agents: 98.2211298353174[0m
[37m[1m[2023-07-11 08:20:49,562][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:20:49,566][233954] mean_value=-186.99207415056088, max_value=417.43670308068397[0m
[37m[1m[2023-07-11 08:20:49,568][233954] New mean coefficients: [[-0.17512378  0.12079461 -0.13057539 -0.5028004   0.64484006 -6.210085  ]][0m
[37m[1m[2023-07-11 08:20:49,569][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:20:58,515][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 08:20:58,515][233954] FPS: 429326.17[0m
[36m[2023-07-11 08:20:58,518][233954] itr=623, itrs=2000, Progress: 31.15%[0m
[36m[2023-07-11 08:21:10,126][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 08:21:10,126][233954] FPS: 333284.63[0m
[36m[2023-07-11 08:21:14,344][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:21:14,344][233954] Reward + Measures: [[87.24238325  0.11006666  0.22020601  0.18866034  0.30003032  1.44856703]][0m
[37m[1m[2023-07-11 08:21:14,345][233954] Max Reward on eval: 87.24238325082965[0m
[37m[1m[2023-07-11 08:21:14,345][233954] Min Reward on eval: 87.24238325082965[0m
[37m[1m[2023-07-11 08:21:14,345][233954] Mean Reward across all agents: 87.24238325082965[0m
[37m[1m[2023-07-11 08:21:14,345][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:21:19,340][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:21:19,340][233954] Reward + Measures: [[-44.49761807   0.90059996   0.90399998   0.88230002   0.89920008
    2.93746257]
 [  9.85222183   0.92810005   0.93940002   0.89659995   0.92089999
    3.08720756]
 [ 31.98453298   0.91610003   0.92210001   0.89569998   0.91650003
    2.75910687]
 ...
 [ 49.31943854   0.81409997   0.8804       0.78130001   0.85299999
    2.43688846]
 [ 37.10002757   0.1787       0.57859999   0.18700001   0.4804
    2.04890585]
 [474.8248081    0.45520002   0.83920002   0.2906       0.81840003
    2.6874938 ]][0m
[37m[1m[2023-07-11 08:21:19,340][233954] Max Reward on eval: 600.7496986524202[0m
[37m[1m[2023-07-11 08:21:19,341][233954] Min Reward on eval: -253.46070527462288[0m
[37m[1m[2023-07-11 08:21:19,341][233954] Mean Reward across all agents: 165.14792995881163[0m
[37m[1m[2023-07-11 08:21:19,341][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:21:19,346][233954] mean_value=-141.62629658963974, max_value=538.5018572257848[0m
[37m[1m[2023-07-11 08:21:19,349][233954] New mean coefficients: [[ 0.19686839  0.24515331 -0.32103267 -0.03511757  0.79939914 -5.525775  ]][0m
[37m[1m[2023-07-11 08:21:19,350][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:21:28,392][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 08:21:28,392][233954] FPS: 424757.33[0m
[36m[2023-07-11 08:21:28,395][233954] itr=624, itrs=2000, Progress: 31.20%[0m
[36m[2023-07-11 08:21:40,131][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 08:21:40,131][233954] FPS: 329563.37[0m
[36m[2023-07-11 08:21:44,484][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:21:44,485][233954] Reward + Measures: [[83.59911324  0.10836866  0.21662167  0.18482767  0.29183668  1.43938088]][0m
[37m[1m[2023-07-11 08:21:44,485][233954] Max Reward on eval: 83.59911323811366[0m
[37m[1m[2023-07-11 08:21:44,485][233954] Min Reward on eval: 83.59911323811366[0m
[37m[1m[2023-07-11 08:21:44,485][233954] Mean Reward across all agents: 83.59911323811366[0m
[37m[1m[2023-07-11 08:21:44,486][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:21:49,771][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:21:49,772][233954] Reward + Measures: [[  76.32556653    0.15890001    0.45770001    0.17419998    0.4197
     2.18279052]
 [-108.26409435    0.43199998    0.65210003    0.39250001    0.58219999
     2.9221487 ]
 [ 189.69977091    0.0096        0.99410003    0.54350001    0.99589998
     3.81740069]
 ...
 [ 167.46281659    0.14250001    0.53500003    0.19200002    0.54290003
     2.73993802]
 [ 211.54195754    0.1638        0.39409998    0.2177        0.41030002
     2.34369659]
 [  97.42060997    0.009         0.99400008    0.52750003    0.99140006
     3.74522758]][0m
[37m[1m[2023-07-11 08:21:49,772][233954] Max Reward on eval: 672.4289016887545[0m
[37m[1m[2023-07-11 08:21:49,772][233954] Min Reward on eval: -278.5114955869387[0m
[37m[1m[2023-07-11 08:21:49,772][233954] Mean Reward across all agents: 132.46149260558806[0m
[37m[1m[2023-07-11 08:21:49,773][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:21:49,777][233954] mean_value=-182.11089219572148, max_value=382.41729032154797[0m
[37m[1m[2023-07-11 08:21:49,779][233954] New mean coefficients: [[-0.14917007 -0.9615183  -0.6215551   0.47932488 -0.4476204  -7.561428  ]][0m
[37m[1m[2023-07-11 08:21:49,780][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:21:58,879][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 08:21:58,880][233954] FPS: 422105.52[0m
[36m[2023-07-11 08:21:58,882][233954] itr=625, itrs=2000, Progress: 31.25%[0m
[36m[2023-07-11 08:22:10,419][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 08:22:10,420][233954] FPS: 335275.35[0m
[36m[2023-07-11 08:22:14,671][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:22:14,671][233954] Reward + Measures: [[72.6942469   0.11520766  0.20948832  0.17697634  0.28207201  1.40453172]][0m
[37m[1m[2023-07-11 08:22:14,672][233954] Max Reward on eval: 72.69424689639716[0m
[37m[1m[2023-07-11 08:22:14,672][233954] Min Reward on eval: 72.69424689639716[0m
[37m[1m[2023-07-11 08:22:14,672][233954] Mean Reward across all agents: 72.69424689639716[0m
[37m[1m[2023-07-11 08:22:14,672][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:22:19,602][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:22:19,603][233954] Reward + Measures: [[375.08689509   0.0464       0.69250005   0.50980002   0.70279998
    2.94231462]
 [-36.43579152   0.0641       0.1224       0.0866       0.15110001
    2.231534  ]
 [157.19269603   0.0619       0.48740003   0.27379999   0.47680002
    2.80321407]
 ...
 [ 14.53553494   0.2242       0.46040002   0.19919999   0.44679999
    2.35648441]
 [-13.99389853   0.0559       0.0734       0.0806       0.1473
    2.29988909]
 [ 70.50876132   0.1399       0.3752       0.191        0.31430003
    2.22453356]][0m
[37m[1m[2023-07-11 08:22:19,603][233954] Max Reward on eval: 496.8043137481436[0m
[37m[1m[2023-07-11 08:22:19,603][233954] Min Reward on eval: -88.73690696591511[0m
[37m[1m[2023-07-11 08:22:19,603][233954] Mean Reward across all agents: 59.79352555732345[0m
[37m[1m[2023-07-11 08:22:19,604][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:22:19,606][233954] mean_value=-333.38824294782256, max_value=323.12312923305046[0m
[37m[1m[2023-07-11 08:22:19,609][233954] New mean coefficients: [[-0.01805887 -1.1007895  -0.23053724  0.74116504 -0.77101636 -7.6431637 ]][0m
[37m[1m[2023-07-11 08:22:19,610][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:22:28,637][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 08:22:28,638][233954] FPS: 425428.84[0m
[36m[2023-07-11 08:22:28,640][233954] itr=626, itrs=2000, Progress: 31.30%[0m
[36m[2023-07-11 08:22:40,415][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 08:22:40,415][233954] FPS: 328496.76[0m
[36m[2023-07-11 08:22:44,741][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:22:44,742][233954] Reward + Measures: [[72.48125991  0.10746533  0.19722666  0.16945733  0.26580533  1.38901412]][0m
[37m[1m[2023-07-11 08:22:44,742][233954] Max Reward on eval: 72.48125991202453[0m
[37m[1m[2023-07-11 08:22:44,742][233954] Min Reward on eval: 72.48125991202453[0m
[37m[1m[2023-07-11 08:22:44,743][233954] Mean Reward across all agents: 72.48125991202453[0m
[37m[1m[2023-07-11 08:22:44,743][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:22:49,668][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:22:49,669][233954] Reward + Measures: [[ 56.2549297    0.15090001   0.12360001   0.13740002   0.24509998
    1.63037574]
 [  4.35892918   0.4657       0.8387       0.51540005   0.87859994
    2.37408566]
 [ 29.23936154   0.14930001   0.39000002   0.15449999   0.28779998
    1.72766006]
 ...
 [ 37.93932564   0.1646       0.26880002   0.16690001   0.23920003
    2.4640553 ]
 [ 76.82126403   0.0524       0.91689998   0.26300001   0.82870001
    2.8192265 ]
 [120.7372775    0.0571       0.94430012   0.4659       0.91530001
    2.62608504]][0m
[37m[1m[2023-07-11 08:22:49,669][233954] Max Reward on eval: 690.7867965706624[0m
[37m[1m[2023-07-11 08:22:49,669][233954] Min Reward on eval: -97.32534903977066[0m
[37m[1m[2023-07-11 08:22:49,670][233954] Mean Reward across all agents: 162.34199085147554[0m
[37m[1m[2023-07-11 08:22:49,670][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:22:49,674][233954] mean_value=-229.83023199563928, max_value=767.6116871461868[0m
[37m[1m[2023-07-11 08:22:49,677][233954] New mean coefficients: [[ 0.16045384 -0.79836744 -0.10571466  0.2761348  -0.58735305 -6.769797  ]][0m
[37m[1m[2023-07-11 08:22:49,678][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:22:58,647][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 08:22:58,647][233954] FPS: 428228.64[0m
[36m[2023-07-11 08:22:58,649][233954] itr=627, itrs=2000, Progress: 31.35%[0m
[36m[2023-07-11 08:23:10,168][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 08:23:10,168][233954] FPS: 335807.31[0m
[36m[2023-07-11 08:23:14,471][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:23:14,472][233954] Reward + Measures: [[65.04291428  0.118466    0.20038033  0.168521    0.26854298  1.35556471]][0m
[37m[1m[2023-07-11 08:23:14,472][233954] Max Reward on eval: 65.04291427854902[0m
[37m[1m[2023-07-11 08:23:14,472][233954] Min Reward on eval: 65.04291427854902[0m
[37m[1m[2023-07-11 08:23:14,472][233954] Mean Reward across all agents: 65.04291427854902[0m
[37m[1m[2023-07-11 08:23:14,473][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:23:19,440][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:23:19,441][233954] Reward + Measures: [[468.77008508   0.2027       0.92129993   0.69460005   0.90179998
    2.69187951]
 [ 99.12861839   0.1802       0.14479999   0.17279999   0.2554
    2.06186962]
 [130.42122263   0.13810001   0.33130002   0.14050001   0.32909998
    1.78929508]
 ...
 [114.78289916   0.175        0.49720001   0.1707       0.45190001
    2.27034163]
 [-57.38741794   0.0627       0.07590001   0.0733       0.096
    2.16986036]
 [ 83.54887944   0.0689       0.2314       0.16250001   0.26370001
    1.94432449]][0m
[37m[1m[2023-07-11 08:23:19,441][233954] Max Reward on eval: 768.3639678951353[0m
[37m[1m[2023-07-11 08:23:19,441][233954] Min Reward on eval: -239.83262216709554[0m
[37m[1m[2023-07-11 08:23:19,441][233954] Mean Reward across all agents: 139.557444429188[0m
[37m[1m[2023-07-11 08:23:19,442][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:23:19,445][233954] mean_value=-230.5778142311679, max_value=341.52731875757877[0m
[37m[1m[2023-07-11 08:23:19,448][233954] New mean coefficients: [[-0.10967387 -0.0125075   0.0711468   0.08454163  0.25644648 -6.0925837 ]][0m
[37m[1m[2023-07-11 08:23:19,449][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:23:28,546][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 08:23:28,546][233954] FPS: 422206.97[0m
[36m[2023-07-11 08:23:28,548][233954] itr=628, itrs=2000, Progress: 31.40%[0m
[36m[2023-07-11 08:23:40,121][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 08:23:40,122][233954] FPS: 334333.84[0m
[36m[2023-07-11 08:23:44,382][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:23:44,383][233954] Reward + Measures: [[63.32214829  0.12223867  0.18961999  0.16021833  0.25317135  1.29204655]][0m
[37m[1m[2023-07-11 08:23:44,383][233954] Max Reward on eval: 63.322148286477315[0m
[37m[1m[2023-07-11 08:23:44,383][233954] Min Reward on eval: 63.322148286477315[0m
[37m[1m[2023-07-11 08:23:44,384][233954] Mean Reward across all agents: 63.322148286477315[0m
[37m[1m[2023-07-11 08:23:44,384][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:23:49,392][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:23:49,392][233954] Reward + Measures: [[  14.031747      0.2789        0.38699999    0.27190003    0.37180001
     1.8988055 ]
 [  49.03205786    0.0909        0.28490001    0.1838        0.30510002
     1.82968676]
 [  16.97583018    0.34209999    0.89650005    0.26930001    0.7863
     2.10837436]
 ...
 [-238.65116121    0.49189997    0.68090004    0.0949        0.65290004
     2.18478942]
 [ 107.05431061    0.64640003    0.20350002    0.64420003    0.25570002
     2.95093012]
 [ 104.60708692    0.0974        0.66079998    0.28260002    0.6426
     2.07628489]][0m
[37m[1m[2023-07-11 08:23:49,393][233954] Max Reward on eval: 814.0245971793308[0m
[37m[1m[2023-07-11 08:23:49,393][233954] Min Reward on eval: -238.65116120772436[0m
[37m[1m[2023-07-11 08:23:49,393][233954] Mean Reward across all agents: 169.2031929776292[0m
[37m[1m[2023-07-11 08:23:49,393][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:23:49,401][233954] mean_value=-104.03176914805948, max_value=778.5879926531954[0m
[37m[1m[2023-07-11 08:23:49,404][233954] New mean coefficients: [[ 0.1663105   0.11596173 -0.14283466 -0.00404099  0.08181381 -5.656693  ]][0m
[37m[1m[2023-07-11 08:23:49,405][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:23:58,424][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 08:23:58,424][233954] FPS: 425829.60[0m
[36m[2023-07-11 08:23:58,426][233954] itr=629, itrs=2000, Progress: 31.45%[0m
[36m[2023-07-11 08:24:10,030][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 08:24:10,030][233954] FPS: 333354.18[0m
[36m[2023-07-11 08:24:14,323][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:24:14,323][233954] Reward + Measures: [[58.60832608  0.11493166  0.18898034  0.15768167  0.24682735  1.26627088]][0m
[37m[1m[2023-07-11 08:24:14,323][233954] Max Reward on eval: 58.60832607618378[0m
[37m[1m[2023-07-11 08:24:14,324][233954] Min Reward on eval: 58.60832607618378[0m
[37m[1m[2023-07-11 08:24:14,324][233954] Mean Reward across all agents: 58.60832607618378[0m
[37m[1m[2023-07-11 08:24:14,324][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:24:19,331][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:24:19,332][233954] Reward + Measures: [[ -37.20229365    0.39130002    0.29320002    0.34039998    0.3502
     2.59880233]
 [  31.43429564    0.18970001    0.46409997    0.1577        0.36760002
     2.47265077]
 [ 287.77167663    0.24519999    0.73989999    0.24380003    0.70769995
     2.22560334]
 ...
 [ -45.39211751    0.66510004    0.80789995    0.66009998    0.80450004
     2.31430364]
 [ 230.75759905    0.26409999    0.55949998    0.12550001    0.56480002
     2.43536878]
 [-324.35491145    0.50630003    0.67130005    0.0936        0.65370005
     2.99481559]][0m
[37m[1m[2023-07-11 08:24:19,332][233954] Max Reward on eval: 831.7143096810207[0m
[37m[1m[2023-07-11 08:24:19,332][233954] Min Reward on eval: -324.3549114499241[0m
[37m[1m[2023-07-11 08:24:19,333][233954] Mean Reward across all agents: 64.7846614500296[0m
[37m[1m[2023-07-11 08:24:19,333][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:24:19,339][233954] mean_value=-133.77356139595446, max_value=727.3387825662653[0m
[37m[1m[2023-07-11 08:24:19,341][233954] New mean coefficients: [[ 0.768125    1.0573933   0.02127378  0.34487763  1.3346373  -4.169201  ]][0m
[37m[1m[2023-07-11 08:24:19,342][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:24:28,441][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 08:24:28,441][233954] FPS: 422136.48[0m
[36m[2023-07-11 08:24:28,443][233954] itr=630, itrs=2000, Progress: 31.50%[0m
[37m[1m[2023-07-11 08:27:51,465][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000610[0m
[36m[2023-07-11 08:28:03,466][233954] train() took 11.41 seconds to complete[0m
[36m[2023-07-11 08:28:03,466][233954] FPS: 336535.90[0m
[36m[2023-07-11 08:28:07,667][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:28:07,668][233954] Reward + Measures: [[63.78455764  0.12599701  0.200341    0.16290566  0.25787133  1.20888674]][0m
[37m[1m[2023-07-11 08:28:07,668][233954] Max Reward on eval: 63.784557642260665[0m
[37m[1m[2023-07-11 08:28:07,668][233954] Min Reward on eval: 63.784557642260665[0m
[37m[1m[2023-07-11 08:28:07,668][233954] Mean Reward across all agents: 63.784557642260665[0m
[37m[1m[2023-07-11 08:28:07,669][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:28:12,625][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:28:12,625][233954] Reward + Measures: [[-102.36782647    0.31210002    0.82980007    0.25420001    0.79410005
     2.98451161]
 [  94.89913199    0.3109        0.66409999    0.35630003    0.6652
     3.17839742]
 [  67.08326047    0.33140001    0.66299999    0.30070001    0.65090007
     3.36208034]
 ...
 [ 230.90498873    0.32510003    0.5485        0.18260001    0.5474
     2.78357363]
 [ 231.15383625    0.24789999    0.63650006    0.38820001    0.67580003
     2.92983794]
 [  11.86343789    0.65210003    0.7561        0.69499999    0.3572
     2.38877892]][0m
[37m[1m[2023-07-11 08:28:12,625][233954] Max Reward on eval: 815.3032455675304[0m
[37m[1m[2023-07-11 08:28:12,626][233954] Min Reward on eval: -250.74879004952965[0m
[37m[1m[2023-07-11 08:28:12,626][233954] Mean Reward across all agents: 131.27863575190804[0m
[37m[1m[2023-07-11 08:28:12,626][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:28:12,632][233954] mean_value=-149.40554745340964, max_value=803.3544478398114[0m
[37m[1m[2023-07-11 08:28:12,635][233954] New mean coefficients: [[ 0.31675282  0.57297343 -0.15032971  0.8989204   0.45704114 -5.020689  ]][0m
[37m[1m[2023-07-11 08:28:12,635][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:28:21,631][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 08:28:21,631][233954] FPS: 426950.15[0m
[36m[2023-07-11 08:28:21,634][233954] itr=631, itrs=2000, Progress: 31.55%[0m
[36m[2023-07-11 08:28:33,189][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 08:28:33,189][233954] FPS: 334776.90[0m
[36m[2023-07-11 08:28:37,474][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:28:37,474][233954] Reward + Measures: [[62.51002059  0.145849    0.18188332  0.15336166  0.24984701  1.16898012]][0m
[37m[1m[2023-07-11 08:28:37,475][233954] Max Reward on eval: 62.510020593084086[0m
[37m[1m[2023-07-11 08:28:37,475][233954] Min Reward on eval: 62.510020593084086[0m
[37m[1m[2023-07-11 08:28:37,475][233954] Mean Reward across all agents: 62.510020593084086[0m
[37m[1m[2023-07-11 08:28:37,475][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:28:42,428][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:28:42,429][233954] Reward + Measures: [[106.09342713   0.6372       0.55379999   0.34220001   0.435
    2.7888763 ]
 [519.33207703   0.0988       0.90750009   0.52470005   0.90869999
    3.64341092]
 [ 45.92271135   0.0967       0.14750001   0.0971       0.12399999
    2.33383632]
 ...
 [ 75.56724531   0.55159998   0.7603001    0.6347       0.57609999
    2.48894382]
 [ 46.51120696   0.50839996   0.65550005   0.49449998   0.51670003
    1.92870939]
 [233.61913813   0.18810001   0.69910002   0.32099998   0.69889998
    2.75047994]][0m
[37m[1m[2023-07-11 08:28:42,429][233954] Max Reward on eval: 694.0384444983677[0m
[37m[1m[2023-07-11 08:28:42,429][233954] Min Reward on eval: -175.0329705393291[0m
[37m[1m[2023-07-11 08:28:42,429][233954] Mean Reward across all agents: 93.48651617748541[0m
[37m[1m[2023-07-11 08:28:42,430][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:28:42,437][233954] mean_value=-71.84480357822106, max_value=567.6951329495013[0m
[37m[1m[2023-07-11 08:28:42,440][233954] New mean coefficients: [[-0.5396737   1.4631705  -0.40608606  0.2277748   1.470928   -4.0671167 ]][0m
[37m[1m[2023-07-11 08:28:42,441][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:28:51,369][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 08:28:51,369][233954] FPS: 430186.08[0m
[36m[2023-07-11 08:28:51,371][233954] itr=632, itrs=2000, Progress: 31.60%[0m
[36m[2023-07-11 08:29:02,919][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 08:29:02,919][233954] FPS: 335031.78[0m
[36m[2023-07-11 08:29:07,161][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:29:07,161][233954] Reward + Measures: [[58.19001706  0.16300967  0.17623566  0.15434667  0.24427967  1.16972744]][0m
[37m[1m[2023-07-11 08:29:07,161][233954] Max Reward on eval: 58.19001705509573[0m
[37m[1m[2023-07-11 08:29:07,162][233954] Min Reward on eval: 58.19001705509573[0m
[37m[1m[2023-07-11 08:29:07,162][233954] Mean Reward across all agents: 58.19001705509573[0m
[37m[1m[2023-07-11 08:29:07,162][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:29:12,400][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:29:12,401][233954] Reward + Measures: [[200.91578454   0.20349999   0.3633       0.1937       0.3743
    3.28795099]
 [ 16.0946821    0.26300001   0.56569999   0.26610002   0.5873
    1.87373579]
 [167.4737886    0.17359999   0.35389999   0.2411       0.39970002
    2.36759114]
 ...
 [154.14002539   0.057        0.96729994   0.70970005   0.94690001
    2.85619426]
 [199.72574923   0.1446       0.31900001   0.29980001   0.39590001
    3.38863921]
 [230.1845684    0.1664       0.99399996   0.60120004   0.99370003
    2.75352502]][0m
[37m[1m[2023-07-11 08:29:12,401][233954] Max Reward on eval: 661.5158462692983[0m
[37m[1m[2023-07-11 08:29:12,402][233954] Min Reward on eval: -219.62627103703562[0m
[37m[1m[2023-07-11 08:29:12,402][233954] Mean Reward across all agents: 148.5313791042214[0m
[37m[1m[2023-07-11 08:29:12,402][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:29:12,407][233954] mean_value=-170.54906364282095, max_value=659.812674037856[0m
[37m[1m[2023-07-11 08:29:12,410][233954] New mean coefficients: [[ 0.07732832  2.0360768  -0.22098139 -0.19257641  1.9677688  -2.7366538 ]][0m
[37m[1m[2023-07-11 08:29:12,411][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:29:21,417][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 08:29:21,417][233954] FPS: 426487.89[0m
[36m[2023-07-11 08:29:21,419][233954] itr=633, itrs=2000, Progress: 31.65%[0m
[36m[2023-07-11 08:29:33,219][233954] train() took 11.72 seconds to complete[0m
[36m[2023-07-11 08:29:33,219][233954] FPS: 327784.44[0m
[36m[2023-07-11 08:29:37,510][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:29:37,515][233954] Reward + Measures: [[54.7603976   0.17898333  0.17389865  0.14385667  0.237248    1.10514033]][0m
[37m[1m[2023-07-11 08:29:37,516][233954] Max Reward on eval: 54.760397603668565[0m
[37m[1m[2023-07-11 08:29:37,516][233954] Min Reward on eval: 54.760397603668565[0m
[37m[1m[2023-07-11 08:29:37,516][233954] Mean Reward across all agents: 54.760397603668565[0m
[37m[1m[2023-07-11 08:29:37,517][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:29:42,515][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:29:42,521][233954] Reward + Measures: [[ -7.07317339   0.37339997   0.66370004   0.35290003   0.65670007
    2.61355972]
 [281.84017373   0.0031       0.99760002   0.58339995   0.99470007
    2.84111524]
 [ 99.20298933   0.0369       0.95029992   0.46669999   0.92790002
    2.92489934]
 ...
 [463.446785     0.017        0.98880005   0.60140002   0.95990002
    3.10879564]
 [ 14.15650986   0.1037       0.75260001   0.35699999   0.75019997
    3.34046412]
 [ 64.94444421   0.13450001   0.66420001   0.29449999   0.55439997
    2.35394144]][0m
[37m[1m[2023-07-11 08:29:42,522][233954] Max Reward on eval: 712.1002373550087[0m
[37m[1m[2023-07-11 08:29:42,522][233954] Min Reward on eval: -110.51873016646132[0m
[37m[1m[2023-07-11 08:29:42,522][233954] Mean Reward across all agents: 131.0642212559357[0m
[37m[1m[2023-07-11 08:29:42,522][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:29:42,525][233954] mean_value=-247.94188362521479, max_value=525.5865027647205[0m
[37m[1m[2023-07-11 08:29:42,528][233954] New mean coefficients: [[-0.8972695   1.0743682  -0.86754954  0.21943521  0.8075477  -4.376436  ]][0m
[37m[1m[2023-07-11 08:29:42,529][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:29:51,594][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 08:29:51,594][233954] FPS: 423683.01[0m
[36m[2023-07-11 08:29:51,596][233954] itr=634, itrs=2000, Progress: 31.70%[0m
[36m[2023-07-11 08:30:03,336][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 08:30:03,336][233954] FPS: 329440.77[0m
[36m[2023-07-11 08:30:07,682][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:30:07,683][233954] Reward + Measures: [[59.69354075  0.18595301  0.17307699  0.15167767  0.24084     1.12472415]][0m
[37m[1m[2023-07-11 08:30:07,683][233954] Max Reward on eval: 59.693540745821394[0m
[37m[1m[2023-07-11 08:30:07,683][233954] Min Reward on eval: 59.693540745821394[0m
[37m[1m[2023-07-11 08:30:07,683][233954] Mean Reward across all agents: 59.693540745821394[0m
[37m[1m[2023-07-11 08:30:07,684][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:30:12,676][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:30:12,676][233954] Reward + Measures: [[55.0414806   0.0527      0.67600006  0.41479999  0.6875      2.89291549]
 [89.32329322  0.2411      0.29560003  0.1662      0.3441      1.97237873]
 [53.65219558  0.21280001  0.61449999  0.23530002  0.59130001  2.5047586 ]
 ...
 [76.6100645   0.35749999  0.2242      0.33190003  0.2228      2.17119098]
 [26.84105009  0.1839      0.29319999  0.14390001  0.26550001  2.03285217]
 [25.04504814  0.671       0.86269999  0.6128      0.75960004  2.98362279]][0m
[37m[1m[2023-07-11 08:30:12,677][233954] Max Reward on eval: 486.56428619883957[0m
[37m[1m[2023-07-11 08:30:12,677][233954] Min Reward on eval: -143.55741002305876[0m
[37m[1m[2023-07-11 08:30:12,677][233954] Mean Reward across all agents: 94.94648931549224[0m
[37m[1m[2023-07-11 08:30:12,678][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:30:12,682][233954] mean_value=-170.2749649545452, max_value=465.437314394256[0m
[37m[1m[2023-07-11 08:30:12,685][233954] New mean coefficients: [[ 0.1173327   1.0513574  -0.5219858   0.42306936  0.8220543  -3.8153286 ]][0m
[37m[1m[2023-07-11 08:30:12,686][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:30:21,623][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 08:30:21,624][233954] FPS: 429740.26[0m
[36m[2023-07-11 08:30:21,626][233954] itr=635, itrs=2000, Progress: 31.75%[0m
[36m[2023-07-11 08:30:33,345][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 08:30:33,345][233954] FPS: 330032.73[0m
[36m[2023-07-11 08:30:37,717][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:30:37,722][233954] Reward + Measures: [[53.16281806  0.18468733  0.16793633  0.142286    0.22929166  1.02545416]][0m
[37m[1m[2023-07-11 08:30:37,723][233954] Max Reward on eval: 53.16281805756624[0m
[37m[1m[2023-07-11 08:30:37,723][233954] Min Reward on eval: 53.16281805756624[0m
[37m[1m[2023-07-11 08:30:37,723][233954] Mean Reward across all agents: 53.16281805756624[0m
[37m[1m[2023-07-11 08:30:37,723][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:30:42,778][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:30:42,784][233954] Reward + Measures: [[ 52.16239612   0.0732       0.11930001   0.09649999   0.12409999
    2.75314713]
 [ 12.98442566   0.25920004   0.5438       0.27330002   0.64280003
    2.6414876 ]
 [337.25851416   0.0625       0.98220009   0.67730004   0.97760004
    3.2293098 ]
 ...
 [ 54.77521529   0.123        0.1981       0.19070001   0.2098
    2.96673179]
 [-29.32547689   0.98590004   0.99270004   0.98619998   0.98750001
    2.49146318]
 [ 14.08712061   0.18370001   0.72240001   0.32179999   0.71700001
    2.85377836]][0m
[37m[1m[2023-07-11 08:30:42,785][233954] Max Reward on eval: 712.1685790952295[0m
[37m[1m[2023-07-11 08:30:42,786][233954] Min Reward on eval: -226.98275631624273[0m
[37m[1m[2023-07-11 08:30:42,786][233954] Mean Reward across all agents: 118.27695627203732[0m
[37m[1m[2023-07-11 08:30:42,787][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:30:42,795][233954] mean_value=-163.5633697027695, max_value=491.2544533365315[0m
[37m[1m[2023-07-11 08:30:42,800][233954] New mean coefficients: [[-0.63184583  0.92515814 -0.809173    0.24853271  0.68404746 -3.8437538 ]][0m
[37m[1m[2023-07-11 08:30:42,801][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:30:51,777][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 08:30:51,777][233954] FPS: 427918.35[0m
[36m[2023-07-11 08:30:51,779][233954] itr=636, itrs=2000, Progress: 31.80%[0m
[36m[2023-07-11 08:31:03,549][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 08:31:03,549][233954] FPS: 328696.13[0m
[36m[2023-07-11 08:31:07,789][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:31:07,789][233954] Reward + Measures: [[49.59427917  0.18812899  0.19418368  0.15164866  0.24421732  1.02445257]][0m
[37m[1m[2023-07-11 08:31:07,789][233954] Max Reward on eval: 49.59427917080322[0m
[37m[1m[2023-07-11 08:31:07,790][233954] Min Reward on eval: 49.59427917080322[0m
[37m[1m[2023-07-11 08:31:07,790][233954] Mean Reward across all agents: 49.59427917080322[0m
[37m[1m[2023-07-11 08:31:07,790][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:31:12,752][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:31:12,753][233954] Reward + Measures: [[257.45084342   0.0558       0.43670008   0.33570001   0.47700006
    1.77926576]
 [-32.95303271   0.48950002   0.84510005   0.42880002   0.81720001
    2.37360358]
 [270.2688212    0.19930001   0.59210002   0.28449997   0.59509999
    2.48518944]
 ...
 [ 25.65079798   0.1895       0.20370002   0.12520002   0.21270001
    1.86109126]
 [110.39384084   0.11399999   0.24150002   0.11550001   0.28080001
    2.07946324]
 [-73.80977264   0.69309998   0.8071       0.6196       0.76070005
    2.75498819]][0m
[37m[1m[2023-07-11 08:31:12,753][233954] Max Reward on eval: 725.7930679196492[0m
[37m[1m[2023-07-11 08:31:12,753][233954] Min Reward on eval: -232.25726876948028[0m
[37m[1m[2023-07-11 08:31:12,754][233954] Mean Reward across all agents: 115.12618126795644[0m
[37m[1m[2023-07-11 08:31:12,754][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:31:12,759][233954] mean_value=-164.14169728795926, max_value=481.988037007822[0m
[37m[1m[2023-07-11 08:31:12,762][233954] New mean coefficients: [[-0.84842503  1.6124319  -1.3982382   0.69023544  1.5572356  -3.0951426 ]][0m
[37m[1m[2023-07-11 08:31:12,763][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:31:21,753][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 08:31:21,754][233954] FPS: 427184.94[0m
[36m[2023-07-11 08:31:21,756][233954] itr=637, itrs=2000, Progress: 31.85%[0m
[36m[2023-07-11 08:31:33,405][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 08:31:33,406][233954] FPS: 332084.83[0m
[36m[2023-07-11 08:31:37,660][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:31:37,661][233954] Reward + Measures: [[46.18541962  0.20546667  0.18436366  0.14852633  0.24460301  0.98513532]][0m
[37m[1m[2023-07-11 08:31:37,661][233954] Max Reward on eval: 46.18541962257542[0m
[37m[1m[2023-07-11 08:31:37,661][233954] Min Reward on eval: 46.18541962257542[0m
[37m[1m[2023-07-11 08:31:37,662][233954] Mean Reward across all agents: 46.18541962257542[0m
[37m[1m[2023-07-11 08:31:37,662][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:31:42,900][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:31:42,900][233954] Reward + Measures: [[-47.31358004   0.1592       0.7579       0.2802       0.76490003
    2.79898643]
 [321.01354122   0.0886       0.87980002   0.51109999   0.77810001
    2.28473544]
 [ 47.54549694   0.1928       0.41319999   0.16360001   0.30770001
    2.22942567]
 ...
 [ 65.09950995   0.0737       0.91769999   0.4434       0.91989994
    3.37751842]
 [472.36026577   0.0465       0.92360002   0.64530003   0.89880002
    2.53291965]
 [ 38.16382572   0.123        0.90090001   0.29319999   0.85900003
    2.63062763]][0m
[37m[1m[2023-07-11 08:31:42,900][233954] Max Reward on eval: 577.9467773497105[0m
[37m[1m[2023-07-11 08:31:42,901][233954] Min Reward on eval: -205.56780448956414[0m
[37m[1m[2023-07-11 08:31:42,901][233954] Mean Reward across all agents: 42.37558752838139[0m
[37m[1m[2023-07-11 08:31:42,901][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:31:42,904][233954] mean_value=-239.8022970233874, max_value=514.2452263210043[0m
[37m[1m[2023-07-11 08:31:42,907][233954] New mean coefficients: [[-0.6001624   1.5937968  -0.86630833  0.16533542  1.3240225  -3.018303  ]][0m
[37m[1m[2023-07-11 08:31:42,908][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:31:51,874][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 08:31:51,875][233954] FPS: 428319.84[0m
[36m[2023-07-11 08:31:51,877][233954] itr=638, itrs=2000, Progress: 31.90%[0m
[36m[2023-07-11 08:32:03,485][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 08:32:03,485][233954] FPS: 333250.81[0m
[36m[2023-07-11 08:32:07,815][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:32:07,816][233954] Reward + Measures: [[42.6900633   0.20084867  0.18711568  0.148927    0.233234    0.9551686 ]][0m
[37m[1m[2023-07-11 08:32:07,816][233954] Max Reward on eval: 42.690063304727545[0m
[37m[1m[2023-07-11 08:32:07,816][233954] Min Reward on eval: 42.690063304727545[0m
[37m[1m[2023-07-11 08:32:07,816][233954] Mean Reward across all agents: 42.690063304727545[0m
[37m[1m[2023-07-11 08:32:07,817][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:32:12,860][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:32:12,861][233954] Reward + Measures: [[ 76.30622864   0.3134       0.7658       0.31130001   0.73609996
    2.53393221]
 [ 79.43919529   0.117        0.31399998   0.19749999   0.29200003
    1.84499514]
 [ 12.74126309   0.2811       0.33020002   0.26929998   0.29440004
    2.39243698]
 ...
 [  6.12905731   0.0969       0.1099       0.07910001   0.1358
    1.59464419]
 [-11.48178862   0.3536       0.51459998   0.24419999   0.5485
    2.5029583 ]
 [ 33.94299556   0.18860002   0.226        0.15810001   0.24550001
    2.22492886]][0m
[37m[1m[2023-07-11 08:32:12,861][233954] Max Reward on eval: 596.4880132586695[0m
[37m[1m[2023-07-11 08:32:12,861][233954] Min Reward on eval: -265.00539825018495[0m
[37m[1m[2023-07-11 08:32:12,862][233954] Mean Reward across all agents: 42.33788085867877[0m
[37m[1m[2023-07-11 08:32:12,862][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:32:12,865][233954] mean_value=-242.60715868722443, max_value=492.1932698071003[0m
[37m[1m[2023-07-11 08:32:12,868][233954] New mean coefficients: [[ 0.11019766  1.9641553  -0.62967205 -0.38838243  2.1606479  -2.194733  ]][0m
[37m[1m[2023-07-11 08:32:12,869][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:32:21,983][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 08:32:21,984][233954] FPS: 421390.27[0m
[36m[2023-07-11 08:32:21,986][233954] itr=639, itrs=2000, Progress: 31.95%[0m
[36m[2023-07-11 08:32:33,697][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 08:32:33,697][233954] FPS: 330309.65[0m
[36m[2023-07-11 08:32:38,008][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:32:38,008][233954] Reward + Measures: [[41.16342395  0.20775032  0.19321001  0.15243332  0.24506965  0.94959193]][0m
[37m[1m[2023-07-11 08:32:38,008][233954] Max Reward on eval: 41.163423948491186[0m
[37m[1m[2023-07-11 08:32:38,009][233954] Min Reward on eval: 41.163423948491186[0m
[37m[1m[2023-07-11 08:32:38,009][233954] Mean Reward across all agents: 41.163423948491186[0m
[37m[1m[2023-07-11 08:32:38,009][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:32:42,972][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:32:42,972][233954] Reward + Measures: [[  6.55583584   0.5079       0.6584       0.45520002   0.6425001
    2.70025373]
 [212.98319938   0.3407       0.61200005   0.20210002   0.59860003
    2.4451406 ]
 [447.25839044   0.0536       0.99080002   0.78420001   0.9835
    2.21178031]
 ...
 [120.42482375   0.41630003   0.49790001   0.45240003   0.53930002
    2.85294509]
 [150.61529637   0.44080001   0.51539999   0.46930003   0.59579998
    3.16745234]
 [143.16278029   0.60610002   0.55500001   0.61610001   0.45080003
    2.57296348]][0m
[37m[1m[2023-07-11 08:32:42,973][233954] Max Reward on eval: 553.6717124954332[0m
[37m[1m[2023-07-11 08:32:42,973][233954] Min Reward on eval: -298.81623650039546[0m
[37m[1m[2023-07-11 08:32:42,973][233954] Mean Reward across all agents: 135.07523255162153[0m
[37m[1m[2023-07-11 08:32:42,973][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:32:42,983][233954] mean_value=-28.35827556241848, max_value=586.4009035210509[0m
[37m[1m[2023-07-11 08:32:42,985][233954] New mean coefficients: [[ 0.6837226   2.5261397  -0.8133296  -0.43100786  2.7474976  -1.2894824 ]][0m
[37m[1m[2023-07-11 08:32:42,986][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:32:51,965][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 08:32:51,965][233954] FPS: 427745.83[0m
[36m[2023-07-11 08:32:51,968][233954] itr=640, itrs=2000, Progress: 32.00%[0m
[37m[1m[2023-07-11 08:36:20,713][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000620[0m
[36m[2023-07-11 08:36:32,907][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 08:36:32,908][233954] FPS: 330427.17[0m
[36m[2023-07-11 08:36:37,077][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:36:37,078][233954] Reward + Measures: [[41.92339854  0.22939099  0.19246067  0.15573266  0.24707365  0.9424693 ]][0m
[37m[1m[2023-07-11 08:36:37,078][233954] Max Reward on eval: 41.92339853547034[0m
[37m[1m[2023-07-11 08:36:37,078][233954] Min Reward on eval: 41.92339853547034[0m
[37m[1m[2023-07-11 08:36:37,078][233954] Mean Reward across all agents: 41.92339853547034[0m
[37m[1m[2023-07-11 08:36:37,078][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:36:41,928][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:36:41,929][233954] Reward + Measures: [[161.713724     0.12809999   0.34189996   0.18590002   0.35750002
    2.0943799 ]
 [278.12131309   0.0149       0.94870007   0.61729997   0.94659996
    3.03391576]
 [254.42548944   0.1983       0.52039999   0.38330001   0.42879996
    3.04874492]
 ...
 [ 20.17261062   0.1305       0.2057       0.14600001   0.1557
    3.07841396]
 [184.37122664   0.13600002   0.465        0.28         0.38140002
    2.91417122]
 [288.21535038   0.1107       0.61700004   0.44049999   0.565
    3.16332221]][0m
[37m[1m[2023-07-11 08:36:41,929][233954] Max Reward on eval: 600.873679149017[0m
[37m[1m[2023-07-11 08:36:41,929][233954] Min Reward on eval: -69.17536517037078[0m
[37m[1m[2023-07-11 08:36:41,929][233954] Mean Reward across all agents: 188.2245591105731[0m
[37m[1m[2023-07-11 08:36:41,930][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:36:41,934][233954] mean_value=-167.94038544273985, max_value=573.3959336277097[0m
[37m[1m[2023-07-11 08:36:41,937][233954] New mean coefficients: [[ 0.06855297  1.9567456  -0.6942079  -0.5406885   2.1904283  -2.4508219 ]][0m
[37m[1m[2023-07-11 08:36:41,938][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:36:50,922][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 08:36:50,922][233954] FPS: 427492.85[0m
[36m[2023-07-11 08:36:50,924][233954] itr=641, itrs=2000, Progress: 32.05%[0m
[36m[2023-07-11 08:37:02,542][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 08:37:02,543][233954] FPS: 333025.47[0m
[36m[2023-07-11 08:37:06,855][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:37:06,855][233954] Reward + Measures: [[44.33055695  0.24389668  0.19449532  0.15434933  0.25244033  0.95420521]][0m
[37m[1m[2023-07-11 08:37:06,855][233954] Max Reward on eval: 44.330556950324464[0m
[37m[1m[2023-07-11 08:37:06,856][233954] Min Reward on eval: 44.330556950324464[0m
[37m[1m[2023-07-11 08:37:06,856][233954] Mean Reward across all agents: 44.330556950324464[0m
[37m[1m[2023-07-11 08:37:06,856][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:37:11,844][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:37:11,850][233954] Reward + Measures: [[ -6.89321979   0.96670002   0.9835       0.9655       0.9853
    2.28087616]
 [  1.84635641   0.9806       0.99100012   0.9842       0.99119997
    2.99403954]
 [  2.96738099   0.94510001   0.95990002   0.96630001   0.9677
    3.04919863]
 ...
 [  8.29930462   0.97299999   0.98460001   0.98439997   0.99209994
    3.07204604]
 [-13.93693191   0.83890003   0.87510008   0.88020003   0.96320003
    3.83307958]
 [-79.05888606   0.52640003   0.67189997   0.5661       0.81539994
    3.6383965 ]][0m
[37m[1m[2023-07-11 08:37:11,851][233954] Max Reward on eval: 531.9723930679262[0m
[37m[1m[2023-07-11 08:37:11,851][233954] Min Reward on eval: -268.94090912360696[0m
[37m[1m[2023-07-11 08:37:11,851][233954] Mean Reward across all agents: 55.88377530344495[0m
[37m[1m[2023-07-11 08:37:11,851][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:37:11,856][233954] mean_value=-122.61947372569743, max_value=482.8782232524701[0m
[37m[1m[2023-07-11 08:37:11,858][233954] New mean coefficients: [[ 0.56091726  2.375802   -0.74372894 -0.34648094  2.5841358  -1.7128007 ]][0m
[37m[1m[2023-07-11 08:37:11,859][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:37:20,786][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 08:37:20,787][233954] FPS: 430231.85[0m
[36m[2023-07-11 08:37:20,789][233954] itr=642, itrs=2000, Progress: 32.10%[0m
[36m[2023-07-11 08:37:32,308][233954] train() took 11.43 seconds to complete[0m
[36m[2023-07-11 08:37:32,308][233954] FPS: 335877.59[0m
[36m[2023-07-11 08:37:36,618][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:37:36,619][233954] Reward + Measures: [[49.4930864   0.25186199  0.19894299  0.15825599  0.25937366  0.96084869]][0m
[37m[1m[2023-07-11 08:37:36,619][233954] Max Reward on eval: 49.493086396911316[0m
[37m[1m[2023-07-11 08:37:36,619][233954] Min Reward on eval: 49.493086396911316[0m
[37m[1m[2023-07-11 08:37:36,619][233954] Mean Reward across all agents: 49.493086396911316[0m
[37m[1m[2023-07-11 08:37:36,620][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:37:41,569][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:37:41,577][233954] Reward + Measures: [[ 46.42669111   0.113        0.3082       0.1601       0.26890001
    2.62439132]
 [ 50.80959585   0.0758       0.15450001   0.11240001   0.12639999
    2.8709085 ]
 [ 78.6250994    0.12620001   0.5395       0.24839997   0.49470001
    2.31912494]
 ...
 [ 33.50066572   0.07969999   0.16510001   0.12340001   0.1419
    3.02096009]
 [101.52515746   0.08620001   0.19090001   0.1349       0.16680001
    3.32108355]
 [ 59.50230451   0.0617       0.20870002   0.13850001   0.19240001
    2.83514094]][0m
[37m[1m[2023-07-11 08:37:41,578][233954] Max Reward on eval: 761.9437561021186[0m
[37m[1m[2023-07-11 08:37:41,578][233954] Min Reward on eval: -132.14653181079774[0m
[37m[1m[2023-07-11 08:37:41,579][233954] Mean Reward across all agents: 127.10640512326873[0m
[37m[1m[2023-07-11 08:37:41,579][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:37:41,585][233954] mean_value=-188.71585736364642, max_value=446.4920815524315[0m
[37m[1m[2023-07-11 08:37:41,589][233954] New mean coefficients: [[ 0.25953528  2.3122382  -0.5157241  -0.36169857  2.5751452  -1.8579558 ]][0m
[37m[1m[2023-07-11 08:37:41,591][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:37:50,555][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 08:37:50,555][233954] FPS: 428485.98[0m
[36m[2023-07-11 08:37:50,557][233954] itr=643, itrs=2000, Progress: 32.15%[0m
[36m[2023-07-11 08:38:02,253][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 08:38:02,253][233954] FPS: 330759.14[0m
[36m[2023-07-11 08:38:06,457][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:38:06,457][233954] Reward + Measures: [[42.82602669  0.25488669  0.21089464  0.16038465  0.25874934  0.94714892]][0m
[37m[1m[2023-07-11 08:38:06,457][233954] Max Reward on eval: 42.82602668860291[0m
[37m[1m[2023-07-11 08:38:06,458][233954] Min Reward on eval: 42.82602668860291[0m
[37m[1m[2023-07-11 08:38:06,458][233954] Mean Reward across all agents: 42.82602668860291[0m
[37m[1m[2023-07-11 08:38:06,458][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:38:11,378][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:38:11,378][233954] Reward + Measures: [[-92.5478614    0.96280003   0.96590006   0.95559996   0.96060002
    3.38726783]
 [761.26501083   0.08970001   0.97469997   0.6893       0.96600002
    3.28592992]
 [ 55.55421282   0.1516       0.7633       0.19500001   0.74119997
    2.5412178 ]
 ...
 [ 53.17239028   0.3664       0.6275       0.39019999   0.66370004
    2.18539596]
 [-52.82286595   0.9691       0.97600001   0.96120006   0.96659994
    3.26559329]
 [ 37.79301171   0.76350003   0.78760004   0.74010003   0.75430006
    2.58112502]][0m
[37m[1m[2023-07-11 08:38:11,379][233954] Max Reward on eval: 761.26501083459[0m
[37m[1m[2023-07-11 08:38:11,379][233954] Min Reward on eval: -197.808542485605[0m
[37m[1m[2023-07-11 08:38:11,379][233954] Mean Reward across all agents: -0.9303862307446199[0m
[37m[1m[2023-07-11 08:38:11,379][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:38:11,383][233954] mean_value=-223.99434432770116, max_value=557.9000162798911[0m
[37m[1m[2023-07-11 08:38:11,386][233954] New mean coefficients: [[ 0.16015331  1.6410682   0.08649099 -0.63229525  2.003895   -2.672534  ]][0m
[37m[1m[2023-07-11 08:38:11,387][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:38:20,320][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 08:38:20,321][233954] FPS: 429917.03[0m
[36m[2023-07-11 08:38:20,323][233954] itr=644, itrs=2000, Progress: 32.20%[0m
[36m[2023-07-11 08:38:32,128][233954] train() took 11.72 seconds to complete[0m
[36m[2023-07-11 08:38:32,128][233954] FPS: 327584.40[0m
[36m[2023-07-11 08:38:36,464][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:38:36,465][233954] Reward + Measures: [[42.48318209  0.26462498  0.22338499  0.161704    0.26913834  0.95321864]][0m
[37m[1m[2023-07-11 08:38:36,465][233954] Max Reward on eval: 42.48318208711066[0m
[37m[1m[2023-07-11 08:38:36,465][233954] Min Reward on eval: 42.48318208711066[0m
[37m[1m[2023-07-11 08:38:36,465][233954] Mean Reward across all agents: 42.48318208711066[0m
[37m[1m[2023-07-11 08:38:36,466][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:38:41,724][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:38:41,725][233954] Reward + Measures: [[101.8212478    0.1882       0.36020002   0.23010002   0.41280004
    2.07509089]
 [155.54284716   0.08630001   0.65700006   0.2299       0.61140007
    2.22161722]
 [ 68.96099114   0.13770001   0.21729998   0.1797       0.29100001
    2.15068698]
 ...
 [-17.17970523   0.7349       0.92450011   0.7511       0.92119998
    2.21439385]
 [ 90.73830164   0.1209       0.33770001   0.30770001   0.35610002
    2.7390821 ]
 [ 66.40891624   0.47629997   0.50879997   0.2253       0.35810003
    2.8315618 ]][0m
[37m[1m[2023-07-11 08:38:41,725][233954] Max Reward on eval: 786.87754058633[0m
[37m[1m[2023-07-11 08:38:41,725][233954] Min Reward on eval: -255.14989497093484[0m
[37m[1m[2023-07-11 08:38:41,725][233954] Mean Reward across all agents: 76.21339233891828[0m
[37m[1m[2023-07-11 08:38:41,726][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:38:41,729][233954] mean_value=-217.2198558229557, max_value=561.2908904817036[0m
[37m[1m[2023-07-11 08:38:41,732][233954] New mean coefficients: [[ 0.19520903  0.8080942   0.19053572  0.08046436  1.2223434  -3.5344684 ]][0m
[37m[1m[2023-07-11 08:38:41,733][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:38:50,785][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 08:38:50,785][233954] FPS: 424319.16[0m
[36m[2023-07-11 08:38:50,787][233954] itr=645, itrs=2000, Progress: 32.25%[0m
[36m[2023-07-11 08:39:02,553][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 08:39:02,553][233954] FPS: 328738.72[0m
[36m[2023-07-11 08:39:06,759][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:39:06,759][233954] Reward + Measures: [[35.39909032  0.28242397  0.221339    0.16165167  0.27127934  0.92215651]][0m
[37m[1m[2023-07-11 08:39:06,759][233954] Max Reward on eval: 35.39909031627463[0m
[37m[1m[2023-07-11 08:39:06,760][233954] Min Reward on eval: 35.39909031627463[0m
[37m[1m[2023-07-11 08:39:06,760][233954] Mean Reward across all agents: 35.39909031627463[0m
[37m[1m[2023-07-11 08:39:06,760][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:39:11,699][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:39:11,700][233954] Reward + Measures: [[  1.37953249   0.2383       0.63080001   0.36990005   0.63960004
    2.38689852]
 [ 66.96709992   0.16600001   0.1938       0.0819       0.24969998
    2.79648352]
 [144.36270915   0.0507       0.95660001   0.4224       0.954
    2.57820296]
 ...
 [ 10.36251589   0.0718       0.23870002   0.197        0.29110003
    2.72275329]
 [ 78.43572903   0.11500001   0.77749997   0.3193       0.75819999
    3.1261847 ]
 [ 53.93196515   0.1252       0.83579999   0.46150002   0.79089993
    2.97565651]][0m
[37m[1m[2023-07-11 08:39:11,700][233954] Max Reward on eval: 790.5872192267328[0m
[37m[1m[2023-07-11 08:39:11,701][233954] Min Reward on eval: -221.95426517212763[0m
[37m[1m[2023-07-11 08:39:11,701][233954] Mean Reward across all agents: 111.63763092735928[0m
[37m[1m[2023-07-11 08:39:11,701][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:39:11,705][233954] mean_value=-204.33516537670658, max_value=451.65231668207406[0m
[37m[1m[2023-07-11 08:39:11,707][233954] New mean coefficients: [[-0.05084534  0.3609088  -0.40938175  0.5267557   0.49543643 -4.2544785 ]][0m
[37m[1m[2023-07-11 08:39:11,708][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:39:20,660][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 08:39:20,660][233954] FPS: 429053.76[0m
[36m[2023-07-11 08:39:20,663][233954] itr=646, itrs=2000, Progress: 32.30%[0m
[36m[2023-07-11 08:39:32,357][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 08:39:32,357][233954] FPS: 330856.30[0m
[36m[2023-07-11 08:39:36,639][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:39:36,639][233954] Reward + Measures: [[41.77034459  0.28631434  0.20530967  0.16277801  0.26712403  0.86186433]][0m
[37m[1m[2023-07-11 08:39:36,639][233954] Max Reward on eval: 41.77034459293339[0m
[37m[1m[2023-07-11 08:39:36,640][233954] Min Reward on eval: 41.77034459293339[0m
[37m[1m[2023-07-11 08:39:36,640][233954] Mean Reward across all agents: 41.77034459293339[0m
[37m[1m[2023-07-11 08:39:36,640][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:39:41,625][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:39:41,626][233954] Reward + Measures: [[ 19.50186849   0.46700001   0.40330002   0.37099999   0.36830002
    2.10840058]
 [ 31.97984907   0.0656       0.0892       0.07310001   0.10770001
    1.91149104]
 [ 42.96152944   0.35760003   0.62480003   0.30580002   0.5722
    2.15945148]
 ...
 [176.82558585   0.0675       0.336        0.2335       0.37900001
    2.84742165]
 [174.78719568   0.42670003   0.84790003   0.53179997   0.83319998
    2.61669564]
 [172.68217563   0.17         0.43169999   0.26420003   0.45429999
    2.28159857]][0m
[37m[1m[2023-07-11 08:39:41,626][233954] Max Reward on eval: 787.861202227883[0m
[37m[1m[2023-07-11 08:39:41,627][233954] Min Reward on eval: -357.1690749740228[0m
[37m[1m[2023-07-11 08:39:41,627][233954] Mean Reward across all agents: 115.46771223420588[0m
[37m[1m[2023-07-11 08:39:41,627][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:39:41,632][233954] mean_value=-157.2354041746058, max_value=491.6412058425273[0m
[37m[1m[2023-07-11 08:39:41,635][233954] New mean coefficients: [[-0.7761769  -0.43057877 -0.664717    0.5561337  -0.52710533 -5.4592066 ]][0m
[37m[1m[2023-07-11 08:39:41,636][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:39:50,695][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 08:39:50,695][233954] FPS: 423950.90[0m
[36m[2023-07-11 08:39:50,697][233954] itr=647, itrs=2000, Progress: 32.35%[0m
[36m[2023-07-11 08:40:02,439][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 08:40:02,439][233954] FPS: 329508.45[0m
[36m[2023-07-11 08:40:06,797][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:40:06,797][233954] Reward + Measures: [[39.22813082  0.27385131  0.20556933  0.15795866  0.26294401  0.87549406]][0m
[37m[1m[2023-07-11 08:40:06,797][233954] Max Reward on eval: 39.22813082430345[0m
[37m[1m[2023-07-11 08:40:06,798][233954] Min Reward on eval: 39.22813082430345[0m
[37m[1m[2023-07-11 08:40:06,798][233954] Mean Reward across all agents: 39.22813082430345[0m
[37m[1m[2023-07-11 08:40:06,798][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:40:11,864][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:40:11,870][233954] Reward + Measures: [[ 57.74261202   0.56700003   0.97840005   0.51400006   0.96900004
    2.71232605]
 [-23.99408195   0.1858       0.32000002   0.0989       0.2516
    1.78488851]
 [ 23.87753271   0.09790001   0.63620001   0.49870005   0.62650007
    2.55624819]
 ...
 [ 36.49872475   0.67360002   0.79339999   0.6214       0.63240004
    2.15579915]
 [619.42909623   0.0012       0.99809998   0.75810003   0.99440002
    2.69521618]
 [580.05341962   0.08369999   0.93580002   0.60210007   0.91580003
    3.67304611]][0m
[37m[1m[2023-07-11 08:40:11,870][233954] Max Reward on eval: 747.2736206166446[0m
[37m[1m[2023-07-11 08:40:11,870][233954] Min Reward on eval: -215.58298964835703[0m
[37m[1m[2023-07-11 08:40:11,871][233954] Mean Reward across all agents: 148.19037177457335[0m
[37m[1m[2023-07-11 08:40:11,871][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:40:11,876][233954] mean_value=-150.57744065658278, max_value=659.7696132950438[0m
[37m[1m[2023-07-11 08:40:11,879][233954] New mean coefficients: [[-1.3808882  -1.675277   -0.9260617   0.51863134 -1.7360866  -7.2157416 ]][0m
[37m[1m[2023-07-11 08:40:11,880][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:40:20,929][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 08:40:20,929][233954] FPS: 424464.26[0m
[36m[2023-07-11 08:40:20,931][233954] itr=648, itrs=2000, Progress: 32.40%[0m
[36m[2023-07-11 08:40:32,768][233954] train() took 11.75 seconds to complete[0m
[36m[2023-07-11 08:40:32,768][233954] FPS: 326725.71[0m
[36m[2023-07-11 08:40:37,064][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:40:37,064][233954] Reward + Measures: [[41.9587121   0.274997    0.20369498  0.16056533  0.25962332  0.86874008]][0m
[37m[1m[2023-07-11 08:40:37,064][233954] Max Reward on eval: 41.958712099861124[0m
[37m[1m[2023-07-11 08:40:37,065][233954] Min Reward on eval: 41.958712099861124[0m
[37m[1m[2023-07-11 08:40:37,065][233954] Mean Reward across all agents: 41.958712099861124[0m
[37m[1m[2023-07-11 08:40:37,065][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:40:42,119][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:40:42,120][233954] Reward + Measures: [[534.70701792   0.0636       0.93219995   0.60509998   0.93559998
    3.37873244]
 [337.11124707   0.0992       0.84389991   0.55849999   0.81389999
    3.11277318]
 [358.66123987   0.17470001   0.9781       0.66570008   0.97700006
    2.23471189]
 ...
 [140.83515786   0.0514       0.8865       0.34530002   0.85050005
    2.29512095]
 [119.94020628   0.09210001   0.55419999   0.2411       0.51879996
    2.3346653 ]
 [484.95086097   0.021        0.96970004   0.73500007   0.96759999
    3.56336403]][0m
[37m[1m[2023-07-11 08:40:42,120][233954] Max Reward on eval: 828.146354667563[0m
[37m[1m[2023-07-11 08:40:42,121][233954] Min Reward on eval: -149.4251217801124[0m
[37m[1m[2023-07-11 08:40:42,121][233954] Mean Reward across all agents: 278.371673451324[0m
[37m[1m[2023-07-11 08:40:42,121][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:40:42,125][233954] mean_value=-181.98362080991618, max_value=669.5663563653433[0m
[37m[1m[2023-07-11 08:40:42,128][233954] New mean coefficients: [[-1.3474915 -2.2402472 -0.5185395  0.8293192 -2.4025486 -7.944345 ]][0m
[37m[1m[2023-07-11 08:40:42,129][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:40:51,266][233954] train() took 9.14 seconds to complete[0m
[36m[2023-07-11 08:40:51,266][233954] FPS: 420350.39[0m
[36m[2023-07-11 08:40:51,268][233954] itr=649, itrs=2000, Progress: 32.45%[0m
[36m[2023-07-11 08:41:02,894][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 08:41:02,894][233954] FPS: 332764.25[0m
[36m[2023-07-11 08:41:07,122][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:41:07,122][233954] Reward + Measures: [[37.60964094  0.24636966  0.188932    0.149919    0.23217668  0.81446987]][0m
[37m[1m[2023-07-11 08:41:07,122][233954] Max Reward on eval: 37.609640940159444[0m
[37m[1m[2023-07-11 08:41:07,123][233954] Min Reward on eval: 37.609640940159444[0m
[37m[1m[2023-07-11 08:41:07,123][233954] Mean Reward across all agents: 37.609640940159444[0m
[37m[1m[2023-07-11 08:41:07,123][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:41:12,105][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:41:12,106][233954] Reward + Measures: [[ 86.75497794   0.0794       0.28410003   0.17110001   0.27110001
    3.35840917]
 [ 49.66676256   0.14250001   0.16159999   0.1362       0.1707
    2.80971265]
 [ 15.22438287   0.1487       0.24569999   0.1692       0.2436
    2.8869679 ]
 ...
 [-90.04580276   0.35900003   0.41249999   0.0878       0.43430001
    3.04255271]
 [ 26.85764462   0.1153       0.0705       0.0719       0.1175
    2.59028602]
 [ 26.44678359   0.1754       0.16870001   0.11010001   0.178
    3.05588698]][0m
[37m[1m[2023-07-11 08:41:12,106][233954] Max Reward on eval: 616.5756569009275[0m
[37m[1m[2023-07-11 08:41:12,106][233954] Min Reward on eval: -517.5685863627295[0m
[37m[1m[2023-07-11 08:41:12,107][233954] Mean Reward across all agents: 94.15759535895[0m
[37m[1m[2023-07-11 08:41:12,107][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:41:12,110][233954] mean_value=-177.4539684950001, max_value=470.8582987047954[0m
[37m[1m[2023-07-11 08:41:12,113][233954] New mean coefficients: [[-1.5431921 -2.436551  -1.4939997  1.5704687 -2.4893122 -8.235744 ]][0m
[37m[1m[2023-07-11 08:41:12,114][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:41:21,043][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 08:41:21,043][233954] FPS: 430156.96[0m
[36m[2023-07-11 08:41:21,045][233954] itr=650, itrs=2000, Progress: 32.50%[0m
[37m[1m[2023-07-11 08:44:41,575][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000630[0m
[36m[2023-07-11 08:44:53,968][233954] train() took 11.77 seconds to complete[0m
[36m[2023-07-11 08:44:53,968][233954] FPS: 326131.19[0m
[36m[2023-07-11 08:44:58,174][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:44:58,175][233954] Reward + Measures: [[39.26551462  0.21746302  0.20428434  0.16478033  0.22502199  0.81476998]][0m
[37m[1m[2023-07-11 08:44:58,175][233954] Max Reward on eval: 39.26551461883012[0m
[37m[1m[2023-07-11 08:44:58,175][233954] Min Reward on eval: 39.26551461883012[0m
[37m[1m[2023-07-11 08:44:58,175][233954] Mean Reward across all agents: 39.26551461883012[0m
[37m[1m[2023-07-11 08:44:58,176][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:45:03,151][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:45:03,152][233954] Reward + Measures: [[224.65224648   0.0042       0.99200004   0.57260001   0.99290007
    2.79226589]
 [ 80.27240221   0.1319       0.2739       0.11900001   0.278
    1.89053917]
 [408.21459674   0.0693       0.92640013   0.51170003   0.90340006
    2.63674712]
 ...
 [105.08555104   0.0696       0.58930004   0.30580002   0.60589999
    2.70734668]
 [-11.9268236    0.28950003   0.24759999   0.2494       0.36480001
    1.47980213]
 [166.35328959   0.0171       0.96050006   0.47589999   0.95230001
    3.1985836 ]][0m
[37m[1m[2023-07-11 08:45:03,152][233954] Max Reward on eval: 706.972965240269[0m
[37m[1m[2023-07-11 08:45:03,152][233954] Min Reward on eval: -151.8835797310807[0m
[37m[1m[2023-07-11 08:45:03,152][233954] Mean Reward across all agents: 105.59433901524876[0m
[37m[1m[2023-07-11 08:45:03,152][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:45:03,155][233954] mean_value=-280.14145860492414, max_value=378.8974696062405[0m
[37m[1m[2023-07-11 08:45:03,157][233954] New mean coefficients: [[-0.482211  -1.8057936 -0.755501   1.1161176 -1.5033065 -7.0049067]][0m
[37m[1m[2023-07-11 08:45:03,158][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:45:12,050][233954] train() took 8.89 seconds to complete[0m
[36m[2023-07-11 08:45:12,050][233954] FPS: 431936.44[0m
[36m[2023-07-11 08:45:12,052][233954] itr=651, itrs=2000, Progress: 32.55%[0m
[36m[2023-07-11 08:45:23,571][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 08:45:23,571][233954] FPS: 335806.89[0m
[36m[2023-07-11 08:45:27,893][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:45:27,894][233954] Reward + Measures: [[31.04811799  0.16817899  0.19752432  0.15686366  0.18991567  0.78053004]][0m
[37m[1m[2023-07-11 08:45:27,894][233954] Max Reward on eval: 31.048117989305446[0m
[37m[1m[2023-07-11 08:45:27,894][233954] Min Reward on eval: 31.048117989305446[0m
[37m[1m[2023-07-11 08:45:27,894][233954] Mean Reward across all agents: 31.048117989305446[0m
[37m[1m[2023-07-11 08:45:27,895][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:45:33,131][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:45:33,137][233954] Reward + Measures: [[603.16056823   0.0093       0.98810005   0.78860003   0.98040009
    2.62492871]
 [490.47204366   0.0508       0.87779999   0.6893       0.87620002
    2.74384546]
 [102.85301843   0.31569999   0.44260001   0.34979999   0.47589999
    1.9614445 ]
 ...
 [-22.39526174   0.28120002   0.69639999   0.2156       0.6358
    2.26331306]
 [ 26.71078956   0.33010003   0.29410002   0.1798       0.32960001
    3.05587435]
 [335.46830705   0.07340001   0.63510007   0.37180001   0.65819997
    2.82721949]][0m
[37m[1m[2023-07-11 08:45:33,137][233954] Max Reward on eval: 782.8493118202081[0m
[37m[1m[2023-07-11 08:45:33,138][233954] Min Reward on eval: -223.63381750825792[0m
[37m[1m[2023-07-11 08:45:33,138][233954] Mean Reward across all agents: 146.5604175012103[0m
[37m[1m[2023-07-11 08:45:33,138][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:45:33,144][233954] mean_value=-129.4088415297499, max_value=678.4455368803535[0m
[37m[1m[2023-07-11 08:45:33,147][233954] New mean coefficients: [[-1.4843515 -1.5316874 -1.5624716  1.5105596 -1.33718   -7.350201 ]][0m
[37m[1m[2023-07-11 08:45:33,148][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:45:42,242][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 08:45:42,243][233954] FPS: 422315.97[0m
[36m[2023-07-11 08:45:42,245][233954] itr=652, itrs=2000, Progress: 32.60%[0m
[36m[2023-07-11 08:45:53,953][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 08:45:53,953][233954] FPS: 330346.82[0m
[36m[2023-07-11 08:45:58,149][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:45:58,149][233954] Reward + Measures: [[33.1077653   0.16664699  0.19127066  0.15183166  0.18677567  0.7618556 ]][0m
[37m[1m[2023-07-11 08:45:58,150][233954] Max Reward on eval: 33.10776530187315[0m
[37m[1m[2023-07-11 08:45:58,150][233954] Min Reward on eval: 33.10776530187315[0m
[37m[1m[2023-07-11 08:45:58,150][233954] Mean Reward across all agents: 33.10776530187315[0m
[37m[1m[2023-07-11 08:45:58,150][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:46:03,142][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:46:03,143][233954] Reward + Measures: [[357.84444703   0.38509998   0.75230002   0.48800001   0.81999999
    3.32721972]
 [370.97624587   0.35230002   0.93940002   0.47080001   0.92649996
    3.63921595]
 [205.69513348   0.14930001   0.49039999   0.33290002   0.48079997
    3.53186417]
 ...
 [324.43351271   0.2313       0.71419996   0.38710001   0.70320004
    3.38759398]
 [ 12.85948989   0.52560002   0.38069999   0.45469999   0.34630001
    1.97329938]
 [ 51.60891104   0.07000001   0.14119999   0.11760002   0.20030001
    2.7346406 ]][0m
[37m[1m[2023-07-11 08:46:03,143][233954] Max Reward on eval: 742.8105353949592[0m
[37m[1m[2023-07-11 08:46:03,143][233954] Min Reward on eval: -417.29951940928584[0m
[37m[1m[2023-07-11 08:46:03,144][233954] Mean Reward across all agents: 118.70993456279051[0m
[37m[1m[2023-07-11 08:46:03,144][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:46:03,149][233954] mean_value=-180.39603838604967, max_value=674.532391734398[0m
[37m[1m[2023-07-11 08:46:03,157][233954] New mean coefficients: [[-1.4091966 -1.5790473 -1.0891563  1.0901235 -1.2136002 -7.5556526]][0m
[37m[1m[2023-07-11 08:46:03,158][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:46:12,114][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 08:46:12,115][233954] FPS: 428820.10[0m
[36m[2023-07-11 08:46:12,117][233954] itr=653, itrs=2000, Progress: 32.65%[0m
[36m[2023-07-11 08:46:23,644][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 08:46:23,644][233954] FPS: 335638.45[0m
[36m[2023-07-11 08:46:27,836][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:46:27,837][233954] Reward + Measures: [[33.9712905   0.16568299  0.186378    0.146607    0.18825567  0.76481301]][0m
[37m[1m[2023-07-11 08:46:27,837][233954] Max Reward on eval: 33.97129049679796[0m
[37m[1m[2023-07-11 08:46:27,837][233954] Min Reward on eval: 33.97129049679796[0m
[37m[1m[2023-07-11 08:46:27,838][233954] Mean Reward across all agents: 33.97129049679796[0m
[37m[1m[2023-07-11 08:46:27,838][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:46:32,812][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:46:32,813][233954] Reward + Measures: [[185.87568283   0.18010001   0.96170008   0.21229999   0.9485001
    2.98623085]
 [  5.51074552   0.4765       0.7683       0.17760001   0.76910001
    2.88866067]
 [230.83482075   0.20560001   0.77240002   0.67680001   0.75100005
    3.00280833]
 ...
 [488.32063487   0.06949999   0.92659998   0.61790001   0.9073
    2.63213539]
 [257.52252006   0.68379998   0.94799995   0.70450002   0.89379996
    3.02816749]
 [ 70.54097324   0.18870001   0.70660001   0.13520001   0.69499999
    2.07117844]][0m
[37m[1m[2023-07-11 08:46:32,813][233954] Max Reward on eval: 689.7411994725466[0m
[37m[1m[2023-07-11 08:46:32,813][233954] Min Reward on eval: -452.5326156653464[0m
[37m[1m[2023-07-11 08:46:32,813][233954] Mean Reward across all agents: 147.29597145682544[0m
[37m[1m[2023-07-11 08:46:32,814][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:46:32,821][233954] mean_value=-81.66753457744223, max_value=570.5646521672251[0m
[37m[1m[2023-07-11 08:46:32,824][233954] New mean coefficients: [[ -2.5367372  -2.8838458  -1.9137199   1.6071005  -2.7152538 -10.079552 ]][0m
[37m[1m[2023-07-11 08:46:32,825][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:46:41,774][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 08:46:41,775][233954] FPS: 429129.88[0m
[36m[2023-07-11 08:46:41,777][233954] itr=654, itrs=2000, Progress: 32.70%[0m
[36m[2023-07-11 08:46:53,663][233954] train() took 11.80 seconds to complete[0m
[36m[2023-07-11 08:46:53,663][233954] FPS: 325472.29[0m
[36m[2023-07-11 08:46:57,952][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:46:57,953][233954] Reward + Measures: [[32.34124429  0.18381901  0.180107    0.15163332  0.192084    0.73083282]][0m
[37m[1m[2023-07-11 08:46:57,953][233954] Max Reward on eval: 32.341244289688255[0m
[37m[1m[2023-07-11 08:46:57,953][233954] Min Reward on eval: 32.341244289688255[0m
[37m[1m[2023-07-11 08:46:57,953][233954] Mean Reward across all agents: 32.341244289688255[0m
[37m[1m[2023-07-11 08:46:57,954][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:47:02,901][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:47:02,902][233954] Reward + Measures: [[ 87.04132642   0.1013       0.1938       0.1514       0.1912
    2.94898701]
 [ 73.52405643   0.3046       0.66849995   0.29140002   0.5808
    2.36093593]
 [ 78.68137489   0.16020001   0.2581       0.0929       0.2441
    2.41412544]
 ...
 [ 65.08984328   0.0901       0.18859999   0.097        0.1732
    2.66126037]
 [ 19.7618487    0.1718       0.38390002   0.1402       0.40920001
    2.6669929 ]
 [102.80158624   0.1838       0.31759998   0.08630001   0.27950001
    2.47149634]][0m
[37m[1m[2023-07-11 08:47:02,902][233954] Max Reward on eval: 392.7005405249074[0m
[37m[1m[2023-07-11 08:47:02,902][233954] Min Reward on eval: -82.85546964975075[0m
[37m[1m[2023-07-11 08:47:02,902][233954] Mean Reward across all agents: 83.71870141584233[0m
[37m[1m[2023-07-11 08:47:02,903][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:47:02,905][233954] mean_value=-263.56306339992415, max_value=268.9863061391465[0m
[37m[1m[2023-07-11 08:47:02,907][233954] New mean coefficients: [[-1.6325613 -2.4221714 -1.6181267  1.3430717 -2.214633  -9.109901 ]][0m
[37m[1m[2023-07-11 08:47:02,908][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:47:11,865][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 08:47:11,865][233954] FPS: 428777.42[0m
[36m[2023-07-11 08:47:11,868][233954] itr=655, itrs=2000, Progress: 32.75%[0m
[36m[2023-07-11 08:47:23,467][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 08:47:23,468][233954] FPS: 333474.01[0m
[36m[2023-07-11 08:47:27,759][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:47:27,760][233954] Reward + Measures: [[32.60542455  0.15736033  0.18879801  0.16248266  0.17880066  0.72461456]][0m
[37m[1m[2023-07-11 08:47:27,760][233954] Max Reward on eval: 32.605424547929644[0m
[37m[1m[2023-07-11 08:47:27,760][233954] Min Reward on eval: 32.605424547929644[0m
[37m[1m[2023-07-11 08:47:27,760][233954] Mean Reward across all agents: 32.605424547929644[0m
[37m[1m[2023-07-11 08:47:27,761][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:47:32,780][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:47:32,786][233954] Reward + Measures: [[ 74.07208319   0.12719999   0.34120002   0.1158       0.3075
    2.17567801]
 [-72.06387516   0.23550001   0.41499996   0.21659999   0.38159999
    3.07911325]
 [ 99.08477998   0.3987       0.49760005   0.2084       0.60329998
    2.6394105 ]
 ...
 [186.19912191   0.0667       0.38270003   0.23370002   0.38970003
    2.8441658 ]
 [268.58343739   0.3021       0.50060004   0.1749       0.44640002
    2.73052335]
 [ 52.4079897    0.19970001   0.3066       0.14310001   0.33790001
    2.7085588 ]][0m
[37m[1m[2023-07-11 08:47:32,786][233954] Max Reward on eval: 783.9763488703408[0m
[37m[1m[2023-07-11 08:47:32,787][233954] Min Reward on eval: -360.9612641984364[0m
[37m[1m[2023-07-11 08:47:32,787][233954] Mean Reward across all agents: 196.50381555735638[0m
[37m[1m[2023-07-11 08:47:32,787][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:47:32,791][233954] mean_value=-148.3569609242357, max_value=456.65177637157774[0m
[37m[1m[2023-07-11 08:47:32,794][233954] New mean coefficients: [[ -2.0271575  -3.2684007  -1.9202279   1.2242441  -3.105524  -10.387145 ]][0m
[37m[1m[2023-07-11 08:47:32,795][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:47:41,855][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 08:47:41,856][233954] FPS: 423905.00[0m
[36m[2023-07-11 08:47:41,858][233954] itr=656, itrs=2000, Progress: 32.80%[0m
[36m[2023-07-11 08:47:53,665][233954] train() took 11.72 seconds to complete[0m
[36m[2023-07-11 08:47:53,666][233954] FPS: 327597.21[0m
[36m[2023-07-11 08:47:57,908][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:47:57,909][233954] Reward + Measures: [[27.91220056  0.13800834  0.19566199  0.17213701  0.18298167  0.7446087 ]][0m
[37m[1m[2023-07-11 08:47:57,909][233954] Max Reward on eval: 27.912200562074286[0m
[37m[1m[2023-07-11 08:47:57,909][233954] Min Reward on eval: 27.912200562074286[0m
[37m[1m[2023-07-11 08:47:57,909][233954] Mean Reward across all agents: 27.912200562074286[0m
[37m[1m[2023-07-11 08:47:57,910][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:48:02,917][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:48:02,923][233954] Reward + Measures: [[ 20.69389465   0.0723       0.93020004   0.46349999   0.93930006
    3.17563057]
 [ 43.81710725   0.1417       0.74630004   0.30239999   0.74330002
    2.94469857]
 [293.65838334   0.0618       0.96349996   0.59560001   0.95050001
    2.47947812]
 ...
 [ 32.22538573   0.25419998   0.5248       0.2992       0.55799997
    2.58918262]
 [118.75777482   0.0134       0.9939       0.50640005   0.99550003
    3.53007317]
 [110.82779167   0.32249999   0.61510003   0.45120001   0.54049999
    2.6966002 ]][0m
[37m[1m[2023-07-11 08:48:02,923][233954] Max Reward on eval: 712.2174377573654[0m
[37m[1m[2023-07-11 08:48:02,923][233954] Min Reward on eval: -336.4624900689581[0m
[37m[1m[2023-07-11 08:48:02,924][233954] Mean Reward across all agents: 79.40419440020581[0m
[37m[1m[2023-07-11 08:48:02,924][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:48:02,929][233954] mean_value=-185.94111745761842, max_value=774.0060296244453[0m
[37m[1m[2023-07-11 08:48:02,931][233954] New mean coefficients: [[ -2.4954536  -3.145535   -1.1152976   0.658656   -2.9566169 -10.286308 ]][0m
[37m[1m[2023-07-11 08:48:02,932][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:48:11,924][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 08:48:11,930][233954] FPS: 427128.74[0m
[36m[2023-07-11 08:48:11,933][233954] itr=657, itrs=2000, Progress: 32.85%[0m
[36m[2023-07-11 08:48:23,653][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 08:48:23,653][233954] FPS: 330056.06[0m
[36m[2023-07-11 08:48:27,991][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:48:27,992][233954] Reward + Measures: [[30.88479098  0.13979033  0.18610701  0.17910498  0.18283467  0.7229507 ]][0m
[37m[1m[2023-07-11 08:48:27,992][233954] Max Reward on eval: 30.8847909761207[0m
[37m[1m[2023-07-11 08:48:27,992][233954] Min Reward on eval: 30.8847909761207[0m
[37m[1m[2023-07-11 08:48:27,992][233954] Mean Reward across all agents: 30.8847909761207[0m
[37m[1m[2023-07-11 08:48:27,993][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:48:33,243][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:48:33,244][233954] Reward + Measures: [[ 12.01558916   0.1901       0.41510001   0.19410001   0.38070002
    3.13482332]
 [236.13951682   0.0697       0.90109998   0.69819993   0.90959996
    3.24028397]
 [-77.54404357   0.17439999   0.35870001   0.19139999   0.3231
    3.12009287]
 ...
 [242.16026347   0.08459999   0.90650004   0.708        0.89910001
    3.3072083 ]
 [ 16.8117024    0.1627       0.35980001   0.15530001   0.37669998
    3.29626632]
 [ 72.10822104   0.0583       0.38530001   0.2119       0.35390002
    2.93085527]][0m
[37m[1m[2023-07-11 08:48:33,244][233954] Max Reward on eval: 404.4933104633354[0m
[37m[1m[2023-07-11 08:48:33,244][233954] Min Reward on eval: -90.04707740582526[0m
[37m[1m[2023-07-11 08:48:33,244][233954] Mean Reward across all agents: 93.05033791011303[0m
[37m[1m[2023-07-11 08:48:33,244][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:48:33,247][233954] mean_value=-257.3227659231251, max_value=376.5115057787265[0m
[37m[1m[2023-07-11 08:48:33,250][233954] New mean coefficients: [[-1.3150923  -1.606791   -0.61284727  0.28832468 -1.1704378  -7.8372393 ]][0m
[37m[1m[2023-07-11 08:48:33,251][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:48:42,292][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 08:48:42,293][233954] FPS: 424768.68[0m
[36m[2023-07-11 08:48:42,295][233954] itr=658, itrs=2000, Progress: 32.90%[0m
[36m[2023-07-11 08:48:54,010][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 08:48:54,010][233954] FPS: 330174.62[0m
[36m[2023-07-11 08:48:58,317][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:48:58,317][233954] Reward + Measures: [[26.64390853  0.14314766  0.18327866  0.17123465  0.17700699  0.69861752]][0m
[37m[1m[2023-07-11 08:48:58,317][233954] Max Reward on eval: 26.643908532700102[0m
[37m[1m[2023-07-11 08:48:58,318][233954] Min Reward on eval: 26.643908532700102[0m
[37m[1m[2023-07-11 08:48:58,318][233954] Mean Reward across all agents: 26.643908532700102[0m
[37m[1m[2023-07-11 08:48:58,318][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:49:03,321][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:49:03,322][233954] Reward + Measures: [[-66.15910973   0.1163       0.16069999   0.1037       0.20850001
    2.5830369 ]
 [ 58.84047553   0.1391       0.27560002   0.18859999   0.26850003
    2.68574953]
 [194.72224292   0.37979999   0.87810004   0.70140004   0.84180003
    2.5501945 ]
 ...
 [103.55582231   0.0754       0.1648       0.12840001   0.20540002
    2.0099628 ]
 [184.13657819   0.24000001   0.62589997   0.45000002   0.60190004
    2.30097508]
 [199.84190463   0.72680008   0.78060001   0.7432       0.82860005
    2.77941966]][0m
[37m[1m[2023-07-11 08:49:03,322][233954] Max Reward on eval: 712.1838760726154[0m
[37m[1m[2023-07-11 08:49:03,322][233954] Min Reward on eval: -187.35457347407936[0m
[37m[1m[2023-07-11 08:49:03,322][233954] Mean Reward across all agents: 78.54810911039395[0m
[37m[1m[2023-07-11 08:49:03,322][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:49:03,328][233954] mean_value=-234.89526134838195, max_value=586.8412313787453[0m
[37m[1m[2023-07-11 08:49:03,330][233954] New mean coefficients: [[-0.16652489  0.48235202 -0.61078507 -0.41388014  1.3029847  -4.863245  ]][0m
[37m[1m[2023-07-11 08:49:03,331][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:49:12,429][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 08:49:12,429][233954] FPS: 422158.25[0m
[36m[2023-07-11 08:49:12,432][233954] itr=659, itrs=2000, Progress: 32.95%[0m
[36m[2023-07-11 08:49:23,949][233954] train() took 11.43 seconds to complete[0m
[36m[2023-07-11 08:49:23,949][233954] FPS: 335910.61[0m
[36m[2023-07-11 08:49:28,183][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:49:28,183][233954] Reward + Measures: [[23.03805615  0.16036732  0.17552866  0.165499    0.16485834  0.66818345]][0m
[37m[1m[2023-07-11 08:49:28,184][233954] Max Reward on eval: 23.03805614636393[0m
[37m[1m[2023-07-11 08:49:28,184][233954] Min Reward on eval: 23.03805614636393[0m
[37m[1m[2023-07-11 08:49:28,184][233954] Mean Reward across all agents: 23.03805614636393[0m
[37m[1m[2023-07-11 08:49:28,184][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:49:33,191][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:49:33,196][233954] Reward + Measures: [[-4.44602375  0.505       0.87880003  0.54470003  0.83750004  2.53407669]
 [43.49410555  0.16410001  0.2221      0.14399999  0.26719999  1.21868682]
 [45.49908234  0.0713      0.18950002  0.10380001  0.23360001  1.46415746]
 ...
 [46.182055    0.1104      0.95030004  0.2791      0.94160002  3.11594701]
 [ 3.40227009  0.1464      0.35230002  0.1269      0.34560001  2.74819446]
 [ 0.88777074  0.39470002  0.5334      0.42950001  0.49309999  2.65085578]][0m
[37m[1m[2023-07-11 08:49:33,196][233954] Max Reward on eval: 768.8349380392349[0m
[37m[1m[2023-07-11 08:49:33,197][233954] Min Reward on eval: -222.85106985564343[0m
[37m[1m[2023-07-11 08:49:33,197][233954] Mean Reward across all agents: 55.64040244574232[0m
[37m[1m[2023-07-11 08:49:33,197][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:49:33,201][233954] mean_value=-253.78041442430234, max_value=682.2654001107644[0m
[37m[1m[2023-07-11 08:49:33,203][233954] New mean coefficients: [[ 0.37202257  0.8905441  -0.8042916  -0.7840023   1.5712194  -3.9788072 ]][0m
[37m[1m[2023-07-11 08:49:33,204][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:49:42,255][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 08:49:42,255][233954] FPS: 424359.17[0m
[36m[2023-07-11 08:49:42,257][233954] itr=660, itrs=2000, Progress: 33.00%[0m
[37m[1m[2023-07-11 08:52:56,060][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000640[0m
[36m[2023-07-11 08:53:08,278][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 08:53:08,279][233954] FPS: 329753.30[0m
[36m[2023-07-11 08:53:12,433][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:53:12,433][233954] Reward + Measures: [[23.00772115  0.19545166  0.16742367  0.16165601  0.18636167  0.67472881]][0m
[37m[1m[2023-07-11 08:53:12,434][233954] Max Reward on eval: 23.007721150401085[0m
[37m[1m[2023-07-11 08:53:12,434][233954] Min Reward on eval: 23.007721150401085[0m
[37m[1m[2023-07-11 08:53:12,434][233954] Mean Reward across all agents: 23.007721150401085[0m
[37m[1m[2023-07-11 08:53:12,435][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:53:17,406][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:53:17,412][233954] Reward + Measures: [[123.08385446   0.1524       0.77820009   0.3545       0.70710003
    3.07316017]
 [ 14.67193325   0.86490005   0.9346       0.84769994   0.9163
    2.27654052]
 [-54.0291143    0.72399998   0.83500004   0.69499999   0.78040004
    2.86436462]
 ...
 [-23.75598494   0.0764       0.0742       0.066        0.10339999
    2.03668404]
 [281.16499066   0.64289999   0.69150001   0.25280002   0.61989999
    3.26312423]
 [191.52850652   0.62490004   0.66960001   0.2595       0.60690004
    3.4242065 ]][0m
[37m[1m[2023-07-11 08:53:17,412][233954] Max Reward on eval: 829.1128540090285[0m
[37m[1m[2023-07-11 08:53:17,412][233954] Min Reward on eval: -220.95176839022898[0m
[37m[1m[2023-07-11 08:53:17,412][233954] Mean Reward across all agents: 77.80627718744263[0m
[37m[1m[2023-07-11 08:53:17,413][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:53:17,417][233954] mean_value=-153.09019225136694, max_value=722.2393262625671[0m
[37m[1m[2023-07-11 08:53:17,420][233954] New mean coefficients: [[ 1.1958432  2.7690518 -0.5665796 -1.4293312  3.4706156 -1.0725772]][0m
[37m[1m[2023-07-11 08:53:17,420][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:53:26,473][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 08:53:26,474][233954] FPS: 424248.63[0m
[36m[2023-07-11 08:53:26,476][233954] itr=661, itrs=2000, Progress: 33.05%[0m
[36m[2023-07-11 08:53:38,185][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 08:53:38,185][233954] FPS: 330355.44[0m
[36m[2023-07-11 08:53:42,488][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:53:42,489][233954] Reward + Measures: [[22.00926912  0.25349864  0.15664366  0.14929733  0.22845066  0.68492603]][0m
[37m[1m[2023-07-11 08:53:42,489][233954] Max Reward on eval: 22.00926912324175[0m
[37m[1m[2023-07-11 08:53:42,489][233954] Min Reward on eval: 22.00926912324175[0m
[37m[1m[2023-07-11 08:53:42,490][233954] Mean Reward across all agents: 22.00926912324175[0m
[37m[1m[2023-07-11 08:53:42,490][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:53:47,483][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:53:47,484][233954] Reward + Measures: [[144.93613149   0.51140004   0.4901       0.43099996   0.56710005
    2.65306544]
 [ 12.97301074   0.3838       0.3682       0.3671       0.2235
    1.69868016]
 [150.0840478    0.11090001   0.38459998   0.40499997   0.3651
    3.39734077]
 ...
 [-33.78452126   0.5158       0.44730002   0.48920003   0.4729
    2.42891049]
 [ 21.6099162    0.48249999   0.41510001   0.4276       0.53599995
    2.72851658]
 [558.30336572   0.0159       0.98850006   0.80560011   0.98150009
    2.40898108]][0m
[37m[1m[2023-07-11 08:53:47,484][233954] Max Reward on eval: 669.1808013807051[0m
[37m[1m[2023-07-11 08:53:47,484][233954] Min Reward on eval: -109.4047909284709[0m
[37m[1m[2023-07-11 08:53:47,485][233954] Mean Reward across all agents: 111.60894022577595[0m
[37m[1m[2023-07-11 08:53:47,485][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:53:47,490][233954] mean_value=-152.57734114366178, max_value=646.7218686550325[0m
[37m[1m[2023-07-11 08:53:47,493][233954] New mean coefficients: [[ 2.850937    3.4293206  -0.06939179 -1.8470725   4.100555    0.63200665]][0m
[37m[1m[2023-07-11 08:53:47,494][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:53:56,536][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 08:53:56,536][233954] FPS: 424791.62[0m
[36m[2023-07-11 08:53:56,538][233954] itr=662, itrs=2000, Progress: 33.10%[0m
[36m[2023-07-11 08:54:08,157][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 08:54:08,157][233954] FPS: 332957.80[0m
[36m[2023-07-11 08:54:12,437][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:54:12,437][233954] Reward + Measures: [[25.7664553   0.27690899  0.16135566  0.146678    0.24783634  0.72508216]][0m
[37m[1m[2023-07-11 08:54:12,438][233954] Max Reward on eval: 25.76645530376694[0m
[37m[1m[2023-07-11 08:54:12,438][233954] Min Reward on eval: 25.76645530376694[0m
[37m[1m[2023-07-11 08:54:12,438][233954] Mean Reward across all agents: 25.76645530376694[0m
[37m[1m[2023-07-11 08:54:12,438][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:54:17,407][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:54:17,408][233954] Reward + Measures: [[-104.92723373    0.43860003    0.32810003    0.35690001    0.45590001
     2.59969974]
 [ -26.84968341    0.87709999    0.94770002    0.88490003    0.92490005
     2.8407886 ]
 [-165.69581879    0.4756        0.95380002    0.32660002    0.8998
     3.32203078]
 ...
 [ 175.10229778    0.25560001    0.6965        0.35970002    0.76870006
     2.9186933 ]
 [  -6.77541143    0.96259993    0.97589999    0.9478001     0.97840005
     2.65829086]
 [  82.14223923    0.2045        0.33510002    0.2744        0.37690002
     2.64488101]][0m
[37m[1m[2023-07-11 08:54:17,408][233954] Max Reward on eval: 602.2511596665718[0m
[37m[1m[2023-07-11 08:54:17,408][233954] Min Reward on eval: -165.695818786975[0m
[37m[1m[2023-07-11 08:54:17,408][233954] Mean Reward across all agents: 83.4144013965054[0m
[37m[1m[2023-07-11 08:54:17,408][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:54:17,414][233954] mean_value=-160.8639739077601, max_value=653.4317042939365[0m
[37m[1m[2023-07-11 08:54:17,416][233954] New mean coefficients: [[ 2.3137937   2.7055292   0.1912114  -1.8462217   3.142653   -0.33497882]][0m
[37m[1m[2023-07-11 08:54:17,417][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:54:26,352][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 08:54:26,352][233954] FPS: 429869.51[0m
[36m[2023-07-11 08:54:26,355][233954] itr=663, itrs=2000, Progress: 33.15%[0m
[36m[2023-07-11 08:54:38,052][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 08:54:38,052][233954] FPS: 330756.42[0m
[36m[2023-07-11 08:54:42,338][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:54:42,339][233954] Reward + Measures: [[35.03707231  0.26882067  0.16050401  0.14366567  0.244762    0.73985565]][0m
[37m[1m[2023-07-11 08:54:42,339][233954] Max Reward on eval: 35.037072312802565[0m
[37m[1m[2023-07-11 08:54:42,339][233954] Min Reward on eval: 35.037072312802565[0m
[37m[1m[2023-07-11 08:54:42,340][233954] Mean Reward across all agents: 35.037072312802565[0m
[37m[1m[2023-07-11 08:54:42,340][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:54:47,298][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:54:47,299][233954] Reward + Measures: [[ 47.10541057   0.2383       0.22260001   0.1736       0.23380001
    2.86157608]
 [ 42.88329409   0.16339999   0.46529999   0.24599998   0.43930003
    2.68487906]
 [158.27168465   0.1929       0.31170002   0.18930002   0.27610001
    2.71024299]
 ...
 [-18.20631739   0.17040001   0.2419       0.1594       0.2093
    2.52763319]
 [ 98.13222974   0.23380001   0.1243       0.21920002   0.2494
    2.36009288]
 [ 50.84228062   0.18629999   0.62709999   0.30050001   0.54890001
    2.15020347]][0m
[37m[1m[2023-07-11 08:54:47,299][233954] Max Reward on eval: 752.5183029118925[0m
[37m[1m[2023-07-11 08:54:47,299][233954] Min Reward on eval: -100.9351539105177[0m
[37m[1m[2023-07-11 08:54:47,300][233954] Mean Reward across all agents: 73.85897280130379[0m
[37m[1m[2023-07-11 08:54:47,300][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:54:47,304][233954] mean_value=-240.30380363370747, max_value=794.5373212857359[0m
[37m[1m[2023-07-11 08:54:47,307][233954] New mean coefficients: [[ 1.5846174   1.3446113   0.39658964 -1.1112537   1.7231405  -2.210707  ]][0m
[37m[1m[2023-07-11 08:54:47,308][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:54:56,365][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 08:54:56,365][233954] FPS: 424050.11[0m
[36m[2023-07-11 08:54:56,367][233954] itr=664, itrs=2000, Progress: 33.20%[0m
[36m[2023-07-11 08:55:08,139][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 08:55:08,140][233954] FPS: 328670.88[0m
[36m[2023-07-11 08:55:12,442][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:55:12,442][233954] Reward + Measures: [[38.19112446  0.28307331  0.17152701  0.14350133  0.26346099  0.76577425]][0m
[37m[1m[2023-07-11 08:55:12,442][233954] Max Reward on eval: 38.191124456895146[0m
[37m[1m[2023-07-11 08:55:12,443][233954] Min Reward on eval: 38.191124456895146[0m
[37m[1m[2023-07-11 08:55:12,443][233954] Mean Reward across all agents: 38.191124456895146[0m
[37m[1m[2023-07-11 08:55:12,443][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:55:17,403][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:55:17,404][233954] Reward + Measures: [[ 73.92813778   0.25470001   0.95779991   0.10960001   0.97770005
    2.94919372]
 [404.929842     0.44490004   0.8495       0.34329998   0.86900008
    2.44638228]
 [ -0.41422983   0.3012       0.48899999   0.24860001   0.44049999
    3.08487129]
 ...
 [406.20236393   0.26770002   0.96490002   0.49600002   0.93759996
    2.43613696]
 [ 82.89619764   0.31829998   0.44860002   0.24679999   0.30630001
    3.69201136]
 [-59.28858266   0.48260003   0.96350002   0.0377       0.96270001
    3.40462947]][0m
[37m[1m[2023-07-11 08:55:17,404][233954] Max Reward on eval: 565.4318637662567[0m
[37m[1m[2023-07-11 08:55:17,404][233954] Min Reward on eval: -176.9084883777541[0m
[37m[1m[2023-07-11 08:55:17,404][233954] Mean Reward across all agents: 82.78670450783396[0m
[37m[1m[2023-07-11 08:55:17,404][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:55:17,410][233954] mean_value=-76.8616620947229, max_value=457.7378303619605[0m
[37m[1m[2023-07-11 08:55:17,413][233954] New mean coefficients: [[ 1.8237538   1.1270857   0.45873174 -0.99453485  1.7662045  -2.3431215 ]][0m
[37m[1m[2023-07-11 08:55:17,414][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:55:26,365][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 08:55:26,365][233954] FPS: 429080.90[0m
[36m[2023-07-11 08:55:26,367][233954] itr=665, itrs=2000, Progress: 33.25%[0m
[36m[2023-07-11 08:55:37,855][233954] train() took 11.40 seconds to complete[0m
[36m[2023-07-11 08:55:37,855][233954] FPS: 336822.24[0m
[36m[2023-07-11 08:55:42,166][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:55:42,167][233954] Reward + Measures: [[43.5283247   0.287707    0.17595834  0.14639966  0.27157968  0.74817961]][0m
[37m[1m[2023-07-11 08:55:42,167][233954] Max Reward on eval: 43.528324695140384[0m
[37m[1m[2023-07-11 08:55:42,168][233954] Min Reward on eval: 43.528324695140384[0m
[37m[1m[2023-07-11 08:55:42,168][233954] Mean Reward across all agents: 43.528324695140384[0m
[37m[1m[2023-07-11 08:55:42,168][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:55:47,334][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:55:47,334][233954] Reward + Measures: [[  3.12865747   0.99090004   0.99430001   0.99329996   0.99440002
    3.11448383]
 [ -1.47184931   0.90080005   0.92760003   0.90009993   0.93409997
    2.34187961]
 [394.55814357   0.06820001   0.7044       0.3448       0.70789999
    3.08981633]
 ...
 [  4.59488477   0.87239999   0.95970005   0.88630003   0.98060006
    2.79180598]
 [476.9576159    0.0608       0.82550001   0.47209999   0.81639999
    3.4378159 ]
 [ 27.15060812   0.86860001   0.87349999   0.8617       0.88770002
    2.1801815 ]][0m
[37m[1m[2023-07-11 08:55:47,334][233954] Max Reward on eval: 533.2970199406379[0m
[37m[1m[2023-07-11 08:55:47,335][233954] Min Reward on eval: -252.80563811506144[0m
[37m[1m[2023-07-11 08:55:47,335][233954] Mean Reward across all agents: 88.7321236003673[0m
[37m[1m[2023-07-11 08:55:47,335][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:55:47,338][233954] mean_value=-203.36663453488003, max_value=488.157617029405[0m
[37m[1m[2023-07-11 08:55:47,341][233954] New mean coefficients: [[ 1.4079968   1.3249545   0.67182463 -1.2136232   1.7745733  -2.3315983 ]][0m
[37m[1m[2023-07-11 08:55:47,342][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:55:56,355][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 08:55:56,356][233954] FPS: 426090.03[0m
[36m[2023-07-11 08:55:56,358][233954] itr=666, itrs=2000, Progress: 33.30%[0m
[36m[2023-07-11 08:56:07,960][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 08:56:07,960][233954] FPS: 333496.80[0m
[36m[2023-07-11 08:56:12,232][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:56:12,232][233954] Reward + Measures: [[46.37065365  0.30505601  0.17388299  0.13431899  0.27461499  0.72929168]][0m
[37m[1m[2023-07-11 08:56:12,232][233954] Max Reward on eval: 46.37065364818607[0m
[37m[1m[2023-07-11 08:56:12,232][233954] Min Reward on eval: 46.37065364818607[0m
[37m[1m[2023-07-11 08:56:12,233][233954] Mean Reward across all agents: 46.37065364818607[0m
[37m[1m[2023-07-11 08:56:12,233][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:56:17,246][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:56:17,247][233954] Reward + Measures: [[  46.96974103    0.36950001    0.51010001    0.40739998    0.56110001
     2.2611692 ]
 [-123.00011899    0.95929998    0.95930004    0.94690001    0.95169991
     3.27344823]
 [ 180.39237076    0.12899999    0.66600001    0.35619998    0.63380003
     2.41240311]
 ...
 [  51.87409309    0.2           0.54769999    0.1803        0.43309999
     1.956321  ]
 [  41.83199034    0.08809999    0.12440001    0.0754        0.15650001
     2.09792924]
 [  59.57363852    0.1117        0.17380001    0.08639999    0.17230001
     2.49411631]][0m
[37m[1m[2023-07-11 08:56:17,247][233954] Max Reward on eval: 714.3552818653174[0m
[37m[1m[2023-07-11 08:56:17,247][233954] Min Reward on eval: -187.614066733839[0m
[37m[1m[2023-07-11 08:56:17,248][233954] Mean Reward across all agents: 94.0964217879871[0m
[37m[1m[2023-07-11 08:56:17,248][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:56:17,250][233954] mean_value=-228.2555605500693, max_value=491.95552924633955[0m
[37m[1m[2023-07-11 08:56:17,253][233954] New mean coefficients: [[ 0.7396286   0.26624513  0.0561021  -0.5903939   0.72270846 -4.3167596 ]][0m
[37m[1m[2023-07-11 08:56:17,254][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:56:26,248][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 08:56:26,249][233954] FPS: 427009.99[0m
[36m[2023-07-11 08:56:26,251][233954] itr=667, itrs=2000, Progress: 33.35%[0m
[36m[2023-07-11 08:56:37,800][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 08:56:37,800][233954] FPS: 334932.75[0m
[36m[2023-07-11 08:56:42,059][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:56:42,059][233954] Reward + Measures: [[46.87142538  0.29269367  0.178085    0.135617    0.27438265  0.74561018]][0m
[37m[1m[2023-07-11 08:56:42,059][233954] Max Reward on eval: 46.87142537620454[0m
[37m[1m[2023-07-11 08:56:42,060][233954] Min Reward on eval: 46.87142537620454[0m
[37m[1m[2023-07-11 08:56:42,060][233954] Mean Reward across all agents: 46.87142537620454[0m
[37m[1m[2023-07-11 08:56:42,060][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:56:47,020][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:56:47,021][233954] Reward + Measures: [[-15.90194466   0.14150001   0.85670006   0.38240001   0.85420001
    2.90944934]
 [ 74.62836506   0.41630003   0.68320006   0.252        0.72970003
    3.15822268]
 [119.04179367   0.1962       0.21140002   0.18000001   0.19240002
    3.17670679]
 ...
 [-21.27307081   0.3026       0.3594       0.28639999   0.30459997
    2.50389481]
 [ 58.67767047   0.17920001   0.95349997   0.35339999   0.95230001
    2.30818343]
 [ 24.8098378    0.31149998   0.36900002   0.39359999   0.35210001
    2.81563616]][0m
[37m[1m[2023-07-11 08:56:47,021][233954] Max Reward on eval: 413.5939718074631[0m
[37m[1m[2023-07-11 08:56:47,021][233954] Min Reward on eval: -267.5408790091984[0m
[37m[1m[2023-07-11 08:56:47,022][233954] Mean Reward across all agents: 83.83421850547025[0m
[37m[1m[2023-07-11 08:56:47,022][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:56:47,027][233954] mean_value=-157.0226857043417, max_value=560.8800725304126[0m
[37m[1m[2023-07-11 08:56:47,030][233954] New mean coefficients: [[ 1.3321545   0.58119535  0.5627376  -0.54123837  0.95314735 -3.3672976 ]][0m
[37m[1m[2023-07-11 08:56:47,031][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:56:55,990][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 08:56:55,991][233954] FPS: 428666.67[0m
[36m[2023-07-11 08:56:55,993][233954] itr=668, itrs=2000, Progress: 33.40%[0m
[36m[2023-07-11 08:57:07,678][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 08:57:07,678][233954] FPS: 331005.07[0m
[36m[2023-07-11 08:57:11,918][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:57:11,923][233954] Reward + Measures: [[48.96437043  0.31850833  0.18477     0.13253734  0.29178068  0.73843467]][0m
[37m[1m[2023-07-11 08:57:11,924][233954] Max Reward on eval: 48.96437043481412[0m
[37m[1m[2023-07-11 08:57:11,924][233954] Min Reward on eval: 48.96437043481412[0m
[37m[1m[2023-07-11 08:57:11,924][233954] Mean Reward across all agents: 48.96437043481412[0m
[37m[1m[2023-07-11 08:57:11,924][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:57:16,962][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:57:16,962][233954] Reward + Measures: [[ -6.25292033   0.45000002   0.36960003   0.38229999   0.40769997
    1.97383881]
 [-56.33207671   0.23290001   0.27420002   0.26320001   0.2811
    2.26148653]
 [ -6.86288822   0.99259996   0.99199992   0.9939       0.9957
    3.49151301]
 ...
 [-52.65701194   0.32010001   0.32800001   0.3206       0.34459996
    2.11940742]
 [ -7.07612586   0.99319994   0.99330008   0.99309999   0.9939
    3.67354846]
 [241.72776244   0.139        0.60000002   0.37559998   0.58490002
    2.80019665]][0m
[37m[1m[2023-07-11 08:57:16,962][233954] Max Reward on eval: 512.8377819117159[0m
[37m[1m[2023-07-11 08:57:16,963][233954] Min Reward on eval: -128.56978822412202[0m
[37m[1m[2023-07-11 08:57:16,963][233954] Mean Reward across all agents: 94.02284034040336[0m
[37m[1m[2023-07-11 08:57:16,963][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:57:16,967][233954] mean_value=-182.54405736375247, max_value=536.8398546314663[0m
[37m[1m[2023-07-11 08:57:16,969][233954] New mean coefficients: [[ 0.621426   -0.2020148   0.308643   -0.1752721   0.30096292 -4.4922204 ]][0m
[37m[1m[2023-07-11 08:57:16,970][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:57:25,978][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 08:57:25,978][233954] FPS: 426382.12[0m
[36m[2023-07-11 08:57:25,981][233954] itr=669, itrs=2000, Progress: 33.45%[0m
[36m[2023-07-11 08:57:37,663][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 08:57:37,663][233954] FPS: 331240.97[0m
[36m[2023-07-11 08:57:41,977][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:57:41,977][233954] Reward + Measures: [[44.10711708  0.30811366  0.19266766  0.14016099  0.27999431  0.73986888]][0m
[37m[1m[2023-07-11 08:57:41,978][233954] Max Reward on eval: 44.107117081616266[0m
[37m[1m[2023-07-11 08:57:41,978][233954] Min Reward on eval: 44.107117081616266[0m
[37m[1m[2023-07-11 08:57:41,978][233954] Mean Reward across all agents: 44.107117081616266[0m
[37m[1m[2023-07-11 08:57:41,978][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:57:46,952][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 08:57:46,952][233954] Reward + Measures: [[106.08574842   0.15049998   0.7148       0.43180004   0.67090005
    3.04097724]
 [ -8.13489833   0.10409999   0.87959999   0.4382       0.79930001
    3.11301303]
 [ 15.89807475   0.31119999   0.97910005   0.0773       0.98589993
    2.88681126]
 ...
 [ 35.62190806   0.24969999   0.48129997   0.22350001   0.39900002
    2.06553459]
 [ 41.27604205   0.41300002   0.94         0.0831       0.93850005
    3.09776235]
 [105.13944712   0.0805       0.22850001   0.17520002   0.28340003
    2.18079162]][0m
[37m[1m[2023-07-11 08:57:46,952][233954] Max Reward on eval: 781.6944008094258[0m
[37m[1m[2023-07-11 08:57:46,953][233954] Min Reward on eval: -143.32052040279376[0m
[37m[1m[2023-07-11 08:57:46,953][233954] Mean Reward across all agents: 128.09986830550977[0m
[37m[1m[2023-07-11 08:57:46,953][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 08:57:46,958][233954] mean_value=-190.38136964461194, max_value=624.275217874689[0m
[37m[1m[2023-07-11 08:57:46,960][233954] New mean coefficients: [[-1.5046955 -2.4854736 -0.6556356  0.7721465 -2.2757397 -8.198046 ]][0m
[37m[1m[2023-07-11 08:57:46,962][233954] Moving the mean solution point...[0m
[36m[2023-07-11 08:57:56,035][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 08:57:56,035][233954] FPS: 423306.71[0m
[36m[2023-07-11 08:57:56,037][233954] itr=670, itrs=2000, Progress: 33.50%[0m
[37m[1m[2023-07-11 09:01:15,063][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000650[0m
[36m[2023-07-11 09:01:27,351][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 09:01:27,351][233954] FPS: 328049.46[0m
[36m[2023-07-11 09:01:31,665][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:01:31,666][233954] Reward + Measures: [[41.48999363  0.301925    0.18829635  0.12990299  0.26748368  0.71259505]][0m
[37m[1m[2023-07-11 09:01:31,666][233954] Max Reward on eval: 41.48999362960346[0m
[37m[1m[2023-07-11 09:01:31,666][233954] Min Reward on eval: 41.48999362960346[0m
[37m[1m[2023-07-11 09:01:31,666][233954] Mean Reward across all agents: 41.48999362960346[0m
[37m[1m[2023-07-11 09:01:31,667][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:01:36,575][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:01:36,576][233954] Reward + Measures: [[201.44410323   0.123        0.85129994   0.54110003   0.81700003
    2.8210721 ]
 [253.51652814   0.1691       0.87470001   0.53170002   0.82849997
    3.36033106]
 [ 70.57983206   0.0804       0.1858       0.12730001   0.1856
    2.65273833]
 ...
 [-44.72007058   0.0624       0.088        0.09910001   0.1286
    3.48305702]
 [ 68.69088102   0.2158       0.69569999   0.1891       0.77939999
    2.41207957]
 [143.27581809   0.11059999   0.5244       0.28620002   0.45619997
    3.05245137]][0m
[37m[1m[2023-07-11 09:01:36,576][233954] Max Reward on eval: 477.2949638668448[0m
[37m[1m[2023-07-11 09:01:36,577][233954] Min Reward on eval: -145.82046032731887[0m
[37m[1m[2023-07-11 09:01:36,577][233954] Mean Reward across all agents: 130.82493661578323[0m
[37m[1m[2023-07-11 09:01:36,577][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:01:36,581][233954] mean_value=-134.63159664015865, max_value=678.1730426274753[0m
[37m[1m[2023-07-11 09:01:36,584][233954] New mean coefficients: [[-2.1575074 -2.7288785 -0.6422125  0.770886  -2.421759  -8.873065 ]][0m
[37m[1m[2023-07-11 09:01:36,585][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:01:45,527][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 09:01:45,527][233954] FPS: 429539.11[0m
[36m[2023-07-11 09:01:45,529][233954] itr=671, itrs=2000, Progress: 33.55%[0m
[36m[2023-07-11 09:01:57,194][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 09:01:57,194][233954] FPS: 331679.17[0m
[36m[2023-07-11 09:02:01,406][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:02:01,407][233954] Reward + Measures: [[36.188482    0.26880601  0.19141333  0.14232899  0.24806532  0.69375831]][0m
[37m[1m[2023-07-11 09:02:01,407][233954] Max Reward on eval: 36.18848200025501[0m
[37m[1m[2023-07-11 09:02:01,407][233954] Min Reward on eval: 36.18848200025501[0m
[37m[1m[2023-07-11 09:02:01,408][233954] Mean Reward across all agents: 36.18848200025501[0m
[37m[1m[2023-07-11 09:02:01,408][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:02:06,609][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:02:06,609][233954] Reward + Measures: [[  39.49447336    0.26540002    0.68460006    0.22880001    0.66590005
     2.41681838]
 [  69.02925958    0.1177        0.5262        0.1319        0.50400007
     1.99819183]
 [-119.83019036    0.50959998    0.87449998    0.2493        0.77509993
     2.61401176]
 ...
 [ -42.97560517    0.23509999    0.47300002    0.23480001    0.44690004
     3.41083527]
 [ 143.33324704    0.21900001    0.48070002    0.19490001    0.4743
     3.04865003]
 [ 210.11646784    0.34639999    0.42080003    0.06          0.40620002
     2.25152659]][0m
[37m[1m[2023-07-11 09:02:06,609][233954] Max Reward on eval: 333.74576977854593[0m
[37m[1m[2023-07-11 09:02:06,610][233954] Min Reward on eval: -139.2950981259346[0m
[37m[1m[2023-07-11 09:02:06,610][233954] Mean Reward across all agents: 51.210289264236955[0m
[37m[1m[2023-07-11 09:02:06,610][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:02:06,614][233954] mean_value=-136.9982112898254, max_value=437.439903670626[0m
[37m[1m[2023-07-11 09:02:06,617][233954] New mean coefficients: [[-2.6292307 -3.2842963 -1.5051545  1.2792706 -2.9054408 -9.993473 ]][0m
[37m[1m[2023-07-11 09:02:06,618][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:02:15,536][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 09:02:15,536][233954] FPS: 430697.13[0m
[36m[2023-07-11 09:02:15,538][233954] itr=672, itrs=2000, Progress: 33.60%[0m
[36m[2023-07-11 09:02:27,158][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 09:02:27,158][233954] FPS: 333004.24[0m
[36m[2023-07-11 09:02:31,481][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:02:31,481][233954] Reward + Measures: [[28.14447514  0.23963265  0.19708365  0.14953034  0.23041032  0.68746549]][0m
[37m[1m[2023-07-11 09:02:31,482][233954] Max Reward on eval: 28.14447513524311[0m
[37m[1m[2023-07-11 09:02:31,482][233954] Min Reward on eval: 28.14447513524311[0m
[37m[1m[2023-07-11 09:02:31,482][233954] Mean Reward across all agents: 28.14447513524311[0m
[37m[1m[2023-07-11 09:02:31,482][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:02:36,514][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:02:36,515][233954] Reward + Measures: [[-95.81209674   0.30609998   0.31819999   0.34489998   0.36750001
    2.26101303]
 [ -2.41657851   0.38859996   0.2472       0.40279999   0.4725
    3.06634593]
 [ 83.41101169   0.23240001   0.18600002   0.38710001   0.3829
    3.14705896]
 ...
 [  6.97685548   0.3195       0.43670002   0.29620001   0.42560002
    2.75639653]
 [159.3205793    0.36480004   0.23839998   0.36600003   0.27040002
    2.68779421]
 [106.52638769   0.41210005   0.1165       0.46779999   0.43659997
    3.39023066]][0m
[37m[1m[2023-07-11 09:02:36,515][233954] Max Reward on eval: 582.0257977992296[0m
[37m[1m[2023-07-11 09:02:36,515][233954] Min Reward on eval: -168.8568697287701[0m
[37m[1m[2023-07-11 09:02:36,515][233954] Mean Reward across all agents: 52.79779546013186[0m
[37m[1m[2023-07-11 09:02:36,516][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:02:36,519][233954] mean_value=-195.64979196712778, max_value=279.8013488605494[0m
[37m[1m[2023-07-11 09:02:36,522][233954] New mean coefficients: [[-1.0691141 -2.4382682 -0.832976   0.9584271 -2.2171888 -7.9758115]][0m
[37m[1m[2023-07-11 09:02:36,523][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:02:45,583][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 09:02:45,584][233954] FPS: 423884.61[0m
[36m[2023-07-11 09:02:45,586][233954] itr=673, itrs=2000, Progress: 33.65%[0m
[36m[2023-07-11 09:02:57,304][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 09:02:57,305][233954] FPS: 330135.91[0m
[36m[2023-07-11 09:03:01,646][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:03:01,647][233954] Reward + Measures: [[19.3240249   0.18098666  0.19780867  0.15779333  0.20176233  0.68440366]][0m
[37m[1m[2023-07-11 09:03:01,647][233954] Max Reward on eval: 19.324024904704128[0m
[37m[1m[2023-07-11 09:03:01,647][233954] Min Reward on eval: 19.324024904704128[0m
[37m[1m[2023-07-11 09:03:01,648][233954] Mean Reward across all agents: 19.324024904704128[0m
[37m[1m[2023-07-11 09:03:01,648][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:03:06,648][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:03:06,649][233954] Reward + Measures: [[-47.81034748   0.49559999   0.35679999   0.47399998   0.2411
    2.49169517]
 [-22.63393627   0.25849998   0.40000001   0.17460001   0.41160002
    1.72763479]
 [113.73012009   0.2507       0.3716       0.2333       0.37099999
    3.26519322]
 ...
 [749.00519947   0.0112       0.97869998   0.7277       0.98220009
    3.51794124]
 [ 74.54519682   0.18040001   0.0763       0.14899999   0.1901
    2.61311221]
 [ 51.34983477   0.54680002   0.43210003   0.5025       0.25360003
    2.22334599]][0m
[37m[1m[2023-07-11 09:03:06,649][233954] Max Reward on eval: 831.9096679670736[0m
[37m[1m[2023-07-11 09:03:06,649][233954] Min Reward on eval: -228.7670421613846[0m
[37m[1m[2023-07-11 09:03:06,650][233954] Mean Reward across all agents: 201.44265078771244[0m
[37m[1m[2023-07-11 09:03:06,650][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:03:06,654][233954] mean_value=-140.6567882538168, max_value=462.30145097070067[0m
[37m[1m[2023-07-11 09:03:06,657][233954] New mean coefficients: [[-1.7071621 -3.171578  -1.8299794  1.0327897 -2.9734278 -9.483172 ]][0m
[37m[1m[2023-07-11 09:03:06,658][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:03:15,674][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 09:03:15,675][233954] FPS: 425962.38[0m
[36m[2023-07-11 09:03:15,677][233954] itr=674, itrs=2000, Progress: 33.70%[0m
[36m[2023-07-11 09:03:27,382][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 09:03:27,383][233954] FPS: 330542.37[0m
[36m[2023-07-11 09:03:31,600][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:03:31,600][233954] Reward + Measures: [[18.34170762  0.15886267  0.20884199  0.17023768  0.2012943   0.67603832]][0m
[37m[1m[2023-07-11 09:03:31,601][233954] Max Reward on eval: 18.34170761533132[0m
[37m[1m[2023-07-11 09:03:31,601][233954] Min Reward on eval: 18.34170761533132[0m
[37m[1m[2023-07-11 09:03:31,601][233954] Mean Reward across all agents: 18.34170761533132[0m
[37m[1m[2023-07-11 09:03:31,601][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:03:36,537][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:03:36,537][233954] Reward + Measures: [[-67.78957036   0.15910001   0.74879998   0.2376       0.74860001
    2.65830946]
 [-22.67032596   0.35040003   0.38820001   0.2148       0.4598
    2.90616012]
 [ 21.12376444   0.0776       0.83640003   0.4316       0.83240002
    2.99477839]
 ...
 [ 17.77626136   0.14600001   0.16050002   0.12959999   0.20089999
    1.79613721]
 [ 15.34208831   0.0527       0.89969999   0.52089995   0.91030008
    3.12065101]
 [-89.42567921   0.18840002   0.55039996   0.2086       0.54349995
    2.8227613 ]][0m
[37m[1m[2023-07-11 09:03:36,537][233954] Max Reward on eval: 515.6506409696303[0m
[37m[1m[2023-07-11 09:03:36,538][233954] Min Reward on eval: -354.93083637347445[0m
[37m[1m[2023-07-11 09:03:36,538][233954] Mean Reward across all agents: 6.105187383412706[0m
[37m[1m[2023-07-11 09:03:36,538][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:03:36,540][233954] mean_value=-267.11231356723295, max_value=278.6599325710496[0m
[37m[1m[2023-07-11 09:03:36,543][233954] New mean coefficients: [[ 0.03766632 -1.4665518  -0.99528277  0.42788672 -1.2997382  -6.3932357 ]][0m
[37m[1m[2023-07-11 09:03:36,544][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:03:45,623][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 09:03:45,624][233954] FPS: 423004.25[0m
[36m[2023-07-11 09:03:45,626][233954] itr=675, itrs=2000, Progress: 33.75%[0m
[36m[2023-07-11 09:03:57,317][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 09:03:57,317][233954] FPS: 330981.46[0m
[36m[2023-07-11 09:04:01,551][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:04:01,551][233954] Reward + Measures: [[11.31486044  0.152705    0.188585    0.158953    0.18870601  0.65677333]][0m
[37m[1m[2023-07-11 09:04:01,551][233954] Max Reward on eval: 11.31486043954633[0m
[37m[1m[2023-07-11 09:04:01,552][233954] Min Reward on eval: 11.31486043954633[0m
[37m[1m[2023-07-11 09:04:01,552][233954] Mean Reward across all agents: 11.31486043954633[0m
[37m[1m[2023-07-11 09:04:01,552][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:04:06,517][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:04:06,518][233954] Reward + Measures: [[104.86030004   0.1664       0.94849998   0.16870001   0.93969995
    2.95508552]
 [185.15067421   0.20869999   0.62760001   0.53640002   0.70560008
    3.34189415]
 [424.35520131   0.04190001   0.85750002   0.59510005   0.84619999
    3.03466105]
 ...
 [155.26372013   0.0765       0.40470001   0.21470001   0.4409
    2.61387944]
 [565.25609971   0.0931       0.912        0.65070003   0.91530001
    3.39941382]
 [ 65.59535428   0.64490002   0.73459995   0.62960005   0.67580003
    2.17279291]][0m
[37m[1m[2023-07-11 09:04:06,518][233954] Max Reward on eval: 687.4263534698636[0m
[37m[1m[2023-07-11 09:04:06,518][233954] Min Reward on eval: -116.23169135863427[0m
[37m[1m[2023-07-11 09:04:06,519][233954] Mean Reward across all agents: 150.68384935192657[0m
[37m[1m[2023-07-11 09:04:06,519][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:04:06,524][233954] mean_value=-156.64588395721833, max_value=620.5619055432021[0m
[37m[1m[2023-07-11 09:04:06,527][233954] New mean coefficients: [[ 0.2073263  -0.9713894  -0.6976364  -0.01192123 -1.0319375  -5.481561  ]][0m
[37m[1m[2023-07-11 09:04:06,528][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:04:15,471][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 09:04:15,471][233954] FPS: 429438.66[0m
[36m[2023-07-11 09:04:15,474][233954] itr=676, itrs=2000, Progress: 33.80%[0m
[36m[2023-07-11 09:04:27,055][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 09:04:27,056][233954] FPS: 333990.31[0m
[36m[2023-07-11 09:04:31,366][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:04:31,367][233954] Reward + Measures: [[17.06119718  0.15276466  0.16559133  0.14587566  0.17811801  0.62232667]][0m
[37m[1m[2023-07-11 09:04:31,367][233954] Max Reward on eval: 17.061197184058383[0m
[37m[1m[2023-07-11 09:04:31,367][233954] Min Reward on eval: 17.061197184058383[0m
[37m[1m[2023-07-11 09:04:31,367][233954] Mean Reward across all agents: 17.061197184058383[0m
[37m[1m[2023-07-11 09:04:31,368][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:04:36,347][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:04:36,347][233954] Reward + Measures: [[-5.48787301  0.5844      0.66230005  0.51050001  0.53119999  2.52479482]
 [-6.0947837   0.40260002  0.41479999  0.37660003  0.30649999  2.7640059 ]
 [61.43903743  0.1956      0.7367      0.1754      0.66409999  1.98527586]
 ...
 [-6.10900243  0.51139998  0.55470002  0.51340002  0.39940003  2.72963023]
 [68.0696636   0.4894      0.60059994  0.41859999  0.54940003  2.89454579]
 [48.730209    0.0928      0.1788      0.1613      0.20299999  2.33985925]][0m
[37m[1m[2023-07-11 09:04:36,347][233954] Max Reward on eval: 478.65610588286074[0m
[37m[1m[2023-07-11 09:04:36,348][233954] Min Reward on eval: -266.75352290757[0m
[37m[1m[2023-07-11 09:04:36,348][233954] Mean Reward across all agents: 40.51422010443114[0m
[37m[1m[2023-07-11 09:04:36,348][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:04:36,352][233954] mean_value=-252.95663289907384, max_value=659.8377283151076[0m
[37m[1m[2023-07-11 09:04:36,355][233954] New mean coefficients: [[-0.01268794 -0.54552543 -0.6783659  -0.32362485 -0.5389192  -4.9521246 ]][0m
[37m[1m[2023-07-11 09:04:36,356][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:04:45,383][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 09:04:45,384][233954] FPS: 425431.44[0m
[36m[2023-07-11 09:04:45,386][233954] itr=677, itrs=2000, Progress: 33.85%[0m
[36m[2023-07-11 09:04:57,257][233954] train() took 11.78 seconds to complete[0m
[36m[2023-07-11 09:04:57,257][233954] FPS: 325922.21[0m
[36m[2023-07-11 09:05:01,549][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:05:01,555][233954] Reward + Measures: [[14.4574704   0.15582399  0.17422432  0.15492468  0.18410867  0.63229012]][0m
[37m[1m[2023-07-11 09:05:01,555][233954] Max Reward on eval: 14.457470404932431[0m
[37m[1m[2023-07-11 09:05:01,555][233954] Min Reward on eval: 14.457470404932431[0m
[37m[1m[2023-07-11 09:05:01,556][233954] Mean Reward across all agents: 14.457470404932431[0m
[37m[1m[2023-07-11 09:05:01,556][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:05:06,746][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:05:06,752][233954] Reward + Measures: [[149.98993825   0.42430001   0.70370001   0.44860002   0.72510004
    3.13421774]
 [ 85.64702009   0.51669997   0.70110005   0.47599998   0.71250004
    2.81415415]
 [ 25.74103439   0.91580003   0.93149996   0.9012       0.90880007
    3.32213902]
 ...
 [492.66104986   0.0454       0.84930003   0.78410006   0.85869998
    2.59843802]
 [ 90.54817556   0.15000002   0.43120003   0.2536       0.47799999
    2.37279487]
 [101.46124012   0.13800001   0.4549       0.17190002   0.3664
    3.0656426 ]][0m
[37m[1m[2023-07-11 09:05:06,752][233954] Max Reward on eval: 740.337474803254[0m
[37m[1m[2023-07-11 09:05:06,752][233954] Min Reward on eval: -245.07705533625557[0m
[37m[1m[2023-07-11 09:05:06,752][233954] Mean Reward across all agents: 103.90237224630118[0m
[37m[1m[2023-07-11 09:05:06,752][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:05:06,759][233954] mean_value=-122.67384685441229, max_value=628.7818092614989[0m
[37m[1m[2023-07-11 09:05:06,761][233954] New mean coefficients: [[-0.6870221  -0.84118295 -1.851228   -0.23604524 -0.8454393  -5.7079005 ]][0m
[37m[1m[2023-07-11 09:05:06,762][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:05:15,744][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 09:05:15,744][233954] FPS: 427629.83[0m
[36m[2023-07-11 09:05:15,746][233954] itr=678, itrs=2000, Progress: 33.90%[0m
[36m[2023-07-11 09:05:27,501][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 09:05:27,501][233954] FPS: 329060.01[0m
[36m[2023-07-11 09:05:31,804][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:05:31,809][233954] Reward + Measures: [[11.39001603  0.16023166  0.17081468  0.15060566  0.18456501  0.63801521]][0m
[37m[1m[2023-07-11 09:05:31,809][233954] Max Reward on eval: 11.39001602962028[0m
[37m[1m[2023-07-11 09:05:31,809][233954] Min Reward on eval: 11.39001602962028[0m
[37m[1m[2023-07-11 09:05:31,810][233954] Mean Reward across all agents: 11.39001602962028[0m
[37m[1m[2023-07-11 09:05:31,810][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:05:36,768][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:05:36,768][233954] Reward + Measures: [[138.36167024   0.18810001   0.46199998   0.18889999   0.51370001
    2.865242  ]
 [-54.11692724   0.31620002   0.3673       0.352        0.30430004
    2.41222572]
 [ 57.43071852   0.0821       0.88339996   0.46919999   0.84679997
    3.01193309]
 ...
 [109.88206386   0.17900001   0.87729996   0.33489999   0.79140002
    2.51273346]
 [350.1796792    0.19350001   0.73140001   0.36000004   0.73610002
    3.08048034]
 [227.38723755   0.19589999   0.7252       0.32940003   0.75
    2.8296771 ]][0m
[37m[1m[2023-07-11 09:05:36,768][233954] Max Reward on eval: 750.0557365171611[0m
[37m[1m[2023-07-11 09:05:36,769][233954] Min Reward on eval: -722.1156692724675[0m
[37m[1m[2023-07-11 09:05:36,769][233954] Mean Reward across all agents: 116.9374086891677[0m
[37m[1m[2023-07-11 09:05:36,769][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:05:36,772][233954] mean_value=-173.67048847086875, max_value=356.86727677728[0m
[37m[1m[2023-07-11 09:05:36,775][233954] New mean coefficients: [[-2.0079997  -2.015914   -2.7323475   0.25720322 -2.0044498  -7.7645597 ]][0m
[37m[1m[2023-07-11 09:05:36,776][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:05:45,717][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 09:05:45,718][233954] FPS: 429528.62[0m
[36m[2023-07-11 09:05:45,720][233954] itr=679, itrs=2000, Progress: 33.95%[0m
[36m[2023-07-11 09:05:57,388][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 09:05:57,389][233954] FPS: 331520.72[0m
[36m[2023-07-11 09:06:01,632][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:06:01,637][233954] Reward + Measures: [[12.15169003  0.16575666  0.168393    0.15323301  0.18574068  0.61915654]][0m
[37m[1m[2023-07-11 09:06:01,638][233954] Max Reward on eval: 12.151690026060145[0m
[37m[1m[2023-07-11 09:06:01,638][233954] Min Reward on eval: 12.151690026060145[0m
[37m[1m[2023-07-11 09:06:01,638][233954] Mean Reward across all agents: 12.151690026060145[0m
[37m[1m[2023-07-11 09:06:01,639][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:06:06,692][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:06:06,698][233954] Reward + Measures: [[-167.3167074     0.27690002    0.45539999    0.28440002    0.5187
     2.02181911]
 [-183.06797906    0.2307        0.21640001    0.24590002    0.23200002
     2.2582047 ]
 [ 116.26444239    0.22230001    0.15930001    0.233         0.25980002
     2.39379621]
 ...
 [  -7.50295695    0.15530001    0.17730001    0.17220001    0.2256
     2.98075485]
 [ -14.03204584    0.1806        0.17470001    0.18599999    0.178
     3.11212397]
 [-153.96410559    0.13090001    0.15640001    0.152         0.1631
     3.13616943]][0m
[37m[1m[2023-07-11 09:06:06,698][233954] Max Reward on eval: 731.4110298342072[0m
[37m[1m[2023-07-11 09:06:06,698][233954] Min Reward on eval: -248.6602153477026[0m
[37m[1m[2023-07-11 09:06:06,699][233954] Mean Reward across all agents: 3.410330686518066[0m
[37m[1m[2023-07-11 09:06:06,699][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:06:06,702][233954] mean_value=-316.0962746607692, max_value=485.1869601717684[0m
[37m[1m[2023-07-11 09:06:06,705][233954] New mean coefficients: [[-0.42270064 -0.24699855 -2.417477   -0.32025337  0.0304656  -4.7815957 ]][0m
[37m[1m[2023-07-11 09:06:06,706][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:06:15,707][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 09:06:15,707][233954] FPS: 426679.69[0m
[36m[2023-07-11 09:06:15,710][233954] itr=680, itrs=2000, Progress: 34.00%[0m
[37m[1m[2023-07-11 09:09:40,445][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000660[0m
[36m[2023-07-11 09:09:52,618][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 09:09:52,618][233954] FPS: 330982.47[0m
[36m[2023-07-11 09:09:56,796][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:09:56,797][233954] Reward + Measures: [[15.61078065  0.17501265  0.14987867  0.14527799  0.17743367  0.59438962]][0m
[37m[1m[2023-07-11 09:09:56,797][233954] Max Reward on eval: 15.61078064762217[0m
[37m[1m[2023-07-11 09:09:56,797][233954] Min Reward on eval: 15.61078064762217[0m
[37m[1m[2023-07-11 09:09:56,797][233954] Mean Reward across all agents: 15.61078064762217[0m
[37m[1m[2023-07-11 09:09:56,798][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:10:01,756][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:10:01,757][233954] Reward + Measures: [[  86.11112263    0.43670002    0.52039999    0.40910003    0.51669997
     3.04917789]
 [ -18.86773076    0.73299998    0.49120003    0.58350003    0.1127
     2.72586584]
 [ 255.86997534    0.10469999    0.46070004    0.26019999    0.46760002
     1.86865163]
 ...
 [-132.52211965    0.4402        0.8714        0.46079999    0.90340006
     3.34001517]
 [ -29.40396203    0.2705        0.41159996    0.23369999    0.34259999
     2.62191463]
 [ 175.68852211    0.0587        0.45660001    0.29239997    0.4535
     2.51019788]][0m
[37m[1m[2023-07-11 09:10:01,757][233954] Max Reward on eval: 802.4373550408636[0m
[37m[1m[2023-07-11 09:10:01,757][233954] Min Reward on eval: -167.90655167941003[0m
[37m[1m[2023-07-11 09:10:01,757][233954] Mean Reward across all agents: 96.15574782671413[0m
[37m[1m[2023-07-11 09:10:01,758][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:10:01,763][233954] mean_value=-171.31477068844387, max_value=632.9530176933855[0m
[37m[1m[2023-07-11 09:10:01,765][233954] New mean coefficients: [[-0.566371    0.78089356 -2.2476099  -0.75016075  1.047195   -3.341542  ]][0m
[37m[1m[2023-07-11 09:10:01,766][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:10:10,733][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 09:10:10,733][233954] FPS: 428327.94[0m
[36m[2023-07-11 09:10:10,736][233954] itr=681, itrs=2000, Progress: 34.05%[0m
[36m[2023-07-11 09:10:22,230][233954] train() took 11.41 seconds to complete[0m
[36m[2023-07-11 09:10:22,230][233954] FPS: 336575.01[0m
[36m[2023-07-11 09:10:26,489][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:10:26,495][233954] Reward + Measures: [[15.51996582  0.19896266  0.13642     0.13946468  0.17990966  0.57764101]][0m
[37m[1m[2023-07-11 09:10:26,495][233954] Max Reward on eval: 15.519965815135162[0m
[37m[1m[2023-07-11 09:10:26,496][233954] Min Reward on eval: 15.519965815135162[0m
[37m[1m[2023-07-11 09:10:26,496][233954] Mean Reward across all agents: 15.519965815135162[0m
[37m[1m[2023-07-11 09:10:26,496][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:10:31,473][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:10:31,474][233954] Reward + Measures: [[ 48.68233056   0.0795       0.14390001   0.10090001   0.13940001
    2.22037172]
 [154.38464797   0.0883       0.41750002   0.2757       0.44690004
    2.19574499]
 [202.18584633   0.3784       0.49359998   0.3159       0.51130003
    2.35351729]
 ...
 [ 36.49309596   0.2016       0.44500002   0.19400001   0.48979998
    2.73848987]
 [311.47188763   0.16940001   0.87560004   0.4443       0.8872
    2.74119163]
 [ 21.66603317   0.22830001   0.30320001   0.23959999   0.26030001
    2.72409892]][0m
[37m[1m[2023-07-11 09:10:31,474][233954] Max Reward on eval: 751.7623214310036[0m
[37m[1m[2023-07-11 09:10:31,474][233954] Min Reward on eval: -320.0846501444932[0m
[37m[1m[2023-07-11 09:10:31,474][233954] Mean Reward across all agents: 126.3782470597774[0m
[37m[1m[2023-07-11 09:10:31,475][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:10:31,478][233954] mean_value=-211.15873857222925, max_value=652.0852321991697[0m
[37m[1m[2023-07-11 09:10:31,481][233954] New mean coefficients: [[-0.44945535  0.8234546  -2.1232495  -0.53376025  0.9557061  -3.1506696 ]][0m
[37m[1m[2023-07-11 09:10:31,482][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:10:40,427][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 09:10:40,427][233954] FPS: 429365.62[0m
[36m[2023-07-11 09:10:40,429][233954] itr=682, itrs=2000, Progress: 34.10%[0m
[36m[2023-07-11 09:10:52,011][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 09:10:52,011][233954] FPS: 334032.52[0m
[36m[2023-07-11 09:10:56,265][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:10:56,270][233954] Reward + Measures: [[15.70801842  0.22921035  0.13122433  0.13886733  0.19308934  0.57654053]][0m
[37m[1m[2023-07-11 09:10:56,271][233954] Max Reward on eval: 15.708018421074298[0m
[37m[1m[2023-07-11 09:10:56,271][233954] Min Reward on eval: 15.708018421074298[0m
[37m[1m[2023-07-11 09:10:56,271][233954] Mean Reward across all agents: 15.708018421074298[0m
[37m[1m[2023-07-11 09:10:56,272][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:11:01,282][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:11:01,283][233954] Reward + Measures: [[-45.07821015   0.44480005   0.41890001   0.44780001   0.40349999
    2.14183331]
 [ 45.55452399   0.57750005   0.29790002   0.5377       0.24150001
    2.50270724]
 [182.9761907    0.0656       0.49229994   0.32069999   0.46540004
    3.01906061]
 ...
 [ -2.24351691   0.60250002   0.42089996   0.64209998   0.3414
    2.08834171]
 [ -6.50410971   0.19760001   0.67100006   0.1505       0.64779997
    1.95432401]
 [ 89.44252032   0.1452       0.14920001   0.1463       0.2464
    2.84280968]][0m
[37m[1m[2023-07-11 09:11:01,283][233954] Max Reward on eval: 613.617254580208[0m
[37m[1m[2023-07-11 09:11:01,283][233954] Min Reward on eval: -209.2095217765309[0m
[37m[1m[2023-07-11 09:11:01,283][233954] Mean Reward across all agents: 55.63209221809163[0m
[37m[1m[2023-07-11 09:11:01,284][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:11:01,287][233954] mean_value=-185.4599293043414, max_value=631.8584762776713[0m
[37m[1m[2023-07-11 09:11:01,290][233954] New mean coefficients: [[-1.0133644   0.73668504 -2.119258    0.12131596  0.6504098  -3.6610646 ]][0m
[37m[1m[2023-07-11 09:11:01,291][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:11:10,301][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 09:11:10,302][233954] FPS: 426247.52[0m
[36m[2023-07-11 09:11:10,304][233954] itr=683, itrs=2000, Progress: 34.15%[0m
[36m[2023-07-11 09:11:22,003][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 09:11:22,003][233954] FPS: 330704.56[0m
[36m[2023-07-11 09:11:26,256][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:11:26,256][233954] Reward + Measures: [[16.35544372  0.23982364  0.11519867  0.12641433  0.20082369  0.57986742]][0m
[37m[1m[2023-07-11 09:11:26,256][233954] Max Reward on eval: 16.355443716107985[0m
[37m[1m[2023-07-11 09:11:26,257][233954] Min Reward on eval: 16.355443716107985[0m
[37m[1m[2023-07-11 09:11:26,257][233954] Mean Reward across all agents: 16.355443716107985[0m
[37m[1m[2023-07-11 09:11:26,257][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:11:31,326][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:11:31,332][233954] Reward + Measures: [[415.63092997   0.1811       0.92500001   0.73980004   0.83760005
    2.68521476]
 [ 96.68723581   0.815        0.75650001   0.83199996   0.68650001
    2.51156592]
 [  4.00198102   0.2642       0.4434       0.18620001   0.47030002
    2.38958716]
 ...
 [ -0.98774083   0.48780003   0.35750002   0.42799997   0.32519999
    2.2472074 ]
 [102.26471453   0.19770001   0.23340002   0.19630001   0.2658
    2.698843  ]
 [  0.45387485   0.75959998   0.83640003   0.7159       0.77720004
    2.05271316]][0m
[37m[1m[2023-07-11 09:11:31,332][233954] Max Reward on eval: 736.9426345847547[0m
[37m[1m[2023-07-11 09:11:31,333][233954] Min Reward on eval: -246.87199777550995[0m
[37m[1m[2023-07-11 09:11:31,333][233954] Mean Reward across all agents: 143.07042045152454[0m
[37m[1m[2023-07-11 09:11:31,333][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:11:31,336][233954] mean_value=-198.73935724237546, max_value=358.120130696201[0m
[37m[1m[2023-07-11 09:11:31,338][233954] New mean coefficients: [[-1.0163579   0.5767482  -1.8500268   0.23734403  0.55799294 -4.0162015 ]][0m
[37m[1m[2023-07-11 09:11:31,339][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:11:40,364][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 09:11:40,365][233954] FPS: 425556.06[0m
[36m[2023-07-11 09:11:40,367][233954] itr=684, itrs=2000, Progress: 34.20%[0m
[36m[2023-07-11 09:11:52,080][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 09:11:52,081][233954] FPS: 330209.46[0m
[36m[2023-07-11 09:11:56,371][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:11:56,371][233954] Reward + Measures: [[17.14067342  0.25471765  0.116299    0.12946734  0.20737167  0.56740451]][0m
[37m[1m[2023-07-11 09:11:56,372][233954] Max Reward on eval: 17.140673415141453[0m
[37m[1m[2023-07-11 09:11:56,372][233954] Min Reward on eval: 17.140673415141453[0m
[37m[1m[2023-07-11 09:11:56,372][233954] Mean Reward across all agents: 17.140673415141453[0m
[37m[1m[2023-07-11 09:11:56,372][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:12:01,654][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:12:01,660][233954] Reward + Measures: [[-24.22842163   0.35190001   0.47609997   0.28790003   0.39059997
    2.29107213]
 [ -1.51191051   0.7295       0.79699999   0.72070003   0.82639998
    2.90253496]
 [-20.69012186   0.178        0.2773       0.0637       0.2316
    1.43132532]
 ...
 [ 13.88428386   0.74150002   0.84709996   0.74610007   0.87519991
    2.37918353]
 [  8.83931209   0.51840001   0.49200001   0.45210004   0.52400005
    3.53272486]
 [154.90728078   0.1207       0.34559998   0.30749997   0.36430001
    2.66259551]][0m
[37m[1m[2023-07-11 09:12:01,660][233954] Max Reward on eval: 713.8081360014155[0m
[37m[1m[2023-07-11 09:12:01,660][233954] Min Reward on eval: -238.69090348584578[0m
[37m[1m[2023-07-11 09:12:01,661][233954] Mean Reward across all agents: 66.39428287756947[0m
[37m[1m[2023-07-11 09:12:01,661][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:12:01,664][233954] mean_value=-221.32925553643398, max_value=450.52290431208905[0m
[37m[1m[2023-07-11 09:12:01,667][233954] New mean coefficients: [[-1.1026767  -0.0121696  -1.5445818   0.3842184  -0.09369612 -4.645123  ]][0m
[37m[1m[2023-07-11 09:12:01,668][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:12:10,740][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 09:12:10,741][233954] FPS: 423319.67[0m
[36m[2023-07-11 09:12:10,743][233954] itr=685, itrs=2000, Progress: 34.25%[0m
[36m[2023-07-11 09:12:22,532][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 09:12:22,532][233954] FPS: 328135.08[0m
[36m[2023-07-11 09:12:26,842][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:12:26,847][233954] Reward + Measures: [[16.44161523  0.26401797  0.12302165  0.147745    0.21388899  0.55102897]][0m
[37m[1m[2023-07-11 09:12:26,847][233954] Max Reward on eval: 16.441615233525575[0m
[37m[1m[2023-07-11 09:12:26,848][233954] Min Reward on eval: 16.441615233525575[0m
[37m[1m[2023-07-11 09:12:26,848][233954] Mean Reward across all agents: 16.441615233525575[0m
[37m[1m[2023-07-11 09:12:26,848][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:12:31,900][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:12:31,906][233954] Reward + Measures: [[ 72.36682326   0.6882       0.7651       0.60390002   0.4824
    2.66334057]
 [499.69854928   0.0092       0.99020004   0.76119995   0.98999995
    2.85303187]
 [ 24.79909155   0.54590005   0.56689996   0.60190004   0.59009999
    2.78602099]
 ...
 [ 51.82503346   0.259        0.86700004   0.27310002   0.84600002
    2.59755635]
 [ 22.06584698   0.27760002   0.44579998   0.23140001   0.40689999
    1.79672432]
 [277.20214363   0.1908       0.65070003   0.4059       0.65400004
    3.41506124]][0m
[37m[1m[2023-07-11 09:12:31,906][233954] Max Reward on eval: 700.9177780033555[0m
[37m[1m[2023-07-11 09:12:31,906][233954] Min Reward on eval: -135.41346356981666[0m
[37m[1m[2023-07-11 09:12:31,906][233954] Mean Reward across all agents: 99.35518904922691[0m
[37m[1m[2023-07-11 09:12:31,907][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:12:31,912][233954] mean_value=-160.01333722066602, max_value=535.493213245418[0m
[37m[1m[2023-07-11 09:12:31,915][233954] New mean coefficients: [[-0.87882954 -0.3250208  -0.91839224  0.21493804 -0.7428534  -4.8566957 ]][0m
[37m[1m[2023-07-11 09:12:31,916][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:12:40,925][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 09:12:40,925][233954] FPS: 426319.81[0m
[36m[2023-07-11 09:12:40,927][233954] itr=686, itrs=2000, Progress: 34.30%[0m
[36m[2023-07-11 09:12:52,777][233954] train() took 11.76 seconds to complete[0m
[36m[2023-07-11 09:12:52,777][233954] FPS: 326447.84[0m
[36m[2023-07-11 09:12:57,077][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:12:57,077][233954] Reward + Measures: [[17.32301848  0.24716634  0.11903501  0.13707733  0.19537365  0.54494077]][0m
[37m[1m[2023-07-11 09:12:57,077][233954] Max Reward on eval: 17.323018477090226[0m
[37m[1m[2023-07-11 09:12:57,078][233954] Min Reward on eval: 17.323018477090226[0m
[37m[1m[2023-07-11 09:12:57,078][233954] Mean Reward across all agents: 17.323018477090226[0m
[37m[1m[2023-07-11 09:12:57,078][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:13:02,078][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:13:02,083][233954] Reward + Measures: [[  87.73737643    0.50809997    0.1429        0.52560002    0.42490003
     2.84893012]
 [-106.251054      0.6044001     0.49160004    0.60320002    0.52870005
     2.67083907]
 [-120.75970697    0.49959999    0.40420005    0.4542        0.48590001
     2.30868888]
 ...
 [  13.89616018    0.16540001    0.55599993    0.1337        0.45230004
     2.36567283]
 [  85.21703843    0.17819999    0.1184        0.13380001    0.21859999
     2.47399116]
 [ 143.29327246    0.1522        0.41750002    0.18440001    0.39520001
     2.28382635]][0m
[37m[1m[2023-07-11 09:13:02,084][233954] Max Reward on eval: 645.3941269075498[0m
[37m[1m[2023-07-11 09:13:02,084][233954] Min Reward on eval: -206.57874000363518[0m
[37m[1m[2023-07-11 09:13:02,084][233954] Mean Reward across all agents: 84.88006687714757[0m
[37m[1m[2023-07-11 09:13:02,085][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:13:02,088][233954] mean_value=-239.3588654916243, max_value=569.2066495724954[0m
[37m[1m[2023-07-11 09:13:02,090][233954] New mean coefficients: [[-1.1686296  -0.26796103 -0.8876436   0.42340434 -0.35186815 -5.186063  ]][0m
[37m[1m[2023-07-11 09:13:02,091][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:13:11,078][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 09:13:11,078][233954] FPS: 427381.01[0m
[36m[2023-07-11 09:13:11,081][233954] itr=687, itrs=2000, Progress: 34.35%[0m
[36m[2023-07-11 09:13:22,839][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 09:13:22,840][233954] FPS: 328958.67[0m
[36m[2023-07-11 09:13:27,071][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:13:27,071][233954] Reward + Measures: [[13.53808885  0.22283699  0.12216267  0.14505701  0.18367232  0.54406983]][0m
[37m[1m[2023-07-11 09:13:27,071][233954] Max Reward on eval: 13.538088848779793[0m
[37m[1m[2023-07-11 09:13:27,072][233954] Min Reward on eval: 13.538088848779793[0m
[37m[1m[2023-07-11 09:13:27,072][233954] Mean Reward across all agents: 13.538088848779793[0m
[37m[1m[2023-07-11 09:13:27,072][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:13:32,015][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:13:32,016][233954] Reward + Measures: [[318.14461173   0.0442       0.69220001   0.50690001   0.69850004
    2.58278346]
 [233.75472877   0.1376       0.63309997   0.46149999   0.58219999
    2.05269814]
 [290.18536437   0.0382       0.97370005   0.5553       0.95930004
    2.74170089]
 ...
 [119.21196732   0.18359999   0.64320004   0.20279999   0.71260005
    2.48822141]
 [338.63937522   0.0395       0.94510001   0.60100001   0.91720003
    2.81515956]
 [ 43.50779726   0.11820002   0.2221       0.15890001   0.2366
    2.53773165]][0m
[37m[1m[2023-07-11 09:13:32,016][233954] Max Reward on eval: 793.0576553348452[0m
[37m[1m[2023-07-11 09:13:32,016][233954] Min Reward on eval: -128.59400319401175[0m
[37m[1m[2023-07-11 09:13:32,016][233954] Mean Reward across all agents: 118.37546417645234[0m
[37m[1m[2023-07-11 09:13:32,017][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:13:32,020][233954] mean_value=-318.29812027353887, max_value=510.63053858943755[0m
[37m[1m[2023-07-11 09:13:32,023][233954] New mean coefficients: [[-0.17944795  0.9939915  -0.34998512 -0.42498147  0.9969156  -2.9049692 ]][0m
[37m[1m[2023-07-11 09:13:32,024][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:13:41,031][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 09:13:41,031][233954] FPS: 426422.52[0m
[36m[2023-07-11 09:13:41,033][233954] itr=688, itrs=2000, Progress: 34.40%[0m
[36m[2023-07-11 09:13:52,810][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 09:13:52,810][233954] FPS: 328499.85[0m
[36m[2023-07-11 09:13:57,139][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:13:57,140][233954] Reward + Measures: [[14.5297479   0.24171166  0.12511167  0.14762233  0.19366764  0.54669952]][0m
[37m[1m[2023-07-11 09:13:57,140][233954] Max Reward on eval: 14.52974789929877[0m
[37m[1m[2023-07-11 09:13:57,140][233954] Min Reward on eval: 14.52974789929877[0m
[37m[1m[2023-07-11 09:13:57,140][233954] Mean Reward across all agents: 14.52974789929877[0m
[37m[1m[2023-07-11 09:13:57,141][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:14:02,141][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:14:02,147][233954] Reward + Measures: [[-21.85213164   0.0709       0.13990001   0.1031       0.1311
    2.84936595]
 [ 34.96132832   0.30130002   0.57860005   0.39930001   0.63309997
    2.56416488]
 [ 75.02719519   0.0919       0.189        0.1018       0.16159999
    1.99877191]
 ...
 [-48.90584828   0.88959998   0.90650004   0.86610001   0.90350002
    2.29308438]
 [ -8.00367571   0.93979996   0.95339996   0.93959999   0.95889997
    2.60870886]
 [ 29.06810083   0.0886       0.34630001   0.1208       0.32660002
    1.73619545]][0m
[37m[1m[2023-07-11 09:14:02,147][233954] Max Reward on eval: 675.0477638446725[0m
[37m[1m[2023-07-11 09:14:02,148][233954] Min Reward on eval: -180.74679472977294[0m
[37m[1m[2023-07-11 09:14:02,148][233954] Mean Reward across all agents: 41.977786525760045[0m
[37m[1m[2023-07-11 09:14:02,148][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:14:02,151][233954] mean_value=-257.44143625375347, max_value=519.9042089574971[0m
[37m[1m[2023-07-11 09:14:02,153][233954] New mean coefficients: [[-0.28111154  0.6985371  -0.07368201 -0.43917978  0.86165273 -3.1636968 ]][0m
[37m[1m[2023-07-11 09:14:02,154][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:14:11,256][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 09:14:11,257][233954] FPS: 421949.27[0m
[36m[2023-07-11 09:14:11,259][233954] itr=689, itrs=2000, Progress: 34.45%[0m
[36m[2023-07-11 09:14:23,024][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 09:14:23,025][233954] FPS: 328766.67[0m
[36m[2023-07-11 09:14:27,371][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:14:27,372][233954] Reward + Measures: [[14.99136208  0.24082865  0.12540334  0.147157    0.20340668  0.54785949]][0m
[37m[1m[2023-07-11 09:14:27,372][233954] Max Reward on eval: 14.991362083144564[0m
[37m[1m[2023-07-11 09:14:27,372][233954] Min Reward on eval: 14.991362083144564[0m
[37m[1m[2023-07-11 09:14:27,373][233954] Mean Reward across all agents: 14.991362083144564[0m
[37m[1m[2023-07-11 09:14:27,373][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:14:32,351][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:14:32,352][233954] Reward + Measures: [[ 82.34008406   0.2913       0.49860001   0.1332       0.4086
    2.41506219]
 [ 81.80362643   0.13960001   0.53999996   0.14590001   0.59859997
    2.99164844]
 [-39.54402738   0.26289999   0.52380002   0.1091       0.37920004
    2.17536187]
 ...
 [-63.2871019    0.17650001   0.60960007   0.30980003   0.56970006
    2.05547929]
 [238.26530944   0.20119999   0.53680003   0.37980002   0.60740006
    1.87176168]
 [ 88.52642727   0.1072       0.33060002   0.12910001   0.36029997
    3.13861775]][0m
[37m[1m[2023-07-11 09:14:32,352][233954] Max Reward on eval: 739.7289199931547[0m
[37m[1m[2023-07-11 09:14:32,352][233954] Min Reward on eval: -176.15187488608063[0m
[37m[1m[2023-07-11 09:14:32,353][233954] Mean Reward across all agents: 97.80012954644526[0m
[37m[1m[2023-07-11 09:14:32,353][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:14:32,356][233954] mean_value=-227.46843102178858, max_value=478.38175244006334[0m
[37m[1m[2023-07-11 09:14:32,359][233954] New mean coefficients: [[-0.40732035  0.691756    0.5276565  -0.0687834   0.62668425 -3.195385  ]][0m
[37m[1m[2023-07-11 09:14:32,360][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:14:41,320][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 09:14:41,320][233954] FPS: 428649.06[0m
[36m[2023-07-11 09:14:41,322][233954] itr=690, itrs=2000, Progress: 34.50%[0m
[37m[1m[2023-07-11 09:17:58,868][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000670[0m
[36m[2023-07-11 09:18:10,859][233954] train() took 11.35 seconds to complete[0m
[36m[2023-07-11 09:18:10,859][233954] FPS: 338361.94[0m
[36m[2023-07-11 09:18:15,026][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:18:15,026][233954] Reward + Measures: [[16.16299782  0.25592932  0.13603599  0.14559966  0.21332733  0.5403499 ]][0m
[37m[1m[2023-07-11 09:18:15,026][233954] Max Reward on eval: 16.16299782312711[0m
[37m[1m[2023-07-11 09:18:15,027][233954] Min Reward on eval: 16.16299782312711[0m
[37m[1m[2023-07-11 09:18:15,027][233954] Mean Reward across all agents: 16.16299782312711[0m
[37m[1m[2023-07-11 09:18:15,027][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:18:20,107][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:18:20,107][233954] Reward + Measures: [[ 17.43029887   0.35340002   0.6038       0.42030001   0.4093
    2.54473042]
 [129.61269416   0.22950001   0.88910002   0.28530002   0.85030001
    2.31117702]
 [ 53.75029774   0.1656       0.77110004   0.2942       0.66530001
    2.74718547]
 ...
 [ 76.42405524   0.108        0.0935       0.07700001   0.1089
    1.70745969]
 [ 62.48063174   0.21070002   0.50509995   0.16790001   0.50229996
    1.99190867]
 [ 25.10260952   0.13900001   0.1837       0.154        0.2036
    1.64135444]][0m
[37m[1m[2023-07-11 09:18:20,108][233954] Max Reward on eval: 697.9070129438071[0m
[37m[1m[2023-07-11 09:18:20,108][233954] Min Reward on eval: -163.68446500683203[0m
[37m[1m[2023-07-11 09:18:20,108][233954] Mean Reward across all agents: 102.6414076366037[0m
[37m[1m[2023-07-11 09:18:20,108][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:18:20,113][233954] mean_value=-238.49054602062358, max_value=733.6246699204669[0m
[37m[1m[2023-07-11 09:18:20,115][233954] New mean coefficients: [[-1.1684299  -0.9986955   0.43003416  0.3606568  -1.3350525  -5.8944654 ]][0m
[37m[1m[2023-07-11 09:18:20,116][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:18:29,031][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 09:18:29,032][233954] FPS: 430802.32[0m
[36m[2023-07-11 09:18:29,034][233954] itr=691, itrs=2000, Progress: 34.55%[0m
[36m[2023-07-11 09:18:40,838][233954] train() took 11.72 seconds to complete[0m
[36m[2023-07-11 09:18:40,838][233954] FPS: 327760.30[0m
[36m[2023-07-11 09:18:45,164][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:18:45,164][233954] Reward + Measures: [[18.57930877  0.248164    0.133747    0.14706199  0.20725033  0.53463244]][0m
[37m[1m[2023-07-11 09:18:45,164][233954] Max Reward on eval: 18.579308769328286[0m
[37m[1m[2023-07-11 09:18:45,165][233954] Min Reward on eval: 18.579308769328286[0m
[37m[1m[2023-07-11 09:18:45,165][233954] Mean Reward across all agents: 18.579308769328286[0m
[37m[1m[2023-07-11 09:18:45,165][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:18:50,184][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:18:50,185][233954] Reward + Measures: [[  3.59528425   0.20460001   0.27250001   0.1561       0.2534
    2.73271728]
 [ 34.68771112   0.15620001   0.3881       0.0957       0.2809
    2.47704315]
 [134.20743343   0.1961       0.54860002   0.25280002   0.60510004
    2.72118545]
 ...
 [ 71.55244795   0.95139998   0.96090001   0.94239998   0.93349999
    3.59219742]
 [289.72628211   0.0086       0.96709996   0.44140002   0.90739995
    2.27034807]
 [-24.22755357   0.1612       0.42739996   0.15210001   0.31579998
    2.42667365]][0m
[37m[1m[2023-07-11 09:18:50,185][233954] Max Reward on eval: 517.302425334882[0m
[37m[1m[2023-07-11 09:18:50,185][233954] Min Reward on eval: -324.8048389505595[0m
[37m[1m[2023-07-11 09:18:50,186][233954] Mean Reward across all agents: 46.65494455224267[0m
[37m[1m[2023-07-11 09:18:50,186][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:18:50,189][233954] mean_value=-259.6369936952768, max_value=525.492708561616[0m
[37m[1m[2023-07-11 09:18:50,191][233954] New mean coefficients: [[-0.92901874 -0.46171266  0.18035144  0.1436822  -0.6044719  -5.2502785 ]][0m
[37m[1m[2023-07-11 09:18:50,192][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:18:59,258][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 09:18:59,258][233954] FPS: 423640.49[0m
[36m[2023-07-11 09:18:59,261][233954] itr=692, itrs=2000, Progress: 34.60%[0m
[36m[2023-07-11 09:19:11,046][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 09:19:11,046][233954] FPS: 328328.74[0m
[36m[2023-07-11 09:19:15,417][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:19:15,417][233954] Reward + Measures: [[17.79529504  0.23509666  0.14269766  0.15060599  0.19392301  0.51789337]][0m
[37m[1m[2023-07-11 09:19:15,417][233954] Max Reward on eval: 17.79529503623175[0m
[37m[1m[2023-07-11 09:19:15,418][233954] Min Reward on eval: 17.79529503623175[0m
[37m[1m[2023-07-11 09:19:15,418][233954] Mean Reward across all agents: 17.79529503623175[0m
[37m[1m[2023-07-11 09:19:15,418][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:19:20,437][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:19:20,438][233954] Reward + Measures: [[-95.05752227   0.27219999   0.93310004   0.10610002   0.88990003
    2.1589303 ]
 [ -4.13353767   0.49380001   0.98520005   0.06370001   0.98889989
    3.17987251]
 [232.53580068   0.33219999   0.84729999   0.75050002   0.61040002
    2.25297022]
 ...
 [ 97.0258317    0.24969999   0.94060004   0.38459998   0.92930013
    3.18731594]
 [148.89397335   0.21870001   0.93540001   0.53249997   0.93370003
    2.52052808]
 [-23.64763924   0.5395       0.70649999   0.22389999   0.84219998
    3.38058162]][0m
[37m[1m[2023-07-11 09:19:20,438][233954] Max Reward on eval: 684.8073806657455[0m
[37m[1m[2023-07-11 09:19:20,438][233954] Min Reward on eval: -165.48334787325[0m
[37m[1m[2023-07-11 09:19:20,439][233954] Mean Reward across all agents: 83.24945777095864[0m
[37m[1m[2023-07-11 09:19:20,439][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:19:20,445][233954] mean_value=-146.61649183027941, max_value=676.07693659388[0m
[37m[1m[2023-07-11 09:19:20,448][233954] New mean coefficients: [[-0.310269    0.4113586   0.33245432 -0.14132632  0.11794269 -4.0375237 ]][0m
[37m[1m[2023-07-11 09:19:20,449][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:19:29,563][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 09:19:29,563][233954] FPS: 421406.21[0m
[36m[2023-07-11 09:19:29,565][233954] itr=693, itrs=2000, Progress: 34.65%[0m
[36m[2023-07-11 09:19:41,371][233954] train() took 11.72 seconds to complete[0m
[36m[2023-07-11 09:19:41,371][233954] FPS: 327619.21[0m
[36m[2023-07-11 09:19:45,668][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:19:45,669][233954] Reward + Measures: [[18.31588522  0.22156234  0.15794866  0.16364866  0.18033934  0.52040398]][0m
[37m[1m[2023-07-11 09:19:45,669][233954] Max Reward on eval: 18.31588521875864[0m
[37m[1m[2023-07-11 09:19:45,669][233954] Min Reward on eval: 18.31588521875864[0m
[37m[1m[2023-07-11 09:19:45,669][233954] Mean Reward across all agents: 18.31588521875864[0m
[37m[1m[2023-07-11 09:19:45,669][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:19:50,667][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:19:50,667][233954] Reward + Measures: [[130.97270312   0.2129       0.18200001   0.22660001   0.31430003
    2.84538746]
 [ 33.66457312   0.16680001   0.14500001   0.17         0.18270001
    1.74240148]
 [ 69.80882642   0.53899997   0.6304       0.6124       0.33950001
    2.30876994]
 ...
 [ 35.79251914   0.0848       0.1311       0.1213       0.1135
    2.89424777]
 [ 38.42919342   0.3513       0.44989997   0.36919999   0.4664
    2.27463746]
 [-94.26370201   0.421        0.43210003   0.38630003   0.44159999
    2.70065928]][0m
[37m[1m[2023-07-11 09:19:50,668][233954] Max Reward on eval: 620.1559770353139[0m
[37m[1m[2023-07-11 09:19:50,668][233954] Min Reward on eval: -109.70322250053286[0m
[37m[1m[2023-07-11 09:19:50,668][233954] Mean Reward across all agents: 61.711740930976[0m
[37m[1m[2023-07-11 09:19:50,668][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:19:50,673][233954] mean_value=-198.70136535312417, max_value=731.4879927631002[0m
[37m[1m[2023-07-11 09:19:50,676][233954] New mean coefficients: [[ 0.5232339   1.4874985   0.17394942 -0.5701005   1.3523995  -2.3817382 ]][0m
[37m[1m[2023-07-11 09:19:50,677][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:19:59,676][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 09:19:59,677][233954] FPS: 426770.25[0m
[36m[2023-07-11 09:19:59,679][233954] itr=694, itrs=2000, Progress: 34.70%[0m
[36m[2023-07-11 09:20:11,441][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 09:20:11,441][233954] FPS: 328923.06[0m
[36m[2023-07-11 09:20:15,840][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:20:15,841][233954] Reward + Measures: [[17.82199991  0.236736    0.14734299  0.14788467  0.19491532  0.51688337]][0m
[37m[1m[2023-07-11 09:20:15,841][233954] Max Reward on eval: 17.821999909645122[0m
[37m[1m[2023-07-11 09:20:15,841][233954] Min Reward on eval: 17.821999909645122[0m
[37m[1m[2023-07-11 09:20:15,842][233954] Mean Reward across all agents: 17.821999909645122[0m
[37m[1m[2023-07-11 09:20:15,842][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:20:20,853][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:20:20,854][233954] Reward + Measures: [[   6.24777567    0.1675        0.34400001    0.1432        0.26490003
     2.40870261]
 [ -43.65357085    0.62950003    0.43449998    0.57880002    0.32810003
     2.36630535]
 [ -87.64570502    0.38929999    0.38120002    0.42350003    0.37129998
     2.32949424]
 ...
 [ 749.84463504    0.0137        0.98519993    0.76999998    0.97979993
     3.09468412]
 [-128.75716441    0.44499999    0.46420002    0.38270003    0.41770002
     2.37764764]
 [  -1.15050544    0.46350002    0.53840005    0.3908        0.45140001
     1.90344584]][0m
[37m[1m[2023-07-11 09:20:20,854][233954] Max Reward on eval: 804.7742614810355[0m
[37m[1m[2023-07-11 09:20:20,855][233954] Min Reward on eval: -237.08819054886698[0m
[37m[1m[2023-07-11 09:20:20,855][233954] Mean Reward across all agents: 45.93836995672666[0m
[37m[1m[2023-07-11 09:20:20,855][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:20:20,859][233954] mean_value=-264.5353671258968, max_value=555.4651750919409[0m
[37m[1m[2023-07-11 09:20:20,861][233954] New mean coefficients: [[ 0.40578067  1.2844934  -0.45048797 -0.9208974   1.33956    -2.296561  ]][0m
[37m[1m[2023-07-11 09:20:20,862][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:20:29,821][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 09:20:29,822][233954] FPS: 428692.21[0m
[36m[2023-07-11 09:20:29,824][233954] itr=695, itrs=2000, Progress: 34.75%[0m
[36m[2023-07-11 09:20:41,672][233954] train() took 11.76 seconds to complete[0m
[36m[2023-07-11 09:20:41,673][233954] FPS: 326466.17[0m
[36m[2023-07-11 09:20:45,971][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:20:45,971][233954] Reward + Measures: [[19.0534827   0.23616935  0.13233134  0.13135767  0.21643333  0.51587969]][0m
[37m[1m[2023-07-11 09:20:45,971][233954] Max Reward on eval: 19.053482704923315[0m
[37m[1m[2023-07-11 09:20:45,972][233954] Min Reward on eval: 19.053482704923315[0m
[37m[1m[2023-07-11 09:20:45,972][233954] Mean Reward across all agents: 19.053482704923315[0m
[37m[1m[2023-07-11 09:20:45,972][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:20:51,008][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:20:51,009][233954] Reward + Measures: [[-63.45057044   0.61710006   0.67309999   0.61409998   0.50860006
    2.27949572]
 [ 76.58702413   0.16419999   0.51990002   0.41360003   0.51719999
    3.00717878]
 [242.26393797   0.0501       0.778        0.42759997   0.7216
    2.9242866 ]
 ...
 [-51.42795849   0.2534       0.50950003   0.41519997   0.5212
    2.89209938]
 [218.31071555   0.1402       0.88240004   0.56029999   0.8386001
    2.94252205]
 [ 32.04470497   0.29899999   0.72799999   0.16449998   0.78960001
    2.51121736]][0m
[37m[1m[2023-07-11 09:20:51,009][233954] Max Reward on eval: 674.1138725172729[0m
[37m[1m[2023-07-11 09:20:51,009][233954] Min Reward on eval: -196.3911198226735[0m
[37m[1m[2023-07-11 09:20:51,010][233954] Mean Reward across all agents: 68.83000843912308[0m
[37m[1m[2023-07-11 09:20:51,010][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:20:51,014][233954] mean_value=-198.6110360852645, max_value=708.6653524911962[0m
[37m[1m[2023-07-11 09:20:51,017][233954] New mean coefficients: [[ 0.17418376  0.6935157  -0.24538302 -0.8301723   0.62694937 -3.103031  ]][0m
[37m[1m[2023-07-11 09:20:51,018][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:21:00,108][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 09:21:00,108][233954] FPS: 422506.31[0m
[36m[2023-07-11 09:21:00,110][233954] itr=696, itrs=2000, Progress: 34.80%[0m
[36m[2023-07-11 09:21:12,002][233954] train() took 11.80 seconds to complete[0m
[36m[2023-07-11 09:21:12,003][233954] FPS: 325351.96[0m
[36m[2023-07-11 09:21:16,327][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:21:16,328][233954] Reward + Measures: [[17.31477338  0.26115432  0.141395    0.13555534  0.22896732  0.53016984]][0m
[37m[1m[2023-07-11 09:21:16,328][233954] Max Reward on eval: 17.314773383308456[0m
[37m[1m[2023-07-11 09:21:16,328][233954] Min Reward on eval: 17.314773383308456[0m
[37m[1m[2023-07-11 09:21:16,328][233954] Mean Reward across all agents: 17.314773383308456[0m
[37m[1m[2023-07-11 09:21:16,329][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:21:21,492][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:21:21,492][233954] Reward + Measures: [[198.33939717   0.1067       0.30019999   0.24319999   0.32949999
    2.39988637]
 [ 80.52675749   0.1313       0.66610003   0.16490002   0.60150003
    2.88549948]
 [-66.31564451   0.1243       0.85760003   0.23099999   0.91120005
    2.5993309 ]
 ...
 [ 12.5587114    0.46549997   0.58450001   0.48970005   0.64230007
    2.65312696]
 [ 92.96088554   0.0059       0.99680007   0.55140001   0.99169999
    3.21362615]
 [276.51510336   0.0707       0.8702001    0.63880002   0.86770004
    2.97238612]][0m
[37m[1m[2023-07-11 09:21:21,492][233954] Max Reward on eval: 804.4871520929039[0m
[37m[1m[2023-07-11 09:21:21,493][233954] Min Reward on eval: -169.59217552617193[0m
[37m[1m[2023-07-11 09:21:21,493][233954] Mean Reward across all agents: 106.21167078654122[0m
[37m[1m[2023-07-11 09:21:21,493][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:21:21,496][233954] mean_value=-305.4277453146409, max_value=453.3044932430341[0m
[37m[1m[2023-07-11 09:21:21,498][233954] New mean coefficients: [[-0.44938847 -0.35992354 -0.24750954 -0.21518898 -0.32750916 -4.7630644 ]][0m
[37m[1m[2023-07-11 09:21:21,499][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:21:30,467][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 09:21:30,473][233954] FPS: 428279.16[0m
[36m[2023-07-11 09:21:30,475][233954] itr=697, itrs=2000, Progress: 34.85%[0m
[36m[2023-07-11 09:21:42,057][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 09:21:42,057][233954] FPS: 334171.25[0m
[36m[2023-07-11 09:21:46,314][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:21:46,315][233954] Reward + Measures: [[19.41711981  0.22514668  0.14787166  0.14752233  0.202813    0.50805324]][0m
[37m[1m[2023-07-11 09:21:46,315][233954] Max Reward on eval: 19.417119814422332[0m
[37m[1m[2023-07-11 09:21:46,315][233954] Min Reward on eval: 19.417119814422332[0m
[37m[1m[2023-07-11 09:21:46,316][233954] Mean Reward across all agents: 19.417119814422332[0m
[37m[1m[2023-07-11 09:21:46,316][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:21:51,361][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:21:51,362][233954] Reward + Measures: [[353.21086599   0.0113       0.82359999   0.67630005   0.7967
    2.99040008]
 [  9.36824452   0.20830002   0.30380002   0.20290001   0.249
    2.40562701]
 [-27.36800601   0.1541       0.1793       0.15269999   0.16419999
    2.62161064]
 ...
 [-10.75981559   0.255        0.5521       0.199        0.53019994
    2.46386933]
 [338.547934     0.0281       0.57950002   0.46900001   0.59040004
    2.42238688]
 [ 31.44229584   0.0805       0.3371       0.10950001   0.23369999
    1.96609175]][0m
[37m[1m[2023-07-11 09:21:51,362][233954] Max Reward on eval: 691.0058212265373[0m
[37m[1m[2023-07-11 09:21:51,362][233954] Min Reward on eval: -106.53973762548995[0m
[37m[1m[2023-07-11 09:21:51,363][233954] Mean Reward across all agents: 86.50980514904847[0m
[37m[1m[2023-07-11 09:21:51,363][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:21:51,367][233954] mean_value=-292.3520054066502, max_value=713.6996836992396[0m
[37m[1m[2023-07-11 09:21:51,370][233954] New mean coefficients: [[ 0.42653224  0.1295115  -0.07360063 -0.50281596 -0.05319509 -3.4754934 ]][0m
[37m[1m[2023-07-11 09:21:51,371][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:22:00,473][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 09:22:00,473][233954] FPS: 421946.02[0m
[36m[2023-07-11 09:22:00,476][233954] itr=698, itrs=2000, Progress: 34.90%[0m
[36m[2023-07-11 09:22:12,541][233954] train() took 11.98 seconds to complete[0m
[36m[2023-07-11 09:22:12,542][233954] FPS: 320645.29[0m
[36m[2023-07-11 09:22:16,852][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:22:16,852][233954] Reward + Measures: [[19.31813052  0.23522799  0.14225768  0.14486267  0.20578833  0.49790341]][0m
[37m[1m[2023-07-11 09:22:16,852][233954] Max Reward on eval: 19.31813052079937[0m
[37m[1m[2023-07-11 09:22:16,853][233954] Min Reward on eval: 19.31813052079937[0m
[37m[1m[2023-07-11 09:22:16,853][233954] Mean Reward across all agents: 19.31813052079937[0m
[37m[1m[2023-07-11 09:22:16,853][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:22:21,821][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:22:21,821][233954] Reward + Measures: [[ 60.91609434   0.44379997   0.59400004   0.0617       0.55880004
    2.8638165 ]
 [ 12.81055979   0.42400002   0.7044       0.51680005   0.41120002
    2.42261076]
 [557.47817231   0.0259       0.93590003   0.70989996   0.9181
    2.79598117]
 ...
 [ 34.01658988   0.21040002   0.55109996   0.42090002   0.55930001
    2.65933967]
 [ -7.71354232   0.43270001   0.59600002   0.40100002   0.41339999
    2.38118935]
 [ 15.68284459   0.36139998   0.77899998   0.17160001   0.77109998
    2.15732265]][0m
[37m[1m[2023-07-11 09:22:21,821][233954] Max Reward on eval: 761.4160747553221[0m
[37m[1m[2023-07-11 09:22:21,822][233954] Min Reward on eval: -407.32601452888923[0m
[37m[1m[2023-07-11 09:22:21,822][233954] Mean Reward across all agents: 55.42244114562146[0m
[37m[1m[2023-07-11 09:22:21,822][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:22:21,826][233954] mean_value=-216.22983080847285, max_value=381.46794840422115[0m
[37m[1m[2023-07-11 09:22:21,829][233954] New mean coefficients: [[ 0.20352837  0.16259694 -0.44390345 -0.2664824   0.10010086 -3.5395606 ]][0m
[37m[1m[2023-07-11 09:22:21,830][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:22:30,816][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 09:22:30,816][233954] FPS: 427405.21[0m
[36m[2023-07-11 09:22:30,818][233954] itr=699, itrs=2000, Progress: 34.95%[0m
[36m[2023-07-11 09:22:42,386][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 09:22:42,386][233954] FPS: 334562.69[0m
[36m[2023-07-11 09:22:46,665][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:22:46,665][233954] Reward + Measures: [[18.8139356   0.27828398  0.13035767  0.12888834  0.20762467  0.48822594]][0m
[37m[1m[2023-07-11 09:22:46,666][233954] Max Reward on eval: 18.813935603884243[0m
[37m[1m[2023-07-11 09:22:46,666][233954] Min Reward on eval: 18.813935603884243[0m
[37m[1m[2023-07-11 09:22:46,666][233954] Mean Reward across all agents: 18.813935603884243[0m
[37m[1m[2023-07-11 09:22:46,667][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:22:51,625][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:22:51,626][233954] Reward + Measures: [[ 97.68166546   0.1489       0.4686       0.1684       0.46580002
    2.16836786]
 [ 20.11693952   0.1772       0.23029999   0.164        0.17760001
    1.85346878]
 [ 39.06679472   0.13330001   0.18990003   0.1576       0.22379999
    2.40756845]
 ...
 [140.3227079    0.22839999   0.1169       0.227        0.25710002
    2.58996439]
 [166.16662504   0.31889999   0.58140004   0.27380002   0.4955
    1.96498954]
 [  6.85145035   0.08110001   0.1568       0.09680001   0.1164
    1.57501316]][0m
[37m[1m[2023-07-11 09:22:51,626][233954] Max Reward on eval: 712.0413093820214[0m
[37m[1m[2023-07-11 09:22:51,626][233954] Min Reward on eval: -283.06239573284984[0m
[37m[1m[2023-07-11 09:22:51,627][233954] Mean Reward across all agents: 86.10111001283059[0m
[37m[1m[2023-07-11 09:22:51,627][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:22:51,630][233954] mean_value=-370.9007657159519, max_value=659.9923973269761[0m
[37m[1m[2023-07-11 09:22:51,633][233954] New mean coefficients: [[-0.16440168 -0.7709147  -0.06390482 -0.15100901 -0.62482387 -4.983555  ]][0m
[37m[1m[2023-07-11 09:22:51,634][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:23:00,622][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 09:23:00,623][233954] FPS: 427280.77[0m
[36m[2023-07-11 09:23:00,625][233954] itr=700, itrs=2000, Progress: 35.00%[0m
[37m[1m[2023-07-11 09:26:24,537][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000680[0m
[36m[2023-07-11 09:26:36,706][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 09:26:36,706][233954] FPS: 335473.49[0m
[36m[2023-07-11 09:26:40,895][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:26:40,895][233954] Reward + Measures: [[18.35788375  0.24476965  0.13229765  0.12840067  0.19441333  0.48424736]][0m
[37m[1m[2023-07-11 09:26:40,896][233954] Max Reward on eval: 18.357883749970064[0m
[37m[1m[2023-07-11 09:26:40,896][233954] Min Reward on eval: 18.357883749970064[0m
[37m[1m[2023-07-11 09:26:40,896][233954] Mean Reward across all agents: 18.357883749970064[0m
[37m[1m[2023-07-11 09:26:40,896][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:26:45,771][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:26:45,771][233954] Reward + Measures: [[ 72.22470567   0.27959999   0.60100001   0.38769999   0.54890007
    2.22359157]
 [-82.8336649    0.0773       0.0807       0.0887       0.15869999
    2.14096618]
 [ 59.85964341   0.0459       0.28529999   0.19930001   0.25110003
    1.64013219]
 ...
 [ 11.37959188   0.8707       0.88900006   0.85680002   0.88289994
    3.29088855]
 [-23.76540005   0.1124       0.2221       0.1054       0.1902
    2.80717826]
 [103.5352087    0.98530006   0.99239999   0.98199999   0.98689997
    2.59573317]][0m
[37m[1m[2023-07-11 09:26:45,772][233954] Max Reward on eval: 499.3762633680366[0m
[37m[1m[2023-07-11 09:26:45,772][233954] Min Reward on eval: -137.43094263477252[0m
[37m[1m[2023-07-11 09:26:45,772][233954] Mean Reward across all agents: 61.09958817902659[0m
[37m[1m[2023-07-11 09:26:45,772][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:26:45,777][233954] mean_value=-198.11202318392097, max_value=464.17468506537705[0m
[37m[1m[2023-07-11 09:26:45,780][233954] New mean coefficients: [[-0.38533133 -1.3529165   0.1427554  -0.0008775  -1.230416   -5.648614  ]][0m
[37m[1m[2023-07-11 09:26:45,781][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:26:54,689][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 09:26:54,689][233954] FPS: 431131.92[0m
[36m[2023-07-11 09:26:54,691][233954] itr=701, itrs=2000, Progress: 35.05%[0m
[36m[2023-07-11 09:27:06,237][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 09:27:06,237][233954] FPS: 335179.82[0m
[36m[2023-07-11 09:27:10,450][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:27:10,450][233954] Reward + Measures: [[19.79873231  0.227736    0.131722    0.12920499  0.19263466  0.49433759]][0m
[37m[1m[2023-07-11 09:27:10,450][233954] Max Reward on eval: 19.79873230694674[0m
[37m[1m[2023-07-11 09:27:10,451][233954] Min Reward on eval: 19.79873230694674[0m
[37m[1m[2023-07-11 09:27:10,451][233954] Mean Reward across all agents: 19.79873230694674[0m
[37m[1m[2023-07-11 09:27:10,451][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:27:15,603][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:27:15,604][233954] Reward + Measures: [[147.78732037   0.41640002   0.28930002   0.37210003   0.48450002
    2.06726336]
 [ 48.78294695   0.24849999   0.62120003   0.53850001   0.69679999
    2.4651773 ]
 [166.50236021   0.0947       0.38420001   0.26760003   0.43059999
    2.39693308]
 ...
 [145.43833372   0.25720003   0.65820003   0.39610001   0.6825
    2.96145034]
 [-29.37398535   0.98019999   0.9927001    0.9806       0.98859996
    3.00493741]
 [ 24.39749387   0.30240002   0.91839999   0.099        0.89589995
    2.46241808]][0m
[37m[1m[2023-07-11 09:27:15,605][233954] Max Reward on eval: 743.8464431628585[0m
[37m[1m[2023-07-11 09:27:15,605][233954] Min Reward on eval: -261.4506562411319[0m
[37m[1m[2023-07-11 09:27:15,605][233954] Mean Reward across all agents: 68.4335725461638[0m
[37m[1m[2023-07-11 09:27:15,605][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:27:15,608][233954] mean_value=-326.62407941378535, max_value=222.33103471282868[0m
[37m[1m[2023-07-11 09:27:15,611][233954] New mean coefficients: [[ 0.05337149 -0.73541737  0.18913539  0.12585035 -0.6303774  -4.5012646 ]][0m
[37m[1m[2023-07-11 09:27:15,612][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:27:24,514][233954] train() took 8.90 seconds to complete[0m
[36m[2023-07-11 09:27:24,514][233954] FPS: 431406.86[0m
[36m[2023-07-11 09:27:24,517][233954] itr=702, itrs=2000, Progress: 35.10%[0m
[36m[2023-07-11 09:27:36,195][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 09:27:36,195][233954] FPS: 331210.56[0m
[36m[2023-07-11 09:27:40,467][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:27:40,467][233954] Reward + Measures: [[21.1613153   0.21241166  0.13523     0.13303566  0.18412933  0.4789218 ]][0m
[37m[1m[2023-07-11 09:27:40,467][233954] Max Reward on eval: 21.161315299444404[0m
[37m[1m[2023-07-11 09:27:40,468][233954] Min Reward on eval: 21.161315299444404[0m
[37m[1m[2023-07-11 09:27:40,468][233954] Mean Reward across all agents: 21.161315299444404[0m
[37m[1m[2023-07-11 09:27:40,468][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:27:45,414][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:27:45,415][233954] Reward + Measures: [[  59.79854093    0.12820001    0.35830003    0.14579999    0.3175
     2.87140012]
 [  47.82020246    0.087         0.1673        0.0871        0.15370001
     1.90358627]
 [  -2.483579      0.25610003    0.4594        0.2626        0.43650004
     2.45905828]
 ...
 [ -46.02792449    0.3725        0.74940002    0.1434        0.77540004
     2.44378281]
 [-200.94875528    0.5025        0.57620001    0.37219998    0.54380006
     2.50160265]
 [ -55.04756553    0.1242        0.1772        0.0778        0.2009
     2.41985369]][0m
[37m[1m[2023-07-11 09:27:45,415][233954] Max Reward on eval: 576.1144599802792[0m
[37m[1m[2023-07-11 09:27:45,415][233954] Min Reward on eval: -285.5756836030632[0m
[37m[1m[2023-07-11 09:27:45,416][233954] Mean Reward across all agents: -11.965958041719098[0m
[37m[1m[2023-07-11 09:27:45,416][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:27:45,419][233954] mean_value=-335.7307198968493, max_value=535.5705174621311[0m
[37m[1m[2023-07-11 09:27:45,421][233954] New mean coefficients: [[-0.17597905 -0.5842558  -0.02866057  0.7022147  -0.4866324  -4.67678   ]][0m
[37m[1m[2023-07-11 09:27:45,422][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:27:54,381][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 09:27:54,381][233954] FPS: 428710.90[0m
[36m[2023-07-11 09:27:54,383][233954] itr=703, itrs=2000, Progress: 35.15%[0m
[36m[2023-07-11 09:28:05,910][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 09:28:05,916][233954] FPS: 335599.16[0m
[36m[2023-07-11 09:28:10,154][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:28:10,155][233954] Reward + Measures: [[19.60769775  0.210199    0.14988033  0.149918    0.16962099  0.47638056]][0m
[37m[1m[2023-07-11 09:28:10,155][233954] Max Reward on eval: 19.607697747683357[0m
[37m[1m[2023-07-11 09:28:10,155][233954] Min Reward on eval: 19.607697747683357[0m
[37m[1m[2023-07-11 09:28:10,155][233954] Mean Reward across all agents: 19.607697747683357[0m
[37m[1m[2023-07-11 09:28:10,156][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:28:15,144][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:28:15,144][233954] Reward + Measures: [[ 19.14651442   0.0991       0.17260002   0.1622       0.15460001
    0.99205035]
 [229.68341779   0.11719999   0.93239993   0.64300007   0.91830009
    2.92897844]
 [  3.3702905    0.11620001   0.18180001   0.11539999   0.1743
    1.27030253]
 ...
 [ -6.06957135   0.18480001   0.2886       0.14210001   0.2728
    2.17091441]
 [ 37.62282983   0.26440001   0.28019997   0.26900002   0.28410003
    2.17007089]
 [ 19.05864004   0.77820009   0.81409997   0.76069999   0.7999
    2.56726789]][0m
[37m[1m[2023-07-11 09:28:15,145][233954] Max Reward on eval: 608.6529981115833[0m
[37m[1m[2023-07-11 09:28:15,145][233954] Min Reward on eval: -273.6910257234005[0m
[37m[1m[2023-07-11 09:28:15,145][233954] Mean Reward across all agents: 96.80068910152269[0m
[37m[1m[2023-07-11 09:28:15,145][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:28:15,149][233954] mean_value=-389.2413496909128, max_value=653.5807450141758[0m
[37m[1m[2023-07-11 09:28:15,152][233954] New mean coefficients: [[-0.2973055  -1.0236142  -0.80855024  0.4713847  -0.9670729  -5.1926956 ]][0m
[37m[1m[2023-07-11 09:28:15,153][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:28:24,132][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 09:28:24,132][233954] FPS: 427733.04[0m
[36m[2023-07-11 09:28:24,134][233954] itr=704, itrs=2000, Progress: 35.20%[0m
[36m[2023-07-11 09:28:35,673][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 09:28:35,678][233954] FPS: 335290.46[0m
[36m[2023-07-11 09:28:39,924][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:28:39,924][233954] Reward + Measures: [[22.34148684  0.18574432  0.14414066  0.155983    0.15198633  0.46134463]][0m
[37m[1m[2023-07-11 09:28:39,924][233954] Max Reward on eval: 22.34148683724038[0m
[37m[1m[2023-07-11 09:28:39,925][233954] Min Reward on eval: 22.34148683724038[0m
[37m[1m[2023-07-11 09:28:39,925][233954] Mean Reward across all agents: 22.34148683724038[0m
[37m[1m[2023-07-11 09:28:39,925][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:28:44,844][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:28:44,844][233954] Reward + Measures: [[ 110.03355742    0.31510001    0.54460001    0.42870003    0.44940004
     2.34905505]
 [  57.54736636    0.1244        0.22379999    0.12280001    0.22219999
     1.93715918]
 [-267.417555      0.45640001    0.79520005    0.17290001    0.7392
     2.68634152]
 ...
 [  -9.03108309    0.1186        0.19100001    0.13690001    0.17320001
     1.93335593]
 [ 392.71014497    0.0267        0.95710003    0.63819999    0.9163
     2.71420741]
 [ 259.1708531     0.23360001    0.73430002    0.2861        0.76130003
     2.79375243]][0m
[37m[1m[2023-07-11 09:28:44,845][233954] Max Reward on eval: 684.3953933659941[0m
[37m[1m[2023-07-11 09:28:44,845][233954] Min Reward on eval: -267.41755500151777[0m
[37m[1m[2023-07-11 09:28:44,845][233954] Mean Reward across all agents: 95.21632390179299[0m
[37m[1m[2023-07-11 09:28:44,845][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:28:44,850][233954] mean_value=-282.1758120838458, max_value=849.6272240016609[0m
[37m[1m[2023-07-11 09:28:44,852][233954] New mean coefficients: [[-0.15267494 -1.7038567  -0.25155997  0.8574233  -1.5366149  -5.941674  ]][0m
[37m[1m[2023-07-11 09:28:44,854][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:28:53,776][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 09:28:53,776][233954] FPS: 430448.77[0m
[36m[2023-07-11 09:28:53,779][233954] itr=705, itrs=2000, Progress: 35.25%[0m
[36m[2023-07-11 09:29:05,342][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 09:29:05,343][233954] FPS: 334534.81[0m
[36m[2023-07-11 09:29:09,568][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:29:09,568][233954] Reward + Measures: [[19.17018917  0.14162965  0.15096833  0.16520099  0.15276267  0.47118261]][0m
[37m[1m[2023-07-11 09:29:09,568][233954] Max Reward on eval: 19.170189173709193[0m
[37m[1m[2023-07-11 09:29:09,569][233954] Min Reward on eval: 19.170189173709193[0m
[37m[1m[2023-07-11 09:29:09,569][233954] Mean Reward across all agents: 19.170189173709193[0m
[37m[1m[2023-07-11 09:29:09,569][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:29:14,577][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:29:14,578][233954] Reward + Measures: [[  20.45179728    0.06690001    0.21630001    0.15269999    0.24320002
     2.84717941]
 [  81.94651759    0.0442        0.29570004    0.2218        0.33820003
     2.58206987]
 [ 184.91551905    0.2577        0.56169999    0.26300001    0.63310003
     2.63203526]
 ...
 [ -25.21531369    0.6997        0.76300001    0.64350003    0.79100001
     2.71795893]
 [-161.50884484    0.28710002    0.39340001    0.1178        0.40909997
     2.7482388 ]
 [  29.56416912    0.29969999    0.3355        0.17349999    0.40269995
     3.41914916]][0m
[37m[1m[2023-07-11 09:29:14,578][233954] Max Reward on eval: 721.2909622311593[0m
[37m[1m[2023-07-11 09:29:14,579][233954] Min Reward on eval: -166.46342182476073[0m
[37m[1m[2023-07-11 09:29:14,579][233954] Mean Reward across all agents: 69.93877690055383[0m
[37m[1m[2023-07-11 09:29:14,579][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:29:14,583][233954] mean_value=-314.2403665214337, max_value=558.2956289799884[0m
[37m[1m[2023-07-11 09:29:14,585][233954] New mean coefficients: [[ 0.03935072 -1.1176915   0.05371785  0.55060065 -1.0328534  -4.92087   ]][0m
[37m[1m[2023-07-11 09:29:14,586][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:29:23,647][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 09:29:23,652][233954] FPS: 423873.73[0m
[36m[2023-07-11 09:29:23,655][233954] itr=706, itrs=2000, Progress: 35.30%[0m
[36m[2023-07-11 09:29:35,247][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 09:29:35,248][233954] FPS: 333808.88[0m
[36m[2023-07-11 09:29:39,472][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:29:39,477][233954] Reward + Measures: [[20.24888165  0.15006533  0.16641067  0.17757666  0.147616    0.47027281]][0m
[37m[1m[2023-07-11 09:29:39,478][233954] Max Reward on eval: 20.248881652204105[0m
[37m[1m[2023-07-11 09:29:39,478][233954] Min Reward on eval: 20.248881652204105[0m
[37m[1m[2023-07-11 09:29:39,478][233954] Mean Reward across all agents: 20.248881652204105[0m
[37m[1m[2023-07-11 09:29:39,478][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:29:44,384][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:29:44,385][233954] Reward + Measures: [[  3.01327483   0.73780006   0.76490003   0.68599999   0.60639995
    2.11616635]
 [711.15859224   0.0052       0.98430008   0.77689999   0.97960007
    3.45751953]
 [-18.51940865   0.2529       0.44180003   0.14380001   0.43430001
    2.84535193]
 ...
 [-41.08513808   0.97780001   0.98870003   0.97970003   0.98699999
    2.7586937 ]
 [313.74209954   0.1499       0.51910007   0.3387       0.55970001
    2.16486335]
 [ 41.97893587   0.073        0.38600001   0.2253       0.39860004
    2.36537004]][0m
[37m[1m[2023-07-11 09:29:44,385][233954] Max Reward on eval: 711.1585922379047[0m
[37m[1m[2023-07-11 09:29:44,385][233954] Min Reward on eval: -319.7961864674464[0m
[37m[1m[2023-07-11 09:29:44,385][233954] Mean Reward across all agents: 128.61129243598552[0m
[37m[1m[2023-07-11 09:29:44,386][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:29:44,389][233954] mean_value=-257.71909277956087, max_value=481.77807272938776[0m
[37m[1m[2023-07-11 09:29:44,392][233954] New mean coefficients: [[-0.3823879  -1.3596958  -0.12770048  0.84841204 -1.1377848  -5.392302  ]][0m
[37m[1m[2023-07-11 09:29:44,393][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:29:53,314][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 09:29:53,315][233954] FPS: 430485.86[0m
[36m[2023-07-11 09:29:53,317][233954] itr=707, itrs=2000, Progress: 35.35%[0m
[36m[2023-07-11 09:30:05,200][233954] train() took 11.80 seconds to complete[0m
[36m[2023-07-11 09:30:05,200][233954] FPS: 325499.27[0m
[36m[2023-07-11 09:30:09,391][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:30:09,391][233954] Reward + Measures: [[19.68083101  0.13750766  0.173722    0.17912532  0.141257    0.46554181]][0m
[37m[1m[2023-07-11 09:30:09,391][233954] Max Reward on eval: 19.680831007763064[0m
[37m[1m[2023-07-11 09:30:09,392][233954] Min Reward on eval: 19.680831007763064[0m
[37m[1m[2023-07-11 09:30:09,392][233954] Mean Reward across all agents: 19.680831007763064[0m
[37m[1m[2023-07-11 09:30:09,392][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:30:14,615][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:30:14,616][233954] Reward + Measures: [[ 98.10571927   0.0935       0.29920003   0.16140001   0.27690002
    2.16301513]
 [ 18.05734205   0.25840002   0.42829999   0.1025       0.33260003
    2.02488184]
 [223.86306153   0.0623       0.46409997   0.29779997   0.44249997
    2.71260142]
 ...
 [-66.29248196   0.25510001   0.50300002   0.13850002   0.4501
    2.24523973]
 [179.332083     0.1068       0.6164       0.2888       0.53790003
    2.48039508]
 [197.05020284   0.36690003   0.84009999   0.4145       0.8301
    2.6359899 ]][0m
[37m[1m[2023-07-11 09:30:14,616][233954] Max Reward on eval: 625.9283046688885[0m
[37m[1m[2023-07-11 09:30:14,616][233954] Min Reward on eval: -84.67495441399515[0m
[37m[1m[2023-07-11 09:30:14,616][233954] Mean Reward across all agents: 106.39904033654393[0m
[37m[1m[2023-07-11 09:30:14,616][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:30:14,619][233954] mean_value=-323.80082538170075, max_value=470.14260440786137[0m
[37m[1m[2023-07-11 09:30:14,622][233954] New mean coefficients: [[-0.06412169 -0.5405222  -0.35545078  0.36271375 -0.32271576 -4.2133117 ]][0m
[37m[1m[2023-07-11 09:30:14,623][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:30:23,646][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 09:30:23,651][233954] FPS: 425663.26[0m
[36m[2023-07-11 09:30:23,654][233954] itr=708, itrs=2000, Progress: 35.40%[0m
[36m[2023-07-11 09:30:35,337][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 09:30:35,337][233954] FPS: 331186.42[0m
[36m[2023-07-11 09:30:39,660][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:30:39,666][233954] Reward + Measures: [[19.45574618  0.15185432  0.16256267  0.16523401  0.14635034  0.47637764]][0m
[37m[1m[2023-07-11 09:30:39,666][233954] Max Reward on eval: 19.45574618006748[0m
[37m[1m[2023-07-11 09:30:39,666][233954] Min Reward on eval: 19.45574618006748[0m
[37m[1m[2023-07-11 09:30:39,667][233954] Mean Reward across all agents: 19.45574618006748[0m
[37m[1m[2023-07-11 09:30:39,667][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:30:44,669][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:30:44,674][233954] Reward + Measures: [[-10.07609199   0.0953       0.2586       0.0984       0.19590001
    1.92398071]
 [-20.49125132   0.7313       0.75740004   0.75080001   0.82979995
    2.54412818]
 [ 22.10034274   0.1015       0.28220001   0.16080001   0.28740001
    2.49640584]
 ...
 [201.55998852   0.12929998   0.61970007   0.27360001   0.63310003
    2.57082725]
 [ -1.10716774   0.69420004   0.84380001   0.66180003   0.77180004
    2.2167592 ]
 [ -2.99184536   0.16540001   0.4526       0.20560001   0.4474
    2.43396974]][0m
[37m[1m[2023-07-11 09:30:44,675][233954] Max Reward on eval: 797.694778453256[0m
[37m[1m[2023-07-11 09:30:44,675][233954] Min Reward on eval: -171.15313078630714[0m
[37m[1m[2023-07-11 09:30:44,675][233954] Mean Reward across all agents: 146.19577228674243[0m
[37m[1m[2023-07-11 09:30:44,676][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:30:44,679][233954] mean_value=-337.0395438244868, max_value=344.9182133174693[0m
[37m[1m[2023-07-11 09:30:44,682][233954] New mean coefficients: [[ 0.8287009   0.5042145  -0.03635982 -0.06842285  0.8339906  -2.5516853 ]][0m
[37m[1m[2023-07-11 09:30:44,683][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:30:53,754][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 09:30:53,760][233954] FPS: 423370.74[0m
[36m[2023-07-11 09:30:53,762][233954] itr=709, itrs=2000, Progress: 35.45%[0m
[36m[2023-07-11 09:31:05,306][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 09:31:05,306][233954] FPS: 335252.23[0m
[36m[2023-07-11 09:31:09,553][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:31:09,558][233954] Reward + Measures: [[22.4866553   0.16600266  0.148276    0.1523      0.14105967  0.45289794]][0m
[37m[1m[2023-07-11 09:31:09,559][233954] Max Reward on eval: 22.486655301703685[0m
[37m[1m[2023-07-11 09:31:09,559][233954] Min Reward on eval: 22.486655301703685[0m
[37m[1m[2023-07-11 09:31:09,559][233954] Mean Reward across all agents: 22.486655301703685[0m
[37m[1m[2023-07-11 09:31:09,560][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:31:14,503][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:31:14,503][233954] Reward + Measures: [[ -1.5482281    0.27920005   0.46840006   0.16759999   0.47410002
    2.10413313]
 [-13.12839771   0.90340006   0.96520007   0.87910002   0.95149994
    2.78858352]
 [130.18375205   0.3917       0.74710006   0.34899998   0.63130003
    2.31966853]
 ...
 [-15.5939975    0.2376       0.4492       0.1652       0.3583
    1.86135352]
 [119.46572902   0.06519999   0.53200001   0.38530001   0.50240004
    2.28573608]
 [-29.98160349   0.73490006   0.85080004   0.68380004   0.81610006
    2.41998172]][0m
[37m[1m[2023-07-11 09:31:14,504][233954] Max Reward on eval: 376.2017707295716[0m
[37m[1m[2023-07-11 09:31:14,504][233954] Min Reward on eval: -415.6803837065119[0m
[37m[1m[2023-07-11 09:31:14,504][233954] Mean Reward across all agents: 50.74446218060722[0m
[37m[1m[2023-07-11 09:31:14,504][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:31:14,508][233954] mean_value=-256.21643993055386, max_value=585.6405581724935[0m
[37m[1m[2023-07-11 09:31:14,510][233954] New mean coefficients: [[ 0.79434496  0.32775718 -0.297385    0.4244143   0.5603593  -2.717994  ]][0m
[37m[1m[2023-07-11 09:31:14,511][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:31:23,534][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 09:31:23,539][233954] FPS: 425680.08[0m
[36m[2023-07-11 09:31:23,542][233954] itr=710, itrs=2000, Progress: 35.50%[0m
[37m[1m[2023-07-11 09:34:40,561][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000690[0m
[36m[2023-07-11 09:34:52,659][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 09:34:52,659][233954] FPS: 335145.26[0m
[36m[2023-07-11 09:34:56,856][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:34:56,857][233954] Reward + Measures: [[23.02095475  0.19891967  0.154257    0.17416267  0.14866833  0.43553784]][0m
[37m[1m[2023-07-11 09:34:56,857][233954] Max Reward on eval: 23.020954750547837[0m
[37m[1m[2023-07-11 09:34:56,857][233954] Min Reward on eval: 23.020954750547837[0m
[37m[1m[2023-07-11 09:34:56,857][233954] Mean Reward across all agents: 23.020954750547837[0m
[37m[1m[2023-07-11 09:34:56,858][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:35:01,735][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:35:01,736][233954] Reward + Measures: [[ 59.77194832   0.0707       0.14130001   0.092        0.11860001
    2.16672325]
 [ 21.18342613   0.152        0.20519999   0.11930001   0.17690001
    1.91769302]
 [157.36820696   0.24340001   0.86310005   0.4817       0.83649999
    2.49500704]
 ...
 [-37.20330687   0.68199998   0.75669998   0.65270001   0.7058
    2.7988739 ]
 [ 50.48823563   0.2771       0.8585       0.51490003   0.85669994
    3.04741096]
 [ 35.48647357   0.23480001   0.79180002   0.37850001   0.76899999
    2.37170267]][0m
[37m[1m[2023-07-11 09:35:01,736][233954] Max Reward on eval: 814.1115646053106[0m
[37m[1m[2023-07-11 09:35:01,736][233954] Min Reward on eval: -147.4120453346055[0m
[37m[1m[2023-07-11 09:35:01,736][233954] Mean Reward across all agents: 119.95798485354656[0m
[37m[1m[2023-07-11 09:35:01,737][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:35:01,740][233954] mean_value=-290.7339294292492, max_value=543.3593210579658[0m
[37m[1m[2023-07-11 09:35:01,742][233954] New mean coefficients: [[ 0.6462675   0.03595558 -0.06023659  0.2352515   0.36861658 -2.9817102 ]][0m
[37m[1m[2023-07-11 09:35:01,743][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:35:10,683][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 09:35:10,683][233954] FPS: 429641.23[0m
[36m[2023-07-11 09:35:10,685][233954] itr=711, itrs=2000, Progress: 35.55%[0m
[36m[2023-07-11 09:35:22,300][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 09:35:22,300][233954] FPS: 333177.21[0m
[36m[2023-07-11 09:35:26,568][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:35:26,569][233954] Reward + Measures: [[23.41896967  0.19315167  0.159872    0.18857267  0.15315168  0.434071  ]][0m
[37m[1m[2023-07-11 09:35:26,569][233954] Max Reward on eval: 23.41896967164656[0m
[37m[1m[2023-07-11 09:35:26,569][233954] Min Reward on eval: 23.41896967164656[0m
[37m[1m[2023-07-11 09:35:26,569][233954] Mean Reward across all agents: 23.41896967164656[0m
[37m[1m[2023-07-11 09:35:26,569][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:35:31,572][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:35:31,573][233954] Reward + Measures: [[ 16.82155146   0.16779999   0.3364       0.2005       0.37970003
    1.71358585]
 [  0.91642285   0.2924       0.303        0.30609998   0.3527
    2.33173871]
 [-52.21598699   0.1033       0.99480003   0.27970001   0.99319994
    2.85122561]
 ...
 [-29.87572234   0.44320002   0.5223       0.37420002   0.51929998
    1.75916374]
 [-45.03464649   0.37920001   0.7992       0.2404       0.79319996
    2.72130179]
 [349.17493726   0.0174       0.98120004   0.78400004   0.96240008
    2.5172267 ]][0m
[37m[1m[2023-07-11 09:35:31,573][233954] Max Reward on eval: 700.3278655957431[0m
[37m[1m[2023-07-11 09:35:31,573][233954] Min Reward on eval: -164.37873231514823[0m
[37m[1m[2023-07-11 09:35:31,573][233954] Mean Reward across all agents: 118.06209982614114[0m
[37m[1m[2023-07-11 09:35:31,574][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:35:31,578][233954] mean_value=-361.5362486175196, max_value=521.711677781084[0m
[37m[1m[2023-07-11 09:35:31,581][233954] New mean coefficients: [[ 0.2692833   0.85771155 -0.31810147  0.08536746  1.2365589  -2.2862058 ]][0m
[37m[1m[2023-07-11 09:35:31,582][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:35:40,654][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 09:35:40,655][233954] FPS: 423336.23[0m
[36m[2023-07-11 09:35:40,657][233954] itr=712, itrs=2000, Progress: 35.60%[0m
[36m[2023-07-11 09:35:52,462][233954] train() took 11.72 seconds to complete[0m
[36m[2023-07-11 09:35:52,463][233954] FPS: 327670.89[0m
[36m[2023-07-11 09:35:56,823][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:35:56,823][233954] Reward + Measures: [[20.53555149  0.22330366  0.14530466  0.17916101  0.18966468  0.44855171]][0m
[37m[1m[2023-07-11 09:35:56,823][233954] Max Reward on eval: 20.53555149078371[0m
[37m[1m[2023-07-11 09:35:56,824][233954] Min Reward on eval: 20.53555149078371[0m
[37m[1m[2023-07-11 09:35:56,824][233954] Mean Reward across all agents: 20.53555149078371[0m
[37m[1m[2023-07-11 09:35:56,824][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:36:01,766][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:36:01,767][233954] Reward + Measures: [[  23.68417929    0.161         0.08270001    0.1214        0.2
     0.83251601]
 [-101.7134285     0.1945        0.47030002    0.1825        0.47209999
     2.47730446]
 [ 563.78626541    0.0088        0.90649998    0.73970002    0.91339999
     2.39226437]
 ...
 [   2.97071385    0.17910001    0.0829        0.17030001    0.20369999
     1.53634381]
 [ -32.17406698    0.64069998    0.63060004    0.53850001    0.43689999
     2.12652373]
 [ -67.18051588    0.69370002    0.62919998    0.64449996    0.3698
     2.35420156]][0m
[37m[1m[2023-07-11 09:36:01,767][233954] Max Reward on eval: 694.2090435241349[0m
[37m[1m[2023-07-11 09:36:01,767][233954] Min Reward on eval: -273.8761701503769[0m
[37m[1m[2023-07-11 09:36:01,767][233954] Mean Reward across all agents: 59.843236337884456[0m
[37m[1m[2023-07-11 09:36:01,768][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:36:01,772][233954] mean_value=-194.40892611345078, max_value=708.8279832224232[0m
[37m[1m[2023-07-11 09:36:01,775][233954] New mean coefficients: [[ 0.10145497  1.4781941  -0.72634375 -0.46489817  1.8134882  -1.5730395 ]][0m
[37m[1m[2023-07-11 09:36:01,776][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:36:10,865][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 09:36:10,865][233954] FPS: 422567.22[0m
[36m[2023-07-11 09:36:10,868][233954] itr=713, itrs=2000, Progress: 35.65%[0m
[36m[2023-07-11 09:36:22,445][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 09:36:22,446][233954] FPS: 334282.85[0m
[36m[2023-07-11 09:36:26,661][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:36:26,661][233954] Reward + Measures: [[24.56854701  0.23126099  0.146264    0.18522835  0.21161732  0.44661862]][0m
[37m[1m[2023-07-11 09:36:26,661][233954] Max Reward on eval: 24.568547010990947[0m
[37m[1m[2023-07-11 09:36:26,661][233954] Min Reward on eval: 24.568547010990947[0m
[37m[1m[2023-07-11 09:36:26,662][233954] Mean Reward across all agents: 24.568547010990947[0m
[37m[1m[2023-07-11 09:36:26,662][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:36:31,624][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:36:31,625][233954] Reward + Measures: [[  97.08391569    0.66839999    0.2465        0.70090002    0.21170001
     2.42788148]
 [  55.0547541     0.81939995    0.18200001    0.838         0.1257
     2.47751045]
 [ 594.560009      0.0085        0.99760002    0.80120003    0.99410003
     2.48307109]
 ...
 [  42.55804455    0.67670006    0.25790003    0.75450003    0.2428
     2.47942567]
 [-125.19196126    0.53890008    0.79220003    0.41160002    0.53740001
     2.42173266]
 [ 165.24460671    0.08110001    0.5169        0.33950001    0.53659999
     2.35696483]][0m
[37m[1m[2023-07-11 09:36:31,625][233954] Max Reward on eval: 633.3772163482383[0m
[37m[1m[2023-07-11 09:36:31,625][233954] Min Reward on eval: -130.8673331814818[0m
[37m[1m[2023-07-11 09:36:31,625][233954] Mean Reward across all agents: 57.67490530241048[0m
[37m[1m[2023-07-11 09:36:31,626][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:36:31,632][233954] mean_value=-119.5025066765552, max_value=555.0547541001812[0m
[37m[1m[2023-07-11 09:36:31,635][233954] New mean coefficients: [[ 1.169973    2.7048237  -0.36474186 -1.2756093   2.911187    0.48428178]][0m
[37m[1m[2023-07-11 09:36:31,636][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:36:40,606][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 09:36:40,606][233954] FPS: 428137.58[0m
[36m[2023-07-11 09:36:40,609][233954] itr=714, itrs=2000, Progress: 35.70%[0m
[36m[2023-07-11 09:36:52,155][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 09:36:52,156][233954] FPS: 335065.13[0m
[36m[2023-07-11 09:36:56,519][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:36:56,519][233954] Reward + Measures: [[21.94243038  0.26893431  0.12440334  0.15171699  0.25330266  0.58556479]][0m
[37m[1m[2023-07-11 09:36:56,519][233954] Max Reward on eval: 21.94243037802565[0m
[37m[1m[2023-07-11 09:36:56,520][233954] Min Reward on eval: 21.94243037802565[0m
[37m[1m[2023-07-11 09:36:56,520][233954] Mean Reward across all agents: 21.94243037802565[0m
[37m[1m[2023-07-11 09:36:56,520][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:37:01,690][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:37:01,690][233954] Reward + Measures: [[ 162.9965069     0.09980001    0.13810001    0.0538        0.1086
     3.24907613]
 [  85.58461393    0.0732        0.2721        0.1645        0.25409999
     1.9911648 ]
 [  27.02922909    0.27770001    0.33710003    0.27250001    0.30689999
     1.75539291]
 ...
 [-148.71330847    0.42269999    0.90600008    0.1107        0.86830008
     2.73752832]
 [  64.76598758    0.07759999    0.1164        0.067         0.11730001
     2.61149621]
 [ -16.10921696    0.24849999    0.454         0.0851        0.44310004
     2.34436679]][0m
[37m[1m[2023-07-11 09:37:01,690][233954] Max Reward on eval: 401.78850363874807[0m
[37m[1m[2023-07-11 09:37:01,691][233954] Min Reward on eval: -337.1537056011148[0m
[37m[1m[2023-07-11 09:37:01,691][233954] Mean Reward across all agents: 44.54211328401034[0m
[37m[1m[2023-07-11 09:37:01,691][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:37:01,694][233954] mean_value=-314.14106606590656, max_value=506.48027207539417[0m
[37m[1m[2023-07-11 09:37:01,696][233954] New mean coefficients: [[ 1.4518335   2.0887487  -0.16686173 -1.1382686   2.3044705  -0.04046863]][0m
[37m[1m[2023-07-11 09:37:01,697][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:37:10,604][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 09:37:10,605][233954] FPS: 431191.80[0m
[36m[2023-07-11 09:37:10,607][233954] itr=715, itrs=2000, Progress: 35.75%[0m
[36m[2023-07-11 09:37:22,109][233954] train() took 11.42 seconds to complete[0m
[36m[2023-07-11 09:37:22,109][233954] FPS: 336343.61[0m
[36m[2023-07-11 09:37:26,356][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:37:26,357][233954] Reward + Measures: [[26.75525244  0.29321668  0.11176833  0.13010199  0.26466697  0.57940918]][0m
[37m[1m[2023-07-11 09:37:26,357][233954] Max Reward on eval: 26.755252440678596[0m
[37m[1m[2023-07-11 09:37:26,357][233954] Min Reward on eval: 26.755252440678596[0m
[37m[1m[2023-07-11 09:37:26,357][233954] Mean Reward across all agents: 26.755252440678596[0m
[37m[1m[2023-07-11 09:37:26,358][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:37:31,325][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:37:31,326][233954] Reward + Measures: [[211.81026246   0.0378       0.62740004   0.36160001   0.56370002
    2.61793971]
 [ 61.09678499   0.182        0.4021       0.2323       0.37450001
    2.37340403]
 [ 20.89893245   0.0981       0.1763       0.1364       0.16940001
    1.26452827]
 ...
 [ 54.86839193   0.126        0.31040001   0.16140001   0.27069998
    2.73954225]
 [ 91.92831504   0.10829999   0.3989       0.19400001   0.35499999
    2.6143074 ]
 [119.99520295   0.14790002   0.3378       0.19850002   0.29319999
    2.80502009]][0m
[37m[1m[2023-07-11 09:37:31,326][233954] Max Reward on eval: 501.98167993196404[0m
[37m[1m[2023-07-11 09:37:31,326][233954] Min Reward on eval: -113.66029362675036[0m
[37m[1m[2023-07-11 09:37:31,327][233954] Mean Reward across all agents: 121.08241600707746[0m
[37m[1m[2023-07-11 09:37:31,327][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:37:31,330][233954] mean_value=-233.84441616966254, max_value=615.562341222357[0m
[37m[1m[2023-07-11 09:37:31,332][233954] New mean coefficients: [[ 0.4136001   1.5184977  -0.61288947 -0.80334353  1.5479438  -0.94748354]][0m
[37m[1m[2023-07-11 09:37:31,333][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:37:40,307][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 09:37:40,308][233954] FPS: 427966.01[0m
[36m[2023-07-11 09:37:40,310][233954] itr=716, itrs=2000, Progress: 35.80%[0m
[36m[2023-07-11 09:37:52,139][233954] train() took 11.74 seconds to complete[0m
[36m[2023-07-11 09:37:52,140][233954] FPS: 327062.63[0m
[36m[2023-07-11 09:37:56,378][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:37:56,378][233954] Reward + Measures: [[27.12528818  0.34334901  0.112063    0.11860866  0.29930669  0.60885602]][0m
[37m[1m[2023-07-11 09:37:56,379][233954] Max Reward on eval: 27.125288184527445[0m
[37m[1m[2023-07-11 09:37:56,379][233954] Min Reward on eval: 27.125288184527445[0m
[37m[1m[2023-07-11 09:37:56,379][233954] Mean Reward across all agents: 27.125288184527445[0m
[37m[1m[2023-07-11 09:37:56,379][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:38:01,263][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:38:01,264][233954] Reward + Measures: [[ 24.46082773   0.21879999   0.634        0.1141       0.70410007
    3.03229761]
 [-11.74066322   0.2631       0.42659998   0.16790001   0.44310004
    2.13355947]
 [-51.44205243   0.33150002   0.6936       0.28079998   0.63679999
    2.16335702]
 ...
 [-21.58262262   0.2651       0.59909999   0.0741       0.62849998
    2.34538317]
 [ -8.28631623   0.33030003   0.3594       0.29519999   0.32549998
    2.06538701]
 [ 10.28068111   0.13600001   0.67530006   0.14570001   0.51660001
    2.23795819]][0m
[37m[1m[2023-07-11 09:38:01,264][233954] Max Reward on eval: 499.3944606651552[0m
[37m[1m[2023-07-11 09:38:01,264][233954] Min Reward on eval: -134.04813697692006[0m
[37m[1m[2023-07-11 09:38:01,265][233954] Mean Reward across all agents: 45.92572337657089[0m
[37m[1m[2023-07-11 09:38:01,265][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:38:01,268][233954] mean_value=-221.74805047897104, max_value=401.2192321795038[0m
[37m[1m[2023-07-11 09:38:01,271][233954] New mean coefficients: [[ 0.69679886  2.0576873  -1.1361206  -0.7666576   1.8395066   0.1340152 ]][0m
[37m[1m[2023-07-11 09:38:01,272][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:38:10,287][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 09:38:10,288][233954] FPS: 425990.78[0m
[36m[2023-07-11 09:38:10,290][233954] itr=717, itrs=2000, Progress: 35.85%[0m
[36m[2023-07-11 09:38:22,020][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 09:38:22,021][233954] FPS: 329884.20[0m
[36m[2023-07-11 09:38:26,356][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:38:26,356][233954] Reward + Measures: [[29.02946822  0.191028    0.11413132  0.11449367  0.17100665  0.84561098]][0m
[37m[1m[2023-07-11 09:38:26,356][233954] Max Reward on eval: 29.029468224032357[0m
[37m[1m[2023-07-11 09:38:26,357][233954] Min Reward on eval: 29.029468224032357[0m
[37m[1m[2023-07-11 09:38:26,357][233954] Mean Reward across all agents: 29.029468224032357[0m
[37m[1m[2023-07-11 09:38:26,357][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:38:31,372][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:38:31,373][233954] Reward + Measures: [[-36.57698319   0.59399998   0.74970001   0.56310004   0.78949994
    2.79844594]
 [ 41.98779513   0.5898       0.75510007   0.6103       0.56720001
    2.28820682]
 [ -3.41844648   0.14579999   0.25820002   0.1153       0.1655
    1.620242  ]
 ...
 [ 43.78307143   0.27149999   0.63430005   0.31560001   0.56470007
    2.80907226]
 [ 25.21797972   0.2119       0.75999999   0.23989999   0.75800002
    1.95456123]
 [-23.14686679   0.35789999   0.45089999   0.23999999   0.38799998
    2.4171834 ]][0m
[37m[1m[2023-07-11 09:38:31,373][233954] Max Reward on eval: 730.5248222222551[0m
[37m[1m[2023-07-11 09:38:31,373][233954] Min Reward on eval: -314.2634424873628[0m
[37m[1m[2023-07-11 09:38:31,374][233954] Mean Reward across all agents: 70.21130021972695[0m
[37m[1m[2023-07-11 09:38:31,374][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:38:31,378][233954] mean_value=-233.61021287739325, max_value=514.8594540659803[0m
[37m[1m[2023-07-11 09:38:31,381][233954] New mean coefficients: [[ 0.48529163  1.4794185  -0.74142224 -0.32545465  1.2130244  -0.6971945 ]][0m
[37m[1m[2023-07-11 09:38:31,382][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:38:40,386][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 09:38:40,386][233954] FPS: 426523.75[0m
[36m[2023-07-11 09:38:40,389][233954] itr=718, itrs=2000, Progress: 35.90%[0m
[36m[2023-07-11 09:38:52,099][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 09:38:52,100][233954] FPS: 330344.41[0m
[36m[2023-07-11 09:38:56,383][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:38:56,388][233954] Reward + Measures: [[32.19944186  0.23106767  0.11209033  0.10307234  0.18266733  0.84023857]][0m
[37m[1m[2023-07-11 09:38:56,389][233954] Max Reward on eval: 32.19944185693226[0m
[37m[1m[2023-07-11 09:38:56,389][233954] Min Reward on eval: 32.19944185693226[0m
[37m[1m[2023-07-11 09:38:56,389][233954] Mean Reward across all agents: 32.19944185693226[0m
[37m[1m[2023-07-11 09:38:56,390][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:39:01,395][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:39:01,400][233954] Reward + Measures: [[ 18.35837948   0.1883       0.38519999   0.16         0.34230003
    2.28296256]
 [ 53.53571907   0.2052       0.29949999   0.2149       0.35530001
    1.71213019]
 [ 58.91923513   0.2395       0.3935       0.271        0.44070002
    2.34255862]
 ...
 [-57.96071646   0.22989999   0.57620001   0.38079998   0.53420001
    2.47291923]
 [149.76882301   0.15800001   0.5927       0.37760001   0.58199996
    2.32367444]
 [115.69266672   0.1488       0.68949997   0.4111       0.75209999
    2.11996269]][0m
[37m[1m[2023-07-11 09:39:01,401][233954] Max Reward on eval: 493.5269454673864[0m
[37m[1m[2023-07-11 09:39:01,401][233954] Min Reward on eval: -116.85907095167786[0m
[37m[1m[2023-07-11 09:39:01,401][233954] Mean Reward across all agents: 60.44350706044734[0m
[37m[1m[2023-07-11 09:39:01,401][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:39:01,405][233954] mean_value=-376.602405629583, max_value=615.6926667199004[0m
[37m[1m[2023-07-11 09:39:01,407][233954] New mean coefficients: [[ 0.18386811  1.3705934  -1.1983552  -0.671443    0.94589806 -0.8997519 ]][0m
[37m[1m[2023-07-11 09:39:01,408][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:39:10,429][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 09:39:10,429][233954] FPS: 425756.57[0m
[36m[2023-07-11 09:39:10,432][233954] itr=719, itrs=2000, Progress: 35.95%[0m
[36m[2023-07-11 09:39:22,009][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 09:39:22,010][233954] FPS: 334304.25[0m
[36m[2023-07-11 09:39:26,371][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:39:26,377][233954] Reward + Measures: [[37.59755644  0.27302468  0.12477934  0.10841434  0.206393    0.80462652]][0m
[37m[1m[2023-07-11 09:39:26,377][233954] Max Reward on eval: 37.59755644499405[0m
[37m[1m[2023-07-11 09:39:26,378][233954] Min Reward on eval: 37.59755644499405[0m
[37m[1m[2023-07-11 09:39:26,378][233954] Mean Reward across all agents: 37.59755644499405[0m
[37m[1m[2023-07-11 09:39:26,378][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:39:31,586][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:39:31,591][233954] Reward + Measures: [[ 60.72073743   0.28779998   0.43490002   0.26250002   0.44959998
    1.96721005]
 [ 61.9185209    0.22119999   0.23290001   0.0522       0.25299999
    2.35824203]
 [-66.05559778   0.38229999   0.43319997   0.4082       0.3759
    2.08189774]
 ...
 [-29.31623944   0.56630003   0.42279997   0.56669998   0.1928
    2.5886457 ]
 [345.03399089   0.17650001   0.51870006   0.40360004   0.59240001
    2.80356908]
 [-68.11410435   0.27059999   0.62059993   0.0761       0.55250001
    2.5138464 ]][0m
[37m[1m[2023-07-11 09:39:31,592][233954] Max Reward on eval: 685.6820030231029[0m
[37m[1m[2023-07-11 09:39:31,592][233954] Min Reward on eval: -214.7326283321483[0m
[37m[1m[2023-07-11 09:39:31,592][233954] Mean Reward across all agents: 27.752064083770687[0m
[37m[1m[2023-07-11 09:39:31,593][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:39:31,596][233954] mean_value=-372.3942336028147, max_value=450.75237781266685[0m
[37m[1m[2023-07-11 09:39:31,599][233954] New mean coefficients: [[ 0.26122147  0.8763733  -0.8300229   0.02311194  0.5241188  -1.5770481 ]][0m
[37m[1m[2023-07-11 09:39:31,600][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:39:40,605][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 09:39:40,606][233954] FPS: 426501.20[0m
[36m[2023-07-11 09:39:40,608][233954] itr=720, itrs=2000, Progress: 36.00%[0m
[37m[1m[2023-07-11 09:43:04,063][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000700[0m
[36m[2023-07-11 09:43:16,265][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 09:43:16,265][233954] FPS: 332114.41[0m
[36m[2023-07-11 09:43:20,557][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:43:20,557][233954] Reward + Measures: [[33.30216692  0.27305201  0.11520433  0.10366499  0.19811967  0.73961151]][0m
[37m[1m[2023-07-11 09:43:20,557][233954] Max Reward on eval: 33.3021669174262[0m
[37m[1m[2023-07-11 09:43:20,558][233954] Min Reward on eval: 33.3021669174262[0m
[37m[1m[2023-07-11 09:43:20,558][233954] Mean Reward across all agents: 33.3021669174262[0m
[37m[1m[2023-07-11 09:43:20,558][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:43:25,550][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:43:25,550][233954] Reward + Measures: [[477.86527633   0.0088       0.98850006   0.73219997   0.98180002
    2.63405108]
 [219.6610937    0.15799999   0.83290005   0.47530004   0.78689998
    2.54575753]
 [149.4677701    0.16900001   0.8858       0.52150005   0.90430003
    2.78736115]
 ...
 [293.20177175   0.0131       0.98870003   0.68849993   0.98120004
    2.65189052]
 [ 53.23298604   0.184        0.42539999   0.28819999   0.49289998
    1.84695077]
 [152.74215985   0.33980003   0.83770001   0.51160002   0.71139997
    2.37221265]][0m
[37m[1m[2023-07-11 09:43:25,550][233954] Max Reward on eval: 651.8651390272659[0m
[37m[1m[2023-07-11 09:43:25,551][233954] Min Reward on eval: -197.58938589580356[0m
[37m[1m[2023-07-11 09:43:25,551][233954] Mean Reward across all agents: 96.55389519961463[0m
[37m[1m[2023-07-11 09:43:25,551][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:43:25,555][233954] mean_value=-259.03812882357676, max_value=446.3420817884392[0m
[37m[1m[2023-07-11 09:43:25,558][233954] New mean coefficients: [[ 0.13060974 -0.04944831 -0.70019734  0.55117655 -0.36232877 -2.9721599 ]][0m
[37m[1m[2023-07-11 09:43:25,559][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:43:34,583][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 09:43:34,583][233954] FPS: 425619.97[0m
[36m[2023-07-11 09:43:34,585][233954] itr=721, itrs=2000, Progress: 36.05%[0m
[36m[2023-07-11 09:43:46,155][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 09:43:46,156][233954] FPS: 334380.02[0m
[36m[2023-07-11 09:43:50,382][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:43:50,383][233954] Reward + Measures: [[30.27267607  0.29038399  0.10835899  0.10124934  0.21046633  0.69198501]][0m
[37m[1m[2023-07-11 09:43:50,383][233954] Max Reward on eval: 30.272676067121978[0m
[37m[1m[2023-07-11 09:43:50,383][233954] Min Reward on eval: 30.272676067121978[0m
[37m[1m[2023-07-11 09:43:50,383][233954] Mean Reward across all agents: 30.272676067121978[0m
[37m[1m[2023-07-11 09:43:50,384][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:43:55,306][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:43:55,306][233954] Reward + Measures: [[ 20.09982371   0.4206       0.62779999   0.33040002   0.42590004
    2.3783114 ]
 [364.30302451   0.0622       0.97059995   0.66920006   0.94880003
    3.03125548]
 [ 48.13764754   0.23170002   0.24750002   0.17900001   0.31750003
    1.73439813]
 ...
 [ 35.1124928    0.294        0.29210001   0.19010001   0.34130001
    1.84134448]
 [ 26.89624997   0.28190002   0.44329998   0.1671       0.48009998
    1.86457062]
 [-58.86209342   0.4605       0.59050006   0.30179998   0.40489998
    2.20103073]][0m
[37m[1m[2023-07-11 09:43:55,306][233954] Max Reward on eval: 616.2671165679582[0m
[37m[1m[2023-07-11 09:43:55,307][233954] Min Reward on eval: -182.21249448489397[0m
[37m[1m[2023-07-11 09:43:55,307][233954] Mean Reward across all agents: 38.357993312847306[0m
[37m[1m[2023-07-11 09:43:55,307][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:43:55,311][233954] mean_value=-378.77965869357223, max_value=408.1635662681912[0m
[37m[1m[2023-07-11 09:43:55,314][233954] New mean coefficients: [[-0.00181524  0.08898729 -0.6731758   0.77112746 -0.1742907  -2.7320158 ]][0m
[37m[1m[2023-07-11 09:43:55,315][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:44:04,226][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 09:44:04,227][233954] FPS: 430981.49[0m
[36m[2023-07-11 09:44:04,229][233954] itr=722, itrs=2000, Progress: 36.10%[0m
[36m[2023-07-11 09:44:15,749][233954] train() took 11.43 seconds to complete[0m
[36m[2023-07-11 09:44:15,749][233954] FPS: 335917.44[0m
[36m[2023-07-11 09:44:20,006][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:44:20,006][233954] Reward + Measures: [[27.73691408  0.30965999  0.11715534  0.12296867  0.23893265  0.64465719]][0m
[37m[1m[2023-07-11 09:44:20,006][233954] Max Reward on eval: 27.73691407787659[0m
[37m[1m[2023-07-11 09:44:20,006][233954] Min Reward on eval: 27.73691407787659[0m
[37m[1m[2023-07-11 09:44:20,007][233954] Mean Reward across all agents: 27.73691407787659[0m
[37m[1m[2023-07-11 09:44:20,007][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:44:24,953][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:44:24,953][233954] Reward + Measures: [[191.65591383   0.1699       0.61729997   0.4041       0.61750001
    2.9926641 ]
 [-12.46385759   0.85400003   0.90230006   0.82849997   0.86330003
    2.29320955]
 [ -1.820423     0.17310002   0.17119999   0.06350001   0.1717
    1.48370242]
 ...
 [ 95.75397509   0.0701       0.56590003   0.32690001   0.52410001
    2.31215906]
 [273.56788886   0.0437       0.73229998   0.61620003   0.69830006
    2.11117244]
 [ 23.24182944   0.2297       0.112        0.17490001   0.24969999
    1.62875545]][0m
[37m[1m[2023-07-11 09:44:24,953][233954] Max Reward on eval: 699.969814286381[0m
[37m[1m[2023-07-11 09:44:24,954][233954] Min Reward on eval: -161.63595125041903[0m
[37m[1m[2023-07-11 09:44:24,954][233954] Mean Reward across all agents: 94.42385222733763[0m
[37m[1m[2023-07-11 09:44:24,954][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:44:24,959][233954] mean_value=-251.79037006403834, max_value=644.5378140687011[0m
[37m[1m[2023-07-11 09:44:24,961][233954] New mean coefficients: [[-0.16432182  0.03154742 -0.2281256   0.7404118   0.07535498 -3.0262918 ]][0m
[37m[1m[2023-07-11 09:44:24,962][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:44:33,912][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 09:44:33,912][233954] FPS: 429128.99[0m
[36m[2023-07-11 09:44:33,915][233954] itr=723, itrs=2000, Progress: 36.15%[0m
[36m[2023-07-11 09:44:45,944][233954] train() took 11.94 seconds to complete[0m
[36m[2023-07-11 09:44:45,944][233954] FPS: 321514.27[0m
[36m[2023-07-11 09:44:50,206][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:44:50,207][233954] Reward + Measures: [[24.449216    0.29457068  0.11946366  0.12889966  0.24664666  0.62428725]][0m
[37m[1m[2023-07-11 09:44:50,207][233954] Max Reward on eval: 24.449216003984386[0m
[37m[1m[2023-07-11 09:44:50,207][233954] Min Reward on eval: 24.449216003984386[0m
[37m[1m[2023-07-11 09:44:50,207][233954] Mean Reward across all agents: 24.449216003984386[0m
[37m[1m[2023-07-11 09:44:50,208][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:44:55,156][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:44:55,162][233954] Reward + Measures: [[208.72947213   0.16129999   0.41500002   0.21619999   0.44969997
    2.63887525]
 [ 61.80804028   0.53540003   0.65040004   0.51350003   0.63739997
    2.25769663]
 [-31.1615164    0.0701       0.58179998   0.23320003   0.59130001
    2.00850415]
 ...
 [ 52.6484234    0.16559999   0.2024       0.14670001   0.24419999
    2.43361068]
 [ -9.64054822   0.11000001   0.23080002   0.0826       0.1733
    1.27736449]
 [ 61.56435177   0.3195       0.1158       0.0573       0.2494
    0.92372382]][0m
[37m[1m[2023-07-11 09:44:55,162][233954] Max Reward on eval: 590.3573036035523[0m
[37m[1m[2023-07-11 09:44:55,163][233954] Min Reward on eval: -108.44187491084449[0m
[37m[1m[2023-07-11 09:44:55,163][233954] Mean Reward across all agents: 95.02937680003026[0m
[37m[1m[2023-07-11 09:44:55,163][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:44:55,168][233954] mean_value=-333.7054744880192, max_value=858.5601328944788[0m
[37m[1m[2023-07-11 09:44:55,170][233954] New mean coefficients: [[ 0.00568399  0.26530996 -0.38432387  0.71717894  0.06696297 -2.608384  ]][0m
[37m[1m[2023-07-11 09:44:55,171][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:45:04,193][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 09:45:04,194][233954] FPS: 425701.47[0m
[36m[2023-07-11 09:45:04,196][233954] itr=724, itrs=2000, Progress: 36.20%[0m
[36m[2023-07-11 09:45:15,759][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 09:45:15,759][233954] FPS: 334654.29[0m
[36m[2023-07-11 09:45:20,025][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:45:20,026][233954] Reward + Measures: [[24.75216825  0.32050499  0.13607167  0.14891666  0.26232466  0.61686093]][0m
[37m[1m[2023-07-11 09:45:20,026][233954] Max Reward on eval: 24.752168249595385[0m
[37m[1m[2023-07-11 09:45:20,026][233954] Min Reward on eval: 24.752168249595385[0m
[37m[1m[2023-07-11 09:45:20,026][233954] Mean Reward across all agents: 24.752168249595385[0m
[37m[1m[2023-07-11 09:45:20,027][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:45:24,994][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:45:24,995][233954] Reward + Measures: [[ 10.57185913   0.6279       0.33950004   0.64709997   0.24159999
    2.04235101]
 [342.44180179   0.24399999   0.84209996   0.65700001   0.8193
    2.39568877]
 [-62.88294626   0.68340003   0.50920004   0.67290002   0.50810003
    2.49742484]
 ...
 [-19.6080738    0.0865       0.0859       0.06390001   0.097
    1.56715357]
 [-27.88892374   0.64219999   0.76189995   0.58999997   0.7136001
    2.04881072]
 [-54.36433118   0.48130003   0.4014       0.45949998   0.4522
    1.73264146]][0m
[37m[1m[2023-07-11 09:45:24,995][233954] Max Reward on eval: 707.3562426064163[0m
[37m[1m[2023-07-11 09:45:24,995][233954] Min Reward on eval: -263.1434122959152[0m
[37m[1m[2023-07-11 09:45:24,996][233954] Mean Reward across all agents: 59.843751417627026[0m
[37m[1m[2023-07-11 09:45:24,996][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:45:25,000][233954] mean_value=-413.8607532164508, max_value=639.3608756089583[0m
[37m[1m[2023-07-11 09:45:25,002][233954] New mean coefficients: [[ 0.82297146  1.2493768  -0.14788704 -0.04251111  0.9714738  -0.9163306 ]][0m
[37m[1m[2023-07-11 09:45:25,003][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:45:33,989][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 09:45:33,990][233954] FPS: 427412.93[0m
[36m[2023-07-11 09:45:33,992][233954] itr=725, itrs=2000, Progress: 36.25%[0m
[36m[2023-07-11 09:45:45,742][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 09:45:45,742][233954] FPS: 329202.19[0m
[36m[2023-07-11 09:45:49,939][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:45:49,940][233954] Reward + Measures: [[23.36031211  0.37998897  0.12594999  0.13641633  0.301882    0.60911578]][0m
[37m[1m[2023-07-11 09:45:49,940][233954] Max Reward on eval: 23.360312110083914[0m
[37m[1m[2023-07-11 09:45:49,940][233954] Min Reward on eval: 23.360312110083914[0m
[37m[1m[2023-07-11 09:45:49,941][233954] Mean Reward across all agents: 23.360312110083914[0m
[37m[1m[2023-07-11 09:45:49,941][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:45:54,886][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:45:54,887][233954] Reward + Measures: [[ 69.48754718   0.0654       0.21789999   0.14350002   0.23210001
    2.16030884]
 [166.79405221   0.0886       0.72090006   0.30399999   0.70970005
    2.06454659]
 [ 76.86466584   0.013        0.98979998   0.56620002   0.99020004
    2.62626338]
 ...
 [ 91.72156997   0.3468       0.1212       0.28270003   0.324
    1.94060802]
 [ 15.8142922    0.55630004   0.67480004   0.5345       0.59619999
    1.93928039]
 [ 53.72218315   0.0756       0.08270001   0.06650001   0.0826
    2.25048375]][0m
[37m[1m[2023-07-11 09:45:54,887][233954] Max Reward on eval: 508.2168819837272[0m
[37m[1m[2023-07-11 09:45:54,887][233954] Min Reward on eval: -155.85421122587286[0m
[37m[1m[2023-07-11 09:45:54,888][233954] Mean Reward across all agents: 82.84707885314941[0m
[37m[1m[2023-07-11 09:45:54,888][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:45:54,892][233954] mean_value=-211.38447257120973, max_value=553.0873681968078[0m
[37m[1m[2023-07-11 09:45:54,895][233954] New mean coefficients: [[ 0.13301885  0.7676079  -0.46649504  0.09517638  0.6146039  -2.0836182 ]][0m
[37m[1m[2023-07-11 09:45:54,896][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:46:03,784][233954] train() took 8.89 seconds to complete[0m
[36m[2023-07-11 09:46:03,784][233954] FPS: 432118.63[0m
[36m[2023-07-11 09:46:03,787][233954] itr=726, itrs=2000, Progress: 36.30%[0m
[36m[2023-07-11 09:46:15,363][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 09:46:15,364][233954] FPS: 334275.40[0m
[36m[2023-07-11 09:46:19,598][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:46:19,598][233954] Reward + Measures: [[23.18317878  0.402031    0.12488899  0.14390767  0.31847566  0.59435618]][0m
[37m[1m[2023-07-11 09:46:19,599][233954] Max Reward on eval: 23.183178782886305[0m
[37m[1m[2023-07-11 09:46:19,599][233954] Min Reward on eval: 23.183178782886305[0m
[37m[1m[2023-07-11 09:46:19,599][233954] Mean Reward across all agents: 23.183178782886305[0m
[37m[1m[2023-07-11 09:46:19,599][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:46:24,572][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:46:24,573][233954] Reward + Measures: [[121.79063669   0.24220002   0.31200001   0.1292       0.3513
    1.7234391 ]
 [  9.67954435   0.44060001   0.1665       0.43670002   0.43720004
    1.36159778]
 [ -7.48682618   0.3387       0.70700008   0.39049998   0.65939999
    1.98814857]
 ...
 [ 74.53883947   0.0929       0.42020002   0.1444       0.29879999
    1.5226084 ]
 [ 91.73719046   0.2067       0.18370001   0.0735       0.2418
    1.79649353]
 [ 30.1403826    0.32090002   0.0667       0.0366       0.2053
    0.65881932]][0m
[37m[1m[2023-07-11 09:46:24,573][233954] Max Reward on eval: 647.7663345262408[0m
[37m[1m[2023-07-11 09:46:24,573][233954] Min Reward on eval: -102.52531194752082[0m
[37m[1m[2023-07-11 09:46:24,573][233954] Mean Reward across all agents: 95.8587863305506[0m
[37m[1m[2023-07-11 09:46:24,574][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:46:24,580][233954] mean_value=-334.7640617885131, max_value=727.9138231169619[0m
[37m[1m[2023-07-11 09:46:24,583][233954] New mean coefficients: [[ 0.49699417  0.81248677 -0.4473713  -0.38915467  0.75203    -1.8884406 ]][0m
[37m[1m[2023-07-11 09:46:24,584][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:46:33,549][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 09:46:33,550][233954] FPS: 428387.83[0m
[36m[2023-07-11 09:46:33,552][233954] itr=727, itrs=2000, Progress: 36.35%[0m
[36m[2023-07-11 09:46:45,306][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 09:46:45,307][233954] FPS: 329091.08[0m
[36m[2023-07-11 09:46:49,553][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:46:49,553][233954] Reward + Measures: [[22.45407118  0.38361433  0.12028566  0.13885266  0.32489735  0.57114738]][0m
[37m[1m[2023-07-11 09:46:49,554][233954] Max Reward on eval: 22.454071177649322[0m
[37m[1m[2023-07-11 09:46:49,554][233954] Min Reward on eval: 22.454071177649322[0m
[37m[1m[2023-07-11 09:46:49,554][233954] Mean Reward across all agents: 22.454071177649322[0m
[37m[1m[2023-07-11 09:46:49,554][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:46:54,585][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:46:54,586][233954] Reward + Measures: [[ 28.22981866   0.1071       0.48020002   0.10880001   0.30060002
    1.87056637]
 [ 54.80864143   0.252        0.33119997   0.24710003   0.33650002
    1.66417885]
 [-39.10536437   0.40639997   0.3249       0.09940001   0.39399999
    1.22701943]
 ...
 [308.81880999   0.0519       0.93669999   0.54000008   0.92250007
    2.55849123]
 [-18.14689907   0.24530001   0.1804       0.0725       0.2199
    1.48168027]
 [262.08191492   0.0925       0.75300002   0.61430001   0.77450001
    2.54192853]][0m
[37m[1m[2023-07-11 09:46:54,586][233954] Max Reward on eval: 737.6207904765382[0m
[37m[1m[2023-07-11 09:46:54,586][233954] Min Reward on eval: -128.56026510391385[0m
[37m[1m[2023-07-11 09:46:54,587][233954] Mean Reward across all agents: 199.09361493459411[0m
[37m[1m[2023-07-11 09:46:54,587][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:46:54,591][233954] mean_value=-294.88142696474296, max_value=632.934492235852[0m
[37m[1m[2023-07-11 09:46:54,594][233954] New mean coefficients: [[ 0.5540318   0.9125931  -0.7577621  -0.47999617  0.69580954 -1.5904719 ]][0m
[37m[1m[2023-07-11 09:46:54,595][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:47:03,489][233954] train() took 8.89 seconds to complete[0m
[36m[2023-07-11 09:47:03,490][233954] FPS: 431814.06[0m
[36m[2023-07-11 09:47:03,492][233954] itr=728, itrs=2000, Progress: 36.40%[0m
[36m[2023-07-11 09:47:15,255][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 09:47:15,256][233954] FPS: 328815.27[0m
[36m[2023-07-11 09:47:19,569][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:47:19,569][233954] Reward + Measures: [[23.39912652  0.42345101  0.12018533  0.13050501  0.33759668  0.56575847]][0m
[37m[1m[2023-07-11 09:47:19,570][233954] Max Reward on eval: 23.39912651601487[0m
[37m[1m[2023-07-11 09:47:19,570][233954] Min Reward on eval: 23.39912651601487[0m
[37m[1m[2023-07-11 09:47:19,570][233954] Mean Reward across all agents: 23.39912651601487[0m
[37m[1m[2023-07-11 09:47:19,570][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:47:24,831][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:47:24,832][233954] Reward + Measures: [[108.93836119   0.2554       0.82450002   0.2247       0.79930007
    2.44236636]
 [206.82697559   0.33259997   0.33849999   0.1533       0.42149997
    2.24096298]
 [  8.00207535   0.20220001   0.13789999   0.0568       0.18179999
    1.19078743]
 ...
 [ 46.42536164   0.24300002   0.368        0.12890001   0.3608
    1.90243709]
 [  0.63982441   0.1061       0.20380001   0.0938       0.1787
    1.54355466]
 [ 80.60160451   0.1323       0.60230005   0.23660003   0.50299996
    1.76451933]][0m
[37m[1m[2023-07-11 09:47:24,832][233954] Max Reward on eval: 747.2515945366816[0m
[37m[1m[2023-07-11 09:47:24,833][233954] Min Reward on eval: -192.91713147561532[0m
[37m[1m[2023-07-11 09:47:24,833][233954] Mean Reward across all agents: 81.04604349850355[0m
[37m[1m[2023-07-11 09:47:24,833][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:47:24,838][233954] mean_value=-459.94511169890416, max_value=489.19991269841114[0m
[37m[1m[2023-07-11 09:47:24,840][233954] New mean coefficients: [[ 0.67643136  0.8059953  -0.5203788  -0.31865004  0.7692181  -1.5293419 ]][0m
[37m[1m[2023-07-11 09:47:24,841][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:47:33,881][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 09:47:33,881][233954] FPS: 424888.17[0m
[36m[2023-07-11 09:47:33,883][233954] itr=729, itrs=2000, Progress: 36.45%[0m
[36m[2023-07-11 09:47:45,440][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 09:47:45,440][233954] FPS: 334889.85[0m
[36m[2023-07-11 09:47:49,809][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:47:49,810][233954] Reward + Measures: [[24.39686733  0.40422797  0.11661067  0.13453567  0.30055097  0.55082268]][0m
[37m[1m[2023-07-11 09:47:49,810][233954] Max Reward on eval: 24.396867327977553[0m
[37m[1m[2023-07-11 09:47:49,810][233954] Min Reward on eval: 24.396867327977553[0m
[37m[1m[2023-07-11 09:47:49,810][233954] Mean Reward across all agents: 24.396867327977553[0m
[37m[1m[2023-07-11 09:47:49,811][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:47:54,819][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:47:54,820][233954] Reward + Measures: [[ 67.85276198   0.6674       0.70180005   0.6476       0.59549999
    1.82485771]
 [ -3.76941925   0.37820002   0.30680001   0.21570002   0.3766
    1.23871279]
 [ -7.5401251    0.3813       0.8707       0.0702       0.86280006
    2.49743581]
 ...
 [ -4.85096557   0.86160004   0.87869996   0.87580007   0.88770008
    2.3022542 ]
 [234.82561873   0.212        0.47310001   0.21210001   0.51419997
    2.50003219]
 [  9.01419851   0.1329       0.15809999   0.13149999   0.21429999
    2.48125005]][0m
[37m[1m[2023-07-11 09:47:54,820][233954] Max Reward on eval: 765.2975807087496[0m
[37m[1m[2023-07-11 09:47:54,820][233954] Min Reward on eval: -262.30106979031115[0m
[37m[1m[2023-07-11 09:47:54,821][233954] Mean Reward across all agents: 89.05399390315492[0m
[37m[1m[2023-07-11 09:47:54,821][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:47:54,825][233954] mean_value=-290.4570985648906, max_value=711.4616696664646[0m
[37m[1m[2023-07-11 09:47:54,828][233954] New mean coefficients: [[ 0.14968628  0.57319474 -0.83205634 -0.2721128   0.77527165 -1.9024565 ]][0m
[37m[1m[2023-07-11 09:47:54,828][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:48:03,924][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 09:48:03,924][233954] FPS: 422285.33[0m
[36m[2023-07-11 09:48:03,926][233954] itr=730, itrs=2000, Progress: 36.50%[0m
[37m[1m[2023-07-11 09:51:20,859][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000710[0m
[36m[2023-07-11 09:51:33,044][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 09:51:33,044][233954] FPS: 332783.51[0m
[36m[2023-07-11 09:51:37,318][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:51:37,318][233954] Reward + Measures: [[24.06430677  0.40603864  0.10174268  0.11125501  0.31298402  0.52225906]][0m
[37m[1m[2023-07-11 09:51:37,318][233954] Max Reward on eval: 24.064306772026164[0m
[37m[1m[2023-07-11 09:51:37,319][233954] Min Reward on eval: 24.064306772026164[0m
[37m[1m[2023-07-11 09:51:37,319][233954] Mean Reward across all agents: 24.064306772026164[0m
[37m[1m[2023-07-11 09:51:37,319][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:51:42,274][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:51:42,275][233954] Reward + Measures: [[ -0.75049251   0.5011       0.62589997   0.47849998   0.5794
    1.85912216]
 [ 36.99005636   0.10039999   0.10829999   0.05449999   0.1
    1.4176861 ]
 [ 17.99491029   0.17380001   0.16419999   0.1366       0.16760002
    1.53441429]
 ...
 [ 10.50317036   0.2429       0.20220001   0.09350001   0.2431
    1.26576424]
 [-19.44756043   0.1235       0.20509999   0.1132       0.23899999
    1.6364857 ]
 [ 23.39229918   0.25030002   0.30950001   0.21430002   0.29120001
    1.69289517]][0m
[37m[1m[2023-07-11 09:51:42,275][233954] Max Reward on eval: 456.6217813506722[0m
[37m[1m[2023-07-11 09:51:42,275][233954] Min Reward on eval: -70.69259112300351[0m
[37m[1m[2023-07-11 09:51:42,275][233954] Mean Reward across all agents: 43.541363807197264[0m
[37m[1m[2023-07-11 09:51:42,276][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:51:42,279][233954] mean_value=-665.0839611180387, max_value=588.4691336617805[0m
[37m[1m[2023-07-11 09:51:42,281][233954] New mean coefficients: [[ 0.8096677   0.7203474  -0.2165789  -0.31386164  0.85252994 -1.5704551 ]][0m
[37m[1m[2023-07-11 09:51:42,282][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:51:51,233][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 09:51:51,233][233954] FPS: 429119.23[0m
[36m[2023-07-11 09:51:51,235][233954] itr=731, itrs=2000, Progress: 36.55%[0m
[36m[2023-07-11 09:52:02,974][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 09:52:02,974][233954] FPS: 329588.22[0m
[36m[2023-07-11 09:52:07,279][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:52:07,280][233954] Reward + Measures: [[25.44593452  0.41168302  0.09938665  0.10756732  0.326738    0.52155083]][0m
[37m[1m[2023-07-11 09:52:07,280][233954] Max Reward on eval: 25.4459345205388[0m
[37m[1m[2023-07-11 09:52:07,280][233954] Min Reward on eval: 25.4459345205388[0m
[37m[1m[2023-07-11 09:52:07,281][233954] Mean Reward across all agents: 25.4459345205388[0m
[37m[1m[2023-07-11 09:52:07,281][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:52:12,200][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:52:12,200][233954] Reward + Measures: [[ 85.78540535   0.90570003   0.92320007   0.88330001   0.91600001
    2.51794267]
 [162.7016063    0.1718       0.96920007   0.50360006   0.96740001
    3.0949924 ]
 [216.6239495    0.0797       0.92870009   0.6304       0.9346
    3.38070154]
 ...
 [-10.53746432   0.11359999   0.25349998   0.08130001   0.2095
    1.66186559]
 [592.3974266    0.0021       0.99560004   0.75660002   0.99249995
    2.55781102]
 [ 75.19391442   0.049        0.89350003   0.40710002   0.88889998
    3.07075262]][0m
[37m[1m[2023-07-11 09:52:12,200][233954] Max Reward on eval: 755.6630554465577[0m
[37m[1m[2023-07-11 09:52:12,201][233954] Min Reward on eval: -190.1161809630401[0m
[37m[1m[2023-07-11 09:52:12,201][233954] Mean Reward across all agents: 156.90410684612274[0m
[37m[1m[2023-07-11 09:52:12,201][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:52:12,204][233954] mean_value=-325.3841542192186, max_value=578.4879241530783[0m
[37m[1m[2023-07-11 09:52:12,207][233954] New mean coefficients: [[ 0.12999809  0.16129768 -0.506268   -0.10095689  0.3988785  -2.8552718 ]][0m
[37m[1m[2023-07-11 09:52:12,208][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:52:21,090][233954] train() took 8.88 seconds to complete[0m
[36m[2023-07-11 09:52:21,090][233954] FPS: 432408.96[0m
[36m[2023-07-11 09:52:21,092][233954] itr=732, itrs=2000, Progress: 36.60%[0m
[36m[2023-07-11 09:52:32,687][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 09:52:32,687][233954] FPS: 333701.90[0m
[36m[2023-07-11 09:52:36,949][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:52:36,949][233954] Reward + Measures: [[23.57938876  0.41806901  0.10715267  0.12008899  0.34260434  0.51637322]][0m
[37m[1m[2023-07-11 09:52:36,949][233954] Max Reward on eval: 23.57938875789195[0m
[37m[1m[2023-07-11 09:52:36,950][233954] Min Reward on eval: 23.57938875789195[0m
[37m[1m[2023-07-11 09:52:36,950][233954] Mean Reward across all agents: 23.57938875789195[0m
[37m[1m[2023-07-11 09:52:36,950][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:52:41,914][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:52:41,915][233954] Reward + Measures: [[566.77636167   0.0587       0.81520003   0.67640001   0.8215
    2.38090444]
 [529.78907395   0.002        0.99279994   0.74430001   0.99089998
    2.90994668]
 [ 23.74759324   0.1767       0.29190001   0.18410002   0.23740001
    1.78465772]
 ...
 [147.57061352   0.1163       0.32539999   0.19059999   0.3536
    2.0850811 ]
 [  4.81132892   0.0632       0.0763       0.05520001   0.1078
    1.94650459]
 [118.06276372   0.0256       0.97480005   0.44230005   0.92770004
    2.90881705]][0m
[37m[1m[2023-07-11 09:52:41,915][233954] Max Reward on eval: 733.5734481735155[0m
[37m[1m[2023-07-11 09:52:41,915][233954] Min Reward on eval: -111.16967986468225[0m
[37m[1m[2023-07-11 09:52:41,916][233954] Mean Reward across all agents: 165.89943886010403[0m
[37m[1m[2023-07-11 09:52:41,916][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:52:41,920][233954] mean_value=-336.7891508023175, max_value=517.6462917133285[0m
[37m[1m[2023-07-11 09:52:41,923][233954] New mean coefficients: [[ 0.15401903  0.13766891 -0.42790172 -0.196836    0.17319977 -2.887929  ]][0m
[37m[1m[2023-07-11 09:52:41,924][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:52:50,843][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 09:52:50,843][233954] FPS: 430614.02[0m
[36m[2023-07-11 09:52:50,845][233954] itr=733, itrs=2000, Progress: 36.65%[0m
[36m[2023-07-11 09:53:02,349][233954] train() took 11.42 seconds to complete[0m
[36m[2023-07-11 09:53:02,349][233954] FPS: 336301.74[0m
[36m[2023-07-11 09:53:06,618][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:53:06,618][233954] Reward + Measures: [[23.19243706  0.42313629  0.10649833  0.11568566  0.34404001  0.51402074]][0m
[37m[1m[2023-07-11 09:53:06,619][233954] Max Reward on eval: 23.19243705666073[0m
[37m[1m[2023-07-11 09:53:06,619][233954] Min Reward on eval: 23.19243705666073[0m
[37m[1m[2023-07-11 09:53:06,619][233954] Mean Reward across all agents: 23.19243705666073[0m
[37m[1m[2023-07-11 09:53:06,619][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:53:11,584][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:53:11,585][233954] Reward + Measures: [[74.76142682  0.24169998  0.92200005  0.12860002  0.92469996  2.59914231]
 [22.22917721  0.19950001  0.68900007  0.16729999  0.62120003  2.71127391]
 [33.85470223  0.0842      0.1247      0.0918      0.16690001  1.16307867]
 ...
 [77.13788724  0.32209998  0.32790002  0.089       0.34829998  1.64965725]
 [56.90310457  0.112       0.2465      0.13039999  0.23510002  2.54838681]
 [68.90600492  0.98680001  0.9884001   0.9752      0.986       3.04290247]][0m
[37m[1m[2023-07-11 09:53:11,585][233954] Max Reward on eval: 733.326965329796[0m
[37m[1m[2023-07-11 09:53:11,585][233954] Min Reward on eval: -60.577920477837324[0m
[37m[1m[2023-07-11 09:53:11,586][233954] Mean Reward across all agents: 95.28759282050996[0m
[37m[1m[2023-07-11 09:53:11,586][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:53:11,589][233954] mean_value=-395.2057872970449, max_value=539.950065098796[0m
[37m[1m[2023-07-11 09:53:11,591][233954] New mean coefficients: [[ 0.31871456  0.26059014 -0.37837505 -0.2504486   0.2841398  -2.7176442 ]][0m
[37m[1m[2023-07-11 09:53:11,592][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:53:20,651][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 09:53:20,652][233954] FPS: 423966.03[0m
[36m[2023-07-11 09:53:20,654][233954] itr=734, itrs=2000, Progress: 36.70%[0m
[36m[2023-07-11 09:53:32,479][233954] train() took 11.74 seconds to complete[0m
[36m[2023-07-11 09:53:32,479][233954] FPS: 327147.84[0m
[36m[2023-07-11 09:53:36,729][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:53:36,730][233954] Reward + Measures: [[22.93176604  0.438795    0.10559367  0.113858    0.36160201  0.50345236]][0m
[37m[1m[2023-07-11 09:53:36,730][233954] Max Reward on eval: 22.93176603741833[0m
[37m[1m[2023-07-11 09:53:36,730][233954] Min Reward on eval: 22.93176603741833[0m
[37m[1m[2023-07-11 09:53:36,730][233954] Mean Reward across all agents: 22.93176603741833[0m
[37m[1m[2023-07-11 09:53:36,731][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:53:41,932][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:53:41,933][233954] Reward + Measures: [[334.89359094   0.0475       0.77649999   0.52509999   0.76989996
    2.85531402]
 [ 24.18536689   0.23889999   0.37530002   0.1754       0.37480003
    1.80776978]
 [ 33.98837947   0.50749999   0.70520002   0.52450007   0.72460002
    2.31423759]
 ...
 [ -6.33780341   0.18669999   0.15449999   0.1691       0.25040001
    1.65635645]
 [ 63.50583001   0.40380001   0.58200008   0.3527       0.67440003
    2.287637  ]
 [  7.4793245    0.3127       0.31870002   0.24529998   0.31469998
    1.80807042]][0m
[37m[1m[2023-07-11 09:53:41,933][233954] Max Reward on eval: 772.4984283445403[0m
[37m[1m[2023-07-11 09:53:41,933][233954] Min Reward on eval: -91.14771748911589[0m
[37m[1m[2023-07-11 09:53:41,934][233954] Mean Reward across all agents: 93.99159837530901[0m
[37m[1m[2023-07-11 09:53:41,934][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:53:41,938][233954] mean_value=-402.0883353251579, max_value=536.3378450970165[0m
[37m[1m[2023-07-11 09:53:41,941][233954] New mean coefficients: [[ 0.44365942  0.35544458  0.18724388 -0.04753332  0.39119038 -2.2524605 ]][0m
[37m[1m[2023-07-11 09:53:41,942][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:53:50,930][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 09:53:50,930][233954] FPS: 427315.86[0m
[36m[2023-07-11 09:53:50,932][233954] itr=735, itrs=2000, Progress: 36.75%[0m
[36m[2023-07-11 09:54:02,762][233954] train() took 11.74 seconds to complete[0m
[36m[2023-07-11 09:54:02,767][233954] FPS: 327010.40[0m
[36m[2023-07-11 09:54:07,061][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:54:07,061][233954] Reward + Measures: [[24.44511135  0.44250366  0.11885399  0.12072233  0.37197134  0.50085473]][0m
[37m[1m[2023-07-11 09:54:07,061][233954] Max Reward on eval: 24.445111348047387[0m
[37m[1m[2023-07-11 09:54:07,062][233954] Min Reward on eval: 24.445111348047387[0m
[37m[1m[2023-07-11 09:54:07,062][233954] Mean Reward across all agents: 24.445111348047387[0m
[37m[1m[2023-07-11 09:54:07,062][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:54:12,013][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:54:12,014][233954] Reward + Measures: [[ 63.57389248   0.62959999   0.64970005   0.62129998   0.6182
    2.50674486]
 [ 69.19325183   0.2113       0.92430001   0.271        0.8585
    2.22510266]
 [-23.6985556    0.546        0.60140008   0.5244       0.55540001
    2.34946513]
 ...
 [ 45.03939795   0.24870001   0.67809999   0.10810001   0.67049998
    1.90306973]
 [611.88623621   0.0174       0.90039998   0.72960001   0.89920008
    2.8089509 ]
 [-35.44920535   0.38770002   0.33909997   0.11790001   0.43430004
    1.67132497]][0m
[37m[1m[2023-07-11 09:54:12,014][233954] Max Reward on eval: 736.4396591203288[0m
[37m[1m[2023-07-11 09:54:12,014][233954] Min Reward on eval: -105.52373524010181[0m
[37m[1m[2023-07-11 09:54:12,014][233954] Mean Reward across all agents: 165.92208145224987[0m
[37m[1m[2023-07-11 09:54:12,015][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:54:12,019][233954] mean_value=-385.71025152139833, max_value=420.85677134351357[0m
[37m[1m[2023-07-11 09:54:12,022][233954] New mean coefficients: [[ 0.44995275  0.14675684  0.1112663  -0.00382746  0.24758904 -2.5955737 ]][0m
[37m[1m[2023-07-11 09:54:12,023][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:54:20,953][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 09:54:20,953][233954] FPS: 430077.27[0m
[36m[2023-07-11 09:54:20,955][233954] itr=736, itrs=2000, Progress: 36.80%[0m
[36m[2023-07-11 09:54:32,520][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 09:54:32,521][233954] FPS: 334546.41[0m
[36m[2023-07-11 09:54:36,740][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:54:36,741][233954] Reward + Measures: [[23.12420039  0.45561966  0.11542466  0.11073133  0.38661966  0.4899964 ]][0m
[37m[1m[2023-07-11 09:54:36,741][233954] Max Reward on eval: 23.124200392136746[0m
[37m[1m[2023-07-11 09:54:36,741][233954] Min Reward on eval: 23.124200392136746[0m
[37m[1m[2023-07-11 09:54:36,742][233954] Mean Reward across all agents: 23.124200392136746[0m
[37m[1m[2023-07-11 09:54:36,742][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:54:41,704][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:54:41,705][233954] Reward + Measures: [[-13.21146144   0.51599997   0.60040003   0.47610003   0.51480001
    1.86816049]
 [ 11.93749083   0.86250001   0.87919998   0.8398       0.86030006
    2.13950324]
 [-18.58407695   0.54500002   0.77219999   0.47939998   0.7202
    2.0141108 ]
 ...
 [-38.96269822   0.96780008   0.9684       0.96670002   0.97220004
    2.69839454]
 [222.68792752   0.25400001   0.56409997   0.35850003   0.61110002
    3.00734329]
 [ 40.42079118   0.06220001   0.1276       0.0715       0.11080001
    2.06748295]][0m
[37m[1m[2023-07-11 09:54:41,705][233954] Max Reward on eval: 680.1304268897511[0m
[37m[1m[2023-07-11 09:54:41,705][233954] Min Reward on eval: -95.88722345242277[0m
[37m[1m[2023-07-11 09:54:41,706][233954] Mean Reward across all agents: 58.271342606667965[0m
[37m[1m[2023-07-11 09:54:41,706][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:54:41,710][233954] mean_value=-521.5968973646892, max_value=563.215600893633[0m
[37m[1m[2023-07-11 09:54:41,712][233954] New mean coefficients: [[ 0.18337053 -0.14546864 -0.13477163 -0.09429002  0.09419583 -3.219868  ]][0m
[37m[1m[2023-07-11 09:54:41,713][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:54:50,674][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 09:54:50,674][233954] FPS: 428614.15[0m
[36m[2023-07-11 09:54:50,677][233954] itr=737, itrs=2000, Progress: 36.85%[0m
[36m[2023-07-11 09:55:02,803][233954] train() took 12.04 seconds to complete[0m
[36m[2023-07-11 09:55:02,803][233954] FPS: 318933.67[0m
[36m[2023-07-11 09:55:07,158][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:55:07,159][233954] Reward + Measures: [[22.93296951  0.40476933  0.11352266  0.105       0.34948868  0.47501701]][0m
[37m[1m[2023-07-11 09:55:07,159][233954] Max Reward on eval: 22.932969514100385[0m
[37m[1m[2023-07-11 09:55:07,159][233954] Min Reward on eval: 22.932969514100385[0m
[37m[1m[2023-07-11 09:55:07,160][233954] Mean Reward across all agents: 22.932969514100385[0m
[37m[1m[2023-07-11 09:55:07,160][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:55:12,151][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:55:12,152][233954] Reward + Measures: [[45.26797316  0.16610001  0.18740001  0.1858      0.2498      1.19905984]
 [43.8240392   0.79549998  0.93339998  0.79329997  0.89469999  2.28236175]
 [78.64278635  0.7428      0.83330005  0.70480001  0.81470007  1.87613142]
 ...
 [22.28241681  0.51150006  0.59709996  0.45300004  0.54520005  1.68740642]
 [21.6435824   0.2086      0.2289      0.18630001  0.22839999  1.95338953]
 [10.51022706  0.59380001  0.84240001  0.57059997  0.70179999  2.04584169]][0m
[37m[1m[2023-07-11 09:55:12,152][233954] Max Reward on eval: 561.2820739629212[0m
[37m[1m[2023-07-11 09:55:12,153][233954] Min Reward on eval: -109.86356257791631[0m
[37m[1m[2023-07-11 09:55:12,153][233954] Mean Reward across all agents: 92.68305993399336[0m
[37m[1m[2023-07-11 09:55:12,153][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:55:12,159][233954] mean_value=-248.0195625069733, max_value=1012.7405384062789[0m
[37m[1m[2023-07-11 09:55:12,162][233954] New mean coefficients: [[-0.10075808 -0.35084817 -0.5109422   0.14813901 -0.05818371 -3.6570034 ]][0m
[37m[1m[2023-07-11 09:55:12,163][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:55:21,220][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 09:55:21,220][233954] FPS: 424050.18[0m
[36m[2023-07-11 09:55:21,223][233954] itr=738, itrs=2000, Progress: 36.90%[0m
[36m[2023-07-11 09:55:32,864][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 09:55:32,864][233954] FPS: 332444.89[0m
[36m[2023-07-11 09:55:37,116][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:55:37,116][233954] Reward + Measures: [[20.18967433  0.360762    0.118559    0.11468833  0.29442635  0.44531062]][0m
[37m[1m[2023-07-11 09:55:37,116][233954] Max Reward on eval: 20.189674332202184[0m
[37m[1m[2023-07-11 09:55:37,117][233954] Min Reward on eval: 20.189674332202184[0m
[37m[1m[2023-07-11 09:55:37,117][233954] Mean Reward across all agents: 20.189674332202184[0m
[37m[1m[2023-07-11 09:55:37,117][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:55:42,136][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:55:42,137][233954] Reward + Measures: [[ 12.58511493   0.47739998   0.58430004   0.4454       0.36879998
    2.07379794]
 [332.34860097   0.1037       0.61610001   0.32570001   0.62840003
    2.56200314]
 [  6.08183112   0.4305       0.6559       0.44140002   0.44900003
    1.96664643]
 ...
 [ 56.6618317    0.38800001   0.78770006   0.48980004   0.57179993
    2.57464385]
 [379.86669731   0.0399       0.62130004   0.4492       0.64729995
    2.39572144]
 [-68.48800172   0.3046       0.33769998   0.0912       0.3777
    2.66529727]][0m
[37m[1m[2023-07-11 09:55:42,137][233954] Max Reward on eval: 777.7751045025885[0m
[37m[1m[2023-07-11 09:55:42,137][233954] Min Reward on eval: -96.77719807811081[0m
[37m[1m[2023-07-11 09:55:42,137][233954] Mean Reward across all agents: 107.27911704526095[0m
[37m[1m[2023-07-11 09:55:42,138][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:55:42,142][233954] mean_value=-428.88423498360595, max_value=534.7754206042496[0m
[37m[1m[2023-07-11 09:55:42,145][233954] New mean coefficients: [[ 0.09577613 -0.43500525  0.2730704   0.04350335 -0.16057698 -3.4447205 ]][0m
[37m[1m[2023-07-11 09:55:42,146][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:55:51,205][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 09:55:51,205][233954] FPS: 423955.47[0m
[36m[2023-07-11 09:55:51,208][233954] itr=739, itrs=2000, Progress: 36.95%[0m
[36m[2023-07-11 09:56:02,828][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 09:56:02,829][233954] FPS: 332944.80[0m
[36m[2023-07-11 09:56:07,101][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:56:07,102][233954] Reward + Measures: [[20.37601758  0.312444    0.12649867  0.12718999  0.28758967  0.46187297]][0m
[37m[1m[2023-07-11 09:56:07,102][233954] Max Reward on eval: 20.376017583051077[0m
[37m[1m[2023-07-11 09:56:07,102][233954] Min Reward on eval: 20.376017583051077[0m
[37m[1m[2023-07-11 09:56:07,102][233954] Mean Reward across all agents: 20.376017583051077[0m
[37m[1m[2023-07-11 09:56:07,103][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:56:12,349][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 09:56:12,350][233954] Reward + Measures: [[  86.86433649    0.1265        0.2617        0.10530001    0.18450001
     1.60595536]
 [  24.17466165    0.16219999    0.1168        0.1664        0.17089999
     1.57112694]
 [-199.12120914    0.32620001    0.64960003    0.2721        0.60250002
     2.50480008]
 ...
 [ -10.53195739    0.80940002    0.97199994    0.80459994    0.94370002
     2.58435225]
 [  64.0553856     0.27740002    0.20680001    0.22739999    0.29360002
     0.81025982]
 [  29.10854213    0.2289        0.0737        0.0485        0.1585
     0.96278858]][0m
[37m[1m[2023-07-11 09:56:12,350][233954] Max Reward on eval: 623.7335432756693[0m
[37m[1m[2023-07-11 09:56:12,350][233954] Min Reward on eval: -316.6138839812949[0m
[37m[1m[2023-07-11 09:56:12,351][233954] Mean Reward across all agents: 53.845695964278555[0m
[37m[1m[2023-07-11 09:56:12,351][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 09:56:12,354][233954] mean_value=-812.1236239452594, max_value=525.6237776922062[0m
[37m[1m[2023-07-11 09:56:12,357][233954] New mean coefficients: [[ 0.12260688 -0.1447595   0.2547602  -0.15831274  0.10157935 -2.7636957 ]][0m
[37m[1m[2023-07-11 09:56:12,358][233954] Moving the mean solution point...[0m
[36m[2023-07-11 09:56:21,335][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 09:56:21,336][233954] FPS: 427810.26[0m
[36m[2023-07-11 09:56:21,338][233954] itr=740, itrs=2000, Progress: 37.00%[0m
[37m[1m[2023-07-11 09:59:44,494][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000720[0m
[36m[2023-07-11 09:59:56,811][233954] train() took 11.72 seconds to complete[0m
[36m[2023-07-11 09:59:56,817][233954] FPS: 327665.13[0m
[36m[2023-07-11 10:00:00,976][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:00:00,977][233954] Reward + Measures: [[22.44049383  0.27438635  0.12993599  0.12532367  0.23660898  0.4414672 ]][0m
[37m[1m[2023-07-11 10:00:00,977][233954] Max Reward on eval: 22.440493826540983[0m
[37m[1m[2023-07-11 10:00:00,977][233954] Min Reward on eval: 22.440493826540983[0m
[37m[1m[2023-07-11 10:00:00,977][233954] Mean Reward across all agents: 22.440493826540983[0m
[37m[1m[2023-07-11 10:00:00,978][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:00:05,980][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:00:05,980][233954] Reward + Measures: [[101.81652856   0.2007       0.75150001   0.18010001   0.6081
    2.32504058]
 [127.43471002   0.0804       0.30749997   0.18010001   0.28960001
    2.56735778]
 [-25.24825615   0.2326       0.89860004   0.1234       0.8009001
    2.26954579]
 ...
 [ 39.18866643   0.22680001   0.2721       0.21630001   0.36070001
    2.17752695]
 [ 51.98031594   0.12879999   0.56890005   0.2297       0.54519999
    1.94838929]
 [-17.05879205   0.18740001   0.69169998   0.42080003   0.66979998
    2.66900945]][0m
[37m[1m[2023-07-11 10:00:05,980][233954] Max Reward on eval: 632.1773233328015[0m
[37m[1m[2023-07-11 10:00:05,981][233954] Min Reward on eval: -378.32253993991765[0m
[37m[1m[2023-07-11 10:00:05,981][233954] Mean Reward across all agents: 74.012744074436[0m
[37m[1m[2023-07-11 10:00:05,981][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:00:05,985][233954] mean_value=-428.88794053553687, max_value=483.9311016458701[0m
[37m[1m[2023-07-11 10:00:05,987][233954] New mean coefficients: [[-0.07410168 -0.06353235  0.0081244   0.11983782  0.1960302  -2.7072732 ]][0m
[37m[1m[2023-07-11 10:00:05,988][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:00:15,026][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 10:00:15,026][233954] FPS: 424982.65[0m
[36m[2023-07-11 10:00:15,028][233954] itr=741, itrs=2000, Progress: 37.05%[0m
[36m[2023-07-11 10:00:26,842][233954] train() took 11.73 seconds to complete[0m
[36m[2023-07-11 10:00:26,842][233954] FPS: 327493.07[0m
[36m[2023-07-11 10:00:31,101][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:00:31,101][233954] Reward + Measures: [[22.30500271  0.246691    0.15748768  0.15025167  0.20695266  0.43254778]][0m
[37m[1m[2023-07-11 10:00:31,101][233954] Max Reward on eval: 22.305002708170875[0m
[37m[1m[2023-07-11 10:00:31,102][233954] Min Reward on eval: 22.305002708170875[0m
[37m[1m[2023-07-11 10:00:31,102][233954] Mean Reward across all agents: 22.305002708170875[0m
[37m[1m[2023-07-11 10:00:31,102][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:00:36,144][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:00:36,144][233954] Reward + Measures: [[ 11.50766152   0.46000001   0.51350003   0.4289       0.57210004
    2.07872128]
 [ -5.64919366   0.2552       0.29949999   0.2445       0.3407
    2.1872077 ]
 [ 46.12064048   0.14410001   0.086        0.16150001   0.16950001
    1.24297917]
 ...
 [ 38.08841633   0.3141       0.9695999    0.14820002   0.97010005
    2.84680152]
 [337.0785308    0.08180001   0.99540007   0.70970005   0.99090004
    2.67134881]
 [ 91.25610545   0.66230005   0.72110003   0.66320002   0.62019998
    2.44386721]][0m
[37m[1m[2023-07-11 10:00:36,145][233954] Max Reward on eval: 751.1020715240389[0m
[37m[1m[2023-07-11 10:00:36,145][233954] Min Reward on eval: -175.00255329022184[0m
[37m[1m[2023-07-11 10:00:36,145][233954] Mean Reward across all agents: 135.616271561767[0m
[37m[1m[2023-07-11 10:00:36,145][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:00:36,150][233954] mean_value=-308.68068170702094, max_value=615.2209105782374[0m
[37m[1m[2023-07-11 10:00:36,153][233954] New mean coefficients: [[-0.30954024 -0.18136042 -0.19593401 -0.06109229  0.21426725 -2.8582208 ]][0m
[37m[1m[2023-07-11 10:00:36,154][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:00:45,140][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 10:00:45,141][233954] FPS: 427394.70[0m
[36m[2023-07-11 10:00:45,143][233954] itr=742, itrs=2000, Progress: 37.10%[0m
[36m[2023-07-11 10:00:56,812][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 10:00:56,812][233954] FPS: 331601.02[0m
[36m[2023-07-11 10:01:01,085][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:01:01,085][233954] Reward + Measures: [[21.71762699  0.23012567  0.17122933  0.166987    0.21802799  0.46180272]][0m
[37m[1m[2023-07-11 10:01:01,086][233954] Max Reward on eval: 21.71762698539077[0m
[37m[1m[2023-07-11 10:01:01,086][233954] Min Reward on eval: 21.71762698539077[0m
[37m[1m[2023-07-11 10:01:01,086][233954] Mean Reward across all agents: 21.71762698539077[0m
[37m[1m[2023-07-11 10:01:01,087][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:01:06,072][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:01:06,073][233954] Reward + Measures: [[217.46077883   0.0263       0.90160006   0.52210003   0.88940001
    2.49007678]
 [ 63.29095139   0.1051       0.96789998   0.3603       0.96060002
    2.79841232]
 [139.12064933   0.1251       0.40790001   0.17119999   0.34999999
    1.89056206]
 ...
 [ 19.07128879   0.25209999   0.05250001   0.11490001   0.2431
    0.60719007]
 [215.55957984   0.           0.99919999   0.57630002   0.99710006
    2.94357443]
 [ 18.6101521    0.44889998   0.56619996   0.421        0.50090003
    1.83459508]][0m
[37m[1m[2023-07-11 10:01:06,073][233954] Max Reward on eval: 640.3836364632473[0m
[37m[1m[2023-07-11 10:01:06,073][233954] Min Reward on eval: -81.62240916155278[0m
[37m[1m[2023-07-11 10:01:06,074][233954] Mean Reward across all agents: 126.83957680397623[0m
[37m[1m[2023-07-11 10:01:06,074][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:01:06,078][233954] mean_value=-532.732597486624, max_value=631.3524947966914[0m
[37m[1m[2023-07-11 10:01:06,080][233954] New mean coefficients: [[ 0.20578125  0.10763445 -0.1299774  -0.0115419   0.33629936 -2.369272  ]][0m
[37m[1m[2023-07-11 10:01:06,081][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:01:15,044][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 10:01:15,045][233954] FPS: 428513.96[0m
[36m[2023-07-11 10:01:15,047][233954] itr=743, itrs=2000, Progress: 37.15%[0m
[36m[2023-07-11 10:01:26,567][233954] train() took 11.43 seconds to complete[0m
[36m[2023-07-11 10:01:26,567][233954] FPS: 335824.82[0m
[36m[2023-07-11 10:01:30,881][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:01:30,881][233954] Reward + Measures: [[20.67171395  0.24609967  0.14196467  0.14100234  0.23155034  0.44624963]][0m
[37m[1m[2023-07-11 10:01:30,882][233954] Max Reward on eval: 20.67171394533367[0m
[37m[1m[2023-07-11 10:01:30,882][233954] Min Reward on eval: 20.67171394533367[0m
[37m[1m[2023-07-11 10:01:30,882][233954] Mean Reward across all agents: 20.67171394533367[0m
[37m[1m[2023-07-11 10:01:30,882][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:01:35,835][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:01:35,835][233954] Reward + Measures: [[112.7011663    0.1517       0.25         0.16680001   0.2484
    1.17113006]
 [164.67448959   0.14920001   0.60879999   0.3312       0.61390001
    2.28908896]
 [327.36896937   0.0448       0.55320001   0.43210003   0.56459999
    1.92276251]
 ...
 [ 28.53910369   0.0412       0.15320002   0.1391       0.0896
    1.14817142]
 [ 19.83871317   0.183        0.2739       0.1375       0.33880001
    1.45097184]
 [627.77280044   0.0012       0.99800009   0.83029997   0.99730009
    2.68463635]][0m
[37m[1m[2023-07-11 10:01:35,835][233954] Max Reward on eval: 742.764045703411[0m
[37m[1m[2023-07-11 10:01:35,836][233954] Min Reward on eval: -52.51013058433309[0m
[37m[1m[2023-07-11 10:01:35,836][233954] Mean Reward across all agents: 131.99915226657245[0m
[37m[1m[2023-07-11 10:01:35,836][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:01:35,840][233954] mean_value=-503.6422088375059, max_value=357.7772569661899[0m
[37m[1m[2023-07-11 10:01:35,842][233954] New mean coefficients: [[ 0.18986091  0.08080403  0.05601707 -0.21117353  0.31324065 -2.2445464 ]][0m
[37m[1m[2023-07-11 10:01:35,843][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:01:44,783][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 10:01:44,783][233954] FPS: 429649.12[0m
[36m[2023-07-11 10:01:44,785][233954] itr=744, itrs=2000, Progress: 37.20%[0m
[36m[2023-07-11 10:01:56,323][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 10:01:56,324][233954] FPS: 335305.20[0m
[36m[2023-07-11 10:02:00,535][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:02:00,536][233954] Reward + Measures: [[20.96443634  0.27481401  0.15382934  0.14392033  0.27572766  0.45692846]][0m
[37m[1m[2023-07-11 10:02:00,536][233954] Max Reward on eval: 20.964436343021895[0m
[37m[1m[2023-07-11 10:02:00,536][233954] Min Reward on eval: 20.964436343021895[0m
[37m[1m[2023-07-11 10:02:00,537][233954] Mean Reward across all agents: 20.964436343021895[0m
[37m[1m[2023-07-11 10:02:00,537][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:02:05,563][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:02:05,563][233954] Reward + Measures: [[763.03360748   0.0009       0.99729997   0.80690002   0.99730009
    3.05806541]
 [566.86545606   0.0472       0.77179998   0.62230003   0.77759999
    2.47803998]
 [267.80450699   0.273        0.72659999   0.55140001   0.72690004
    2.26321459]
 ...
 [152.49119853   0.12030001   0.93559998   0.20579998   0.93169993
    2.45801234]
 [475.07051541   0.2139       0.92220002   0.71329999   0.91229993
    2.5485158 ]
 [ 51.37647616   0.07310001   0.1037       0.08000001   0.11489999
    2.00504923]][0m
[37m[1m[2023-07-11 10:02:05,564][233954] Max Reward on eval: 788.5193710509687[0m
[37m[1m[2023-07-11 10:02:05,564][233954] Min Reward on eval: -123.40477011576294[0m
[37m[1m[2023-07-11 10:02:05,564][233954] Mean Reward across all agents: 100.24332547621769[0m
[37m[1m[2023-07-11 10:02:05,564][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:02:05,569][233954] mean_value=-420.5534785160015, max_value=527.1904055684349[0m
[37m[1m[2023-07-11 10:02:05,572][233954] New mean coefficients: [[-0.19141415 -0.3808472  -0.19596466 -0.10669494 -0.11304173 -3.0132031 ]][0m
[37m[1m[2023-07-11 10:02:05,573][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:02:14,530][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 10:02:14,530][233954] FPS: 428797.00[0m
[36m[2023-07-11 10:02:14,532][233954] itr=745, itrs=2000, Progress: 37.25%[0m
[36m[2023-07-11 10:02:26,295][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 10:02:26,295][233954] FPS: 328842.29[0m
[36m[2023-07-11 10:02:30,510][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:02:30,510][233954] Reward + Measures: [[17.52028239  0.26731431  0.16060866  0.14472567  0.26352167  0.46328744]][0m
[37m[1m[2023-07-11 10:02:30,510][233954] Max Reward on eval: 17.520282389004443[0m
[37m[1m[2023-07-11 10:02:30,511][233954] Min Reward on eval: 17.520282389004443[0m
[37m[1m[2023-07-11 10:02:30,511][233954] Mean Reward across all agents: 17.520282389004443[0m
[37m[1m[2023-07-11 10:02:30,511][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:02:35,496][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:02:35,497][233954] Reward + Measures: [[ 33.01485547   0.71390003   0.77590001   0.70090002   0.75
    1.90367877]
 [ 46.5439285    0.7802       0.83329993   0.78289998   0.82860005
    2.15303278]
 [ -6.43804601   0.6965       0.5686       0.6627       0.19569999
    1.94267642]
 ...
 [ 45.24711321   0.81650001   0.8998       0.80370009   0.8210001
    2.17117667]
 [ 53.64186873   0.2349       0.31940001   0.19140001   0.42540002
    1.21478498]
 [629.2754593    0.0008       0.99610007   0.74329996   0.99690002
    2.83190346]][0m
[37m[1m[2023-07-11 10:02:35,497][233954] Max Reward on eval: 650.6018524164334[0m
[37m[1m[2023-07-11 10:02:35,497][233954] Min Reward on eval: -220.1403578401543[0m
[37m[1m[2023-07-11 10:02:35,497][233954] Mean Reward across all agents: 116.35556881931971[0m
[37m[1m[2023-07-11 10:02:35,498][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:02:35,503][233954] mean_value=-337.5836510779715, max_value=694.7901302377879[0m
[37m[1m[2023-07-11 10:02:35,506][233954] New mean coefficients: [[-0.2415824  -0.19922303 -0.44783872  0.00440959  0.04278505 -2.8059561 ]][0m
[37m[1m[2023-07-11 10:02:35,507][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:02:44,456][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 10:02:44,456][233954] FPS: 429178.45[0m
[36m[2023-07-11 10:02:44,458][233954] itr=746, itrs=2000, Progress: 37.30%[0m
[36m[2023-07-11 10:02:56,087][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 10:02:56,087][233954] FPS: 332699.01[0m
[36m[2023-07-11 10:03:00,396][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:03:00,397][233954] Reward + Measures: [[20.89153328  0.26571199  0.15111299  0.14619     0.26536167  0.46156937]][0m
[37m[1m[2023-07-11 10:03:00,397][233954] Max Reward on eval: 20.89153327708932[0m
[37m[1m[2023-07-11 10:03:00,397][233954] Min Reward on eval: 20.89153327708932[0m
[37m[1m[2023-07-11 10:03:00,398][233954] Mean Reward across all agents: 20.89153327708932[0m
[37m[1m[2023-07-11 10:03:00,398][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:03:05,428][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:03:05,428][233954] Reward + Measures: [[  36.47852529    0.22860001    0.1321        0.10710001    0.3251
     0.83816737]
 [  67.06442629    0.26820001    0.27990001    0.16589999    0.3321
     1.78950047]
 [  16.97295682    0.29570001    0.79619998    0.24490002    0.71200001
     2.12100768]
 ...
 [  70.28008617    0.5194        0.68670005    0.48380002    0.63810003
     1.86489105]
 [-115.98457718    0.64120001    0.78979999    0.47129998    0.88140005
     2.25577545]
 [  77.18711839    0.2737        0.59350002    0.24229999    0.5927
     1.87198639]][0m
[37m[1m[2023-07-11 10:03:05,429][233954] Max Reward on eval: 599.1721038625576[0m
[37m[1m[2023-07-11 10:03:05,429][233954] Min Reward on eval: -255.66317174718716[0m
[37m[1m[2023-07-11 10:03:05,429][233954] Mean Reward across all agents: 46.96592632684911[0m
[37m[1m[2023-07-11 10:03:05,430][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:03:05,434][233954] mean_value=-492.68995488932484, max_value=682.4768988301419[0m
[37m[1m[2023-07-11 10:03:05,437][233954] New mean coefficients: [[-0.06162311 -0.07577305 -0.14901167  0.0914086   0.13686004 -2.516704  ]][0m
[37m[1m[2023-07-11 10:03:05,438][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:03:14,533][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 10:03:14,539][233954] FPS: 422286.10[0m
[36m[2023-07-11 10:03:14,541][233954] itr=747, itrs=2000, Progress: 37.35%[0m
[36m[2023-07-11 10:03:26,191][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 10:03:26,191][233954] FPS: 332074.54[0m
[36m[2023-07-11 10:03:30,481][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:03:30,482][233954] Reward + Measures: [[18.28627509  0.28506601  0.16132367  0.15694733  0.29662165  0.46622682]][0m
[37m[1m[2023-07-11 10:03:30,482][233954] Max Reward on eval: 18.28627509469857[0m
[37m[1m[2023-07-11 10:03:30,482][233954] Min Reward on eval: 18.28627509469857[0m
[37m[1m[2023-07-11 10:03:30,482][233954] Mean Reward across all agents: 18.28627509469857[0m
[37m[1m[2023-07-11 10:03:30,483][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:03:35,466][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:03:35,467][233954] Reward + Measures: [[231.74547814   0.0496       0.62180001   0.3827       0.51859999
    1.95066762]
 [177.38023845   0.0256       0.70660001   0.48310003   0.70279998
    2.32134223]
 [ 83.27507348   0.23570001   0.45650002   0.21159999   0.44320002
    2.03638935]
 ...
 [  4.75973683   0.35879999   0.2529       0.25489998   0.34300002
    1.05833113]
 [168.39009953   0.1446       0.52930003   0.15530001   0.3486
    1.98563516]
 [113.30678582   0.2879       0.37719998   0.10760001   0.32600001
    1.34546483]][0m
[37m[1m[2023-07-11 10:03:35,467][233954] Max Reward on eval: 655.2773456539959[0m
[37m[1m[2023-07-11 10:03:35,467][233954] Min Reward on eval: -58.728074969135925[0m
[37m[1m[2023-07-11 10:03:35,468][233954] Mean Reward across all agents: 108.32524495015166[0m
[37m[1m[2023-07-11 10:03:35,468][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:03:35,472][233954] mean_value=-622.8482132143978, max_value=694.3166231946786[0m
[37m[1m[2023-07-11 10:03:35,475][233954] New mean coefficients: [[-0.00950345 -0.00494254  0.07168502  0.0252181   0.27455068 -2.4598482 ]][0m
[37m[1m[2023-07-11 10:03:35,476][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:03:44,500][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 10:03:44,501][233954] FPS: 425595.12[0m
[36m[2023-07-11 10:03:44,503][233954] itr=748, itrs=2000, Progress: 37.40%[0m
[36m[2023-07-11 10:03:56,188][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 10:03:56,188][233954] FPS: 331211.19[0m
[36m[2023-07-11 10:04:00,563][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:04:00,563][233954] Reward + Measures: [[20.74415893  0.28023064  0.14275999  0.13845499  0.29694599  0.43257934]][0m
[37m[1m[2023-07-11 10:04:00,563][233954] Max Reward on eval: 20.744158932458785[0m
[37m[1m[2023-07-11 10:04:00,564][233954] Min Reward on eval: 20.744158932458785[0m
[37m[1m[2023-07-11 10:04:00,564][233954] Mean Reward across all agents: 20.744158932458785[0m
[37m[1m[2023-07-11 10:04:00,564][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:04:05,809][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:04:05,809][233954] Reward + Measures: [[356.5893931    0.18099999   0.71460003   0.32990003   0.71310002
    2.61000967]
 [ 51.68986339   0.1375       0.56600004   0.32089999   0.57650006
    1.89564168]
 [140.62964984   0.1295       0.42880002   0.2534       0.4197
    2.09875226]
 ...
 [-15.11678699   0.49210006   0.95230001   0.53560001   0.95840007
    2.4813993 ]
 [200.93339076   0.1189       0.52829999   0.40839997   0.56099999
    2.35015082]
 [ 20.76900095   0.76440001   0.89890003   0.78000003   0.90469998
    2.26718855]][0m
[37m[1m[2023-07-11 10:04:05,809][233954] Max Reward on eval: 416.401297671441[0m
[37m[1m[2023-07-11 10:04:05,810][233954] Min Reward on eval: -153.0445865187794[0m
[37m[1m[2023-07-11 10:04:05,810][233954] Mean Reward across all agents: 74.6167471460328[0m
[37m[1m[2023-07-11 10:04:05,810][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:04:05,815][233954] mean_value=-313.21574674500704, max_value=535.2544514746387[0m
[37m[1m[2023-07-11 10:04:05,818][233954] New mean coefficients: [[ 0.08549154  0.09199597 -0.10383415 -0.04052883  0.29740152 -2.5862865 ]][0m
[37m[1m[2023-07-11 10:04:05,819][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:04:14,843][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 10:04:14,844][233954] FPS: 425569.71[0m
[36m[2023-07-11 10:04:14,846][233954] itr=749, itrs=2000, Progress: 37.45%[0m
[36m[2023-07-11 10:04:26,454][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 10:04:26,454][233954] FPS: 333273.66[0m
[36m[2023-07-11 10:04:30,745][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:04:30,745][233954] Reward + Measures: [[22.28360846  0.33904734  0.15109065  0.14225568  0.34102467  0.4602567 ]][0m
[37m[1m[2023-07-11 10:04:30,745][233954] Max Reward on eval: 22.28360845610721[0m
[37m[1m[2023-07-11 10:04:30,745][233954] Min Reward on eval: 22.28360845610721[0m
[37m[1m[2023-07-11 10:04:30,746][233954] Mean Reward across all agents: 22.28360845610721[0m
[37m[1m[2023-07-11 10:04:30,746][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:04:35,705][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:04:35,705][233954] Reward + Measures: [[ 17.55214385   0.1399       0.28550002   0.1051       0.2755
    1.67648029]
 [122.72987784   0.1098       0.35280004   0.24960001   0.37120003
    1.89360797]
 [ 87.43660056   0.30889997   0.6261       0.35300002   0.56119996
    1.83715665]
 ...
 [244.3752313    0.1247       0.949        0.62029999   0.89219999
    2.32420921]
 [ 26.87967732   0.85460007   0.8743       0.83460009   0.86920005
    2.35938716]
 [-10.29862958   0.71090001   0.81109995   0.7001       0.6401
    2.0113194 ]][0m
[37m[1m[2023-07-11 10:04:35,705][233954] Max Reward on eval: 499.3497200044338[0m
[37m[1m[2023-07-11 10:04:35,706][233954] Min Reward on eval: -97.2082472486887[0m
[37m[1m[2023-07-11 10:04:35,706][233954] Mean Reward across all agents: 37.641177567837005[0m
[37m[1m[2023-07-11 10:04:35,706][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:04:35,711][233954] mean_value=-550.2727435810649, max_value=540.8385794609115[0m
[37m[1m[2023-07-11 10:04:35,713][233954] New mean coefficients: [[ 0.17815323  0.42784613 -0.60779285 -0.13378575  0.4004646  -2.4181924 ]][0m
[37m[1m[2023-07-11 10:04:35,714][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:04:44,759][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 10:04:44,765][233954] FPS: 424605.39[0m
[36m[2023-07-11 10:04:44,771][233954] itr=750, itrs=2000, Progress: 37.50%[0m
[37m[1m[2023-07-11 10:08:01,263][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000730[0m
[36m[2023-07-11 10:08:13,644][233954] train() took 11.79 seconds to complete[0m
[36m[2023-07-11 10:08:13,644][233954] FPS: 325660.83[0m
[36m[2023-07-11 10:08:17,910][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:08:17,910][233954] Reward + Measures: [[22.28150446  0.35085034  0.13766766  0.13706401  0.3610343   0.46936774]][0m
[37m[1m[2023-07-11 10:08:17,910][233954] Max Reward on eval: 22.281504457356913[0m
[37m[1m[2023-07-11 10:08:17,911][233954] Min Reward on eval: 22.281504457356913[0m
[37m[1m[2023-07-11 10:08:17,911][233954] Mean Reward across all agents: 22.281504457356913[0m
[37m[1m[2023-07-11 10:08:17,911][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:08:22,910][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:08:22,911][233954] Reward + Measures: [[-46.75486325   0.91429996   0.9443       0.89890003   0.93460006
    2.83615422]
 [133.41950859   0.1815       0.36820003   0.30130002   0.43619999
    1.676875  ]
 [-43.15737697   0.36670002   0.52630001   0.28120002   0.48410001
    1.8423245 ]
 ...
 [-51.12434947   0.81589997   0.81720012   0.80989999   0.82840008
    3.15290117]
 [ 74.3341325    0.35779998   0.50120002   0.33950001   0.46950004
    1.88721681]
 [  4.30514457   0.36680004   0.21970001   0.15350001   0.29670003
    1.4220531 ]][0m
[37m[1m[2023-07-11 10:08:22,911][233954] Max Reward on eval: 629.1758880716749[0m
[37m[1m[2023-07-11 10:08:22,911][233954] Min Reward on eval: -221.5417924242094[0m
[37m[1m[2023-07-11 10:08:22,911][233954] Mean Reward across all agents: 66.33483233216374[0m
[37m[1m[2023-07-11 10:08:22,912][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:08:22,915][233954] mean_value=-352.18060871711367, max_value=663.7846414433551[0m
[37m[1m[2023-07-11 10:08:22,918][233954] New mean coefficients: [[ 0.40731663  0.4255415  -0.58471364 -0.08486548  0.25271046 -2.3982804 ]][0m
[37m[1m[2023-07-11 10:08:22,919][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:08:31,951][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 10:08:31,952][233954] FPS: 425196.98[0m
[36m[2023-07-11 10:08:31,954][233954] itr=751, itrs=2000, Progress: 37.55%[0m
[36m[2023-07-11 10:08:43,745][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 10:08:43,745][233954] FPS: 328089.10[0m
[36m[2023-07-11 10:08:48,064][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:08:48,064][233954] Reward + Measures: [[22.41814689  0.40172368  0.127533    0.13594267  0.36283934  0.43386552]][0m
[37m[1m[2023-07-11 10:08:48,064][233954] Max Reward on eval: 22.418146885512556[0m
[37m[1m[2023-07-11 10:08:48,064][233954] Min Reward on eval: 22.418146885512556[0m
[37m[1m[2023-07-11 10:08:48,065][233954] Mean Reward across all agents: 22.418146885512556[0m
[37m[1m[2023-07-11 10:08:48,065][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:08:53,032][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:08:53,033][233954] Reward + Measures: [[-167.96831256    0.27050003    0.34720001    0.07840001    0.41020003
     1.77897286]
 [ 359.87169911    0.0654        0.62080002    0.43610001    0.64959997
     1.94602931]
 [  35.74847962    0.0861        0.1462        0.083         0.14920001
     1.37388361]
 ...
 [  64.26186216    0.18360001    0.16129999    0.0426        0.20379999
     0.84664077]
 [  85.53511288    0.32699999    0.45739999    0.0692        0.48400003
     1.93905854]
 [ -11.65144888    0.2474        0.49320003    0.13869999    0.3698
     1.4697808 ]][0m
[37m[1m[2023-07-11 10:08:53,033][233954] Max Reward on eval: 589.0708655972965[0m
[37m[1m[2023-07-11 10:08:53,033][233954] Min Reward on eval: -221.85028812047094[0m
[37m[1m[2023-07-11 10:08:53,034][233954] Mean Reward across all agents: 126.94842357769669[0m
[37m[1m[2023-07-11 10:08:53,034][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:08:53,039][233954] mean_value=-346.7814117156559, max_value=716.9258524016577[0m
[37m[1m[2023-07-11 10:08:53,042][233954] New mean coefficients: [[ 0.46319255  0.29538742 -0.42772645 -0.25255114  0.03966954 -2.4743881 ]][0m
[37m[1m[2023-07-11 10:08:53,043][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:09:02,040][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 10:09:02,041][233954] FPS: 426852.50[0m
[36m[2023-07-11 10:09:02,043][233954] itr=752, itrs=2000, Progress: 37.60%[0m
[36m[2023-07-11 10:09:13,684][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 10:09:13,684][233954] FPS: 332364.77[0m
[36m[2023-07-11 10:09:17,906][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:09:17,907][233954] Reward + Measures: [[23.4395456   0.40364     0.13036799  0.12851733  0.33032197  0.45664519]][0m
[37m[1m[2023-07-11 10:09:17,907][233954] Max Reward on eval: 23.439545595719267[0m
[37m[1m[2023-07-11 10:09:17,907][233954] Min Reward on eval: 23.439545595719267[0m
[37m[1m[2023-07-11 10:09:17,908][233954] Mean Reward across all agents: 23.439545595719267[0m
[37m[1m[2023-07-11 10:09:17,908][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:09:22,816][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:09:22,817][233954] Reward + Measures: [[ 20.86651392   0.1655       0.11010001   0.13419999   0.1666
    1.2972306 ]
 [271.09282829   0.1248       0.57069999   0.34580001   0.58920002
    1.90273976]
 [ -9.49040958   0.1728       0.28770003   0.14120001   0.2879
    1.86217105]
 ...
 [ 26.71871474   0.0894       0.13920002   0.09850001   0.17090002
    1.18968189]
 [ 47.50527335   0.82159996   0.89589995   0.80190003   0.85549992
    2.10527968]
 [206.6997343    0.10700001   0.46050006   0.23950003   0.4797
    2.58650947]][0m
[37m[1m[2023-07-11 10:09:22,817][233954] Max Reward on eval: 672.5893936175853[0m
[37m[1m[2023-07-11 10:09:22,817][233954] Min Reward on eval: -181.92422020807862[0m
[37m[1m[2023-07-11 10:09:22,818][233954] Mean Reward across all agents: 87.07621727613528[0m
[37m[1m[2023-07-11 10:09:22,818][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:09:22,823][233954] mean_value=-488.1111294836471, max_value=584.4711581062526[0m
[37m[1m[2023-07-11 10:09:22,825][233954] New mean coefficients: [[ 0.5048809   0.18084209 -0.02587262 -0.17035957 -0.1405285  -2.431898  ]][0m
[37m[1m[2023-07-11 10:09:22,826][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:09:31,829][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 10:09:31,829][233954] FPS: 426613.87[0m
[36m[2023-07-11 10:09:31,831][233954] itr=753, itrs=2000, Progress: 37.65%[0m
[36m[2023-07-11 10:09:43,399][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 10:09:43,399][233954] FPS: 334494.77[0m
[36m[2023-07-11 10:09:47,631][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:09:47,632][233954] Reward + Measures: [[22.28191931  0.38516366  0.13156033  0.13212167  0.27801567  0.44213209]][0m
[37m[1m[2023-07-11 10:09:47,632][233954] Max Reward on eval: 22.281919309092956[0m
[37m[1m[2023-07-11 10:09:47,632][233954] Min Reward on eval: 22.281919309092956[0m
[37m[1m[2023-07-11 10:09:47,633][233954] Mean Reward across all agents: 22.281919309092956[0m
[37m[1m[2023-07-11 10:09:47,633][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:09:52,644][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:09:52,658][233954] Reward + Measures: [[232.79226502   0.0531       0.36550003   0.26929998   0.38479999
    2.07714844]
 [148.23502539   0.31380001   0.32619998   0.18430001   0.36790001
    1.22887647]
 [183.65981673   0.0157       0.98859996   0.59679997   0.96679991
    2.9504106 ]
 ...
 [504.84963224   0.0077       0.99120009   0.74690002   0.98359996
    2.90778565]
 [ -4.64298606   0.34910002   0.26449999   0.19050001   0.34810001
    1.54807281]
 [ 15.42239732   0.3707       0.6164       0.3053       0.53960001
    2.11249232]][0m
[37m[1m[2023-07-11 10:09:52,658][233954] Max Reward on eval: 734.6929321383592[0m
[37m[1m[2023-07-11 10:09:52,658][233954] Min Reward on eval: -55.454102049209176[0m
[37m[1m[2023-07-11 10:09:52,658][233954] Mean Reward across all agents: 107.42752576771657[0m
[37m[1m[2023-07-11 10:09:52,659][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:09:52,662][233954] mean_value=-708.8810641741355, max_value=537.5326714628935[0m
[37m[1m[2023-07-11 10:09:52,665][233954] New mean coefficients: [[ 0.27230301  0.0173343   0.02491157  0.02816287 -0.10414621 -2.6399772 ]][0m
[37m[1m[2023-07-11 10:09:52,666][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:10:01,775][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 10:10:01,776][233954] FPS: 421600.74[0m
[36m[2023-07-11 10:10:01,778][233954] itr=754, itrs=2000, Progress: 37.70%[0m
[36m[2023-07-11 10:10:13,478][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 10:10:13,478][233954] FPS: 330649.71[0m
[36m[2023-07-11 10:10:17,766][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:10:17,767][233954] Reward + Measures: [[23.52669556  0.33903468  0.144325    0.14918067  0.24332434  0.42028263]][0m
[37m[1m[2023-07-11 10:10:17,767][233954] Max Reward on eval: 23.52669555508231[0m
[37m[1m[2023-07-11 10:10:17,767][233954] Min Reward on eval: 23.52669555508231[0m
[37m[1m[2023-07-11 10:10:17,767][233954] Mean Reward across all agents: 23.52669555508231[0m
[37m[1m[2023-07-11 10:10:17,767][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:10:22,724][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:10:22,725][233954] Reward + Measures: [[167.87190927   0.0563       0.8422001    0.45320001   0.8441
    2.23927951]
 [  7.88757008   0.21960001   0.19149999   0.14850001   0.29520002
    1.70254636]
 [ 99.52413583   0.1208       0.95380002   0.1867       0.89379996
    1.98939502]
 ...
 [278.54758644   0.0203       0.9835       0.69149995   0.98099995
    2.19820333]
 [ 49.82950071   0.33310002   0.4269       0.23650001   0.4224
    1.57774544]
 [ 39.79631728   0.252        0.08230001   0.0368       0.1788
    0.92949343]][0m
[37m[1m[2023-07-11 10:10:22,725][233954] Max Reward on eval: 807.8030395429581[0m
[37m[1m[2023-07-11 10:10:22,725][233954] Min Reward on eval: -134.26064754389228[0m
[37m[1m[2023-07-11 10:10:22,726][233954] Mean Reward across all agents: 88.19828033049392[0m
[37m[1m[2023-07-11 10:10:22,726][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:10:22,731][233954] mean_value=-431.22582000741784, max_value=477.3055100445543[0m
[37m[1m[2023-07-11 10:10:22,734][233954] New mean coefficients: [[ 0.4705168   0.32682192 -0.07207464 -0.21012658  0.28457263 -2.5369565 ]][0m
[37m[1m[2023-07-11 10:10:22,735][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:10:31,737][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 10:10:31,738][233954] FPS: 426626.53[0m
[36m[2023-07-11 10:10:31,740][233954] itr=755, itrs=2000, Progress: 37.75%[0m
[36m[2023-07-11 10:10:43,268][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 10:10:43,268][233954] FPS: 335613.80[0m
[36m[2023-07-11 10:10:47,490][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:10:47,491][233954] Reward + Measures: [[23.50460195  0.36899465  0.138846    0.13602199  0.25309968  0.41828838]][0m
[37m[1m[2023-07-11 10:10:47,491][233954] Max Reward on eval: 23.504601947222408[0m
[37m[1m[2023-07-11 10:10:47,491][233954] Min Reward on eval: 23.504601947222408[0m
[37m[1m[2023-07-11 10:10:47,492][233954] Mean Reward across all agents: 23.504601947222408[0m
[37m[1m[2023-07-11 10:10:47,492][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:10:52,748][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:10:52,748][233954] Reward + Measures: [[ 13.25333046   0.71329993   0.72220004   0.68330002   0.73430008
    2.89083505]
 [  1.02145002   0.5654       0.68370003   0.53929996   0.6494
    2.12321663]
 [130.11576373   0.15230002   0.43490002   0.50999999   0.61519998
    1.55946851]
 ...
 [-29.87621031   0.45380002   0.97690004   0.0508       0.98089999
    3.10342073]
 [ 86.75768377   0.26250002   0.60510004   0.13339999   0.62370008
    1.96126485]
 [ 15.34274736   0.0558       0.0975       0.0919       0.1464
    0.60315555]][0m
[37m[1m[2023-07-11 10:10:52,749][233954] Max Reward on eval: 563.6145859133452[0m
[37m[1m[2023-07-11 10:10:52,749][233954] Min Reward on eval: -91.77024326929822[0m
[37m[1m[2023-07-11 10:10:52,749][233954] Mean Reward across all agents: 49.2014608696381[0m
[37m[1m[2023-07-11 10:10:52,749][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:10:52,753][233954] mean_value=-533.9533702442744, max_value=530.1755616156385[0m
[37m[1m[2023-07-11 10:10:52,756][233954] New mean coefficients: [[ 0.4217844   0.3657472  -0.35309055 -0.22923556  0.3608654  -2.2955601 ]][0m
[37m[1m[2023-07-11 10:10:52,757][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:11:01,804][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 10:11:01,804][233954] FPS: 424522.84[0m
[36m[2023-07-11 10:11:01,807][233954] itr=756, itrs=2000, Progress: 37.80%[0m
[36m[2023-07-11 10:11:13,525][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 10:11:13,525][233954] FPS: 330283.26[0m
[36m[2023-07-11 10:11:17,768][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:11:17,769][233954] Reward + Measures: [[24.21836971  0.35722932  0.133564    0.12949601  0.25108567  0.41632584]][0m
[37m[1m[2023-07-11 10:11:17,769][233954] Max Reward on eval: 24.21836971301048[0m
[37m[1m[2023-07-11 10:11:17,769][233954] Min Reward on eval: 24.21836971301048[0m
[37m[1m[2023-07-11 10:11:17,769][233954] Mean Reward across all agents: 24.21836971301048[0m
[37m[1m[2023-07-11 10:11:17,770][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:11:22,757][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:11:22,758][233954] Reward + Measures: [[ 66.44700503   0.1072       0.16860001   0.1992       0.20819998
    1.267887  ]
 [ 65.83114661   0.22850001   0.40990001   0.1123       0.40290004
    2.08657932]
 [ 56.91174795   0.1415       0.21760002   0.15990001   0.2552
    1.57450283]
 ...
 [192.62012763   0.0645       0.3493       0.25189999   0.39200002
    1.66303623]
 [ 56.35434001   0.26180002   0.2332       0.24050002   0.33250001
    1.66617548]
 [ 45.81773619   0.15699999   0.1243       0.13310002   0.24050002
    1.51855803]][0m
[37m[1m[2023-07-11 10:11:22,758][233954] Max Reward on eval: 729.6454086185898[0m
[37m[1m[2023-07-11 10:11:22,758][233954] Min Reward on eval: -205.6234321472235[0m
[37m[1m[2023-07-11 10:11:22,759][233954] Mean Reward across all agents: 74.6725016330027[0m
[37m[1m[2023-07-11 10:11:22,759][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:11:22,763][233954] mean_value=-655.5970402509328, max_value=473.73775920303535[0m
[37m[1m[2023-07-11 10:11:22,765][233954] New mean coefficients: [[ 0.09695071  0.17260197 -0.17388372 -0.09325212  0.15738396 -2.4628298 ]][0m
[37m[1m[2023-07-11 10:11:22,766][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:11:31,785][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 10:11:31,785][233954] FPS: 425875.22[0m
[36m[2023-07-11 10:11:31,787][233954] itr=757, itrs=2000, Progress: 37.85%[0m
[36m[2023-07-11 10:11:43,540][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 10:11:43,540][233954] FPS: 329172.50[0m
[36m[2023-07-11 10:11:47,832][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:11:47,832][233954] Reward + Measures: [[27.38712745  0.37681836  0.12884232  0.13037266  0.27249366  0.40648544]][0m
[37m[1m[2023-07-11 10:11:47,833][233954] Max Reward on eval: 27.38712744986981[0m
[37m[1m[2023-07-11 10:11:47,833][233954] Min Reward on eval: 27.38712744986981[0m
[37m[1m[2023-07-11 10:11:47,833][233954] Mean Reward across all agents: 27.38712744986981[0m
[37m[1m[2023-07-11 10:11:47,833][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:11:52,804][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:11:52,805][233954] Reward + Measures: [[  25.00709763    0.18190001    0.133         0.11260001    0.12280001
     0.63524848]
 [-179.24322604    0.479         0.9853        0.0115        0.99250001
     2.28064513]
 [  24.38796481    0.17990001    0.29390001    0.1139        0.31819999
     2.12858438]
 ...
 [  11.29293337    0.2719        0.0729        0.0289        0.21259999
     0.51483375]
 [  71.62489752    0.32210001    0.1679        0.0616        0.2502
     0.9428032 ]
 [  91.27638019    0.05950001    0.198         0.1552        0.18380001
     1.66955853]][0m
[37m[1m[2023-07-11 10:11:52,805][233954] Max Reward on eval: 716.6321410959587[0m
[37m[1m[2023-07-11 10:11:52,805][233954] Min Reward on eval: -305.43973996760326[0m
[37m[1m[2023-07-11 10:11:52,806][233954] Mean Reward across all agents: 98.21777511295066[0m
[37m[1m[2023-07-11 10:11:52,806][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:11:52,810][233954] mean_value=-523.6608103577865, max_value=482.96667336681855[0m
[37m[1m[2023-07-11 10:11:52,813][233954] New mean coefficients: [[ 0.08812505  0.14382869 -0.01611526 -0.02811026  0.00777929 -2.79798   ]][0m
[37m[1m[2023-07-11 10:11:52,814][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:12:01,917][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 10:12:01,917][233954] FPS: 421909.90[0m
[36m[2023-07-11 10:12:01,919][233954] itr=758, itrs=2000, Progress: 37.90%[0m
[36m[2023-07-11 10:12:13,615][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 10:12:13,616][233954] FPS: 330927.34[0m
[36m[2023-07-11 10:12:17,926][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:12:17,927][233954] Reward + Measures: [[27.94152659  0.34658733  0.132227    0.13342501  0.26947466  0.39802539]][0m
[37m[1m[2023-07-11 10:12:17,927][233954] Max Reward on eval: 27.94152658932327[0m
[37m[1m[2023-07-11 10:12:17,927][233954] Min Reward on eval: 27.94152658932327[0m
[37m[1m[2023-07-11 10:12:17,927][233954] Mean Reward across all agents: 27.94152658932327[0m
[37m[1m[2023-07-11 10:12:17,928][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:12:22,917][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:12:22,918][233954] Reward + Measures: [[ 10.43938388   0.39299998   0.32880002   0.2184       0.35330001
    1.28319633]
 [ 14.2190277    0.16580001   0.458        0.1523       0.33880001
    1.8629936 ]
 [ -3.48765501   0.4862       0.72469997   0.42109999   0.6577
    2.2425518 ]
 ...
 [ 15.15762127   0.30490002   0.38560003   0.28690001   0.4765
    1.36865985]
 [-21.94384377   0.8308       0.86590004   0.77649999   0.88910007
    1.98873544]
 [190.98728652   0.19219999   0.3017       0.28980002   0.38339999
    1.71477509]][0m
[37m[1m[2023-07-11 10:12:22,918][233954] Max Reward on eval: 684.6710472421721[0m
[37m[1m[2023-07-11 10:12:22,918][233954] Min Reward on eval: -75.92010543681681[0m
[37m[1m[2023-07-11 10:12:22,918][233954] Mean Reward across all agents: 93.43996157166858[0m
[37m[1m[2023-07-11 10:12:22,918][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:12:22,923][233954] mean_value=-554.7959987957836, max_value=450.5976986905039[0m
[37m[1m[2023-07-11 10:12:22,926][233954] New mean coefficients: [[ 0.00006221  0.03539374 -0.17429097 -0.05453518 -0.17234851 -2.737176  ]][0m
[37m[1m[2023-07-11 10:12:22,927][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:12:31,899][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 10:12:31,900][233954] FPS: 428065.31[0m
[36m[2023-07-11 10:12:31,902][233954] itr=759, itrs=2000, Progress: 37.95%[0m
[36m[2023-07-11 10:12:43,711][233954] train() took 11.72 seconds to complete[0m
[36m[2023-07-11 10:12:43,711][233954] FPS: 327611.24[0m
[36m[2023-07-11 10:12:48,004][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:12:48,005][233954] Reward + Measures: [[28.01666008  0.34855103  0.13112001  0.12850666  0.26781267  0.38919589]][0m
[37m[1m[2023-07-11 10:12:48,005][233954] Max Reward on eval: 28.016660077418354[0m
[37m[1m[2023-07-11 10:12:48,005][233954] Min Reward on eval: 28.016660077418354[0m
[37m[1m[2023-07-11 10:12:48,005][233954] Mean Reward across all agents: 28.016660077418354[0m
[37m[1m[2023-07-11 10:12:48,006][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:12:52,984][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:12:52,984][233954] Reward + Measures: [[-72.36251005   0.21780001   0.43849999   0.15799999   0.31330001
    1.86390328]
 [ 21.34294109   0.0485       0.1046       0.0649       0.1497
    1.4874661 ]
 [ 44.95546186   0.1742       0.14430001   0.1122       0.29150003
    1.39226341]
 ...
 [ 14.5314442    0.32730001   0.29210001   0.1797       0.33990002
    1.29467475]
 [ 31.77892847   0.125        0.13169999   0.11130001   0.17309999
    1.37370968]
 [ 45.28859034   0.20630001   0.1795       0.1707       0.22490001
    1.80076504]][0m
[37m[1m[2023-07-11 10:12:52,985][233954] Max Reward on eval: 641.1040992586874[0m
[37m[1m[2023-07-11 10:12:52,985][233954] Min Reward on eval: -171.09911966454237[0m
[37m[1m[2023-07-11 10:12:52,985][233954] Mean Reward across all agents: 42.50840963841052[0m
[37m[1m[2023-07-11 10:12:52,985][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:12:52,989][233954] mean_value=-809.0013821423196, max_value=659.9277558215465[0m
[37m[1m[2023-07-11 10:12:52,992][233954] New mean coefficients: [[-0.12875521  0.00918466 -0.28081065 -0.055945   -0.07584759 -2.5556657 ]][0m
[37m[1m[2023-07-11 10:12:52,993][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:13:02,083][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 10:13:02,084][233954] FPS: 422479.60[0m
[36m[2023-07-11 10:13:02,086][233954] itr=760, itrs=2000, Progress: 38.00%[0m
[37m[1m[2023-07-11 10:16:22,994][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000740[0m
[36m[2023-07-11 10:16:35,344][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 10:16:35,344][233954] FPS: 330199.65[0m
[36m[2023-07-11 10:16:39,571][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:16:39,572][233954] Reward + Measures: [[26.35904352  0.32429233  0.12556     0.12315832  0.24141935  0.38353047]][0m
[37m[1m[2023-07-11 10:16:39,572][233954] Max Reward on eval: 26.35904351566696[0m
[37m[1m[2023-07-11 10:16:39,572][233954] Min Reward on eval: 26.35904351566696[0m
[37m[1m[2023-07-11 10:16:39,572][233954] Mean Reward across all agents: 26.35904351566696[0m
[37m[1m[2023-07-11 10:16:39,572][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:16:44,514][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:16:44,515][233954] Reward + Measures: [[ 59.47348762   0.47740003   0.93860006   0.60100001   0.78390002
    2.07703853]
 [-24.5866816    0.65939999   0.74399996   0.68699998   0.72959995
    1.92455757]
 [ 70.0359179    0.1141       0.1237       0.22160001   0.18550001
    0.8579402 ]
 ...
 [ 23.47021673   0.16680001   0.0753       0.0641       0.13510001
    0.74116373]
 [ 76.57775832   0.34590003   0.84810001   0.48639998   0.78200001
    1.98662841]
 [ -1.8291373    0.67440003   0.6886       0.74489993   0.66510004
    1.99028301]][0m
[37m[1m[2023-07-11 10:16:44,515][233954] Max Reward on eval: 608.4858150590211[0m
[37m[1m[2023-07-11 10:16:44,515][233954] Min Reward on eval: -122.26582235777751[0m
[37m[1m[2023-07-11 10:16:44,516][233954] Mean Reward across all agents: 74.23057692652723[0m
[37m[1m[2023-07-11 10:16:44,516][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:16:44,521][233954] mean_value=-599.3604748646387, max_value=600.4891169038135[0m
[37m[1m[2023-07-11 10:16:44,524][233954] New mean coefficients: [[-0.10998754  0.23891735 -0.35996637  0.16238026  0.07962587 -2.6075814 ]][0m
[37m[1m[2023-07-11 10:16:44,525][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:16:53,511][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 10:16:53,512][233954] FPS: 427368.43[0m
[36m[2023-07-11 10:16:53,514][233954] itr=761, itrs=2000, Progress: 38.05%[0m
[36m[2023-07-11 10:17:05,363][233954] train() took 11.76 seconds to complete[0m
[36m[2023-07-11 10:17:05,364][233954] FPS: 326585.24[0m
[36m[2023-07-11 10:17:09,605][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:17:09,605][233954] Reward + Measures: [[25.90465935  0.33635798  0.11334533  0.11660533  0.23886232  0.38036075]][0m
[37m[1m[2023-07-11 10:17:09,606][233954] Max Reward on eval: 25.904659347516105[0m
[37m[1m[2023-07-11 10:17:09,606][233954] Min Reward on eval: 25.904659347516105[0m
[37m[1m[2023-07-11 10:17:09,606][233954] Mean Reward across all agents: 25.904659347516105[0m
[37m[1m[2023-07-11 10:17:09,606][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:17:14,857][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:17:14,858][233954] Reward + Measures: [[ 81.51271932   0.2323       0.26030001   0.1987       0.31230003
    1.49022853]
 [251.4137559    0.0723       0.8434       0.47660002   0.73809999
    2.04118347]
 [ 36.2910352    0.59780008   0.53009999   0.47530004   0.56450003
    1.49756587]
 ...
 [-14.48798184   0.20729999   0.4429       0.1815       0.41079998
    1.42936969]
 [ 54.54791342   0.30820003   0.15110001   0.23410001   0.31110001
    1.28185463]
 [ 38.94300713   0.14930001   0.09859999   0.1573       0.20369999
    0.92177087]][0m
[37m[1m[2023-07-11 10:17:14,858][233954] Max Reward on eval: 528.4711170300841[0m
[37m[1m[2023-07-11 10:17:14,858][233954] Min Reward on eval: -192.05928039774298[0m
[37m[1m[2023-07-11 10:17:14,858][233954] Mean Reward across all agents: 50.54201550018292[0m
[37m[1m[2023-07-11 10:17:14,859][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:17:14,862][233954] mean_value=-655.4111121922398, max_value=355.1462239637359[0m
[37m[1m[2023-07-11 10:17:14,865][233954] New mean coefficients: [[ 0.23436673  0.29561314 -0.15583086 -0.02009097 -0.11667145 -2.2907631 ]][0m
[37m[1m[2023-07-11 10:17:14,866][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:17:23,872][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 10:17:23,872][233954] FPS: 426475.54[0m
[36m[2023-07-11 10:17:23,874][233954] itr=762, itrs=2000, Progress: 38.10%[0m
[36m[2023-07-11 10:17:35,426][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 10:17:35,427][233954] FPS: 334990.01[0m
[36m[2023-07-11 10:17:39,683][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:17:39,683][233954] Reward + Measures: [[25.24557924  0.38184935  0.11448266  0.11515932  0.27681234  0.38697875]][0m
[37m[1m[2023-07-11 10:17:39,683][233954] Max Reward on eval: 25.245579239375115[0m
[37m[1m[2023-07-11 10:17:39,684][233954] Min Reward on eval: 25.245579239375115[0m
[37m[1m[2023-07-11 10:17:39,684][233954] Mean Reward across all agents: 25.245579239375115[0m
[37m[1m[2023-07-11 10:17:39,684][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:17:44,591][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:17:44,592][233954] Reward + Measures: [[ 64.15203143   0.66039997   0.72460002   0.6092       0.68119997
    2.09275794]
 [267.86472701   0.29910001   0.96089995   0.61860007   0.95120001
    2.99815106]
 [ 12.27267754   0.30720001   0.25569999   0.1177       0.27220002
    1.09493196]
 ...
 [236.98875676   0.06280001   0.43630004   0.30519998   0.46450001
    2.21954393]
 [ 49.25165993   0.16580001   0.19389999   0.0747       0.17580001
    0.98780233]
 [ 93.77847945   0.25830004   0.36310002   0.1999       0.4296
    1.36111414]][0m
[37m[1m[2023-07-11 10:17:44,592][233954] Max Reward on eval: 517.2824955141172[0m
[37m[1m[2023-07-11 10:17:44,592][233954] Min Reward on eval: -265.34492685720323[0m
[37m[1m[2023-07-11 10:17:44,593][233954] Mean Reward across all agents: 50.169358303707455[0m
[37m[1m[2023-07-11 10:17:44,593][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:17:44,597][233954] mean_value=-429.6569499294103, max_value=535.6258626899914[0m
[37m[1m[2023-07-11 10:17:44,600][233954] New mean coefficients: [[ 0.24537957  0.38846228  0.08393192 -0.10059074  0.11535934 -2.1950758 ]][0m
[37m[1m[2023-07-11 10:17:44,601][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:17:53,642][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 10:17:53,642][233954] FPS: 424804.50[0m
[36m[2023-07-11 10:17:53,645][233954] itr=763, itrs=2000, Progress: 38.15%[0m
[36m[2023-07-11 10:18:05,856][233954] train() took 12.12 seconds to complete[0m
[36m[2023-07-11 10:18:05,856][233954] FPS: 316877.53[0m
[36m[2023-07-11 10:18:10,113][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:18:10,114][233954] Reward + Measures: [[24.40491806  0.3806293   0.115261    0.12523466  0.26193333  0.37942407]][0m
[37m[1m[2023-07-11 10:18:10,114][233954] Max Reward on eval: 24.404918063198817[0m
[37m[1m[2023-07-11 10:18:10,114][233954] Min Reward on eval: 24.404918063198817[0m
[37m[1m[2023-07-11 10:18:10,114][233954] Mean Reward across all agents: 24.404918063198817[0m
[37m[1m[2023-07-11 10:18:10,115][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:18:15,106][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:18:15,106][233954] Reward + Measures: [[19.99855501  0.2228      0.065       0.0378      0.17619999  0.65394086]
 [14.75965073  0.28959998  0.16870001  0.0561      0.2045      0.86310118]
 [44.43469393  0.0787      0.067       0.07919999  0.14259999  1.33592129]
 ...
 [13.60124752  0.13160001  0.26110002  0.09670001  0.2333      1.63256919]
 [59.58191204  0.0589      0.17580001  0.152       0.2235      1.78016877]
 [21.65996109  0.28220001  0.1997      0.2313      0.33749995  1.42734981]][0m
[37m[1m[2023-07-11 10:18:15,107][233954] Max Reward on eval: 325.5659110451117[0m
[37m[1m[2023-07-11 10:18:15,107][233954] Min Reward on eval: -63.109251158998816[0m
[37m[1m[2023-07-11 10:18:15,107][233954] Mean Reward across all agents: 30.506692041605323[0m
[37m[1m[2023-07-11 10:18:15,107][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:18:15,110][233954] mean_value=-978.1216769910537, max_value=264.5867235181701[0m
[37m[1m[2023-07-11 10:18:15,113][233954] New mean coefficients: [[ 0.06308863  0.05750042  0.14882728  0.12032637 -0.11497369 -2.5691657 ]][0m
[37m[1m[2023-07-11 10:18:15,114][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:18:24,106][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 10:18:24,107][233954] FPS: 427093.41[0m
[36m[2023-07-11 10:18:24,109][233954] itr=764, itrs=2000, Progress: 38.20%[0m
[36m[2023-07-11 10:18:35,868][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 10:18:35,868][233954] FPS: 328991.89[0m
[36m[2023-07-11 10:18:40,127][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:18:40,127][233954] Reward + Measures: [[26.24549771  0.4008697   0.112931    0.12522167  0.2970843   0.40834039]][0m
[37m[1m[2023-07-11 10:18:40,128][233954] Max Reward on eval: 26.24549770592506[0m
[37m[1m[2023-07-11 10:18:40,128][233954] Min Reward on eval: 26.24549770592506[0m
[37m[1m[2023-07-11 10:18:40,128][233954] Mean Reward across all agents: 26.24549770592506[0m
[37m[1m[2023-07-11 10:18:40,128][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:18:45,161][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:18:45,162][233954] Reward + Measures: [[ -2.07353853   0.611        0.88329995   0.60869998   0.8215
    2.13735271]
 [ 21.91792511   0.2304       0.41249999   0.1998       0.2818
    1.14543808]
 [-25.58735804   0.24819998   0.29809999   0.16540001   0.30329999
    1.00094855]
 ...
 [  4.32749891   0.55669999   0.93360007   0.62910002   0.73180002
    2.13416266]
 [  8.76792043   0.10999999   0.1232       0.0869       0.17839999
    1.85109222]
 [ 67.83306639   0.1076       0.28459999   0.18620001   0.2651
    1.38560903]][0m
[37m[1m[2023-07-11 10:18:45,162][233954] Max Reward on eval: 618.9280431436375[0m
[37m[1m[2023-07-11 10:18:45,162][233954] Min Reward on eval: -72.24260578006506[0m
[37m[1m[2023-07-11 10:18:45,162][233954] Mean Reward across all agents: 76.57116264555164[0m
[37m[1m[2023-07-11 10:18:45,163][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:18:45,168][233954] mean_value=-697.504238684816, max_value=791.116821338851[0m
[37m[1m[2023-07-11 10:18:45,171][233954] New mean coefficients: [[ 0.09680019  0.03109825  0.06288075  0.06869135 -0.22564676 -2.4210758 ]][0m
[37m[1m[2023-07-11 10:18:45,172][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:18:54,119][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 10:18:54,120][233954] FPS: 429248.09[0m
[36m[2023-07-11 10:18:54,122][233954] itr=765, itrs=2000, Progress: 38.25%[0m
[36m[2023-07-11 10:19:05,828][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 10:19:05,829][233954] FPS: 330638.15[0m
[36m[2023-07-11 10:19:10,026][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:19:10,026][233954] Reward + Measures: [[18.78406034  0.140406    0.18250231  0.159587    0.20060968  0.9748978 ]][0m
[37m[1m[2023-07-11 10:19:10,027][233954] Max Reward on eval: 18.784060335579174[0m
[37m[1m[2023-07-11 10:19:10,027][233954] Min Reward on eval: 18.784060335579174[0m
[37m[1m[2023-07-11 10:19:10,027][233954] Mean Reward across all agents: 18.784060335579174[0m
[37m[1m[2023-07-11 10:19:10,027][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:19:14,962][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:19:14,963][233954] Reward + Measures: [[20.96511858  0.15710001  0.0529      0.036       0.17290001  0.88859123]
 [10.9437677   0.20840001  0.2624      0.16489999  0.3175      1.54439163]
 [86.18834493  0.1028      0.34370002  0.2177      0.37309998  2.05833769]
 ...
 [21.9529137   0.16140001  0.40970001  0.16250001  0.39229998  1.8845017 ]
 [45.76593395  0.245       0.2383      0.0332      0.33049998  1.22646415]
 [19.79702112  0.2184      0.25190002  0.16199999  0.32070002  1.32595384]][0m
[37m[1m[2023-07-11 10:19:14,963][233954] Max Reward on eval: 617.1057739200071[0m
[37m[1m[2023-07-11 10:19:14,964][233954] Min Reward on eval: -173.48339844428702[0m
[37m[1m[2023-07-11 10:19:14,964][233954] Mean Reward across all agents: 61.53438889588752[0m
[37m[1m[2023-07-11 10:19:14,964][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:19:14,967][233954] mean_value=-353.96133317481514, max_value=485.4502156813149[0m
[37m[1m[2023-07-11 10:19:14,970][233954] New mean coefficients: [[-0.03494324 -0.13661095 -0.09311832  0.24713597 -0.30873942 -2.372845  ]][0m
[37m[1m[2023-07-11 10:19:14,971][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:19:24,013][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 10:19:24,014][233954] FPS: 424744.81[0m
[36m[2023-07-11 10:19:24,016][233954] itr=766, itrs=2000, Progress: 38.30%[0m
[36m[2023-07-11 10:19:35,652][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 10:19:35,652][233954] FPS: 332638.80[0m
[36m[2023-07-11 10:19:39,984][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:19:39,985][233954] Reward + Measures: [[19.27872624  0.13318533  0.18037198  0.16235234  0.18707567  0.9251712 ]][0m
[37m[1m[2023-07-11 10:19:39,985][233954] Max Reward on eval: 19.278726235411927[0m
[37m[1m[2023-07-11 10:19:39,985][233954] Min Reward on eval: 19.278726235411927[0m
[37m[1m[2023-07-11 10:19:39,986][233954] Mean Reward across all agents: 19.278726235411927[0m
[37m[1m[2023-07-11 10:19:39,986][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:19:44,873][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:19:44,873][233954] Reward + Measures: [[-10.15280435   0.1147       0.18910001   0.10550001   0.1868
    1.08302772]
 [ 88.47138787   0.0809       0.22810002   0.14850001   0.26550004
    1.99830782]
 [-11.55023845   0.1017       0.18699999   0.1001       0.19610001
    1.69874763]
 ...
 [-22.25634457   0.1661       0.198        0.13599999   0.27190003
    1.59435201]
 [ 40.4312185    0.2703       0.097        0.18900001   0.25680003
    1.16836071]
 [ 68.80884338   0.60100001   0.83279991   0.62410003   0.75159997
    2.02492905]][0m
[37m[1m[2023-07-11 10:19:44,874][233954] Max Reward on eval: 486.96406172979624[0m
[37m[1m[2023-07-11 10:19:44,874][233954] Min Reward on eval: -174.55551029958298[0m
[37m[1m[2023-07-11 10:19:44,874][233954] Mean Reward across all agents: 49.15964476570647[0m
[37m[1m[2023-07-11 10:19:44,874][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:19:44,878][233954] mean_value=-700.050083038941, max_value=534.8027053904719[0m
[37m[1m[2023-07-11 10:19:44,880][233954] New mean coefficients: [[ 0.09199555 -0.09869746  0.00428991  0.21808833 -0.21915296 -2.4069927 ]][0m
[37m[1m[2023-07-11 10:19:44,881][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:19:53,831][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 10:19:53,831][233954] FPS: 429160.43[0m
[36m[2023-07-11 10:19:53,833][233954] itr=767, itrs=2000, Progress: 38.35%[0m
[36m[2023-07-11 10:20:05,589][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 10:20:05,589][233954] FPS: 329165.71[0m
[36m[2023-07-11 10:20:09,912][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:20:09,913][233954] Reward + Measures: [[20.34786189  0.13020267  0.16476701  0.16336568  0.16876532  0.81816292]][0m
[37m[1m[2023-07-11 10:20:09,913][233954] Max Reward on eval: 20.347861890275624[0m
[37m[1m[2023-07-11 10:20:09,913][233954] Min Reward on eval: 20.347861890275624[0m
[37m[1m[2023-07-11 10:20:09,913][233954] Mean Reward across all agents: 20.347861890275624[0m
[37m[1m[2023-07-11 10:20:09,914][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:20:15,140][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:20:15,146][233954] Reward + Measures: [[ 11.66366512   0.77720004   0.8585       0.76910001   0.84759998
    2.17190838]
 [ -5.36554944   0.80579996   0.8373       0.77579999   0.84930003
    2.2676158 ]
 [ 29.69203395   0.1715       0.19660001   0.1867       0.2297
    1.02141762]
 ...
 [ 29.93053859   0.79210001   0.89860004   0.79800004   0.88679999
    2.12676048]
 [  3.3194418    0.1946       0.0898       0.13270001   0.18910001
    1.78663945]
 [-23.90773729   0.1102       0.11080001   0.0966       0.17560001
    1.7519244 ]][0m
[37m[1m[2023-07-11 10:20:15,147][233954] Max Reward on eval: 197.66202207766474[0m
[37m[1m[2023-07-11 10:20:15,147][233954] Min Reward on eval: -102.48014068598859[0m
[37m[1m[2023-07-11 10:20:15,147][233954] Mean Reward across all agents: 24.507252143902917[0m
[37m[1m[2023-07-11 10:20:15,147][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:20:15,150][233954] mean_value=-1021.7471633238961, max_value=250.08565397498296[0m
[37m[1m[2023-07-11 10:20:15,152][233954] New mean coefficients: [[ 0.08342418 -0.11451007 -0.27641732  0.39066237 -0.20639746 -2.4401865 ]][0m
[37m[1m[2023-07-11 10:20:15,153][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:20:24,203][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 10:20:24,204][233954] FPS: 424378.05[0m
[36m[2023-07-11 10:20:24,206][233954] itr=768, itrs=2000, Progress: 38.40%[0m
[36m[2023-07-11 10:20:35,859][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 10:20:35,859][233954] FPS: 332134.88[0m
[36m[2023-07-11 10:20:40,148][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:20:40,148][233954] Reward + Measures: [[21.1609787   0.14085899  0.15543066  0.16858999  0.16868533  0.76488101]][0m
[37m[1m[2023-07-11 10:20:40,149][233954] Max Reward on eval: 21.160978696086755[0m
[37m[1m[2023-07-11 10:20:40,149][233954] Min Reward on eval: 21.160978696086755[0m
[37m[1m[2023-07-11 10:20:40,149][233954] Mean Reward across all agents: 21.160978696086755[0m
[37m[1m[2023-07-11 10:20:40,149][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:20:45,126][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:20:45,127][233954] Reward + Measures: [[ 77.94937381   0.24360001   0.09249999   0.1402       0.3249
    1.15027273]
 [ 33.92911589   0.48890001   0.55040002   0.46960002   0.56120002
    2.06225944]
 [-31.63592192   0.40420005   0.50730002   0.43069997   0.27129999
    2.04361463]
 ...
 [ 95.89730545   0.4743       0.53960001   0.43109998   0.56809992
    1.6987642 ]
 [-17.85466182   0.53190005   0.58210003   0.50990003   0.3154
    1.8431263 ]
 [ 26.71125826   0.56490004   0.63299996   0.5169       0.59680003
    1.94959247]][0m
[37m[1m[2023-07-11 10:20:45,127][233954] Max Reward on eval: 778.7205276298337[0m
[37m[1m[2023-07-11 10:20:45,127][233954] Min Reward on eval: -102.21177961986977[0m
[37m[1m[2023-07-11 10:20:45,127][233954] Mean Reward across all agents: 108.65805421325601[0m
[37m[1m[2023-07-11 10:20:45,128][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:20:45,131][233954] mean_value=-717.3327115972961, max_value=414.5942419739999[0m
[37m[1m[2023-07-11 10:20:45,134][233954] New mean coefficients: [[ 0.24081612 -0.01799895 -0.19652939  0.5577736  -0.0275117  -2.4329    ]][0m
[37m[1m[2023-07-11 10:20:45,135][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:20:54,042][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 10:20:54,042][233954] FPS: 431195.20[0m
[36m[2023-07-11 10:20:54,044][233954] itr=769, itrs=2000, Progress: 38.45%[0m
[36m[2023-07-11 10:21:05,564][233954] train() took 11.43 seconds to complete[0m
[36m[2023-07-11 10:21:05,564][233954] FPS: 335905.92[0m
[36m[2023-07-11 10:21:09,841][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:21:09,841][233954] Reward + Measures: [[19.84823222  0.14157566  0.15084133  0.17060766  0.16923733  0.73445493]][0m
[37m[1m[2023-07-11 10:21:09,842][233954] Max Reward on eval: 19.84823221740183[0m
[37m[1m[2023-07-11 10:21:09,842][233954] Min Reward on eval: 19.84823221740183[0m
[37m[1m[2023-07-11 10:21:09,842][233954] Mean Reward across all agents: 19.84823221740183[0m
[37m[1m[2023-07-11 10:21:09,842][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:21:14,806][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:21:14,806][233954] Reward + Measures: [[40.68872257  0.0918      0.1201      0.0746      0.10339999  1.08311808]
 [28.2303036   0.0682      0.0616      0.0746      0.1188      1.5744344 ]
 [26.80522241  0.1696      0.0727      0.14140001  0.22040001  1.58380628]
 ...
 [-7.5439232   0.09240001  0.12279999  0.09280001  0.1202      1.44466925]
 [-4.36091134  0.09240001  0.1437      0.1066      0.18440001  2.22971153]
 [-6.74255304  0.38620001  0.40909997  0.34290001  0.45250002  1.92948973]][0m
[37m[1m[2023-07-11 10:21:14,807][233954] Max Reward on eval: 425.39413450902794[0m
[37m[1m[2023-07-11 10:21:14,807][233954] Min Reward on eval: -93.1926587414113[0m
[37m[1m[2023-07-11 10:21:14,807][233954] Mean Reward across all agents: 29.381532977782413[0m
[37m[1m[2023-07-11 10:21:14,807][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:21:14,810][233954] mean_value=-911.8359673869604, max_value=166.36446824361346[0m
[37m[1m[2023-07-11 10:21:14,812][233954] New mean coefficients: [[ 0.00565316 -0.23822874  0.10159492  0.6211585   0.07893714 -2.7499173 ]][0m
[37m[1m[2023-07-11 10:21:14,813][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:21:23,752][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 10:21:23,752][233954] FPS: 429654.60[0m
[36m[2023-07-11 10:21:23,755][233954] itr=770, itrs=2000, Progress: 38.50%[0m
[37m[1m[2023-07-11 10:24:40,685][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000750[0m
[36m[2023-07-11 10:24:53,044][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 10:24:53,044][233954] FPS: 327829.28[0m
[36m[2023-07-11 10:24:57,263][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:24:57,269][233954] Reward + Measures: [[19.63828838  0.13045067  0.16096933  0.19054134  0.15677066  0.69320232]][0m
[37m[1m[2023-07-11 10:24:57,269][233954] Max Reward on eval: 19.638288382826214[0m
[37m[1m[2023-07-11 10:24:57,270][233954] Min Reward on eval: 19.638288382826214[0m
[37m[1m[2023-07-11 10:24:57,270][233954] Mean Reward across all agents: 19.638288382826214[0m
[37m[1m[2023-07-11 10:24:57,270][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:25:02,130][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:25:02,130][233954] Reward + Measures: [[-66.70582855   0.21899998   0.5284       0.2633       0.56490004
    1.70655942]
 [ -4.78868526   0.1373       0.23099999   0.26480001   0.25490001
    1.23314011]
 [ 33.80720989   0.1972       0.24160002   0.14170001   0.28270003
    1.06823957]
 ...
 [ 28.20312701   0.08280001   0.099        0.0866       0.1375
    0.70283175]
 [ -2.99660897   0.69999999   0.75949997   0.67769998   0.72660005
    2.10843301]
 [215.44361686   0.45019999   0.88749999   0.47259998   0.88300002
    2.25777555]][0m
[37m[1m[2023-07-11 10:25:02,130][233954] Max Reward on eval: 778.8097000263631[0m
[37m[1m[2023-07-11 10:25:02,131][233954] Min Reward on eval: -132.57519148440332[0m
[37m[1m[2023-07-11 10:25:02,131][233954] Mean Reward across all agents: 144.79656547341884[0m
[37m[1m[2023-07-11 10:25:02,131][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:25:02,136][233954] mean_value=-631.0322364035783, max_value=241.41720955474858[0m
[37m[1m[2023-07-11 10:25:02,138][233954] New mean coefficients: [[-0.06310055 -0.4785564  -0.08843485  0.85107267  0.06754092 -3.1290565 ]][0m
[37m[1m[2023-07-11 10:25:02,139][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:25:10,928][233954] train() took 8.79 seconds to complete[0m
[36m[2023-07-11 10:25:10,928][233954] FPS: 436992.97[0m
[36m[2023-07-11 10:25:10,931][233954] itr=771, itrs=2000, Progress: 38.55%[0m
[36m[2023-07-11 10:25:22,562][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 10:25:22,563][233954] FPS: 332750.51[0m
[36m[2023-07-11 10:25:26,750][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:25:26,750][233954] Reward + Measures: [[21.24594545  0.13071066  0.16235799  0.19637032  0.15734868  0.64960247]][0m
[37m[1m[2023-07-11 10:25:26,750][233954] Max Reward on eval: 21.24594544506502[0m
[37m[1m[2023-07-11 10:25:26,751][233954] Min Reward on eval: 21.24594544506502[0m
[37m[1m[2023-07-11 10:25:26,751][233954] Mean Reward across all agents: 21.24594544506502[0m
[37m[1m[2023-07-11 10:25:26,751][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:25:31,698][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:25:31,698][233954] Reward + Measures: [[ -1.82717154   0.1355       0.36989999   0.1662       0.32839999
    1.22589207]
 [107.32401823   0.20720001   0.17400001   0.1487       0.24440001
    1.40339458]
 [ 40.32225739   0.2703       0.2057       0.16379999   0.30000001
    1.48742092]
 ...
 [ 20.49350829   0.0963       0.0758       0.0732       0.0769
    1.12323916]
 [ 16.11371261   0.21160002   0.3515       0.21350001   0.35760003
    1.35212517]
 [ 89.55489395   0.1374       0.43069997   0.24760003   0.47269997
    1.7827183 ]][0m
[37m[1m[2023-07-11 10:25:31,699][233954] Max Reward on eval: 522.0772502399981[0m
[37m[1m[2023-07-11 10:25:31,699][233954] Min Reward on eval: -115.06178127042949[0m
[37m[1m[2023-07-11 10:25:31,699][233954] Mean Reward across all agents: 86.82064715672583[0m
[37m[1m[2023-07-11 10:25:31,699][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:25:31,702][233954] mean_value=-1007.170017081457, max_value=567.7933652799577[0m
[37m[1m[2023-07-11 10:25:31,705][233954] New mean coefficients: [[ 0.03602617 -0.4258762   0.00565583  0.8821521   0.22778428 -3.260632  ]][0m
[37m[1m[2023-07-11 10:25:31,705][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:25:40,635][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 10:25:40,635][233954] FPS: 430113.39[0m
[36m[2023-07-11 10:25:40,637][233954] itr=772, itrs=2000, Progress: 38.60%[0m
[36m[2023-07-11 10:25:52,413][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 10:25:52,418][233954] FPS: 328641.95[0m
[36m[2023-07-11 10:25:56,687][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:25:56,687][233954] Reward + Measures: [[20.70677297  0.13174     0.17985001  0.21546601  0.15655999  0.62431926]][0m
[37m[1m[2023-07-11 10:25:56,687][233954] Max Reward on eval: 20.706772966198777[0m
[37m[1m[2023-07-11 10:25:56,688][233954] Min Reward on eval: 20.706772966198777[0m
[37m[1m[2023-07-11 10:25:56,688][233954] Mean Reward across all agents: 20.706772966198777[0m
[37m[1m[2023-07-11 10:25:56,688][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:26:01,651][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:26:01,652][233954] Reward + Measures: [[-13.45103987   0.23709999   0.23609999   0.15880001   0.33019999
    1.39049125]
 [ 76.59428193   0.2739       0.6027       0.34779999   0.53240001
    2.2296536 ]
 [ 20.19770022   0.1833       0.1892       0.19940001   0.28790003
    0.99694866]
 ...
 [ 22.64513297   0.21729998   0.30879998   0.2775       0.30440003
    0.92924213]
 [ 49.5690059    0.41160002   0.49710003   0.39440003   0.4896
    1.5718888 ]
 [ 61.0320676    0.16559999   0.16970001   0.19990002   0.26940003
    1.25610423]][0m
[37m[1m[2023-07-11 10:26:01,652][233954] Max Reward on eval: 741.1557159775869[0m
[37m[1m[2023-07-11 10:26:01,653][233954] Min Reward on eval: -87.09746080324985[0m
[37m[1m[2023-07-11 10:26:01,653][233954] Mean Reward across all agents: 172.78918533658134[0m
[37m[1m[2023-07-11 10:26:01,653][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:26:01,656][233954] mean_value=-799.2640337854529, max_value=504.18841301727423[0m
[37m[1m[2023-07-11 10:26:01,658][233954] New mean coefficients: [[-0.26017594 -0.5114844  -0.09373257  1.0075643   0.11225154 -3.2753847 ]][0m
[37m[1m[2023-07-11 10:26:01,659][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:26:10,624][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 10:26:10,625][233954] FPS: 428390.35[0m
[36m[2023-07-11 10:26:10,627][233954] itr=773, itrs=2000, Progress: 38.65%[0m
[36m[2023-07-11 10:26:22,434][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 10:26:22,439][233954] FPS: 327787.49[0m
[36m[2023-07-11 10:26:26,640][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:26:26,641][233954] Reward + Measures: [[20.62405247  0.12446366  0.17602366  0.21899365  0.16177133  0.6157701 ]][0m
[37m[1m[2023-07-11 10:26:26,641][233954] Max Reward on eval: 20.62405246767754[0m
[37m[1m[2023-07-11 10:26:26,641][233954] Min Reward on eval: 20.62405246767754[0m
[37m[1m[2023-07-11 10:26:26,642][233954] Mean Reward across all agents: 20.62405246767754[0m
[37m[1m[2023-07-11 10:26:26,642][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:26:31,611][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:26:31,612][233954] Reward + Measures: [[ 52.27553638   0.26260003   0.44320002   0.234        0.36379999
    1.64166343]
 [ 40.3196691    0.0675       0.19770001   0.11370001   0.13410001
    1.10351849]
 [184.74179096   0.19410001   0.40229997   0.37939999   0.53979999
    1.59792244]
 ...
 [ 54.80504887   0.51789999   0.55150002   0.52320004   0.51850003
    1.85391164]
 [190.70227842   0.0455       0.72239995   0.4786       0.69050002
    1.97962153]
 [ 14.26424992   0.0844       0.07910001   0.08020001   0.12730001
    1.16804564]][0m
[37m[1m[2023-07-11 10:26:31,612][233954] Max Reward on eval: 720.8996048234403[0m
[37m[1m[2023-07-11 10:26:31,612][233954] Min Reward on eval: -44.994840686989484[0m
[37m[1m[2023-07-11 10:26:31,612][233954] Mean Reward across all agents: 113.51026262314372[0m
[37m[1m[2023-07-11 10:26:31,613][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:26:31,617][233954] mean_value=-479.77261689100396, max_value=358.43999238415654[0m
[37m[1m[2023-07-11 10:26:31,619][233954] New mean coefficients: [[-0.2348658  -0.21816212  0.19109043  1.3307158   0.2916513  -2.8260038 ]][0m
[37m[1m[2023-07-11 10:26:31,621][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:26:40,559][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 10:26:40,559][233954] FPS: 429681.21[0m
[36m[2023-07-11 10:26:40,562][233954] itr=774, itrs=2000, Progress: 38.70%[0m
[36m[2023-07-11 10:26:52,354][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 10:26:52,354][233954] FPS: 328149.58[0m
[36m[2023-07-11 10:26:56,642][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:26:56,642][233954] Reward + Measures: [[20.65114154  0.13750066  0.18756567  0.24124865  0.15878832  0.59181178]][0m
[37m[1m[2023-07-11 10:26:56,642][233954] Max Reward on eval: 20.651141538586575[0m
[37m[1m[2023-07-11 10:26:56,643][233954] Min Reward on eval: 20.651141538586575[0m
[37m[1m[2023-07-11 10:26:56,643][233954] Mean Reward across all agents: 20.651141538586575[0m
[37m[1m[2023-07-11 10:26:56,643][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:27:01,870][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:27:01,870][233954] Reward + Measures: [[  1.73649989   0.1635       0.0928       0.0469       0.26390001
    1.33182323]
 [ 78.08220584   0.62440008   0.81879997   0.59990001   0.77350003
    2.085536  ]
 [ 11.89941719   0.08540001   0.2016       0.09300001   0.1629
    1.28086889]
 ...
 [ 22.3199767    0.19790001   0.1837       0.1206       0.2852
    1.28895783]
 [-18.92692318   0.48570004   0.90669996   0.0953       0.91350001
    2.23911929]
 [ 23.78200823   0.1099       0.07839999   0.0606       0.1048
    1.52167344]][0m
[37m[1m[2023-07-11 10:27:01,871][233954] Max Reward on eval: 466.4703583980678[0m
[37m[1m[2023-07-11 10:27:01,871][233954] Min Reward on eval: -108.82862463425845[0m
[37m[1m[2023-07-11 10:27:01,871][233954] Mean Reward across all agents: 41.06053820968045[0m
[37m[1m[2023-07-11 10:27:01,871][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:27:01,876][233954] mean_value=-526.1103224268409, max_value=710.5254508736462[0m
[37m[1m[2023-07-11 10:27:01,878][233954] New mean coefficients: [[ 0.1153837   0.16692552  0.17697488  1.4051024   0.4368021  -2.250454  ]][0m
[37m[1m[2023-07-11 10:27:01,879][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:27:10,951][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 10:27:10,952][233954] FPS: 423348.51[0m
[36m[2023-07-11 10:27:10,954][233954] itr=775, itrs=2000, Progress: 38.75%[0m
[36m[2023-07-11 10:27:22,510][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 10:27:22,510][233954] FPS: 334860.96[0m
[36m[2023-07-11 10:27:26,752][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:27:26,752][233954] Reward + Measures: [[20.023173    0.14846732  0.21198267  0.24974199  0.164795    0.58209747]][0m
[37m[1m[2023-07-11 10:27:26,753][233954] Max Reward on eval: 20.023172996454527[0m
[37m[1m[2023-07-11 10:27:26,753][233954] Min Reward on eval: 20.023172996454527[0m
[37m[1m[2023-07-11 10:27:26,753][233954] Mean Reward across all agents: 20.023172996454527[0m
[37m[1m[2023-07-11 10:27:26,753][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:27:31,674][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:27:31,674][233954] Reward + Measures: [[ 32.55406827   0.83449996   0.9091       0.81959993   0.87160009
    2.13167477]
 [-13.14374533   0.92250007   0.95889997   0.92700005   0.95279998
    2.4170444 ]
 [  6.45121124   0.63300002   0.68400002   0.60860008   0.41050002
    2.12660646]
 ...
 [-24.81214891   0.94970006   0.97939998   0.94820005   0.97489995
    2.29410648]
 [ 20.22297287   0.0858       0.23969999   0.12230001   0.23550002
    1.67820644]
 [ 26.09377575   0.56999999   0.57010001   0.52250004   0.56520003
    1.81022227]][0m
[37m[1m[2023-07-11 10:27:31,675][233954] Max Reward on eval: 498.5665073480457[0m
[37m[1m[2023-07-11 10:27:31,675][233954] Min Reward on eval: -94.98896016397048[0m
[37m[1m[2023-07-11 10:27:31,675][233954] Mean Reward across all agents: 52.81316202878411[0m
[37m[1m[2023-07-11 10:27:31,675][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:27:31,679][233954] mean_value=-222.55620648408376, max_value=604.831880037244[0m
[37m[1m[2023-07-11 10:27:31,682][233954] New mean coefficients: [[-0.16322862  0.16504636 -0.00917889  1.7815485   0.49957392 -2.441102  ]][0m
[37m[1m[2023-07-11 10:27:31,683][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:27:40,707][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 10:27:40,708][233954] FPS: 425569.02[0m
[36m[2023-07-11 10:27:40,710][233954] itr=776, itrs=2000, Progress: 38.80%[0m
[36m[2023-07-11 10:27:52,372][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 10:27:52,373][233954] FPS: 331962.17[0m
[36m[2023-07-11 10:27:56,645][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:27:56,646][233954] Reward + Measures: [[20.2311342   0.15569767  0.21718699  0.24627966  0.17133833  0.5636192 ]][0m
[37m[1m[2023-07-11 10:27:56,646][233954] Max Reward on eval: 20.231134200093535[0m
[37m[1m[2023-07-11 10:27:56,646][233954] Min Reward on eval: 20.231134200093535[0m
[37m[1m[2023-07-11 10:27:56,646][233954] Mean Reward across all agents: 20.231134200093535[0m
[37m[1m[2023-07-11 10:27:56,647][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:28:01,689][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:28:01,689][233954] Reward + Measures: [[ 23.4990201    0.064        0.1295       0.0988       0.0944
    1.23467326]
 [422.0446387    0.0415       0.82390004   0.65009999   0.81970006
    2.16079593]
 [537.99851228   0.0062       0.995        0.83400005   0.9952001
    2.32366991]
 ...
 [538.92258071   0.035        0.97259998   0.65880001   0.95700008
    2.7536242 ]
 [ 61.33937231   0.0824       0.20039999   0.1265       0.1902
    1.32106102]
 [ 76.109898     0.0705       0.21440001   0.17030001   0.2199
    1.57775688]][0m
[37m[1m[2023-07-11 10:28:01,689][233954] Max Reward on eval: 725.3031158481724[0m
[37m[1m[2023-07-11 10:28:01,690][233954] Min Reward on eval: -79.27713779443874[0m
[37m[1m[2023-07-11 10:28:01,690][233954] Mean Reward across all agents: 209.28199017747377[0m
[37m[1m[2023-07-11 10:28:01,690][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:28:01,692][233954] mean_value=-778.018383723046, max_value=571.0931127522628[0m
[37m[1m[2023-07-11 10:28:01,695][233954] New mean coefficients: [[ 0.09516577  0.12150913  0.11377572  1.5112348   0.5199287  -2.2998598 ]][0m
[37m[1m[2023-07-11 10:28:01,696][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:28:10,676][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 10:28:10,676][233954] FPS: 427666.68[0m
[36m[2023-07-11 10:28:10,679][233954] itr=777, itrs=2000, Progress: 38.85%[0m
[36m[2023-07-11 10:28:22,233][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 10:28:22,234][233954] FPS: 335001.28[0m
[36m[2023-07-11 10:28:26,500][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:28:26,500][233954] Reward + Measures: [[21.16336367  0.16242667  0.23614933  0.28208867  0.16646966  0.5510655 ]][0m
[37m[1m[2023-07-11 10:28:26,500][233954] Max Reward on eval: 21.163363669322372[0m
[37m[1m[2023-07-11 10:28:26,501][233954] Min Reward on eval: 21.163363669322372[0m
[37m[1m[2023-07-11 10:28:26,501][233954] Mean Reward across all agents: 21.163363669322372[0m
[37m[1m[2023-07-11 10:28:26,501][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:28:31,469][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:28:31,470][233954] Reward + Measures: [[224.10286586   0.083        0.5323       0.38860002   0.57249999
    2.22651911]
 [ 67.32299534   0.32750002   0.1779       0.2376       0.30960003
    1.27652824]
 [ -7.31342525   0.15610002   0.2771       0.21370001   0.29789999
    1.7157284 ]
 ...
 [ 13.63218693   0.0967       0.14660001   0.10649999   0.13860001
    1.46290994]
 [ 46.33916016   0.2192       0.1186       0.2445       0.23450004
    1.75361717]
 [274.02639529   0.1568       0.93690008   0.58610004   0.89039993
    2.37488151]][0m
[37m[1m[2023-07-11 10:28:31,470][233954] Max Reward on eval: 757.1669425892178[0m
[37m[1m[2023-07-11 10:28:31,470][233954] Min Reward on eval: -80.42352909471374[0m
[37m[1m[2023-07-11 10:28:31,470][233954] Mean Reward across all agents: 88.93045269729228[0m
[37m[1m[2023-07-11 10:28:31,471][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:28:31,473][233954] mean_value=-919.174892732929, max_value=669.2518619222496[0m
[37m[1m[2023-07-11 10:28:31,476][233954] New mean coefficients: [[ 0.3161202   0.11406507  0.60561776  1.3602881   0.5625947  -1.8303967 ]][0m
[37m[1m[2023-07-11 10:28:31,477][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:28:40,454][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 10:28:40,454][233954] FPS: 427847.14[0m
[36m[2023-07-11 10:28:40,456][233954] itr=778, itrs=2000, Progress: 38.90%[0m
[36m[2023-07-11 10:28:52,036][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 10:28:52,036][233954] FPS: 334170.80[0m
[36m[2023-07-11 10:28:56,309][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:28:56,309][233954] Reward + Measures: [[21.01213463  0.155278    0.27221799  0.30029166  0.16118766  0.52522016]][0m
[37m[1m[2023-07-11 10:28:56,309][233954] Max Reward on eval: 21.01213463394546[0m
[37m[1m[2023-07-11 10:28:56,310][233954] Min Reward on eval: 21.01213463394546[0m
[37m[1m[2023-07-11 10:28:56,310][233954] Mean Reward across all agents: 21.01213463394546[0m
[37m[1m[2023-07-11 10:28:56,310][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:29:01,293][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:29:01,294][233954] Reward + Measures: [[ 75.61430612   0.0937       0.17970002   0.0905       0.21030001
    1.75106907]
 [ 57.68217393   0.1035       0.28400001   0.13680001   0.26980001
    1.60472047]
 [ 89.40563821   0.2217       0.60869998   0.138        0.56200004
    1.70639229]
 ...
 [197.56951523   0.27740002   0.80120009   0.42319998   0.70269996
    2.22653198]
 [ 91.7171102    0.30490002   0.35440001   0.27610001   0.35900003
    1.85733187]
 [ 19.77498028   0.0728       0.1153       0.07770001   0.1455
    1.36142194]][0m
[37m[1m[2023-07-11 10:29:01,294][233954] Max Reward on eval: 249.09774260716512[0m
[37m[1m[2023-07-11 10:29:01,294][233954] Min Reward on eval: -67.63001428088174[0m
[37m[1m[2023-07-11 10:29:01,295][233954] Mean Reward across all agents: 66.28976568436055[0m
[37m[1m[2023-07-11 10:29:01,295][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:29:01,298][233954] mean_value=-692.1731393575541, max_value=329.4837445201101[0m
[37m[1m[2023-07-11 10:29:01,301][233954] New mean coefficients: [[ 0.64981323  0.15098064  0.8783965   0.94656855  0.5442812  -1.8603753 ]][0m
[37m[1m[2023-07-11 10:29:01,302][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:29:10,352][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 10:29:10,352][233954] FPS: 424391.43[0m
[36m[2023-07-11 10:29:10,355][233954] itr=779, itrs=2000, Progress: 38.95%[0m
[36m[2023-07-11 10:29:22,119][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 10:29:22,119][233954] FPS: 328987.42[0m
[36m[2023-07-11 10:29:26,399][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:29:26,399][233954] Reward + Measures: [[20.43633748  0.16391265  0.26771501  0.28831401  0.171967    0.5098806 ]][0m
[37m[1m[2023-07-11 10:29:26,399][233954] Max Reward on eval: 20.436337478674876[0m
[37m[1m[2023-07-11 10:29:26,400][233954] Min Reward on eval: 20.436337478674876[0m
[37m[1m[2023-07-11 10:29:26,400][233954] Mean Reward across all agents: 20.436337478674876[0m
[37m[1m[2023-07-11 10:29:26,400][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:29:31,598][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:29:31,599][233954] Reward + Measures: [[  8.06109394   0.79290003   0.9052       0.80170006   0.76090002
    2.26761985]
 [202.30172288   0.2412       0.85369998   0.63620001   0.7518
    2.64901924]
 [ 43.06398507   0.48100001   0.83710003   0.55240005   0.61440003
    2.2769165 ]
 ...
 [ 99.35452464   0.1742       0.28980002   0.19250001   0.2404
    1.93711269]
 [  8.18184542   0.45860001   0.59140003   0.5011       0.45479998
    2.18480349]
 [ 76.16213781   0.2325       0.0868       0.1506       0.21500002
    0.96688825]][0m
[37m[1m[2023-07-11 10:29:31,599][233954] Max Reward on eval: 651.164409640152[0m
[37m[1m[2023-07-11 10:29:31,600][233954] Min Reward on eval: -78.76370973953745[0m
[37m[1m[2023-07-11 10:29:31,600][233954] Mean Reward across all agents: 113.33075209438464[0m
[37m[1m[2023-07-11 10:29:31,600][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:29:31,605][233954] mean_value=-376.0909250944536, max_value=578.1738013414531[0m
[37m[1m[2023-07-11 10:29:31,607][233954] New mean coefficients: [[ 0.15310526 -0.04460818  0.92642426  1.0089694   0.381691   -2.113398  ]][0m
[37m[1m[2023-07-11 10:29:31,608][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:29:40,547][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 10:29:40,548][233954] FPS: 429641.50[0m
[36m[2023-07-11 10:29:40,550][233954] itr=780, itrs=2000, Progress: 39.00%[0m
[37m[1m[2023-07-11 10:33:01,984][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000760[0m
[36m[2023-07-11 10:33:14,293][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 10:33:14,293][233954] FPS: 329803.78[0m
[36m[2023-07-11 10:33:18,511][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:33:18,511][233954] Reward + Measures: [[21.31354534  0.17791666  0.28121901  0.304842    0.18589766  0.53439683]][0m
[37m[1m[2023-07-11 10:33:18,512][233954] Max Reward on eval: 21.313545336562196[0m
[37m[1m[2023-07-11 10:33:18,512][233954] Min Reward on eval: 21.313545336562196[0m
[37m[1m[2023-07-11 10:33:18,512][233954] Mean Reward across all agents: 21.313545336562196[0m
[37m[1m[2023-07-11 10:33:18,512][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:33:23,482][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:33:23,483][233954] Reward + Measures: [[18.31624708  0.76919997  0.86110002  0.75730002  0.81910002  2.18782401]
 [39.82024567  0.4901      0.68699998  0.46950004  0.67850006  1.86231315]
 [44.69741292  0.1339      0.1902      0.12100001  0.22040001  1.65726566]
 ...
 [18.22749667  0.3867      0.46920004  0.30950001  0.32710001  1.44742095]
 [ 4.00623073  0.96289998  0.97240001  0.95219994  0.95020002  2.22681117]
 [55.93293584  0.72930002  0.80410004  0.70400006  0.7658      2.08216119]][0m
[37m[1m[2023-07-11 10:33:23,483][233954] Max Reward on eval: 385.46436593602414[0m
[37m[1m[2023-07-11 10:33:23,484][233954] Min Reward on eval: -149.48142384993844[0m
[37m[1m[2023-07-11 10:33:23,484][233954] Mean Reward across all agents: 36.26401841744236[0m
[37m[1m[2023-07-11 10:33:23,484][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:33:23,487][233954] mean_value=-624.6159995849047, max_value=495.65095265845764[0m
[37m[1m[2023-07-11 10:33:23,490][233954] New mean coefficients: [[-0.1546098  -0.07319894  0.799739    1.447398    0.5687617  -2.1871703 ]][0m
[37m[1m[2023-07-11 10:33:23,491][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:33:32,430][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 10:33:32,431][233954] FPS: 429626.77[0m
[36m[2023-07-11 10:33:32,433][233954] itr=781, itrs=2000, Progress: 39.05%[0m
[36m[2023-07-11 10:33:44,165][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 10:33:44,166][233954] FPS: 329821.88[0m
[36m[2023-07-11 10:33:48,390][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:33:48,390][233954] Reward + Measures: [[21.51293273  0.17800833  0.27608767  0.32501233  0.18411666  0.52528822]][0m
[37m[1m[2023-07-11 10:33:48,391][233954] Max Reward on eval: 21.512932727192442[0m
[37m[1m[2023-07-11 10:33:48,391][233954] Min Reward on eval: 21.512932727192442[0m
[37m[1m[2023-07-11 10:33:48,391][233954] Mean Reward across all agents: 21.512932727192442[0m
[37m[1m[2023-07-11 10:33:48,391][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:33:53,289][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:33:53,290][233954] Reward + Measures: [[ -5.44756193   0.2024       0.1549       0.0833       0.2067
    1.51282704]
 [ 63.22750284   0.2282       0.15100001   0.16849999   0.24660002
    1.40133154]
 [425.98591615   0.0528       0.9752       0.59630001   0.96630001
    2.41413665]
 ...
 [193.8644078    0.0433       0.29820001   0.21080001   0.33080003
    2.02455068]
 [-45.10744705   0.59060001   0.53920001   0.55310005   0.36680001
    1.89762104]
 [371.34670827   0.10160001   0.8136999    0.46479997   0.85089999
    2.68275619]][0m
[37m[1m[2023-07-11 10:33:53,290][233954] Max Reward on eval: 664.7363510224968[0m
[37m[1m[2023-07-11 10:33:53,290][233954] Min Reward on eval: -55.28103233785369[0m
[37m[1m[2023-07-11 10:33:53,290][233954] Mean Reward across all agents: 163.65346364038967[0m
[37m[1m[2023-07-11 10:33:53,291][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:33:53,295][233954] mean_value=-609.0094573909773, max_value=646.6320857378654[0m
[37m[1m[2023-07-11 10:33:53,298][233954] New mean coefficients: [[-0.02687088  0.34024236  0.63873756  1.5819886   0.97707283 -1.8498487 ]][0m
[37m[1m[2023-07-11 10:33:53,299][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:34:02,286][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 10:34:02,286][233954] FPS: 427364.27[0m
[36m[2023-07-11 10:34:02,288][233954] itr=782, itrs=2000, Progress: 39.10%[0m
[36m[2023-07-11 10:34:13,889][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 10:34:13,889][233954] FPS: 333536.63[0m
[36m[2023-07-11 10:34:18,141][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:34:18,142][233954] Reward + Measures: [[19.92855027  0.18407333  0.31029636  0.35197997  0.18769799  0.52352053]][0m
[37m[1m[2023-07-11 10:34:18,142][233954] Max Reward on eval: 19.928550273675594[0m
[37m[1m[2023-07-11 10:34:18,142][233954] Min Reward on eval: 19.928550273675594[0m
[37m[1m[2023-07-11 10:34:18,142][233954] Mean Reward across all agents: 19.928550273675594[0m
[37m[1m[2023-07-11 10:34:18,143][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:34:23,151][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:34:23,152][233954] Reward + Measures: [[ 95.20977719   0.22620001   0.68390006   0.13700001   0.62420005
    2.18311501]
 [ 30.37617898   0.53640002   0.54629999   0.59820002   0.49730006
    2.19814277]
 [ 61.85509894   0.21619999   0.32459998   0.26359999   0.35119998
    1.42645633]
 ...
 [165.59330273   0.24070001   0.81110001   0.2045       0.76350003
    1.91558993]
 [ 86.76080895   0.60659999   0.5794       0.69820005   0.50480002
    2.46673369]
 [ 11.21803202   0.24910001   0.43969998   0.30379999   0.49020004
    1.55046809]][0m
[37m[1m[2023-07-11 10:34:23,152][233954] Max Reward on eval: 573.1961784372572[0m
[37m[1m[2023-07-11 10:34:23,152][233954] Min Reward on eval: -50.49292411897331[0m
[37m[1m[2023-07-11 10:34:23,153][233954] Mean Reward across all agents: 79.73829120654366[0m
[37m[1m[2023-07-11 10:34:23,153][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:34:23,159][233954] mean_value=-309.1196083426199, max_value=582.4032544359646[0m
[37m[1m[2023-07-11 10:34:23,161][233954] New mean coefficients: [[-0.20234749 -0.15306917  1.1610638   2.0037935   0.660776   -2.3680923 ]][0m
[37m[1m[2023-07-11 10:34:23,162][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:34:32,209][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 10:34:32,210][233954] FPS: 424521.74[0m
[36m[2023-07-11 10:34:32,212][233954] itr=783, itrs=2000, Progress: 39.15%[0m
[36m[2023-07-11 10:34:44,116][233954] train() took 11.82 seconds to complete[0m
[36m[2023-07-11 10:34:44,117][233954] FPS: 324985.95[0m
[36m[2023-07-11 10:34:48,431][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:34:48,432][233954] Reward + Measures: [[19.55407077  0.16484132  0.33827832  0.37126398  0.181548    0.50760895]][0m
[37m[1m[2023-07-11 10:34:48,432][233954] Max Reward on eval: 19.554070768314986[0m
[37m[1m[2023-07-11 10:34:48,432][233954] Min Reward on eval: 19.554070768314986[0m
[37m[1m[2023-07-11 10:34:48,433][233954] Mean Reward across all agents: 19.554070768314986[0m
[37m[1m[2023-07-11 10:34:48,433][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:34:53,456][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:34:53,462][233954] Reward + Measures: [[-44.92790245   0.64020008   0.58700001   0.55029994   0.2483
    1.82559013]
 [-53.69828587   0.45460007   0.45620003   0.39809999   0.34559998
    1.61332929]
 [-29.38942779   0.58790004   0.66050005   0.56409997   0.42600003
    1.94599497]
 ...
 [ 11.6382022    0.23199999   0.34080002   0.26699999   0.207
    2.01054358]
 [-37.27423528   0.67740005   0.59060001   0.57959998   0.24679999
    1.98305309]
 [-55.96733931   0.7209       0.67820007   0.62250006   0.19810002
    1.99921072]][0m
[37m[1m[2023-07-11 10:34:53,463][233954] Max Reward on eval: 855.3248291305674[0m
[37m[1m[2023-07-11 10:34:53,464][233954] Min Reward on eval: -127.55127714546397[0m
[37m[1m[2023-07-11 10:34:53,464][233954] Mean Reward across all agents: 53.220787052524905[0m
[37m[1m[2023-07-11 10:34:53,465][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:34:53,477][233954] mean_value=-100.93404265438866, max_value=570.2093444190454[0m
[37m[1m[2023-07-11 10:34:53,481][233954] New mean coefficients: [[ 0.01765096  0.19192135  1.0781358   2.3019695   0.91542304 -2.0398295 ]][0m
[37m[1m[2023-07-11 10:34:53,483][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:35:02,567][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 10:35:02,567][233954] FPS: 422799.98[0m
[36m[2023-07-11 10:35:02,569][233954] itr=784, itrs=2000, Progress: 39.20%[0m
[36m[2023-07-11 10:35:14,351][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 10:35:14,351][233954] FPS: 328475.85[0m
[36m[2023-07-11 10:35:18,591][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:35:18,592][233954] Reward + Measures: [[19.85406916  0.16046     0.28211665  0.3448393   0.177875    0.51202065]][0m
[37m[1m[2023-07-11 10:35:18,592][233954] Max Reward on eval: 19.854069157902448[0m
[37m[1m[2023-07-11 10:35:18,592][233954] Min Reward on eval: 19.854069157902448[0m
[37m[1m[2023-07-11 10:35:18,593][233954] Mean Reward across all agents: 19.854069157902448[0m
[37m[1m[2023-07-11 10:35:18,593][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:35:23,608][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:35:23,613][233954] Reward + Measures: [[244.02093032   0.0605       0.94580001   0.62770003   0.9155001
    2.53738546]
 [293.31302333   0.0208       0.61180001   0.43970004   0.64350003
    2.36865449]
 [ 79.06210993   0.34639999   0.84130001   0.48389998   0.83350003
    2.12237263]
 ...
 [112.5290707    0.23469999   0.63420004   0.41700003   0.63330001
    2.53871608]
 [162.59719283   0.0307       0.43689999   0.33050001   0.51070005
    2.42558289]
 [ 82.67181194   0.1936       0.88620007   0.4824       0.77819997
    2.63454103]][0m
[37m[1m[2023-07-11 10:35:23,613][233954] Max Reward on eval: 779.0029144293163[0m
[37m[1m[2023-07-11 10:35:23,614][233954] Min Reward on eval: -55.16289008031599[0m
[37m[1m[2023-07-11 10:35:23,614][233954] Mean Reward across all agents: 281.31142164785126[0m
[37m[1m[2023-07-11 10:35:23,614][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:35:23,617][233954] mean_value=-297.8606141879796, max_value=538.0657113539253[0m
[37m[1m[2023-07-11 10:35:23,619][233954] New mean coefficients: [[-0.07731771  0.09955259  0.7673862   2.0585446   0.86395705 -2.2656794 ]][0m
[37m[1m[2023-07-11 10:35:23,620][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:35:32,611][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 10:35:32,611][233954] FPS: 427188.40[0m
[36m[2023-07-11 10:35:32,613][233954] itr=785, itrs=2000, Progress: 39.25%[0m
[36m[2023-07-11 10:35:44,338][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 10:35:44,338][233954] FPS: 329964.47[0m
[36m[2023-07-11 10:35:48,603][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:35:48,609][233954] Reward + Measures: [[20.26568395  0.17021966  0.25381598  0.33671033  0.18364233  0.50306511]][0m
[37m[1m[2023-07-11 10:35:48,609][233954] Max Reward on eval: 20.265683949218683[0m
[37m[1m[2023-07-11 10:35:48,609][233954] Min Reward on eval: 20.265683949218683[0m
[37m[1m[2023-07-11 10:35:48,609][233954] Mean Reward across all agents: 20.265683949218683[0m
[37m[1m[2023-07-11 10:35:48,610][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:35:53,531][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:35:53,531][233954] Reward + Measures: [[ 52.20840119   0.6182       0.69620001   0.5765       0.69370002
    2.1628449 ]
 [-10.06654319   0.69479996   0.72370005   0.68910003   0.74330002
    2.79971313]
 [-30.14513054   0.80910009   0.8235001    0.79540008   0.84540004
    2.77103472]
 ...
 [-22.0855306    0.40089998   0.49700004   0.39710003   0.47259998
    2.09126973]
 [ -9.53460075   0.64969999   0.68370003   0.61510003   0.73210001
    2.63794065]
 [ 17.46784253   0.59250003   0.62280005   0.57969999   0.65390003
    2.42310452]][0m
[37m[1m[2023-07-11 10:35:53,531][233954] Max Reward on eval: 391.38523090789096[0m
[37m[1m[2023-07-11 10:35:53,532][233954] Min Reward on eval: -132.23227240080013[0m
[37m[1m[2023-07-11 10:35:53,532][233954] Mean Reward across all agents: 16.918722293860416[0m
[37m[1m[2023-07-11 10:35:53,532][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:35:53,535][233954] mean_value=-173.3976321245098, max_value=527.0848078067917[0m
[37m[1m[2023-07-11 10:35:53,538][233954] New mean coefficients: [[ 0.2669826   0.472305    0.31682506  1.9742428   1.1115041  -1.4859138 ]][0m
[37m[1m[2023-07-11 10:35:53,538][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:36:02,505][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 10:36:02,505][233954] FPS: 428348.45[0m
[36m[2023-07-11 10:36:02,507][233954] itr=786, itrs=2000, Progress: 39.30%[0m
[36m[2023-07-11 10:36:14,115][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 10:36:14,115][233954] FPS: 333346.59[0m
[36m[2023-07-11 10:36:18,387][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:36:18,393][233954] Reward + Measures: [[19.57943752  0.17790735  0.31116769  0.37079698  0.19299801  0.50070417]][0m
[37m[1m[2023-07-11 10:36:18,393][233954] Max Reward on eval: 19.5794375193228[0m
[37m[1m[2023-07-11 10:36:18,393][233954] Min Reward on eval: 19.5794375193228[0m
[37m[1m[2023-07-11 10:36:18,394][233954] Mean Reward across all agents: 19.5794375193228[0m
[37m[1m[2023-07-11 10:36:18,394][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:36:23,349][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:36:23,350][233954] Reward + Measures: [[320.19252394   0.0063       0.99039996   0.67440003   0.99010003
    2.8208952 ]
 [ 52.89737232   0.1336       0.2098       0.0956       0.2638
    1.77898335]
 [100.12314368   0.0184       0.99669999   0.50169998   0.99239999
    3.59934282]
 ...
 [200.71744347   0.0005       0.99629992   0.64459997   0.99650002
    2.90548444]
 [ 63.95170821   0.1079       0.18260001   0.13850002   0.26680002
    2.24896169]
 [581.59847264   0.0095       0.9884001    0.74529999   0.98730004
    3.16395569]][0m
[37m[1m[2023-07-11 10:36:23,350][233954] Max Reward on eval: 686.2666625782847[0m
[37m[1m[2023-07-11 10:36:23,350][233954] Min Reward on eval: -100.2043790937867[0m
[37m[1m[2023-07-11 10:36:23,351][233954] Mean Reward across all agents: 150.30753292706243[0m
[37m[1m[2023-07-11 10:36:23,351][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:36:23,353][233954] mean_value=-335.0694217200906, max_value=522.2736186204944[0m
[37m[1m[2023-07-11 10:36:23,356][233954] New mean coefficients: [[ 0.22932546  0.07531238  0.43861815  1.4452088   0.76730496 -1.5523335 ]][0m
[37m[1m[2023-07-11 10:36:23,357][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:36:32,284][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 10:36:32,285][233954] FPS: 430198.87[0m
[36m[2023-07-11 10:36:32,287][233954] itr=787, itrs=2000, Progress: 39.35%[0m
[36m[2023-07-11 10:36:43,892][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 10:36:43,893][233954] FPS: 333495.12[0m
[36m[2023-07-11 10:36:48,106][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:36:48,111][233954] Reward + Measures: [[20.41133037  0.17824133  0.33653799  0.37554368  0.19816801  0.482687  ]][0m
[37m[1m[2023-07-11 10:36:48,111][233954] Max Reward on eval: 20.41133037395037[0m
[37m[1m[2023-07-11 10:36:48,112][233954] Min Reward on eval: 20.41133037395037[0m
[37m[1m[2023-07-11 10:36:48,112][233954] Mean Reward across all agents: 20.41133037395037[0m
[37m[1m[2023-07-11 10:36:48,112][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:36:53,269][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:36:53,275][233954] Reward + Measures: [[ 44.27773857   0.82880002   0.84240001   0.81440002   0.83430004
    3.01224518]
 [ 14.89169704   0.16599999   0.17660001   0.0675       0.1816
    1.50162888]
 [ 71.302263     0.52660006   0.6225       0.46619996   0.58280003
    1.80600584]
 ...
 [132.1146984    0.51810002   0.70789999   0.60869998   0.6498
    2.36094904]
 [ 60.44926856   0.29899999   0.57730001   0.31870002   0.56669998
    2.13423014]
 [ 40.75952216   0.3134       0.30739999   0.28169999   0.35180002
    1.66798675]][0m
[37m[1m[2023-07-11 10:36:53,275][233954] Max Reward on eval: 425.51682569654656[0m
[37m[1m[2023-07-11 10:36:53,276][233954] Min Reward on eval: -130.26139999083244[0m
[37m[1m[2023-07-11 10:36:53,276][233954] Mean Reward across all agents: 48.61573225865672[0m
[37m[1m[2023-07-11 10:36:53,276][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:36:53,279][233954] mean_value=-431.5887933930875, max_value=304.05062275238464[0m
[37m[1m[2023-07-11 10:36:53,282][233954] New mean coefficients: [[ 0.03326707 -0.18707016  0.62894356  1.566632    0.4775522  -1.9945716 ]][0m
[37m[1m[2023-07-11 10:36:53,283][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:37:02,219][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 10:37:02,219][233954] FPS: 429817.42[0m
[36m[2023-07-11 10:37:02,221][233954] itr=788, itrs=2000, Progress: 39.40%[0m
[36m[2023-07-11 10:37:13,779][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 10:37:13,779][233954] FPS: 334736.32[0m
[36m[2023-07-11 10:37:18,027][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:37:18,032][233954] Reward + Measures: [[18.93302054  0.19360535  0.34291133  0.37582999  0.20741767  0.51418573]][0m
[37m[1m[2023-07-11 10:37:18,033][233954] Max Reward on eval: 18.933020536554462[0m
[37m[1m[2023-07-11 10:37:18,033][233954] Min Reward on eval: 18.933020536554462[0m
[37m[1m[2023-07-11 10:37:18,033][233954] Mean Reward across all agents: 18.933020536554462[0m
[37m[1m[2023-07-11 10:37:18,033][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:37:22,976][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:37:22,977][233954] Reward + Measures: [[ 15.41984771   0.51550001   0.65369999   0.44899997   0.40450001
    1.98498118]
 [126.86507287   0.08549999   0.54530001   0.40450001   0.59649998
    1.84341621]
 [ 13.68418202   0.18770002   0.2474       0.1146       0.24460001
    1.28037965]
 ...
 [ 40.6197858    0.20020001   0.2428       0.12050001   0.27519998
    1.44157851]
 [  9.43977564   0.1767       0.17909999   0.148        0.20120001
    1.65356159]
 [ 39.00081148   0.0743       0.0632       0.0692       0.10120001
    1.36917138]][0m
[37m[1m[2023-07-11 10:37:22,977][233954] Max Reward on eval: 418.663612226164[0m
[37m[1m[2023-07-11 10:37:22,977][233954] Min Reward on eval: -108.79577043401078[0m
[37m[1m[2023-07-11 10:37:22,978][233954] Mean Reward across all agents: 75.31923984139604[0m
[37m[1m[2023-07-11 10:37:22,978][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:37:22,982][233954] mean_value=-482.89781283128696, max_value=398.3224848011751[0m
[37m[1m[2023-07-11 10:37:22,984][233954] New mean coefficients: [[ 0.15020977  0.10150972  0.44861448  1.266039    0.59532446 -1.8909463 ]][0m
[37m[1m[2023-07-11 10:37:22,985][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:37:32,009][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 10:37:32,009][233954] FPS: 425644.57[0m
[36m[2023-07-11 10:37:32,011][233954] itr=789, itrs=2000, Progress: 39.45%[0m
[36m[2023-07-11 10:37:43,612][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 10:37:43,612][233954] FPS: 333574.08[0m
[36m[2023-07-11 10:37:47,866][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:37:47,866][233954] Reward + Measures: [[19.76017866  0.16246733  0.3735463   0.393828    0.18659633  0.49835879]][0m
[37m[1m[2023-07-11 10:37:47,866][233954] Max Reward on eval: 19.760178663710054[0m
[37m[1m[2023-07-11 10:37:47,867][233954] Min Reward on eval: 19.760178663710054[0m
[37m[1m[2023-07-11 10:37:47,867][233954] Mean Reward across all agents: 19.760178663710054[0m
[37m[1m[2023-07-11 10:37:47,867][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:37:52,863][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:37:52,864][233954] Reward + Measures: [[-135.5385752     0.44280005    0.59240001    0.29010004    0.59849995
     2.38343668]
 [ -11.23942132    0.1191        0.30140001    0.18670002    0.26610002
     1.67397809]
 [ -56.48127849    0.1584        0.40549999    0.27069998    0.38319999
     2.10880542]
 ...
 [ -80.41604537    0.29700002    0.51940006    0.1705        0.50459999
     2.42730212]
 [   5.1420526     0.2033        0.15210001    0.18450001    0.21529999
     0.97821999]
 [  43.7194675     0.09340001    0.24779998    0.1558        0.2418
     2.02680469]][0m
[37m[1m[2023-07-11 10:37:52,864][233954] Max Reward on eval: 258.1101452946197[0m
[37m[1m[2023-07-11 10:37:52,865][233954] Min Reward on eval: -247.24591962117702[0m
[37m[1m[2023-07-11 10:37:52,865][233954] Mean Reward across all agents: 18.176244737734294[0m
[37m[1m[2023-07-11 10:37:52,865][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:37:52,867][233954] mean_value=-970.3695940461237, max_value=31.57891181287313[0m
[37m[1m[2023-07-11 10:37:52,869][233954] New mean coefficients: [[ 0.18410942 -0.20037547  0.48491883  1.1257741   0.55411965 -2.1883216 ]][0m
[37m[1m[2023-07-11 10:37:52,870][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:38:01,873][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 10:38:01,873][233954] FPS: 426608.75[0m
[36m[2023-07-11 10:38:01,875][233954] itr=790, itrs=2000, Progress: 39.50%[0m
[37m[1m[2023-07-11 10:41:19,952][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000770[0m
[36m[2023-07-11 10:41:32,466][233954] train() took 11.77 seconds to complete[0m
[36m[2023-07-11 10:41:32,467][233954] FPS: 326317.85[0m
[36m[2023-07-11 10:41:36,653][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:41:36,653][233954] Reward + Measures: [[19.22502268  0.16430233  0.37004101  0.38996565  0.19551367  0.51314825]][0m
[37m[1m[2023-07-11 10:41:36,654][233954] Max Reward on eval: 19.22502268164734[0m
[37m[1m[2023-07-11 10:41:36,654][233954] Min Reward on eval: 19.22502268164734[0m
[37m[1m[2023-07-11 10:41:36,654][233954] Mean Reward across all agents: 19.22502268164734[0m
[37m[1m[2023-07-11 10:41:36,654][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:41:41,574][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:41:41,574][233954] Reward + Measures: [[ 25.60124187   0.1584       0.0852       0.15550001   0.1962
    0.8426488 ]
 [ 41.50102418   0.0878       0.1186       0.0702       0.23190001
    0.89084625]
 [ 14.38179132   0.2036       0.3091       0.2484       0.26139998
    1.21995628]
 ...
 [195.31334463   0.13         0.59219998   0.3989       0.51109999
    1.87897909]
 [ -9.82092713   0.1778       0.3348       0.17140001   0.32980001
    1.36401212]
 [-14.00395867   0.48389998   0.59810001   0.41389999   0.56210005
    1.81205213]][0m
[37m[1m[2023-07-11 10:41:41,575][233954] Max Reward on eval: 423.88346862727775[0m
[37m[1m[2023-07-11 10:41:41,575][233954] Min Reward on eval: -111.09115363189485[0m
[37m[1m[2023-07-11 10:41:41,575][233954] Mean Reward across all agents: 50.993470838893835[0m
[37m[1m[2023-07-11 10:41:41,575][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:41:41,578][233954] mean_value=-1388.084156812911, max_value=434.56341071740377[0m
[37m[1m[2023-07-11 10:41:41,581][233954] New mean coefficients: [[ 0.06881175 -0.2091534   0.92196333  1.0824602   0.50404966 -2.245317  ]][0m
[37m[1m[2023-07-11 10:41:41,582][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:41:50,494][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 10:41:50,494][233954] FPS: 430938.30[0m
[36m[2023-07-11 10:41:50,497][233954] itr=791, itrs=2000, Progress: 39.55%[0m
[36m[2023-07-11 10:42:02,026][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 10:42:02,026][233954] FPS: 335621.97[0m
[36m[2023-07-11 10:42:06,261][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:42:06,262][233954] Reward + Measures: [[18.87934493  0.15355232  0.39875463  0.41845337  0.19652331  0.49397489]][0m
[37m[1m[2023-07-11 10:42:06,262][233954] Max Reward on eval: 18.87934492669909[0m
[37m[1m[2023-07-11 10:42:06,262][233954] Min Reward on eval: 18.87934492669909[0m
[37m[1m[2023-07-11 10:42:06,263][233954] Mean Reward across all agents: 18.87934492669909[0m
[37m[1m[2023-07-11 10:42:06,263][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:42:11,171][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:42:11,172][233954] Reward + Measures: [[36.4161768   0.47240001  0.62849998  0.44050002  0.55320001  1.90664327]
 [58.05887683  0.11430001  0.19290002  0.08630001  0.25710002  1.28479266]
 [ 1.78044727  0.63020003  0.79350001  0.5988      0.74079996  1.97678268]
 ...
 [37.11857561  0.22070001  0.2518      0.1476      0.26210001  1.53699028]
 [22.30305585  0.2942      0.2316      0.26550004  0.36680004  1.39635515]
 [20.56189901  0.2325      0.23860002  0.2325      0.266       1.81268466]][0m
[37m[1m[2023-07-11 10:42:11,172][233954] Max Reward on eval: 342.103189127706[0m
[37m[1m[2023-07-11 10:42:11,172][233954] Min Reward on eval: -98.37608478093753[0m
[37m[1m[2023-07-11 10:42:11,173][233954] Mean Reward across all agents: 47.59314250989667[0m
[37m[1m[2023-07-11 10:42:11,173][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:42:11,177][233954] mean_value=-422.7933882060234, max_value=547.2100844316185[0m
[37m[1m[2023-07-11 10:42:11,180][233954] New mean coefficients: [[ 0.27584943  0.1295697   0.9431683   1.2662582   0.49598044 -1.7248532 ]][0m
[37m[1m[2023-07-11 10:42:11,181][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:42:20,045][233954] train() took 8.86 seconds to complete[0m
[36m[2023-07-11 10:42:20,045][233954] FPS: 433293.55[0m
[36m[2023-07-11 10:42:20,047][233954] itr=792, itrs=2000, Progress: 39.60%[0m
[36m[2023-07-11 10:42:31,788][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 10:42:31,788][233954] FPS: 329507.80[0m
[36m[2023-07-11 10:42:36,035][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:42:36,040][233954] Reward + Measures: [[24.3858568   0.14149368  0.16539633  0.18791901  0.20331568  1.25941205]][0m
[37m[1m[2023-07-11 10:42:36,041][233954] Max Reward on eval: 24.38585680322527[0m
[37m[1m[2023-07-11 10:42:36,041][233954] Min Reward on eval: 24.38585680322527[0m
[37m[1m[2023-07-11 10:42:36,041][233954] Mean Reward across all agents: 24.38585680322527[0m
[37m[1m[2023-07-11 10:42:36,041][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:42:41,043][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:42:41,049][233954] Reward + Measures: [[116.16457463   0.0251       0.97150004   0.53400004   0.95249999
    2.26617718]
 [ 58.59699691   0.08290001   0.24910001   0.1864       0.2253
    1.74820292]
 [307.99591064   0.0319       0.94390005   0.71060002   0.93720001
    2.21525812]
 ...
 [155.37677855   0.18079999   0.63160002   0.1741       0.57780004
    2.04024768]
 [ 36.01476409   0.18550001   0.24210003   0.0947       0.2721
    1.29331338]
 [ 90.22990577   0.0843       0.74739999   0.36630002   0.73730004
    2.41940427]][0m
[37m[1m[2023-07-11 10:42:41,049][233954] Max Reward on eval: 372.17271992424503[0m
[37m[1m[2023-07-11 10:42:41,049][233954] Min Reward on eval: -56.26858051957097[0m
[37m[1m[2023-07-11 10:42:41,050][233954] Mean Reward across all agents: 109.02922109242992[0m
[37m[1m[2023-07-11 10:42:41,050][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:42:41,053][233954] mean_value=-460.9646054399646, max_value=557.9923143964252[0m
[37m[1m[2023-07-11 10:42:41,055][233954] New mean coefficients: [[ 0.5281221   0.0591172   0.92880064  0.91524875  0.23866946 -1.729566  ]][0m
[37m[1m[2023-07-11 10:42:41,056][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:42:50,024][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 10:42:50,025][233954] FPS: 428276.43[0m
[36m[2023-07-11 10:42:50,027][233954] itr=793, itrs=2000, Progress: 39.65%[0m
[36m[2023-07-11 10:43:01,692][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 10:43:01,692][233954] FPS: 331817.04[0m
[36m[2023-07-11 10:43:06,081][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:43:06,081][233954] Reward + Measures: [[25.43383798  0.125054    0.16047433  0.18408732  0.18707566  1.2346791 ]][0m
[37m[1m[2023-07-11 10:43:06,081][233954] Max Reward on eval: 25.433837984406853[0m
[37m[1m[2023-07-11 10:43:06,082][233954] Min Reward on eval: 25.433837984406853[0m
[37m[1m[2023-07-11 10:43:06,082][233954] Mean Reward across all agents: 25.433837984406853[0m
[37m[1m[2023-07-11 10:43:06,082][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:43:11,382][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:43:11,388][233954] Reward + Measures: [[ 56.79218949   0.15859999   0.40570003   0.1781       0.37480003
    1.7712971 ]
 [-30.08883591   0.68629998   0.46709999   0.60460007   0.32070002
    1.8029331 ]
 [ 35.04388007   0.37040001   0.24170001   0.36410001   0.41610003
    1.33286703]
 ...
 [ -2.29483604   0.0788       0.149        0.0819       0.1383
    1.58558977]
 [ -8.8357276    0.52179998   0.73380005   0.55129999   0.52719998
    2.29763031]
 [ 44.2248832    0.54629993   0.55339998   0.53640002   0.50380003
    1.92493904]][0m
[37m[1m[2023-07-11 10:43:11,388][233954] Max Reward on eval: 752.6008071938471[0m
[37m[1m[2023-07-11 10:43:11,388][233954] Min Reward on eval: -60.203905673627744[0m
[37m[1m[2023-07-11 10:43:11,389][233954] Mean Reward across all agents: 87.11843871136324[0m
[37m[1m[2023-07-11 10:43:11,389][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:43:11,393][233954] mean_value=-489.0252234980144, max_value=436.56820782835825[0m
[37m[1m[2023-07-11 10:43:11,396][233954] New mean coefficients: [[ 0.8846922   0.49880505  0.710325    1.3238668   0.50891197 -1.1400393 ]][0m
[37m[1m[2023-07-11 10:43:11,397][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:43:20,450][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 10:43:20,451][233954] FPS: 424213.80[0m
[36m[2023-07-11 10:43:20,453][233954] itr=794, itrs=2000, Progress: 39.70%[0m
[36m[2023-07-11 10:43:32,069][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 10:43:32,069][233954] FPS: 333138.99[0m
[36m[2023-07-11 10:43:36,360][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:43:36,360][233954] Reward + Measures: [[26.11085778  0.11968368  0.158732    0.18907365  0.18559067  1.15698826]][0m
[37m[1m[2023-07-11 10:43:36,361][233954] Max Reward on eval: 26.110857781549257[0m
[37m[1m[2023-07-11 10:43:36,361][233954] Min Reward on eval: 26.110857781549257[0m
[37m[1m[2023-07-11 10:43:36,361][233954] Mean Reward across all agents: 26.110857781549257[0m
[37m[1m[2023-07-11 10:43:36,361][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:43:41,389][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:43:41,395][233954] Reward + Measures: [[ 25.92795881   0.0496       0.10520001   0.09890001   0.1153
    1.32552803]
 [-12.43757504   0.75570005   0.87670004   0.73250002   0.82460004
    2.57232308]
 [ 40.92405096   0.24340001   0.2498       0.28080001   0.29749998
    1.33317971]
 ...
 [ -1.6213831    0.94770002   0.97369999   0.93950003   0.96470004
    2.62516642]
 [ 55.35830183   0.1741       0.21789999   0.16880001   0.24770001
    1.83071792]
 [ 31.79948994   0.70200008   0.75600004   0.6656       0.71390003
    2.25384974]][0m
[37m[1m[2023-07-11 10:43:41,395][233954] Max Reward on eval: 185.141316669737[0m
[37m[1m[2023-07-11 10:43:41,395][233954] Min Reward on eval: -47.37630512882024[0m
[37m[1m[2023-07-11 10:43:41,396][233954] Mean Reward across all agents: 24.5001663229574[0m
[37m[1m[2023-07-11 10:43:41,396][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:43:41,398][233954] mean_value=-473.38807148003883, max_value=171.73174905363882[0m
[37m[1m[2023-07-11 10:43:41,400][233954] New mean coefficients: [[ 0.95274186  0.9242284   0.5499931   1.35382     0.99057114 -0.781031  ]][0m
[37m[1m[2023-07-11 10:43:41,401][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:43:50,409][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 10:43:50,409][233954] FPS: 426364.00[0m
[36m[2023-07-11 10:43:50,412][233954] itr=795, itrs=2000, Progress: 39.75%[0m
[36m[2023-07-11 10:44:02,496][233954] train() took 11.99 seconds to complete[0m
[36m[2023-07-11 10:44:02,496][233954] FPS: 320202.59[0m
[36m[2023-07-11 10:44:06,724][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:44:06,725][233954] Reward + Measures: [[24.83427869  0.12058834  0.157767    0.19754532  0.19061568  1.12020528]][0m
[37m[1m[2023-07-11 10:44:06,725][233954] Max Reward on eval: 24.834278689214834[0m
[37m[1m[2023-07-11 10:44:06,725][233954] Min Reward on eval: 24.834278689214834[0m
[37m[1m[2023-07-11 10:44:06,726][233954] Mean Reward across all agents: 24.834278689214834[0m
[37m[1m[2023-07-11 10:44:06,726][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:44:11,742][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:44:11,748][233954] Reward + Measures: [[ 53.35636049   0.0881       0.0726       0.0784       0.0925
    2.15176082]
 [ 56.77501421   0.233        0.30690002   0.12730001   0.24280003
    1.57265854]
 [167.04861197   0.12640001   0.3757       0.23870002   0.35180002
    1.77503669]
 ...
 [248.61119988   0.1912       0.33410001   0.2854       0.42430001
    2.48356414]
 [179.80418477   0.10209999   0.1566       0.1269       0.19720002
    2.45348167]
 [ 69.11438988   0.2289       0.1717       0.1567       0.2974
    2.28960538]][0m
[37m[1m[2023-07-11 10:44:11,749][233954] Max Reward on eval: 661.6587562622502[0m
[37m[1m[2023-07-11 10:44:11,749][233954] Min Reward on eval: -62.7842874079477[0m
[37m[1m[2023-07-11 10:44:11,749][233954] Mean Reward across all agents: 107.27508855176389[0m
[37m[1m[2023-07-11 10:44:11,749][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:44:11,752][233954] mean_value=-465.7300597067191, max_value=545.4782977493387[0m
[37m[1m[2023-07-11 10:44:11,755][233954] New mean coefficients: [[ 1.0723628   1.0515332   0.29954118  1.2290254   0.9565674  -1.0130228 ]][0m
[37m[1m[2023-07-11 10:44:11,756][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:44:20,872][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 10:44:20,873][233954] FPS: 421283.95[0m
[36m[2023-07-11 10:44:20,875][233954] itr=796, itrs=2000, Progress: 39.80%[0m
[36m[2023-07-11 10:44:32,490][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 10:44:32,490][233954] FPS: 333279.37[0m
[36m[2023-07-11 10:44:36,852][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:44:36,852][233954] Reward + Measures: [[24.90298932  0.13005733  0.17112066  0.212534    0.20118631  1.13102365]][0m
[37m[1m[2023-07-11 10:44:36,852][233954] Max Reward on eval: 24.902989317527748[0m
[37m[1m[2023-07-11 10:44:36,853][233954] Min Reward on eval: 24.902989317527748[0m
[37m[1m[2023-07-11 10:44:36,853][233954] Mean Reward across all agents: 24.902989317527748[0m
[37m[1m[2023-07-11 10:44:36,853][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:44:41,887][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:44:41,893][233954] Reward + Measures: [[ -0.59904666   0.66640002   0.70030004   0.65630001   0.68610001
    2.63136292]
 [256.91166426   0.1031       0.46779999   0.27850002   0.46930003
    2.11443114]
 [ 17.95414069   0.22519998   0.2225       0.2335       0.30520001
    1.43600023]
 ...
 [ 18.20098667   0.0578       0.1          0.0856       0.0971
    1.80328786]
 [  6.56041596   0.16270001   0.27669999   0.19939999   0.29859999
    2.1378727 ]
 [  3.11937658   0.48129997   0.57820004   0.47820002   0.53190005
    2.53442788]][0m
[37m[1m[2023-07-11 10:44:41,893][233954] Max Reward on eval: 769.9057006833143[0m
[37m[1m[2023-07-11 10:44:41,894][233954] Min Reward on eval: -73.54354862980544[0m
[37m[1m[2023-07-11 10:44:41,894][233954] Mean Reward across all agents: 74.62658501767696[0m
[37m[1m[2023-07-11 10:44:41,894][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:44:41,896][233954] mean_value=-397.30428302638205, max_value=556.8430518944202[0m
[37m[1m[2023-07-11 10:44:41,899][233954] New mean coefficients: [[ 0.91671306  0.6043673   0.67324686  1.3776416   0.7248244  -1.4207637 ]][0m
[37m[1m[2023-07-11 10:44:41,900][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:44:50,978][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 10:44:50,978][233954] FPS: 423081.98[0m
[36m[2023-07-11 10:44:50,980][233954] itr=797, itrs=2000, Progress: 39.85%[0m
[36m[2023-07-11 10:45:02,802][233954] train() took 11.73 seconds to complete[0m
[36m[2023-07-11 10:45:02,803][233954] FPS: 327309.01[0m
[36m[2023-07-11 10:45:07,100][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:45:07,101][233954] Reward + Measures: [[24.80518981  0.11684033  0.18101534  0.23973732  0.18928599  1.05757594]][0m
[37m[1m[2023-07-11 10:45:07,101][233954] Max Reward on eval: 24.80518980861566[0m
[37m[1m[2023-07-11 10:45:07,101][233954] Min Reward on eval: 24.80518980861566[0m
[37m[1m[2023-07-11 10:45:07,102][233954] Mean Reward across all agents: 24.80518980861566[0m
[37m[1m[2023-07-11 10:45:07,102][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:45:12,074][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:45:12,075][233954] Reward + Measures: [[ 17.54988352   0.031        0.0892       0.19509999   0.2228
    1.11451232]
 [203.11119187   0.33320004   0.87340003   0.52759999   0.87140006
    2.22636247]
 [261.90220209   0.25390002   0.90090001   0.65630001   0.81569999
    2.2446382 ]
 ...
 [327.37825536   0.0709       0.85909998   0.55010003   0.83150005
    2.29022169]
 [441.48108863   0.1512       0.93310004   0.69049996   0.94400007
    2.46486163]
 [ 16.72147638   0.477        0.85710001   0.63420004   0.74040002
    2.15447807]][0m
[37m[1m[2023-07-11 10:45:12,075][233954] Max Reward on eval: 555.946363230003[0m
[37m[1m[2023-07-11 10:45:12,075][233954] Min Reward on eval: -94.92804257590323[0m
[37m[1m[2023-07-11 10:45:12,075][233954] Mean Reward across all agents: 117.3988898760191[0m
[37m[1m[2023-07-11 10:45:12,076][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:45:12,080][233954] mean_value=-268.1482874434016, max_value=621.0631155083228[0m
[37m[1m[2023-07-11 10:45:12,082][233954] New mean coefficients: [[ 0.81817496  0.6096071   0.70699316  1.4265527   0.7704237  -1.5684899 ]][0m
[37m[1m[2023-07-11 10:45:12,083][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:45:21,004][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 10:45:21,004][233954] FPS: 430545.39[0m
[36m[2023-07-11 10:45:21,007][233954] itr=798, itrs=2000, Progress: 39.90%[0m
[36m[2023-07-11 10:45:32,698][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 10:45:32,698][233954] FPS: 331052.52[0m
[36m[2023-07-11 10:45:36,914][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:45:36,914][233954] Reward + Measures: [[24.06783093  0.13577533  0.21168634  0.27354768  0.21034366  1.02998567]][0m
[37m[1m[2023-07-11 10:45:36,915][233954] Max Reward on eval: 24.06783093448096[0m
[37m[1m[2023-07-11 10:45:36,915][233954] Min Reward on eval: 24.06783093448096[0m
[37m[1m[2023-07-11 10:45:36,915][233954] Mean Reward across all agents: 24.06783093448096[0m
[37m[1m[2023-07-11 10:45:36,915][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:45:41,879][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:45:41,880][233954] Reward + Measures: [[ 23.27342754   0.48129997   0.65420002   0.45549998   0.60980004
    1.78663337]
 [-28.98102474   0.64230007   0.79090005   0.59380001   0.57050002
    2.12197948]
 [ 41.33342411   0.23050001   0.42340001   0.19579999   0.31430003
    1.39361155]
 ...
 [ 42.6450713    0.41850004   0.56940001   0.36160001   0.54170007
    2.11901855]
 [ 32.31464691   0.49570003   0.6322       0.46830001   0.5733
    1.5962733 ]
 [ 37.26146828   0.8028       0.7906       0.79479998   0.67160004
    2.08887482]][0m
[37m[1m[2023-07-11 10:45:41,880][233954] Max Reward on eval: 554.5379180906341[0m
[37m[1m[2023-07-11 10:45:41,880][233954] Min Reward on eval: -80.54680441883393[0m
[37m[1m[2023-07-11 10:45:41,880][233954] Mean Reward across all agents: 54.381305956785056[0m
[37m[1m[2023-07-11 10:45:41,880][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:45:41,885][233954] mean_value=-202.81335646077778, max_value=670.5895046417602[0m
[37m[1m[2023-07-11 10:45:41,888][233954] New mean coefficients: [[ 1.0048659   1.137302   -0.11027104  1.3378459   1.2104437  -1.5624429 ]][0m
[37m[1m[2023-07-11 10:45:41,889][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:45:50,888][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 10:45:50,888][233954] FPS: 426802.47[0m
[36m[2023-07-11 10:45:50,891][233954] itr=799, itrs=2000, Progress: 39.95%[0m
[36m[2023-07-11 10:46:02,461][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 10:46:02,461][233954] FPS: 334516.98[0m
[36m[2023-07-11 10:46:06,708][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:46:06,709][233954] Reward + Measures: [[27.29613838  0.14399199  0.217712    0.282947    0.22103332  1.00710809]][0m
[37m[1m[2023-07-11 10:46:06,709][233954] Max Reward on eval: 27.296138384486632[0m
[37m[1m[2023-07-11 10:46:06,709][233954] Min Reward on eval: 27.296138384486632[0m
[37m[1m[2023-07-11 10:46:06,709][233954] Mean Reward across all agents: 27.296138384486632[0m
[37m[1m[2023-07-11 10:46:06,710][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:46:11,916][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:46:11,923][233954] Reward + Measures: [[213.56735803   0.2017       0.94729996   0.46430001   0.89609998
    2.35510373]
 [ 41.70794577   0.25740001   0.43450004   0.38800001   0.48300001
    1.7822026 ]
 [-66.44882945   0.3802       0.67469996   0.20080002   0.56400007
    1.7216233 ]
 ...
 [ 24.80986904   0.7665       0.79960006   0.73010004   0.75190002
    2.34948325]
 [ -5.03188875   0.31609997   0.82299995   0.53000003   0.68920004
    2.12893462]
 [-17.58724822   0.70320004   0.63549995   0.67830002   0.28190002
    1.90731609]][0m
[37m[1m[2023-07-11 10:46:11,924][233954] Max Reward on eval: 423.64213944328947[0m
[37m[1m[2023-07-11 10:46:11,924][233954] Min Reward on eval: -130.26004166686909[0m
[37m[1m[2023-07-11 10:46:11,925][233954] Mean Reward across all agents: 46.527759791487675[0m
[37m[1m[2023-07-11 10:46:11,925][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:46:11,935][233954] mean_value=-118.5274610608388, max_value=501.6547664437443[0m
[37m[1m[2023-07-11 10:46:11,939][233954] New mean coefficients: [[ 0.8241924  1.216438  -0.3041129  1.4261184  1.3472576 -1.5542845]][0m
[37m[1m[2023-07-11 10:46:11,940][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:46:20,878][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 10:46:20,878][233954] FPS: 429749.62[0m
[36m[2023-07-11 10:46:20,880][233954] itr=800, itrs=2000, Progress: 40.00%[0m
[37m[1m[2023-07-11 10:49:46,169][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000780[0m
[36m[2023-07-11 10:49:58,236][233954] train() took 11.43 seconds to complete[0m
[36m[2023-07-11 10:49:58,236][233954] FPS: 336056.33[0m
[36m[2023-07-11 10:50:02,331][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:50:02,331][233954] Reward + Measures: [[27.49298315  0.16700399  0.24121834  0.30199799  0.24992199  1.01696062]][0m
[37m[1m[2023-07-11 10:50:02,331][233954] Max Reward on eval: 27.492983152495455[0m
[37m[1m[2023-07-11 10:50:02,332][233954] Min Reward on eval: 27.492983152495455[0m
[37m[1m[2023-07-11 10:50:02,332][233954] Mean Reward across all agents: 27.492983152495455[0m
[37m[1m[2023-07-11 10:50:02,332][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:50:07,287][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:50:07,287][233954] Reward + Measures: [[350.27190024   0.033        0.80310005   0.5851       0.77960002
    2.37523723]
 [ 65.82690167   0.16610001   0.27920002   0.1417       0.32030001
    2.07318854]
 [174.09346582   0.17639999   0.46560001   0.37470001   0.50500005
    2.04992199]
 ...
 [212.02815978   0.28929999   0.87080002   0.57910007   0.89750004
    2.33460617]
 [308.50428106   0.12329999   0.93090004   0.60969996   0.92819995
    2.32972431]
 [531.21154307   0.0166       0.83579999   0.60580009   0.87120008
    2.48080397]][0m
[37m[1m[2023-07-11 10:50:07,288][233954] Max Reward on eval: 666.6987876733765[0m
[37m[1m[2023-07-11 10:50:07,288][233954] Min Reward on eval: -45.06498576579615[0m
[37m[1m[2023-07-11 10:50:07,288][233954] Mean Reward across all agents: 168.37698078516095[0m
[37m[1m[2023-07-11 10:50:07,288][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:50:07,291][233954] mean_value=-252.8377340104523, max_value=195.26294639682618[0m
[37m[1m[2023-07-11 10:50:07,294][233954] New mean coefficients: [[ 1.0923026   1.528392   -0.40230817  1.7677904   1.5604234  -1.4451501 ]][0m
[37m[1m[2023-07-11 10:50:07,295][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:50:16,356][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 10:50:16,356][233954] FPS: 423893.94[0m
[36m[2023-07-11 10:50:16,359][233954] itr=801, itrs=2000, Progress: 40.05%[0m
[36m[2023-07-11 10:50:28,335][233954] train() took 11.88 seconds to complete[0m
[36m[2023-07-11 10:50:28,335][233954] FPS: 323111.49[0m
[36m[2023-07-11 10:50:32,662][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:50:32,663][233954] Reward + Measures: [[26.83143656  0.17201933  0.23722701  0.30569068  0.25637832  0.99089479]][0m
[37m[1m[2023-07-11 10:50:32,663][233954] Max Reward on eval: 26.831436555397158[0m
[37m[1m[2023-07-11 10:50:32,663][233954] Min Reward on eval: 26.831436555397158[0m
[37m[1m[2023-07-11 10:50:32,664][233954] Mean Reward across all agents: 26.831436555397158[0m
[37m[1m[2023-07-11 10:50:32,664][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:50:37,652][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:50:37,653][233954] Reward + Measures: [[ 18.15686634   0.75100005   0.86499995   0.72189999   0.81890005
    2.13383985]
 [ 31.71471437   0.72210002   0.87760001   0.66829997   0.81910002
    2.27167988]
 [-29.09959487   0.7511       0.8757       0.73500001   0.84020007
    2.24609756]
 ...
 [ 54.05719885   0.56169999   0.58499998   0.57610005   0.59400004
    2.82765007]
 [378.08103085   0.10520001   0.79100001   0.61079997   0.81160003
    1.87101066]
 [ 37.18849106   0.68720001   0.77030003   0.64400005   0.74509996
    2.25605655]][0m
[37m[1m[2023-07-11 10:50:37,653][233954] Max Reward on eval: 378.0810308474116[0m
[37m[1m[2023-07-11 10:50:37,653][233954] Min Reward on eval: -91.02165072530042[0m
[37m[1m[2023-07-11 10:50:37,654][233954] Mean Reward across all agents: 9.364980560757726[0m
[37m[1m[2023-07-11 10:50:37,654][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:50:37,657][233954] mean_value=-162.31102735547992, max_value=520.4316388225745[0m
[37m[1m[2023-07-11 10:50:37,659][233954] New mean coefficients: [[ 1.5509449   1.5571932  -0.07402167  1.5305748   1.6272585  -1.0068607 ]][0m
[37m[1m[2023-07-11 10:50:37,660][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:50:46,637][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 10:50:46,638][233954] FPS: 427838.62[0m
[36m[2023-07-11 10:50:46,640][233954] itr=802, itrs=2000, Progress: 40.10%[0m
[36m[2023-07-11 10:50:58,165][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 10:50:58,165][233954] FPS: 335717.91[0m
[36m[2023-07-11 10:51:02,474][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:51:02,475][233954] Reward + Measures: [[33.049338    0.21001831  0.29057634  0.33774766  0.29368067  1.0259788 ]][0m
[37m[1m[2023-07-11 10:51:02,475][233954] Max Reward on eval: 33.04933800481051[0m
[37m[1m[2023-07-11 10:51:02,475][233954] Min Reward on eval: 33.04933800481051[0m
[37m[1m[2023-07-11 10:51:02,476][233954] Mean Reward across all agents: 33.04933800481051[0m
[37m[1m[2023-07-11 10:51:02,476][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:51:07,413][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:51:07,414][233954] Reward + Measures: [[381.49219515   0.0054       0.99230003   0.722        0.98750001
    2.31060815]
 [380.1477184    0.1293       0.84709996   0.64840001   0.84009999
    3.05167747]
 [-12.3478123    0.58160001   0.55140001   0.6494       0.44150001
    2.09509254]
 ...
 [ -5.99289175   0.62690002   0.42420003   0.6778       0.44990006
    2.73220134]
 [ 61.51147028   0.2122       0.90990001   0.11310001   0.92129993
    2.02399898]
 [446.28794861   0.0032       0.98839998   0.71780002   0.98449993
    3.12272406]][0m
[37m[1m[2023-07-11 10:51:07,414][233954] Max Reward on eval: 507.6736106541008[0m
[37m[1m[2023-07-11 10:51:07,415][233954] Min Reward on eval: -126.23848725520074[0m
[37m[1m[2023-07-11 10:51:07,415][233954] Mean Reward across all agents: 157.2124602978828[0m
[37m[1m[2023-07-11 10:51:07,415][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:51:07,421][233954] mean_value=-81.3662689138747, max_value=492.0456495569553[0m
[37m[1m[2023-07-11 10:51:07,423][233954] New mean coefficients: [[ 1.4012401   1.0824792   0.21384943  1.6715561   1.2629335  -1.0844978 ]][0m
[37m[1m[2023-07-11 10:51:07,424][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:51:16,410][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 10:51:16,411][233954] FPS: 427408.06[0m
[36m[2023-07-11 10:51:16,413][233954] itr=803, itrs=2000, Progress: 40.15%[0m
[36m[2023-07-11 10:51:28,150][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 10:51:28,150][233954] FPS: 329644.02[0m
[36m[2023-07-11 10:51:32,477][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:51:32,478][233954] Reward + Measures: [[34.78268081  0.21680433  0.28559166  0.34393066  0.30508232  1.0227077 ]][0m
[37m[1m[2023-07-11 10:51:32,478][233954] Max Reward on eval: 34.78268080868769[0m
[37m[1m[2023-07-11 10:51:32,478][233954] Min Reward on eval: 34.78268080868769[0m
[37m[1m[2023-07-11 10:51:32,478][233954] Mean Reward across all agents: 34.78268080868769[0m
[37m[1m[2023-07-11 10:51:32,478][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:51:37,463][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:51:37,464][233954] Reward + Measures: [[ 60.78486158   0.49390003   0.70619994   0.41929999   0.67990005
    2.26232409]
 [-26.86521195   0.23310001   0.78000003   0.1402       0.76449996
    2.30949593]
 [-37.72499728   0.2527       0.84400004   0.1245       0.8222
    2.33871531]
 ...
 [ 25.15767158   0.20630001   0.3326       0.24720001   0.2994
    1.64146507]
 [-22.05510266   0.1991       0.80730003   0.1708       0.78710002
    2.40817618]
 [ 44.91355905   0.0819       0.14999999   0.0887       0.12810001
    1.38994908]][0m
[37m[1m[2023-07-11 10:51:37,464][233954] Max Reward on eval: 344.36588671207426[0m
[37m[1m[2023-07-11 10:51:37,464][233954] Min Reward on eval: -67.55676263375207[0m
[37m[1m[2023-07-11 10:51:37,464][233954] Mean Reward across all agents: 57.54045287367415[0m
[37m[1m[2023-07-11 10:51:37,465][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:51:37,467][233954] mean_value=-744.5776665907752, max_value=321.801116179439[0m
[37m[1m[2023-07-11 10:51:37,469][233954] New mean coefficients: [[ 1.5356715   1.2566569   0.21599346  1.3777041   1.2216206  -1.2421515 ]][0m
[37m[1m[2023-07-11 10:51:37,470][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:51:46,509][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 10:51:46,510][233954] FPS: 424876.85[0m
[36m[2023-07-11 10:51:46,512][233954] itr=804, itrs=2000, Progress: 40.20%[0m
[36m[2023-07-11 10:51:58,305][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 10:51:58,305][233954] FPS: 328176.79[0m
[36m[2023-07-11 10:52:02,717][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:52:02,717][233954] Reward + Measures: [[35.69874187  0.20989168  0.28631434  0.33825564  0.30036566  1.00191116]][0m
[37m[1m[2023-07-11 10:52:02,718][233954] Max Reward on eval: 35.69874187156198[0m
[37m[1m[2023-07-11 10:52:02,718][233954] Min Reward on eval: 35.69874187156198[0m
[37m[1m[2023-07-11 10:52:02,718][233954] Mean Reward across all agents: 35.69874187156198[0m
[37m[1m[2023-07-11 10:52:02,718][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:52:07,736][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:52:07,737][233954] Reward + Measures: [[75.02650703  0.19330001  0.8344      0.1682      0.81150001  2.27089548]
 [64.2675634   0.19760001  0.79449999  0.13810001  0.79720002  2.44431281]
 [25.13176644  0.33440003  0.95699996  0.0913      0.96420002  2.88820887]
 ...
 [64.00376531  0.21610001  0.94750005  0.1305      0.93620008  2.23193049]
 [26.42137786  0.54360002  0.7834      0.18960001  0.88930005  2.87566924]
 [-9.70809585  0.2651      0.67339998  0.2033      0.64210004  1.93752217]][0m
[37m[1m[2023-07-11 10:52:07,737][233954] Max Reward on eval: 270.63869067626075[0m
[37m[1m[2023-07-11 10:52:07,738][233954] Min Reward on eval: -106.1414301390294[0m
[37m[1m[2023-07-11 10:52:07,738][233954] Mean Reward across all agents: 51.50816660747481[0m
[37m[1m[2023-07-11 10:52:07,738][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:52:07,742][233954] mean_value=-189.51975096753674, max_value=466.5682638843451[0m
[37m[1m[2023-07-11 10:52:07,745][233954] New mean coefficients: [[ 1.7217131   1.5380709   0.00369212  1.5330989   1.6616106  -0.90906   ]][0m
[37m[1m[2023-07-11 10:52:07,746][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:52:16,840][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 10:52:16,840][233954] FPS: 422309.13[0m
[36m[2023-07-11 10:52:16,843][233954] itr=805, itrs=2000, Progress: 40.25%[0m
[36m[2023-07-11 10:52:28,489][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 10:52:28,489][233954] FPS: 332298.64[0m
[36m[2023-07-11 10:52:32,721][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:52:32,721][233954] Reward + Measures: [[36.89410983  0.23043266  0.30432498  0.35410631  0.32586366  0.99181926]][0m
[37m[1m[2023-07-11 10:52:32,722][233954] Max Reward on eval: 36.8941098347864[0m
[37m[1m[2023-07-11 10:52:32,722][233954] Min Reward on eval: 36.8941098347864[0m
[37m[1m[2023-07-11 10:52:32,722][233954] Mean Reward across all agents: 36.8941098347864[0m
[37m[1m[2023-07-11 10:52:32,723][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:52:37,780][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:52:37,781][233954] Reward + Measures: [[82.05780284  0.10680001  0.28480002  0.1384      0.24890001  2.34885645]
 [55.88619517  0.29100001  0.34560004  0.1347      0.3321      1.93057287]
 [91.71604035  0.30389997  0.26190001  0.24370001  0.38840005  1.93026257]
 ...
 [87.47781269  0.0497      0.1758      0.12590002  0.1864      1.79354632]
 [20.76655493  0.37270004  0.45090005  0.0568      0.43830004  2.11857033]
 [52.20078012  0.1992      0.454       0.1389      0.42940003  2.05678344]][0m
[37m[1m[2023-07-11 10:52:37,781][233954] Max Reward on eval: 323.7080403525964[0m
[37m[1m[2023-07-11 10:52:37,781][233954] Min Reward on eval: -195.87512354166248[0m
[37m[1m[2023-07-11 10:52:37,781][233954] Mean Reward across all agents: 81.08591121910808[0m
[37m[1m[2023-07-11 10:52:37,782][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:52:37,785][233954] mean_value=-287.88135986098484, max_value=435.63401464721596[0m
[37m[1m[2023-07-11 10:52:37,787][233954] New mean coefficients: [[ 1.5982077   1.1979246   0.20483533  1.5131762   1.4566956  -1.0915765 ]][0m
[37m[1m[2023-07-11 10:52:37,788][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:52:46,949][233954] train() took 9.16 seconds to complete[0m
[36m[2023-07-11 10:52:46,949][233954] FPS: 419237.66[0m
[36m[2023-07-11 10:52:46,952][233954] itr=806, itrs=2000, Progress: 40.30%[0m
[36m[2023-07-11 10:52:58,637][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 10:52:58,637][233954] FPS: 331224.61[0m
[36m[2023-07-11 10:53:02,994][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:53:02,994][233954] Reward + Measures: [[37.20401994  0.23315299  0.31138533  0.354307    0.33585566  0.96494061]][0m
[37m[1m[2023-07-11 10:53:02,995][233954] Max Reward on eval: 37.20401994258152[0m
[37m[1m[2023-07-11 10:53:02,995][233954] Min Reward on eval: 37.20401994258152[0m
[37m[1m[2023-07-11 10:53:02,995][233954] Mean Reward across all agents: 37.20401994258152[0m
[37m[1m[2023-07-11 10:53:02,995][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:53:08,088][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:53:08,088][233954] Reward + Measures: [[53.41555942  0.18920001  0.57170004  0.1367      0.57409996  2.38935256]
 [49.10023903  0.2974      0.6947      0.31009999  0.62330002  2.13198781]
 [32.46525142  0.11479999  0.3057      0.2177      0.26680002  1.01152849]
 ...
 [ 4.47985261  0.11490001  0.1831      0.19800001  0.15879999  1.52899992]
 [18.151537    0.038       0.09679999  0.0962      0.1109      1.32105386]
 [29.15347107  0.2904      0.33990002  0.54650003  0.30340001  0.96653217]][0m
[37m[1m[2023-07-11 10:53:08,089][233954] Max Reward on eval: 440.4064437404275[0m
[37m[1m[2023-07-11 10:53:08,089][233954] Min Reward on eval: -57.05914231706411[0m
[37m[1m[2023-07-11 10:53:08,089][233954] Mean Reward across all agents: 32.43583553618392[0m
[37m[1m[2023-07-11 10:53:08,089][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:53:08,091][233954] mean_value=-869.6231411248899, max_value=525.8311320822686[0m
[37m[1m[2023-07-11 10:53:08,094][233954] New mean coefficients: [[ 1.5573411   1.1481452   0.04131359  1.1204494   1.2354378  -1.130095  ]][0m
[37m[1m[2023-07-11 10:53:08,094][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:53:17,154][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 10:53:17,154][233954] FPS: 423955.44[0m
[36m[2023-07-11 10:53:17,156][233954] itr=807, itrs=2000, Progress: 40.35%[0m
[36m[2023-07-11 10:53:29,020][233954] train() took 11.77 seconds to complete[0m
[36m[2023-07-11 10:53:29,020][233954] FPS: 326148.57[0m
[36m[2023-07-11 10:53:33,349][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:53:33,349][233954] Reward + Measures: [[42.24236989  0.269665    0.32479101  0.34839663  0.38014701  0.96682882]][0m
[37m[1m[2023-07-11 10:53:33,349][233954] Max Reward on eval: 42.2423698882614[0m
[37m[1m[2023-07-11 10:53:33,350][233954] Min Reward on eval: 42.2423698882614[0m
[37m[1m[2023-07-11 10:53:33,350][233954] Mean Reward across all agents: 42.2423698882614[0m
[37m[1m[2023-07-11 10:53:33,350][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:53:38,606][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:53:38,607][233954] Reward + Measures: [[ 29.36946855   0.35209998   0.38659999   0.28369999   0.57260001
    2.17645478]
 [ 88.99557527   0.0772       0.30770001   0.15809999   0.3831
    2.30039334]
 [ 24.16877732   0.22830001   0.32539999   0.18770002   0.39750001
    1.75331247]
 ...
 [ 37.09992623   0.29480001   0.38910004   0.40710002   0.35570002
    1.3360194 ]
 [201.39315064   0.1224       0.4436       0.49319997   0.46440002
    1.50594711]
 [ -5.02541509   0.3865       0.6972       0.35960004   0.6631
    2.01180911]][0m
[37m[1m[2023-07-11 10:53:38,607][233954] Max Reward on eval: 724.266944879666[0m
[37m[1m[2023-07-11 10:53:38,608][233954] Min Reward on eval: -92.53886613647337[0m
[37m[1m[2023-07-11 10:53:38,608][233954] Mean Reward across all agents: 147.8501024674081[0m
[37m[1m[2023-07-11 10:53:38,608][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:53:38,613][233954] mean_value=-393.8440729822328, max_value=738.6316948172786[0m
[37m[1m[2023-07-11 10:53:38,615][233954] New mean coefficients: [[ 1.8758904   1.5564129   0.04037984  0.7606003   1.4405727  -0.8120941 ]][0m
[37m[1m[2023-07-11 10:53:38,616][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:53:47,595][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 10:53:47,596][233954] FPS: 427744.61[0m
[36m[2023-07-11 10:53:47,598][233954] itr=808, itrs=2000, Progress: 40.40%[0m
[36m[2023-07-11 10:53:59,133][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 10:53:59,133][233954] FPS: 335461.82[0m
[36m[2023-07-11 10:54:03,383][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:54:03,384][233954] Reward + Measures: [[42.94920872  0.289343    0.33787233  0.34807998  0.40393302  0.94569772]][0m
[37m[1m[2023-07-11 10:54:03,384][233954] Max Reward on eval: 42.94920871907351[0m
[37m[1m[2023-07-11 10:54:03,384][233954] Min Reward on eval: 42.94920871907351[0m
[37m[1m[2023-07-11 10:54:03,385][233954] Mean Reward across all agents: 42.94920871907351[0m
[37m[1m[2023-07-11 10:54:03,385][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:54:08,350][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:54:08,351][233954] Reward + Measures: [[ 83.84610553   0.28290001   0.2412       0.2915       0.3678
    1.99487424]
 [ 33.97103917   0.0653       0.1816       0.0947       0.10580001
    1.86404157]
 [ 79.04485878   0.09860001   0.48280001   0.1602       0.44580004
    2.26211643]
 ...
 [ 63.49369144   0.24529998   0.8132       0.12630001   0.81080002
    2.46808863]
 [232.63484011   0.22920001   0.57030004   0.29380003   0.55510002
    1.88676   ]
 [ 31.65063936   0.17889999   0.2185       0.1956       0.2357
    2.05295515]][0m
[37m[1m[2023-07-11 10:54:08,351][233954] Max Reward on eval: 353.7711307584774[0m
[37m[1m[2023-07-11 10:54:08,351][233954] Min Reward on eval: -118.93271755771711[0m
[37m[1m[2023-07-11 10:54:08,352][233954] Mean Reward across all agents: 102.82654661933016[0m
[37m[1m[2023-07-11 10:54:08,352][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:54:08,356][233954] mean_value=-510.31635749313546, max_value=727.5915916014393[0m
[37m[1m[2023-07-11 10:54:08,358][233954] New mean coefficients: [[ 1.7566572   0.9617277   0.04101704  0.2831128   1.2156302  -1.3337965 ]][0m
[37m[1m[2023-07-11 10:54:08,359][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:54:17,306][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 10:54:17,306][233954] FPS: 429292.67[0m
[36m[2023-07-11 10:54:17,308][233954] itr=809, itrs=2000, Progress: 40.45%[0m
[36m[2023-07-11 10:54:28,871][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 10:54:28,872][233954] FPS: 334655.91[0m
[36m[2023-07-11 10:54:33,201][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:54:33,201][233954] Reward + Measures: [[43.15220071  0.29778701  0.32425264  0.33002535  0.42160466  0.90909308]][0m
[37m[1m[2023-07-11 10:54:33,202][233954] Max Reward on eval: 43.15220070537082[0m
[37m[1m[2023-07-11 10:54:33,202][233954] Min Reward on eval: 43.15220070537082[0m
[37m[1m[2023-07-11 10:54:33,202][233954] Mean Reward across all agents: 43.15220070537082[0m
[37m[1m[2023-07-11 10:54:33,202][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:54:38,281][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:54:38,282][233954] Reward + Measures: [[252.29895592   0.1158       0.86269999   0.64099997   0.82089996
    2.29800177]
 [ 11.64728567   0.31329998   0.40739998   0.32909998   0.41739997
    1.22083163]
 [360.11558724   0.1001       0.96999997   0.59330004   0.93650001
    2.41488314]
 ...
 [554.82281233   0.1015       0.96630001   0.55520004   0.93550009
    2.5737617 ]
 [127.18971991   0.21330002   0.80529994   0.40159997   0.70810002
    1.94986057]
 [ 12.16534022   0.52279997   0.59260005   0.61379999   0.41780001
    2.28151059]][0m
[37m[1m[2023-07-11 10:54:38,282][233954] Max Reward on eval: 723.9575195334503[0m
[37m[1m[2023-07-11 10:54:38,282][233954] Min Reward on eval: -78.9925093466416[0m
[37m[1m[2023-07-11 10:54:38,282][233954] Mean Reward across all agents: 144.62328818518446[0m
[37m[1m[2023-07-11 10:54:38,283][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:54:38,287][233954] mean_value=-186.80051201007407, max_value=520.0208263480517[0m
[37m[1m[2023-07-11 10:54:38,290][233954] New mean coefficients: [[ 1.5490692   1.102663    0.04112977  0.71125954  1.3807557  -1.481025  ]][0m
[37m[1m[2023-07-11 10:54:38,291][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:54:47,307][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 10:54:47,308][233954] FPS: 425979.68[0m
[36m[2023-07-11 10:54:47,310][233954] itr=810, itrs=2000, Progress: 40.50%[0m
[37m[1m[2023-07-11 10:58:07,706][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000790[0m
[36m[2023-07-11 10:58:20,082][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 10:58:20,082][233954] FPS: 329696.17[0m
[36m[2023-07-11 10:58:24,361][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:58:24,362][233954] Reward + Measures: [[48.84553492  0.31173164  0.33842635  0.32604468  0.44252896  0.9058243 ]][0m
[37m[1m[2023-07-11 10:58:24,362][233954] Max Reward on eval: 48.84553492162246[0m
[37m[1m[2023-07-11 10:58:24,362][233954] Min Reward on eval: 48.84553492162246[0m
[37m[1m[2023-07-11 10:58:24,362][233954] Mean Reward across all agents: 48.84553492162246[0m
[37m[1m[2023-07-11 10:58:24,363][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:58:29,308][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:58:29,309][233954] Reward + Measures: [[-31.50532839   0.25479999   0.72310007   0.22049999   0.60650003
    1.7631979 ]
 [ 51.96433139   0.1829       0.56150001   0.2253       0.51109999
    1.96046388]
 [194.76390648   0.13430001   0.37290001   0.36289999   0.46609998
    2.23143435]
 ...
 [ 21.63284864   0.53780001   0.64000005   0.54360002   0.62519997
    1.21641195]
 [ 58.53488091   0.32960001   0.41599998   0.22430001   0.42129999
    1.65706539]
 [ 32.08973302   0.15480003   0.70679998   0.16630001   0.64910001
    2.08593726]][0m
[37m[1m[2023-07-11 10:58:29,309][233954] Max Reward on eval: 367.3846645301208[0m
[37m[1m[2023-07-11 10:58:29,309][233954] Min Reward on eval: -53.03333769800374[0m
[37m[1m[2023-07-11 10:58:29,310][233954] Mean Reward across all agents: 96.95868471791661[0m
[37m[1m[2023-07-11 10:58:29,310][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:58:29,314][233954] mean_value=-514.4011657976574, max_value=580.9171946802177[0m
[37m[1m[2023-07-11 10:58:29,317][233954] New mean coefficients: [[ 1.094491   0.4535842  0.2364881  0.6927536  1.0923818 -2.2169309]][0m
[37m[1m[2023-07-11 10:58:29,318][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:58:38,224][233954] train() took 8.90 seconds to complete[0m
[36m[2023-07-11 10:58:38,224][233954] FPS: 431250.38[0m
[36m[2023-07-11 10:58:38,226][233954] itr=811, itrs=2000, Progress: 40.55%[0m
[36m[2023-07-11 10:58:49,855][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 10:58:49,855][233954] FPS: 332804.01[0m
[36m[2023-07-11 10:58:54,108][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:58:54,109][233954] Reward + Measures: [[46.86342309  0.32386667  0.33930498  0.31390232  0.45838469  0.90031648]][0m
[37m[1m[2023-07-11 10:58:54,109][233954] Max Reward on eval: 46.86342308975039[0m
[37m[1m[2023-07-11 10:58:54,109][233954] Min Reward on eval: 46.86342308975039[0m
[37m[1m[2023-07-11 10:58:54,109][233954] Mean Reward across all agents: 46.86342308975039[0m
[37m[1m[2023-07-11 10:58:54,110][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:58:59,032][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:58:59,033][233954] Reward + Measures: [[421.59453891   0.0697       0.78770006   0.56059998   0.75940001
    2.57896399]
 [ 71.34278042   0.23179999   0.43610001   0.17390001   0.43249997
    1.47723222]
 [132.92812661   0.18550001   0.4962       0.18239999   0.52629995
    2.10762453]
 ...
 [122.91298175   0.2184       0.67020005   0.25940001   0.73800004
    1.96821344]
 [ 99.82564928   0.16330001   0.27200001   0.24360001   0.38260004
    2.1399343 ]
 [287.33117032   0.3996       0.67559999   0.2066       0.67210001
    1.86977375]][0m
[37m[1m[2023-07-11 10:58:59,033][233954] Max Reward on eval: 743.3782577316276[0m
[37m[1m[2023-07-11 10:58:59,033][233954] Min Reward on eval: -111.011935952981[0m
[37m[1m[2023-07-11 10:58:59,033][233954] Mean Reward across all agents: 184.4711325948261[0m
[37m[1m[2023-07-11 10:58:59,034][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:58:59,038][233954] mean_value=-174.63782176326583, max_value=557.1683920642245[0m
[37m[1m[2023-07-11 10:58:59,041][233954] New mean coefficients: [[ 0.84958833 -0.2575785   0.38142917  0.3996079   0.5175353  -2.6330838 ]][0m
[37m[1m[2023-07-11 10:58:59,042][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:59:08,008][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 10:59:08,014][233954] FPS: 428359.22[0m
[36m[2023-07-11 10:59:08,016][233954] itr=812, itrs=2000, Progress: 40.60%[0m
[36m[2023-07-11 10:59:19,598][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 10:59:19,599][233954] FPS: 334091.68[0m
[36m[2023-07-11 10:59:23,824][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:59:23,824][233954] Reward + Measures: [[47.38957093  0.33635601  0.35227132  0.32187033  0.48015401  0.88295525]][0m
[37m[1m[2023-07-11 10:59:23,824][233954] Max Reward on eval: 47.389570930381225[0m
[37m[1m[2023-07-11 10:59:23,825][233954] Min Reward on eval: 47.389570930381225[0m
[37m[1m[2023-07-11 10:59:23,825][233954] Mean Reward across all agents: 47.389570930381225[0m
[37m[1m[2023-07-11 10:59:23,825][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:59:28,791][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:59:28,792][233954] Reward + Measures: [[  0.69738809   0.84770006   0.86750001   0.82839996   0.87150002
    2.79767346]
 [-10.43045443   0.76660007   0.80030006   0.7198       0.80629998
    2.37296414]
 [124.69624731   0.27930003   0.44109997   0.20829999   0.3299
    1.37180495]
 ...
 [ 17.2693376    0.82099992   0.8969       0.79939997   0.86799997
    2.09787965]
 [ 62.94798404   0.3179       0.18300001   0.3136       0.37179998
    2.00119257]
 [ 13.67773093   0.63710004   0.80719995   0.60080004   0.74329996
    2.05540395]][0m
[37m[1m[2023-07-11 10:59:28,792][233954] Max Reward on eval: 411.92909482950347[0m
[37m[1m[2023-07-11 10:59:28,792][233954] Min Reward on eval: -178.06074363302906[0m
[37m[1m[2023-07-11 10:59:28,792][233954] Mean Reward across all agents: 40.098431814475575[0m
[37m[1m[2023-07-11 10:59:28,792][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:59:28,797][233954] mean_value=-203.54149599430392, max_value=548.5177213618101[0m
[37m[1m[2023-07-11 10:59:28,800][233954] New mean coefficients: [[ 0.5505863  -0.4589653  -0.00622663  0.49743214  0.19957492 -2.8388479 ]][0m
[37m[1m[2023-07-11 10:59:28,801][233954] Moving the mean solution point...[0m
[36m[2023-07-11 10:59:37,807][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 10:59:37,807][233954] FPS: 426445.59[0m
[36m[2023-07-11 10:59:37,810][233954] itr=813, itrs=2000, Progress: 40.65%[0m
[36m[2023-07-11 10:59:49,350][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 10:59:49,350][233954] FPS: 335409.18[0m
[36m[2023-07-11 10:59:53,586][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:59:53,586][233954] Reward + Measures: [[42.12873015  0.30860603  0.324853    0.31586632  0.457149    0.83426398]][0m
[37m[1m[2023-07-11 10:59:53,587][233954] Max Reward on eval: 42.1287301501352[0m
[37m[1m[2023-07-11 10:59:53,587][233954] Min Reward on eval: 42.1287301501352[0m
[37m[1m[2023-07-11 10:59:53,587][233954] Mean Reward across all agents: 42.1287301501352[0m
[37m[1m[2023-07-11 10:59:53,587][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:59:58,527][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 10:59:58,528][233954] Reward + Measures: [[132.02514075   0.1462       0.55059999   0.27869999   0.56660002
    1.65567994]
 [120.33458426   0.29220003   0.31160003   0.0859       0.414
    0.94543833]
 [107.15956118   0.28659999   0.45970002   0.3224       0.45570001
    1.54377067]
 ...
 [304.94673559   0.2016       0.53260005   0.47890002   0.73320001
    1.69703984]
 [146.4845855    0.1903       0.69310004   0.41580001   0.71670002
    2.04488158]
 [  1.69982531   0.27700004   0.22810002   0.27320001   0.48359999
    1.14647341]][0m
[37m[1m[2023-07-11 10:59:58,528][233954] Max Reward on eval: 512.4839668482542[0m
[37m[1m[2023-07-11 10:59:58,528][233954] Min Reward on eval: -63.81749574122951[0m
[37m[1m[2023-07-11 10:59:58,528][233954] Mean Reward across all agents: 159.97113406160653[0m
[37m[1m[2023-07-11 10:59:58,529][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 10:59:58,536][233954] mean_value=-243.0763260283904, max_value=755.5034158948529[0m
[37m[1m[2023-07-11 10:59:58,539][233954] New mean coefficients: [[ 0.33519554 -0.82862526  0.17444316  0.38169774  0.01847647 -2.8277671 ]][0m
[37m[1m[2023-07-11 10:59:58,540][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:00:07,497][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 11:00:07,498][233954] FPS: 428780.97[0m
[36m[2023-07-11 11:00:07,500][233954] itr=814, itrs=2000, Progress: 40.70%[0m
[36m[2023-07-11 11:00:19,035][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 11:00:19,035][233954] FPS: 335503.15[0m
[36m[2023-07-11 11:00:23,283][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:00:23,284][233954] Reward + Measures: [[41.76467757  0.31070134  0.316618    0.31294301  0.46329501  0.8257255 ]][0m
[37m[1m[2023-07-11 11:00:23,284][233954] Max Reward on eval: 41.76467756590484[0m
[37m[1m[2023-07-11 11:00:23,284][233954] Min Reward on eval: 41.76467756590484[0m
[37m[1m[2023-07-11 11:00:23,285][233954] Mean Reward across all agents: 41.76467756590484[0m
[37m[1m[2023-07-11 11:00:23,285][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:00:28,364][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:00:28,365][233954] Reward + Measures: [[  99.93600538    0.2502        0.2739        0.1274        0.47329998
     0.88909817]
 [ 121.0347738     0.0832        0.24249999    0.16580001    0.3506
     1.88802373]
 [ -38.07160921    0.29629999    0.85319996    0.0799        0.85339993
     1.98853326]
 ...
 [-103.3693943     0.20999999    0.85719997    0.278         0.92770004
     2.88865662]
 [ 172.72870894    0.3441        0.41769996    0.44120002    0.67110008
     1.52610302]
 [  50.82432486    0.54649997    0.43409997    0.48639998    0.61230004
     1.23655343]][0m
[37m[1m[2023-07-11 11:00:28,365][233954] Max Reward on eval: 430.37126304291303[0m
[37m[1m[2023-07-11 11:00:28,366][233954] Min Reward on eval: -268.55913702498657[0m
[37m[1m[2023-07-11 11:00:28,366][233954] Mean Reward across all agents: 80.07767358151968[0m
[37m[1m[2023-07-11 11:00:28,366][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:00:28,374][233954] mean_value=-164.89892053701936, max_value=758.3944119165419[0m
[37m[1m[2023-07-11 11:00:28,377][233954] New mean coefficients: [[ 0.09055406 -1.2403814   0.13466066  0.17373855 -0.1886334  -3.0554347 ]][0m
[37m[1m[2023-07-11 11:00:28,378][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:00:37,470][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 11:00:37,470][233954] FPS: 422398.69[0m
[36m[2023-07-11 11:00:37,473][233954] itr=815, itrs=2000, Progress: 40.75%[0m
[36m[2023-07-11 11:00:49,363][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 11:00:49,364][233954] FPS: 332164.32[0m
[36m[2023-07-11 11:00:53,677][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:00:53,678][233954] Reward + Measures: [[41.34855116  0.33367065  0.34708935  0.31780633  0.48901165  0.83100778]][0m
[37m[1m[2023-07-11 11:00:53,678][233954] Max Reward on eval: 41.34855116302162[0m
[37m[1m[2023-07-11 11:00:53,678][233954] Min Reward on eval: 41.34855116302162[0m
[37m[1m[2023-07-11 11:00:53,678][233954] Mean Reward across all agents: 41.34855116302162[0m
[37m[1m[2023-07-11 11:00:53,679][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:00:58,683][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:00:58,684][233954] Reward + Measures: [[  1.10328354   0.23959999   0.37330005   0.1151       0.3497
    1.56573522]
 [ 32.00982548   0.32519999   0.54490006   0.39480001   0.5539
    1.35697198]
 [ 40.85991764   0.73089999   0.7956       0.70749998   0.7834
    1.93923843]
 ...
 [-11.77991459   0.7141       0.74500006   0.70270008   0.73469996
    1.96085441]
 [ 34.77183701   0.66420013   0.86390001   0.64700001   0.75380003
    2.12247992]
 [  9.34126012   0.21870001   0.34300002   0.17769998   0.36200002
    1.97670364]][0m
[37m[1m[2023-07-11 11:00:58,684][233954] Max Reward on eval: 388.62928385259585[0m
[37m[1m[2023-07-11 11:00:58,684][233954] Min Reward on eval: -97.94163364589913[0m
[37m[1m[2023-07-11 11:00:58,685][233954] Mean Reward across all agents: 32.058929150713375[0m
[37m[1m[2023-07-11 11:00:58,685][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:00:58,688][233954] mean_value=-400.53098632114717, max_value=486.3190988917556[0m
[37m[1m[2023-07-11 11:00:58,691][233954] New mean coefficients: [[ 0.16020003 -0.9260076   0.07890388 -0.3215016  -0.03424904 -2.934003  ]][0m
[37m[1m[2023-07-11 11:00:58,692][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:01:07,647][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 11:01:07,647][233954] FPS: 428919.74[0m
[36m[2023-07-11 11:01:07,649][233954] itr=816, itrs=2000, Progress: 40.80%[0m
[36m[2023-07-11 11:01:19,158][233954] train() took 11.42 seconds to complete[0m
[36m[2023-07-11 11:01:19,158][233954] FPS: 336271.46[0m
[36m[2023-07-11 11:01:23,391][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:01:23,391][233954] Reward + Measures: [[37.07413963  0.32432801  0.345644    0.32748768  0.47874466  0.798042  ]][0m
[37m[1m[2023-07-11 11:01:23,392][233954] Max Reward on eval: 37.07413963156355[0m
[37m[1m[2023-07-11 11:01:23,392][233954] Min Reward on eval: 37.07413963156355[0m
[37m[1m[2023-07-11 11:01:23,392][233954] Mean Reward across all agents: 37.07413963156355[0m
[37m[1m[2023-07-11 11:01:23,392][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:01:28,349][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:01:28,350][233954] Reward + Measures: [[156.69441004   0.17390001   0.25480002   0.12340001   0.28330001
    1.47231805]
 [ 20.22627207   0.26890001   0.51730001   0.50520003   0.31720001
    1.03540099]
 [ 56.53683005   0.2158       0.36390001   0.2086       0.39980003
    1.18182755]
 ...
 [ 69.52210023   0.05700001   0.30180001   0.21139999   0.19419999
    1.36756897]
 [ 44.47236447   0.51570004   0.61129999   0.09410001   0.63650006
    1.89575386]
 [ 60.19532998   0.116        0.1609       0.23169999   0.1682
    1.11196101]][0m
[37m[1m[2023-07-11 11:01:28,350][233954] Max Reward on eval: 461.1956155000022[0m
[37m[1m[2023-07-11 11:01:28,351][233954] Min Reward on eval: -70.71017022212035[0m
[37m[1m[2023-07-11 11:01:28,351][233954] Mean Reward across all agents: 75.72393695807285[0m
[37m[1m[2023-07-11 11:01:28,351][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:01:28,355][233954] mean_value=-1039.522899288896, max_value=534.3039132161997[0m
[37m[1m[2023-07-11 11:01:28,358][233954] New mean coefficients: [[ 0.3360895  -0.69484156  0.02402736 -0.40392077 -0.00304357 -2.916962  ]][0m
[37m[1m[2023-07-11 11:01:28,359][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:01:37,386][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 11:01:37,387][233954] FPS: 425449.60[0m
[36m[2023-07-11 11:01:37,389][233954] itr=817, itrs=2000, Progress: 40.85%[0m
[36m[2023-07-11 11:01:48,956][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 11:01:48,961][233954] FPS: 334546.58[0m
[36m[2023-07-11 11:01:53,193][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:01:53,193][233954] Reward + Measures: [[35.43395953  0.32547334  0.33074534  0.31283766  0.481875    0.78204125]][0m
[37m[1m[2023-07-11 11:01:53,194][233954] Max Reward on eval: 35.43395953190239[0m
[37m[1m[2023-07-11 11:01:53,194][233954] Min Reward on eval: 35.43395953190239[0m
[37m[1m[2023-07-11 11:01:53,194][233954] Mean Reward across all agents: 35.43395953190239[0m
[37m[1m[2023-07-11 11:01:53,194][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:01:58,209][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:01:58,210][233954] Reward + Measures: [[  21.38996033    0.6645        0.78229994    0.63730001    0.78439999
     1.81798768]
 [  21.03597218    0.2105        0.2985        0.29440004    0.2462
     1.31008005]
 [ 507.30778073    0.1177        0.92049998    0.71940005    0.95300007
     2.59764266]
 ...
 [-104.61039185    0.58700001    0.67220002    0.56269997    0.30579999
     1.94425857]
 [  46.59701283    0.39050001    0.62740004    0.20990001    0.60350001
     1.71448672]
 [  26.54496743    0.30649999    0.336         0.25700003    0.44329998
     1.3571769 ]][0m
[37m[1m[2023-07-11 11:01:58,210][233954] Max Reward on eval: 662.5519447327591[0m
[37m[1m[2023-07-11 11:01:58,210][233954] Min Reward on eval: -186.73320228264203[0m
[37m[1m[2023-07-11 11:01:58,211][233954] Mean Reward across all agents: 77.8822367149424[0m
[37m[1m[2023-07-11 11:01:58,211][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:01:58,216][233954] mean_value=-333.9402376621382, max_value=560.6468422804885[0m
[37m[1m[2023-07-11 11:01:58,218][233954] New mean coefficients: [[ 0.23316935 -0.2645809   0.1141737  -0.43798012  0.33445597 -2.4720967 ]][0m
[37m[1m[2023-07-11 11:01:58,219][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:02:07,258][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 11:02:07,258][233954] FPS: 424918.71[0m
[36m[2023-07-11 11:02:07,261][233954] itr=818, itrs=2000, Progress: 40.90%[0m
[36m[2023-07-11 11:02:19,116][233954] train() took 11.76 seconds to complete[0m
[36m[2023-07-11 11:02:19,116][233954] FPS: 326441.53[0m
[36m[2023-07-11 11:02:23,454][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:02:23,455][233954] Reward + Measures: [[31.89780378  0.29381233  0.29251134  0.28136966  0.46934533  0.75438607]][0m
[37m[1m[2023-07-11 11:02:23,455][233954] Max Reward on eval: 31.89780378115642[0m
[37m[1m[2023-07-11 11:02:23,455][233954] Min Reward on eval: 31.89780378115642[0m
[37m[1m[2023-07-11 11:02:23,456][233954] Mean Reward across all agents: 31.89780378115642[0m
[37m[1m[2023-07-11 11:02:23,456][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:02:28,536][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:02:28,537][233954] Reward + Measures: [[ 85.07018414   0.13829999   0.16620001   0.21139999   0.3242
    1.2825017 ]
 [ 18.40339651   0.1053       0.1904       0.0987       0.17639999
    1.24884236]
 [153.21686949   0.0983       0.77570003   0.30429998   0.73159999
    2.24433637]
 ...
 [135.87076424   0.15550001   0.63770002   0.33860001   0.68819994
    2.23324728]
 [310.21202518   0.0188       0.86380005   0.80979997   0.87210006
    2.34676743]
 [ 28.73695295   0.53280002   0.62260002   0.50640005   0.67009997
    0.96486706]][0m
[37m[1m[2023-07-11 11:02:28,537][233954] Max Reward on eval: 329.3963868692284[0m
[37m[1m[2023-07-11 11:02:28,537][233954] Min Reward on eval: -172.87823853176087[0m
[37m[1m[2023-07-11 11:02:28,538][233954] Mean Reward across all agents: 46.46851467698908[0m
[37m[1m[2023-07-11 11:02:28,538][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:02:28,541][233954] mean_value=-454.5805012906932, max_value=529.6669919109438[0m
[37m[1m[2023-07-11 11:02:28,544][233954] New mean coefficients: [[ 0.1448873  -0.3929324  -0.04369037 -0.06574726  0.13158605 -2.4365854 ]][0m
[37m[1m[2023-07-11 11:02:28,545][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:02:37,624][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 11:02:37,624][233954] FPS: 423049.91[0m
[36m[2023-07-11 11:02:37,626][233954] itr=819, itrs=2000, Progress: 40.95%[0m
[36m[2023-07-11 11:02:49,407][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 11:02:49,408][233954] FPS: 328402.45[0m
[36m[2023-07-11 11:02:53,674][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:02:53,675][233954] Reward + Measures: [[32.4741979   0.32093701  0.28833702  0.26997834  0.50853866  0.73918039]][0m
[37m[1m[2023-07-11 11:02:53,675][233954] Max Reward on eval: 32.47419789643107[0m
[37m[1m[2023-07-11 11:02:53,675][233954] Min Reward on eval: 32.47419789643107[0m
[37m[1m[2023-07-11 11:02:53,675][233954] Mean Reward across all agents: 32.47419789643107[0m
[37m[1m[2023-07-11 11:02:53,676][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:02:58,676][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:02:58,677][233954] Reward + Measures: [[ -10.88802624    0.20710002    0.23540001    0.1354        0.2692
     1.23468769]
 [ 114.87226791    0.28690001    0.61739999    0.26130003    0.60530001
     2.20957375]
 [-139.86444243    0.41209999    0.59329998    0.1006        0.61129999
     1.65818465]
 ...
 [  36.17739515    0.27239999    0.1586        0.22529998    0.42940003
     0.71481907]
 [ -57.27403162    0.4851        0.56590003    0.40279999    0.63070005
     1.39913499]
 [ -80.63031353    0.32259998    0.52319998    0.15400003    0.53590006
     1.70002007]][0m
[37m[1m[2023-07-11 11:02:58,677][233954] Max Reward on eval: 403.8019485326484[0m
[37m[1m[2023-07-11 11:02:58,677][233954] Min Reward on eval: -148.14765382092446[0m
[37m[1m[2023-07-11 11:02:58,678][233954] Mean Reward across all agents: 55.672086319386[0m
[37m[1m[2023-07-11 11:02:58,678][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:02:58,683][233954] mean_value=-452.5848561019504, max_value=482.3619531938834[0m
[37m[1m[2023-07-11 11:02:58,685][233954] New mean coefficients: [[ 0.33924705 -0.18019974  0.0071428  -0.44438893  0.30884695 -2.2728593 ]][0m
[37m[1m[2023-07-11 11:02:58,686][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:03:07,692][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 11:03:07,692][233954] FPS: 426464.94[0m
[36m[2023-07-11 11:03:07,695][233954] itr=820, itrs=2000, Progress: 41.00%[0m
[37m[1m[2023-07-11 11:06:35,225][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000800[0m
[36m[2023-07-11 11:06:47,555][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 11:06:47,555][233954] FPS: 330584.79[0m
[36m[2023-07-11 11:06:51,671][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:06:51,671][233954] Reward + Measures: [[32.64944971  0.30732998  0.27837867  0.25772867  0.50127637  0.7253086 ]][0m
[37m[1m[2023-07-11 11:06:51,671][233954] Max Reward on eval: 32.64944971454264[0m
[37m[1m[2023-07-11 11:06:51,672][233954] Min Reward on eval: 32.64944971454264[0m
[37m[1m[2023-07-11 11:06:51,672][233954] Mean Reward across all agents: 32.64944971454264[0m
[37m[1m[2023-07-11 11:06:51,672][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:06:56,856][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:06:56,856][233954] Reward + Measures: [[214.97719669   0.0848       0.94709998   0.56730002   0.88080007
    2.10812449]
 [ 61.94477931   0.34460002   0.33610001   0.25770003   0.47040001
    0.84389764]
 [ 63.10780554   0.14580001   0.18429999   0.22259998   0.32399997
    1.70899832]
 ...
 [ 26.26976926   0.61139995   0.73950005   0.40859994   0.76489997
    1.47430801]
 [115.12147675   0.24389999   0.57030004   0.3418       0.49559999
    1.4897722 ]
 [164.17790428   0.30669999   0.51910001   0.19530001   0.50599998
    2.12428975]][0m
[37m[1m[2023-07-11 11:06:56,856][233954] Max Reward on eval: 398.7331924246391[0m
[37m[1m[2023-07-11 11:06:56,857][233954] Min Reward on eval: -72.22058439113898[0m
[37m[1m[2023-07-11 11:06:56,857][233954] Mean Reward across all agents: 86.97357155374323[0m
[37m[1m[2023-07-11 11:06:56,857][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:06:56,863][233954] mean_value=-435.39417456685294, max_value=561.4385142655112[0m
[37m[1m[2023-07-11 11:06:56,865][233954] New mean coefficients: [[ 0.32424024 -0.01766162 -0.48616216 -0.10019052  0.55471957 -2.5866065 ]][0m
[37m[1m[2023-07-11 11:06:56,866][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:07:05,826][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 11:07:05,826][233954] FPS: 428677.38[0m
[36m[2023-07-11 11:07:05,828][233954] itr=821, itrs=2000, Progress: 41.05%[0m
[36m[2023-07-11 11:07:17,415][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 11:07:17,416][233954] FPS: 334104.56[0m
[36m[2023-07-11 11:07:21,635][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:07:21,636][233954] Reward + Measures: [[33.77942766  0.29495966  0.27444401  0.25614035  0.50236499  0.71329224]][0m
[37m[1m[2023-07-11 11:07:21,636][233954] Max Reward on eval: 33.77942765997459[0m
[37m[1m[2023-07-11 11:07:21,636][233954] Min Reward on eval: 33.77942765997459[0m
[37m[1m[2023-07-11 11:07:21,636][233954] Mean Reward across all agents: 33.77942765997459[0m
[37m[1m[2023-07-11 11:07:21,637][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:07:26,579][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:07:26,580][233954] Reward + Measures: [[-25.18202396   0.2999       0.3206       0.20130002   0.43579999
    0.78920716]
 [185.69260251   0.45959997   0.76870006   0.7076       0.86370009
    2.0053432 ]
 [ 24.24736682   0.24039999   0.26250002   0.28710002   0.28169999
    0.89173412]
 ...
 [-11.61420009   0.7748       0.88230002   0.73909998   0.86779994
    2.2119298 ]
 [ 97.95525966   0.44420001   0.58380002   0.33780003   0.54020005
    1.3117404 ]
 [-23.72974343   0.51840001   0.63230002   0.40630004   0.64950001
    1.19218242]][0m
[37m[1m[2023-07-11 11:07:26,580][233954] Max Reward on eval: 635.5329191845376[0m
[37m[1m[2023-07-11 11:07:26,580][233954] Min Reward on eval: -92.52115738987922[0m
[37m[1m[2023-07-11 11:07:26,580][233954] Mean Reward across all agents: 72.43633008335674[0m
[37m[1m[2023-07-11 11:07:26,581][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:07:26,586][233954] mean_value=-431.76908168739357, max_value=660.6485864124033[0m
[37m[1m[2023-07-11 11:07:26,588][233954] New mean coefficients: [[ 0.09673761 -0.04154766 -0.5128734  -0.09162717  0.5120621  -2.8243017 ]][0m
[37m[1m[2023-07-11 11:07:26,589][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:07:35,654][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 11:07:35,654][233954] FPS: 423716.26[0m
[36m[2023-07-11 11:07:35,656][233954] itr=822, itrs=2000, Progress: 41.10%[0m
[36m[2023-07-11 11:07:47,431][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 11:07:47,432][233954] FPS: 328605.88[0m
[36m[2023-07-11 11:07:51,668][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:07:51,673][233954] Reward + Measures: [[32.6210956   0.28659999  0.23610999  0.22629102  0.49530435  0.67388588]][0m
[37m[1m[2023-07-11 11:07:51,673][233954] Max Reward on eval: 32.62109559701272[0m
[37m[1m[2023-07-11 11:07:51,674][233954] Min Reward on eval: 32.62109559701272[0m
[37m[1m[2023-07-11 11:07:51,674][233954] Mean Reward across all agents: 32.62109559701272[0m
[37m[1m[2023-07-11 11:07:51,674][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:07:56,676][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:07:56,682][233954] Reward + Measures: [[39.15519118  0.1268      0.22389999  0.24850002  0.20030001  1.23396146]
 [-0.78981634  0.38390002  0.5248      0.34020001  0.51140004  1.65554011]
 [ 3.86772793  0.68260002  0.76179999  0.67620003  0.74310005  2.11281323]
 ...
 [ 9.26090364  0.21560001  0.33200002  0.2685      0.36180001  1.32546151]
 [59.40324858  0.31390002  0.1793      0.38949999  0.41389999  0.87842888]
 [20.06541922  0.2823      0.27339998  0.26840001  0.3355      1.68485832]][0m
[37m[1m[2023-07-11 11:07:56,682][233954] Max Reward on eval: 512.1189759828151[0m
[37m[1m[2023-07-11 11:07:56,683][233954] Min Reward on eval: -139.6953248599777[0m
[37m[1m[2023-07-11 11:07:56,683][233954] Mean Reward across all agents: 56.61264375007958[0m
[37m[1m[2023-07-11 11:07:56,683][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:07:56,688][233954] mean_value=-481.8523007830008, max_value=538.1965222701896[0m
[37m[1m[2023-07-11 11:07:56,691][233954] New mean coefficients: [[ 0.13653582 -0.5304129  -0.7731392  -0.2771918  -0.03824335 -3.325011  ]][0m
[37m[1m[2023-07-11 11:07:56,692][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:08:05,769][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 11:08:05,769][233954] FPS: 423137.00[0m
[36m[2023-07-11 11:08:05,772][233954] itr=823, itrs=2000, Progress: 41.15%[0m
[36m[2023-07-11 11:08:17,757][233954] train() took 11.89 seconds to complete[0m
[36m[2023-07-11 11:08:17,757][233954] FPS: 322899.84[0m
[36m[2023-07-11 11:08:22,040][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:08:22,040][233954] Reward + Measures: [[34.53532229  0.31155699  0.245849    0.22883898  0.49089566  0.66476762]][0m
[37m[1m[2023-07-11 11:08:22,040][233954] Max Reward on eval: 34.53532228603243[0m
[37m[1m[2023-07-11 11:08:22,041][233954] Min Reward on eval: 34.53532228603243[0m
[37m[1m[2023-07-11 11:08:22,041][233954] Mean Reward across all agents: 34.53532228603243[0m
[37m[1m[2023-07-11 11:08:22,041][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:08:27,017][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:08:27,017][233954] Reward + Measures: [[  95.96155686    0.11010001    0.58520001    0.1806        0.56639999
     1.73598826]
 [ 165.00653358    0.0744        0.45560002    0.2474        0.44160005
     1.9449271 ]
 [  11.01407022    0.17850001    0.36930001    0.28280002    0.39219999
     1.21860063]
 ...
 [  28.59511222    0.28940001    0.31570002    0.42069998    0.3946
     0.70183897]
 [-106.05966813    0.53459996    0.71619999    0.27710003    0.70010006
     1.56726742]
 [  92.53282311    0.68940002    0.67289996    0.64280003    0.69300002
     1.97486532]][0m
[37m[1m[2023-07-11 11:08:27,017][233954] Max Reward on eval: 609.5776338558644[0m
[37m[1m[2023-07-11 11:08:27,018][233954] Min Reward on eval: -107.58526182607166[0m
[37m[1m[2023-07-11 11:08:27,018][233954] Mean Reward across all agents: 50.92388710584323[0m
[37m[1m[2023-07-11 11:08:27,018][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:08:27,023][233954] mean_value=-461.83950628371724, max_value=719.8690203423612[0m
[37m[1m[2023-07-11 11:08:27,025][233954] New mean coefficients: [[-0.03776307 -0.76077706 -0.6307975  -0.02807388 -0.09284955 -3.2236135 ]][0m
[37m[1m[2023-07-11 11:08:27,026][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:08:36,025][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 11:08:36,025][233954] FPS: 426820.30[0m
[36m[2023-07-11 11:08:36,027][233954] itr=824, itrs=2000, Progress: 41.20%[0m
[36m[2023-07-11 11:08:47,561][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 11:08:47,561][233954] FPS: 335589.69[0m
[36m[2023-07-11 11:08:51,760][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:08:51,761][233954] Reward + Measures: [[32.01343117  0.28072301  0.22068001  0.21491398  0.4551363   0.63439459]][0m
[37m[1m[2023-07-11 11:08:51,761][233954] Max Reward on eval: 32.013431168638455[0m
[37m[1m[2023-07-11 11:08:51,761][233954] Min Reward on eval: 32.013431168638455[0m
[37m[1m[2023-07-11 11:08:51,762][233954] Mean Reward across all agents: 32.013431168638455[0m
[37m[1m[2023-07-11 11:08:51,762][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:08:56,764][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:08:56,770][233954] Reward + Measures: [[ 34.58655103   0.1323       0.26460001   0.10940001   0.3502
    1.53600252]
 [-13.9789051    0.32690001   0.37729999   0.1472       0.3757
    1.31554317]
 [ 13.65145649   0.40710002   0.54820007   0.36210001   0.53610003
    1.60381126]
 ...
 [-25.97912464   0.19500001   0.36110002   0.18470001   0.44070002
    1.63928413]
 [ 15.40548838   0.3057       0.25959998   0.22670002   0.35830003
    0.82491082]
 [ 38.75035538   0.25999999   0.24010001   0.2597       0.396
    1.03999555]][0m
[37m[1m[2023-07-11 11:08:56,771][233954] Max Reward on eval: 404.4764747590758[0m
[37m[1m[2023-07-11 11:08:56,771][233954] Min Reward on eval: -104.81219845544547[0m
[37m[1m[2023-07-11 11:08:56,772][233954] Mean Reward across all agents: 26.549682038590834[0m
[37m[1m[2023-07-11 11:08:56,772][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:08:56,782][233954] mean_value=-599.039736306731, max_value=556.7945733458643[0m
[37m[1m[2023-07-11 11:08:56,786][233954] New mean coefficients: [[-0.25329742 -0.5363202  -0.55959415 -0.2958223   0.01416248 -3.1039717 ]][0m
[37m[1m[2023-07-11 11:08:56,788][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:09:05,737][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 11:09:05,738][233954] FPS: 429163.42[0m
[36m[2023-07-11 11:09:05,740][233954] itr=825, itrs=2000, Progress: 41.25%[0m
[36m[2023-07-11 11:09:17,459][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 11:09:17,459][233954] FPS: 330198.06[0m
[36m[2023-07-11 11:09:21,723][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:09:21,728][233954] Reward + Measures: [[32.02219928  0.283672    0.21474099  0.20809999  0.46954402  0.6227901 ]][0m
[37m[1m[2023-07-11 11:09:21,729][233954] Max Reward on eval: 32.02219927948646[0m
[37m[1m[2023-07-11 11:09:21,729][233954] Min Reward on eval: 32.02219927948646[0m
[37m[1m[2023-07-11 11:09:21,729][233954] Mean Reward across all agents: 32.02219927948646[0m
[37m[1m[2023-07-11 11:09:21,730][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:09:26,729][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:09:26,735][233954] Reward + Measures: [[238.17594303   0.23340002   0.90219992   0.54300004   0.83689994
    2.18524361]
 [ 97.76139922   0.1921       0.54050004   0.23599999   0.55759996
    1.80313015]
 [-21.90859913   0.3028       0.48990002   0.25740001   0.50669998
    1.64409447]
 ...
 [-32.65786339   0.48719999   0.98629999   0.0686       0.98839998
    2.98141861]
 [-61.63730903   0.56950003   0.9781       0.0343       0.97290003
    3.10274386]
 [ 20.95855543   0.40330002   0.1489       0.2368       0.37799999
    1.17367685]][0m
[37m[1m[2023-07-11 11:09:26,735][233954] Max Reward on eval: 671.2418746712268[0m
[37m[1m[2023-07-11 11:09:26,736][233954] Min Reward on eval: -124.69670075895264[0m
[37m[1m[2023-07-11 11:09:26,736][233954] Mean Reward across all agents: 56.308810290596675[0m
[37m[1m[2023-07-11 11:09:26,736][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:09:26,741][233954] mean_value=-278.3569770049717, max_value=607.6594376308378[0m
[37m[1m[2023-07-11 11:09:26,744][233954] New mean coefficients: [[-0.30844283 -0.47486126 -0.2800383  -0.1262897  -0.05662809 -2.7568297 ]][0m
[37m[1m[2023-07-11 11:09:26,745][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:09:35,700][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 11:09:35,700][233954] FPS: 428922.51[0m
[36m[2023-07-11 11:09:35,702][233954] itr=826, itrs=2000, Progress: 41.30%[0m
[36m[2023-07-11 11:09:47,319][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 11:09:47,320][233954] FPS: 333139.48[0m
[36m[2023-07-11 11:09:51,609][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:09:51,609][233954] Reward + Measures: [[30.73491565  0.241909    0.18495765  0.18077967  0.44675097  0.58708227]][0m
[37m[1m[2023-07-11 11:09:51,610][233954] Max Reward on eval: 30.734915649615907[0m
[37m[1m[2023-07-11 11:09:51,610][233954] Min Reward on eval: 30.734915649615907[0m
[37m[1m[2023-07-11 11:09:51,610][233954] Mean Reward across all agents: 30.734915649615907[0m
[37m[1m[2023-07-11 11:09:51,610][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:09:56,841][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:09:56,846][233954] Reward + Measures: [[ 40.29307982   0.85789996   0.90210003   0.83470005   0.8761
    2.29522395]
 [319.06395495   0.0986       0.63400006   0.42389998   0.66020006
    1.68730831]
 [-55.11423187   0.93449992   0.94029999   0.90960008   0.93600005
    3.07918477]
 ...
 [ -4.00215542   0.91420001   0.92670006   0.89120007   0.92210001
    2.48162889]
 [ 33.34380576   0.08090001   0.0423       0.0609       0.24389999
    0.65460742]
 [ -8.61029179   0.29550001   0.40560004   0.21789999   0.45029998
    1.28233695]][0m
[37m[1m[2023-07-11 11:09:56,847][233954] Max Reward on eval: 464.26145553963727[0m
[37m[1m[2023-07-11 11:09:56,847][233954] Min Reward on eval: -167.37890054173766[0m
[37m[1m[2023-07-11 11:09:56,847][233954] Mean Reward across all agents: 39.81114067263237[0m
[37m[1m[2023-07-11 11:09:56,848][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:09:56,852][233954] mean_value=-272.94443395466277, max_value=830.2567182861269[0m
[37m[1m[2023-07-11 11:09:56,855][233954] New mean coefficients: [[-0.00235108  0.0233067  -0.27458438  0.07926041  0.25675225 -2.1492722 ]][0m
[37m[1m[2023-07-11 11:09:56,856][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:10:05,846][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 11:10:05,847][233954] FPS: 427202.72[0m
[36m[2023-07-11 11:10:05,849][233954] itr=827, itrs=2000, Progress: 41.35%[0m
[36m[2023-07-11 11:10:17,417][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 11:10:17,418][233954] FPS: 334486.04[0m
[36m[2023-07-11 11:10:21,660][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:10:21,660][233954] Reward + Measures: [[33.02754681  0.23579769  0.16959967  0.17045267  0.46768001  0.57012808]][0m
[37m[1m[2023-07-11 11:10:21,661][233954] Max Reward on eval: 33.02754680856928[0m
[37m[1m[2023-07-11 11:10:21,661][233954] Min Reward on eval: 33.02754680856928[0m
[37m[1m[2023-07-11 11:10:21,661][233954] Mean Reward across all agents: 33.02754680856928[0m
[37m[1m[2023-07-11 11:10:21,661][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:10:26,601][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:10:26,602][233954] Reward + Measures: [[ 80.08521139   0.08090001   0.1574       0.1041       0.15949999
    1.53259051]
 [114.79468754   0.59230006   0.55810004   0.19960001   0.66060001
    1.7073164 ]
 [321.88763103   0.0606       0.81309998   0.68160003   0.80470002
    2.14856672]
 ...
 [146.25755699   0.32730001   0.52580005   0.1868       0.54890007
    2.00328064]
 [ 27.90199749   0.11319999   0.2924       0.249        0.21580003
    0.80956143]
 [ 24.69301424   0.42319998   0.50019997   0.45370004   0.53780001
    1.27953076]][0m
[37m[1m[2023-07-11 11:10:26,602][233954] Max Reward on eval: 507.99129102793523[0m
[37m[1m[2023-07-11 11:10:26,602][233954] Min Reward on eval: -101.66631653034128[0m
[37m[1m[2023-07-11 11:10:26,602][233954] Mean Reward across all agents: 114.9318653850677[0m
[37m[1m[2023-07-11 11:10:26,603][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:10:26,608][233954] mean_value=-342.594447258981, max_value=638.5746991053456[0m
[37m[1m[2023-07-11 11:10:26,610][233954] New mean coefficients: [[-0.11069659 -0.17236942 -0.33379984 -0.00359992  0.11150986 -2.2322383 ]][0m
[37m[1m[2023-07-11 11:10:26,611][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:10:35,609][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 11:10:35,609][233954] FPS: 426862.16[0m
[36m[2023-07-11 11:10:35,611][233954] itr=828, itrs=2000, Progress: 41.40%[0m
[36m[2023-07-11 11:10:47,606][233954] train() took 11.90 seconds to complete[0m
[36m[2023-07-11 11:10:47,606][233954] FPS: 322573.71[0m
[36m[2023-07-11 11:10:51,978][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:10:51,979][233954] Reward + Measures: [[32.49845845  0.23238467  0.16541067  0.16693267  0.47046736  0.56064606]][0m
[37m[1m[2023-07-11 11:10:51,979][233954] Max Reward on eval: 32.49845844836776[0m
[37m[1m[2023-07-11 11:10:51,979][233954] Min Reward on eval: 32.49845844836776[0m
[37m[1m[2023-07-11 11:10:51,980][233954] Mean Reward across all agents: 32.49845844836776[0m
[37m[1m[2023-07-11 11:10:51,980][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:10:57,046][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:10:57,052][233954] Reward + Measures: [[ 36.67278348   0.19410001   0.08710001   0.0557       0.1552
    1.10147977]
 [-14.143937     0.35339999   0.35139999   0.26250002   0.47490001
    1.51356113]
 [422.69916033   0.15000001   0.66110003   0.45749998   0.67099994
    2.44332147]
 ...
 [  7.41424053   0.35350001   0.17559999   0.29719999   0.39430004
    1.49286497]
 [234.57883004   0.0805       0.70850003   0.46180001   0.68400002
    2.10715652]
 [ -3.05529796   0.42740002   0.42570001   0.43360001   0.4165
    1.98968351]][0m
[37m[1m[2023-07-11 11:10:57,053][233954] Max Reward on eval: 474.4452664664015[0m
[37m[1m[2023-07-11 11:10:57,053][233954] Min Reward on eval: -88.05962204458191[0m
[37m[1m[2023-07-11 11:10:57,053][233954] Mean Reward across all agents: 46.35735598506478[0m
[37m[1m[2023-07-11 11:10:57,053][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:10:57,059][233954] mean_value=-643.3796727939442, max_value=519.0009553826973[0m
[37m[1m[2023-07-11 11:10:57,061][233954] New mean coefficients: [[ 0.04841351 -0.1061592  -0.3299168   0.09498565  0.07923227 -2.0149245 ]][0m
[37m[1m[2023-07-11 11:10:57,062][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:11:06,213][233954] train() took 9.15 seconds to complete[0m
[36m[2023-07-11 11:11:06,213][233954] FPS: 419709.30[0m
[36m[2023-07-11 11:11:06,216][233954] itr=829, itrs=2000, Progress: 41.45%[0m
[36m[2023-07-11 11:11:17,995][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 11:11:17,995][233954] FPS: 328520.05[0m
[36m[2023-07-11 11:11:22,232][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:11:22,233][233954] Reward + Measures: [[31.56651831  0.25388733  0.16771266  0.167244    0.51294869  0.53519869]][0m
[37m[1m[2023-07-11 11:11:22,233][233954] Max Reward on eval: 31.566518312434578[0m
[37m[1m[2023-07-11 11:11:22,233][233954] Min Reward on eval: 31.566518312434578[0m
[37m[1m[2023-07-11 11:11:22,233][233954] Mean Reward across all agents: 31.566518312434578[0m
[37m[1m[2023-07-11 11:11:22,234][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:11:27,211][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:11:27,212][233954] Reward + Measures: [[ 52.70135249   0.1155       0.1751       0.12150001   0.26730001
    1.03755593]
 [ 91.39102296   0.12990001   0.0661       0.148        0.17040001
    1.01763725]
 [ 28.62119095   0.20580001   0.36480004   0.3008       0.36960003
    1.02227604]
 ...
 [ 84.72179316   0.198        0.1231       0.31380001   0.30880001
    0.87601823]
 [ 22.84022941   0.1816       0.14750001   0.1488       0.44280002
    0.4980064 ]
 [-21.16690379   0.3515       0.97410005   0.0545       0.98339999
    2.16118217]][0m
[37m[1m[2023-07-11 11:11:27,212][233954] Max Reward on eval: 449.9854015892255[0m
[37m[1m[2023-07-11 11:11:27,213][233954] Min Reward on eval: -141.6459885150194[0m
[37m[1m[2023-07-11 11:11:27,213][233954] Mean Reward across all agents: 51.03766759226835[0m
[37m[1m[2023-07-11 11:11:27,213][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:11:27,219][233954] mean_value=-548.1269003356523, max_value=535.6239089332288[0m
[37m[1m[2023-07-11 11:11:27,222][233954] New mean coefficients: [[ 0.13376068 -0.0100897  -0.215648    0.12230933  0.08929225 -1.8285042 ]][0m
[37m[1m[2023-07-11 11:11:27,223][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:11:36,176][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 11:11:36,176][233954] FPS: 428989.82[0m
[36m[2023-07-11 11:11:36,178][233954] itr=830, itrs=2000, Progress: 41.50%[0m
[37m[1m[2023-07-11 11:14:56,872][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000810[0m
[36m[2023-07-11 11:15:09,155][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 11:15:09,155][233954] FPS: 328514.57[0m
[36m[2023-07-11 11:15:13,398][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:15:13,398][233954] Reward + Measures: [[31.37855276  0.21931298  0.17400067  0.177918    0.51415032  0.51461953]][0m
[37m[1m[2023-07-11 11:15:13,399][233954] Max Reward on eval: 31.378552764544143[0m
[37m[1m[2023-07-11 11:15:13,399][233954] Min Reward on eval: 31.378552764544143[0m
[37m[1m[2023-07-11 11:15:13,399][233954] Mean Reward across all agents: 31.378552764544143[0m
[37m[1m[2023-07-11 11:15:13,399][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:15:18,386][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:15:18,387][233954] Reward + Measures: [[ 58.4294746    0.31990001   0.5068       0.25060001   0.6329
    1.73389363]
 [233.99270203   0.16360001   0.62790006   0.52790004   0.70090002
    1.70570242]
 [205.89006499   0.1556       0.70039999   0.42940003   0.71800005
    2.18729806]
 ...
 [318.82805253   0.0192       0.9939       0.7457       0.9896
    2.50615859]
 [ 42.84091223   0.29029998   0.24040003   0.15700001   0.34940001
    1.14067864]
 [ 51.5820588    0.18560003   0.45450002   0.1912       0.43850002
    1.35940766]][0m
[37m[1m[2023-07-11 11:15:18,387][233954] Max Reward on eval: 790.1111602768302[0m
[37m[1m[2023-07-11 11:15:18,387][233954] Min Reward on eval: -115.15143562527373[0m
[37m[1m[2023-07-11 11:15:18,387][233954] Mean Reward across all agents: 110.01948570973802[0m
[37m[1m[2023-07-11 11:15:18,388][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:15:18,394][233954] mean_value=-198.78912651248805, max_value=596.9249908715486[0m
[37m[1m[2023-07-11 11:15:18,396][233954] New mean coefficients: [[ 0.16584003 -0.02517987 -0.45182973 -0.02832481 -0.0055513  -2.1887274 ]][0m
[37m[1m[2023-07-11 11:15:18,397][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:15:27,323][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 11:15:27,323][233954] FPS: 430309.57[0m
[36m[2023-07-11 11:15:27,325][233954] itr=831, itrs=2000, Progress: 41.55%[0m
[36m[2023-07-11 11:15:38,943][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 11:15:38,943][233954] FPS: 333207.04[0m
[36m[2023-07-11 11:15:43,155][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:15:43,155][233954] Reward + Measures: [[29.76153447  0.21453768  0.17489702  0.180005    0.49802566  0.51083004]][0m
[37m[1m[2023-07-11 11:15:43,156][233954] Max Reward on eval: 29.76153447171026[0m
[37m[1m[2023-07-11 11:15:43,156][233954] Min Reward on eval: 29.76153447171026[0m
[37m[1m[2023-07-11 11:15:43,156][233954] Mean Reward across all agents: 29.76153447171026[0m
[37m[1m[2023-07-11 11:15:43,156][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:15:48,317][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:15:48,323][233954] Reward + Measures: [[33.77949287  0.186       0.46100003  0.22260001  0.38540003  1.42578411]
 [73.69680266  0.2877      0.17050001  0.34600002  0.44969997  1.14820373]
 [32.27959513  0.14850001  0.1391      0.14500001  0.30930001  0.63487899]
 ...
 [27.0312513   0.2631      0.2174      0.2122      0.51029998  0.61116666]
 [33.15640409  0.17639999  0.17920001  0.1701      0.2502      1.2572062 ]
 [-0.88728893  0.50510001  0.71240008  0.50220001  0.69460005  1.33214509]][0m
[37m[1m[2023-07-11 11:15:48,324][233954] Max Reward on eval: 382.56647542174903[0m
[37m[1m[2023-07-11 11:15:48,324][233954] Min Reward on eval: -210.6420431612758[0m
[37m[1m[2023-07-11 11:15:48,324][233954] Mean Reward across all agents: 38.69787142622782[0m
[37m[1m[2023-07-11 11:15:48,324][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:15:48,330][233954] mean_value=-422.2962837016842, max_value=507.5536912747028[0m
[37m[1m[2023-07-11 11:15:48,333][233954] New mean coefficients: [[ 0.14485852  0.04904567 -0.46682876  0.12886798  0.03866419 -1.8412976 ]][0m
[37m[1m[2023-07-11 11:15:48,334][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:15:57,366][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 11:15:57,367][233954] FPS: 425187.53[0m
[36m[2023-07-11 11:15:57,369][233954] itr=832, itrs=2000, Progress: 41.60%[0m
[36m[2023-07-11 11:16:09,127][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 11:16:09,128][233954] FPS: 329192.72[0m
[36m[2023-07-11 11:16:13,411][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:16:13,411][233954] Reward + Measures: [[33.15052053  0.231635    0.16611433  0.17743868  0.53355962  0.50145376]][0m
[37m[1m[2023-07-11 11:16:13,411][233954] Max Reward on eval: 33.15052052657769[0m
[37m[1m[2023-07-11 11:16:13,412][233954] Min Reward on eval: 33.15052052657769[0m
[37m[1m[2023-07-11 11:16:13,412][233954] Mean Reward across all agents: 33.15052052657769[0m
[37m[1m[2023-07-11 11:16:13,412][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:16:18,378][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:16:18,378][233954] Reward + Measures: [[ 23.38650149   0.1382       0.0964       0.11740001   0.17209999
    0.71694803]
 [ 13.89838057   0.0561       0.12019999   0.0945       0.182
    1.09040976]
 [ 28.61836242   0.0952       0.06640001   0.0669       0.32640001
    0.37556848]
 ...
 [175.85177004   0.0706       0.69650006   0.39229998   0.59910005
    1.80117917]
 [ 30.23421723   0.075        0.1026       0.0981       0.08639999
    1.16188383]
 [ 25.24475922   0.35190001   0.1645       0.24210003   0.37079999
    1.1279906 ]][0m
[37m[1m[2023-07-11 11:16:18,378][233954] Max Reward on eval: 553.2429783064872[0m
[37m[1m[2023-07-11 11:16:18,379][233954] Min Reward on eval: -120.85485593043268[0m
[37m[1m[2023-07-11 11:16:18,379][233954] Mean Reward across all agents: 33.197274377726096[0m
[37m[1m[2023-07-11 11:16:18,379][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:16:18,383][233954] mean_value=-699.7522733168196, max_value=424.03002018664404[0m
[37m[1m[2023-07-11 11:16:18,386][233954] New mean coefficients: [[ 0.01773712 -0.01374521 -0.43748814  0.1849224   0.05236447 -1.6895217 ]][0m
[37m[1m[2023-07-11 11:16:18,387][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:16:27,301][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 11:16:27,301][233954] FPS: 430848.88[0m
[36m[2023-07-11 11:16:27,304][233954] itr=833, itrs=2000, Progress: 41.65%[0m
[36m[2023-07-11 11:16:38,993][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 11:16:38,993][233954] FPS: 331065.40[0m
[36m[2023-07-11 11:16:43,255][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:16:43,255][233954] Reward + Measures: [[29.9406874   0.23612766  0.17050934  0.17579366  0.52731931  0.49082723]][0m
[37m[1m[2023-07-11 11:16:43,255][233954] Max Reward on eval: 29.940687396611082[0m
[37m[1m[2023-07-11 11:16:43,256][233954] Min Reward on eval: 29.940687396611082[0m
[37m[1m[2023-07-11 11:16:43,256][233954] Mean Reward across all agents: 29.940687396611082[0m
[37m[1m[2023-07-11 11:16:43,256][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:16:48,260][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:16:48,266][233954] Reward + Measures: [[-36.30419371   0.51030004   0.65990001   0.29440001   0.67699999
    1.37102878]
 [ 34.87020864   0.1745       0.16320001   0.19219999   0.31210002
    1.01313937]
 [ 13.85427189   0.49830005   0.31490001   0.40490004   0.56930006
    1.49734962]
 ...
 [ 23.92152536   0.2352       0.20999999   0.21630001   0.22379999
    0.87646294]
 [ 37.9904068    0.28200001   0.13109998   0.23169999   0.27599999
    0.55905837]
 [ 95.04533935   0.10030001   0.2588       0.3418       0.45029998
    0.82169354]][0m
[37m[1m[2023-07-11 11:16:48,266][233954] Max Reward on eval: 430.6144180533709[0m
[37m[1m[2023-07-11 11:16:48,267][233954] Min Reward on eval: -122.69099999549799[0m
[37m[1m[2023-07-11 11:16:48,267][233954] Mean Reward across all agents: 36.24972465814269[0m
[37m[1m[2023-07-11 11:16:48,267][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:16:48,273][233954] mean_value=-440.11446818441607, max_value=595.0453393483069[0m
[37m[1m[2023-07-11 11:16:48,276][233954] New mean coefficients: [[ 0.20918703 -0.19394243 -0.18684345  0.03805561  0.10536796 -1.4999503 ]][0m
[37m[1m[2023-07-11 11:16:48,277][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:16:57,369][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 11:16:57,369][233954] FPS: 422444.91[0m
[36m[2023-07-11 11:16:57,371][233954] itr=834, itrs=2000, Progress: 41.70%[0m
[36m[2023-07-11 11:17:09,135][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 11:17:09,135][233954] FPS: 328970.78[0m
[36m[2023-07-11 11:17:13,402][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:17:13,402][233954] Reward + Measures: [[31.69605374  0.24629034  0.16079867  0.17098966  0.52557331  0.48583236]][0m
[37m[1m[2023-07-11 11:17:13,403][233954] Max Reward on eval: 31.6960537394234[0m
[37m[1m[2023-07-11 11:17:13,403][233954] Min Reward on eval: 31.6960537394234[0m
[37m[1m[2023-07-11 11:17:13,403][233954] Mean Reward across all agents: 31.6960537394234[0m
[37m[1m[2023-07-11 11:17:13,403][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:17:18,392][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:17:18,393][233954] Reward + Measures: [[ 38.11889337   0.2155       0.034        0.1085       0.456
    0.40731403]
 [456.33138622   0.0351       0.97760004   0.73559999   0.94390005
    2.20220757]
 [ 33.78010713   0.22660001   0.31189999   0.08490001   0.36680004
    1.11357749]
 ...
 [ 20.03914005   0.34239998   0.38600001   0.2234       0.45720002
    0.89443457]
 [102.5813086    0.12550001   0.29160002   0.11830001   0.34510002
    1.56339991]
 [ 43.26879932   0.38980001   0.21290003   0.20480001   0.60000002
    0.71984911]][0m
[37m[1m[2023-07-11 11:17:18,393][233954] Max Reward on eval: 536.4534778504633[0m
[37m[1m[2023-07-11 11:17:18,393][233954] Min Reward on eval: -118.12537877266296[0m
[37m[1m[2023-07-11 11:17:18,394][233954] Mean Reward across all agents: 43.80505883318742[0m
[37m[1m[2023-07-11 11:17:18,394][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:17:18,400][233954] mean_value=-615.397694622175, max_value=536.599598624371[0m
[37m[1m[2023-07-11 11:17:18,402][233954] New mean coefficients: [[ 0.17978066 -0.12756148 -0.29360184  0.01689108  0.2197167  -1.0963519 ]][0m
[37m[1m[2023-07-11 11:17:18,403][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:17:27,322][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 11:17:27,322][233954] FPS: 430645.49[0m
[36m[2023-07-11 11:17:27,324][233954] itr=835, itrs=2000, Progress: 41.75%[0m
[36m[2023-07-11 11:17:39,056][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 11:17:39,057][233954] FPS: 329955.65[0m
[36m[2023-07-11 11:17:43,302][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:17:43,303][233954] Reward + Measures: [[30.50580774  0.23547432  0.14757434  0.15688834  0.54992568  0.46009052]][0m
[37m[1m[2023-07-11 11:17:43,303][233954] Max Reward on eval: 30.505807738047782[0m
[37m[1m[2023-07-11 11:17:43,303][233954] Min Reward on eval: 30.505807738047782[0m
[37m[1m[2023-07-11 11:17:43,304][233954] Mean Reward across all agents: 30.505807738047782[0m
[37m[1m[2023-07-11 11:17:43,304][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:17:48,320][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:17:48,320][233954] Reward + Measures: [[ 29.00631296   0.2342       0.22520001   0.19750001   0.33249998
    0.83608305]
 [-24.6713737    0.3601       0.56230003   0.3186       0.56470001
    1.44185221]
 [-22.56458872   0.41630003   0.56400007   0.3876       0.54710001
    1.49629009]
 ...
 [ 58.39608126   0.1276       0.3547       0.2369       0.25420001
    1.05712605]
 [ 35.47989227   0.2191       0.1304       0.1163       0.48019996
    0.44599447]
 [146.43755941   0.1707       0.42550001   0.22790001   0.44969997
    1.59627426]][0m
[37m[1m[2023-07-11 11:17:48,320][233954] Max Reward on eval: 357.98968550870194[0m
[37m[1m[2023-07-11 11:17:48,321][233954] Min Reward on eval: -140.68274449706078[0m
[37m[1m[2023-07-11 11:17:48,321][233954] Mean Reward across all agents: 19.627206617923676[0m
[37m[1m[2023-07-11 11:17:48,321][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:17:48,327][233954] mean_value=-698.4095925338918, max_value=551.2146811465733[0m
[37m[1m[2023-07-11 11:17:48,329][233954] New mean coefficients: [[-0.04163909 -0.2335599  -0.27850592  0.04683535  0.126881   -1.4959652 ]][0m
[37m[1m[2023-07-11 11:17:48,330][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:17:57,406][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 11:17:57,406][233954] FPS: 423183.74[0m
[36m[2023-07-11 11:17:57,408][233954] itr=836, itrs=2000, Progress: 41.80%[0m
[36m[2023-07-11 11:18:09,040][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 11:18:09,041][233954] FPS: 332750.44[0m
[36m[2023-07-11 11:18:13,292][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:18:13,293][233954] Reward + Measures: [[31.13541195  0.25948203  0.16516733  0.17292899  0.5306713   0.47087881]][0m
[37m[1m[2023-07-11 11:18:13,293][233954] Max Reward on eval: 31.13541195443928[0m
[37m[1m[2023-07-11 11:18:13,293][233954] Min Reward on eval: 31.13541195443928[0m
[37m[1m[2023-07-11 11:18:13,294][233954] Mean Reward across all agents: 31.13541195443928[0m
[37m[1m[2023-07-11 11:18:13,294][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:18:18,343][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:18:18,344][233954] Reward + Measures: [[23.903709    0.87790006  0.8872      0.85229999  0.89880002  1.87625682]
 [17.97167053  0.51389998  0.75019997  0.47040001  0.69629997  1.72234523]
 [23.33648796  0.3389      0.42339998  0.37580001  0.40550002  1.003546  ]
 ...
 [ 6.56289941  0.1123      0.1814      0.14600001  0.2987      0.8373003 ]
 [31.45350587  0.29229999  0.24630001  0.40630004  0.38890001  0.83469355]
 [ 8.6080027   0.42400002  0.50040001  0.42200002  0.4989      1.05669045]][0m
[37m[1m[2023-07-11 11:18:18,344][233954] Max Reward on eval: 637.1230926373042[0m
[37m[1m[2023-07-11 11:18:18,344][233954] Min Reward on eval: -96.66461980263702[0m
[37m[1m[2023-07-11 11:18:18,345][233954] Mean Reward across all agents: 39.04691499499011[0m
[37m[1m[2023-07-11 11:18:18,345][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:18:18,352][233954] mean_value=-353.84107620554846, max_value=641.9986221667938[0m
[37m[1m[2023-07-11 11:18:18,355][233954] New mean coefficients: [[-0.14529115 -0.13552848 -0.3540753   0.13119015  0.09363874 -1.3499131 ]][0m
[37m[1m[2023-07-11 11:18:18,355][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:18:27,429][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 11:18:27,429][233954] FPS: 423293.38[0m
[36m[2023-07-11 11:18:27,432][233954] itr=837, itrs=2000, Progress: 41.85%[0m
[36m[2023-07-11 11:18:39,243][233954] train() took 11.72 seconds to complete[0m
[36m[2023-07-11 11:18:39,243][233954] FPS: 327703.89[0m
[36m[2023-07-11 11:18:43,614][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:18:43,614][233954] Reward + Measures: [[30.48878675  0.24567832  0.14579299  0.15081699  0.5293197   0.4525516 ]][0m
[37m[1m[2023-07-11 11:18:43,614][233954] Max Reward on eval: 30.48878675442284[0m
[37m[1m[2023-07-11 11:18:43,615][233954] Min Reward on eval: 30.48878675442284[0m
[37m[1m[2023-07-11 11:18:43,615][233954] Mean Reward across all agents: 30.48878675442284[0m
[37m[1m[2023-07-11 11:18:43,615][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:18:48,898][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:18:48,898][233954] Reward + Measures: [[47.76499222  0.14310001  0.18109998  0.23100002  0.2414      0.68144339]
 [ 6.02280263  0.3989      0.44590002  0.28510001  0.51639998  1.03322899]
 [56.93705947  0.0621      0.42390004  0.2431      0.32269999  1.06831467]
 ...
 [76.99768159  0.3317      0.22880001  0.22350001  0.47819996  1.07885003]
 [13.70418575  0.17740001  0.41490003  0.3039      0.29609999  1.13196003]
 [30.77449328  0.26320001  0.23120001  0.1319      0.29520002  0.86028779]][0m
[37m[1m[2023-07-11 11:18:48,899][233954] Max Reward on eval: 311.7177429184318[0m
[37m[1m[2023-07-11 11:18:48,899][233954] Min Reward on eval: -99.27811215661931[0m
[37m[1m[2023-07-11 11:18:48,899][233954] Mean Reward across all agents: 26.115585610896517[0m
[37m[1m[2023-07-11 11:18:48,899][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:18:48,905][233954] mean_value=-731.3440219562932, max_value=563.8020382623188[0m
[37m[1m[2023-07-11 11:18:48,907][233954] New mean coefficients: [[-0.19556907 -0.23254345 -0.13377666  0.12571932  0.07558571 -1.4256446 ]][0m
[37m[1m[2023-07-11 11:18:48,908][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:18:58,037][233954] train() took 9.13 seconds to complete[0m
[36m[2023-07-11 11:18:58,038][233954] FPS: 420709.00[0m
[36m[2023-07-11 11:18:58,040][233954] itr=838, itrs=2000, Progress: 41.90%[0m
[36m[2023-07-11 11:19:09,738][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 11:19:09,739][233954] FPS: 330953.52[0m
[36m[2023-07-11 11:19:14,003][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:19:14,003][233954] Reward + Measures: [[32.44658961  0.25830066  0.14545201  0.15240099  0.55796033  0.44845879]][0m
[37m[1m[2023-07-11 11:19:14,003][233954] Max Reward on eval: 32.44658960982589[0m
[37m[1m[2023-07-11 11:19:14,004][233954] Min Reward on eval: 32.44658960982589[0m
[37m[1m[2023-07-11 11:19:14,004][233954] Mean Reward across all agents: 32.44658960982589[0m
[37m[1m[2023-07-11 11:19:14,004][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:19:19,025][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:19:19,030][233954] Reward + Measures: [[46.88772037  0.14930001  0.16790001  0.15519999  0.19270001  1.38842952]
 [28.65755152  0.041       0.097       0.0821      0.0878      1.15825367]
 [24.78130494  0.37979999  0.32220003  0.24469998  0.39120001  0.72750473]
 ...
 [30.77532419  0.1701      0.0642      0.06330001  0.16140001  0.47692633]
 [30.36954652  0.26630002  0.4052      0.38910004  0.3299      1.36844885]
 [13.90298005  0.31530002  0.22059999  0.0238      0.46040002  0.73444659]][0m
[37m[1m[2023-07-11 11:19:19,031][233954] Max Reward on eval: 462.48734210794794[0m
[37m[1m[2023-07-11 11:19:19,031][233954] Min Reward on eval: -76.57905773087404[0m
[37m[1m[2023-07-11 11:19:19,031][233954] Mean Reward across all agents: 33.07520862280063[0m
[37m[1m[2023-07-11 11:19:19,032][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:19:19,036][233954] mean_value=-784.9508872986208, max_value=514.933300302457[0m
[37m[1m[2023-07-11 11:19:19,038][233954] New mean coefficients: [[-0.01269451 -0.03555278  0.0222111   0.20408593  0.00385126 -0.9321613 ]][0m
[37m[1m[2023-07-11 11:19:19,039][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:19:28,123][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 11:19:28,123][233954] FPS: 422815.05[0m
[36m[2023-07-11 11:19:28,125][233954] itr=839, itrs=2000, Progress: 41.95%[0m
[36m[2023-07-11 11:19:39,895][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 11:19:39,895][233954] FPS: 328875.04[0m
[36m[2023-07-11 11:19:44,216][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:19:44,216][233954] Reward + Measures: [[31.42451127  0.14341635  0.190393    0.24223533  0.20347133  0.93697506]][0m
[37m[1m[2023-07-11 11:19:44,216][233954] Max Reward on eval: 31.424511272469516[0m
[37m[1m[2023-07-11 11:19:44,217][233954] Min Reward on eval: 31.424511272469516[0m
[37m[1m[2023-07-11 11:19:44,217][233954] Mean Reward across all agents: 31.424511272469516[0m
[37m[1m[2023-07-11 11:19:44,217][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:19:49,258][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:19:49,271][233954] Reward + Measures: [[ 36.8909342    0.22059999   0.39480001   0.23459999   0.32479998
    1.37063873]
 [ 49.71010341   0.32880002   0.40099999   0.39250001   0.36080003
    1.13668418]
 [ 37.49867797   0.17240001   0.27260002   0.26519999   0.19679999
    1.24199474]
 ...
 [169.83338674   0.43059999   0.67230004   0.4298       0.71630001
    1.66824555]
 [ -7.69075531   0.2122       0.37670001   0.1869       0.31490001
    1.39514005]
 [  6.76886235   0.1663       0.33790001   0.1349       0.32810003
    1.47069633]][0m
[37m[1m[2023-07-11 11:19:49,271][233954] Max Reward on eval: 603.5991897414904[0m
[37m[1m[2023-07-11 11:19:49,271][233954] Min Reward on eval: -60.429378502070904[0m
[37m[1m[2023-07-11 11:19:49,271][233954] Mean Reward across all agents: 69.22208005113845[0m
[37m[1m[2023-07-11 11:19:49,272][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:19:49,275][233954] mean_value=-927.5860632760118, max_value=529.6408427634772[0m
[37m[1m[2023-07-11 11:19:49,278][233954] New mean coefficients: [[ 0.00387911  0.11968264  0.12172957  0.26561427  0.11191266 -0.7528597 ]][0m
[37m[1m[2023-07-11 11:19:49,279][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:19:58,345][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 11:19:58,345][233954] FPS: 423643.23[0m
[36m[2023-07-11 11:19:58,348][233954] itr=840, itrs=2000, Progress: 42.00%[0m
[37m[1m[2023-07-11 11:23:29,150][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000820[0m
[36m[2023-07-11 11:23:41,500][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 11:23:41,500][233954] FPS: 330085.42[0m
[36m[2023-07-11 11:23:45,699][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:23:45,699][233954] Reward + Measures: [[38.03730636  0.13624333  0.17459135  0.25507298  0.20455767  0.92546237]][0m
[37m[1m[2023-07-11 11:23:45,699][233954] Max Reward on eval: 38.03730636148079[0m
[37m[1m[2023-07-11 11:23:45,700][233954] Min Reward on eval: 38.03730636148079[0m
[37m[1m[2023-07-11 11:23:45,700][233954] Mean Reward across all agents: 38.03730636148079[0m
[37m[1m[2023-07-11 11:23:45,700][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:23:50,643][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:23:50,644][233954] Reward + Measures: [[ 26.55160793   0.10770001   0.16500001   0.1392       0.19600001
    0.96002638]
 [ 15.506496     0.1911       0.18679999   0.25470001   0.27739999
    0.98989457]
 [ 24.43845166   0.0987       0.1855       0.1743       0.169
    1.47967243]
 ...
 [ 37.79437644   0.36840001   0.43269998   0.3937       0.42080003
    1.2335546 ]
 [229.88705518   0.36490002   0.09500001   0.36970001   0.39190003
    1.84292209]
 [ 49.55781816   0.72830003   0.6972       0.69840002   0.28749999
    1.86104   ]][0m
[37m[1m[2023-07-11 11:23:50,644][233954] Max Reward on eval: 363.8629588957876[0m
[37m[1m[2023-07-11 11:23:50,644][233954] Min Reward on eval: -93.32896994482726[0m
[37m[1m[2023-07-11 11:23:50,644][233954] Mean Reward across all agents: 29.86142886282367[0m
[37m[1m[2023-07-11 11:23:50,644][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:23:50,647][233954] mean_value=-900.2714630082631, max_value=263.79264389181714[0m
[37m[1m[2023-07-11 11:23:50,650][233954] New mean coefficients: [[-0.00716125  0.10402936  0.29904008  0.37810498  0.16914417 -0.606019  ]][0m
[37m[1m[2023-07-11 11:23:50,651][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:23:59,547][233954] train() took 8.89 seconds to complete[0m
[36m[2023-07-11 11:23:59,547][233954] FPS: 431708.75[0m
[36m[2023-07-11 11:23:59,549][233954] itr=841, itrs=2000, Progress: 42.05%[0m
[36m[2023-07-11 11:24:11,065][233954] train() took 11.42 seconds to complete[0m
[36m[2023-07-11 11:24:11,065][233954] FPS: 336167.08[0m
[36m[2023-07-11 11:24:15,252][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:24:15,252][233954] Reward + Measures: [[33.73430351  0.13451435  0.17750067  0.28012401  0.20955701  0.84961057]][0m
[37m[1m[2023-07-11 11:24:15,253][233954] Max Reward on eval: 33.73430351030396[0m
[37m[1m[2023-07-11 11:24:15,253][233954] Min Reward on eval: 33.73430351030396[0m
[37m[1m[2023-07-11 11:24:15,253][233954] Mean Reward across all agents: 33.73430351030396[0m
[37m[1m[2023-07-11 11:24:15,253][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:24:20,216][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:24:20,216][233954] Reward + Measures: [[-13.91540342   0.20500003   0.62290001   0.17040001   0.60659999
    1.71338165]
 [ 36.34272074   0.05320001   0.0872       0.0972       0.13060001
    0.95566046]
 [ 30.46263945   0.0398       0.11720001   0.19200002   0.10880001
    0.85161114]
 ...
 [ 26.48198897   0.11989999   0.15989999   0.21570002   0.21999998
    1.41881013]
 [ -4.95997726   0.48319998   0.63479996   0.51540005   0.58649999
    1.26067531]
 [ 24.63231446   0.2142       0.15690002   0.23940001   0.25350001
    1.09751415]][0m
[37m[1m[2023-07-11 11:24:20,217][233954] Max Reward on eval: 592.1261177057401[0m
[37m[1m[2023-07-11 11:24:20,217][233954] Min Reward on eval: -37.00185599140823[0m
[37m[1m[2023-07-11 11:24:20,217][233954] Mean Reward across all agents: 36.30906698096554[0m
[37m[1m[2023-07-11 11:24:20,217][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:24:20,222][233954] mean_value=-1132.8724325928943, max_value=635.7962317608296[0m
[37m[1m[2023-07-11 11:24:20,224][233954] New mean coefficients: [[ 0.21480493  0.3026241   0.3409903   0.5377089   0.30300564 -0.00997639]][0m
[37m[1m[2023-07-11 11:24:20,225][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:24:29,159][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 11:24:29,160][233954] FPS: 429887.76[0m
[36m[2023-07-11 11:24:29,162][233954] itr=842, itrs=2000, Progress: 42.10%[0m
[36m[2023-07-11 11:24:40,717][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 11:24:40,717][233954] FPS: 334955.96[0m
[36m[2023-07-11 11:24:44,990][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:24:44,991][233954] Reward + Measures: [[38.26573785  0.12206333  0.15412301  0.20146334  0.207691    1.05725014]][0m
[37m[1m[2023-07-11 11:24:44,991][233954] Max Reward on eval: 38.26573785395439[0m
[37m[1m[2023-07-11 11:24:44,991][233954] Min Reward on eval: 38.26573785395439[0m
[37m[1m[2023-07-11 11:24:44,992][233954] Mean Reward across all agents: 38.26573785395439[0m
[37m[1m[2023-07-11 11:24:44,992][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:24:49,955][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:24:49,961][233954] Reward + Measures: [[ 37.93595846   0.28349999   0.31890002   0.1885       0.3723
    1.12023294]
 [ 74.92881493   0.0727       0.0882       0.1014       0.10699999
    1.66250825]
 [ 41.34651718   0.0399       0.1166       0.2095       0.0834
    1.07182634]
 ...
 [222.66386318   0.09260001   0.44280002   0.27680001   0.42120001
    2.25822902]
 [ 37.82165026   0.12360001   0.1525       0.15460001   0.19999999
    1.32709205]
 [ 27.32092102   0.1991       0.2388       0.2017       0.28930002
    1.20461643]][0m
[37m[1m[2023-07-11 11:24:49,961][233954] Max Reward on eval: 315.20053786891515[0m
[37m[1m[2023-07-11 11:24:49,962][233954] Min Reward on eval: -57.99881017524749[0m
[37m[1m[2023-07-11 11:24:49,962][233954] Mean Reward across all agents: 48.76324385029669[0m
[37m[1m[2023-07-11 11:24:49,962][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:24:49,965][233954] mean_value=-1070.6739041533908, max_value=533.8023641350233[0m
[37m[1m[2023-07-11 11:24:49,967][233954] New mean coefficients: [[ 0.23566571  0.43142992  0.4364878   0.56855446  0.43609875 -0.04824859]][0m
[37m[1m[2023-07-11 11:24:49,968][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:24:58,993][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 11:24:58,993][233954] FPS: 425552.62[0m
[36m[2023-07-11 11:24:58,996][233954] itr=843, itrs=2000, Progress: 42.15%[0m
[36m[2023-07-11 11:25:10,561][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 11:25:10,561][233954] FPS: 334589.19[0m
[36m[2023-07-11 11:25:14,851][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:25:14,852][233954] Reward + Measures: [[41.98833014  0.12098067  0.14448966  0.20966667  0.21122502  1.06047368]][0m
[37m[1m[2023-07-11 11:25:14,852][233954] Max Reward on eval: 41.98833014249076[0m
[37m[1m[2023-07-11 11:25:14,852][233954] Min Reward on eval: 41.98833014249076[0m
[37m[1m[2023-07-11 11:25:14,853][233954] Mean Reward across all agents: 41.98833014249076[0m
[37m[1m[2023-07-11 11:25:14,853][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:25:20,087][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:25:20,093][233954] Reward + Measures: [[ 7.00441288  0.54820007  0.50129998  0.53800005  0.56300002  1.31939983]
 [26.69547284  0.1382      0.1521      0.21470001  0.24599998  0.99424231]
 [13.52219403  0.58100003  0.72800004  0.54210001  0.72889996  2.02513242]
 ...
 [49.67435332  0.2225      0.16530001  0.27250001  0.2631      1.2410388 ]
 [26.73770186  0.2359      0.28620002  0.2529      0.33530003  1.08534098]
 [35.08412943  0.1864      0.27720001  0.2271      0.29860002  1.06862974]][0m
[37m[1m[2023-07-11 11:25:20,093][233954] Max Reward on eval: 506.959563700296[0m
[37m[1m[2023-07-11 11:25:20,093][233954] Min Reward on eval: -203.04733086144552[0m
[37m[1m[2023-07-11 11:25:20,094][233954] Mean Reward across all agents: 22.703133510575583[0m
[37m[1m[2023-07-11 11:25:20,094][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:25:20,096][233954] mean_value=-835.961237644869, max_value=542.1363236825587[0m
[37m[1m[2023-07-11 11:25:20,099][233954] New mean coefficients: [[ 0.2677624   0.32682616  0.54200864  0.6043236   0.3591236  -0.33057886]][0m
[37m[1m[2023-07-11 11:25:20,100][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:25:29,046][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 11:25:29,046][233954] FPS: 429323.68[0m
[36m[2023-07-11 11:25:29,048][233954] itr=844, itrs=2000, Progress: 42.20%[0m
[36m[2023-07-11 11:25:40,636][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 11:25:40,636][233954] FPS: 334017.89[0m
[36m[2023-07-11 11:25:44,909][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:25:44,910][233954] Reward + Measures: [[43.6928214   0.123249    0.153763    0.214488    0.21346635  1.03662348]][0m
[37m[1m[2023-07-11 11:25:44,910][233954] Max Reward on eval: 43.692821395636265[0m
[37m[1m[2023-07-11 11:25:44,910][233954] Min Reward on eval: 43.692821395636265[0m
[37m[1m[2023-07-11 11:25:44,911][233954] Mean Reward across all agents: 43.692821395636265[0m
[37m[1m[2023-07-11 11:25:44,911][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:25:49,936][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:25:49,937][233954] Reward + Measures: [[85.67305512  0.1815      0.2378      0.15570001  0.26139998  1.47693121]
 [70.35861492  0.3565      0.34800002  0.30000001  0.46170002  1.27431488]
 [41.80555416  0.21329999  0.82139999  0.28639999  0.69850004  1.87274706]
 ...
 [20.44382735  0.3655      0.1085      0.36930001  0.4104      1.50552976]
 [48.61162086  0.29459998  0.32800004  0.22880001  0.41599998  1.34316468]
 [33.44998342  0.0529      0.14359999  0.14560001  0.0922      1.39887047]][0m
[37m[1m[2023-07-11 11:25:49,937][233954] Max Reward on eval: 663.5511512717233[0m
[37m[1m[2023-07-11 11:25:49,937][233954] Min Reward on eval: -36.666513006482276[0m
[37m[1m[2023-07-11 11:25:49,937][233954] Mean Reward across all agents: 132.06105117386906[0m
[37m[1m[2023-07-11 11:25:49,938][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:25:49,942][233954] mean_value=-718.830374692421, max_value=755.5393843447275[0m
[37m[1m[2023-07-11 11:25:49,945][233954] New mean coefficients: [[ 0.3366285   0.5624095   0.5396788   0.78547525  0.48945314 -0.03813317]][0m
[37m[1m[2023-07-11 11:25:49,946][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:25:58,932][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 11:25:58,933][233954] FPS: 427399.86[0m
[36m[2023-07-11 11:25:58,935][233954] itr=845, itrs=2000, Progress: 42.25%[0m
[36m[2023-07-11 11:26:10,540][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 11:26:10,540][233954] FPS: 333552.19[0m
[36m[2023-07-11 11:26:14,836][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:26:14,836][233954] Reward + Measures: [[43.15813958  0.13049699  0.17478168  0.25135332  0.22342332  1.02142382]][0m
[37m[1m[2023-07-11 11:26:14,837][233954] Max Reward on eval: 43.158139579757055[0m
[37m[1m[2023-07-11 11:26:14,837][233954] Min Reward on eval: 43.158139579757055[0m
[37m[1m[2023-07-11 11:26:14,837][233954] Mean Reward across all agents: 43.158139579757055[0m
[37m[1m[2023-07-11 11:26:14,837][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:26:19,872][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:26:19,872][233954] Reward + Measures: [[212.39003621   0.09170001   0.4244       0.34779999   0.45720002
    1.77861214]
 [-20.57733694   0.38940001   0.35350001   0.28309998   0.44089994
    1.73496747]
 [ 25.42429248   0.2253       0.2465       0.23540001   0.2509
    1.28724134]
 ...
 [  2.78670834   0.244        0.31220004   0.39449999   0.39140001
    1.07793546]
 [ 82.29058733   0.18609999   0.14250001   0.32730001   0.2949
    1.08069265]
 [346.11893443   0.0984       0.80549997   0.579        0.78429997
    2.16539884]][0m
[37m[1m[2023-07-11 11:26:19,873][233954] Max Reward on eval: 728.863288885355[0m
[37m[1m[2023-07-11 11:26:19,873][233954] Min Reward on eval: -50.07659422392025[0m
[37m[1m[2023-07-11 11:26:19,873][233954] Mean Reward across all agents: 144.2416457267978[0m
[37m[1m[2023-07-11 11:26:19,873][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:26:19,877][233954] mean_value=-682.3021827598861, max_value=568.6476648199413[0m
[37m[1m[2023-07-11 11:26:19,880][233954] New mean coefficients: [[0.3635445  0.79406893 0.45658112 0.86824167 0.6987287  0.10409243]][0m
[37m[1m[2023-07-11 11:26:19,881][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:26:28,938][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 11:26:28,943][233954] FPS: 424078.54[0m
[36m[2023-07-11 11:26:28,946][233954] itr=846, itrs=2000, Progress: 42.30%[0m
[36m[2023-07-11 11:26:40,599][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 11:26:40,600][233954] FPS: 332184.01[0m
[36m[2023-07-11 11:26:44,813][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:26:44,814][233954] Reward + Measures: [[43.62369231  0.14551599  0.18082833  0.28149366  0.24436533  1.00396657]][0m
[37m[1m[2023-07-11 11:26:44,814][233954] Max Reward on eval: 43.62369230696689[0m
[37m[1m[2023-07-11 11:26:44,814][233954] Min Reward on eval: 43.62369230696689[0m
[37m[1m[2023-07-11 11:26:44,815][233954] Mean Reward across all agents: 43.62369230696689[0m
[37m[1m[2023-07-11 11:26:44,815][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:26:49,770][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:26:49,770][233954] Reward + Measures: [[ 8.00166028  0.24100001  0.37980002  0.2379      0.36189997  1.96925509]
 [ 9.09548561  0.13500002  0.25150001  0.14119999  0.20570002  1.6433624 ]
 [ 1.65356906  0.18520001  0.31020001  0.2683      0.25930002  1.41964638]
 ...
 [-4.06525113  0.2852      0.33489999  0.25470001  0.38050005  1.4261415 ]
 [23.90740681  0.11110001  0.2026      0.29270002  0.1697      0.90414232]
 [31.59827205  0.0404      0.0768      0.1612      0.152       1.06294692]][0m
[37m[1m[2023-07-11 11:26:49,771][233954] Max Reward on eval: 290.1973113916814[0m
[37m[1m[2023-07-11 11:26:49,771][233954] Min Reward on eval: -94.72814616356045[0m
[37m[1m[2023-07-11 11:26:49,771][233954] Mean Reward across all agents: 27.3368012975003[0m
[37m[1m[2023-07-11 11:26:49,771][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:26:49,774][233954] mean_value=-1628.8273924893167, max_value=186.07127155416532[0m
[37m[1m[2023-07-11 11:26:49,777][233954] New mean coefficients: [[0.25173402 0.5549085  0.66070986 0.9756725  0.81014264 0.2340918 ]][0m
[37m[1m[2023-07-11 11:26:49,778][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:26:58,762][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 11:26:58,762][233954] FPS: 427494.20[0m
[36m[2023-07-11 11:26:58,764][233954] itr=847, itrs=2000, Progress: 42.35%[0m
[36m[2023-07-11 11:27:10,306][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 11:27:10,307][233954] FPS: 335322.03[0m
[36m[2023-07-11 11:27:14,543][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:27:14,543][233954] Reward + Measures: [[44.48135745  0.16255033  0.20710534  0.30171803  0.26076934  0.98628241]][0m
[37m[1m[2023-07-11 11:27:14,543][233954] Max Reward on eval: 44.481357445251625[0m
[37m[1m[2023-07-11 11:27:14,544][233954] Min Reward on eval: 44.481357445251625[0m
[37m[1m[2023-07-11 11:27:14,544][233954] Mean Reward across all agents: 44.481357445251625[0m
[37m[1m[2023-07-11 11:27:14,544][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:27:19,556][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:27:19,556][233954] Reward + Measures: [[ 76.10408493   0.33060002   0.14969999   0.3019       0.36380002
    2.16315055]
 [-40.83692117   0.25409999   0.39949998   0.21229999   0.44369999
    1.4010303 ]
 [-26.76621594   0.53300005   0.5819       0.2156       0.56660002
    2.08029485]
 ...
 [ 26.48221799   0.43099999   0.18360001   0.40850002   0.44109997
    1.69076669]
 [  2.14922019   0.29410002   0.51789999   0.23249999   0.48400003
    2.17237711]
 [ 91.92214727   0.36310002   0.56910002   0.32640001   0.5546
    2.05355811]][0m
[37m[1m[2023-07-11 11:27:19,557][233954] Max Reward on eval: 291.1412849358283[0m
[37m[1m[2023-07-11 11:27:19,557][233954] Min Reward on eval: -144.16263704355805[0m
[37m[1m[2023-07-11 11:27:19,557][233954] Mean Reward across all agents: 26.319119685904862[0m
[37m[1m[2023-07-11 11:27:19,557][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:27:19,561][233954] mean_value=-360.8164088983652, max_value=436.78797545787216[0m
[37m[1m[2023-07-11 11:27:19,564][233954] New mean coefficients: [[0.18557346 0.59967065 0.71088475 1.1155585  0.6855737  0.247073  ]][0m
[37m[1m[2023-07-11 11:27:19,564][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:27:28,557][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 11:27:28,557][233954] FPS: 427096.99[0m
[36m[2023-07-11 11:27:28,560][233954] itr=848, itrs=2000, Progress: 42.40%[0m
[36m[2023-07-11 11:27:40,248][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 11:27:40,249][233954] FPS: 331197.38[0m
[36m[2023-07-11 11:27:44,517][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:27:44,517][233954] Reward + Measures: [[45.54137691  0.16590732  0.20445566  0.32441834  0.26830965  0.9861905 ]][0m
[37m[1m[2023-07-11 11:27:44,517][233954] Max Reward on eval: 45.54137691278019[0m
[37m[1m[2023-07-11 11:27:44,518][233954] Min Reward on eval: 45.54137691278019[0m
[37m[1m[2023-07-11 11:27:44,518][233954] Mean Reward across all agents: 45.54137691278019[0m
[37m[1m[2023-07-11 11:27:44,518][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:27:49,415][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:27:49,416][233954] Reward + Measures: [[ 51.90149583   0.31619999   0.4824       0.35349998   0.54820007
    1.77893436]
 [543.85697558   0.0026       0.99529994   0.82139999   0.99419993
    2.25983763]
 [ 99.17740967   0.36830002   0.28340003   0.34100002   0.51240003
    1.5814904 ]
 ...
 [260.14563752   0.0699       0.89890003   0.48149997   0.82989997
    2.08419013]
 [ 49.88231801   0.115        0.21910003   0.19939999   0.29810002
    1.39526105]
 [328.33613014   0.20579998   0.94639999   0.70910001   0.94990009
    2.13151717]][0m
[37m[1m[2023-07-11 11:27:49,416][233954] Max Reward on eval: 559.2626228226349[0m
[37m[1m[2023-07-11 11:27:49,417][233954] Min Reward on eval: -87.36073780581356[0m
[37m[1m[2023-07-11 11:27:49,417][233954] Mean Reward across all agents: 119.37639149716902[0m
[37m[1m[2023-07-11 11:27:49,417][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:27:49,420][233954] mean_value=-648.8294869176939, max_value=412.0698759185025[0m
[37m[1m[2023-07-11 11:27:49,423][233954] New mean coefficients: [[0.05060428 0.6068618  0.5301984  1.1565739  0.6071714  0.0198838 ]][0m
[37m[1m[2023-07-11 11:27:49,424][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:27:58,368][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 11:27:58,369][233954] FPS: 429389.87[0m
[36m[2023-07-11 11:27:58,371][233954] itr=849, itrs=2000, Progress: 42.45%[0m
[36m[2023-07-11 11:28:09,934][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 11:28:09,935][233954] FPS: 334731.24[0m
[36m[2023-07-11 11:28:14,182][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:28:14,182][233954] Reward + Measures: [[49.1149381   0.188802    0.22581367  0.35464367  0.29396802  0.97771633]][0m
[37m[1m[2023-07-11 11:28:14,183][233954] Max Reward on eval: 49.11493809767611[0m
[37m[1m[2023-07-11 11:28:14,183][233954] Min Reward on eval: 49.11493809767611[0m
[37m[1m[2023-07-11 11:28:14,183][233954] Mean Reward across all agents: 49.11493809767611[0m
[37m[1m[2023-07-11 11:28:14,183][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:28:19,455][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:28:19,455][233954] Reward + Measures: [[ 94.3438183    0.41420004   0.46790001   0.44420001   0.49470001
    1.27099621]
 [ 12.88531947   0.34830001   0.34890005   0.37380001   0.40000001
    1.09040952]
 [ 26.43133006   0.30329999   0.24959998   0.30809999   0.32780001
    1.47159696]
 ...
 [ 19.83449626   0.0465       0.122        0.0724       0.1301
    1.48267865]
 [ -2.48827502   0.20080002   0.51660001   0.1398       0.44610006
    1.70924342]
 [102.73598504   0.29719999   0.049        0.42560002   0.48599997
    1.26110673]][0m
[37m[1m[2023-07-11 11:28:19,455][233954] Max Reward on eval: 270.5600532360375[0m
[37m[1m[2023-07-11 11:28:19,456][233954] Min Reward on eval: -17.823410485778005[0m
[37m[1m[2023-07-11 11:28:19,456][233954] Mean Reward across all agents: 46.982734872675074[0m
[37m[1m[2023-07-11 11:28:19,456][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:28:19,460][233954] mean_value=-745.7476389092385, max_value=746.4723834415897[0m
[37m[1m[2023-07-11 11:28:19,463][233954] New mean coefficients: [[ 0.03490003  0.32561588  0.415634    1.0556595   0.54201496 -0.09226848]][0m
[37m[1m[2023-07-11 11:28:19,464][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:28:28,570][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 11:28:28,570][233954] FPS: 421778.06[0m
[36m[2023-07-11 11:28:28,572][233954] itr=850, itrs=2000, Progress: 42.50%[0m
[37m[1m[2023-07-11 11:31:52,734][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000830[0m
[36m[2023-07-11 11:32:04,807][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 11:32:04,807][233954] FPS: 334239.87[0m
[36m[2023-07-11 11:32:09,022][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:32:09,022][233954] Reward + Measures: [[45.22676864  0.18619999  0.21909033  0.36323002  0.30661467  0.93656474]][0m
[37m[1m[2023-07-11 11:32:09,022][233954] Max Reward on eval: 45.2267686388952[0m
[37m[1m[2023-07-11 11:32:09,023][233954] Min Reward on eval: 45.2267686388952[0m
[37m[1m[2023-07-11 11:32:09,023][233954] Mean Reward across all agents: 45.2267686388952[0m
[37m[1m[2023-07-11 11:32:09,023][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:32:13,961][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:32:13,961][233954] Reward + Measures: [[ 0.1533446   0.41809997  0.96180004  0.377       0.954       2.28276038]
 [14.41551542  0.28010002  0.93110007  0.13420001  0.90339994  2.29553628]
 [58.2074995   0.1364      0.70279998  0.20179999  0.64580005  1.95570338]
 ...
 [10.65482915  0.3143      0.91390002  0.29460001  0.86819994  2.2251265 ]
 [ 2.99726706  0.30770001  0.90429991  0.1399      0.86219996  2.15668464]
 [-0.34114908  0.12990001  0.28650001  0.16780001  0.29550001  1.31192136]][0m
[37m[1m[2023-07-11 11:32:13,961][233954] Max Reward on eval: 325.5034135833383[0m
[37m[1m[2023-07-11 11:32:13,962][233954] Min Reward on eval: -90.68723744042218[0m
[37m[1m[2023-07-11 11:32:13,962][233954] Mean Reward across all agents: 27.445729841461276[0m
[37m[1m[2023-07-11 11:32:13,962][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:32:13,965][233954] mean_value=-388.86714153629686, max_value=575.4000132211484[0m
[37m[1m[2023-07-11 11:32:13,967][233954] New mean coefficients: [[-0.06275492  0.24570318  0.36468664  1.0567178   0.41448957 -0.0904284 ]][0m
[37m[1m[2023-07-11 11:32:13,968][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:32:22,857][233954] train() took 8.89 seconds to complete[0m
[36m[2023-07-11 11:32:22,857][233954] FPS: 432072.46[0m
[36m[2023-07-11 11:32:22,860][233954] itr=851, itrs=2000, Progress: 42.55%[0m
[36m[2023-07-11 11:32:34,659][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 11:32:34,659][233954] FPS: 328025.92[0m
[36m[2023-07-11 11:32:38,975][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:32:38,976][233954] Reward + Measures: [[47.7122159   0.20255733  0.24201299  0.39223132  0.32208398  0.93707031]][0m
[37m[1m[2023-07-11 11:32:38,976][233954] Max Reward on eval: 47.71221589611028[0m
[37m[1m[2023-07-11 11:32:38,976][233954] Min Reward on eval: 47.71221589611028[0m
[37m[1m[2023-07-11 11:32:38,976][233954] Mean Reward across all agents: 47.71221589611028[0m
[37m[1m[2023-07-11 11:32:38,977][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:32:43,898][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:32:43,899][233954] Reward + Measures: [[ 40.36402374   0.56599993   0.67930001   0.48370001   0.68349999
    1.31396091]
 [  2.5051819    0.67000002   0.78130001   0.57679999   0.80559999
    2.16577148]
 [ 54.24464152   0.82279998   0.88389999   0.78680003   0.8545
    2.12305379]
 ...
 [ 16.29578118   0.2701       0.33750004   0.35919997   0.41870004
    1.22457635]
 [ -3.63262199   0.37869999   0.45019999   0.4118       0.56689996
    1.40336788]
 [-82.95574853   0.80129999   0.84980005   0.66650003   0.87089998
    1.49148452]][0m
[37m[1m[2023-07-11 11:32:43,899][233954] Max Reward on eval: 202.11739657362924[0m
[37m[1m[2023-07-11 11:32:43,899][233954] Min Reward on eval: -160.45955711986463[0m
[37m[1m[2023-07-11 11:32:43,899][233954] Mean Reward across all agents: 3.0442647856489544[0m
[37m[1m[2023-07-11 11:32:43,900][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:32:43,903][233954] mean_value=-263.90097513700687, max_value=358.73089435654515[0m
[37m[1m[2023-07-11 11:32:43,906][233954] New mean coefficients: [[-0.15203002  0.31886515  0.35776657  1.0084689   0.45312712  0.04054116]][0m
[37m[1m[2023-07-11 11:32:43,907][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:32:52,894][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 11:32:52,894][233954] FPS: 427360.52[0m
[36m[2023-07-11 11:32:52,896][233954] itr=852, itrs=2000, Progress: 42.60%[0m
[36m[2023-07-11 11:33:04,625][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 11:33:04,626][233954] FPS: 330074.70[0m
[36m[2023-07-11 11:33:08,900][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:33:08,900][233954] Reward + Measures: [[50.73757215  0.20128466  0.24605633  0.41246966  0.33121198  0.93461132]][0m
[37m[1m[2023-07-11 11:33:08,900][233954] Max Reward on eval: 50.73757214619596[0m
[37m[1m[2023-07-11 11:33:08,901][233954] Min Reward on eval: 50.73757214619596[0m
[37m[1m[2023-07-11 11:33:08,901][233954] Mean Reward across all agents: 50.73757214619596[0m
[37m[1m[2023-07-11 11:33:08,901][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:33:13,903][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:33:13,903][233954] Reward + Measures: [[ 60.39094139   0.65450001   0.6207       0.71080005   0.2455
    2.19172859]
 [ 25.47183537   0.12100001   0.32769999   0.19419999   0.27240002
    1.31317425]
 [146.2141514    0.79869998   0.75739998   0.86339998   0.22390001
    2.01596022]
 ...
 [104.45749973   0.47189999   0.82740003   0.63999999   0.58540004
    1.97668207]
 [129.00992679   0.71759999   0.76400006   0.7938       0.2881
    1.99422455]
 [145.06817676   0.94290012   0.79750001   0.95809996   0.0219
    2.17081857]][0m
[37m[1m[2023-07-11 11:33:13,904][233954] Max Reward on eval: 510.21878816392274[0m
[37m[1m[2023-07-11 11:33:13,904][233954] Min Reward on eval: -96.11009330525994[0m
[37m[1m[2023-07-11 11:33:13,904][233954] Mean Reward across all agents: 138.2907937276812[0m
[37m[1m[2023-07-11 11:33:13,904][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:33:13,912][233954] mean_value=-96.14062791578428, max_value=688.9071687514361[0m
[37m[1m[2023-07-11 11:33:13,915][233954] New mean coefficients: [[-0.21159187  0.36183354  0.2811325   1.4652367   0.6451019   0.49215764]][0m
[37m[1m[2023-07-11 11:33:13,916][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:33:22,869][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 11:33:22,869][233954] FPS: 428991.29[0m
[36m[2023-07-11 11:33:22,871][233954] itr=853, itrs=2000, Progress: 42.65%[0m
[36m[2023-07-11 11:33:34,497][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 11:33:34,497][233954] FPS: 333064.27[0m
[36m[2023-07-11 11:33:38,734][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:33:38,734][233954] Reward + Measures: [[46.94439444  0.20774235  0.26545033  0.42989534  0.33933234  0.94230139]][0m
[37m[1m[2023-07-11 11:33:38,734][233954] Max Reward on eval: 46.94439443819171[0m
[37m[1m[2023-07-11 11:33:38,734][233954] Min Reward on eval: 46.94439443819171[0m
[37m[1m[2023-07-11 11:33:38,735][233954] Mean Reward across all agents: 46.94439443819171[0m
[37m[1m[2023-07-11 11:33:38,735][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:33:43,764][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:33:43,765][233954] Reward + Measures: [[ -5.81505785   0.53949994   0.81949997   0.50999999   0.76890004
    2.00093079]
 [ 12.82860295   0.20610002   0.29580003   0.30379996   0.42130002
    1.18646371]
 [ 79.05323506   0.39699998   0.24439998   0.37310001   0.42669997
    1.6681155 ]
 ...
 [ 28.13367437   0.20820001   0.25570002   0.32980001   0.3418
    0.94569838]
 [146.08686877   0.15550001   0.3524       0.4219       0.45070001
    1.68604887]
 [ 19.73508718   0.0209       0.0948       0.28260002   0.15710001
    0.98332351]][0m
[37m[1m[2023-07-11 11:33:43,765][233954] Max Reward on eval: 390.0535116109066[0m
[37m[1m[2023-07-11 11:33:43,765][233954] Min Reward on eval: -29.990793086681514[0m
[37m[1m[2023-07-11 11:33:43,766][233954] Mean Reward across all agents: 80.37591324581511[0m
[37m[1m[2023-07-11 11:33:43,766][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:33:43,771][233954] mean_value=-505.72973764510095, max_value=632.7830421459768[0m
[37m[1m[2023-07-11 11:33:43,774][233954] New mean coefficients: [[0.03666751 0.45195884 0.19112095 1.269804   0.6202409  0.3143083 ]][0m
[37m[1m[2023-07-11 11:33:43,775][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:33:52,883][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 11:33:52,884][233954] FPS: 421666.19[0m
[36m[2023-07-11 11:33:52,886][233954] itr=854, itrs=2000, Progress: 42.70%[0m
[36m[2023-07-11 11:34:04,861][233954] train() took 11.88 seconds to complete[0m
[36m[2023-07-11 11:34:04,867][233954] FPS: 323144.21[0m
[36m[2023-07-11 11:34:09,131][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:34:09,131][233954] Reward + Measures: [[51.57357601  0.21666133  0.27872464  0.45946768  0.35209268  0.96673447]][0m
[37m[1m[2023-07-11 11:34:09,131][233954] Max Reward on eval: 51.573576005503455[0m
[37m[1m[2023-07-11 11:34:09,131][233954] Min Reward on eval: 51.573576005503455[0m
[37m[1m[2023-07-11 11:34:09,132][233954] Mean Reward across all agents: 51.573576005503455[0m
[37m[1m[2023-07-11 11:34:09,132][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:34:14,109][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:34:14,110][233954] Reward + Measures: [[121.17568789   0.1221       0.33840004   0.48160002   0.47810003
    1.54738367]
 [  1.7506369    0.098        0.18339999   0.1512       0.22739999
    1.50383937]
 [ 54.98948984   0.1795       0.1558       0.1788       0.2471
    1.59507406]
 ...
 [-11.33952868   0.1479       0.23549998   0.1688       0.2771
    1.63026798]
 [156.77622604   0.0559       0.27099997   0.20350002   0.28099999
    1.96730256]
 [ -7.79053579   0.15769999   0.3378       0.23960002   0.36129999
    1.29305804]][0m
[37m[1m[2023-07-11 11:34:14,110][233954] Max Reward on eval: 672.7294578593225[0m
[37m[1m[2023-07-11 11:34:14,110][233954] Min Reward on eval: -46.86943866703659[0m
[37m[1m[2023-07-11 11:34:14,111][233954] Mean Reward across all agents: 73.28251610866084[0m
[37m[1m[2023-07-11 11:34:14,111][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:34:14,115][233954] mean_value=-909.2075361919607, max_value=612.2557266740129[0m
[37m[1m[2023-07-11 11:34:14,117][233954] New mean coefficients: [[-0.08040515  0.48447585  0.15770459  1.0940021   0.63120824  0.1830084 ]][0m
[37m[1m[2023-07-11 11:34:14,118][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:34:23,052][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 11:34:23,052][233954] FPS: 429928.32[0m
[36m[2023-07-11 11:34:23,054][233954] itr=855, itrs=2000, Progress: 42.75%[0m
[36m[2023-07-11 11:34:35,011][233954] train() took 11.86 seconds to complete[0m
[36m[2023-07-11 11:34:35,012][233954] FPS: 323675.20[0m
[36m[2023-07-11 11:34:39,322][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:34:39,323][233954] Reward + Measures: [[48.79176964  0.22544299  0.31463099  0.46733031  0.37413695  1.00658929]][0m
[37m[1m[2023-07-11 11:34:39,323][233954] Max Reward on eval: 48.791769641686194[0m
[37m[1m[2023-07-11 11:34:39,323][233954] Min Reward on eval: 48.791769641686194[0m
[37m[1m[2023-07-11 11:34:39,324][233954] Mean Reward across all agents: 48.791769641686194[0m
[37m[1m[2023-07-11 11:34:39,324][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:34:44,317][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:34:44,325][233954] Reward + Measures: [[107.23925576   0.19780001   0.40540001   0.44060001   0.36570001
    1.1145401 ]
 [ 28.38337719   0.1145       0.1524       0.40799999   0.27690002
    0.85899681]
 [ 36.95735105   0.29169998   0.3389       0.4409       0.48330003
    1.33120668]
 ...
 [104.2735087    0.17260002   0.71080005   0.35859999   0.70139998
    2.47408342]
 [ 99.89165244   0.26449999   0.29700002   0.28599998   0.41510001
    1.89981389]
 [182.84737024   0.1452       0.38629997   0.39850003   0.44140002
    1.54792106]][0m
[37m[1m[2023-07-11 11:34:44,326][233954] Max Reward on eval: 740.9017066980712[0m
[37m[1m[2023-07-11 11:34:44,326][233954] Min Reward on eval: -82.35584605187178[0m
[37m[1m[2023-07-11 11:34:44,326][233954] Mean Reward across all agents: 142.93418527319218[0m
[37m[1m[2023-07-11 11:34:44,327][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:34:44,332][233954] mean_value=-491.0863969174214, max_value=689.3251297190669[0m
[37m[1m[2023-07-11 11:34:44,335][233954] New mean coefficients: [[-0.0217062   0.5504989   0.16776665  0.8342097   0.6133914  -0.15595034]][0m
[37m[1m[2023-07-11 11:34:44,336][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:34:53,331][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 11:34:53,331][233954] FPS: 426999.42[0m
[36m[2023-07-11 11:34:53,333][233954] itr=856, itrs=2000, Progress: 42.80%[0m
[36m[2023-07-11 11:35:05,023][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 11:35:05,023][233954] FPS: 331143.34[0m
[36m[2023-07-11 11:35:09,326][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:35:09,326][233954] Reward + Measures: [[51.50907186  0.24900466  0.34358332  0.47868598  0.40910664  1.03401184]][0m
[37m[1m[2023-07-11 11:35:09,326][233954] Max Reward on eval: 51.5090718627346[0m
[37m[1m[2023-07-11 11:35:09,327][233954] Min Reward on eval: 51.5090718627346[0m
[37m[1m[2023-07-11 11:35:09,327][233954] Mean Reward across all agents: 51.5090718627346[0m
[37m[1m[2023-07-11 11:35:09,327][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:35:14,570][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:35:14,571][233954] Reward + Measures: [[127.41614087   0.0293       0.20920001   0.25110003   0.2843
    1.24672258]
 [ 53.89831899   0.1239       0.43759999   0.17410001   0.4082
    1.67473185]
 [144.2169834    0.1473       0.91499996   0.41549999   0.87050003
    2.15379   ]
 ...
 [217.43608413   0.3409       0.41459998   0.46040002   0.57429999
    1.73446167]
 [253.06699874   0.0768       0.90620005   0.51420003   0.86980003
    2.19713855]
 [218.40214562   0.164        0.44350001   0.396        0.52060002
    1.54637575]][0m
[37m[1m[2023-07-11 11:35:14,571][233954] Max Reward on eval: 436.24497610023246[0m
[37m[1m[2023-07-11 11:35:14,571][233954] Min Reward on eval: -122.54870774580631[0m
[37m[1m[2023-07-11 11:35:14,571][233954] Mean Reward across all agents: 115.26098351143398[0m
[37m[1m[2023-07-11 11:35:14,572][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:35:14,575][233954] mean_value=-549.8180229353509, max_value=359.188353375558[0m
[37m[1m[2023-07-11 11:35:14,577][233954] New mean coefficients: [[-0.10116906  0.28129166  0.15714768  0.7364651   0.5049636  -0.20216663]][0m
[37m[1m[2023-07-11 11:35:14,578][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:35:23,511][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 11:35:23,511][233954] FPS: 429970.76[0m
[36m[2023-07-11 11:35:23,513][233954] itr=857, itrs=2000, Progress: 42.85%[0m
[36m[2023-07-11 11:35:35,240][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 11:35:35,240][233954] FPS: 330003.35[0m
[36m[2023-07-11 11:35:39,547][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:35:39,548][233954] Reward + Measures: [[48.97855006  0.28339499  0.39516968  0.50293303  0.46037564  1.0296315 ]][0m
[37m[1m[2023-07-11 11:35:39,548][233954] Max Reward on eval: 48.97855006294186[0m
[37m[1m[2023-07-11 11:35:39,548][233954] Min Reward on eval: 48.97855006294186[0m
[37m[1m[2023-07-11 11:35:39,549][233954] Mean Reward across all agents: 48.97855006294186[0m
[37m[1m[2023-07-11 11:35:39,549][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:35:44,582][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:35:44,583][233954] Reward + Measures: [[  -2.69328579    0.44619998    0.64679998    0.46799999    0.63119996
     1.51373327]
 [-134.27495803    0.48090002    0.74919999    0.31880003    0.68259996
     1.81405413]
 [   9.43955428    0.69999999    0.87519997    0.64510006    0.83540004
     2.03012919]
 ...
 [  42.65676738    0.67600006    0.76650006    0.59869999    0.83920002
     1.91172981]
 [ 149.47804629    0.22760001    0.33200002    0.3944        0.47389999
     1.25070798]
 [   9.83364249    0.71749997    0.85050005    0.66240007    0.81689996
     1.95448709]][0m
[37m[1m[2023-07-11 11:35:44,583][233954] Max Reward on eval: 320.53582968879493[0m
[37m[1m[2023-07-11 11:35:44,583][233954] Min Reward on eval: -157.189393908903[0m
[37m[1m[2023-07-11 11:35:44,584][233954] Mean Reward across all agents: 5.794958582118153[0m
[37m[1m[2023-07-11 11:35:44,584][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:35:44,587][233954] mean_value=-260.42352765492654, max_value=661.8177310647256[0m
[37m[1m[2023-07-11 11:35:44,590][233954] New mean coefficients: [[-0.08467939  0.1456484   0.10811038  0.587373    0.32374227 -0.33257237]][0m
[37m[1m[2023-07-11 11:35:44,591][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:35:53,655][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 11:35:53,655][233954] FPS: 423734.08[0m
[36m[2023-07-11 11:35:53,657][233954] itr=858, itrs=2000, Progress: 42.90%[0m
[36m[2023-07-11 11:36:05,305][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 11:36:05,305][233954] FPS: 332271.64[0m
[36m[2023-07-11 11:36:09,599][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:36:09,604][233954] Reward + Measures: [[46.95574351  0.328852    0.46733698  0.53653002  0.52433038  1.09279203]][0m
[37m[1m[2023-07-11 11:36:09,604][233954] Max Reward on eval: 46.955743510358744[0m
[37m[1m[2023-07-11 11:36:09,605][233954] Min Reward on eval: 46.955743510358744[0m
[37m[1m[2023-07-11 11:36:09,605][233954] Mean Reward across all agents: 46.955743510358744[0m
[37m[1m[2023-07-11 11:36:09,605][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:36:14,624][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:36:14,629][233954] Reward + Measures: [[ 18.74968589   0.1347       0.33670002   0.31110001   0.3908
    1.24760556]
 [-58.63574087   0.42200002   0.57700008   0.4262       0.55129999
    1.56051052]
 [108.26178356   0.32370001   0.25009999   0.4217       0.4612
    1.24151659]
 ...
 [105.15786395   0.16250001   0.33180004   0.32080004   0.43949994
    1.25578618]
 [ 38.98760999   0.2102       0.59439999   0.36559999   0.61930001
    1.58888888]
 [210.24866968   0.0217       0.35420001   0.40310001   0.38930002
    1.28204787]][0m
[37m[1m[2023-07-11 11:36:14,630][233954] Max Reward on eval: 360.6213081870228[0m
[37m[1m[2023-07-11 11:36:14,630][233954] Min Reward on eval: -59.83281316040084[0m
[37m[1m[2023-07-11 11:36:14,630][233954] Mean Reward across all agents: 77.16990626743437[0m
[37m[1m[2023-07-11 11:36:14,630][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:36:14,635][233954] mean_value=-662.4585926596804, max_value=499.8762469261326[0m
[37m[1m[2023-07-11 11:36:14,638][233954] New mean coefficients: [[-0.18866009 -0.06974937  0.14768137  0.4324767  -0.00328636 -0.66879624]][0m
[37m[1m[2023-07-11 11:36:14,639][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:36:23,679][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 11:36:23,680][233954] FPS: 424847.35[0m
[36m[2023-07-11 11:36:23,682][233954] itr=859, itrs=2000, Progress: 42.95%[0m
[36m[2023-07-11 11:36:35,347][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 11:36:35,347][233954] FPS: 331834.47[0m
[36m[2023-07-11 11:36:39,659][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:36:39,660][233954] Reward + Measures: [[50.35621416  0.34018099  0.46759135  0.54900831  0.5210973   1.06044698]][0m
[37m[1m[2023-07-11 11:36:39,660][233954] Max Reward on eval: 50.35621415845978[0m
[37m[1m[2023-07-11 11:36:39,660][233954] Min Reward on eval: 50.35621415845978[0m
[37m[1m[2023-07-11 11:36:39,660][233954] Mean Reward across all agents: 50.35621415845978[0m
[37m[1m[2023-07-11 11:36:39,661][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:36:44,725][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:36:44,725][233954] Reward + Measures: [[ 30.5545909    0.149        0.24680002   0.1383       0.24489999
    1.69764137]
 [132.63639462   0.2924       0.48880002   0.41409999   0.49659997
    1.53906751]
 [225.16044018   0.27309999   0.72610003   0.37319997   0.69040006
    1.78726697]
 ...
 [  8.25335081   0.5776       0.72399998   0.5413       0.74089998
    1.91111088]
 [-51.05813387   0.361        0.67830002   0.1594       0.67690003
    2.29834676]
 [  4.16709706   0.59830004   0.76440001   0.61790001   0.75190002
    1.50277925]][0m
[37m[1m[2023-07-11 11:36:44,725][233954] Max Reward on eval: 474.121936800424[0m
[37m[1m[2023-07-11 11:36:44,726][233954] Min Reward on eval: -126.76917139440775[0m
[37m[1m[2023-07-11 11:36:44,726][233954] Mean Reward across all agents: 80.1848696058899[0m
[37m[1m[2023-07-11 11:36:44,726][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:36:44,731][233954] mean_value=-286.19704173006403, max_value=539.83082058689[0m
[37m[1m[2023-07-11 11:36:44,734][233954] New mean coefficients: [[-0.23714513 -0.00264131 -0.16045935  0.38486338  0.12587146 -0.6167699 ]][0m
[37m[1m[2023-07-11 11:36:44,735][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:36:53,833][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 11:36:53,833][233954] FPS: 422158.73[0m
[36m[2023-07-11 11:36:53,835][233954] itr=860, itrs=2000, Progress: 43.00%[0m
[37m[1m[2023-07-11 11:40:12,817][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000840[0m
[36m[2023-07-11 11:40:25,048][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 11:40:25,048][233954] FPS: 330799.82[0m
[36m[2023-07-11 11:40:29,242][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:40:29,243][233954] Reward + Measures: [[44.59477973  0.33537501  0.45464367  0.55163467  0.51927829  1.0343771 ]][0m
[37m[1m[2023-07-11 11:40:29,243][233954] Max Reward on eval: 44.59477972843416[0m
[37m[1m[2023-07-11 11:40:29,243][233954] Min Reward on eval: 44.59477972843416[0m
[37m[1m[2023-07-11 11:40:29,243][233954] Mean Reward across all agents: 44.59477972843416[0m
[37m[1m[2023-07-11 11:40:29,244][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:40:34,116][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:40:34,116][233954] Reward + Measures: [[ 73.82975863   0.1163       0.2342       0.3348       0.35530001
    1.02655816]
 [ 17.83916903   0.24490002   0.3576       0.33640003   0.40229997
    1.04182136]
 [138.79748867   0.19749999   0.41929999   0.40049997   0.51819998
    1.39390397]
 ...
 [132.67390155   0.31849998   0.7198       0.4323       0.67870003
    1.5408231 ]
 [  2.02217937   0.2773       0.92059994   0.13209999   0.90390009
    2.12925529]
 [ 35.8904667    0.6024       0.76080006   0.7208001    0.75930005
    1.0355258 ]][0m
[37m[1m[2023-07-11 11:40:34,116][233954] Max Reward on eval: 612.7738418603782[0m
[37m[1m[2023-07-11 11:40:34,117][233954] Min Reward on eval: -131.24330926239492[0m
[37m[1m[2023-07-11 11:40:34,117][233954] Mean Reward across all agents: 95.5331402427884[0m
[37m[1m[2023-07-11 11:40:34,117][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:40:34,123][233954] mean_value=-461.89935217178544, max_value=756.7932717982679[0m
[37m[1m[2023-07-11 11:40:34,125][233954] New mean coefficients: [[-0.2910517  -0.2714345  -0.1020706   0.255102    0.02065976 -0.9555839 ]][0m
[37m[1m[2023-07-11 11:40:34,126][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:40:43,069][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 11:40:43,069][233954] FPS: 429464.87[0m
[36m[2023-07-11 11:40:43,072][233954] itr=861, itrs=2000, Progress: 43.05%[0m
[36m[2023-07-11 11:40:54,651][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 11:40:54,651][233954] FPS: 334365.63[0m
[36m[2023-07-11 11:40:58,858][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:40:58,858][233954] Reward + Measures: [[41.97792549  0.349446    0.45142603  0.51410002  0.52925634  0.96630532]][0m
[37m[1m[2023-07-11 11:40:58,859][233954] Max Reward on eval: 41.97792549320392[0m
[37m[1m[2023-07-11 11:40:58,859][233954] Min Reward on eval: 41.97792549320392[0m
[37m[1m[2023-07-11 11:40:58,859][233954] Mean Reward across all agents: 41.97792549320392[0m
[37m[1m[2023-07-11 11:40:58,859][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:41:03,818][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:41:03,819][233954] Reward + Measures: [[-32.14122562   0.61430001   0.82860005   0.55070007   0.78310007
    1.98569   ]
 [ -1.70398502   0.87490004   0.91960001   0.83939999   0.90479994
    1.84098625]
 [ -6.43379787   0.85070002   0.90290004   0.79649997   0.89790004
    1.70646608]
 ...
 [  0.92430525   0.4587       0.82429999   0.49020004   0.70679998
    1.90418279]
 [  6.02402598   0.61580002   0.96340001   0.60810006   0.93069994
    2.34512877]
 [-23.68244092   0.53719997   0.82770008   0.5151       0.78330004
    2.03600717]][0m
[37m[1m[2023-07-11 11:41:03,819][233954] Max Reward on eval: 397.62612382853405[0m
[37m[1m[2023-07-11 11:41:03,819][233954] Min Reward on eval: -180.81940950062125[0m
[37m[1m[2023-07-11 11:41:03,819][233954] Mean Reward across all agents: 22.8131958379998[0m
[37m[1m[2023-07-11 11:41:03,820][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:41:03,824][233954] mean_value=-278.66115879263145, max_value=614.6074153119596[0m
[37m[1m[2023-07-11 11:41:03,827][233954] New mean coefficients: [[-0.21716818  0.02268389 -0.16952425 -0.08640248 -0.09381439 -1.1481004 ]][0m
[37m[1m[2023-07-11 11:41:03,828][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:41:12,796][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 11:41:12,796][233954] FPS: 428240.17[0m
[36m[2023-07-11 11:41:12,799][233954] itr=862, itrs=2000, Progress: 43.10%[0m
[36m[2023-07-11 11:41:24,431][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 11:41:24,431][233954] FPS: 332712.87[0m
[36m[2023-07-11 11:41:28,714][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:41:28,714][233954] Reward + Measures: [[37.43401213  0.31879032  0.40553197  0.518879    0.49197233  0.92196012]][0m
[37m[1m[2023-07-11 11:41:28,714][233954] Max Reward on eval: 37.43401213141524[0m
[37m[1m[2023-07-11 11:41:28,715][233954] Min Reward on eval: 37.43401213141524[0m
[37m[1m[2023-07-11 11:41:28,715][233954] Mean Reward across all agents: 37.43401213141524[0m
[37m[1m[2023-07-11 11:41:28,715][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:41:33,884][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:41:33,885][233954] Reward + Measures: [[-12.48817993   0.40830001   0.97679996   0.1402       0.9695999
    2.59093118]
 [ 41.0179501    0.28509998   0.38470003   0.3599       0.37979999
    1.1276387 ]
 [ 72.51315237   0.0786       0.23049998   0.30309999   0.28420001
    0.97693676]
 ...
 [ 11.18477898   0.4657       0.5334       0.42720005   0.49109998
    1.04033375]
 [ 21.05495318   0.20820001   0.22490001   0.37900001   0.36590001
    1.10214138]
 [  4.75438436   0.79950005   0.85459995   0.73519999   0.89659995
    1.65907609]][0m
[37m[1m[2023-07-11 11:41:33,885][233954] Max Reward on eval: 332.7239310835488[0m
[37m[1m[2023-07-11 11:41:33,885][233954] Min Reward on eval: -176.35974120013415[0m
[37m[1m[2023-07-11 11:41:33,886][233954] Mean Reward across all agents: 46.403196227057485[0m
[37m[1m[2023-07-11 11:41:33,886][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:41:33,891][233954] mean_value=-362.0784634614572, max_value=781.7180911098607[0m
[37m[1m[2023-07-11 11:41:33,894][233954] New mean coefficients: [[-0.16893655  0.04429618 -0.20451686 -0.25529808  0.01584427 -1.2868732 ]][0m
[37m[1m[2023-07-11 11:41:33,895][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:41:42,836][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 11:41:42,837][233954] FPS: 429527.79[0m
[36m[2023-07-11 11:41:42,839][233954] itr=863, itrs=2000, Progress: 43.15%[0m
[36m[2023-07-11 11:41:54,353][233954] train() took 11.42 seconds to complete[0m
[36m[2023-07-11 11:41:54,353][233954] FPS: 336156.77[0m
[36m[2023-07-11 11:41:58,598][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:41:58,603][233954] Reward + Measures: [[34.93723183  0.30749533  0.36951765  0.50733131  0.47706896  0.8483125 ]][0m
[37m[1m[2023-07-11 11:41:58,604][233954] Max Reward on eval: 34.937231832153394[0m
[37m[1m[2023-07-11 11:41:58,605][233954] Min Reward on eval: 34.937231832153394[0m
[37m[1m[2023-07-11 11:41:58,605][233954] Mean Reward across all agents: 34.937231832153394[0m
[37m[1m[2023-07-11 11:41:58,606][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:42:03,552][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:42:03,552][233954] Reward + Measures: [[ -9.62672105   0.26040003   0.38200003   0.244        0.38999999
    1.67770195]
 [ 11.70449246   0.33970001   0.435        0.3743       0.4219
    1.2004149 ]
 [ 22.5065565    0.24870001   0.917        0.2192       0.85719997
    2.08764529]
 ...
 [ 70.07603926   0.73640001   0.85409993   0.6954       0.83430004
    2.05576587]
 [ 35.71430787   0.2938       0.3283       0.396        0.3897
    0.89213294]
 [-18.53994714   0.60149997   0.84230006   0.53730005   0.79080003
    1.78616929]][0m
[37m[1m[2023-07-11 11:42:03,552][233954] Max Reward on eval: 313.63321782052515[0m
[37m[1m[2023-07-11 11:42:03,553][233954] Min Reward on eval: -132.93302487842737[0m
[37m[1m[2023-07-11 11:42:03,553][233954] Mean Reward across all agents: 33.310841783804214[0m
[37m[1m[2023-07-11 11:42:03,553][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:42:03,557][233954] mean_value=-327.0406388838943, max_value=519.5580096911478[0m
[37m[1m[2023-07-11 11:42:03,560][233954] New mean coefficients: [[-0.16828385 -0.22217941 -0.11019548 -0.01616216 -0.10420774 -1.4232908 ]][0m
[37m[1m[2023-07-11 11:42:03,561][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:42:12,493][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 11:42:12,494][233954] FPS: 429970.61[0m
[36m[2023-07-11 11:42:12,496][233954] itr=864, itrs=2000, Progress: 43.20%[0m
[36m[2023-07-11 11:42:24,325][233954] train() took 11.74 seconds to complete[0m
[36m[2023-07-11 11:42:24,325][233954] FPS: 327134.26[0m
[36m[2023-07-11 11:42:28,603][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:42:28,604][233954] Reward + Measures: [[36.97715317  0.27635264  0.32647035  0.51313996  0.44209194  0.8075729 ]][0m
[37m[1m[2023-07-11 11:42:28,604][233954] Max Reward on eval: 36.977153166160726[0m
[37m[1m[2023-07-11 11:42:28,604][233954] Min Reward on eval: 36.977153166160726[0m
[37m[1m[2023-07-11 11:42:28,604][233954] Mean Reward across all agents: 36.977153166160726[0m
[37m[1m[2023-07-11 11:42:28,604][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:42:33,560][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:42:33,561][233954] Reward + Measures: [[101.18232779   0.46479997   0.36020002   0.51520002   0.59100002
    1.53273511]
 [ 60.13696721   0.42720005   0.42999998   0.41780001   0.61970001
    1.30486298]
 [207.92187772   0.1191       0.41780004   0.52609998   0.4869
    1.29362297]
 ...
 [ 77.35443915   0.21140002   0.23890002   0.48660001   0.43920001
    1.19200695]
 [ 25.76276843   0.25320002   0.37270004   0.36520001   0.37490001
    1.03810549]
 [ 18.2712615    0.57980001   0.66549999   0.56739998   0.70090002
    1.10081697]][0m
[37m[1m[2023-07-11 11:42:33,561][233954] Max Reward on eval: 464.21111586280165[0m
[37m[1m[2023-07-11 11:42:33,561][233954] Min Reward on eval: -133.35372590888292[0m
[37m[1m[2023-07-11 11:42:33,562][233954] Mean Reward across all agents: 79.50387468856766[0m
[37m[1m[2023-07-11 11:42:33,562][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:42:33,568][233954] mean_value=-456.7438307298874, max_value=688.87439017467[0m
[37m[1m[2023-07-11 11:42:33,571][233954] New mean coefficients: [[-0.4113053  -0.24409491 -0.0453807   0.2206292  -0.04021217 -1.3710245 ]][0m
[37m[1m[2023-07-11 11:42:33,572][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:42:42,524][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 11:42:42,525][233954] FPS: 429014.68[0m
[36m[2023-07-11 11:42:42,527][233954] itr=865, itrs=2000, Progress: 43.25%[0m
[36m[2023-07-11 11:42:54,078][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 11:42:54,078][233954] FPS: 335078.96[0m
[36m[2023-07-11 11:42:58,397][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:42:58,398][233954] Reward + Measures: [[34.58847125  0.27194235  0.32488498  0.53769398  0.43026599  0.79520863]][0m
[37m[1m[2023-07-11 11:42:58,398][233954] Max Reward on eval: 34.58847124783805[0m
[37m[1m[2023-07-11 11:42:58,398][233954] Min Reward on eval: 34.58847124783805[0m
[37m[1m[2023-07-11 11:42:58,398][233954] Mean Reward across all agents: 34.58847124783805[0m
[37m[1m[2023-07-11 11:42:58,399][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:43:03,415][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:43:03,421][233954] Reward + Measures: [[53.69965506  0.29730001  0.266       0.51969999  0.41630003  1.01376998]
 [25.00288063  0.56050003  0.59499997  0.64770001  0.64250004  0.897771  ]
 [68.18825766  0.22860001  0.15210001  0.24090002  0.2737      1.30444109]
 ...
 [24.48401886  0.29480001  0.34529996  0.47830001  0.3987      1.02056444]
 [51.46644552  0.39919999  0.4052      0.51579994  0.54480004  1.04231417]
 [20.42612171  0.20060001  0.26950002  0.27050003  0.4842      1.24390817]][0m
[37m[1m[2023-07-11 11:43:03,421][233954] Max Reward on eval: 372.87087070345876[0m
[37m[1m[2023-07-11 11:43:03,422][233954] Min Reward on eval: -86.80694201709703[0m
[37m[1m[2023-07-11 11:43:03,422][233954] Mean Reward across all agents: 45.39899921920387[0m
[37m[1m[2023-07-11 11:43:03,422][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:43:03,428][233954] mean_value=-362.5333567065778, max_value=533.6461653802544[0m
[37m[1m[2023-07-11 11:43:03,431][233954] New mean coefficients: [[-0.49912226 -0.25673407 -0.11109009  0.09229399 -0.10986121 -1.4186885 ]][0m
[37m[1m[2023-07-11 11:43:03,432][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:43:12,528][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 11:43:12,528][233954] FPS: 422226.27[0m
[36m[2023-07-11 11:43:12,531][233954] itr=866, itrs=2000, Progress: 43.30%[0m
[36m[2023-07-11 11:43:24,122][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 11:43:24,123][233954] FPS: 334006.84[0m
[36m[2023-07-11 11:43:28,382][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:43:28,383][233954] Reward + Measures: [[30.27065377  0.26667634  0.30697566  0.553455    0.433139    0.73013467]][0m
[37m[1m[2023-07-11 11:43:28,383][233954] Max Reward on eval: 30.27065377080778[0m
[37m[1m[2023-07-11 11:43:28,383][233954] Min Reward on eval: 30.27065377080778[0m
[37m[1m[2023-07-11 11:43:28,384][233954] Mean Reward across all agents: 30.27065377080778[0m
[37m[1m[2023-07-11 11:43:28,384][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:43:33,445][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:43:33,446][233954] Reward + Measures: [[ 25.68496904   0.1073       0.1419       0.36420003   0.2606
    0.93816561]
 [ -3.60229255   0.32820001   0.4815       0.37919998   0.42570001
    1.14054084]
 [ 45.96153747   0.0499       0.1014       0.1514       0.0972
    1.10822523]
 ...
 [ -7.14015886   0.69590008   0.78380007   0.67010003   0.79210001
    1.89002478]
 [-14.89422706   0.53640002   0.67540002   0.56129998   0.6875
    0.95985049]
 [126.21417017   0.0472       0.27940002   0.27670002   0.31720001
    1.47077835]][0m
[37m[1m[2023-07-11 11:43:33,446][233954] Max Reward on eval: 596.4151459071785[0m
[37m[1m[2023-07-11 11:43:33,446][233954] Min Reward on eval: -69.86759883128107[0m
[37m[1m[2023-07-11 11:43:33,446][233954] Mean Reward across all agents: 74.14193062957875[0m
[37m[1m[2023-07-11 11:43:33,447][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:43:33,451][233954] mean_value=-632.723157161154, max_value=807.7447099656798[0m
[37m[1m[2023-07-11 11:43:33,454][233954] New mean coefficients: [[-0.2943298  -0.27468106  0.0464686  -0.06204316 -0.08115836 -1.3651084 ]][0m
[37m[1m[2023-07-11 11:43:33,455][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:43:42,538][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 11:43:42,538][233954] FPS: 422854.12[0m
[36m[2023-07-11 11:43:42,540][233954] itr=867, itrs=2000, Progress: 43.35%[0m
[36m[2023-07-11 11:43:54,319][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 11:43:54,319][233954] FPS: 328565.34[0m
[36m[2023-07-11 11:43:58,571][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:43:58,572][233954] Reward + Measures: [[33.7590046   0.26186967  0.300154    0.53846103  0.43941236  0.71157682]][0m
[37m[1m[2023-07-11 11:43:58,572][233954] Max Reward on eval: 33.759004598021235[0m
[37m[1m[2023-07-11 11:43:58,572][233954] Min Reward on eval: 33.759004598021235[0m
[37m[1m[2023-07-11 11:43:58,572][233954] Mean Reward across all agents: 33.759004598021235[0m
[37m[1m[2023-07-11 11:43:58,573][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:44:03,770][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:44:03,776][233954] Reward + Measures: [[294.89431773   0.43420002   0.71259999   0.46750003   0.78030008
    1.65274513]
 [ 15.36497531   0.58210003   0.67390007   0.55989999   0.6929
    1.63218236]
 [425.18668132   0.28760001   0.81269997   0.41069999   0.79660004
    2.23296762]
 ...
 [ -8.26861135   0.80019999   0.84890002   0.75489998   0.85289997
    1.86330926]
 [178.00076549   0.3759       0.25299999   0.60610002   0.4698
    1.03935945]
 [ 99.38612333   0.35410002   0.50419998   0.61490005   0.55400002
    1.02611673]][0m
[37m[1m[2023-07-11 11:44:03,776][233954] Max Reward on eval: 459.41897105686365[0m
[37m[1m[2023-07-11 11:44:03,777][233954] Min Reward on eval: -91.49951016914565[0m
[37m[1m[2023-07-11 11:44:03,777][233954] Mean Reward across all agents: 77.89558101026091[0m
[37m[1m[2023-07-11 11:44:03,777][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:44:03,784][233954] mean_value=-225.42656449055391, max_value=870.3717085414753[0m
[37m[1m[2023-07-11 11:44:03,787][233954] New mean coefficients: [[-0.0548254  -0.5524901   0.08956495  0.02279618 -0.21680182 -1.0169034 ]][0m
[37m[1m[2023-07-11 11:44:03,788][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:44:12,766][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 11:44:12,766][233954] FPS: 427770.87[0m
[36m[2023-07-11 11:44:12,768][233954] itr=868, itrs=2000, Progress: 43.40%[0m
[36m[2023-07-11 11:44:24,297][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 11:44:24,297][233954] FPS: 335724.46[0m
[36m[2023-07-11 11:44:28,590][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:44:28,590][233954] Reward + Measures: [[30.11426962  0.22944665  0.27519736  0.52348834  0.40923664  0.67471403]][0m
[37m[1m[2023-07-11 11:44:28,590][233954] Max Reward on eval: 30.11426962174702[0m
[37m[1m[2023-07-11 11:44:28,591][233954] Min Reward on eval: 30.11426962174702[0m
[37m[1m[2023-07-11 11:44:28,591][233954] Mean Reward across all agents: 30.11426962174702[0m
[37m[1m[2023-07-11 11:44:28,591][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:44:33,605][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:44:33,610][233954] Reward + Measures: [[76.25652555  0.0441      0.15740001  0.13700001  0.1894      1.19709802]
 [22.08559037  0.18190001  0.4436      0.25740001  0.34449998  1.62640083]
 [14.34677499  0.3321      0.43450004  0.53290004  0.49330005  0.88186282]
 ...
 [20.99674626  0.0395      0.0752      0.0975      0.1089      1.07171381]
 [25.95783927  0.0382      0.0686      0.1002      0.1238      1.01068509]
 [20.58584697  0.18410002  0.24430001  0.25729999  0.28130001  0.97179013]][0m
[37m[1m[2023-07-11 11:44:33,611][233954] Max Reward on eval: 391.44891357915475[0m
[37m[1m[2023-07-11 11:44:33,611][233954] Min Reward on eval: -148.3069519918412[0m
[37m[1m[2023-07-11 11:44:33,611][233954] Mean Reward across all agents: 50.365717991257725[0m
[37m[1m[2023-07-11 11:44:33,611][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:44:33,614][233954] mean_value=-814.9827327550784, max_value=520.5687837570906[0m
[37m[1m[2023-07-11 11:44:33,617][233954] New mean coefficients: [[-0.09522935 -0.25525483 -0.02205102  0.03218367 -0.09019659 -1.060352  ]][0m
[37m[1m[2023-07-11 11:44:33,618][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:44:42,713][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 11:44:42,713][233954] FPS: 422297.56[0m
[36m[2023-07-11 11:44:42,715][233954] itr=869, itrs=2000, Progress: 43.45%[0m
[36m[2023-07-11 11:44:54,469][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 11:44:54,469][233954] FPS: 329273.01[0m
[36m[2023-07-11 11:44:58,802][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:44:58,803][233954] Reward + Measures: [[31.43674584  0.23231468  0.26447135  0.50923532  0.40756837  0.65129018]][0m
[37m[1m[2023-07-11 11:44:58,803][233954] Max Reward on eval: 31.4367458368919[0m
[37m[1m[2023-07-11 11:44:58,803][233954] Min Reward on eval: 31.4367458368919[0m
[37m[1m[2023-07-11 11:44:58,803][233954] Mean Reward across all agents: 31.4367458368919[0m
[37m[1m[2023-07-11 11:44:58,804][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:45:03,759][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:45:03,760][233954] Reward + Measures: [[ 31.04830101   0.70850003   0.79410005   0.68790001   0.76139998
    2.13823509]
 [ 65.73659201   0.5327       0.50560004   0.39899999   0.61510003
    0.89474612]
 [ 18.07705631   0.44600001   0.41890001   0.30640003   0.51310003
    1.05744076]
 ...
 [208.65779675   0.19520001   0.36320001   0.44180003   0.47920004
    1.23044133]
 [ 24.73408169   0.3845       0.42480001   0.52829999   0.48079997
    0.72918224]
 [ 24.23292245   0.1902       0.2155       0.36400002   0.35089999
    0.84606045]][0m
[37m[1m[2023-07-11 11:45:03,760][233954] Max Reward on eval: 443.8379406485648[0m
[37m[1m[2023-07-11 11:45:03,760][233954] Min Reward on eval: -40.358312622550876[0m
[37m[1m[2023-07-11 11:45:03,760][233954] Mean Reward across all agents: 43.35398545294777[0m
[37m[1m[2023-07-11 11:45:03,761][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:45:03,766][233954] mean_value=-460.5671304530311, max_value=525.9736085860059[0m
[37m[1m[2023-07-11 11:45:03,769][233954] New mean coefficients: [[-0.00612652 -0.102511   -0.04351012 -0.11868472 -0.01095866 -0.7738584 ]][0m
[37m[1m[2023-07-11 11:45:03,770][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:45:12,717][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 11:45:12,718][233954] FPS: 429236.86[0m
[36m[2023-07-11 11:45:12,720][233954] itr=870, itrs=2000, Progress: 43.50%[0m
[37m[1m[2023-07-11 11:48:39,773][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000850[0m
[36m[2023-07-11 11:48:52,225][233954] train() took 11.83 seconds to complete[0m
[36m[2023-07-11 11:48:52,225][233954] FPS: 324500.62[0m
[36m[2023-07-11 11:48:56,408][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:48:56,409][233954] Reward + Measures: [[32.21293268  0.21836665  0.24896565  0.46297169  0.36025569  0.60603362]][0m
[37m[1m[2023-07-11 11:48:56,409][233954] Max Reward on eval: 32.21293267949915[0m
[37m[1m[2023-07-11 11:48:56,409][233954] Min Reward on eval: 32.21293267949915[0m
[37m[1m[2023-07-11 11:48:56,409][233954] Mean Reward across all agents: 32.21293267949915[0m
[37m[1m[2023-07-11 11:48:56,410][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:49:01,411][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:49:01,416][233954] Reward + Measures: [[64.01787897  0.45670006  0.43340001  0.49780002  0.52870005  0.99387503]
 [20.29817305  0.35969999  0.41889998  0.43700004  0.42529997  1.10859585]
 [14.82965492  0.2141      0.2493      0.32390001  0.31640002  1.06208456]
 ...
 [34.01446435  0.3188      0.42210004  0.43249997  0.43969998  0.93913555]
 [ 0.22998422  0.43940002  0.59260005  0.42920002  0.56450003  1.1277467 ]
 [12.47164843  0.39140001  0.55849999  0.41289997  0.48259997  1.17409933]][0m
[37m[1m[2023-07-11 11:49:01,417][233954] Max Reward on eval: 384.3339092928218[0m
[37m[1m[2023-07-11 11:49:01,417][233954] Min Reward on eval: -16.150877803063487[0m
[37m[1m[2023-07-11 11:49:01,417][233954] Mean Reward across all agents: 38.90574765312412[0m
[37m[1m[2023-07-11 11:49:01,418][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:49:01,422][233954] mean_value=-514.8844267104513, max_value=524.9541557369987[0m
[37m[1m[2023-07-11 11:49:01,425][233954] New mean coefficients: [[ 0.03921492  0.01684663  0.02705833  0.02894274  0.01296407 -0.3541215 ]][0m
[37m[1m[2023-07-11 11:49:01,426][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:49:10,359][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 11:49:10,359][233954] FPS: 429954.74[0m
[36m[2023-07-11 11:49:10,361][233954] itr=871, itrs=2000, Progress: 43.55%[0m
[36m[2023-07-11 11:49:21,924][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 11:49:21,924][233954] FPS: 334712.52[0m
[36m[2023-07-11 11:49:26,161][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:49:26,162][233954] Reward + Measures: [[34.65169929  0.21271032  0.23025532  0.46967533  0.377592    0.60088861]][0m
[37m[1m[2023-07-11 11:49:26,162][233954] Max Reward on eval: 34.65169928520809[0m
[37m[1m[2023-07-11 11:49:26,162][233954] Min Reward on eval: 34.65169928520809[0m
[37m[1m[2023-07-11 11:49:26,163][233954] Mean Reward across all agents: 34.65169928520809[0m
[37m[1m[2023-07-11 11:49:26,163][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:49:31,109][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:49:31,109][233954] Reward + Measures: [[21.26556528  0.11939999  0.0557      0.19660001  0.1973      0.72754312]
 [28.30988524  0.20370002  0.23190001  0.34809998  0.30539998  0.64699525]
 [29.42033399  0.0195      0.0869      0.42829999  0.26950002  0.47383174]
 ...
 [89.33450665  0.0389      0.19380002  0.20539999  0.18730001  1.38852537]
 [12.78075369  0.77960002  0.88150007  0.74449998  0.85550004  1.58426988]
 [30.9862813   0.28240001  0.3696      0.31949997  0.34199998  0.99898607]][0m
[37m[1m[2023-07-11 11:49:31,110][233954] Max Reward on eval: 383.14029678278604[0m
[37m[1m[2023-07-11 11:49:31,110][233954] Min Reward on eval: -47.32214709976688[0m
[37m[1m[2023-07-11 11:49:31,110][233954] Mean Reward across all agents: 43.71185104996137[0m
[37m[1m[2023-07-11 11:49:31,110][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:49:31,115][233954] mean_value=-499.2312103734435, max_value=528.2236845490522[0m
[37m[1m[2023-07-11 11:49:31,118][233954] New mean coefficients: [[ 0.0022947  -0.01476916  0.01738228 -0.09245607 -0.02367049 -0.4042136 ]][0m
[37m[1m[2023-07-11 11:49:31,119][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:49:40,102][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 11:49:40,102][233954] FPS: 427576.39[0m
[36m[2023-07-11 11:49:40,104][233954] itr=872, itrs=2000, Progress: 43.60%[0m
[36m[2023-07-11 11:49:51,758][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 11:49:51,759][233954] FPS: 332250.06[0m
[36m[2023-07-11 11:49:55,964][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:49:55,970][233954] Reward + Measures: [[32.23471799  0.21691234  0.22801133  0.450479    0.37637866  0.58450872]][0m
[37m[1m[2023-07-11 11:49:55,970][233954] Max Reward on eval: 32.23471798734331[0m
[37m[1m[2023-07-11 11:49:55,970][233954] Min Reward on eval: 32.23471798734331[0m
[37m[1m[2023-07-11 11:49:55,970][233954] Mean Reward across all agents: 32.23471798734331[0m
[37m[1m[2023-07-11 11:49:55,971][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:50:00,909][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:50:00,909][233954] Reward + Measures: [[23.00235419  0.1344      0.13980001  0.18090001  0.25369999  0.84606498]
 [27.59942363  0.1218      0.1628      0.2421      0.27779999  0.57733482]
 [60.90425779  0.0671      0.2483      0.18450001  0.18789999  1.07385147]
 ...
 [27.31599223  0.26349998  0.34870002  0.37420002  0.3396      0.94789886]
 [69.07853155  0.3134      0.2368      0.33829999  0.3547      0.94983369]
 [49.98653746  0.12990001  0.24319999  0.28580001  0.30940002  1.27521694]][0m
[37m[1m[2023-07-11 11:50:00,910][233954] Max Reward on eval: 377.99452077038586[0m
[37m[1m[2023-07-11 11:50:00,910][233954] Min Reward on eval: -87.90393268372864[0m
[37m[1m[2023-07-11 11:50:00,910][233954] Mean Reward across all agents: 29.12967654192277[0m
[37m[1m[2023-07-11 11:50:00,910][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:50:00,915][233954] mean_value=-428.87564295485777, max_value=526.2756593217142[0m
[37m[1m[2023-07-11 11:50:00,917][233954] New mean coefficients: [[-0.08328731  0.06591306 -0.13220893 -0.05924844  0.05069736 -0.6125627 ]][0m
[37m[1m[2023-07-11 11:50:00,918][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:50:09,922][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 11:50:09,922][233954] FPS: 426588.18[0m
[36m[2023-07-11 11:50:09,924][233954] itr=873, itrs=2000, Progress: 43.65%[0m
[36m[2023-07-11 11:50:21,508][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 11:50:21,508][233954] FPS: 334100.21[0m
[36m[2023-07-11 11:50:25,798][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:50:25,803][233954] Reward + Measures: [[28.73535169  0.22261867  0.23612766  0.46289966  0.38286498  0.56167555]][0m
[37m[1m[2023-07-11 11:50:25,804][233954] Max Reward on eval: 28.735351693894355[0m
[37m[1m[2023-07-11 11:50:25,804][233954] Min Reward on eval: 28.735351693894355[0m
[37m[1m[2023-07-11 11:50:25,804][233954] Mean Reward across all agents: 28.735351693894355[0m
[37m[1m[2023-07-11 11:50:25,804][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:50:31,061][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:50:31,066][233954] Reward + Measures: [[18.62708263  0.0238      0.08449999  0.32699999  0.24960001  0.80852318]
 [29.62753855  0.0801      0.15430002  0.21880002  0.204       0.65091783]
 [12.50668884  0.40879998  0.2757      0.49000001  0.55120003  0.87743896]
 ...
 [21.22614492  0.18880001  0.23750003  0.42610002  0.30540001  0.59828782]
 [25.41963208  0.1912      0.2536      0.39770001  0.28770003  0.69828159]
 [ 8.10115602  0.41370001  0.6146      0.42070004  0.59390002  1.06374967]][0m
[37m[1m[2023-07-11 11:50:31,067][233954] Max Reward on eval: 317.45775391554923[0m
[37m[1m[2023-07-11 11:50:31,067][233954] Min Reward on eval: -74.12034113309346[0m
[37m[1m[2023-07-11 11:50:31,067][233954] Mean Reward across all agents: 33.9830301915795[0m
[37m[1m[2023-07-11 11:50:31,067][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:50:31,073][233954] mean_value=-266.6233984583568, max_value=646.2075509989634[0m
[37m[1m[2023-07-11 11:50:31,075][233954] New mean coefficients: [[-0.08986972  0.15306973 -0.19871902 -0.01429998  0.09467695 -0.74942064]][0m
[37m[1m[2023-07-11 11:50:31,076][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:50:40,068][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 11:50:40,068][233954] FPS: 427163.66[0m
[36m[2023-07-11 11:50:40,070][233954] itr=874, itrs=2000, Progress: 43.70%[0m
[36m[2023-07-11 11:50:51,590][233954] train() took 11.43 seconds to complete[0m
[36m[2023-07-11 11:50:51,590][233954] FPS: 336003.49[0m
[36m[2023-07-11 11:50:55,827][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:50:55,828][233954] Reward + Measures: [[29.67482594  0.22423369  0.23387134  0.45787898  0.39291331  0.55137485]][0m
[37m[1m[2023-07-11 11:50:55,828][233954] Max Reward on eval: 29.674825936925558[0m
[37m[1m[2023-07-11 11:50:55,828][233954] Min Reward on eval: 29.674825936925558[0m
[37m[1m[2023-07-11 11:50:55,829][233954] Mean Reward across all agents: 29.674825936925558[0m
[37m[1m[2023-07-11 11:50:55,829][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:51:00,776][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:51:00,777][233954] Reward + Measures: [[ 30.58531831   0.2782       0.16559999   0.39919999   0.37
    0.70446128]
 [ 92.11422538   0.1105       0.0451       0.44329998   0.26630002
    0.6967361 ]
 [ 92.02715041   0.1559       0.1468       0.31810004   0.2951
    1.115834  ]
 ...
 [295.60552673   0.1925       0.56209999   0.50510001   0.70069999
    1.85007405]
 [  6.80860446   0.45570001   0.56459999   0.54119998   0.57209998
    0.82423002]
 [103.94830317   0.33000001   0.244        0.40030003   0.47580001
    1.17501283]][0m
[37m[1m[2023-07-11 11:51:00,777][233954] Max Reward on eval: 373.1369946461171[0m
[37m[1m[2023-07-11 11:51:00,777][233954] Min Reward on eval: -61.10662240171805[0m
[37m[1m[2023-07-11 11:51:00,777][233954] Mean Reward across all agents: 32.15463234010876[0m
[37m[1m[2023-07-11 11:51:00,778][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:51:00,782][233954] mean_value=-274.42095352492765, max_value=591.4974953259807[0m
[37m[1m[2023-07-11 11:51:00,785][233954] New mean coefficients: [[-0.17043327  0.00226629 -0.18610416 -0.0386517  -0.0399925  -0.7350154 ]][0m
[37m[1m[2023-07-11 11:51:00,786][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:51:09,742][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 11:51:09,742][233954] FPS: 428863.51[0m
[36m[2023-07-11 11:51:09,744][233954] itr=875, itrs=2000, Progress: 43.75%[0m
[36m[2023-07-11 11:51:21,304][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 11:51:21,305][233954] FPS: 334903.99[0m
[36m[2023-07-11 11:51:25,669][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:51:25,670][233954] Reward + Measures: [[32.04508766  0.22648266  0.23377401  0.44351235  0.40609333  0.56504637]][0m
[37m[1m[2023-07-11 11:51:25,670][233954] Max Reward on eval: 32.04508766364624[0m
[37m[1m[2023-07-11 11:51:25,670][233954] Min Reward on eval: 32.04508766364624[0m
[37m[1m[2023-07-11 11:51:25,671][233954] Mean Reward across all agents: 32.04508766364624[0m
[37m[1m[2023-07-11 11:51:25,671][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:51:30,680][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:51:30,741][233954] Reward + Measures: [[ 8.22122735  0.84310001  0.81680006  0.8082      0.5848      1.15321648]
 [39.35491131  0.57140005  0.72430003  0.65800005  0.61650002  1.00141203]
 [32.10784039  0.27850002  0.33719999  0.37419999  0.37619999  0.7950002 ]
 ...
 [16.47280886  0.289       0.23340002  0.42790005  0.41730005  0.55986422]
 [10.38318469  0.2969      0.34349999  0.1705      0.38519999  1.02852058]
 [19.71682386  0.28480002  0.3186      0.52520007  0.40039998  0.51857752]][0m
[37m[1m[2023-07-11 11:51:30,741][233954] Max Reward on eval: 180.90477441446856[0m
[37m[1m[2023-07-11 11:51:30,742][233954] Min Reward on eval: -10.058424160070718[0m
[37m[1m[2023-07-11 11:51:30,742][233954] Mean Reward across all agents: 30.46646661642409[0m
[37m[1m[2023-07-11 11:51:30,742][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:51:30,748][233954] mean_value=-284.7589850476916, max_value=474.5497663729265[0m
[37m[1m[2023-07-11 11:51:30,751][233954] New mean coefficients: [[-0.16735612  0.01749139 -0.1380156   0.0706265  -0.09030939 -0.688432  ]][0m
[37m[1m[2023-07-11 11:51:30,753][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:51:39,793][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 11:51:39,794][233954] FPS: 424825.35[0m
[36m[2023-07-11 11:51:39,796][233954] itr=876, itrs=2000, Progress: 43.80%[0m
[36m[2023-07-11 11:51:51,386][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 11:51:51,386][233954] FPS: 333968.14[0m
[36m[2023-07-11 11:51:55,683][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:51:55,683][233954] Reward + Measures: [[30.25649496  0.23160799  0.23471834  0.45570466  0.42883033  0.53703582]][0m
[37m[1m[2023-07-11 11:51:55,683][233954] Max Reward on eval: 30.256494963395312[0m
[37m[1m[2023-07-11 11:51:55,684][233954] Min Reward on eval: 30.256494963395312[0m
[37m[1m[2023-07-11 11:51:55,684][233954] Mean Reward across all agents: 30.256494963395312[0m
[37m[1m[2023-07-11 11:51:55,684][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:52:00,683][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:52:00,689][233954] Reward + Measures: [[ 35.44443585   0.3186       0.32269999   0.5115       0.48950002
    0.68660253]
 [ 13.86565095   0.1114       0.1401       0.41940004   0.29030001
    0.54408699]
 [ 35.61663916   0.1019       0.17639999   0.11360001   0.15620001
    1.58587134]
 ...
 [194.79549184   0.17519999   0.37149999   0.51640004   0.52210003
    1.27129972]
 [ -6.10128519   0.63689995   0.82810003   0.61470002   0.82840008
    1.005566  ]
 [-23.8764974    0.44060001   0.59069997   0.4355       0.54540002
    1.11415851]][0m
[37m[1m[2023-07-11 11:52:00,689][233954] Max Reward on eval: 453.75660456302575[0m
[37m[1m[2023-07-11 11:52:00,689][233954] Min Reward on eval: -48.17895219251513[0m
[37m[1m[2023-07-11 11:52:00,690][233954] Mean Reward across all agents: 45.36023872797885[0m
[37m[1m[2023-07-11 11:52:00,690][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:52:00,696][233954] mean_value=-457.2716728015252, max_value=623.8837274085172[0m
[37m[1m[2023-07-11 11:52:00,699][233954] New mean coefficients: [[-0.10527086  0.1448486  -0.30261722 -0.19823647 -0.09898914 -0.8077936 ]][0m
[37m[1m[2023-07-11 11:52:00,700][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:52:09,678][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 11:52:09,679][233954] FPS: 427756.14[0m
[36m[2023-07-11 11:52:09,681][233954] itr=877, itrs=2000, Progress: 43.85%[0m
[36m[2023-07-11 11:52:21,418][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 11:52:21,418][233954] FPS: 329726.39[0m
[36m[2023-07-11 11:52:25,689][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:52:25,690][233954] Reward + Measures: [[29.70002558  0.23561034  0.24354436  0.44071731  0.43752897  0.52806658]][0m
[37m[1m[2023-07-11 11:52:25,690][233954] Max Reward on eval: 29.7000255800206[0m
[37m[1m[2023-07-11 11:52:25,690][233954] Min Reward on eval: 29.7000255800206[0m
[37m[1m[2023-07-11 11:52:25,691][233954] Mean Reward across all agents: 29.7000255800206[0m
[37m[1m[2023-07-11 11:52:25,691][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:52:30,718][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:52:30,719][233954] Reward + Measures: [[31.5588863   0.211       0.25960001  0.2938      0.2987      0.68478507]
 [20.1010192   0.28080001  0.35910001  0.48760006  0.3628      0.59566087]
 [18.04084184  0.28910002  0.31940001  0.4928      0.42640001  0.59650034]
 ...
 [29.81871099  0.61649996  0.58460003  0.66820002  0.68809998  0.69851774]
 [13.49069232  0.20640002  0.2289      0.35370001  0.3971      0.46800873]
 [16.9290962   0.46000001  0.49710003  0.57569999  0.5564      0.61671734]][0m
[37m[1m[2023-07-11 11:52:30,719][233954] Max Reward on eval: 327.1537458005594[0m
[37m[1m[2023-07-11 11:52:30,719][233954] Min Reward on eval: -62.5176367437467[0m
[37m[1m[2023-07-11 11:52:30,719][233954] Mean Reward across all agents: 22.549574312016738[0m
[37m[1m[2023-07-11 11:52:30,720][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:52:30,725][233954] mean_value=-113.13482574006373, max_value=516.8118381580338[0m
[37m[1m[2023-07-11 11:52:30,728][233954] New mean coefficients: [[-0.07815699  0.02595642 -0.1723405  -0.24867931 -0.20123607 -0.913316  ]][0m
[37m[1m[2023-07-11 11:52:30,729][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:52:39,842][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 11:52:39,842][233954] FPS: 421454.25[0m
[36m[2023-07-11 11:52:39,844][233954] itr=878, itrs=2000, Progress: 43.90%[0m
[36m[2023-07-11 11:52:51,561][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 11:52:51,561][233954] FPS: 330370.89[0m
[36m[2023-07-11 11:52:55,866][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:52:55,867][233954] Reward + Measures: [[29.71145797  0.22865434  0.23337699  0.39571631  0.43342569  0.52794254]][0m
[37m[1m[2023-07-11 11:52:55,867][233954] Max Reward on eval: 29.711457965483618[0m
[37m[1m[2023-07-11 11:52:55,867][233954] Min Reward on eval: 29.711457965483618[0m
[37m[1m[2023-07-11 11:52:55,867][233954] Mean Reward across all agents: 29.711457965483618[0m
[37m[1m[2023-07-11 11:52:55,868][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:53:00,861][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:53:00,861][233954] Reward + Measures: [[ 6.41017499  0.53649998  0.57040006  0.59609997  0.74690002  0.48296252]
 [36.69228961  0.0391      0.16160001  0.2467      0.0793      0.84012967]
 [18.81976234  0.2493      0.32879999  0.34109998  0.35600004  0.82417393]
 ...
 [14.53112515  0.1714      0.2811      0.27590001  0.2385      0.89520979]
 [16.14008096  0.23120001  0.67520005  0.18340001  0.65270007  1.87080443]
 [27.26649372  0.2145      0.22780001  0.25470001  0.29809999  0.80819958]][0m
[37m[1m[2023-07-11 11:53:00,861][233954] Max Reward on eval: 148.3483102052938[0m
[37m[1m[2023-07-11 11:53:00,862][233954] Min Reward on eval: -63.5410490159411[0m
[37m[1m[2023-07-11 11:53:00,862][233954] Mean Reward across all agents: 25.80627429851098[0m
[37m[1m[2023-07-11 11:53:00,862][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:53:00,866][233954] mean_value=-474.0048940195345, max_value=434.3712821009476[0m
[37m[1m[2023-07-11 11:53:00,878][233954] New mean coefficients: [[-0.0302335  -0.04093137 -0.13034841 -0.16207358 -0.2302719  -0.7203418 ]][0m
[37m[1m[2023-07-11 11:53:00,880][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:53:09,911][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 11:53:09,911][233954] FPS: 425342.71[0m
[36m[2023-07-11 11:53:09,913][233954] itr=879, itrs=2000, Progress: 43.95%[0m
[36m[2023-07-11 11:53:21,677][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 11:53:21,677][233954] FPS: 329021.64[0m
[36m[2023-07-11 11:53:25,996][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:53:25,996][233954] Reward + Measures: [[28.35372863  0.21287031  0.22034232  0.35628435  0.4015927   0.52102512]][0m
[37m[1m[2023-07-11 11:53:25,996][233954] Max Reward on eval: 28.353728632268126[0m
[37m[1m[2023-07-11 11:53:25,997][233954] Min Reward on eval: 28.353728632268126[0m
[37m[1m[2023-07-11 11:53:25,997][233954] Mean Reward across all agents: 28.353728632268126[0m
[37m[1m[2023-07-11 11:53:25,997][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:53:31,301][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:53:31,302][233954] Reward + Measures: [[ 23.1474357    0.1116       0.20039999   0.37650001   0.24609999
    0.52715182]
 [ 73.9525038    0.12639999   0.1576       0.2667       0.35859999
    0.93682051]
 [  4.68112901   0.80809993   0.91029996   0.7615       0.88300002
    1.89740777]
 ...
 [ -7.04713729   0.48100001   0.65360004   0.4368       0.61770004
    1.03181517]
 [-19.98789395   0.28130001   0.51020002   0.36300001   0.45460001
    1.00353432]
 [ 24.22025833   0.65329999   0.69910002   0.6286       0.74159998
    1.70673525]][0m
[37m[1m[2023-07-11 11:53:31,302][233954] Max Reward on eval: 552.8959350563586[0m
[37m[1m[2023-07-11 11:53:31,303][233954] Min Reward on eval: -53.74575215280056[0m
[37m[1m[2023-07-11 11:53:31,303][233954] Mean Reward across all agents: 54.994699188069994[0m
[37m[1m[2023-07-11 11:53:31,303][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:53:31,309][233954] mean_value=-220.86905375443504, max_value=598.2714846446179[0m
[37m[1m[2023-07-11 11:53:31,311][233954] New mean coefficients: [[ 0.07524958 -0.08201677 -0.08564151 -0.15491997 -0.20791243 -0.53530455]][0m
[37m[1m[2023-07-11 11:53:31,312][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:53:40,451][233954] train() took 9.14 seconds to complete[0m
[36m[2023-07-11 11:53:40,451][233954] FPS: 420270.12[0m
[36m[2023-07-11 11:53:40,454][233954] itr=880, itrs=2000, Progress: 44.00%[0m
[37m[1m[2023-07-11 11:57:06,227][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000860[0m
[36m[2023-07-11 11:57:18,341][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 11:57:18,341][233954] FPS: 331945.20[0m
[36m[2023-07-11 11:57:22,517][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:57:22,518][233954] Reward + Measures: [[26.69574283  0.20744164  0.21989802  0.35503069  0.34612995  0.50742108]][0m
[37m[1m[2023-07-11 11:57:22,518][233954] Max Reward on eval: 26.69574282762803[0m
[37m[1m[2023-07-11 11:57:22,518][233954] Min Reward on eval: 26.69574282762803[0m
[37m[1m[2023-07-11 11:57:22,519][233954] Mean Reward across all agents: 26.69574282762803[0m
[37m[1m[2023-07-11 11:57:22,519][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:57:27,431][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:57:27,432][233954] Reward + Measures: [[25.38760382  0.23559999  0.46509996  0.26519999  0.46090004  1.28012764]
 [30.11503625  0.0181      0.0777      0.1806      0.12230001  0.7690506 ]
 [ 3.28764958  0.1758      0.25169998  0.26799998  0.3409      0.6689114 ]
 ...
 [-7.89761404  0.56989998  0.57470006  0.542       0.6473      0.89640015]
 [-3.5554491   0.24090002  0.35100001  0.39879999  0.41190001  0.88346845]
 [17.61975422  0.13270001  0.1428      0.1734      0.2034      0.98848248]][0m
[37m[1m[2023-07-11 11:57:27,432][233954] Max Reward on eval: 272.57308952456805[0m
[37m[1m[2023-07-11 11:57:27,432][233954] Min Reward on eval: -67.98015528284013[0m
[37m[1m[2023-07-11 11:57:27,433][233954] Mean Reward across all agents: 22.792670217745208[0m
[37m[1m[2023-07-11 11:57:27,433][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:57:27,436][233954] mean_value=-638.9462510744897, max_value=384.9799549428741[0m
[37m[1m[2023-07-11 11:57:27,438][233954] New mean coefficients: [[ 0.00629811 -0.10498016  0.07964549 -0.06062132 -0.10017937 -0.47244132]][0m
[37m[1m[2023-07-11 11:57:27,439][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:57:36,379][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 11:57:36,379][233954] FPS: 429602.68[0m
[36m[2023-07-11 11:57:36,381][233954] itr=881, itrs=2000, Progress: 44.05%[0m
[36m[2023-07-11 11:57:47,959][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 11:57:47,960][233954] FPS: 334382.76[0m
[36m[2023-07-11 11:57:52,233][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:57:52,233][233954] Reward + Measures: [[25.37606113  0.20557199  0.21934366  0.33844036  0.34019101  0.50380909]][0m
[37m[1m[2023-07-11 11:57:52,233][233954] Max Reward on eval: 25.376061130669335[0m
[37m[1m[2023-07-11 11:57:52,233][233954] Min Reward on eval: 25.376061130669335[0m
[37m[1m[2023-07-11 11:57:52,234][233954] Mean Reward across all agents: 25.376061130669335[0m
[37m[1m[2023-07-11 11:57:52,234][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:57:57,273][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:57:57,274][233954] Reward + Measures: [[22.84095227  0.0281      0.0804      0.2053      0.1373      0.55350351]
 [21.96016426  0.0213      0.0412      0.1112      0.14390002  0.64700615]
 [36.51753473  0.32570001  0.42770001  0.33880004  0.50099999  1.38647211]
 ...
 [16.1583222   0.47280002  0.31350002  0.49020001  0.61610001  0.77191275]
 [22.43344227  0.1962      0.24790001  0.36830002  0.33110002  0.48689467]
 [28.06141843  0.0489      0.0861      0.0953      0.0961      0.99574941]][0m
[37m[1m[2023-07-11 11:57:57,274][233954] Max Reward on eval: 152.90002611503004[0m
[37m[1m[2023-07-11 11:57:57,274][233954] Min Reward on eval: -18.873449550010264[0m
[37m[1m[2023-07-11 11:57:57,274][233954] Mean Reward across all agents: 27.403597262892934[0m
[37m[1m[2023-07-11 11:57:57,275][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:57:57,278][233954] mean_value=-409.8923702027782, max_value=527.1377055792138[0m
[37m[1m[2023-07-11 11:57:57,281][233954] New mean coefficients: [[ 0.09276324 -0.10781084  0.06109406 -0.14404437 -0.06634215 -0.25102288]][0m
[37m[1m[2023-07-11 11:57:57,282][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:58:06,263][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 11:58:06,263][233954] FPS: 427669.24[0m
[36m[2023-07-11 11:58:06,265][233954] itr=882, itrs=2000, Progress: 44.10%[0m
[36m[2023-07-11 11:58:18,264][233954] train() took 11.90 seconds to complete[0m
[36m[2023-07-11 11:58:18,264][233954] FPS: 322585.55[0m
[36m[2023-07-11 11:58:22,455][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:58:22,456][233954] Reward + Measures: [[25.77353978  0.22745499  0.23920435  0.32797265  0.35575065  0.50603855]][0m
[37m[1m[2023-07-11 11:58:22,456][233954] Max Reward on eval: 25.77353978357572[0m
[37m[1m[2023-07-11 11:58:22,456][233954] Min Reward on eval: 25.77353978357572[0m
[37m[1m[2023-07-11 11:58:22,457][233954] Mean Reward across all agents: 25.77353978357572[0m
[37m[1m[2023-07-11 11:58:22,457][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:58:27,407][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:58:27,407][233954] Reward + Measures: [[ 30.86509086   0.2113       0.2615       0.40799999   0.5521
    0.54974723]
 [ -3.97346425   0.48410001   0.49200001   0.4815       0.5553
    1.08932662]
 [-11.17465197   0.36200002   0.47809997   0.40160003   0.49330005
    0.835127  ]
 ...
 [-21.2881781    0.37550002   0.4224       0.38439998   0.47259998
    1.02900171]
 [ 21.53149985   0.18539999   0.27519998   0.36180001   0.4147
    0.58066034]
 [ 22.0633111    0.20219998   0.24419999   0.33460003   0.28710002
    0.69734436]][0m
[37m[1m[2023-07-11 11:58:27,408][233954] Max Reward on eval: 92.10393520947545[0m
[37m[1m[2023-07-11 11:58:27,408][233954] Min Reward on eval: -50.777785894274714[0m
[37m[1m[2023-07-11 11:58:27,408][233954] Mean Reward across all agents: 12.561293001173745[0m
[37m[1m[2023-07-11 11:58:27,408][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:58:27,412][233954] mean_value=-301.7291124652291, max_value=420.69693034548453[0m
[37m[1m[2023-07-11 11:58:27,414][233954] New mean coefficients: [[-0.00322895 -0.17612846  0.03423    -0.11364907 -0.14705077 -0.35154837]][0m
[37m[1m[2023-07-11 11:58:27,415][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:58:36,412][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 11:58:36,412][233954] FPS: 426912.07[0m
[36m[2023-07-11 11:58:36,415][233954] itr=883, itrs=2000, Progress: 44.15%[0m
[36m[2023-07-11 11:58:48,121][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 11:58:48,122][233954] FPS: 330686.13[0m
[36m[2023-07-11 11:58:52,374][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:58:52,374][233954] Reward + Measures: [[27.50995658  0.21129802  0.22561932  0.30378866  0.35567567  0.50182778]][0m
[37m[1m[2023-07-11 11:58:52,375][233954] Max Reward on eval: 27.509956575421946[0m
[37m[1m[2023-07-11 11:58:52,375][233954] Min Reward on eval: 27.509956575421946[0m
[37m[1m[2023-07-11 11:58:52,375][233954] Mean Reward across all agents: 27.509956575421946[0m
[37m[1m[2023-07-11 11:58:52,375][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:58:57,380][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:58:57,380][233954] Reward + Measures: [[ 9.4163023   0.36410001  0.39850003  0.4298      0.48209998  0.50199223]
 [19.97424761  0.11900001  0.14        0.2422      0.2484      0.49427438]
 [21.82155417  0.0747      0.12150001  0.17550001  0.26320001  0.57231981]
 ...
 [ 9.57839141  0.56529999  0.62840003  0.54070002  0.67519999  0.8928318 ]
 [31.98494087  0.34590003  0.41619998  0.3858      0.40459999  1.02377129]
 [ 1.64018437  0.39309999  0.58450001  0.44820005  0.55290002  1.01627934]][0m
[37m[1m[2023-07-11 11:58:57,380][233954] Max Reward on eval: 107.88485742858612[0m
[37m[1m[2023-07-11 11:58:57,381][233954] Min Reward on eval: -51.44394931998104[0m
[37m[1m[2023-07-11 11:58:57,381][233954] Mean Reward across all agents: 15.309498250134638[0m
[37m[1m[2023-07-11 11:58:57,381][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:58:57,385][233954] mean_value=-309.0444574193223, max_value=383.8529327574888[0m
[37m[1m[2023-07-11 11:58:57,388][233954] New mean coefficients: [[-0.03514097 -0.18596597  0.02289117 -0.12470712 -0.26563114 -0.36691707]][0m
[37m[1m[2023-07-11 11:58:57,389][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:59:06,365][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 11:59:06,365][233954] FPS: 427864.34[0m
[36m[2023-07-11 11:59:06,368][233954] itr=884, itrs=2000, Progress: 44.20%[0m
[36m[2023-07-11 11:59:18,031][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 11:59:18,031][233954] FPS: 331937.29[0m
[36m[2023-07-11 11:59:22,372][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:59:22,372][233954] Reward + Measures: [[31.33078656  0.17511067  0.185183    0.27124032  0.32448798  0.50414509]][0m
[37m[1m[2023-07-11 11:59:22,372][233954] Max Reward on eval: 31.330786562684207[0m
[37m[1m[2023-07-11 11:59:22,373][233954] Min Reward on eval: 31.330786562684207[0m
[37m[1m[2023-07-11 11:59:22,373][233954] Mean Reward across all agents: 31.330786562684207[0m
[37m[1m[2023-07-11 11:59:22,373][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:59:27,390][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:59:27,391][233954] Reward + Measures: [[ 22.77798831   0.13430001   0.1577       0.20170002   0.22160001
    0.7418763 ]
 [ 15.09405532   0.23980001   0.29819998   0.249        0.33590001
    0.85316098]
 [ -6.75244435   0.20879999   0.39579999   0.24690001   0.47370002
    1.19542539]
 ...
 [  2.51907992   0.19100001   0.37910002   0.22940002   0.46430001
    1.10415006]
 [-45.29339991   0.45030004   0.71550006   0.39610001   0.69270003
    1.40265942]
 [ -1.3785656    0.30870003   0.51940006   0.36050001   0.5291
    1.03222358]][0m
[37m[1m[2023-07-11 11:59:27,391][233954] Max Reward on eval: 122.65207364992239[0m
[37m[1m[2023-07-11 11:59:27,391][233954] Min Reward on eval: -96.58246917100624[0m
[37m[1m[2023-07-11 11:59:27,392][233954] Mean Reward across all agents: 9.945406685062084[0m
[37m[1m[2023-07-11 11:59:27,392][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:59:27,395][233954] mean_value=-398.0217371613373, max_value=460.22051410434767[0m
[37m[1m[2023-07-11 11:59:27,398][233954] New mean coefficients: [[ 0.00381643 -0.11405684  0.09703276  0.03028464 -0.12228321 -0.590899  ]][0m
[37m[1m[2023-07-11 11:59:27,399][233954] Moving the mean solution point...[0m
[36m[2023-07-11 11:59:36,467][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 11:59:36,467][233954] FPS: 423539.85[0m
[36m[2023-07-11 11:59:36,470][233954] itr=885, itrs=2000, Progress: 44.25%[0m
[36m[2023-07-11 11:59:48,193][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 11:59:48,193][233954] FPS: 330210.34[0m
[36m[2023-07-11 11:59:52,477][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:59:52,478][233954] Reward + Measures: [[26.43203033  0.18578     0.20380566  0.30621433  0.35258999  0.47829241]][0m
[37m[1m[2023-07-11 11:59:52,478][233954] Max Reward on eval: 26.432030327026432[0m
[37m[1m[2023-07-11 11:59:52,478][233954] Min Reward on eval: 26.432030327026432[0m
[37m[1m[2023-07-11 11:59:52,479][233954] Mean Reward across all agents: 26.432030327026432[0m
[37m[1m[2023-07-11 11:59:52,479][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:59:57,468][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 11:59:57,469][233954] Reward + Measures: [[ 93.72672731   0.29210001   0.2309       0.3633       0.46949998
    0.59194851]
 [  1.84853745   0.34189999   0.49090001   0.37260002   0.51960003
    1.22683179]
 [ 83.11267113   0.20120001   0.1697       0.3132       0.3795
    0.70196813]
 ...
 [  5.27975244   0.39560002   0.40949997   0.40820003   0.50040001
    0.53060281]
 [ 33.7928042    0.0491       0.0753       0.0942       0.0939
    0.68631542]
 [-37.52734534   0.38439998   0.54320002   0.35850003   0.51910007
    1.04161906]][0m
[37m[1m[2023-07-11 11:59:57,469][233954] Max Reward on eval: 150.83545211218296[0m
[37m[1m[2023-07-11 11:59:57,470][233954] Min Reward on eval: -40.43955459764693[0m
[37m[1m[2023-07-11 11:59:57,470][233954] Mean Reward across all agents: 24.766172272284457[0m
[37m[1m[2023-07-11 11:59:57,470][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 11:59:57,473][233954] mean_value=-433.3010139239786, max_value=211.86462861879278[0m
[37m[1m[2023-07-11 11:59:57,476][233954] New mean coefficients: [[ 0.01002477 -0.13696003  0.06802972  0.04706059 -0.08926846 -0.46754113]][0m
[37m[1m[2023-07-11 11:59:57,477][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:00:06,542][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 12:00:06,542][233954] FPS: 423685.83[0m
[36m[2023-07-11 12:00:06,545][233954] itr=886, itrs=2000, Progress: 44.30%[0m
[36m[2023-07-11 12:00:18,184][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 12:00:18,184][233954] FPS: 332613.73[0m
[36m[2023-07-11 12:00:22,423][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:00:22,424][233954] Reward + Measures: [[28.4658416   0.16673568  0.181492    0.32381368  0.31917799  0.44795451]][0m
[37m[1m[2023-07-11 12:00:22,424][233954] Max Reward on eval: 28.46584160399575[0m
[37m[1m[2023-07-11 12:00:22,424][233954] Min Reward on eval: 28.46584160399575[0m
[37m[1m[2023-07-11 12:00:22,424][233954] Mean Reward across all agents: 28.46584160399575[0m
[37m[1m[2023-07-11 12:00:22,425][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:00:27,634][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:00:27,640][233954] Reward + Measures: [[13.63869711  0.68970007  0.79570001  0.741       0.7482      0.53364331]
 [22.42327946  0.27500001  0.30930004  0.41739997  0.40720001  0.43371391]
 [15.26264834  0.46779999  0.54210001  0.61070001  0.55079997  0.40570909]
 ...
 [10.88374055  0.10659999  0.15060002  0.1829      0.24390002  0.61943489]
 [18.23600111  0.1089      0.14        0.2577      0.25349998  0.50562364]
 [17.39476186  0.18990001  0.15720001  0.32790002  0.2719      0.52606767]][0m
[37m[1m[2023-07-11 12:00:27,640][233954] Max Reward on eval: 315.5476061526686[0m
[37m[1m[2023-07-11 12:00:27,640][233954] Min Reward on eval: -104.56879994701595[0m
[37m[1m[2023-07-11 12:00:27,641][233954] Mean Reward across all agents: 30.099463694273144[0m
[37m[1m[2023-07-11 12:00:27,641][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:00:27,645][233954] mean_value=-407.1117338975804, max_value=626.5961384667551[0m
[37m[1m[2023-07-11 12:00:27,647][233954] New mean coefficients: [[ 0.11402254 -0.24410903  0.14670414  0.04095757 -0.12117469 -0.1426234 ]][0m
[37m[1m[2023-07-11 12:00:27,648][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:00:36,607][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 12:00:36,607][233954] FPS: 428735.63[0m
[36m[2023-07-11 12:00:36,609][233954] itr=887, itrs=2000, Progress: 44.35%[0m
[36m[2023-07-11 12:00:48,375][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 12:00:48,376][233954] FPS: 328912.85[0m
[36m[2023-07-11 12:00:52,737][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:00:52,737][233954] Reward + Measures: [[27.98528897  0.162039    0.17777498  0.32000601  0.30081567  0.4487749 ]][0m
[37m[1m[2023-07-11 12:00:52,737][233954] Max Reward on eval: 27.985288970950137[0m
[37m[1m[2023-07-11 12:00:52,738][233954] Min Reward on eval: 27.985288970950137[0m
[37m[1m[2023-07-11 12:00:52,738][233954] Mean Reward across all agents: 27.985288970950137[0m
[37m[1m[2023-07-11 12:00:52,738][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:00:57,745][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:00:57,746][233954] Reward + Measures: [[27.58516728  0.2059      0.14540002  0.35880002  0.31850001  0.63228363]
 [15.94412902  0.47569999  0.63250005  0.45380002  0.5952      0.98118633]
 [54.80178089  0.14120001  0.06470001  0.19340001  0.21130002  0.85459995]
 ...
 [17.55848891  0.30410001  0.3867      0.39820001  0.43190002  0.55576849]
 [16.69043275  0.11919999  0.19160001  0.20410001  0.1582      0.86026078]
 [34.98280477  0.0299      0.07470001  0.18069999  0.101       0.67253059]][0m
[37m[1m[2023-07-11 12:00:57,746][233954] Max Reward on eval: 131.2881379013881[0m
[37m[1m[2023-07-11 12:00:57,746][233954] Min Reward on eval: -41.640681949071585[0m
[37m[1m[2023-07-11 12:00:57,747][233954] Mean Reward across all agents: 26.447160274477287[0m
[37m[1m[2023-07-11 12:00:57,747][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:00:57,750][233954] mean_value=-765.8045553825198, max_value=423.1897555909203[0m
[37m[1m[2023-07-11 12:00:57,753][233954] New mean coefficients: [[ 0.15742975 -0.21376714  0.1158222   0.03244363  0.04979338 -0.3215741 ]][0m
[37m[1m[2023-07-11 12:00:57,754][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:01:06,854][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 12:01:06,854][233954] FPS: 422043.91[0m
[36m[2023-07-11 12:01:06,857][233954] itr=888, itrs=2000, Progress: 44.40%[0m
[36m[2023-07-11 12:01:18,739][233954] train() took 11.79 seconds to complete[0m
[36m[2023-07-11 12:01:18,740][233954] FPS: 325697.58[0m
[36m[2023-07-11 12:01:23,039][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:01:23,039][233954] Reward + Measures: [[26.34354649  0.16299433  0.19022566  0.33773366  0.30626997  0.4417786 ]][0m
[37m[1m[2023-07-11 12:01:23,040][233954] Max Reward on eval: 26.34354649163156[0m
[37m[1m[2023-07-11 12:01:23,040][233954] Min Reward on eval: 26.34354649163156[0m
[37m[1m[2023-07-11 12:01:23,040][233954] Mean Reward across all agents: 26.34354649163156[0m
[37m[1m[2023-07-11 12:01:23,040][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:01:27,968][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:01:27,970][233954] Reward + Measures: [[ 7.38486771  0.56859994  0.86430007  0.5722      0.81380004  1.69946861]
 [23.71453514  0.0628      0.0554      0.1086      0.14500001  0.63130563]
 [32.51334549  0.0438      0.0969      0.1621      0.1418      0.47931415]
 ...
 [62.53412264  0.29369998  0.23659997  0.40130001  0.37549999  0.94009918]
 [ 7.12075619  0.2999      0.31860003  0.32590002  0.35530001  0.88334411]
 [63.61873049  0.37490001  0.22060001  0.42570001  0.45790002  0.74257493]][0m
[37m[1m[2023-07-11 12:01:27,971][233954] Max Reward on eval: 99.52289247321896[0m
[37m[1m[2023-07-11 12:01:27,971][233954] Min Reward on eval: -39.787085184222086[0m
[37m[1m[2023-07-11 12:01:27,971][233954] Mean Reward across all agents: 22.459668477471666[0m
[37m[1m[2023-07-11 12:01:27,972][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:01:27,975][233954] mean_value=-574.2670156773199, max_value=525.207385049446[0m
[37m[1m[2023-07-11 12:01:27,978][233954] New mean coefficients: [[ 0.15998861 -0.12079565  0.07232095  0.02646678  0.06933589 -0.5491189 ]][0m
[37m[1m[2023-07-11 12:01:27,979][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:01:36,985][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 12:01:36,986][233954] FPS: 426419.12[0m
[36m[2023-07-11 12:01:36,988][233954] itr=889, itrs=2000, Progress: 44.45%[0m
[36m[2023-07-11 12:01:48,818][233954] train() took 11.74 seconds to complete[0m
[36m[2023-07-11 12:01:48,818][233954] FPS: 327151.17[0m
[36m[2023-07-11 12:01:53,077][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:01:53,078][233954] Reward + Measures: [[23.68845673  0.18932332  0.22078133  0.37064901  0.34194601  0.43174502]][0m
[37m[1m[2023-07-11 12:01:53,078][233954] Max Reward on eval: 23.688456726375705[0m
[37m[1m[2023-07-11 12:01:53,078][233954] Min Reward on eval: 23.688456726375705[0m
[37m[1m[2023-07-11 12:01:53,078][233954] Mean Reward across all agents: 23.688456726375705[0m
[37m[1m[2023-07-11 12:01:53,079][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:01:58,107][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:01:58,108][233954] Reward + Measures: [[ 14.73161956   0.2586       0.33270001   0.4113       0.39540002
    0.55853212]
 [ 26.00757944   0.1218       0.16690001   0.26950002   0.2185
    0.61136574]
 [  7.74922226   0.60299999   0.64300001   0.55930007   0.74750006
    1.58887196]
 ...
 [ 18.0731163    0.20439999   0.26629999   0.42749998   0.31710002
    0.37301776]
 [ 11.98405135   0.1481       0.2897       0.24600001   0.40450001
    0.83780998]
 [-10.47079574   0.46440002   0.63220006   0.42010003   0.68530005
    0.83475727]][0m
[37m[1m[2023-07-11 12:01:58,108][233954] Max Reward on eval: 464.12763259150086[0m
[37m[1m[2023-07-11 12:01:58,108][233954] Min Reward on eval: -75.15311597548425[0m
[37m[1m[2023-07-11 12:01:58,108][233954] Mean Reward across all agents: 10.769719425054355[0m
[37m[1m[2023-07-11 12:01:58,109][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:01:58,112][233954] mean_value=-251.70845449722376, max_value=383.102246229889[0m
[37m[1m[2023-07-11 12:01:58,115][233954] New mean coefficients: [[ 0.11550418 -0.18339312  0.0565447   0.00241909  0.07488897 -0.6638163 ]][0m
[37m[1m[2023-07-11 12:01:58,116][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:02:07,075][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 12:02:07,075][233954] FPS: 428706.53[0m
[36m[2023-07-11 12:02:07,078][233954] itr=890, itrs=2000, Progress: 44.50%[0m
[37m[1m[2023-07-11 12:05:41,364][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000870[0m
[36m[2023-07-11 12:05:53,543][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 12:05:53,543][233954] FPS: 330176.21[0m
[36m[2023-07-11 12:05:57,711][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:05:57,711][233954] Reward + Measures: [[26.59959092  0.18913399  0.21579634  0.39907435  0.34065896  0.44865087]][0m
[37m[1m[2023-07-11 12:05:57,712][233954] Max Reward on eval: 26.599590921669538[0m
[37m[1m[2023-07-11 12:05:57,712][233954] Min Reward on eval: 26.599590921669538[0m
[37m[1m[2023-07-11 12:05:57,712][233954] Mean Reward across all agents: 26.599590921669538[0m
[37m[1m[2023-07-11 12:05:57,712][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:06:02,540][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:06:02,540][233954] Reward + Measures: [[33.2837565   0.0401      0.14049999  0.21820001  0.24089999  0.60743898]
 [15.42352178  0.29200003  0.35139999  0.38499999  0.34639999  0.66691607]
 [ 5.54957056  0.3669      0.39120001  0.45689997  0.46970001  0.49801546]
 ...
 [13.00638003  0.5474      0.59390002  0.6645      0.61869997  0.59938496]
 [21.33213005  0.0722      0.1437      0.32610002  0.2297      0.57399482]
 [ 9.85173331  0.14600001  0.27970001  0.2978      0.26540002  0.57163125]][0m
[37m[1m[2023-07-11 12:06:02,540][233954] Max Reward on eval: 152.9192162432708[0m
[37m[1m[2023-07-11 12:06:02,541][233954] Min Reward on eval: -44.44257165151648[0m
[37m[1m[2023-07-11 12:06:02,541][233954] Mean Reward across all agents: 18.408192105839454[0m
[37m[1m[2023-07-11 12:06:02,541][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:06:02,545][233954] mean_value=-440.4861670227831, max_value=520.2529004168697[0m
[37m[1m[2023-07-11 12:06:02,548][233954] New mean coefficients: [[ 0.1173377  -0.19164482  0.03926012 -0.05001678  0.10682172 -0.70286983]][0m
[37m[1m[2023-07-11 12:06:02,549][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:06:11,502][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 12:06:11,508][233954] FPS: 428963.72[0m
[36m[2023-07-11 12:06:11,511][233954] itr=891, itrs=2000, Progress: 44.55%[0m
[36m[2023-07-11 12:06:23,170][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 12:06:23,170][233954] FPS: 332074.03[0m
[36m[2023-07-11 12:06:27,085][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:06:27,091][233954] Reward + Measures: [[24.48361143  0.18730533  0.22087133  0.39758399  0.339818    0.42578474]][0m
[37m[1m[2023-07-11 12:06:27,091][233954] Max Reward on eval: 24.4836114321411[0m
[37m[1m[2023-07-11 12:06:27,091][233954] Min Reward on eval: 24.4836114321411[0m
[37m[1m[2023-07-11 12:06:27,091][233954] Mean Reward across all agents: 24.4836114321411[0m
[37m[1m[2023-07-11 12:06:27,092][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:06:32,328][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:06:32,334][233954] Reward + Measures: [[ -3.71872703   0.70819998   0.74610007   0.74590009   0.76440001
    0.43798456]
 [  4.87945199   0.3457       0.39570004   0.4752       0.47399998
    0.43790516]
 [ 52.4845364    0.0992       0.12290001   0.2494       0.19490001
    0.72190899]
 ...
 [ 27.73865985   0.26090002   0.36819997   0.35730001   0.44280002
    0.84217471]
 [ 65.01557765   0.27490002   0.22910002   0.25350001   0.37080002
    1.00638878]
 [-16.23803605   0.42610002   0.64579999   0.4786       0.579
    1.05362761]][0m
[37m[1m[2023-07-11 12:06:32,334][233954] Max Reward on eval: 96.6974558361806[0m
[37m[1m[2023-07-11 12:06:32,334][233954] Min Reward on eval: -55.666952959261835[0m
[37m[1m[2023-07-11 12:06:32,335][233954] Mean Reward across all agents: 16.548193751775944[0m
[37m[1m[2023-07-11 12:06:32,335][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:06:32,339][233954] mean_value=-284.13378482024183, max_value=460.7026385023794[0m
[37m[1m[2023-07-11 12:06:32,342][233954] New mean coefficients: [[ 0.08441091 -0.2192975   0.1525392   0.01663918  0.15010265 -0.75108874]][0m
[37m[1m[2023-07-11 12:06:32,342][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:06:41,207][233954] train() took 8.86 seconds to complete[0m
[36m[2023-07-11 12:06:41,208][233954] FPS: 433250.41[0m
[36m[2023-07-11 12:06:41,210][233954] itr=892, itrs=2000, Progress: 44.60%[0m
[36m[2023-07-11 12:06:52,847][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 12:06:52,847][233954] FPS: 332598.78[0m
[36m[2023-07-11 12:06:57,128][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:06:57,133][233954] Reward + Measures: [[28.10861681  0.17912102  0.21826734  0.43117931  0.33971134  0.41760659]][0m
[37m[1m[2023-07-11 12:06:57,133][233954] Max Reward on eval: 28.10861681047016[0m
[37m[1m[2023-07-11 12:06:57,134][233954] Min Reward on eval: 28.10861681047016[0m
[37m[1m[2023-07-11 12:06:57,134][233954] Mean Reward across all agents: 28.10861681047016[0m
[37m[1m[2023-07-11 12:06:57,134][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:07:02,060][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:07:02,061][233954] Reward + Measures: [[17.20813564  0.0219      0.0371      0.19260001  0.19250003  0.32661077]
 [17.5178829   0.2705      0.35180002  0.30469999  0.33969998  1.04713237]
 [10.01787889  0.46080002  0.52039999  0.63350004  0.54320002  0.58899564]
 ...
 [43.93164098  0.1295      0.06420001  0.183       0.18370001  0.89366865]
 [28.05524643  0.0195      0.0613      0.22589998  0.20469999  0.3503302 ]
 [33.42708612  0.20050001  0.29360005  0.42680001  0.33230001  0.53283012]][0m
[37m[1m[2023-07-11 12:07:02,061][233954] Max Reward on eval: 270.6220983015286[0m
[37m[1m[2023-07-11 12:07:02,061][233954] Min Reward on eval: -35.07595313768834[0m
[37m[1m[2023-07-11 12:07:02,061][233954] Mean Reward across all agents: 35.17843684033743[0m
[37m[1m[2023-07-11 12:07:02,062][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:07:02,066][233954] mean_value=-577.7589415514803, max_value=523.188136342261[0m
[37m[1m[2023-07-11 12:07:02,068][233954] New mean coefficients: [[ 0.00981513 -0.3183778   0.17906253  0.03967699  0.07017165 -0.75995827]][0m
[37m[1m[2023-07-11 12:07:02,069][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:07:11,005][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 12:07:11,010][233954] FPS: 429834.38[0m
[36m[2023-07-11 12:07:11,012][233954] itr=893, itrs=2000, Progress: 44.65%[0m
[36m[2023-07-11 12:07:22,840][233954] train() took 11.73 seconds to complete[0m
[36m[2023-07-11 12:07:22,840][233954] FPS: 327244.31[0m
[36m[2023-07-11 12:07:27,055][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:07:27,055][233954] Reward + Measures: [[26.29898765  0.158563    0.19534899  0.41907832  0.30730599  0.41360515]][0m
[37m[1m[2023-07-11 12:07:27,055][233954] Max Reward on eval: 26.298987645552096[0m
[37m[1m[2023-07-11 12:07:27,056][233954] Min Reward on eval: 26.298987645552096[0m
[37m[1m[2023-07-11 12:07:27,056][233954] Mean Reward across all agents: 26.298987645552096[0m
[37m[1m[2023-07-11 12:07:27,056][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:07:32,020][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:07:32,020][233954] Reward + Measures: [[ 8.12473746  0.55800003  0.73690003  0.50360006  0.70450002  1.71836364]
 [11.15725856  0.38660002  0.61939996  0.54540002  0.4993      0.56684911]
 [-6.32046531  0.6408      0.74690002  0.6767      0.75170004  0.77876151]
 ...
 [19.65992213  0.108       0.17349999  0.32409999  0.21870001  0.47011003]
 [20.93913644  0.1842      0.26130003  0.34999999  0.29700002  0.54715747]
 [22.5645464   0.11129999  0.1478      0.30379999  0.24460001  0.38941613]][0m
[37m[1m[2023-07-11 12:07:32,021][233954] Max Reward on eval: 122.77294000536203[0m
[37m[1m[2023-07-11 12:07:32,021][233954] Min Reward on eval: -58.26762810237706[0m
[37m[1m[2023-07-11 12:07:32,021][233954] Mean Reward across all agents: 16.669381572903298[0m
[37m[1m[2023-07-11 12:07:32,021][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:07:32,025][233954] mean_value=-219.0943690123038, max_value=421.18789107324534[0m
[37m[1m[2023-07-11 12:07:32,028][233954] New mean coefficients: [[ 0.04894713 -0.28644764  0.16353312  0.06510657  0.04792982 -0.79427457]][0m
[37m[1m[2023-07-11 12:07:32,029][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:07:40,990][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 12:07:40,990][233954] FPS: 428595.99[0m
[36m[2023-07-11 12:07:40,993][233954] itr=894, itrs=2000, Progress: 44.70%[0m
[36m[2023-07-11 12:07:52,709][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 12:07:52,709][233954] FPS: 330426.14[0m
[36m[2023-07-11 12:07:57,010][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:07:57,011][233954] Reward + Measures: [[25.16352334  0.15137634  0.19329968  0.42322001  0.31759268  0.40489623]][0m
[37m[1m[2023-07-11 12:07:57,011][233954] Max Reward on eval: 25.163523340447874[0m
[37m[1m[2023-07-11 12:07:57,011][233954] Min Reward on eval: 25.163523340447874[0m
[37m[1m[2023-07-11 12:07:57,012][233954] Mean Reward across all agents: 25.163523340447874[0m
[37m[1m[2023-07-11 12:07:57,012][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:08:02,051][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:08:02,051][233954] Reward + Measures: [[13.59179322  0.0261      0.1443      0.2667      0.15390001  0.57145017]
 [ 1.92666553  0.87309998  0.88810009  0.81339997  0.89490002  1.55933607]
 [30.15184035  0.36520001  0.35620004  0.52080005  0.45159999  0.46104422]
 ...
 [31.98286498  0.36530003  0.36610001  0.5521      0.43560001  0.47006449]
 [ 0.32730781  0.70390004  0.78200001  0.67449999  0.76940006  1.52782297]
 [53.04616739  0.14369999  0.0419      0.29620001  0.25869998  0.56725043]][0m
[37m[1m[2023-07-11 12:08:02,052][233954] Max Reward on eval: 144.98854659888895[0m
[37m[1m[2023-07-11 12:08:02,052][233954] Min Reward on eval: -68.4778828321956[0m
[37m[1m[2023-07-11 12:08:02,052][233954] Mean Reward across all agents: 10.18686002278248[0m
[37m[1m[2023-07-11 12:08:02,052][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:08:02,056][233954] mean_value=-355.0082836963188, max_value=263.6753683172982[0m
[37m[1m[2023-07-11 12:08:02,058][233954] New mean coefficients: [[ 0.071348   -0.22225189  0.17211926  0.04398805 -0.01012639 -0.5512455 ]][0m
[37m[1m[2023-07-11 12:08:02,059][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:08:10,994][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 12:08:10,994][233954] FPS: 429869.82[0m
[36m[2023-07-11 12:08:10,996][233954] itr=895, itrs=2000, Progress: 44.75%[0m
[36m[2023-07-11 12:08:22,634][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 12:08:22,634][233954] FPS: 332746.83[0m
[36m[2023-07-11 12:08:26,987][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:08:26,987][233954] Reward + Measures: [[25.92310386  0.15672399  0.20216233  0.43212667  0.32023799  0.40606281]][0m
[37m[1m[2023-07-11 12:08:26,987][233954] Max Reward on eval: 25.9231038582071[0m
[37m[1m[2023-07-11 12:08:26,988][233954] Min Reward on eval: 25.9231038582071[0m
[37m[1m[2023-07-11 12:08:26,988][233954] Mean Reward across all agents: 25.9231038582071[0m
[37m[1m[2023-07-11 12:08:26,988][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:08:32,033][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:08:32,034][233954] Reward + Measures: [[106.44937698   0.29170001   0.62009996   0.57959998   0.59300005
    1.0050205 ]
 [ 15.52387134   0.35339999   0.2139       0.47110006   0.44759998
    0.57998908]
 [ 16.63977004   0.3732       0.41100001   0.51840001   0.46350002
    0.31077749]
 ...
 [-19.3744716    0.28190002   0.49379998   0.3752       0.47910005
    0.85646784]
 [ 32.16243969   0.0277       0.098        0.2942       0.1504
    0.43801457]
 [ 64.12606477   0.12270001   0.0581       0.27449998   0.2422
    0.78750181]][0m
[37m[1m[2023-07-11 12:08:32,034][233954] Max Reward on eval: 414.36978730522094[0m
[37m[1m[2023-07-11 12:08:32,034][233954] Min Reward on eval: -43.229044556897136[0m
[37m[1m[2023-07-11 12:08:32,034][233954] Mean Reward across all agents: 31.199626259003203[0m
[37m[1m[2023-07-11 12:08:32,035][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:08:32,040][233954] mean_value=-373.8075868758976, max_value=624.8929394375347[0m
[37m[1m[2023-07-11 12:08:32,043][233954] New mean coefficients: [[ 0.10135679 -0.19239375  0.20685092  0.01816196  0.02322203 -0.62332547]][0m
[37m[1m[2023-07-11 12:08:32,044][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:08:41,136][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 12:08:41,136][233954] FPS: 422405.42[0m
[36m[2023-07-11 12:08:41,139][233954] itr=896, itrs=2000, Progress: 44.80%[0m
[36m[2023-07-11 12:08:52,969][233954] train() took 11.74 seconds to complete[0m
[36m[2023-07-11 12:08:52,969][233954] FPS: 327209.10[0m
[36m[2023-07-11 12:08:57,260][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:08:57,261][233954] Reward + Measures: [[22.18427539  0.18929799  0.23299031  0.46658128  0.34679264  0.38020378]][0m
[37m[1m[2023-07-11 12:08:57,261][233954] Max Reward on eval: 22.184275391935852[0m
[37m[1m[2023-07-11 12:08:57,261][233954] Min Reward on eval: 22.184275391935852[0m
[37m[1m[2023-07-11 12:08:57,262][233954] Mean Reward across all agents: 22.184275391935852[0m
[37m[1m[2023-07-11 12:08:57,262][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:09:02,557][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:09:02,562][233954] Reward + Measures: [[14.36472718  0.54030001  0.60360003  0.65900004  0.57730001  0.49185911]
 [16.42624062  0.27070001  0.37450001  0.36410001  0.3493      0.76517171]
 [30.39216293  0.20460001  0.22770002  0.35600001  0.26859999  0.88439506]
 ...
 [22.80667957  0.111       0.16360001  0.35180002  0.20560001  0.51601905]
 [23.62782286  0.29160002  0.32600001  0.48590001  0.41510001  0.41676116]
 [22.30170761  0.6631      0.815       0.55599999  0.77030003  1.63321435]][0m
[37m[1m[2023-07-11 12:09:02,563][233954] Max Reward on eval: 335.4832859080285[0m
[37m[1m[2023-07-11 12:09:02,563][233954] Min Reward on eval: -65.91489035785199[0m
[37m[1m[2023-07-11 12:09:02,563][233954] Mean Reward across all agents: 19.567032145100942[0m
[37m[1m[2023-07-11 12:09:02,564][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:09:02,569][233954] mean_value=-302.4789504554, max_value=528.6279375689104[0m
[37m[1m[2023-07-11 12:09:02,571][233954] New mean coefficients: [[ 0.12974943 -0.1264987   0.11523624  0.0317957   0.11539994 -0.66797525]][0m
[37m[1m[2023-07-11 12:09:02,572][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:09:11,675][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 12:09:11,675][233954] FPS: 421960.49[0m
[36m[2023-07-11 12:09:11,677][233954] itr=897, itrs=2000, Progress: 44.85%[0m
[36m[2023-07-11 12:09:23,279][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 12:09:23,279][233954] FPS: 333644.47[0m
[36m[2023-07-11 12:09:27,545][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:09:27,550][233954] Reward + Measures: [[25.57651272  0.18439567  0.23460466  0.46123865  0.34723333  0.39728373]][0m
[37m[1m[2023-07-11 12:09:27,551][233954] Max Reward on eval: 25.576512721107605[0m
[37m[1m[2023-07-11 12:09:27,551][233954] Min Reward on eval: 25.576512721107605[0m
[37m[1m[2023-07-11 12:09:27,551][233954] Mean Reward across all agents: 25.576512721107605[0m
[37m[1m[2023-07-11 12:09:27,552][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:09:32,551][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:09:32,557][233954] Reward + Measures: [[43.92382587  0.30029997  0.42300001  0.49590001  0.4666      0.60111135]
 [25.86348662  0.22910002  0.2297      0.26030001  0.31290001  0.38547891]
 [ 9.17821534  0.52310002  0.56890005  0.62459999  0.6171      0.38116941]
 ...
 [29.50079344  0.0195      0.09940001  0.33559999  0.18789999  0.45210153]
 [-1.87961221  0.44800001  0.61960006  0.57010001  0.60250002  0.72449654]
 [ 1.27647724  0.4777      0.5927      0.5266      0.59020001  0.84846252]][0m
[37m[1m[2023-07-11 12:09:32,557][233954] Max Reward on eval: 228.88460094733165[0m
[37m[1m[2023-07-11 12:09:32,557][233954] Min Reward on eval: -44.31090243421495[0m
[37m[1m[2023-07-11 12:09:32,558][233954] Mean Reward across all agents: 24.672859229168797[0m
[37m[1m[2023-07-11 12:09:32,558][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:09:32,562][233954] mean_value=-160.64723021360894, max_value=582.0416309785098[0m
[37m[1m[2023-07-11 12:09:32,565][233954] New mean coefficients: [[ 0.15219058 -0.17778884  0.31603575  0.05759871  0.17300318 -0.4840287 ]][0m
[37m[1m[2023-07-11 12:09:32,566][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:09:41,585][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 12:09:41,586][233954] FPS: 425836.13[0m
[36m[2023-07-11 12:09:41,588][233954] itr=898, itrs=2000, Progress: 44.90%[0m
[36m[2023-07-11 12:09:53,233][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 12:09:53,233][233954] FPS: 332450.05[0m
[36m[2023-07-11 12:09:57,617][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:09:57,622][233954] Reward + Measures: [[22.33460503  0.17983599  0.22862634  0.43441904  0.34191832  0.39354745]][0m
[37m[1m[2023-07-11 12:09:57,623][233954] Max Reward on eval: 22.33460502604624[0m
[37m[1m[2023-07-11 12:09:57,623][233954] Min Reward on eval: 22.33460502604624[0m
[37m[1m[2023-07-11 12:09:57,623][233954] Mean Reward across all agents: 22.33460502604624[0m
[37m[1m[2023-07-11 12:09:57,623][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:10:02,664][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:10:02,670][233954] Reward + Measures: [[13.50242785  0.5905      0.6997      0.54700005  0.68689996  1.26025903]
 [58.97803821  0.30669999  0.27140003  0.36939999  0.36129999  0.70041937]
 [-3.51340118  0.63310003  0.66049999  0.64370006  0.74779999  0.87252516]
 ...
 [-7.26124081  0.40599999  0.59560007  0.46520001  0.55170006  0.92850554]
 [14.29241197  0.11249999  0.13520001  0.1569      0.18980001  0.71076649]
 [22.1949834   0.1276      0.17290001  0.21069999  0.1816      0.83665794]][0m
[37m[1m[2023-07-11 12:10:02,670][233954] Max Reward on eval: 116.9425668457523[0m
[37m[1m[2023-07-11 12:10:02,670][233954] Min Reward on eval: -48.981875306228176[0m
[37m[1m[2023-07-11 12:10:02,671][233954] Mean Reward across all agents: 15.016284576761691[0m
[37m[1m[2023-07-11 12:10:02,671][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:10:02,674][233954] mean_value=-716.1251395938907, max_value=345.67501876809933[0m
[37m[1m[2023-07-11 12:10:02,676][233954] New mean coefficients: [[ 0.12780584 -0.16857879  0.18649808  0.1287308   0.1988505  -0.74823797]][0m
[37m[1m[2023-07-11 12:10:02,677][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:10:11,730][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 12:10:11,730][233954] FPS: 424260.75[0m
[36m[2023-07-11 12:10:11,733][233954] itr=899, itrs=2000, Progress: 44.95%[0m
[36m[2023-07-11 12:10:23,537][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 12:10:23,537][233954] FPS: 328115.81[0m
[36m[2023-07-11 12:10:27,787][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:10:27,787][233954] Reward + Measures: [[25.65299128  0.15868533  0.19871834  0.47915536  0.34006399  0.39591488]][0m
[37m[1m[2023-07-11 12:10:27,787][233954] Max Reward on eval: 25.652991276564542[0m
[37m[1m[2023-07-11 12:10:27,787][233954] Min Reward on eval: 25.652991276564542[0m
[37m[1m[2023-07-11 12:10:27,788][233954] Mean Reward across all agents: 25.652991276564542[0m
[37m[1m[2023-07-11 12:10:27,788][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:10:32,740][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:10:32,741][233954] Reward + Measures: [[18.00302877  0.1373      0.16290002  0.18669999  0.28550002  0.62869281]
 [39.32505822  0.1481      0.25549999  0.3475      0.32630002  0.60923862]
 [22.11728996  0.1832      0.21690002  0.39680001  0.35660002  0.51742738]
 ...
 [ 0.78434586  0.4941      0.6807      0.46479997  0.63730001  1.27248704]
 [70.63088905  0.19850001  0.1996      0.39750001  0.30080003  0.70725292]
 [ 7.77100667  0.31459996  0.45880005  0.35019997  0.41670004  0.89154905]][0m
[37m[1m[2023-07-11 12:10:32,741][233954] Max Reward on eval: 125.60676752142608[0m
[37m[1m[2023-07-11 12:10:32,742][233954] Min Reward on eval: -54.307401683926585[0m
[37m[1m[2023-07-11 12:10:32,742][233954] Mean Reward across all agents: 12.91814333407171[0m
[37m[1m[2023-07-11 12:10:32,742][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:10:32,745][233954] mean_value=-285.65896982883726, max_value=472.00000649164025[0m
[37m[1m[2023-07-11 12:10:32,748][233954] New mean coefficients: [[ 0.13721387 -0.08959369  0.06322382  0.08579567  0.2519311  -0.7828335 ]][0m
[37m[1m[2023-07-11 12:10:32,749][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:10:41,731][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 12:10:41,731][233954] FPS: 427584.97[0m
[36m[2023-07-11 12:10:41,734][233954] itr=900, itrs=2000, Progress: 45.00%[0m
[37m[1m[2023-07-11 12:14:11,198][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000880[0m
[36m[2023-07-11 12:14:23,414][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 12:14:23,414][233954] FPS: 329836.84[0m
[36m[2023-07-11 12:14:27,659][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:14:27,660][233954] Reward + Measures: [[24.98266736  0.159117    0.199057    0.4464353   0.34369633  0.39391014]][0m
[37m[1m[2023-07-11 12:14:27,660][233954] Max Reward on eval: 24.982667361563248[0m
[37m[1m[2023-07-11 12:14:27,660][233954] Min Reward on eval: 24.982667361563248[0m
[37m[1m[2023-07-11 12:14:27,661][233954] Mean Reward across all agents: 24.982667361563248[0m
[37m[1m[2023-07-11 12:14:27,661][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:14:32,665][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:14:32,666][233954] Reward + Measures: [[ 10.6681155    0.3646       0.40100002   0.49149999   0.47620001
    0.56283659]
 [ 25.98428968   0.116        0.13740002   0.33330002   0.37709999
    0.50428814]
 [  0.34069327   0.12980001   0.31420001   0.28709999   0.25390002
    0.97578013]
 ...
 [ 15.34517908   0.11180001   0.1314       0.38530001   0.23140001
    0.59836453]
 [-33.72761804   0.3671       0.62680006   0.36070004   0.63880008
    1.11679125]
 [ 16.2990139    0.0339       0.098        0.14240001   0.1207
    0.84457266]][0m
[37m[1m[2023-07-11 12:14:32,666][233954] Max Reward on eval: 110.75985676203854[0m
[37m[1m[2023-07-11 12:14:32,666][233954] Min Reward on eval: -33.72761804359034[0m
[37m[1m[2023-07-11 12:14:32,666][233954] Mean Reward across all agents: 26.03899788074336[0m
[37m[1m[2023-07-11 12:14:32,667][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:14:32,670][233954] mean_value=-222.35476601742087, max_value=310.1205034223736[0m
[37m[1m[2023-07-11 12:14:32,673][233954] New mean coefficients: [[ 0.14597762 -0.06465224  0.00153736  0.01356892  0.23479912 -0.7764971 ]][0m
[37m[1m[2023-07-11 12:14:32,674][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:14:41,747][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 12:14:41,748][233954] FPS: 423310.86[0m
[36m[2023-07-11 12:14:41,750][233954] itr=901, itrs=2000, Progress: 45.05%[0m
[36m[2023-07-11 12:14:53,502][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 12:14:53,502][233954] FPS: 329340.03[0m
[36m[2023-07-11 12:14:57,746][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:14:57,746][233954] Reward + Measures: [[29.83522726  0.14426833  0.17419134  0.43432364  0.35476863  0.40621427]][0m
[37m[1m[2023-07-11 12:14:57,746][233954] Max Reward on eval: 29.83522726056135[0m
[37m[1m[2023-07-11 12:14:57,747][233954] Min Reward on eval: 29.83522726056135[0m
[37m[1m[2023-07-11 12:14:57,747][233954] Mean Reward across all agents: 29.83522726056135[0m
[37m[1m[2023-07-11 12:14:57,747][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:15:02,902][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:15:02,903][233954] Reward + Measures: [[14.59143109  0.2992      0.3612      0.39180002  0.39870003  0.48859674]
 [-7.01150604  0.70670003  0.73979998  0.62819999  0.7985      1.71927226]
 [18.49724675  0.22010003  0.29620001  0.34779999  0.27760002  0.48021927]
 ...
 [18.82271495  0.11430001  0.30599999  0.33039999  0.17479999  0.66486424]
 [10.64480012  0.19230001  0.2545      0.3612      0.30230001  0.64482999]
 [23.0686261   0.1171      0.15620001  0.1929      0.1904      0.74110776]][0m
[37m[1m[2023-07-11 12:15:02,903][233954] Max Reward on eval: 92.52811913462355[0m
[37m[1m[2023-07-11 12:15:02,904][233954] Min Reward on eval: -67.99817770970985[0m
[37m[1m[2023-07-11 12:15:02,904][233954] Mean Reward across all agents: 18.759519841499987[0m
[37m[1m[2023-07-11 12:15:02,904][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:15:02,907][233954] mean_value=-558.6402501140362, max_value=525.0181889517232[0m
[37m[1m[2023-07-11 12:15:02,910][233954] New mean coefficients: [[ 0.10632608 -0.02200673  0.0449394   0.00148175  0.09879203 -0.8044162 ]][0m
[37m[1m[2023-07-11 12:15:02,911][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:15:11,873][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 12:15:11,874][233954] FPS: 428518.40[0m
[36m[2023-07-11 12:15:11,876][233954] itr=902, itrs=2000, Progress: 45.10%[0m
[36m[2023-07-11 12:15:23,583][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 12:15:23,583][233954] FPS: 330638.84[0m
[36m[2023-07-11 12:15:27,822][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:15:27,822][233954] Reward + Measures: [[26.58682071  0.13972634  0.17698999  0.46887699  0.332151    0.39425471]][0m
[37m[1m[2023-07-11 12:15:27,822][233954] Max Reward on eval: 26.58682070611588[0m
[37m[1m[2023-07-11 12:15:27,823][233954] Min Reward on eval: 26.58682070611588[0m
[37m[1m[2023-07-11 12:15:27,823][233954] Mean Reward across all agents: 26.58682070611588[0m
[37m[1m[2023-07-11 12:15:27,823][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:15:32,762][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:15:32,762][233954] Reward + Measures: [[25.71366829  0.111       0.16        0.39919999  0.26809999  0.38470253]
 [19.51173093  0.77679998  0.90200007  0.73120004  0.8603999   1.88759315]
 [21.50052381  0.21440001  0.14189999  0.25389999  0.3019      0.53388351]
 ...
 [23.17954288  0.13510001  0.2069      0.22950001  0.30200002  0.62247658]
 [66.07352767  0.3829      0.338       0.44269997  0.43919998  0.70519704]
 [28.64394259  0.0232      0.097       0.39310002  0.205       0.50352794]][0m
[37m[1m[2023-07-11 12:15:32,763][233954] Max Reward on eval: 336.5212368972134[0m
[37m[1m[2023-07-11 12:15:32,763][233954] Min Reward on eval: -64.70802977164276[0m
[37m[1m[2023-07-11 12:15:32,763][233954] Mean Reward across all agents: 22.84023688350501[0m
[37m[1m[2023-07-11 12:15:32,763][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:15:32,769][233954] mean_value=-217.30643202958038, max_value=569.8022556326353[0m
[37m[1m[2023-07-11 12:15:32,772][233954] New mean coefficients: [[ 0.12524083 -0.03120331  0.24862657 -0.04775053  0.18711734 -0.3801111 ]][0m
[37m[1m[2023-07-11 12:15:32,773][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:15:41,747][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 12:15:41,747][233954] FPS: 427980.53[0m
[36m[2023-07-11 12:15:41,750][233954] itr=903, itrs=2000, Progress: 45.15%[0m
[36m[2023-07-11 12:15:53,404][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 12:15:53,404][233954] FPS: 332249.13[0m
[36m[2023-07-11 12:15:57,669][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:15:57,669][233954] Reward + Measures: [[26.9645915   0.13442467  0.17313565  0.46579     0.32755798  0.39228407]][0m
[37m[1m[2023-07-11 12:15:57,669][233954] Max Reward on eval: 26.964591501452908[0m
[37m[1m[2023-07-11 12:15:57,669][233954] Min Reward on eval: 26.964591501452908[0m
[37m[1m[2023-07-11 12:15:57,670][233954] Mean Reward across all agents: 26.964591501452908[0m
[37m[1m[2023-07-11 12:15:57,670][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:16:02,678][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:16:02,679][233954] Reward + Measures: [[21.52742135  0.368       0.44420004  0.66790003  0.51730007  0.51720232]
 [ 9.41950426  0.2357      0.37619996  0.47320005  0.37560001  0.82478058]
 [22.34663621  0.35789999  0.49240002  0.55150002  0.52520001  0.4719319 ]
 ...
 [-1.24375425  0.72750008  0.90920013  0.67320007  0.88710004  1.73295808]
 [ 1.93210863  0.7098      0.87169999  0.66950005  0.88260001  1.57235515]
 [18.8560326   0.22259998  0.2624      0.25260001  0.2586      0.67602623]][0m
[37m[1m[2023-07-11 12:16:02,679][233954] Max Reward on eval: 91.31072759479284[0m
[37m[1m[2023-07-11 12:16:02,679][233954] Min Reward on eval: -106.11030007516965[0m
[37m[1m[2023-07-11 12:16:02,680][233954] Mean Reward across all agents: 9.875669225988508[0m
[37m[1m[2023-07-11 12:16:02,680][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:16:02,683][233954] mean_value=-255.41028723693614, max_value=422.13476000907406[0m
[37m[1m[2023-07-11 12:16:02,686][233954] New mean coefficients: [[ 0.15343279  0.0300996   0.14396784 -0.08843303  0.15535383 -0.5028886 ]][0m
[37m[1m[2023-07-11 12:16:02,687][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:16:11,736][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 12:16:11,736][233954] FPS: 424453.96[0m
[36m[2023-07-11 12:16:11,738][233954] itr=904, itrs=2000, Progress: 45.20%[0m
[36m[2023-07-11 12:16:23,399][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 12:16:23,400][233954] FPS: 331979.43[0m
[36m[2023-07-11 12:16:27,713][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:16:27,718][233954] Reward + Measures: [[28.32404689  0.16630234  0.20287266  0.41995999  0.37176099  0.39448816]][0m
[37m[1m[2023-07-11 12:16:27,719][233954] Max Reward on eval: 28.32404688862395[0m
[37m[1m[2023-07-11 12:16:27,719][233954] Min Reward on eval: 28.32404688862395[0m
[37m[1m[2023-07-11 12:16:27,719][233954] Mean Reward across all agents: 28.32404688862395[0m
[37m[1m[2023-07-11 12:16:27,720][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:16:32,712][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:16:32,712][233954] Reward + Measures: [[11.0040582   0.27160001  0.33060002  0.39060003  0.38919997  0.53402132]
 [ 4.4772732   0.13870001  0.22939999  0.31799999  0.45510003  0.84363949]
 [ 4.35118709  0.30000001  0.48359996  0.49600002  0.44400001  0.62156397]
 ...
 [ 0.50867018  0.5751      0.71420002  0.64899999  0.69029999  0.57059997]
 [20.25095988  0.1944      0.29359999  0.37129998  0.34059998  0.49302569]
 [22.25250271  0.1168      0.1277      0.44169998  0.28799999  0.30018768]][0m
[37m[1m[2023-07-11 12:16:32,712][233954] Max Reward on eval: 84.01566431112587[0m
[37m[1m[2023-07-11 12:16:32,718][233954] Min Reward on eval: -102.21728821992875[0m
[37m[1m[2023-07-11 12:16:32,718][233954] Mean Reward across all agents: 6.318772438616143[0m
[37m[1m[2023-07-11 12:16:32,718][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:16:32,723][233954] mean_value=-88.13188767663381, max_value=474.23166870198213[0m
[37m[1m[2023-07-11 12:16:32,726][233954] New mean coefficients: [[ 0.18806273  0.05440065  0.24760023 -0.1181796   0.24742714 -0.35941195]][0m
[37m[1m[2023-07-11 12:16:32,727][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:16:41,767][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 12:16:41,767][233954] FPS: 424853.66[0m
[36m[2023-07-11 12:16:41,769][233954] itr=905, itrs=2000, Progress: 45.25%[0m
[36m[2023-07-11 12:16:53,411][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 12:16:53,411][233954] FPS: 332536.92[0m
[36m[2023-07-11 12:16:57,698][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:16:57,703][233954] Reward + Measures: [[20.69026292  0.233046    0.23617533  0.24894966  0.31474999  0.63102561]][0m
[37m[1m[2023-07-11 12:16:57,704][233954] Max Reward on eval: 20.690262916289367[0m
[37m[1m[2023-07-11 12:16:57,704][233954] Min Reward on eval: 20.690262916289367[0m
[37m[1m[2023-07-11 12:16:57,704][233954] Mean Reward across all agents: 20.690262916289367[0m
[37m[1m[2023-07-11 12:16:57,705][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:17:02,608][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:17:02,609][233954] Reward + Measures: [[ -0.54802025   0.30950001   0.23079999   0.25889999   0.39030001
    0.6180588 ]
 [ 34.98671019   0.2203       0.24889998   0.22019999   0.30060002
    0.8138656 ]
 [ 20.38697091   0.27500001   0.35890001   0.28599998   0.347
    0.8792147 ]
 ...
 [  9.59749758   0.206        0.22760001   0.14470001   0.2836
    0.94735241]
 [ 27.27816279   0.03670001   0.10550001   0.15510002   0.1587
    0.65710413]
 [-33.71977788   0.57050008   0.67940003   0.57640004   0.62690002
    0.79203188]][0m
[37m[1m[2023-07-11 12:17:02,609][233954] Max Reward on eval: 145.5084102064371[0m
[37m[1m[2023-07-11 12:17:02,609][233954] Min Reward on eval: -116.8791237099038[0m
[37m[1m[2023-07-11 12:17:02,609][233954] Mean Reward across all agents: 9.557589947264056[0m
[37m[1m[2023-07-11 12:17:02,610][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:17:02,613][233954] mean_value=-637.2793441491426, max_value=489.8878925622441[0m
[37m[1m[2023-07-11 12:17:02,616][233954] New mean coefficients: [[ 0.18817586  0.0982254   0.23391658 -0.07365499  0.27818066 -0.31421605]][0m
[37m[1m[2023-07-11 12:17:02,617][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:17:11,518][233954] train() took 8.90 seconds to complete[0m
[36m[2023-07-11 12:17:11,519][233954] FPS: 431460.73[0m
[36m[2023-07-11 12:17:11,521][233954] itr=906, itrs=2000, Progress: 45.30%[0m
[36m[2023-07-11 12:17:23,254][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 12:17:23,254][233954] FPS: 329965.61[0m
[36m[2023-07-11 12:17:27,491][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:17:27,496][233954] Reward + Measures: [[20.92860334  0.24571934  0.26287931  0.31583667  0.40514702  0.53746784]][0m
[37m[1m[2023-07-11 12:17:27,497][233954] Max Reward on eval: 20.928603337580586[0m
[37m[1m[2023-07-11 12:17:27,497][233954] Min Reward on eval: 20.928603337580586[0m
[37m[1m[2023-07-11 12:17:27,497][233954] Mean Reward across all agents: 20.928603337580586[0m
[37m[1m[2023-07-11 12:17:27,497][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:17:32,542][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:17:32,547][233954] Reward + Measures: [[16.9283259   0.32260001  0.32819998  0.3459      0.4745      0.5596506 ]
 [ 1.90020934  0.29210001  0.32440001  0.32270002  0.36579999  0.70958555]
 [25.65121935  0.3206      0.40310001  0.33239999  0.47320005  0.73766631]
 ...
 [40.60421915  0.2352      0.35069999  0.37860003  0.53740001  0.66326267]
 [57.2718585   0.34039998  0.29749998  0.44190001  0.38080001  0.85836357]
 [16.44649189  0.101       0.19000001  0.26390001  0.38579997  0.7012164 ]][0m
[37m[1m[2023-07-11 12:17:32,548][233954] Max Reward on eval: 516.9993603236974[0m
[37m[1m[2023-07-11 12:17:32,548][233954] Min Reward on eval: -59.551594405435026[0m
[37m[1m[2023-07-11 12:17:32,548][233954] Mean Reward across all agents: 22.893677007067346[0m
[37m[1m[2023-07-11 12:17:32,548][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:17:32,552][233954] mean_value=-350.9113742578333, max_value=373.52711182316204[0m
[37m[1m[2023-07-11 12:17:32,555][233954] New mean coefficients: [[ 0.18437696  0.01597567  0.28328782 -0.03916885  0.30787936 -0.21818495]][0m
[37m[1m[2023-07-11 12:17:32,556][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:17:41,573][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 12:17:41,573][233954] FPS: 425936.69[0m
[36m[2023-07-11 12:17:41,575][233954] itr=907, itrs=2000, Progress: 45.35%[0m
[36m[2023-07-11 12:17:53,105][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 12:17:53,105][233954] FPS: 335789.46[0m
[36m[2023-07-11 12:17:57,293][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:17:57,293][233954] Reward + Measures: [[20.45055397  0.27692732  0.311409    0.36316401  0.48170403  0.52552789]][0m
[37m[1m[2023-07-11 12:17:57,293][233954] Max Reward on eval: 20.45055396987882[0m
[37m[1m[2023-07-11 12:17:57,294][233954] Min Reward on eval: 20.45055396987882[0m
[37m[1m[2023-07-11 12:17:57,294][233954] Mean Reward across all agents: 20.45055396987882[0m
[37m[1m[2023-07-11 12:17:57,294][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:18:02,504][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:18:02,510][233954] Reward + Measures: [[ 11.29687967   0.1786       0.1532       0.1644       0.26719999
    0.80894148]
 [ 12.79982251   0.47680002   0.51859999   0.5291       0.61980003
    0.64974701]
 [ 22.36007357   0.26730001   0.2507       0.2624       0.36770001
    0.5518409 ]
 ...
 [ 17.27484228   0.2978       0.25299999   0.39140001   0.43129998
    0.46088696]
 [393.75753165   0.07340001   0.86779994   0.71460003   0.87670004
    2.06113029]
 [ 25.0448905    0.12989999   0.138        0.20749998   0.4014
    0.57873249]][0m
[37m[1m[2023-07-11 12:18:02,510][233954] Max Reward on eval: 511.15435980192854[0m
[37m[1m[2023-07-11 12:18:02,510][233954] Min Reward on eval: -88.26217543795937[0m
[37m[1m[2023-07-11 12:18:02,511][233954] Mean Reward across all agents: 24.168063108012422[0m
[37m[1m[2023-07-11 12:18:02,511][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:18:02,515][233954] mean_value=-139.61283504261363, max_value=478.28409493864325[0m
[37m[1m[2023-07-11 12:18:02,518][233954] New mean coefficients: [[ 0.21016593  0.04170368  0.16250974  0.01183118  0.37134415 -0.26444188]][0m
[37m[1m[2023-07-11 12:18:02,519][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:18:11,571][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 12:18:11,571][233954] FPS: 424305.39[0m
[36m[2023-07-11 12:18:11,573][233954] itr=908, itrs=2000, Progress: 45.40%[0m
[36m[2023-07-11 12:18:23,247][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 12:18:23,248][233954] FPS: 331663.22[0m
[36m[2023-07-11 12:18:27,605][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:18:27,605][233954] Reward + Measures: [[19.11887944  0.30179301  0.34658936  0.4045963   0.54663599  0.50760341]][0m
[37m[1m[2023-07-11 12:18:27,605][233954] Max Reward on eval: 19.118879436707285[0m
[37m[1m[2023-07-11 12:18:27,606][233954] Min Reward on eval: 19.118879436707285[0m
[37m[1m[2023-07-11 12:18:27,606][233954] Mean Reward across all agents: 19.118879436707285[0m
[37m[1m[2023-07-11 12:18:27,606][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:18:32,649][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:18:32,650][233954] Reward + Measures: [[ 8.48395258  0.31530002  0.51590002  0.47529998  0.50840002  0.59166253]
 [19.13918605  0.41620001  0.58990002  0.55489999  0.56630003  0.57335246]
 [13.84678612  0.3184      0.4876      0.46040002  0.54789996  0.57918489]
 ...
 [12.82480744  0.36590001  0.40540001  0.44850001  0.6649      0.42690808]
 [25.18852099  0.22790001  0.2931      0.26929998  0.30739999  0.5245111 ]
 [44.93491102  0.13470002  0.30140004  0.26789999  0.38250002  0.68819386]][0m
[37m[1m[2023-07-11 12:18:32,650][233954] Max Reward on eval: 274.90342045985165[0m
[37m[1m[2023-07-11 12:18:32,650][233954] Min Reward on eval: -126.5243929519318[0m
[37m[1m[2023-07-11 12:18:32,650][233954] Mean Reward across all agents: 17.80271046327559[0m
[37m[1m[2023-07-11 12:18:32,651][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:18:32,655][233954] mean_value=-192.45379046608102, max_value=516.7364989439957[0m
[37m[1m[2023-07-11 12:18:32,658][233954] New mean coefficients: [[ 0.15386465  0.09846334  0.11063752  0.02382927  0.3607448  -0.1364121 ]][0m
[37m[1m[2023-07-11 12:18:32,659][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:18:41,713][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 12:18:41,713][233954] FPS: 424193.50[0m
[36m[2023-07-11 12:18:41,715][233954] itr=909, itrs=2000, Progress: 45.45%[0m
[36m[2023-07-11 12:18:53,439][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 12:18:53,440][233954] FPS: 330184.75[0m
[36m[2023-07-11 12:18:57,720][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:18:57,721][233954] Reward + Measures: [[19.37953088  0.332616    0.37621233  0.41769934  0.54425335  0.52235734]][0m
[37m[1m[2023-07-11 12:18:57,721][233954] Max Reward on eval: 19.37953088416527[0m
[37m[1m[2023-07-11 12:18:57,721][233954] Min Reward on eval: 19.37953088416527[0m
[37m[1m[2023-07-11 12:18:57,721][233954] Mean Reward across all agents: 19.37953088416527[0m
[37m[1m[2023-07-11 12:18:57,722][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:19:02,737][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:19:02,743][233954] Reward + Measures: [[21.9919484   0.83770001  0.92089999  0.7610001   0.91119999  1.57871687]
 [30.94048932  0.79030001  0.84710008  0.75639999  0.82030004  1.86250007]
 [45.02771472  0.30160001  0.27600002  0.3567      0.4199      0.60699123]
 ...
 [ 8.30217947  0.66079998  0.81540006  0.56350005  0.84569997  1.61946297]
 [32.46195696  0.26380002  0.32710001  0.29599997  0.43019995  0.68573564]
 [18.28990772  0.86359996  0.90369999  0.82719994  0.87900001  1.92505074]][0m
[37m[1m[2023-07-11 12:19:02,743][233954] Max Reward on eval: 112.93487780181749[0m
[37m[1m[2023-07-11 12:19:02,744][233954] Min Reward on eval: -93.85315598801245[0m
[37m[1m[2023-07-11 12:19:02,744][233954] Mean Reward across all agents: 5.207451860122548[0m
[37m[1m[2023-07-11 12:19:02,744][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:19:02,748][233954] mean_value=-122.4867194083731, max_value=449.3181765459245[0m
[37m[1m[2023-07-11 12:19:02,751][233954] New mean coefficients: [[ 0.08048072  0.07309958  0.01815256 -0.01053647  0.2699159  -0.3794942 ]][0m
[37m[1m[2023-07-11 12:19:02,752][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:19:11,704][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 12:19:11,705][233954] FPS: 429015.72[0m
[36m[2023-07-11 12:19:11,707][233954] itr=910, itrs=2000, Progress: 45.50%[0m
[37m[1m[2023-07-11 12:22:38,022][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000890[0m
[36m[2023-07-11 12:22:50,369][233954] train() took 11.74 seconds to complete[0m
[36m[2023-07-11 12:22:50,369][233954] FPS: 327194.79[0m
[36m[2023-07-11 12:22:54,607][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:22:54,607][233954] Reward + Measures: [[17.50857333  0.35046798  0.3923527   0.44435266  0.59366202  0.50366455]][0m
[37m[1m[2023-07-11 12:22:54,607][233954] Max Reward on eval: 17.50857332814324[0m
[37m[1m[2023-07-11 12:22:54,608][233954] Min Reward on eval: 17.50857332814324[0m
[37m[1m[2023-07-11 12:22:54,608][233954] Mean Reward across all agents: 17.50857332814324[0m
[37m[1m[2023-07-11 12:22:54,608][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:22:59,623][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:22:59,624][233954] Reward + Measures: [[ 1.75267448  0.67930001  0.76139998  0.59720004  0.77570003  0.75467128]
 [ 8.44416534  0.22880001  0.3231      0.2843      0.44209996  0.74944144]
 [38.39092304  0.0405      0.134       0.1928      0.17560001  0.64708781]
 ...
 [13.15061805  0.2192      0.25840002  0.30809999  0.47750002  0.48433501]
 [11.32455408  0.37860003  0.28169999  0.37100002  0.49200001  0.85681832]
 [42.78084064  0.29710001  0.45440003  0.41750002  0.35020003  0.82348156]][0m
[37m[1m[2023-07-11 12:22:59,624][233954] Max Reward on eval: 206.7320055967197[0m
[37m[1m[2023-07-11 12:22:59,624][233954] Min Reward on eval: -98.63196204584092[0m
[37m[1m[2023-07-11 12:22:59,624][233954] Mean Reward across all agents: 20.682882702197677[0m
[37m[1m[2023-07-11 12:22:59,625][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:22:59,628][233954] mean_value=-438.48758230902354, max_value=561.8657153777312[0m
[37m[1m[2023-07-11 12:22:59,631][233954] New mean coefficients: [[ 0.00440937  0.02939855  0.00558448 -0.08561108  0.1834864  -0.14721182]][0m
[37m[1m[2023-07-11 12:22:59,632][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:23:08,541][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 12:23:08,541][233954] FPS: 431092.40[0m
[36m[2023-07-11 12:23:08,544][233954] itr=911, itrs=2000, Progress: 45.55%[0m
[36m[2023-07-11 12:23:20,082][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 12:23:20,082][233954] FPS: 335555.71[0m
[36m[2023-07-11 12:23:24,290][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:23:24,290][233954] Reward + Measures: [[18.57424173  0.37150237  0.409298    0.45855328  0.59974563  0.50822002]][0m
[37m[1m[2023-07-11 12:23:24,291][233954] Max Reward on eval: 18.57424173478176[0m
[37m[1m[2023-07-11 12:23:24,291][233954] Min Reward on eval: 18.57424173478176[0m
[37m[1m[2023-07-11 12:23:24,291][233954] Mean Reward across all agents: 18.57424173478176[0m
[37m[1m[2023-07-11 12:23:24,292][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:23:29,237][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:23:29,238][233954] Reward + Measures: [[ 15.11766504   0.2132       0.22119999   0.2343       0.3567
    0.62211382]
 [ 20.44339194   0.4409       0.53280002   0.55170006   0.68659991
    0.56639987]
 [-16.17949124   0.56960005   0.73690003   0.59300005   0.66039997
    0.72224313]
 ...
 [ 20.90164914   0.11870001   0.14830001   0.2458       0.43420002
    0.42105064]
 [ 20.93450002   0.76050007   0.78890002   0.71360004   0.81610006
    1.58611035]
 [ -4.07297247   0.72160006   0.9483       0.69959998   0.91949999
    0.94334328]][0m
[37m[1m[2023-07-11 12:23:29,238][233954] Max Reward on eval: 487.2451133698225[0m
[37m[1m[2023-07-11 12:23:29,239][233954] Min Reward on eval: -108.86702271715039[0m
[37m[1m[2023-07-11 12:23:29,239][233954] Mean Reward across all agents: 10.636061394696789[0m
[37m[1m[2023-07-11 12:23:29,239][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:23:29,244][233954] mean_value=-71.34098793626528, max_value=515.082706992887[0m
[37m[1m[2023-07-11 12:23:29,247][233954] New mean coefficients: [[ 0.00885747  0.01891272 -0.00127712 -0.17690668  0.21181995 -0.04236953]][0m
[37m[1m[2023-07-11 12:23:29,248][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:23:38,323][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 12:23:38,323][233954] FPS: 423215.93[0m
[36m[2023-07-11 12:23:38,326][233954] itr=912, itrs=2000, Progress: 45.60%[0m
[36m[2023-07-11 12:23:50,118][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 12:23:50,118][233954] FPS: 328227.91[0m
[36m[2023-07-11 12:23:54,477][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:23:54,478][233954] Reward + Measures: [[17.38196062  0.37212533  0.41030002  0.46235001  0.60560101  0.48596081]][0m
[37m[1m[2023-07-11 12:23:54,478][233954] Max Reward on eval: 17.38196062082352[0m
[37m[1m[2023-07-11 12:23:54,478][233954] Min Reward on eval: 17.38196062082352[0m
[37m[1m[2023-07-11 12:23:54,479][233954] Mean Reward across all agents: 17.38196062082352[0m
[37m[1m[2023-07-11 12:23:54,479][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:23:59,518][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:23:59,519][233954] Reward + Measures: [[-31.05400523   0.56020004   0.89899999   0.46490002   0.84320003
    1.52922559]
 [413.41419704   0.21060002   0.98060006   0.80270004   0.98070002
    2.12610483]
 [ 16.86027312   0.46650001   0.63640004   0.65039998   0.51899999
    0.52367115]
 ...
 [-33.82540697   0.3353       0.41060001   0.24330001   0.5244
    0.75590515]
 [ 28.40224639   0.19560002   0.23309998   0.24370001   0.39039999
    0.557895  ]
 [ 10.84639613   0.46650001   0.5248       0.45120001   0.51789999
    0.63964558]][0m
[37m[1m[2023-07-11 12:23:59,519][233954] Max Reward on eval: 588.6453018249944[0m
[37m[1m[2023-07-11 12:23:59,519][233954] Min Reward on eval: -82.90907846689224[0m
[37m[1m[2023-07-11 12:23:59,520][233954] Mean Reward across all agents: 51.776845132555[0m
[37m[1m[2023-07-11 12:23:59,520][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:23:59,526][233954] mean_value=-50.95678749361431, max_value=737.7593396230675[0m
[37m[1m[2023-07-11 12:23:59,528][233954] New mean coefficients: [[ 0.05516703  0.03514918 -0.07650065 -0.19044381  0.17949641 -0.19586267]][0m
[37m[1m[2023-07-11 12:23:59,529][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:24:08,632][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 12:24:08,632][233954] FPS: 421918.78[0m
[36m[2023-07-11 12:24:08,635][233954] itr=913, itrs=2000, Progress: 45.65%[0m
[36m[2023-07-11 12:24:20,431][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 12:24:20,431][233954] FPS: 328132.32[0m
[36m[2023-07-11 12:24:24,739][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:24:24,739][233954] Reward + Measures: [[15.4763679   0.42414933  0.45447966  0.47207299  0.62702435  0.47886479]][0m
[37m[1m[2023-07-11 12:24:24,739][233954] Max Reward on eval: 15.476367896796587[0m
[37m[1m[2023-07-11 12:24:24,739][233954] Min Reward on eval: 15.476367896796587[0m
[37m[1m[2023-07-11 12:24:24,740][233954] Mean Reward across all agents: 15.476367896796587[0m
[37m[1m[2023-07-11 12:24:24,740][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:24:29,989][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:24:29,990][233954] Reward + Measures: [[-20.60406345   0.67320007   0.7586       0.58200002   0.83170003
    1.60871911]
 [ 10.53668231   0.59759998   0.57730001   0.51240003   0.64129996
    0.61983544]
 [  6.85678075   0.73180002   0.77460003   0.73760003   0.82609999
    0.3609131 ]
 ...
 [  1.56170274   0.82310003   0.85890007   0.76389998   0.87620002
    0.45896521]
 [ 91.64231647   0.34009999   0.49869999   0.46819997   0.54170001
    0.83673972]
 [ -4.33197514   0.65020001   0.66590005   0.59619999   0.71630001
    0.56203979]][0m
[37m[1m[2023-07-11 12:24:29,990][233954] Max Reward on eval: 142.57981003783644[0m
[37m[1m[2023-07-11 12:24:29,990][233954] Min Reward on eval: -91.35302324416116[0m
[37m[1m[2023-07-11 12:24:29,990][233954] Mean Reward across all agents: 2.401713887512087[0m
[37m[1m[2023-07-11 12:24:29,991][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:24:29,996][233954] mean_value=-34.8111180077466, max_value=469.919856358245[0m
[37m[1m[2023-07-11 12:24:29,998][233954] New mean coefficients: [[ 0.06494029  0.08809645 -0.11311993 -0.20636053  0.13344982  0.00053377]][0m
[37m[1m[2023-07-11 12:24:29,999][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:24:38,936][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 12:24:38,936][233954] FPS: 429758.62[0m
[36m[2023-07-11 12:24:38,939][233954] itr=914, itrs=2000, Progress: 45.70%[0m
[36m[2023-07-11 12:24:50,545][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 12:24:50,545][233954] FPS: 333615.23[0m
[36m[2023-07-11 12:24:54,826][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:24:54,826][233954] Reward + Measures: [[15.77758437  0.431299    0.463476    0.47174397  0.64464903  0.4772194 ]][0m
[37m[1m[2023-07-11 12:24:54,827][233954] Max Reward on eval: 15.777584373473493[0m
[37m[1m[2023-07-11 12:24:54,827][233954] Min Reward on eval: 15.777584373473493[0m
[37m[1m[2023-07-11 12:24:54,827][233954] Mean Reward across all agents: 15.777584373473493[0m
[37m[1m[2023-07-11 12:24:54,827][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:24:59,788][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:24:59,789][233954] Reward + Measures: [[  7.03317819   0.55579996   0.58329999   0.59319997   0.7331
    0.40592122]
 [-20.2136551    0.82840008   0.93409997   0.72410005   0.90939999
    0.78368801]
 [  4.47445244   0.49880001   0.59439999   0.47569999   0.62849998
    0.74844962]
 ...
 [ 16.6364521    0.8538       0.85780001   0.81040001   0.89609998
    1.65582049]
 [ 14.77671091   0.32300001   0.31799999   0.34400001   0.40500003
    0.69054681]
 [-47.0890317    0.58410007   0.80590004   0.45919999   0.84039992
    1.41504896]][0m
[37m[1m[2023-07-11 12:24:59,789][233954] Max Reward on eval: 546.1909065361135[0m
[37m[1m[2023-07-11 12:24:59,789][233954] Min Reward on eval: -87.32738343221135[0m
[37m[1m[2023-07-11 12:24:59,789][233954] Mean Reward across all agents: 24.542695775312506[0m
[37m[1m[2023-07-11 12:24:59,790][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:24:59,795][233954] mean_value=-13.093084216316754, max_value=458.11211696019166[0m
[37m[1m[2023-07-11 12:24:59,798][233954] New mean coefficients: [[ 0.01236122  0.09934123 -0.04484132 -0.19844913  0.07282977  0.13267346]][0m
[37m[1m[2023-07-11 12:24:59,799][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:25:08,716][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 12:25:08,716][233954] FPS: 430710.15[0m
[36m[2023-07-11 12:25:08,719][233954] itr=915, itrs=2000, Progress: 45.75%[0m
[36m[2023-07-11 12:25:20,431][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 12:25:20,432][233954] FPS: 330490.27[0m
[36m[2023-07-11 12:25:24,689][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:25:24,689][233954] Reward + Measures: [[18.47982583  0.44101164  0.48569131  0.44761267  0.64097565  0.71807885]][0m
[37m[1m[2023-07-11 12:25:24,689][233954] Max Reward on eval: 18.479825825535[0m
[37m[1m[2023-07-11 12:25:24,690][233954] Min Reward on eval: 18.479825825535[0m
[37m[1m[2023-07-11 12:25:24,690][233954] Mean Reward across all agents: 18.479825825535[0m
[37m[1m[2023-07-11 12:25:24,690][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:25:29,704][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:25:29,704][233954] Reward + Measures: [[-87.27102819   0.4289       0.80839998   0.30840001   0.77010006
    1.68526459]
 [  5.48300719   0.79610002   0.86350006   0.76620001   0.8563
    0.89540976]
 [108.95170562   0.41049996   0.55059999   0.54680002   0.67210001
    0.99656773]
 ...
 [-30.62084654   0.6214       0.66619998   0.49649999   0.70379996
    0.91940278]
 [ 69.55864005   0.18380001   0.32030001   0.25980002   0.40599999
    0.99768764]
 [-42.76617706   0.64230001   0.81000006   0.52969998   0.81030005
    1.42734265]][0m
[37m[1m[2023-07-11 12:25:29,705][233954] Max Reward on eval: 359.2541647447273[0m
[37m[1m[2023-07-11 12:25:29,705][233954] Min Reward on eval: -87.27102819169522[0m
[37m[1m[2023-07-11 12:25:29,705][233954] Mean Reward across all agents: 23.379570703249435[0m
[37m[1m[2023-07-11 12:25:29,706][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:25:29,711][233954] mean_value=-26.261545966798323, max_value=644.7852932834998[0m
[37m[1m[2023-07-11 12:25:29,714][233954] New mean coefficients: [[ 0.09616399  0.06386684 -0.02114618 -0.16880293  0.07440365 -0.12957971]][0m
[37m[1m[2023-07-11 12:25:29,715][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:25:38,765][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 12:25:38,765][233954] FPS: 424382.20[0m
[36m[2023-07-11 12:25:38,767][233954] itr=916, itrs=2000, Progress: 45.80%[0m
[36m[2023-07-11 12:25:50,371][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 12:25:50,372][233954] FPS: 333600.54[0m
[36m[2023-07-11 12:25:54,574][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:25:54,574][233954] Reward + Measures: [[17.09216395  0.43803564  0.45589432  0.41111267  0.59599203  0.61484784]][0m
[37m[1m[2023-07-11 12:25:54,574][233954] Max Reward on eval: 17.092163950617493[0m
[37m[1m[2023-07-11 12:25:54,575][233954] Min Reward on eval: 17.092163950617493[0m
[37m[1m[2023-07-11 12:25:54,575][233954] Mean Reward across all agents: 17.092163950617493[0m
[37m[1m[2023-07-11 12:25:54,575][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:25:59,524][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:25:59,525][233954] Reward + Measures: [[ 20.14504897   0.44060001   0.3312       0.3994       0.48199996
    0.59534901]
 [  2.91067084   0.31250003   0.37340003   0.1965       0.38800001
    0.9066807 ]
 [ 19.78402501   0.1939       0.2791       0.19680001   0.366
    0.68228573]
 ...
 [ 91.4289171    0.49390003   0.4377       0.52410001   0.59910005
    0.93786156]
 [-42.72406064   0.65750003   0.75959998   0.45520002   0.79420006
    0.95135748]
 [  9.43431581   0.39610001   0.51139992   0.40260002   0.59820002
    0.81219667]][0m
[37m[1m[2023-07-11 12:25:59,525][233954] Max Reward on eval: 122.16033008466475[0m
[37m[1m[2023-07-11 12:25:59,525][233954] Min Reward on eval: -143.7059996546246[0m
[37m[1m[2023-07-11 12:25:59,525][233954] Mean Reward across all agents: 5.005477518100232[0m
[37m[1m[2023-07-11 12:25:59,526][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:25:59,530][233954] mean_value=-32.75675107411985, max_value=448.29301084229627[0m
[37m[1m[2023-07-11 12:25:59,533][233954] New mean coefficients: [[ 0.05031274  0.05493528 -0.07888899 -0.22953798 -0.03088333 -0.07086197]][0m
[37m[1m[2023-07-11 12:25:59,534][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:26:08,519][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 12:26:08,520][233954] FPS: 427427.75[0m
[36m[2023-07-11 12:26:08,522][233954] itr=917, itrs=2000, Progress: 45.85%[0m
[36m[2023-07-11 12:26:20,141][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 12:26:20,141][233954] FPS: 333188.96[0m
[36m[2023-07-11 12:26:24,490][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:26:24,491][233954] Reward + Measures: [[19.78317601  0.41790965  0.43146566  0.37549934  0.57640433  0.60300994]][0m
[37m[1m[2023-07-11 12:26:24,491][233954] Max Reward on eval: 19.78317601256769[0m
[37m[1m[2023-07-11 12:26:24,491][233954] Min Reward on eval: 19.78317601256769[0m
[37m[1m[2023-07-11 12:26:24,492][233954] Mean Reward across all agents: 19.78317601256769[0m
[37m[1m[2023-07-11 12:26:24,492][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:26:29,540][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:26:29,541][233954] Reward + Measures: [[181.34956427   0.0982       0.66839999   0.34230003   0.65009999
    1.58546638]
 [223.24708113   0.1367       0.78599995   0.43579999   0.77490002
    1.74146736]
 [ -8.7318997    0.73470002   0.92259997   0.67950004   0.88320011
    1.10303545]
 ...
 [ 29.58860177   0.0895       0.3055       0.19960003   0.39309999
    0.83189738]
 [ 22.08867942   0.19460002   0.32660002   0.1767       0.42740002
    0.77947438]
 [  4.09236012   0.68850005   0.76120001   0.69660002   0.78580004
    0.71089441]][0m
[37m[1m[2023-07-11 12:26:29,541][233954] Max Reward on eval: 500.7313766525593[0m
[37m[1m[2023-07-11 12:26:29,541][233954] Min Reward on eval: -86.1448001834564[0m
[37m[1m[2023-07-11 12:26:29,542][233954] Mean Reward across all agents: 28.41577532543731[0m
[37m[1m[2023-07-11 12:26:29,542][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:26:29,547][233954] mean_value=-139.81938465373057, max_value=555.5465296133538[0m
[37m[1m[2023-07-11 12:26:29,550][233954] New mean coefficients: [[ 0.04096824 -0.04724704 -0.11195591 -0.2503972  -0.09564184  0.04379836]][0m
[37m[1m[2023-07-11 12:26:29,551][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:26:38,637][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 12:26:38,637][233954] FPS: 422690.49[0m
[36m[2023-07-11 12:26:38,639][233954] itr=918, itrs=2000, Progress: 45.90%[0m
[36m[2023-07-11 12:26:50,202][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 12:26:50,203][233954] FPS: 334793.74[0m
[36m[2023-07-11 12:26:54,463][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:26:54,464][233954] Reward + Measures: [[25.24454817  0.34640729  0.39152369  0.30392799  0.53834802  0.68972313]][0m
[37m[1m[2023-07-11 12:26:54,464][233954] Max Reward on eval: 25.244548170030477[0m
[37m[1m[2023-07-11 12:26:54,464][233954] Min Reward on eval: 25.244548170030477[0m
[37m[1m[2023-07-11 12:26:54,465][233954] Mean Reward across all agents: 25.244548170030477[0m
[37m[1m[2023-07-11 12:26:54,465][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:26:59,418][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:26:59,418][233954] Reward + Measures: [[44.8789357   0.20739999  0.45199999  0.25120002  0.62519997  1.1271565 ]
 [ 0.44668426  0.53579998  0.67270005  0.48979998  0.7058      0.98462504]
 [17.42798291  0.74259996  0.78499997  0.51019996  0.76249999  0.83110106]
 ...
 [36.86670543  0.1416      0.12249999  0.06330001  0.29460001  0.73026067]
 [25.62853998  0.24890001  0.3364      0.17730001  0.39489999  0.76316923]
 [28.5521876   0.49370003  0.61880004  0.4646      0.59370005  0.72227287]][0m
[37m[1m[2023-07-11 12:26:59,419][233954] Max Reward on eval: 458.6789936631685[0m
[37m[1m[2023-07-11 12:26:59,419][233954] Min Reward on eval: -55.87213548878208[0m
[37m[1m[2023-07-11 12:26:59,419][233954] Mean Reward across all agents: 31.78458510145063[0m
[37m[1m[2023-07-11 12:26:59,419][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:26:59,426][233954] mean_value=-80.147497572212, max_value=581.926348776184[0m
[37m[1m[2023-07-11 12:26:59,429][233954] New mean coefficients: [[ 0.01056051 -0.04676986 -0.16126901 -0.30884308 -0.1536897   0.14677072]][0m
[37m[1m[2023-07-11 12:26:59,430][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:27:08,413][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 12:27:08,413][233954] FPS: 427553.58[0m
[36m[2023-07-11 12:27:08,415][233954] itr=919, itrs=2000, Progress: 45.95%[0m
[36m[2023-07-11 12:27:20,260][233954] train() took 11.75 seconds to complete[0m
[36m[2023-07-11 12:27:20,260][233954] FPS: 326794.74[0m
[36m[2023-07-11 12:27:24,610][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:27:24,615][233954] Reward + Measures: [[51.35862253  0.18745968  0.34359267  0.11444434  0.46504167  0.96445513]][0m
[37m[1m[2023-07-11 12:27:24,616][233954] Max Reward on eval: 51.35862252575289[0m
[37m[1m[2023-07-11 12:27:24,616][233954] Min Reward on eval: 51.35862252575289[0m
[37m[1m[2023-07-11 12:27:24,616][233954] Mean Reward across all agents: 51.35862252575289[0m
[37m[1m[2023-07-11 12:27:24,616][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:27:29,819][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:27:29,824][233954] Reward + Measures: [[ 78.98094117   0.1778       0.51099998   0.1762       0.50699997
    1.12751663]
 [ 46.22288459   0.2696       0.39219999   0.18360001   0.47119999
    0.87260836]
 [ 70.37990454   0.0502       0.19590001   0.2465       0.25929999
    0.72668689]
 ...
 [ 40.25710277   0.26730001   0.70059997   0.1837       0.69410002
    1.44114101]
 [158.9653391    0.19690001   0.50169998   0.3962       0.58920002
    1.18299329]
 [ 43.29956333   0.0661       0.20820001   0.12540001   0.42449999
    0.80429572]][0m
[37m[1m[2023-07-11 12:27:29,825][233954] Max Reward on eval: 326.96090577058493[0m
[37m[1m[2023-07-11 12:27:29,825][233954] Min Reward on eval: -16.516777847334744[0m
[37m[1m[2023-07-11 12:27:29,825][233954] Mean Reward across all agents: 49.845868299201086[0m
[37m[1m[2023-07-11 12:27:29,826][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:27:29,832][233954] mean_value=-144.82348993010467, max_value=601.6993250134308[0m
[37m[1m[2023-07-11 12:27:29,835][233954] New mean coefficients: [[ 0.07296614  0.01075854 -0.20698532 -0.22626242 -0.04038608 -0.15345743]][0m
[37m[1m[2023-07-11 12:27:29,836][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:27:38,864][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 12:27:38,864][233954] FPS: 425409.17[0m
[36m[2023-07-11 12:27:38,866][233954] itr=920, itrs=2000, Progress: 46.00%[0m
[37m[1m[2023-07-11 12:31:14,781][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000900[0m
[36m[2023-07-11 12:31:27,135][233954] train() took 11.75 seconds to complete[0m
[36m[2023-07-11 12:31:27,136][233954] FPS: 326729.56[0m
[36m[2023-07-11 12:31:31,473][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:31:31,473][233954] Reward + Measures: [[48.21159131  0.19510667  0.30129668  0.09828833  0.41469833  0.9304235 ]][0m
[37m[1m[2023-07-11 12:31:31,474][233954] Max Reward on eval: 48.21159130534143[0m
[37m[1m[2023-07-11 12:31:31,474][233954] Min Reward on eval: 48.21159130534143[0m
[37m[1m[2023-07-11 12:31:31,474][233954] Mean Reward across all agents: 48.21159130534143[0m
[37m[1m[2023-07-11 12:31:31,475][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:31:36,559][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:31:36,560][233954] Reward + Measures: [[42.58987567  0.0977      0.222       0.1139      0.3263      0.66522497]
 [42.8511409   0.1224      0.16440001  0.0993      0.2131      0.87630504]
 [39.53105733  0.1218      0.28909999  0.0988      0.38099998  0.93143189]
 ...
 [32.6039294   0.12910001  0.43010002  0.1384      0.41499996  0.97336167]
 [31.75084452  0.19180001  0.22150002  0.1024      0.31280002  0.72798306]
 [26.6413075   0.16170001  0.2263      0.1051      0.29410002  1.03535092]][0m
[37m[1m[2023-07-11 12:31:36,560][233954] Max Reward on eval: 194.18206572555937[0m
[37m[1m[2023-07-11 12:31:36,560][233954] Min Reward on eval: -23.291830618586392[0m
[37m[1m[2023-07-11 12:31:36,560][233954] Mean Reward across all agents: 44.724431552890834[0m
[37m[1m[2023-07-11 12:31:36,560][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:31:36,564][233954] mean_value=-370.27361259790763, max_value=474.5829896391253[0m
[37m[1m[2023-07-11 12:31:36,567][233954] New mean coefficients: [[ 0.03197079  0.09364241 -0.22556035 -0.2097454  -0.01995031 -0.19298287]][0m
[37m[1m[2023-07-11 12:31:36,568][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:31:45,626][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 12:31:45,626][233954] FPS: 424007.60[0m
[36m[2023-07-11 12:31:45,628][233954] itr=921, itrs=2000, Progress: 46.05%[0m
[36m[2023-07-11 12:31:57,525][233954] train() took 11.80 seconds to complete[0m
[36m[2023-07-11 12:31:57,525][233954] FPS: 325393.59[0m
[36m[2023-07-11 12:32:01,772][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:32:01,773][233954] Reward + Measures: [[44.55878701  0.18073568  0.27056798  0.09381134  0.394281    0.82467103]][0m
[37m[1m[2023-07-11 12:32:01,773][233954] Max Reward on eval: 44.55878701037189[0m
[37m[1m[2023-07-11 12:32:01,773][233954] Min Reward on eval: 44.55878701037189[0m
[37m[1m[2023-07-11 12:32:01,773][233954] Mean Reward across all agents: 44.55878701037189[0m
[37m[1m[2023-07-11 12:32:01,774][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:32:06,714][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:32:06,714][233954] Reward + Measures: [[22.25913763  0.25420001  0.315       0.29800001  0.40710002  0.63861674]
 [35.12885689  0.31780002  0.47499999  0.3303      0.55519998  0.83440953]
 [19.39364516  0.1547      0.23160003  0.0751      0.26530001  0.89842778]
 ...
 [44.71772528  0.1822      0.3055      0.15269999  0.3944      0.76790822]
 [39.60189677  0.39980003  0.73800004  0.2067      0.74119997  1.63759649]
 [63.52366031  0.27109998  0.54539996  0.26770002  0.56969994  1.09331012]][0m
[37m[1m[2023-07-11 12:32:06,714][233954] Max Reward on eval: 332.99299943380754[0m
[37m[1m[2023-07-11 12:32:06,715][233954] Min Reward on eval: -49.39999572206288[0m
[37m[1m[2023-07-11 12:32:06,715][233954] Mean Reward across all agents: 53.94589319847011[0m
[37m[1m[2023-07-11 12:32:06,715][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:32:06,720][233954] mean_value=-230.51573247620948, max_value=390.634517447893[0m
[37m[1m[2023-07-11 12:32:06,723][233954] New mean coefficients: [[ 0.05564696  0.0868908  -0.22300795 -0.23093475 -0.04151373 -0.2769574 ]][0m
[37m[1m[2023-07-11 12:32:06,723][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:32:15,684][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 12:32:15,685][233954] FPS: 428611.16[0m
[36m[2023-07-11 12:32:15,687][233954] itr=922, itrs=2000, Progress: 46.10%[0m
[36m[2023-07-11 12:32:27,344][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 12:32:27,345][233954] FPS: 332018.94[0m
[36m[2023-07-11 12:32:31,614][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:32:31,615][233954] Reward + Measures: [[46.6425044   0.18284798  0.28293267  0.09503899  0.41549197  0.81851155]][0m
[37m[1m[2023-07-11 12:32:31,615][233954] Max Reward on eval: 46.64250439659268[0m
[37m[1m[2023-07-11 12:32:31,615][233954] Min Reward on eval: 46.64250439659268[0m
[37m[1m[2023-07-11 12:32:31,616][233954] Mean Reward across all agents: 46.64250439659268[0m
[37m[1m[2023-07-11 12:32:31,616][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:32:36,528][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:32:36,529][233954] Reward + Measures: [[43.70676468  0.0774      0.26879999  0.1347      0.45629999  0.85558617]
 [90.34882963  0.47650003  0.58199996  0.4817      0.6013      0.96287644]
 [24.78795854  0.3461      0.21789999  0.2472      0.41500002  0.68131453]
 ...
 [76.43378009  0.13070001  0.50050002  0.1728      0.51010001  1.21100962]
 [22.07785279  0.16919999  0.23260002  0.1224      0.3608      0.57847553]
 [77.07763059  0.23559999  0.34950003  0.16610001  0.52790004  1.03721893]][0m
[37m[1m[2023-07-11 12:32:36,529][233954] Max Reward on eval: 342.8210876823403[0m
[37m[1m[2023-07-11 12:32:36,529][233954] Min Reward on eval: -64.62442407844355[0m
[37m[1m[2023-07-11 12:32:36,530][233954] Mean Reward across all agents: 45.18660678020406[0m
[37m[1m[2023-07-11 12:32:36,530][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:32:36,535][233954] mean_value=-188.45286115830092, max_value=585.8924043356441[0m
[37m[1m[2023-07-11 12:32:36,538][233954] New mean coefficients: [[ 0.0186924   0.0543066  -0.13675675 -0.26815796 -0.03963167  0.07072666]][0m
[37m[1m[2023-07-11 12:32:36,539][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:32:45,452][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 12:32:45,452][233954] FPS: 430876.15[0m
[36m[2023-07-11 12:32:45,455][233954] itr=923, itrs=2000, Progress: 46.15%[0m
[36m[2023-07-11 12:32:57,066][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 12:32:57,066][233954] FPS: 333465.90[0m
[36m[2023-07-11 12:33:01,308][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:33:01,309][233954] Reward + Measures: [[45.23984237  0.19009399  0.27108964  0.09082066  0.40055037  0.82750684]][0m
[37m[1m[2023-07-11 12:33:01,309][233954] Max Reward on eval: 45.23984236807821[0m
[37m[1m[2023-07-11 12:33:01,309][233954] Min Reward on eval: 45.23984236807821[0m
[37m[1m[2023-07-11 12:33:01,309][233954] Mean Reward across all agents: 45.23984236807821[0m
[37m[1m[2023-07-11 12:33:01,310][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:33:06,271][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:33:06,271][233954] Reward + Measures: [[28.20675706  0.28770003  0.22459999  0.1576      0.3436      0.72523504]
 [36.05941102  0.3486      0.3899      0.24029998  0.47799999  0.80514735]
 [15.96043154  0.36990005  0.36579999  0.23740001  0.49590001  1.10492945]
 ...
 [72.12001049  0.1145      0.44499999  0.11540001  0.59310001  1.07268155]
 [13.57675892  0.26429999  0.2376      0.20780002  0.40100002  1.02074742]
 [49.04208712  0.33950001  0.47009999  0.1319      0.51240003  1.08130872]][0m
[37m[1m[2023-07-11 12:33:06,272][233954] Max Reward on eval: 409.1291146391071[0m
[37m[1m[2023-07-11 12:33:06,272][233954] Min Reward on eval: -46.26117246528156[0m
[37m[1m[2023-07-11 12:33:06,272][233954] Mean Reward across all agents: 53.51433363662578[0m
[37m[1m[2023-07-11 12:33:06,272][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:33:06,277][233954] mean_value=-259.75138671963197, max_value=458.19895231265343[0m
[37m[1m[2023-07-11 12:33:06,280][233954] New mean coefficients: [[ 0.08046423  0.0793599  -0.3006661  -0.32516518 -0.08224112  0.0288146 ]][0m
[37m[1m[2023-07-11 12:33:06,281][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:33:15,253][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 12:33:15,253][233954] FPS: 428093.55[0m
[36m[2023-07-11 12:33:15,255][233954] itr=924, itrs=2000, Progress: 46.20%[0m
[36m[2023-07-11 12:33:26,916][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 12:33:26,917][233954] FPS: 331937.34[0m
[36m[2023-07-11 12:33:31,177][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:33:31,177][233954] Reward + Measures: [[48.99677494  0.19996433  0.28350067  0.09355634  0.39995897  0.87919295]][0m
[37m[1m[2023-07-11 12:33:31,178][233954] Max Reward on eval: 48.99677494458051[0m
[37m[1m[2023-07-11 12:33:31,178][233954] Min Reward on eval: 48.99677494458051[0m
[37m[1m[2023-07-11 12:33:31,178][233954] Mean Reward across all agents: 48.99677494458051[0m
[37m[1m[2023-07-11 12:33:31,178][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:33:36,170][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:33:36,171][233954] Reward + Measures: [[53.23985944  0.15140001  0.384       0.1569      0.43030006  1.03090918]
 [45.86591957  0.0973      0.12630001  0.0753      0.25030002  1.18798769]
 [31.52271811  0.14049999  0.14920001  0.0972      0.22720002  0.84879559]
 ...
 [31.22093962  0.3127      0.37990001  0.0674      0.49490005  1.26263726]
 [43.95405018  0.46660003  0.56800002  0.32460001  0.5916      0.96582931]
 [32.67257118  0.27420002  0.32940003  0.23459999  0.36269999  0.7549538 ]][0m
[37m[1m[2023-07-11 12:33:36,171][233954] Max Reward on eval: 130.8183153978549[0m
[37m[1m[2023-07-11 12:33:36,171][233954] Min Reward on eval: -13.68469265298918[0m
[37m[1m[2023-07-11 12:33:36,171][233954] Mean Reward across all agents: 44.948451834404864[0m
[37m[1m[2023-07-11 12:33:36,172][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:33:36,175][233954] mean_value=-404.8041860071387, max_value=283.87466821612816[0m
[37m[1m[2023-07-11 12:33:36,177][233954] New mean coefficients: [[ 0.10237876  0.07115678 -0.28956208 -0.35184065 -0.06518446 -0.3269496 ]][0m
[37m[1m[2023-07-11 12:33:36,178][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:33:45,200][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 12:33:45,200][233954] FPS: 425731.11[0m
[36m[2023-07-11 12:33:45,202][233954] itr=925, itrs=2000, Progress: 46.25%[0m
[36m[2023-07-11 12:33:57,141][233954] train() took 11.84 seconds to complete[0m
[36m[2023-07-11 12:33:57,141][233954] FPS: 324241.46[0m
[36m[2023-07-11 12:34:01,405][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:34:01,410][233954] Reward + Measures: [[49.69977539  0.18144099  0.28238699  0.09202866  0.41773367  0.85094124]][0m
[37m[1m[2023-07-11 12:34:01,411][233954] Max Reward on eval: 49.69977539137827[0m
[37m[1m[2023-07-11 12:34:01,411][233954] Min Reward on eval: 49.69977539137827[0m
[37m[1m[2023-07-11 12:34:01,411][233954] Mean Reward across all agents: 49.69977539137827[0m
[37m[1m[2023-07-11 12:34:01,411][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:34:06,450][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:34:06,456][233954] Reward + Measures: [[37.50958282  0.13270001  0.20720001  0.1028      0.333       0.99990195]
 [28.54455525  0.0348      0.0627      0.10030001  0.20349999  0.54813218]
 [36.51377788  0.2674      0.29130003  0.16680001  0.37730002  0.70976323]
 ...
 [57.84480362  0.0832      0.27740002  0.12230001  0.41999999  0.93583697]
 [54.37184315  0.1194      0.19230001  0.0798      0.33560002  0.86304301]
 [69.1887049   0.13859999  0.35600001  0.0925      0.44420001  1.05132771]][0m
[37m[1m[2023-07-11 12:34:06,457][233954] Max Reward on eval: 173.77231770800427[0m
[37m[1m[2023-07-11 12:34:06,457][233954] Min Reward on eval: -12.088528624805623[0m
[37m[1m[2023-07-11 12:34:06,458][233954] Mean Reward across all agents: 47.65254005062907[0m
[37m[1m[2023-07-11 12:34:06,459][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:34:06,466][233954] mean_value=-351.44981788801914, max_value=213.89655790077734[0m
[37m[1m[2023-07-11 12:34:06,471][233954] New mean coefficients: [[ 0.099094    0.14338659 -0.3512971  -0.24463692  0.00493347 -0.47652587]][0m
[37m[1m[2023-07-11 12:34:06,472][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:34:15,544][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 12:34:15,544][233954] FPS: 423397.10[0m
[36m[2023-07-11 12:34:15,546][233954] itr=926, itrs=2000, Progress: 46.30%[0m
[36m[2023-07-11 12:34:27,320][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 12:34:27,320][233954] FPS: 328857.64[0m
[36m[2023-07-11 12:34:31,626][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:34:31,626][233954] Reward + Measures: [[47.77726805  0.19970031  0.27037635  0.08211967  0.40393335  0.81984013]][0m
[37m[1m[2023-07-11 12:34:31,626][233954] Max Reward on eval: 47.777268051497444[0m
[37m[1m[2023-07-11 12:34:31,627][233954] Min Reward on eval: 47.777268051497444[0m
[37m[1m[2023-07-11 12:34:31,627][233954] Mean Reward across all agents: 47.777268051497444[0m
[37m[1m[2023-07-11 12:34:31,627][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:34:36,870][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:34:36,870][233954] Reward + Measures: [[ 31.83972644   0.58920002   0.72509998   0.53970003   0.83309996
    1.73850405]
 [ 30.93456749   0.0509       0.0874       0.09870001   0.1628
    0.58887768]
 [218.35903551   0.0721       0.83109999   0.51499999   0.78439999
    1.79231918]
 ...
 [ 32.88819276   0.18180001   0.2115       0.1184       0.359
    0.62240642]
 [ 77.03697276   0.19140001   0.4382       0.11309999   0.50100005
    1.11066568]
 [-17.70217327   0.1538       0.76229995   0.25050002   0.6954
    1.63145721]][0m
[37m[1m[2023-07-11 12:34:36,870][233954] Max Reward on eval: 577.0713615058906[0m
[37m[1m[2023-07-11 12:34:36,871][233954] Min Reward on eval: -74.94913772430736[0m
[37m[1m[2023-07-11 12:34:36,871][233954] Mean Reward across all agents: 78.39067275205186[0m
[37m[1m[2023-07-11 12:34:36,871][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:34:36,874][233954] mean_value=-149.0881016555562, max_value=395.24398837094066[0m
[37m[1m[2023-07-11 12:34:36,877][233954] New mean coefficients: [[ 0.08033589  0.14800611 -0.36820605 -0.23653084  0.00200805 -0.44235137]][0m
[37m[1m[2023-07-11 12:34:36,878][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:34:45,904][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 12:34:45,910][233954] FPS: 425496.65[0m
[36m[2023-07-11 12:34:45,913][233954] itr=927, itrs=2000, Progress: 46.35%[0m
[36m[2023-07-11 12:34:57,602][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 12:34:57,603][233954] FPS: 331157.39[0m
[36m[2023-07-11 12:35:01,819][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:35:01,820][233954] Reward + Measures: [[51.59310855  0.21176232  0.27510035  0.09082333  0.42417765  0.80726868]][0m
[37m[1m[2023-07-11 12:35:01,820][233954] Max Reward on eval: 51.593108552565525[0m
[37m[1m[2023-07-11 12:35:01,820][233954] Min Reward on eval: 51.593108552565525[0m
[37m[1m[2023-07-11 12:35:01,821][233954] Mean Reward across all agents: 51.593108552565525[0m
[37m[1m[2023-07-11 12:35:01,821][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:35:06,799][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:35:06,799][233954] Reward + Measures: [[-10.28699097   0.3874       0.54680002   0.17729999   0.64639997
    1.25754035]
 [ 58.32105583   0.24620001   0.41360003   0.1116       0.48880002
    1.09432232]
 [ 29.79888272   0.20070003   0.3026       0.0849       0.38420001
    0.87772095]
 ...
 [ 52.20222232   0.25470001   0.36500001   0.0794       0.47129998
    0.95223427]
 [ 34.67691342   0.20039999   0.207        0.05930001   0.38290003
    0.78735965]
 [ 74.60919064   0.18429999   0.3335       0.09159999   0.50450003
    0.90067595]][0m
[37m[1m[2023-07-11 12:35:06,799][233954] Max Reward on eval: 109.01135374964215[0m
[37m[1m[2023-07-11 12:35:06,800][233954] Min Reward on eval: -25.10895133408485[0m
[37m[1m[2023-07-11 12:35:06,800][233954] Mean Reward across all agents: 44.28034148671654[0m
[37m[1m[2023-07-11 12:35:06,800][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:35:06,803][233954] mean_value=-224.81076383129025, max_value=444.88339995651717[0m
[37m[1m[2023-07-11 12:35:06,806][233954] New mean coefficients: [[ 0.0297857   0.10390018 -0.3663131  -0.24009109  0.00563051 -0.3255812 ]][0m
[37m[1m[2023-07-11 12:35:06,807][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:35:15,739][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 12:35:15,744][233954] FPS: 430011.56[0m
[36m[2023-07-11 12:35:15,747][233954] itr=928, itrs=2000, Progress: 46.40%[0m
[36m[2023-07-11 12:35:27,350][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 12:35:27,350][233954] FPS: 333640.64[0m
[36m[2023-07-11 12:35:31,605][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:35:31,610][233954] Reward + Measures: [[48.85222263  0.21966602  0.26741332  0.09404735  0.40548202  0.77572381]][0m
[37m[1m[2023-07-11 12:35:31,611][233954] Max Reward on eval: 48.85222263049905[0m
[37m[1m[2023-07-11 12:35:31,611][233954] Min Reward on eval: 48.85222263049905[0m
[37m[1m[2023-07-11 12:35:31,611][233954] Mean Reward across all agents: 48.85222263049905[0m
[37m[1m[2023-07-11 12:35:31,612][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:35:36,593][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:35:36,593][233954] Reward + Measures: [[48.23356549  0.25619999  0.42279997  0.16470002  0.49090001  1.22142482]
 [82.91039132  0.2701      0.4605      0.11210001  0.55520004  1.12293231]
 [67.25960057  0.1873      0.32759997  0.10469999  0.44409999  0.93309498]
 ...
 [50.99926771  0.153       0.2106      0.0714      0.34470001  0.72827053]
 [98.24878472  0.30309999  0.36150002  0.21179998  0.56159997  0.99215966]
 [42.76062021  0.26009998  0.55310005  0.2509      0.60270005  1.04761541]][0m
[37m[1m[2023-07-11 12:35:36,594][233954] Max Reward on eval: 248.93920642537995[0m
[37m[1m[2023-07-11 12:35:36,594][233954] Min Reward on eval: -7.3708447351004[0m
[37m[1m[2023-07-11 12:35:36,594][233954] Mean Reward across all agents: 49.956109161136546[0m
[37m[1m[2023-07-11 12:35:36,594][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:35:36,598][233954] mean_value=-292.67674663016584, max_value=587.7847536519985[0m
[37m[1m[2023-07-11 12:35:36,600][233954] New mean coefficients: [[-0.03811206  0.07136451 -0.38847983 -0.20711397  0.00031784 -0.24126408]][0m
[37m[1m[2023-07-11 12:35:36,601][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:35:45,585][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 12:35:45,585][233954] FPS: 427536.19[0m
[36m[2023-07-11 12:35:45,587][233954] itr=929, itrs=2000, Progress: 46.45%[0m
[36m[2023-07-11 12:35:57,138][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 12:35:57,138][233954] FPS: 335143.63[0m
[36m[2023-07-11 12:36:01,450][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:36:01,456][233954] Reward + Measures: [[47.82080959  0.23665601  0.24700999  0.09543034  0.40113133  0.74606073]][0m
[37m[1m[2023-07-11 12:36:01,456][233954] Max Reward on eval: 47.820809585726316[0m
[37m[1m[2023-07-11 12:36:01,456][233954] Min Reward on eval: 47.820809585726316[0m
[37m[1m[2023-07-11 12:36:01,457][233954] Mean Reward across all agents: 47.820809585726316[0m
[37m[1m[2023-07-11 12:36:01,457][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:36:06,507][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:36:06,513][233954] Reward + Measures: [[ 57.9063504    0.17040001   0.3495       0.1078       0.4463
    1.06316936]
 [ 21.79013275   0.177        0.1279       0.0573       0.2376
    0.62403047]
 [-21.44317745   0.32350001   0.6376       0.2633       0.60930002
    1.4859122 ]
 ...
 [ 80.92177373   0.38120005   0.53980005   0.23840001   0.64540005
    1.34888673]
 [ 42.16581582   0.1691       0.27500001   0.08630001   0.37670001
    0.97698826]
 [ 40.7442007    0.204        0.27770001   0.09070001   0.35220003
    0.85839349]][0m
[37m[1m[2023-07-11 12:36:06,513][233954] Max Reward on eval: 162.93023835428176[0m
[37m[1m[2023-07-11 12:36:06,513][233954] Min Reward on eval: -83.23692688969895[0m
[37m[1m[2023-07-11 12:36:06,514][233954] Mean Reward across all agents: 50.81099764514914[0m
[37m[1m[2023-07-11 12:36:06,514][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:36:06,517][233954] mean_value=-350.5111310439795, max_value=206.5109497508908[0m
[37m[1m[2023-07-11 12:36:06,520][233954] New mean coefficients: [[-0.06079626  0.05873626 -0.3608094  -0.25346923 -0.08138428 -0.3163167 ]][0m
[37m[1m[2023-07-11 12:36:06,521][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:36:15,576][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 12:36:15,576][233954] FPS: 424145.62[0m
[36m[2023-07-11 12:36:15,579][233954] itr=930, itrs=2000, Progress: 46.50%[0m
[37m[1m[2023-07-11 12:39:45,503][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000910[0m
[36m[2023-07-11 12:39:57,973][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 12:39:57,973][233954] FPS: 329236.63[0m
[36m[2023-07-11 12:40:02,131][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:40:02,132][233954] Reward + Measures: [[46.49691217  0.23487532  0.231776    0.09557033  0.37965998  0.72303873]][0m
[37m[1m[2023-07-11 12:40:02,132][233954] Max Reward on eval: 46.49691216629016[0m
[37m[1m[2023-07-11 12:40:02,132][233954] Min Reward on eval: 46.49691216629016[0m
[37m[1m[2023-07-11 12:40:02,133][233954] Mean Reward across all agents: 46.49691216629016[0m
[37m[1m[2023-07-11 12:40:02,133][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:40:07,099][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:40:07,100][233954] Reward + Measures: [[100.96371855   0.21720003   0.3184       0.11570001   0.42050001
    0.8991701 ]
 [ 18.54967386   0.30160001   0.2861       0.14179999   0.43239999
    0.74132293]
 [ 28.79741381   0.46989998   0.33110002   0.30190003   0.54969996
    0.78958893]
 ...
 [ 38.52920484   0.6728       0.77630001   0.53459996   0.84720004
    1.69992089]
 [ 73.61375667   0.25540003   0.47020003   0.1279       0.54969996
    1.16668022]
 [ 33.56609343   0.17600001   0.1846       0.0555       0.3348
    0.61807626]][0m
[37m[1m[2023-07-11 12:40:07,100][233954] Max Reward on eval: 143.16451980150305[0m
[37m[1m[2023-07-11 12:40:07,100][233954] Min Reward on eval: -59.822319090832025[0m
[37m[1m[2023-07-11 12:40:07,101][233954] Mean Reward across all agents: 44.90434523058889[0m
[37m[1m[2023-07-11 12:40:07,101][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:40:07,104][233954] mean_value=-221.14700719300734, max_value=426.0104144076994[0m
[37m[1m[2023-07-11 12:40:07,107][233954] New mean coefficients: [[-0.0388808   0.04194918 -0.3541803  -0.25828567 -0.13575724 -0.27751535]][0m
[37m[1m[2023-07-11 12:40:07,108][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:40:16,081][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 12:40:16,081][233954] FPS: 428043.28[0m
[36m[2023-07-11 12:40:16,083][233954] itr=931, itrs=2000, Progress: 46.55%[0m
[36m[2023-07-11 12:40:27,752][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 12:40:27,752][233954] FPS: 331732.80[0m
[36m[2023-07-11 12:40:31,981][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:40:31,981][233954] Reward + Measures: [[45.55030191  0.23253232  0.22457865  0.10893966  0.36644432  0.70978719]][0m
[37m[1m[2023-07-11 12:40:31,982][233954] Max Reward on eval: 45.55030190524567[0m
[37m[1m[2023-07-11 12:40:31,982][233954] Min Reward on eval: 45.55030190524567[0m
[37m[1m[2023-07-11 12:40:31,982][233954] Mean Reward across all agents: 45.55030190524567[0m
[37m[1m[2023-07-11 12:40:31,982][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:40:36,854][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:40:36,855][233954] Reward + Measures: [[ 47.51070745   0.2023       0.1724       0.0643       0.34110001
    0.64893419]
 [ 34.82529697   0.1573       0.30990002   0.1195       0.40039998
    1.01957917]
 [ 18.26311506   0.17470001   0.3601       0.1211       0.35440001
    0.80609435]
 ...
 [-17.40854482   0.3558       0.63080001   0.3003       0.63450003
    1.66446459]
 [ 88.48843853   0.30020002   0.27790001   0.1463       0.46679997
    0.85355943]
 [ 53.86641483   0.27949998   0.31990001   0.0823       0.44350001
    0.87022352]][0m
[37m[1m[2023-07-11 12:40:36,855][233954] Max Reward on eval: 124.28318420844153[0m
[37m[1m[2023-07-11 12:40:36,855][233954] Min Reward on eval: -19.33229120937176[0m
[37m[1m[2023-07-11 12:40:36,856][233954] Mean Reward across all agents: 41.23047706028329[0m
[37m[1m[2023-07-11 12:40:36,856][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:40:36,858][233954] mean_value=-265.8536283350009, max_value=325.01279140580357[0m
[37m[1m[2023-07-11 12:40:36,861][233954] New mean coefficients: [[-0.05240514  0.0653204  -0.3433305  -0.21388409 -0.14552899 -0.27584088]][0m
[37m[1m[2023-07-11 12:40:36,862][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:40:45,758][233954] train() took 8.89 seconds to complete[0m
[36m[2023-07-11 12:40:45,758][233954] FPS: 431730.60[0m
[36m[2023-07-11 12:40:45,760][233954] itr=932, itrs=2000, Progress: 46.60%[0m
[36m[2023-07-11 12:40:57,368][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 12:40:57,368][233954] FPS: 333461.61[0m
[36m[2023-07-11 12:41:01,662][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:41:01,662][233954] Reward + Measures: [[41.46492807  0.25006267  0.21259399  0.120307    0.34964299  0.66221631]][0m
[37m[1m[2023-07-11 12:41:01,662][233954] Max Reward on eval: 41.464928071736885[0m
[37m[1m[2023-07-11 12:41:01,663][233954] Min Reward on eval: 41.464928071736885[0m
[37m[1m[2023-07-11 12:41:01,663][233954] Mean Reward across all agents: 41.464928071736885[0m
[37m[1m[2023-07-11 12:41:01,663][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:41:06,802][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:41:06,802][233954] Reward + Measures: [[11.05610808  0.44409999  0.42389998  0.42720005  0.46310002  0.8289324 ]
 [23.98229841  0.29679999  0.21170001  0.16309999  0.33560002  0.70997423]
 [28.72694351  0.1072      0.13950001  0.09600001  0.21900001  0.54171175]
 ...
 [15.48807566  0.43110004  0.39500001  0.3714      0.43400002  0.82527429]
 [21.73214611  0.24280003  0.2129      0.1891      0.3039      0.68302077]
 [24.04368506  0.23509999  0.1336      0.1037      0.23030002  0.70304197]][0m
[37m[1m[2023-07-11 12:41:06,803][233954] Max Reward on eval: 112.1283524999395[0m
[37m[1m[2023-07-11 12:41:06,803][233954] Min Reward on eval: -50.35727887152461[0m
[37m[1m[2023-07-11 12:41:06,803][233954] Mean Reward across all agents: 32.40236678779568[0m
[37m[1m[2023-07-11 12:41:06,803][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:41:06,806][233954] mean_value=-529.7470245539321, max_value=524.3396308586001[0m
[37m[1m[2023-07-11 12:41:06,809][233954] New mean coefficients: [[-0.06480388  0.09243241 -0.33746472 -0.20853306 -0.13348252 -0.40727788]][0m
[37m[1m[2023-07-11 12:41:06,810][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:41:15,702][233954] train() took 8.89 seconds to complete[0m
[36m[2023-07-11 12:41:15,702][233954] FPS: 431924.00[0m
[36m[2023-07-11 12:41:15,704][233954] itr=933, itrs=2000, Progress: 46.65%[0m
[36m[2023-07-11 12:41:27,413][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 12:41:27,414][233954] FPS: 330597.60[0m
[36m[2023-07-11 12:41:31,703][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:41:31,703][233954] Reward + Measures: [[42.81755798  0.24446133  0.19295833  0.11641633  0.35106599  0.65671444]][0m
[37m[1m[2023-07-11 12:41:31,704][233954] Max Reward on eval: 42.81755798028132[0m
[37m[1m[2023-07-11 12:41:31,704][233954] Min Reward on eval: 42.81755798028132[0m
[37m[1m[2023-07-11 12:41:31,704][233954] Mean Reward across all agents: 42.81755798028132[0m
[37m[1m[2023-07-11 12:41:31,704][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:41:36,717][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:41:36,717][233954] Reward + Measures: [[20.13346301  0.2392      0.44409999  0.18249999  0.48010001  1.2804209 ]
 [26.42923811  0.35120001  0.58140004  0.3184      0.58590001  1.48849642]
 [48.57433085  0.16900001  0.23380001  0.1184      0.3427      0.85608608]
 ...
 [59.80256618  0.354       0.32980001  0.33390003  0.42720005  0.7933616 ]
 [28.98831054  0.133       0.07250001  0.062       0.20109999  0.55184859]
 [26.69671798  0.23810001  0.14909999  0.0843      0.31619999  0.67640358]][0m
[37m[1m[2023-07-11 12:41:36,718][233954] Max Reward on eval: 313.9186438100878[0m
[37m[1m[2023-07-11 12:41:36,718][233954] Min Reward on eval: -40.57265364034102[0m
[37m[1m[2023-07-11 12:41:36,718][233954] Mean Reward across all agents: 41.27618391602018[0m
[37m[1m[2023-07-11 12:41:36,718][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:41:36,721][233954] mean_value=-259.08286473211604, max_value=573.5259574975166[0m
[37m[1m[2023-07-11 12:41:36,724][233954] New mean coefficients: [[-0.08530831  0.05646131 -0.3677549  -0.12762597 -0.06755051 -0.68032384]][0m
[37m[1m[2023-07-11 12:41:36,725][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:41:45,780][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 12:41:45,780][233954] FPS: 424131.85[0m
[36m[2023-07-11 12:41:45,783][233954] itr=934, itrs=2000, Progress: 46.70%[0m
[36m[2023-07-11 12:41:57,527][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 12:41:57,527][233954] FPS: 329642.57[0m
[36m[2023-07-11 12:42:01,843][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:42:01,844][233954] Reward + Measures: [[40.11343792  0.230152    0.17369966  0.117602    0.34769627  0.63423389]][0m
[37m[1m[2023-07-11 12:42:01,844][233954] Max Reward on eval: 40.11343792262114[0m
[37m[1m[2023-07-11 12:42:01,844][233954] Min Reward on eval: 40.11343792262114[0m
[37m[1m[2023-07-11 12:42:01,844][233954] Mean Reward across all agents: 40.11343792262114[0m
[37m[1m[2023-07-11 12:42:01,845][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:42:06,925][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:42:06,926][233954] Reward + Measures: [[43.23005866  0.10700001  0.11229999  0.078       0.25780001  0.57178658]
 [84.45718497  0.37130001  0.28        0.32600003  0.49989995  0.74474436]
 [36.17019695  0.20780002  0.28340003  0.12019999  0.37729999  0.7678051 ]
 ...
 [25.33156615  0.17080002  0.23639999  0.1188      0.25310001  0.97077543]
 [26.21336898  0.20260003  0.17729999  0.0688      0.33510002  0.68944854]
 [16.67686615  0.13530001  0.0686      0.06900001  0.20369999  0.61214465]][0m
[37m[1m[2023-07-11 12:42:06,926][233954] Max Reward on eval: 130.9750638112426[0m
[37m[1m[2023-07-11 12:42:06,926][233954] Min Reward on eval: -5.796065833885223[0m
[37m[1m[2023-07-11 12:42:06,926][233954] Mean Reward across all agents: 41.164778392880386[0m
[37m[1m[2023-07-11 12:42:06,927][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:42:06,930][233954] mean_value=-241.41939742960835, max_value=163.26180736511785[0m
[37m[1m[2023-07-11 12:42:06,932][233954] New mean coefficients: [[-0.07969026  0.0479537  -0.4187207  -0.11897828 -0.08665641 -0.78173316]][0m
[37m[1m[2023-07-11 12:42:06,933][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:42:15,892][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 12:42:15,892][233954] FPS: 428724.33[0m
[36m[2023-07-11 12:42:15,894][233954] itr=935, itrs=2000, Progress: 46.75%[0m
[36m[2023-07-11 12:42:27,909][233954] train() took 11.92 seconds to complete[0m
[36m[2023-07-11 12:42:27,910][233954] FPS: 322207.51[0m
[36m[2023-07-11 12:42:32,161][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:42:32,162][233954] Reward + Measures: [[39.53392181  0.21727     0.17940499  0.11719933  0.345047    0.62562901]][0m
[37m[1m[2023-07-11 12:42:32,162][233954] Max Reward on eval: 39.53392181411833[0m
[37m[1m[2023-07-11 12:42:32,162][233954] Min Reward on eval: 39.53392181411833[0m
[37m[1m[2023-07-11 12:42:32,163][233954] Mean Reward across all agents: 39.53392181411833[0m
[37m[1m[2023-07-11 12:42:32,163][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:42:37,184][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:42:37,184][233954] Reward + Measures: [[33.49591836  0.2411      0.22309999  0.1724      0.29990003  0.64694399]
 [47.25608513  0.235       0.2246      0.0865      0.29970002  0.69296002]
 [79.63308389  0.21699999  0.1089      0.18530001  0.34120002  0.71269774]
 ...
 [57.2738491   0.31329998  0.4941      0.17919999  0.56820005  1.09141052]
 [24.08970253  0.29949999  0.22720003  0.2246      0.39499998  0.61769855]
 [40.0373144   0.22879998  0.13740002  0.13270001  0.2667      0.98606247]][0m
[37m[1m[2023-07-11 12:42:37,185][233954] Max Reward on eval: 113.71943680609111[0m
[37m[1m[2023-07-11 12:42:37,185][233954] Min Reward on eval: -5.312128293258138[0m
[37m[1m[2023-07-11 12:42:37,185][233954] Mean Reward across all agents: 45.443491162505126[0m
[37m[1m[2023-07-11 12:42:37,185][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:42:37,188][233954] mean_value=-348.2594120029548, max_value=531.3735382731072[0m
[37m[1m[2023-07-11 12:42:37,191][233954] New mean coefficients: [[-0.07353926  0.02370179 -0.45013595 -0.1469979  -0.07001814 -0.902724  ]][0m
[37m[1m[2023-07-11 12:42:37,192][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:42:46,183][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 12:42:46,188][233954] FPS: 427144.94[0m
[36m[2023-07-11 12:42:46,191][233954] itr=936, itrs=2000, Progress: 46.80%[0m
[36m[2023-07-11 12:42:57,919][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 12:42:57,919][233954] FPS: 330121.31[0m
[36m[2023-07-11 12:43:02,157][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:43:02,158][233954] Reward + Measures: [[33.0834328   0.25775033  0.20311569  0.14981833  0.35170305  0.5921852 ]][0m
[37m[1m[2023-07-11 12:43:02,158][233954] Max Reward on eval: 33.08343279811647[0m
[37m[1m[2023-07-11 12:43:02,158][233954] Min Reward on eval: 33.08343279811647[0m
[37m[1m[2023-07-11 12:43:02,159][233954] Mean Reward across all agents: 33.08343279811647[0m
[37m[1m[2023-07-11 12:43:02,159][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:43:07,122][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:43:07,123][233954] Reward + Measures: [[61.65534329  0.12909999  0.1041      0.1265      0.3229      0.59674543]
 [44.96130881  0.16800001  0.21350001  0.1556      0.27109998  0.78056425]
 [38.69201195  0.21350002  0.24459998  0.13950001  0.40300003  0.68173516]
 ...
 [49.71123717  0.2386      0.26430002  0.23550001  0.4152      0.84602815]
 [47.288149    0.27220002  0.16689999  0.22490001  0.39480001  0.6319474 ]
 [40.60697108  0.19660001  0.32179999  0.25890002  0.3369      0.96350878]][0m
[37m[1m[2023-07-11 12:43:07,123][233954] Max Reward on eval: 136.7457562015392[0m
[37m[1m[2023-07-11 12:43:07,123][233954] Min Reward on eval: -42.84290140084922[0m
[37m[1m[2023-07-11 12:43:07,123][233954] Mean Reward across all agents: 37.801707925771034[0m
[37m[1m[2023-07-11 12:43:07,124][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:43:07,126][233954] mean_value=-315.3796558311807, max_value=470.93820062752815[0m
[37m[1m[2023-07-11 12:43:07,128][233954] New mean coefficients: [[-0.01373326  0.02910423 -0.4141432  -0.06343908 -0.03607865 -0.8194431 ]][0m
[37m[1m[2023-07-11 12:43:07,129][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:43:16,230][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 12:43:16,230][233954] FPS: 422032.34[0m
[36m[2023-07-11 12:43:16,232][233954] itr=937, itrs=2000, Progress: 46.85%[0m
[36m[2023-07-11 12:43:28,039][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 12:43:28,040][233954] FPS: 327795.05[0m
[36m[2023-07-11 12:43:32,263][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:43:32,269][233954] Reward + Measures: [[24.68049672  0.32394966  0.22248067  0.20587532  0.37475601  0.49232787]][0m
[37m[1m[2023-07-11 12:43:32,269][233954] Max Reward on eval: 24.68049671978525[0m
[37m[1m[2023-07-11 12:43:32,269][233954] Min Reward on eval: 24.68049671978525[0m
[37m[1m[2023-07-11 12:43:32,270][233954] Mean Reward across all agents: 24.68049671978525[0m
[37m[1m[2023-07-11 12:43:32,270][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:43:37,226][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:43:37,226][233954] Reward + Measures: [[26.34325391  0.17729999  0.15350001  0.1093      0.23779999  0.55716032]
 [86.98318828  0.52470005  0.41219997  0.45830002  0.56310004  0.65183127]
 [79.4440749   0.1097      0.1441      0.24419999  0.38410002  0.74313283]
 ...
 [19.29263208  0.49429998  0.39799997  0.2983      0.5492      0.58203626]
 [16.2754585   0.117       0.0377      0.0358      0.23409998  0.38306519]
 [ 6.8704676   0.18230002  0.0405      0.0243      0.2256      0.33500376]][0m
[37m[1m[2023-07-11 12:43:37,227][233954] Max Reward on eval: 202.84195153699255[0m
[37m[1m[2023-07-11 12:43:37,227][233954] Min Reward on eval: -43.19122654469684[0m
[37m[1m[2023-07-11 12:43:37,227][233954] Mean Reward across all agents: 28.637512148076492[0m
[37m[1m[2023-07-11 12:43:37,227][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:43:37,232][233954] mean_value=-81.19457911404308, max_value=524.2902306249831[0m
[37m[1m[2023-07-11 12:43:37,234][233954] New mean coefficients: [[-0.03744087  0.12384015 -0.4647667  -0.09751707 -0.10250703 -0.8765442 ]][0m
[37m[1m[2023-07-11 12:43:37,235][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:43:46,201][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 12:43:46,201][233954] FPS: 428367.91[0m
[36m[2023-07-11 12:43:46,203][233954] itr=938, itrs=2000, Progress: 46.90%[0m
[36m[2023-07-11 12:43:57,765][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 12:43:57,765][233954] FPS: 334869.65[0m
[36m[2023-07-11 12:44:02,127][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:44:02,128][233954] Reward + Measures: [[23.00312332  0.37584496  0.21260233  0.20479466  0.35271564  0.45815864]][0m
[37m[1m[2023-07-11 12:44:02,128][233954] Max Reward on eval: 23.00312331744431[0m
[37m[1m[2023-07-11 12:44:02,128][233954] Min Reward on eval: 23.00312331744431[0m
[37m[1m[2023-07-11 12:44:02,129][233954] Mean Reward across all agents: 23.00312331744431[0m
[37m[1m[2023-07-11 12:44:02,129][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:44:07,417][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:44:07,418][233954] Reward + Measures: [[ 20.36162025   0.4073       0.24860001   0.3145       0.39980003
    0.45456076]
 [ 24.106969     0.2211       0.1437       0.1247       0.2457
    0.64709616]
 [ 66.8170453    0.29800001   0.10829999   0.15810001   0.27849999
    0.64611143]
 ...
 [ 51.01350334   0.3529       0.27509999   0.28050002   0.42290002
    0.8079201 ]
 [128.06647099   0.41299996   0.2814       0.32109997   0.51870006
    0.960971  ]
 [ 61.47432923   0.3177       0.20419998   0.20460002   0.36050001
    0.78792602]][0m
[37m[1m[2023-07-11 12:44:07,418][233954] Max Reward on eval: 398.54543090779333[0m
[37m[1m[2023-07-11 12:44:07,418][233954] Min Reward on eval: -0.6025056327693165[0m
[37m[1m[2023-07-11 12:44:07,419][233954] Mean Reward across all agents: 40.24769360438255[0m
[37m[1m[2023-07-11 12:44:07,419][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:44:07,425][233954] mean_value=-170.89931264347658, max_value=517.240505544175[0m
[37m[1m[2023-07-11 12:44:07,428][233954] New mean coefficients: [[ 0.02335888  0.0325266  -0.4831378  -0.10050093 -0.13652764 -1.1112598 ]][0m
[37m[1m[2023-07-11 12:44:07,429][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:44:16,570][233954] train() took 9.14 seconds to complete[0m
[36m[2023-07-11 12:44:16,570][233954] FPS: 420166.38[0m
[36m[2023-07-11 12:44:16,572][233954] itr=939, itrs=2000, Progress: 46.95%[0m
[36m[2023-07-11 12:44:28,170][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 12:44:28,170][233954] FPS: 333901.97[0m
[36m[2023-07-11 12:44:32,461][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:44:32,466][233954] Reward + Measures: [[22.16191026  0.36229134  0.20631099  0.204955    0.36023203  0.42535204]][0m
[37m[1m[2023-07-11 12:44:32,466][233954] Max Reward on eval: 22.16191026040764[0m
[37m[1m[2023-07-11 12:44:32,467][233954] Min Reward on eval: 22.16191026040764[0m
[37m[1m[2023-07-11 12:44:32,467][233954] Mean Reward across all agents: 22.16191026040764[0m
[37m[1m[2023-07-11 12:44:32,467][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:44:37,439][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:44:37,439][233954] Reward + Measures: [[ 9.62530957  0.37720001  0.21149997  0.1918      0.34290001  0.38521889]
 [23.77553359  0.3044      0.13360001  0.1237      0.24070001  0.44187751]
 [14.98957143  0.2854      0.13310002  0.1283      0.28260002  0.37954327]
 ...
 [13.57352582  0.42880002  0.21659999  0.28909999  0.4364      0.370116  ]
 [20.58183884  0.17390001  0.1652      0.1603      0.31470001  0.42349973]
 [21.48107048  0.18820001  0.0704      0.0562      0.1603      0.36611333]][0m
[37m[1m[2023-07-11 12:44:37,439][233954] Max Reward on eval: 245.56472682794555[0m
[37m[1m[2023-07-11 12:44:37,440][233954] Min Reward on eval: 1.3289077632594855[0m
[37m[1m[2023-07-11 12:44:37,440][233954] Mean Reward across all agents: 25.20938116306364[0m
[37m[1m[2023-07-11 12:44:37,440][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:44:37,446][233954] mean_value=-13.196161811500557, max_value=549.1925857468275[0m
[37m[1m[2023-07-11 12:44:37,449][233954] New mean coefficients: [[ 0.01671043 -0.01034799 -0.43081248 -0.16800375 -0.13190699 -1.1543391 ]][0m
[37m[1m[2023-07-11 12:44:37,450][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:44:46,478][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 12:44:46,478][233954] FPS: 425421.62[0m
[36m[2023-07-11 12:44:46,480][233954] itr=940, itrs=2000, Progress: 47.00%[0m
[37m[1m[2023-07-11 12:48:14,322][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000920[0m
[36m[2023-07-11 12:48:26,564][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 12:48:26,564][233954] FPS: 329917.91[0m
[36m[2023-07-11 12:48:30,815][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:48:30,816][233954] Reward + Measures: [[22.2748613   0.35382602  0.20533     0.206085    0.37413535  0.42369613]][0m
[37m[1m[2023-07-11 12:48:30,816][233954] Max Reward on eval: 22.27486130060163[0m
[37m[1m[2023-07-11 12:48:30,816][233954] Min Reward on eval: 22.27486130060163[0m
[37m[1m[2023-07-11 12:48:30,816][233954] Mean Reward across all agents: 22.27486130060163[0m
[37m[1m[2023-07-11 12:48:30,816][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:48:35,742][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:48:35,742][233954] Reward + Measures: [[15.92527954  0.55419999  0.5126      0.4896      0.60910004  0.7282424 ]
 [ 8.57407817  0.19340001  0.1355      0.11870001  0.29350001  0.40173885]
 [13.22538754  0.45890003  0.3942      0.4831      0.58240002  0.40448037]
 ...
 [18.35857502  0.43070003  0.23280001  0.30779999  0.50470001  0.39062765]
 [ 0.95669565  0.72589999  0.68990004  0.74279994  0.76330006  0.38490805]
 [10.74595071  0.32749999  0.3242      0.2397      0.42490003  0.58393252]][0m
[37m[1m[2023-07-11 12:48:35,743][233954] Max Reward on eval: 275.72556559015067[0m
[37m[1m[2023-07-11 12:48:35,743][233954] Min Reward on eval: -3.596056150679942[0m
[37m[1m[2023-07-11 12:48:35,743][233954] Mean Reward across all agents: 28.824337589229174[0m
[37m[1m[2023-07-11 12:48:35,743][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:48:35,748][233954] mean_value=-59.43270995915797, max_value=472.30359093294476[0m
[37m[1m[2023-07-11 12:48:35,751][233954] New mean coefficients: [[ 0.04645557 -0.02548764 -0.3942545  -0.13617767 -0.12596543 -1.1229174 ]][0m
[37m[1m[2023-07-11 12:48:35,752][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:48:44,673][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 12:48:44,673][233954] FPS: 430526.39[0m
[36m[2023-07-11 12:48:44,675][233954] itr=941, itrs=2000, Progress: 47.05%[0m
[36m[2023-07-11 12:48:56,233][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 12:48:56,233][233954] FPS: 335044.68[0m
[36m[2023-07-11 12:49:00,444][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:49:00,444][233954] Reward + Measures: [[21.87240719  0.32290864  0.19477466  0.19308166  0.37262899  0.40727639]][0m
[37m[1m[2023-07-11 12:49:00,444][233954] Max Reward on eval: 21.872407191063065[0m
[37m[1m[2023-07-11 12:49:00,445][233954] Min Reward on eval: 21.872407191063065[0m
[37m[1m[2023-07-11 12:49:00,445][233954] Mean Reward across all agents: 21.872407191063065[0m
[37m[1m[2023-07-11 12:49:00,445][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:49:05,453][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:49:05,454][233954] Reward + Measures: [[39.46239675  0.4179      0.22129999  0.31210002  0.47569999  0.52981776]
 [13.61240072  0.45460001  0.31829998  0.37380001  0.51130003  0.34681082]
 [ 3.57373785  0.63239998  0.6577      0.60100001  0.71359998  0.46927747]
 ...
 [26.57526796  0.34240004  0.20380001  0.1807      0.38120002  0.41786775]
 [71.89511798  0.3321      0.12959999  0.1983      0.39109999  0.50854963]
 [26.48756684  0.29590002  0.2119      0.19660001  0.35089999  0.44775566]][0m
[37m[1m[2023-07-11 12:49:05,454][233954] Max Reward on eval: 86.31959405727684[0m
[37m[1m[2023-07-11 12:49:05,454][233954] Min Reward on eval: -13.708295912854373[0m
[37m[1m[2023-07-11 12:49:05,454][233954] Mean Reward across all agents: 18.123475982581592[0m
[37m[1m[2023-07-11 12:49:05,455][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:49:05,459][233954] mean_value=-60.909313130032615, max_value=437.49934990505915[0m
[37m[1m[2023-07-11 12:49:05,461][233954] New mean coefficients: [[-0.00731881 -0.08155969 -0.28586075 -0.1506584  -0.09560645 -0.69017637]][0m
[37m[1m[2023-07-11 12:49:05,462][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:49:14,466][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 12:49:14,466][233954] FPS: 426583.96[0m
[36m[2023-07-11 12:49:14,468][233954] itr=942, itrs=2000, Progress: 47.10%[0m
[36m[2023-07-11 12:49:26,056][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 12:49:26,056][233954] FPS: 334197.73[0m
[36m[2023-07-11 12:49:30,359][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:49:30,359][233954] Reward + Measures: [[20.64556471  0.308963    0.20792033  0.21013999  0.38933668  0.3912847 ]][0m
[37m[1m[2023-07-11 12:49:30,359][233954] Max Reward on eval: 20.64556470884251[0m
[37m[1m[2023-07-11 12:49:30,360][233954] Min Reward on eval: 20.64556470884251[0m
[37m[1m[2023-07-11 12:49:30,360][233954] Mean Reward across all agents: 20.64556470884251[0m
[37m[1m[2023-07-11 12:49:30,360][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:49:35,332][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:49:35,333][233954] Reward + Measures: [[20.52405398  0.2658      0.1295      0.117       0.28480002  0.44934392]
 [12.32904664  0.48839998  0.3976      0.37360001  0.509       0.37930241]
 [16.6241883   0.5248      0.41580001  0.4842      0.53109998  0.52309006]
 ...
 [67.21247925  0.16719998  0.035       0.12890001  0.28440002  0.57333153]
 [69.37224555  0.3335      0.22739999  0.28649998  0.44129997  0.57268381]
 [24.20469791  0.32769999  0.235       0.2172      0.32589999  0.47361252]][0m
[37m[1m[2023-07-11 12:49:35,333][233954] Max Reward on eval: 191.54777079655324[0m
[37m[1m[2023-07-11 12:49:35,333][233954] Min Reward on eval: -30.759714473132043[0m
[37m[1m[2023-07-11 12:49:35,334][233954] Mean Reward across all agents: 26.236023244060778[0m
[37m[1m[2023-07-11 12:49:35,334][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:49:35,338][233954] mean_value=-148.59988163155077, max_value=447.2613733773484[0m
[37m[1m[2023-07-11 12:49:35,341][233954] New mean coefficients: [[ 0.0585227  -0.09569256 -0.22844136 -0.12388603 -0.05726631 -0.61592084]][0m
[37m[1m[2023-07-11 12:49:35,342][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:49:44,261][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 12:49:44,261][233954] FPS: 430613.03[0m
[36m[2023-07-11 12:49:44,264][233954] itr=943, itrs=2000, Progress: 47.15%[0m
[36m[2023-07-11 12:49:55,928][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 12:49:55,929][233954] FPS: 331952.63[0m
[36m[2023-07-11 12:50:00,211][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:50:00,211][233954] Reward + Measures: [[19.94301374  0.30637598  0.20229234  0.207058    0.37467065  0.37820068]][0m
[37m[1m[2023-07-11 12:50:00,212][233954] Max Reward on eval: 19.943013738744195[0m
[37m[1m[2023-07-11 12:50:00,212][233954] Min Reward on eval: 19.943013738744195[0m
[37m[1m[2023-07-11 12:50:00,212][233954] Mean Reward across all agents: 19.943013738744195[0m
[37m[1m[2023-07-11 12:50:00,212][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:50:05,407][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:50:05,407][233954] Reward + Measures: [[-1.77048551  0.36810002  0.23280001  0.271       0.4427      0.51132101]
 [13.01178867  0.61180001  0.5679      0.48569998  0.63360006  0.4771398 ]
 [18.70912506  0.29980001  0.22360002  0.198       0.35390002  0.45480967]
 ...
 [ 8.2765902   0.37359998  0.38709998  0.33610001  0.49499997  0.41975379]
 [18.19925691  0.28630003  0.15540001  0.14230001  0.24099998  0.60025162]
 [15.84889869  0.31820002  0.31030002  0.31710002  0.42700002  0.44607267]][0m
[37m[1m[2023-07-11 12:50:05,408][233954] Max Reward on eval: 301.4911570686847[0m
[37m[1m[2023-07-11 12:50:05,408][233954] Min Reward on eval: -150.7419831379666[0m
[37m[1m[2023-07-11 12:50:05,408][233954] Mean Reward across all agents: 19.23279809074212[0m
[37m[1m[2023-07-11 12:50:05,408][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:50:05,413][233954] mean_value=-135.7398226782602, max_value=567.9383214729372[0m
[37m[1m[2023-07-11 12:50:05,416][233954] New mean coefficients: [[ 0.01266667 -0.12959778 -0.24779752 -0.15654688 -0.1085785  -0.7334794 ]][0m
[37m[1m[2023-07-11 12:50:05,417][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:50:14,394][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 12:50:14,394][233954] FPS: 427895.26[0m
[36m[2023-07-11 12:50:14,396][233954] itr=944, itrs=2000, Progress: 47.20%[0m
[36m[2023-07-11 12:50:26,301][233954] train() took 11.81 seconds to complete[0m
[36m[2023-07-11 12:50:26,301][233954] FPS: 325121.64[0m
[36m[2023-07-11 12:50:30,507][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:50:30,508][233954] Reward + Measures: [[21.36484886  0.29103166  0.18236364  0.18806432  0.36146533  0.38443181]][0m
[37m[1m[2023-07-11 12:50:30,508][233954] Max Reward on eval: 21.364848860827802[0m
[37m[1m[2023-07-11 12:50:30,508][233954] Min Reward on eval: 21.364848860827802[0m
[37m[1m[2023-07-11 12:50:30,508][233954] Mean Reward across all agents: 21.364848860827802[0m
[37m[1m[2023-07-11 12:50:30,509][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:50:35,488][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:50:35,488][233954] Reward + Measures: [[ 26.71508151   0.1969       0.0648       0.1464       0.22400001
    0.70009077]
 [  2.61579264   0.3414       0.33220002   0.2863       0.42150003
    0.40951535]
 [  4.69624557   0.49070001   0.53560001   0.52150005   0.56119996
    0.46617541]
 ...
 [ 11.91716606   0.32710001   0.20939998   0.184        0.35390002
    0.55512065]
 [ 47.32236992   0.21900001   0.05160001   0.1245       0.32300001
    0.75623959]
 [100.1028099    0.1655       0.09139999   0.19890001   0.21780001
    0.78956318]][0m
[37m[1m[2023-07-11 12:50:35,488][233954] Max Reward on eval: 100.10280989939346[0m
[37m[1m[2023-07-11 12:50:35,489][233954] Min Reward on eval: -10.376247338205577[0m
[37m[1m[2023-07-11 12:50:35,489][233954] Mean Reward across all agents: 22.374777751554213[0m
[37m[1m[2023-07-11 12:50:35,489][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:50:35,493][233954] mean_value=-107.53326422375316, max_value=511.89737042980266[0m
[37m[1m[2023-07-11 12:50:35,496][233954] New mean coefficients: [[ 0.06435251 -0.13721338 -0.18513593 -0.18310872 -0.11075132 -0.70496035]][0m
[37m[1m[2023-07-11 12:50:35,497][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:50:44,599][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 12:50:44,600][233954] FPS: 421944.88[0m
[36m[2023-07-11 12:50:44,602][233954] itr=945, itrs=2000, Progress: 47.25%[0m
[36m[2023-07-11 12:50:56,310][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 12:50:56,310][233954] FPS: 330743.01[0m
[36m[2023-07-11 12:51:00,648][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:51:00,648][233954] Reward + Measures: [[21.20008608  0.27620134  0.18087764  0.18136498  0.34821564  0.37183234]][0m
[37m[1m[2023-07-11 12:51:00,648][233954] Max Reward on eval: 21.200086084504772[0m
[37m[1m[2023-07-11 12:51:00,649][233954] Min Reward on eval: 21.200086084504772[0m
[37m[1m[2023-07-11 12:51:00,649][233954] Mean Reward across all agents: 21.200086084504772[0m
[37m[1m[2023-07-11 12:51:00,649][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:51:05,610][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:51:05,611][233954] Reward + Measures: [[11.71523601  0.14549999  0.0438      0.0308      0.3057      0.36224204]
 [13.31787345  0.36620003  0.31710002  0.29980001  0.41980001  0.35952672]
 [12.54350958  0.1444      0.06040001  0.14140001  0.29710004  0.39540505]
 ...
 [35.82226241  0.43670002  0.4276      0.4183      0.53610003  0.50970751]
 [15.07883529  0.36679998  0.30630001  0.29000005  0.52060002  0.34273782]
 [19.39033313  0.15809999  0.16960001  0.17110001  0.2915      0.36761796]][0m
[37m[1m[2023-07-11 12:51:05,611][233954] Max Reward on eval: 115.16588830403052[0m
[37m[1m[2023-07-11 12:51:05,611][233954] Min Reward on eval: -39.02665567144286[0m
[37m[1m[2023-07-11 12:51:05,611][233954] Mean Reward across all agents: 17.58280941156414[0m
[37m[1m[2023-07-11 12:51:05,612][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:51:05,615][233954] mean_value=-91.59334350095432, max_value=347.53445695726566[0m
[37m[1m[2023-07-11 12:51:05,618][233954] New mean coefficients: [[ 0.04013813 -0.1570461  -0.15804872 -0.1655485  -0.07063885 -0.8061147 ]][0m
[37m[1m[2023-07-11 12:51:05,619][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:51:14,561][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 12:51:14,561][233954] FPS: 429506.57[0m
[36m[2023-07-11 12:51:14,563][233954] itr=946, itrs=2000, Progress: 47.30%[0m
[36m[2023-07-11 12:51:26,106][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 12:51:26,106][233954] FPS: 335411.32[0m
[36m[2023-07-11 12:51:30,341][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:51:30,342][233954] Reward + Measures: [[20.16090997  0.26253399  0.17583233  0.17508265  0.34586003  0.36750039]][0m
[37m[1m[2023-07-11 12:51:30,342][233954] Max Reward on eval: 20.160909965083075[0m
[37m[1m[2023-07-11 12:51:30,342][233954] Min Reward on eval: 20.160909965083075[0m
[37m[1m[2023-07-11 12:51:30,342][233954] Mean Reward across all agents: 20.160909965083075[0m
[37m[1m[2023-07-11 12:51:30,343][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:51:35,254][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:51:35,254][233954] Reward + Measures: [[25.84518345  0.1839      0.13970001  0.12460001  0.39390001  0.47248241]
 [15.58534352  0.29840001  0.24600001  0.2534      0.32940003  0.35800263]
 [20.66387633  0.31290004  0.21780001  0.20510001  0.4025      0.36511812]
 ...
 [ 9.5292846   0.70209998  0.83070004  0.68449992  0.77630001  1.79487836]
 [38.657244    0.21589999  0.09320001  0.13450001  0.2361      0.58737671]
 [17.4829065   0.24660002  0.037       0.0258      0.28380004  0.27356997]][0m
[37m[1m[2023-07-11 12:51:35,255][233954] Max Reward on eval: 565.5547637615236[0m
[37m[1m[2023-07-11 12:51:35,255][233954] Min Reward on eval: -34.7638365356368[0m
[37m[1m[2023-07-11 12:51:35,255][233954] Mean Reward across all agents: 35.58106870983914[0m
[37m[1m[2023-07-11 12:51:35,255][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:51:35,259][233954] mean_value=-66.07477498154893, max_value=597.5334525502831[0m
[37m[1m[2023-07-11 12:51:35,262][233954] New mean coefficients: [[ 0.02882969 -0.1843596  -0.1733068  -0.10837592 -0.00543942 -0.7180209 ]][0m
[37m[1m[2023-07-11 12:51:35,263][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:51:44,166][233954] train() took 8.90 seconds to complete[0m
[36m[2023-07-11 12:51:44,166][233954] FPS: 431392.42[0m
[36m[2023-07-11 12:51:44,168][233954] itr=947, itrs=2000, Progress: 47.35%[0m
[36m[2023-07-11 12:51:55,773][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 12:51:55,773][233954] FPS: 333583.04[0m
[36m[2023-07-11 12:52:00,019][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:52:00,019][233954] Reward + Measures: [[22.84198698  0.25960568  0.18273568  0.18966635  0.37471268  0.36936682]][0m
[37m[1m[2023-07-11 12:52:00,019][233954] Max Reward on eval: 22.8419869796434[0m
[37m[1m[2023-07-11 12:52:00,020][233954] Min Reward on eval: 22.8419869796434[0m
[37m[1m[2023-07-11 12:52:00,020][233954] Mean Reward across all agents: 22.8419869796434[0m
[37m[1m[2023-07-11 12:52:00,020][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:52:04,987][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:52:04,988][233954] Reward + Measures: [[12.19673884  0.4817      0.39470002  0.49740002  0.5848      0.41584349]
 [81.27329799  0.50909996  0.4034      0.41780001  0.60529995  0.6598354 ]
 [22.85875587  0.19929999  0.1245      0.12        0.37240002  0.26264307]
 ...
 [14.21282453  0.19530001  0.14009999  0.13700001  0.31909999  0.26227352]
 [19.09314323  0.29899999  0.22710001  0.1208      0.33860001  0.46773529]
 [20.33075363  0.26460001  0.21660002  0.2053      0.49349999  0.30408922]][0m
[37m[1m[2023-07-11 12:52:04,988][233954] Max Reward on eval: 117.29662897862727[0m
[37m[1m[2023-07-11 12:52:04,988][233954] Min Reward on eval: -12.597506104386412[0m
[37m[1m[2023-07-11 12:52:04,988][233954] Mean Reward across all agents: 19.340232266986778[0m
[37m[1m[2023-07-11 12:52:04,989][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:52:04,992][233954] mean_value=-165.78695661781836, max_value=523.9993083990878[0m
[37m[1m[2023-07-11 12:52:04,995][233954] New mean coefficients: [[-0.02580112 -0.13852796 -0.15204805 -0.15899444 -0.11661567 -0.8460046 ]][0m
[37m[1m[2023-07-11 12:52:04,996][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:52:14,008][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 12:52:14,008][233954] FPS: 426164.97[0m
[36m[2023-07-11 12:52:14,010][233954] itr=948, itrs=2000, Progress: 47.40%[0m
[36m[2023-07-11 12:52:25,718][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 12:52:25,718][233954] FPS: 330657.54[0m
[36m[2023-07-11 12:52:30,082][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:52:30,082][233954] Reward + Measures: [[22.77578553  0.24463302  0.17505433  0.19307201  0.36970133  0.35871673]][0m
[37m[1m[2023-07-11 12:52:30,082][233954] Max Reward on eval: 22.775785534143825[0m
[37m[1m[2023-07-11 12:52:30,083][233954] Min Reward on eval: 22.775785534143825[0m
[37m[1m[2023-07-11 12:52:30,083][233954] Mean Reward across all agents: 22.775785534143825[0m
[37m[1m[2023-07-11 12:52:30,083][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:52:35,085][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:52:35,085][233954] Reward + Measures: [[20.7870844   0.1019      0.0379      0.0303      0.289       0.34669274]
 [24.51499831  0.27780002  0.0514      0.18819998  0.34960002  0.48109561]
 [20.12547136  0.33430001  0.21360002  0.20100002  0.51370001  0.31952584]
 ...
 [21.69657922  0.31170002  0.2339      0.21829998  0.36740002  0.41046149]
 [39.76158278  0.35310003  0.23410001  0.2306      0.40079999  0.62775034]
 [28.35244389  0.36269999  0.21870001  0.294       0.43870002  0.44878027]][0m
[37m[1m[2023-07-11 12:52:35,086][233954] Max Reward on eval: 133.6848186464049[0m
[37m[1m[2023-07-11 12:52:35,086][233954] Min Reward on eval: -0.9989143116166815[0m
[37m[1m[2023-07-11 12:52:35,086][233954] Mean Reward across all agents: 23.926545419363343[0m
[37m[1m[2023-07-11 12:52:35,086][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:52:35,090][233954] mean_value=-124.62584668337108, max_value=531.685289604209[0m
[37m[1m[2023-07-11 12:52:35,092][233954] New mean coefficients: [[ 0.00378642 -0.13672088 -0.12908453 -0.12075089 -0.05559757 -0.9125706 ]][0m
[37m[1m[2023-07-11 12:52:35,093][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:52:44,076][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 12:52:44,076][233954] FPS: 427569.92[0m
[36m[2023-07-11 12:52:44,079][233954] itr=949, itrs=2000, Progress: 47.45%[0m
[36m[2023-07-11 12:52:55,834][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 12:52:55,834][233954] FPS: 329370.99[0m
[36m[2023-07-11 12:53:00,107][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:53:00,107][233954] Reward + Measures: [[24.27275126  0.23935367  0.18474735  0.19635001  0.36271065  0.37077421]][0m
[37m[1m[2023-07-11 12:53:00,108][233954] Max Reward on eval: 24.272751263073125[0m
[37m[1m[2023-07-11 12:53:00,108][233954] Min Reward on eval: 24.272751263073125[0m
[37m[1m[2023-07-11 12:53:00,108][233954] Mean Reward across all agents: 24.272751263073125[0m
[37m[1m[2023-07-11 12:53:00,108][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:53:05,365][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:53:05,366][233954] Reward + Measures: [[47.55023941  0.48920003  0.3933      0.44180003  0.58100003  0.56443542]
 [23.91424529  0.1031      0.06940001  0.0746      0.17910001  0.32761502]
 [22.34161015  0.13600001  0.0501      0.0505      0.22819999  0.40848255]
 ...
 [20.79174211  0.17350002  0.14600001  0.12630001  0.31580001  0.34946361]
 [12.63728063  0.39300001  0.3159      0.47090003  0.44259998  0.38464031]
 [16.35639515  0.2974      0.252       0.24030001  0.3152      0.45102215]][0m
[37m[1m[2023-07-11 12:53:05,366][233954] Max Reward on eval: 123.15553874572507[0m
[37m[1m[2023-07-11 12:53:05,366][233954] Min Reward on eval: -7.212549596186728[0m
[37m[1m[2023-07-11 12:53:05,367][233954] Mean Reward across all agents: 24.749278885032883[0m
[37m[1m[2023-07-11 12:53:05,367][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:53:05,371][233954] mean_value=-85.67947349752308, max_value=554.8124231206718[0m
[37m[1m[2023-07-11 12:53:05,374][233954] New mean coefficients: [[ 0.04321416 -0.13415638 -0.20786697 -0.10771934 -0.0669174  -0.90356207]][0m
[37m[1m[2023-07-11 12:53:05,375][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:53:14,299][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 12:53:14,300][233954] FPS: 430341.47[0m
[36m[2023-07-11 12:53:14,302][233954] itr=950, itrs=2000, Progress: 47.50%[0m
[37m[1m[2023-07-11 12:56:50,127][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000930[0m
[36m[2023-07-11 12:57:02,540][233954] train() took 11.72 seconds to complete[0m
[36m[2023-07-11 12:57:02,540][233954] FPS: 327508.12[0m
[36m[2023-07-11 12:57:06,794][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:57:06,795][233954] Reward + Measures: [[23.36461411  0.22462201  0.17297502  0.186298    0.34734863  0.36144975]][0m
[37m[1m[2023-07-11 12:57:06,795][233954] Max Reward on eval: 23.364614109000946[0m
[37m[1m[2023-07-11 12:57:06,795][233954] Min Reward on eval: 23.364614109000946[0m
[37m[1m[2023-07-11 12:57:06,796][233954] Mean Reward across all agents: 23.364614109000946[0m
[37m[1m[2023-07-11 12:57:06,796][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:57:11,685][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:57:11,685][233954] Reward + Measures: [[20.30139478  0.28459999  0.1534      0.22309999  0.38680002  0.3579666 ]
 [15.61794553  0.40400001  0.29960001  0.36090001  0.50999999  0.36283106]
 [19.04008781  0.34670001  0.30990002  0.29960001  0.44889998  0.35633188]
 ...
 [48.14828719  0.34010002  0.20899999  0.12280001  0.39390001  0.53299475]
 [10.66640115  0.41939998  0.4025      0.36950001  0.51230001  0.44363615]
 [11.21878337  0.23169999  0.1339      0.1763      0.35250002  0.71479952]][0m
[37m[1m[2023-07-11 12:57:11,685][233954] Max Reward on eval: 244.50038112293697[0m
[37m[1m[2023-07-11 12:57:11,686][233954] Min Reward on eval: -14.128840498899809[0m
[37m[1m[2023-07-11 12:57:11,686][233954] Mean Reward across all agents: 26.751139709536552[0m
[37m[1m[2023-07-11 12:57:11,686][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:57:11,689][233954] mean_value=-114.38545709877721, max_value=515.4467844602884[0m
[37m[1m[2023-07-11 12:57:11,692][233954] New mean coefficients: [[ 0.03766634 -0.10777885 -0.20006794 -0.07494998 -0.05403512 -1.081325  ]][0m
[37m[1m[2023-07-11 12:57:11,693][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:57:20,602][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 12:57:20,602][233954] FPS: 431107.44[0m
[36m[2023-07-11 12:57:20,604][233954] itr=951, itrs=2000, Progress: 47.55%[0m
[36m[2023-07-11 12:57:32,125][233954] train() took 11.42 seconds to complete[0m
[36m[2023-07-11 12:57:32,125][233954] FPS: 336136.85[0m
[36m[2023-07-11 12:57:36,417][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:57:36,417][233954] Reward + Measures: [[24.23928558  0.22802535  0.16591398  0.17454267  0.33478132  0.34565878]][0m
[37m[1m[2023-07-11 12:57:36,417][233954] Max Reward on eval: 24.239285581013373[0m
[37m[1m[2023-07-11 12:57:36,418][233954] Min Reward on eval: 24.239285581013373[0m
[37m[1m[2023-07-11 12:57:36,418][233954] Mean Reward across all agents: 24.239285581013373[0m
[37m[1m[2023-07-11 12:57:36,418][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:57:41,361][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:57:41,362][233954] Reward + Measures: [[11.36929021  0.34469998  0.26180002  0.25780001  0.4021      0.62137079]
 [33.01115406  0.11799999  0.0535      0.0421      0.22510003  0.38970295]
 [30.331292    0.35780001  0.30880004  0.30090001  0.46630001  0.50176495]
 ...
 [17.3354017   0.12099999  0.0767      0.1044      0.16510001  0.39830586]
 [28.67495094  0.2969      0.22040001  0.20710002  0.37279999  0.62868416]
 [19.65414293  0.22930001  0.1239      0.11240001  0.30029997  0.31816134]][0m
[37m[1m[2023-07-11 12:57:41,362][233954] Max Reward on eval: 178.1075946688652[0m
[37m[1m[2023-07-11 12:57:41,362][233954] Min Reward on eval: -31.522506376262754[0m
[37m[1m[2023-07-11 12:57:41,363][233954] Mean Reward across all agents: 25.547310998737235[0m
[37m[1m[2023-07-11 12:57:41,363][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:57:41,366][233954] mean_value=-114.29019506465961, max_value=474.5732860346197[0m
[37m[1m[2023-07-11 12:57:41,369][233954] New mean coefficients: [[ 0.01232604 -0.09547619 -0.11590154  0.00687656  0.03063481 -1.0608579 ]][0m
[37m[1m[2023-07-11 12:57:41,370][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:57:50,373][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 12:57:50,373][233954] FPS: 426614.60[0m
[36m[2023-07-11 12:57:50,376][233954] itr=952, itrs=2000, Progress: 47.60%[0m
[36m[2023-07-11 12:58:02,205][233954] train() took 11.73 seconds to complete[0m
[36m[2023-07-11 12:58:02,205][233954] FPS: 327293.02[0m
[36m[2023-07-11 12:58:06,442][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:58:06,442][233954] Reward + Measures: [[23.76345251  0.22714567  0.16427399  0.17258099  0.3498033   0.34520864]][0m
[37m[1m[2023-07-11 12:58:06,442][233954] Max Reward on eval: 23.763452513279102[0m
[37m[1m[2023-07-11 12:58:06,443][233954] Min Reward on eval: 23.763452513279102[0m
[37m[1m[2023-07-11 12:58:06,443][233954] Mean Reward across all agents: 23.763452513279102[0m
[37m[1m[2023-07-11 12:58:06,443][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:58:11,484][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:58:11,485][233954] Reward + Measures: [[65.55131161  0.34450004  0.13650002  0.0502      0.27049997  0.58832455]
 [17.44409326  0.27310002  0.2306      0.22360002  0.34470001  0.35176563]
 [24.71589403  0.3211      0.25050002  0.21970001  0.3572      0.4987568 ]
 ...
 [20.43099329  0.15979999  0.067       0.0654      0.1838      0.27431718]
 [32.10482964  0.17930001  0.1578      0.16640002  0.29900002  0.42662916]
 [32.85818251  0.2823      0.1962      0.1821      0.33610001  0.33653048]][0m
[37m[1m[2023-07-11 12:58:11,485][233954] Max Reward on eval: 109.40007635513321[0m
[37m[1m[2023-07-11 12:58:11,485][233954] Min Reward on eval: -5.192471437854692[0m
[37m[1m[2023-07-11 12:58:11,486][233954] Mean Reward across all agents: 28.605183972223017[0m
[37m[1m[2023-07-11 12:58:11,486][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:58:11,490][233954] mean_value=-175.39282554663498, max_value=376.43652379932047[0m
[37m[1m[2023-07-11 12:58:11,492][233954] New mean coefficients: [[ 0.02503267 -0.10313659 -0.0461123  -0.00695     0.00815492 -1.0236045 ]][0m
[37m[1m[2023-07-11 12:58:11,493][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:58:20,468][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 12:58:20,468][233954] FPS: 427960.84[0m
[36m[2023-07-11 12:58:20,470][233954] itr=953, itrs=2000, Progress: 47.65%[0m
[36m[2023-07-11 12:58:32,035][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 12:58:32,035][233954] FPS: 334854.36[0m
[36m[2023-07-11 12:58:36,266][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:58:36,267][233954] Reward + Measures: [[23.41810113  0.18905833  0.154377    0.17193133  0.33509764  0.3305859 ]][0m
[37m[1m[2023-07-11 12:58:36,267][233954] Max Reward on eval: 23.41810113197768[0m
[37m[1m[2023-07-11 12:58:36,267][233954] Min Reward on eval: 23.41810113197768[0m
[37m[1m[2023-07-11 12:58:36,267][233954] Mean Reward across all agents: 23.41810113197768[0m
[37m[1m[2023-07-11 12:58:36,268][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:58:41,269][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:58:41,287][233954] Reward + Measures: [[32.0014876   0.19800001  0.1226      0.1156      0.49110004  0.3423076 ]
 [25.7068476   0.23410001  0.1569      0.15409999  0.25840002  0.34144065]
 [29.13833761  0.20539999  0.12029999  0.26280001  0.36199999  0.33956587]
 ...
 [23.91879821  0.1927      0.14300001  0.11220001  0.33750001  0.41274962]
 [13.82122094  0.28079998  0.24990001  0.271       0.38010001  0.28487143]
 [-1.20780092  0.27470002  0.23439999  0.23839998  0.37760001  0.2924934 ]][0m
[37m[1m[2023-07-11 12:58:41,288][233954] Max Reward on eval: 222.2225305947708[0m
[37m[1m[2023-07-11 12:58:41,288][233954] Min Reward on eval: -23.394487008143916[0m
[37m[1m[2023-07-11 12:58:41,288][233954] Mean Reward across all agents: 28.363654352379175[0m
[37m[1m[2023-07-11 12:58:41,288][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:58:41,292][233954] mean_value=-68.94725971856816, max_value=529.0194829775021[0m
[37m[1m[2023-07-11 12:58:41,295][233954] New mean coefficients: [[ 0.0322445  -0.08886942 -0.08703412  0.06293405  0.0867155  -0.91944414]][0m
[37m[1m[2023-07-11 12:58:41,296][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:58:50,249][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 12:58:50,249][233954] FPS: 429001.32[0m
[36m[2023-07-11 12:58:50,251][233954] itr=954, itrs=2000, Progress: 47.70%[0m
[36m[2023-07-11 12:59:01,836][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 12:59:01,842][233954] FPS: 334237.79[0m
[36m[2023-07-11 12:59:06,118][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:59:06,119][233954] Reward + Measures: [[21.69339585  0.23075166  0.16950765  0.18384168  0.33845067  0.33228478]][0m
[37m[1m[2023-07-11 12:59:06,119][233954] Max Reward on eval: 21.693395845419325[0m
[37m[1m[2023-07-11 12:59:06,119][233954] Min Reward on eval: 21.693395845419325[0m
[37m[1m[2023-07-11 12:59:06,120][233954] Mean Reward across all agents: 21.693395845419325[0m
[37m[1m[2023-07-11 12:59:06,120][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:59:11,322][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:59:11,323][233954] Reward + Measures: [[ 20.58034585   0.10550001   0.0453       0.0489       0.24870001
    0.3457709 ]
 [127.47274136   0.22790001   0.22319999   0.27160001   0.48750001
    1.07547724]
 [ 23.49682002   0.12970001   0.21859999   0.2676       0.25600001
    0.30919513]
 ...
 [ 23.55978596   0.249        0.22750001   0.21330002   0.3779
    0.41993281]
 [-24.47213631   0.53520006   0.58330005   0.48839998   0.63600004
    0.75596255]
 [ 31.27055503   0.0904       0.0597       0.0696       0.2343
    0.41012651]][0m
[37m[1m[2023-07-11 12:59:11,323][233954] Max Reward on eval: 366.37135595718865[0m
[37m[1m[2023-07-11 12:59:11,323][233954] Min Reward on eval: -29.90111744790338[0m
[37m[1m[2023-07-11 12:59:11,324][233954] Mean Reward across all agents: 27.55394445041051[0m
[37m[1m[2023-07-11 12:59:11,324][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:59:11,327][233954] mean_value=-177.54255544354896, max_value=382.8596194234282[0m
[37m[1m[2023-07-11 12:59:11,330][233954] New mean coefficients: [[ 0.1023876  -0.02621007 -0.04585574  0.0242716   0.12782376 -0.8182523 ]][0m
[37m[1m[2023-07-11 12:59:11,331][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:59:20,295][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 12:59:20,295][233954] FPS: 428481.62[0m
[36m[2023-07-11 12:59:20,297][233954] itr=955, itrs=2000, Progress: 47.75%[0m
[36m[2023-07-11 12:59:32,085][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 12:59:32,085][233954] FPS: 328390.56[0m
[36m[2023-07-11 12:59:36,340][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:59:36,341][233954] Reward + Measures: [[22.9279139   0.23578866  0.16763833  0.18465067  0.37266999  0.33315009]][0m
[37m[1m[2023-07-11 12:59:36,341][233954] Max Reward on eval: 22.92791390082324[0m
[37m[1m[2023-07-11 12:59:36,341][233954] Min Reward on eval: 22.92791390082324[0m
[37m[1m[2023-07-11 12:59:36,342][233954] Mean Reward across all agents: 22.92791390082324[0m
[37m[1m[2023-07-11 12:59:36,342][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:59:41,425][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 12:59:41,426][233954] Reward + Measures: [[14.31364709  0.4772      0.35340002  0.31870002  0.54020005  0.63127136]
 [15.10655505  0.2816      0.25569999  0.25920001  0.3479      0.35155445]
 [24.67026547  0.13689999  0.1041      0.12070002  0.20609999  0.45606384]
 ...
 [25.20531209  0.31750003  0.13079999  0.11900001  0.3554      0.51210493]
 [19.051952    0.2509      0.1487      0.1574      0.3696      0.3962816 ]
 [15.68303173  0.271       0.12890001  0.21210001  0.45839998  0.27878419]][0m
[37m[1m[2023-07-11 12:59:41,426][233954] Max Reward on eval: 131.68752235430875[0m
[37m[1m[2023-07-11 12:59:41,426][233954] Min Reward on eval: -28.216587979346514[0m
[37m[1m[2023-07-11 12:59:41,427][233954] Mean Reward across all agents: 23.407735775575095[0m
[37m[1m[2023-07-11 12:59:41,427][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 12:59:41,430][233954] mean_value=-137.68247197088152, max_value=336.5098481325553[0m
[37m[1m[2023-07-11 12:59:41,433][233954] New mean coefficients: [[ 0.09648643 -0.01709523 -0.10925911 -0.0260269   0.11286236 -0.6604756 ]][0m
[37m[1m[2023-07-11 12:59:41,434][233954] Moving the mean solution point...[0m
[36m[2023-07-11 12:59:50,515][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 12:59:50,515][233954] FPS: 422922.98[0m
[36m[2023-07-11 12:59:50,518][233954] itr=956, itrs=2000, Progress: 47.80%[0m
[36m[2023-07-11 13:00:02,312][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 13:00:02,313][233954] FPS: 328220.26[0m
[36m[2023-07-11 13:00:06,641][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:00:06,642][233954] Reward + Measures: [[21.67256958  0.26426432  0.17755833  0.18901135  0.38693231  0.33583203]][0m
[37m[1m[2023-07-11 13:00:06,642][233954] Max Reward on eval: 21.672569576071478[0m
[37m[1m[2023-07-11 13:00:06,642][233954] Min Reward on eval: 21.672569576071478[0m
[37m[1m[2023-07-11 13:00:06,642][233954] Mean Reward across all agents: 21.672569576071478[0m
[37m[1m[2023-07-11 13:00:06,643][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:00:11,723][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:00:11,723][233954] Reward + Measures: [[10.01351086  0.37919998  0.30520001  0.2775      0.46399999  0.48968849]
 [21.39422553  0.19859999  0.13719998  0.1226      0.3339      0.42690849]
 [16.7752494   0.29010001  0.26289999  0.24950002  0.31529999  0.45110545]
 ...
 [11.77296787  0.37910002  0.3127      0.19600001  0.51620001  0.51800525]
 [11.19035332  0.20310001  0.26640001  0.34919998  0.3651      0.3869836 ]
 [32.03428928  0.12        0.0802      0.26180002  0.23070002  0.35108498]][0m
[37m[1m[2023-07-11 13:00:11,724][233954] Max Reward on eval: 120.2799505468225[0m
[37m[1m[2023-07-11 13:00:11,724][233954] Min Reward on eval: -85.94463968740311[0m
[37m[1m[2023-07-11 13:00:11,724][233954] Mean Reward across all agents: 23.136164135925316[0m
[37m[1m[2023-07-11 13:00:11,724][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:00:11,729][233954] mean_value=-87.1375094854292, max_value=511.9851434815675[0m
[37m[1m[2023-07-11 13:00:11,731][233954] New mean coefficients: [[ 0.11682293 -0.03154502 -0.08201802  0.02170174  0.1562494  -0.5861423 ]][0m
[37m[1m[2023-07-11 13:00:11,732][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:00:20,717][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 13:00:20,717][233954] FPS: 427473.49[0m
[36m[2023-07-11 13:00:20,719][233954] itr=957, itrs=2000, Progress: 47.85%[0m
[36m[2023-07-11 13:00:32,355][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 13:00:32,356][233954] FPS: 332668.99[0m
[36m[2023-07-11 13:00:36,619][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:00:36,619][233954] Reward + Measures: [[20.51655869  0.26890332  0.18378699  0.186709    0.39834002  0.33265156]][0m
[37m[1m[2023-07-11 13:00:36,619][233954] Max Reward on eval: 20.51655868859213[0m
[37m[1m[2023-07-11 13:00:36,620][233954] Min Reward on eval: 20.51655868859213[0m
[37m[1m[2023-07-11 13:00:36,620][233954] Mean Reward across all agents: 20.51655868859213[0m
[37m[1m[2023-07-11 13:00:36,620][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:00:41,645][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:00:41,646][233954] Reward + Measures: [[29.94896714  0.22579999  0.17209999  0.1451      0.35929999  0.64907616]
 [76.22902392  0.2669      0.21760002  0.35170004  0.44440004  0.59958428]
 [33.20911053  0.0523      0.23510002  0.1886      0.20739999  0.75129765]
 ...
 [12.84194186  0.59030002  0.58540004  0.56820005  0.66760004  0.32422   ]
 [79.92936437  0.31890002  0.14649999  0.2339      0.39809999  0.50202221]
 [10.44069824  0.2685      0.3673      0.41350004  0.4289      0.3958894 ]][0m
[37m[1m[2023-07-11 13:00:41,646][233954] Max Reward on eval: 119.40853069182485[0m
[37m[1m[2023-07-11 13:00:41,647][233954] Min Reward on eval: -48.74549129982188[0m
[37m[1m[2023-07-11 13:00:41,647][233954] Mean Reward across all agents: 27.96388817159436[0m
[37m[1m[2023-07-11 13:00:41,647][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:00:41,651][233954] mean_value=-142.07697782402246, max_value=528.9595445566113[0m
[37m[1m[2023-07-11 13:00:41,654][233954] New mean coefficients: [[ 0.10959558 -0.08964846 -0.04189518  0.00856247  0.14791825 -0.6734512 ]][0m
[37m[1m[2023-07-11 13:00:41,655][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:00:50,694][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 13:00:50,700][233954] FPS: 424901.25[0m
[36m[2023-07-11 13:00:50,702][233954] itr=958, itrs=2000, Progress: 47.90%[0m
[36m[2023-07-11 13:01:02,335][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 13:01:02,336][233954] FPS: 332804.57[0m
[36m[2023-07-11 13:01:06,623][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:01:06,624][233954] Reward + Measures: [[20.03574498  0.26754335  0.18548633  0.18506433  0.37807029  0.33448926]][0m
[37m[1m[2023-07-11 13:01:06,624][233954] Max Reward on eval: 20.03574497921631[0m
[37m[1m[2023-07-11 13:01:06,624][233954] Min Reward on eval: 20.03574497921631[0m
[37m[1m[2023-07-11 13:01:06,624][233954] Mean Reward across all agents: 20.03574497921631[0m
[37m[1m[2023-07-11 13:01:06,625][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:01:11,600][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:01:11,600][233954] Reward + Measures: [[10.06570037  0.60500002  0.48769999  0.5575      0.67610002  0.51089019]
 [11.42863225  0.40000001  0.40419999  0.34200001  0.53290004  0.50814754]
 [11.60121234  0.36919999  0.35950002  0.29420003  0.3849      0.62978649]
 ...
 [18.9462604   0.44720003  0.31969997  0.30529997  0.43059999  0.36631775]
 [25.22923744  0.1314      0.0604      0.06820001  0.28399998  0.48544574]
 [23.43199252  0.14909999  0.0659      0.0537      0.2617      0.29225135]][0m
[37m[1m[2023-07-11 13:01:11,600][233954] Max Reward on eval: 92.31390447141602[0m
[37m[1m[2023-07-11 13:01:11,601][233954] Min Reward on eval: -25.81152728557936[0m
[37m[1m[2023-07-11 13:01:11,601][233954] Mean Reward across all agents: 20.61940136792917[0m
[37m[1m[2023-07-11 13:01:11,601][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:01:11,604][233954] mean_value=-96.63517835520749, max_value=513.4670806846349[0m
[37m[1m[2023-07-11 13:01:11,607][233954] New mean coefficients: [[ 0.1119361  -0.11876243 -0.04528048  0.02269204  0.09071854 -0.53412163]][0m
[37m[1m[2023-07-11 13:01:11,608][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:01:20,652][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 13:01:20,658][233954] FPS: 424654.48[0m
[36m[2023-07-11 13:01:20,660][233954] itr=959, itrs=2000, Progress: 47.95%[0m
[36m[2023-07-11 13:01:32,242][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 13:01:32,242][233954] FPS: 334381.23[0m
[36m[2023-07-11 13:01:36,492][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:01:36,498][233954] Reward + Measures: [[20.99855308  0.26472834  0.18167232  0.188769    0.39334464  0.32547531]][0m
[37m[1m[2023-07-11 13:01:36,498][233954] Max Reward on eval: 20.99855307911117[0m
[37m[1m[2023-07-11 13:01:36,498][233954] Min Reward on eval: 20.99855307911117[0m
[37m[1m[2023-07-11 13:01:36,498][233954] Mean Reward across all agents: 20.99855307911117[0m
[37m[1m[2023-07-11 13:01:36,499][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:01:41,537][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:01:41,543][233954] Reward + Measures: [[66.60873922  0.22850001  0.15310001  0.0609      0.34780002  0.48347369]
 [16.71280501  0.45759997  0.28200001  0.29609999  0.44790003  0.70074815]
 [ 8.9151537   0.30200002  0.3195      0.34890002  0.39070001  0.48407698]
 ...
 [21.80009173  0.2791      0.2784      0.38330004  0.38840005  0.48851964]
 [19.49567129  0.24389999  0.1288      0.11439999  0.3883      0.25473705]
 [ 9.71019805  0.34230003  0.35549998  0.35999998  0.42130002  0.5560289 ]][0m
[37m[1m[2023-07-11 13:01:41,543][233954] Max Reward on eval: 126.8401643047633[0m
[37m[1m[2023-07-11 13:01:41,544][233954] Min Reward on eval: -30.9503901280812[0m
[37m[1m[2023-07-11 13:01:41,544][233954] Mean Reward across all agents: 22.78595911181274[0m
[37m[1m[2023-07-11 13:01:41,544][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:01:41,548][233954] mean_value=-74.98461290455596, max_value=525.3731006413466[0m
[37m[1m[2023-07-11 13:01:41,551][233954] New mean coefficients: [[ 0.17190444 -0.09701207 -0.08725223  0.01616362  0.0351712  -0.46322897]][0m
[37m[1m[2023-07-11 13:01:41,552][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:01:50,551][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 13:01:50,557][233954] FPS: 426773.45[0m
[36m[2023-07-11 13:01:50,559][233954] itr=960, itrs=2000, Progress: 48.00%[0m
[37m[1m[2023-07-11 13:05:24,561][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000940[0m
[36m[2023-07-11 13:05:36,869][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 13:05:36,878][233954] FPS: 334019.51[0m
[36m[2023-07-11 13:05:41,124][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:05:41,124][233954] Reward + Measures: [[21.18482607  0.24652366  0.17737333  0.18379267  0.39809734  0.33365446]][0m
[37m[1m[2023-07-11 13:05:41,125][233954] Max Reward on eval: 21.184826073734065[0m
[37m[1m[2023-07-11 13:05:41,125][233954] Min Reward on eval: 21.184826073734065[0m
[37m[1m[2023-07-11 13:05:41,125][233954] Mean Reward across all agents: 21.184826073734065[0m
[37m[1m[2023-07-11 13:05:41,125][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:05:46,094][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:05:46,094][233954] Reward + Measures: [[ 30.66231524   0.31690001   0.2349       0.22259998   0.4059
    0.34866101]
 [ 25.09558259   0.17909999   0.1494       0.1596       0.36670002
    0.35628673]
 [ 18.44352714   0.25190002   0.0416       0.0205       0.2685
    0.32031509]
 ...
 [ 35.193645     0.24780002   0.1363       0.11490001   0.38410002
    0.38328701]
 [107.92682926   0.32230002   0.40739998   0.31820002   0.4973
    0.98477453]
 [ 23.08672322   0.35709998   0.1512       0.1455       0.25270003
    0.54271907]][0m
[37m[1m[2023-07-11 13:05:46,094][233954] Max Reward on eval: 160.4432033639401[0m
[37m[1m[2023-07-11 13:05:46,095][233954] Min Reward on eval: -8.404771449510008[0m
[37m[1m[2023-07-11 13:05:46,095][233954] Mean Reward across all agents: 25.174280359499807[0m
[37m[1m[2023-07-11 13:05:46,095][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:05:46,099][233954] mean_value=-148.89085432209563, max_value=474.20117821867575[0m
[37m[1m[2023-07-11 13:05:46,101][233954] New mean coefficients: [[ 0.21224724 -0.1190355  -0.04349336  0.03574359 -0.010075   -0.57424104]][0m
[37m[1m[2023-07-11 13:05:46,102][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:05:54,998][233954] train() took 8.89 seconds to complete[0m
[36m[2023-07-11 13:05:54,998][233954] FPS: 431763.16[0m
[36m[2023-07-11 13:05:55,000][233954] itr=961, itrs=2000, Progress: 48.05%[0m
[36m[2023-07-11 13:06:06,725][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 13:06:06,725][233954] FPS: 330246.19[0m
[36m[2023-07-11 13:06:10,943][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:06:10,943][233954] Reward + Measures: [[20.96397058  0.23492466  0.19958834  0.21182701  0.40545365  0.34206656]][0m
[37m[1m[2023-07-11 13:06:10,943][233954] Max Reward on eval: 20.96397057570935[0m
[37m[1m[2023-07-11 13:06:10,944][233954] Min Reward on eval: 20.96397057570935[0m
[37m[1m[2023-07-11 13:06:10,944][233954] Mean Reward across all agents: 20.96397057570935[0m
[37m[1m[2023-07-11 13:06:10,944][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:06:15,899][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:06:15,900][233954] Reward + Measures: [[84.87406761  0.31310001  0.1434      0.23100002  0.32189998  0.84019148]
 [ 9.12586687  0.54420006  0.58750004  0.58140004  0.64350003  0.42804763]
 [13.10079172  0.29999998  0.12219999  0.1019      0.36230001  0.41416845]
 ...
 [10.47945997  0.32460001  0.18730001  0.31799999  0.37840003  0.68056005]
 [18.46080692  0.2139      0.1576      0.17050001  0.31300002  0.46615687]
 [22.51230431  0.18450001  0.1276      0.14310001  0.3211      0.33550093]][0m
[37m[1m[2023-07-11 13:06:15,900][233954] Max Reward on eval: 107.59920459399[0m
[37m[1m[2023-07-11 13:06:15,901][233954] Min Reward on eval: -11.70906594293192[0m
[37m[1m[2023-07-11 13:06:15,901][233954] Mean Reward across all agents: 22.385590361924418[0m
[37m[1m[2023-07-11 13:06:15,901][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:06:15,904][233954] mean_value=-152.72560577774956, max_value=420.83479511358127[0m
[37m[1m[2023-07-11 13:06:15,907][233954] New mean coefficients: [[ 0.16450416 -0.08516768 -0.04165765  0.07221265  0.01595117 -0.4171803 ]][0m
[37m[1m[2023-07-11 13:06:15,908][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:06:24,954][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 13:06:24,954][233954] FPS: 424542.40[0m
[36m[2023-07-11 13:06:24,957][233954] itr=962, itrs=2000, Progress: 48.10%[0m
[36m[2023-07-11 13:06:36,686][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 13:06:36,686][233954] FPS: 330143.91[0m
[36m[2023-07-11 13:06:41,043][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:06:41,044][233954] Reward + Measures: [[22.29080543  0.21102233  0.18755899  0.215756    0.3923063   0.33402738]][0m
[37m[1m[2023-07-11 13:06:41,044][233954] Max Reward on eval: 22.29080542833567[0m
[37m[1m[2023-07-11 13:06:41,044][233954] Min Reward on eval: 22.29080542833567[0m
[37m[1m[2023-07-11 13:06:41,044][233954] Mean Reward across all agents: 22.29080542833567[0m
[37m[1m[2023-07-11 13:06:41,045][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:06:46,078][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:06:46,078][233954] Reward + Measures: [[25.91060401  0.28860003  0.3096      0.3804      0.48260003  0.31159446]
 [27.89836618  0.46750003  0.46100003  0.52540004  0.5165      0.40474936]
 [20.24029351  0.22140001  0.14230001  0.1469      0.27689999  0.36308143]
 ...
 [29.1071994   0.1178      0.14920001  0.2096      0.30900002  0.45810643]
 [22.86472126  0.14480001  0.14930001  0.15210001  0.29000002  0.36912844]
 [16.07611582  0.14380001  0.055       0.0701      0.25120002  0.30255476]][0m
[37m[1m[2023-07-11 13:06:46,079][233954] Max Reward on eval: 181.40224862425822[0m
[37m[1m[2023-07-11 13:06:46,079][233954] Min Reward on eval: -18.09707214870723[0m
[37m[1m[2023-07-11 13:06:46,079][233954] Mean Reward across all agents: 27.65273568472961[0m
[37m[1m[2023-07-11 13:06:46,079][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:06:46,082][233954] mean_value=-129.3575329867853, max_value=471.5742704852961[0m
[37m[1m[2023-07-11 13:06:46,085][233954] New mean coefficients: [[ 0.13164595 -0.11652555 -0.0393304   0.02534438  0.00085216 -0.43688038]][0m
[37m[1m[2023-07-11 13:06:46,086][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:06:55,082][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 13:06:55,082][233954] FPS: 426931.78[0m
[36m[2023-07-11 13:06:55,085][233954] itr=963, itrs=2000, Progress: 48.15%[0m
[36m[2023-07-11 13:07:06,724][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 13:07:06,724][233954] FPS: 332646.18[0m
[36m[2023-07-11 13:07:11,019][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:07:11,020][233954] Reward + Measures: [[21.86643752  0.21504067  0.19382033  0.22223568  0.38822469  0.33165795]][0m
[37m[1m[2023-07-11 13:07:11,020][233954] Max Reward on eval: 21.866437516934297[0m
[37m[1m[2023-07-11 13:07:11,020][233954] Min Reward on eval: 21.866437516934297[0m
[37m[1m[2023-07-11 13:07:11,020][233954] Mean Reward across all agents: 21.866437516934297[0m
[37m[1m[2023-07-11 13:07:11,021][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:07:16,005][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:07:16,006][233954] Reward + Measures: [[10.28875057  0.19530001  0.1283      0.11709999  0.2861      0.49585462]
 [24.0534649   0.2667      0.13509999  0.14820002  0.3141      0.45636597]
 [34.55221519  0.1138      0.09010001  0.19400001  0.25759998  0.50024408]
 ...
 [11.54861642  0.3231      0.1433      0.1461      0.2651      0.3755537 ]
 [23.97464936  0.25300002  0.1816      0.22469997  0.22810002  0.44573173]
 [58.55008324  0.37540004  0.21430002  0.1213      0.34230003  0.56404585]][0m
[37m[1m[2023-07-11 13:07:16,006][233954] Max Reward on eval: 143.35468173028204[0m
[37m[1m[2023-07-11 13:07:16,006][233954] Min Reward on eval: -24.750521384913007[0m
[37m[1m[2023-07-11 13:07:16,007][233954] Mean Reward across all agents: 24.109505369691085[0m
[37m[1m[2023-07-11 13:07:16,007][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:07:16,010][233954] mean_value=-168.69899984457226, max_value=510.74383779575584[0m
[37m[1m[2023-07-11 13:07:16,013][233954] New mean coefficients: [[ 0.15972844 -0.11822119 -0.08516148  0.06283374  0.04319588 -0.27088642]][0m
[37m[1m[2023-07-11 13:07:16,014][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:07:25,156][233954] train() took 9.14 seconds to complete[0m
[36m[2023-07-11 13:07:25,156][233954] FPS: 420143.26[0m
[36m[2023-07-11 13:07:25,158][233954] itr=964, itrs=2000, Progress: 48.20%[0m
[36m[2023-07-11 13:07:36,927][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 13:07:36,927][233954] FPS: 328916.91[0m
[36m[2023-07-11 13:07:41,159][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:07:41,160][233954] Reward + Measures: [[25.04555298  0.198323    0.169856    0.20639366  0.34609431  0.34318921]][0m
[37m[1m[2023-07-11 13:07:41,160][233954] Max Reward on eval: 25.04555298146539[0m
[37m[1m[2023-07-11 13:07:41,160][233954] Min Reward on eval: 25.04555298146539[0m
[37m[1m[2023-07-11 13:07:41,161][233954] Mean Reward across all agents: 25.04555298146539[0m
[37m[1m[2023-07-11 13:07:41,161][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:07:46,117][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:07:46,117][233954] Reward + Measures: [[20.54698119  0.31069997  0.0511      0.12119999  0.2985      0.38696003]
 [13.55674504  0.23889999  0.1664      0.16510001  0.2165      0.47848129]
 [21.99905179  0.21269999  0.22190002  0.1752      0.52240002  0.47739726]
 ...
 [33.85841394  0.24890001  0.16489999  0.1619      0.30950001  0.48012367]
 [19.61867786  0.2155      0.17220001  0.1656      0.23769999  0.46060839]
 [15.00488638  0.40840003  0.1292      0.20850001  0.35900006  0.54311132]][0m
[37m[1m[2023-07-11 13:07:46,118][233954] Max Reward on eval: 90.26370607193094[0m
[37m[1m[2023-07-11 13:07:46,118][233954] Min Reward on eval: -3.9021280011860653[0m
[37m[1m[2023-07-11 13:07:46,118][233954] Mean Reward across all agents: 22.782772464120583[0m
[37m[1m[2023-07-11 13:07:46,118][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:07:46,121][233954] mean_value=-185.35729889057333, max_value=327.04401233995196[0m
[37m[1m[2023-07-11 13:07:46,123][233954] New mean coefficients: [[ 0.20483345 -0.09941324 -0.15214616  0.0629039   0.08146282 -0.4797479 ]][0m
[37m[1m[2023-07-11 13:07:46,124][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:07:55,050][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 13:07:55,051][233954] FPS: 430277.67[0m
[36m[2023-07-11 13:07:55,053][233954] itr=965, itrs=2000, Progress: 48.25%[0m
[36m[2023-07-11 13:08:06,568][233954] train() took 11.42 seconds to complete[0m
[36m[2023-07-11 13:08:06,568][233954] FPS: 336200.05[0m
[36m[2023-07-11 13:08:10,902][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:08:10,902][233954] Reward + Measures: [[27.88722208  0.165934    0.164252    0.23237999  0.37765664  0.34422478]][0m
[37m[1m[2023-07-11 13:08:10,902][233954] Max Reward on eval: 27.88722207896324[0m
[37m[1m[2023-07-11 13:08:10,903][233954] Min Reward on eval: 27.88722207896324[0m
[37m[1m[2023-07-11 13:08:10,903][233954] Mean Reward across all agents: 27.88722207896324[0m
[37m[1m[2023-07-11 13:08:10,903][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:08:15,912][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:08:15,913][233954] Reward + Measures: [[29.69598803  0.0892      0.051       0.06260001  0.30730003  0.24842794]
 [30.2427641   0.0857      0.0706      0.0768      0.23029999  0.35627255]
 [28.62306143  0.13410001  0.16400002  0.20299999  0.27379999  0.44380793]
 ...
 [29.05178618  0.27590001  0.22750001  0.22190002  0.41999999  0.39540729]
 [26.68635584  0.1112      0.0521      0.0493      0.2951      0.35328174]
 [20.70354184  0.28740001  0.3285      0.32230002  0.36740002  0.79294205]][0m
[37m[1m[2023-07-11 13:08:15,913][233954] Max Reward on eval: 137.9391200594604[0m
[37m[1m[2023-07-11 13:08:15,913][233954] Min Reward on eval: -14.199393733777105[0m
[37m[1m[2023-07-11 13:08:15,914][233954] Mean Reward across all agents: 29.21345965949248[0m
[37m[1m[2023-07-11 13:08:15,914][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:08:15,917][233954] mean_value=-172.60495239596307, max_value=388.2111908428809[0m
[37m[1m[2023-07-11 13:08:15,919][233954] New mean coefficients: [[ 0.19952779 -0.04841746 -0.1912964   0.04671881  0.0817973  -0.34830666]][0m
[37m[1m[2023-07-11 13:08:15,920][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:08:24,975][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 13:08:24,975][233954] FPS: 424174.80[0m
[36m[2023-07-11 13:08:24,978][233954] itr=966, itrs=2000, Progress: 48.30%[0m
[36m[2023-07-11 13:08:36,671][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 13:08:36,671][233954] FPS: 331113.22[0m
[36m[2023-07-11 13:08:40,899][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:08:40,904][233954] Reward + Measures: [[25.9627094   0.13542999  0.16939999  0.23822132  0.21497698  0.60446304]][0m
[37m[1m[2023-07-11 13:08:40,905][233954] Max Reward on eval: 25.962709402466984[0m
[37m[1m[2023-07-11 13:08:40,905][233954] Min Reward on eval: 25.962709402466984[0m
[37m[1m[2023-07-11 13:08:40,905][233954] Mean Reward across all agents: 25.962709402466984[0m
[37m[1m[2023-07-11 13:08:40,906][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:08:45,874][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:08:45,874][233954] Reward + Measures: [[21.15520368  0.38059998  0.46179995  0.454       0.41089997  0.61841023]
 [12.06577075  0.22220002  0.30210003  0.26030001  0.28640002  0.92536825]
 [31.98379587  0.0819      0.1062      0.1596      0.11080001  0.75206846]
 ...
 [48.01502455  0.35490003  0.3461      0.4152      0.45199999  0.75596303]
 [10.35527898  0.38200003  0.33059999  0.47059998  0.44260001  0.64007133]
 [20.86822607  0.20940001  0.22780001  0.26710004  0.28630003  0.49987936]][0m
[37m[1m[2023-07-11 13:08:45,874][233954] Max Reward on eval: 93.5084295529523[0m
[37m[1m[2023-07-11 13:08:45,875][233954] Min Reward on eval: -31.797484617959707[0m
[37m[1m[2023-07-11 13:08:45,875][233954] Mean Reward across all agents: 21.509409482878322[0m
[37m[1m[2023-07-11 13:08:45,875][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:08:45,877][233954] mean_value=-1092.210611586213, max_value=466.617382595276[0m
[37m[1m[2023-07-11 13:08:45,879][233954] New mean coefficients: [[ 0.20958546 -0.05380201 -0.16037099  0.01047434  0.13875806 -0.44142753]][0m
[37m[1m[2023-07-11 13:08:45,880][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:08:54,862][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 13:08:54,862][233954] FPS: 427612.06[0m
[36m[2023-07-11 13:08:54,865][233954] itr=967, itrs=2000, Progress: 48.35%[0m
[36m[2023-07-11 13:09:06,448][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 13:09:06,448][233954] FPS: 334319.15[0m
[36m[2023-07-11 13:09:10,805][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:09:10,811][233954] Reward + Measures: [[26.71708776  0.13431966  0.17375334  0.267115    0.22874531  0.54821819]][0m
[37m[1m[2023-07-11 13:09:10,811][233954] Max Reward on eval: 26.717087763632122[0m
[37m[1m[2023-07-11 13:09:10,812][233954] Min Reward on eval: 26.717087763632122[0m
[37m[1m[2023-07-11 13:09:10,812][233954] Mean Reward across all agents: 26.717087763632122[0m
[37m[1m[2023-07-11 13:09:10,812][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:09:15,838][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:09:15,839][233954] Reward + Measures: [[75.4896442   0.28149998  0.30019999  0.26360002  0.35789999  0.66839355]
 [72.00585257  0.1148      0.0765      0.28600001  0.21110001  0.61667931]
 [15.39681978  0.6397      0.66979998  0.685       0.70760006  0.45512387]
 ...
 [18.38909742  0.1112      0.1305      0.24150001  0.2462      0.46682605]
 [88.59284091  0.2816      0.20869999  0.40689999  0.4118      0.58595264]
 [-9.18946565  0.30950001  0.35160002  0.37890002  0.41879997  0.67071193]][0m
[37m[1m[2023-07-11 13:09:15,839][233954] Max Reward on eval: 234.04064620579592[0m
[37m[1m[2023-07-11 13:09:15,839][233954] Min Reward on eval: -27.079969346290454[0m
[37m[1m[2023-07-11 13:09:15,839][233954] Mean Reward across all agents: 29.674596135873806[0m
[37m[1m[2023-07-11 13:09:15,840][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:09:15,842][233954] mean_value=-670.0385864920803, max_value=242.25563097555224[0m
[37m[1m[2023-07-11 13:09:15,844][233954] New mean coefficients: [[ 0.24116884 -0.02199682 -0.17283666  0.01375143  0.15184318 -0.3354107 ]][0m
[37m[1m[2023-07-11 13:09:15,845][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:09:24,857][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 13:09:24,858][233954] FPS: 426179.29[0m
[36m[2023-07-11 13:09:24,860][233954] itr=968, itrs=2000, Progress: 48.40%[0m
[36m[2023-07-11 13:09:36,445][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 13:09:36,445][233954] FPS: 334149.15[0m
[36m[2023-07-11 13:09:40,801][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:09:40,807][233954] Reward + Measures: [[26.51175601  0.136742    0.17157601  0.32815066  0.25114     0.50459296]][0m
[37m[1m[2023-07-11 13:09:40,807][233954] Max Reward on eval: 26.51175601306839[0m
[37m[1m[2023-07-11 13:09:40,807][233954] Min Reward on eval: 26.51175601306839[0m
[37m[1m[2023-07-11 13:09:40,808][233954] Mean Reward across all agents: 26.51175601306839[0m
[37m[1m[2023-07-11 13:09:40,808][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:09:46,124][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:09:46,125][233954] Reward + Measures: [[57.61877021  0.19850002  0.2168      0.44960004  0.273       0.46174192]
 [40.39405449  0.1056      0.1561      0.32430002  0.21490002  0.70973843]
 [26.12948788  0.10710001  0.13870001  0.42700002  0.23530002  0.4986558 ]
 ...
 [24.74585723  0.19780001  0.26830003  0.46479997  0.28330001  0.43865833]
 [35.49525538  0.18629998  0.1603      0.3779      0.28279999  0.52840573]
 [28.02455261  0.20080002  0.2431      0.40370002  0.30309999  0.47174364]][0m
[37m[1m[2023-07-11 13:09:46,125][233954] Max Reward on eval: 95.01762325388844[0m
[37m[1m[2023-07-11 13:09:46,126][233954] Min Reward on eval: -21.54059315789491[0m
[37m[1m[2023-07-11 13:09:46,126][233954] Mean Reward across all agents: 27.031674899014153[0m
[37m[1m[2023-07-11 13:09:46,126][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:09:46,128][233954] mean_value=-319.3561618615861, max_value=184.46879211655815[0m
[37m[1m[2023-07-11 13:09:46,131][233954] New mean coefficients: [[ 0.27242994 -0.0278672  -0.16124359 -0.02565474  0.15695304 -0.23482248]][0m
[37m[1m[2023-07-11 13:09:46,132][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:09:55,238][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 13:09:55,238][233954] FPS: 421744.11[0m
[36m[2023-07-11 13:09:55,241][233954] itr=969, itrs=2000, Progress: 48.45%[0m
[36m[2023-07-11 13:10:07,071][233954] train() took 11.73 seconds to complete[0m
[36m[2023-07-11 13:10:07,071][233954] FPS: 327271.04[0m
[36m[2023-07-11 13:10:11,340][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:10:11,346][233954] Reward + Measures: [[27.91281086  0.14868367  0.17662834  0.36281967  0.26977     0.47286946]][0m
[37m[1m[2023-07-11 13:10:11,346][233954] Max Reward on eval: 27.91281086100663[0m
[37m[1m[2023-07-11 13:10:11,346][233954] Min Reward on eval: 27.91281086100663[0m
[37m[1m[2023-07-11 13:10:11,347][233954] Mean Reward across all agents: 27.91281086100663[0m
[37m[1m[2023-07-11 13:10:11,347][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:10:16,314][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:10:16,315][233954] Reward + Measures: [[73.11917763  0.1231      0.14240001  0.28840002  0.15280001  0.75719923]
 [26.31939374  0.0288      0.0703      0.2139      0.16409999  0.76726604]
 [32.86764478  0.0216      0.0658      0.2445      0.16069999  0.37011123]
 ...
 [10.22664364  0.18710001  0.17080002  0.3256      0.2746      0.61396563]
 [20.49122597  0.2014      0.255       0.40079999  0.33800003  0.42184415]
 [30.51938554  0.18210001  0.167       0.3678      0.31460002  0.43724701]][0m
[37m[1m[2023-07-11 13:10:16,315][233954] Max Reward on eval: 171.27087112325245[0m
[37m[1m[2023-07-11 13:10:16,315][233954] Min Reward on eval: -10.833893371047452[0m
[37m[1m[2023-07-11 13:10:16,315][233954] Mean Reward across all agents: 32.07236029131681[0m
[37m[1m[2023-07-11 13:10:16,316][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:10:16,318][233954] mean_value=-921.0993743799264, max_value=432.896909602626[0m
[37m[1m[2023-07-11 13:10:16,320][233954] New mean coefficients: [[ 0.2468204  -0.02559821 -0.0748165   0.03854891  0.16815877 -0.3292434 ]][0m
[37m[1m[2023-07-11 13:10:16,321][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:10:25,305][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 13:10:25,305][233954] FPS: 427504.90[0m
[36m[2023-07-11 13:10:25,307][233954] itr=970, itrs=2000, Progress: 48.50%[0m
[37m[1m[2023-07-11 13:13:54,780][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000950[0m
[36m[2023-07-11 13:14:07,169][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 13:14:07,169][233954] FPS: 329780.79[0m
[36m[2023-07-11 13:14:11,398][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:14:11,399][233954] Reward + Measures: [[26.89896372  0.13999668  0.16480467  0.43989632  0.26590869  0.43427336]][0m
[37m[1m[2023-07-11 13:14:11,399][233954] Max Reward on eval: 26.898963717147474[0m
[37m[1m[2023-07-11 13:14:11,399][233954] Min Reward on eval: 26.898963717147474[0m
[37m[1m[2023-07-11 13:14:11,399][233954] Mean Reward across all agents: 26.898963717147474[0m
[37m[1m[2023-07-11 13:14:11,400][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:14:16,283][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:14:16,283][233954] Reward + Measures: [[19.75308679  0.0281      0.058       0.2361      0.18609999  0.43415919]
 [31.92122122  0.11099999  0.21339999  0.37830001  0.2227      0.49338385]
 [20.16333295  0.19990002  0.25960001  0.47940001  0.31909999  0.35666677]
 ...
 [12.90417554  0.112       0.1912      0.3224      0.20420001  0.51736116]
 [15.70488213  0.19669999  0.2746      0.37730002  0.29130003  0.49534759]
 [33.22186361  0.1164      0.20729999  0.3725      0.21159999  0.40808725]][0m
[37m[1m[2023-07-11 13:14:16,283][233954] Max Reward on eval: 123.9220549843507[0m
[37m[1m[2023-07-11 13:14:16,284][233954] Min Reward on eval: -4.454969763034024[0m
[37m[1m[2023-07-11 13:14:16,284][233954] Mean Reward across all agents: 27.36097528926389[0m
[37m[1m[2023-07-11 13:14:16,284][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:14:16,287][233954] mean_value=-207.43320088232997, max_value=380.11610280365505[0m
[37m[1m[2023-07-11 13:14:16,290][233954] New mean coefficients: [[ 0.23321028  0.00618486 -0.00509305  0.00945546  0.19200796 -0.39615756]][0m
[37m[1m[2023-07-11 13:14:16,291][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:14:25,206][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 13:14:25,206][233954] FPS: 430813.31[0m
[36m[2023-07-11 13:14:25,208][233954] itr=971, itrs=2000, Progress: 48.55%[0m
[36m[2023-07-11 13:14:36,819][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 13:14:36,819][233954] FPS: 333427.47[0m
[36m[2023-07-11 13:14:41,051][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:14:41,052][233954] Reward + Measures: [[29.73625484  0.13092667  0.15607999  0.40880734  0.26181099  0.43400806]][0m
[37m[1m[2023-07-11 13:14:41,052][233954] Max Reward on eval: 29.736254839637002[0m
[37m[1m[2023-07-11 13:14:41,052][233954] Min Reward on eval: 29.736254839637002[0m
[37m[1m[2023-07-11 13:14:41,053][233954] Mean Reward across all agents: 29.736254839637002[0m
[37m[1m[2023-07-11 13:14:41,053][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:14:46,013][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:14:46,014][233954] Reward + Measures: [[53.83263892  0.11739999  0.0722      0.27649999  0.23210001  0.66112065]
 [19.29780921  0.19060002  0.24600001  0.48910004  0.29990003  0.36738363]
 [27.82676321  0.12150001  0.1292      0.30350003  0.28009999  0.47911116]
 ...
 [ 4.92206862  0.0278      0.0612      0.23959999  0.17850001  0.80185503]
 [18.53606333  0.13060001  0.1464      0.35570002  0.2613      0.53466582]
 [ 4.71045134  0.292       0.29839998  0.43700001  0.33430001  0.58723509]][0m
[37m[1m[2023-07-11 13:14:46,014][233954] Max Reward on eval: 147.63491805805825[0m
[37m[1m[2023-07-11 13:14:46,014][233954] Min Reward on eval: -27.15238078092225[0m
[37m[1m[2023-07-11 13:14:46,014][233954] Mean Reward across all agents: 26.474666339639644[0m
[37m[1m[2023-07-11 13:14:46,014][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:14:46,017][233954] mean_value=-217.4193476071223, max_value=515.2375384563114[0m
[37m[1m[2023-07-11 13:14:46,019][233954] New mean coefficients: [[ 0.18422699  0.00861516 -0.08320574 -0.01759279  0.16037716 -0.2711255 ]][0m
[37m[1m[2023-07-11 13:14:46,020][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:14:55,075][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 13:14:55,075][233954] FPS: 424142.47[0m
[36m[2023-07-11 13:14:55,077][233954] itr=972, itrs=2000, Progress: 48.60%[0m
[36m[2023-07-11 13:15:06,797][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 13:15:06,798][233954] FPS: 330243.39[0m
[36m[2023-07-11 13:15:10,960][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:15:10,960][233954] Reward + Measures: [[26.17778384  0.12812066  0.15780801  0.38197362  0.26330599  0.42270777]][0m
[37m[1m[2023-07-11 13:15:10,961][233954] Max Reward on eval: 26.177783843530023[0m
[37m[1m[2023-07-11 13:15:10,961][233954] Min Reward on eval: 26.177783843530023[0m
[37m[1m[2023-07-11 13:15:10,961][233954] Mean Reward across all agents: 26.177783843530023[0m
[37m[1m[2023-07-11 13:15:10,961][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:15:16,147][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:15:16,147][233954] Reward + Measures: [[ 5.80965607  0.12330001  0.1018      0.20190001  0.1841      0.68384492]
 [26.20645611  0.10760001  0.1551      0.32469997  0.25770003  0.41750222]
 [27.39352866  0.1992      0.25670001  0.47329998  0.3109      0.40685105]
 ...
 [20.61028668  0.0259      0.0555      0.20560001  0.20019999  0.50136316]
 [30.4515044   0.0318      0.10830001  0.25439999  0.16919999  0.66311067]
 [32.98761606  0.0232      0.0582      0.29860002  0.1856      0.43030104]][0m
[37m[1m[2023-07-11 13:15:16,147][233954] Max Reward on eval: 360.23819994051007[0m
[37m[1m[2023-07-11 13:15:16,148][233954] Min Reward on eval: -22.746356223267505[0m
[37m[1m[2023-07-11 13:15:16,148][233954] Mean Reward across all agents: 33.343754343558366[0m
[37m[1m[2023-07-11 13:15:16,148][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:15:16,151][233954] mean_value=-535.219917255974, max_value=555.1455280198168[0m
[37m[1m[2023-07-11 13:15:16,153][233954] New mean coefficients: [[ 0.21407132 -0.01851381 -0.1069604   0.00280648  0.10762903 -0.32982218]][0m
[37m[1m[2023-07-11 13:15:16,154][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:15:25,206][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 13:15:25,206][233954] FPS: 424287.51[0m
[36m[2023-07-11 13:15:25,208][233954] itr=973, itrs=2000, Progress: 48.65%[0m
[36m[2023-07-11 13:15:36,944][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 13:15:36,944][233954] FPS: 329942.53[0m
[36m[2023-07-11 13:15:41,252][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:15:41,253][233954] Reward + Measures: [[27.36320508  0.14440566  0.17232133  0.41861165  0.28143233  0.41111192]][0m
[37m[1m[2023-07-11 13:15:41,253][233954] Max Reward on eval: 27.36320508291122[0m
[37m[1m[2023-07-11 13:15:41,253][233954] Min Reward on eval: 27.36320508291122[0m
[37m[1m[2023-07-11 13:15:41,254][233954] Mean Reward across all agents: 27.36320508291122[0m
[37m[1m[2023-07-11 13:15:41,254][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:15:46,226][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:15:46,227][233954] Reward + Measures: [[ 22.19919774   0.0351       0.06260001   0.28920001   0.14909999
    0.55478102]
 [ 15.42244782   0.21170001   0.23270002   0.26289999   0.30700001
    0.54867071]
 [-30.32442218   0.53579998   0.62629998   0.53360003   0.59359998
    0.70336205]
 ...
 [ 11.24528806   0.20470002   0.21920002   0.33419999   0.34849998
    0.42042205]
 [ 30.41675673   0.0266       0.0877       0.27540001   0.1628
    0.40830621]
 [ 14.44863191   0.13700001   0.16010001   0.1628       0.2
    0.62668723]][0m
[37m[1m[2023-07-11 13:15:46,227][233954] Max Reward on eval: 123.95661513009108[0m
[37m[1m[2023-07-11 13:15:46,227][233954] Min Reward on eval: -104.76659649687353[0m
[37m[1m[2023-07-11 13:15:46,228][233954] Mean Reward across all agents: 19.759850519197098[0m
[37m[1m[2023-07-11 13:15:46,228][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:15:46,230][233954] mean_value=-499.5323082079358, max_value=229.16363886173158[0m
[37m[1m[2023-07-11 13:15:46,233][233954] New mean coefficients: [[ 0.17337772  0.02405116 -0.07399108 -0.04545688  0.07946668 -0.50129837]][0m
[37m[1m[2023-07-11 13:15:46,234][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:15:55,254][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 13:15:55,254][233954] FPS: 425794.02[0m
[36m[2023-07-11 13:15:55,256][233954] itr=974, itrs=2000, Progress: 48.70%[0m
[36m[2023-07-11 13:16:06,929][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 13:16:06,929][233954] FPS: 331632.91[0m
[36m[2023-07-11 13:16:11,166][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:16:11,166][233954] Reward + Measures: [[27.85105053  0.12952666  0.156068    0.37778068  0.26745233  0.41214281]][0m
[37m[1m[2023-07-11 13:16:11,167][233954] Max Reward on eval: 27.85105053010853[0m
[37m[1m[2023-07-11 13:16:11,167][233954] Min Reward on eval: 27.85105053010853[0m
[37m[1m[2023-07-11 13:16:11,167][233954] Mean Reward across all agents: 27.85105053010853[0m
[37m[1m[2023-07-11 13:16:11,167][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:16:16,163][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:16:16,163][233954] Reward + Measures: [[-44.24376131   0.5837       0.63450003   0.60579997   0.66830003
    0.54314452]
 [ 22.61822247   0.0323       0.1107       0.26900002   0.15260001
    0.44778308]
 [ 14.90442934   0.19750001   0.2172       0.41869998   0.34200001
    0.34702206]
 ...
 [ 23.28577      0.12210001   0.1402       0.27950001   0.25030002
    0.37622142]
 [ 38.54476369   0.29660001   0.23899999   0.382        0.37149999
    0.68810439]
 [  4.31913937   0.49109998   0.60219997   0.59219998   0.56780005
    0.59916651]][0m
[37m[1m[2023-07-11 13:16:16,163][233954] Max Reward on eval: 127.98511227185372[0m
[37m[1m[2023-07-11 13:16:16,164][233954] Min Reward on eval: -119.01853942554445[0m
[37m[1m[2023-07-11 13:16:16,164][233954] Mean Reward across all agents: 22.828225082234233[0m
[37m[1m[2023-07-11 13:16:16,164][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:16:16,167][233954] mean_value=-86.16566989623271, max_value=384.13582816843035[0m
[37m[1m[2023-07-11 13:16:16,178][233954] New mean coefficients: [[ 0.1649966   0.03757511 -0.06742181 -0.0441508   0.12816468 -0.37402463]][0m
[37m[1m[2023-07-11 13:16:16,180][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:16:25,139][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 13:16:25,139][233954] FPS: 428729.34[0m
[36m[2023-07-11 13:16:25,142][233954] itr=975, itrs=2000, Progress: 48.75%[0m
[36m[2023-07-11 13:16:36,903][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 13:16:36,903][233954] FPS: 329270.16[0m
[36m[2023-07-11 13:16:41,172][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:16:41,173][233954] Reward + Measures: [[26.72025776  0.131661    0.15610133  0.34729198  0.26820832  0.39364383]][0m
[37m[1m[2023-07-11 13:16:41,173][233954] Max Reward on eval: 26.72025775671301[0m
[37m[1m[2023-07-11 13:16:41,173][233954] Min Reward on eval: 26.72025775671301[0m
[37m[1m[2023-07-11 13:16:41,173][233954] Mean Reward across all agents: 26.72025775671301[0m
[37m[1m[2023-07-11 13:16:41,174][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:16:46,159][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:16:46,160][233954] Reward + Measures: [[57.31376516  0.13540001  0.1046      0.19410001  0.17269999  0.65630227]
 [ 7.72408135  0.2845      0.34619999  0.42969999  0.3935      0.55231416]
 [16.09210324  0.0196      0.0418      0.22239999  0.1846      0.3708294 ]
 ...
 [23.48155309  0.0306      0.06460001  0.266       0.2168      0.45182887]
 [15.14448831  0.19950001  0.26009998  0.37        0.30270001  0.47259447]
 [39.0397022   0.10120001  0.08010001  0.31040001  0.22430001  0.4880943 ]][0m
[37m[1m[2023-07-11 13:16:46,160][233954] Max Reward on eval: 105.14593419758603[0m
[37m[1m[2023-07-11 13:16:46,160][233954] Min Reward on eval: -6.584928028825379[0m
[37m[1m[2023-07-11 13:16:46,161][233954] Mean Reward across all agents: 25.739652896075082[0m
[37m[1m[2023-07-11 13:16:46,161][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:16:46,163][233954] mean_value=-478.9590558152185, max_value=45.79372722831033[0m
[37m[1m[2023-07-11 13:16:46,165][233954] New mean coefficients: [[ 0.13889472  0.02542677 -0.02793749 -0.03289058  0.13468306 -0.25224617]][0m
[37m[1m[2023-07-11 13:16:46,166][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:16:55,217][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 13:16:55,217][233954] FPS: 424354.88[0m
[36m[2023-07-11 13:16:55,219][233954] itr=976, itrs=2000, Progress: 48.80%[0m
[36m[2023-07-11 13:17:07,053][233954] train() took 11.74 seconds to complete[0m
[36m[2023-07-11 13:17:07,053][233954] FPS: 327135.16[0m
[36m[2023-07-11 13:17:11,309][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:17:11,310][233954] Reward + Measures: [[26.82119091  0.12530333  0.152164    0.32279333  0.26267898  0.38525969]][0m
[37m[1m[2023-07-11 13:17:11,310][233954] Max Reward on eval: 26.821190905952374[0m
[37m[1m[2023-07-11 13:17:11,310][233954] Min Reward on eval: 26.821190905952374[0m
[37m[1m[2023-07-11 13:17:11,310][233954] Mean Reward across all agents: 26.821190905952374[0m
[37m[1m[2023-07-11 13:17:11,311][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:17:16,263][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:17:16,263][233954] Reward + Measures: [[88.39421374  0.12180001  0.0762      0.2366      0.22970001  0.62516677]
 [13.28842739  0.2016      0.23699999  0.26140001  0.28959998  0.58245957]
 [22.08333226  0.28690001  0.24460001  0.47849998  0.4068      0.35478565]
 ...
 [35.0185736   0.21440001  0.22120002  0.479       0.338       0.45298225]
 [31.91162011  0.0196      0.065       0.22380002  0.17640001  0.43213826]
 [18.83982586  0.28660002  0.36340004  0.45090005  0.37260002  0.35618654]][0m
[37m[1m[2023-07-11 13:17:16,263][233954] Max Reward on eval: 90.30926127841231[0m
[37m[1m[2023-07-11 13:17:16,264][233954] Min Reward on eval: -9.823938882537187[0m
[37m[1m[2023-07-11 13:17:16,264][233954] Mean Reward across all agents: 26.38566166903736[0m
[37m[1m[2023-07-11 13:17:16,264][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:17:16,267][233954] mean_value=-337.3112176554382, max_value=463.2258439229522[0m
[37m[1m[2023-07-11 13:17:16,269][233954] New mean coefficients: [[ 0.10027085 -0.00728895 -0.05863585 -0.11221061  0.04839111 -0.3663938 ]][0m
[37m[1m[2023-07-11 13:17:16,270][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:17:25,254][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 13:17:25,254][233954] FPS: 427503.84[0m
[36m[2023-07-11 13:17:25,256][233954] itr=977, itrs=2000, Progress: 48.85%[0m
[36m[2023-07-11 13:17:36,916][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 13:17:36,916][233954] FPS: 332039.04[0m
[36m[2023-07-11 13:17:41,262][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:17:41,262][233954] Reward + Measures: [[24.60816306  0.13727032  0.165564    0.31810901  0.26668701  0.38479623]][0m
[37m[1m[2023-07-11 13:17:41,262][233954] Max Reward on eval: 24.60816305526543[0m
[37m[1m[2023-07-11 13:17:41,263][233954] Min Reward on eval: 24.60816305526543[0m
[37m[1m[2023-07-11 13:17:41,263][233954] Mean Reward across all agents: 24.60816305526543[0m
[37m[1m[2023-07-11 13:17:41,263][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:17:46,234][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:17:46,240][233954] Reward + Measures: [[-31.00730218   0.6354       0.66900003   0.63300008   0.71730006
    0.66587728]
 [ 23.4160892    0.27669999   0.23509999   0.44179997   0.3863
    0.41381702]
 [ 23.18577202   0.28480002   0.34870002   0.48710003   0.3777
    0.3035737 ]
 ...
 [  0.60920327   0.0314       0.0546       0.1847       0.1728
    0.62107372]
 [-76.18780019   0.70999998   0.75819999   0.63730001   0.77190006
    0.72194701]
 [ 27.81101438   0.092        0.08970001   0.2545       0.23730002
    0.51172465]][0m
[37m[1m[2023-07-11 13:17:46,240][233954] Max Reward on eval: 105.33392554421444[0m
[37m[1m[2023-07-11 13:17:46,240][233954] Min Reward on eval: -114.96853062452283[0m
[37m[1m[2023-07-11 13:17:46,241][233954] Mean Reward across all agents: 17.5489890490535[0m
[37m[1m[2023-07-11 13:17:46,241][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:17:46,243][233954] mean_value=-147.42253801891812, max_value=368.8647891277514[0m
[37m[1m[2023-07-11 13:17:46,246][233954] New mean coefficients: [[ 0.07369231 -0.01563928 -0.03278247 -0.14458294 -0.01145952 -0.35059598]][0m
[37m[1m[2023-07-11 13:17:46,247][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:17:55,198][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 13:17:55,198][233954] FPS: 429055.95[0m
[36m[2023-07-11 13:17:55,201][233954] itr=978, itrs=2000, Progress: 48.90%[0m
[36m[2023-07-11 13:18:06,776][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 13:18:06,776][233954] FPS: 334490.67[0m
[36m[2023-07-11 13:18:11,117][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:18:11,117][233954] Reward + Measures: [[26.64522422  0.13078332  0.15365233  0.29629833  0.25253767  0.39680174]][0m
[37m[1m[2023-07-11 13:18:11,117][233954] Max Reward on eval: 26.64522421567981[0m
[37m[1m[2023-07-11 13:18:11,118][233954] Min Reward on eval: 26.64522421567981[0m
[37m[1m[2023-07-11 13:18:11,118][233954] Mean Reward across all agents: 26.64522421567981[0m
[37m[1m[2023-07-11 13:18:11,118][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:18:16,351][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:18:16,351][233954] Reward + Measures: [[26.26887197  0.11069999  0.16590001  0.27680001  0.2333      0.32468054]
 [11.72535075  0.17950001  0.23340002  0.35179996  0.289       0.51822591]
 [30.40128391  0.0226      0.062       0.23669998  0.1429      0.38028884]
 ...
 [19.80161143  0.1991      0.2359      0.3917      0.31059998  0.55244601]
 [15.4939852   0.0578      0.0587      0.0928      0.1594      0.5962103 ]
 [33.71332447  0.0227      0.0623      0.23049998  0.16689999  0.40630952]][0m
[37m[1m[2023-07-11 13:18:16,352][233954] Max Reward on eval: 112.87499139029532[0m
[37m[1m[2023-07-11 13:18:16,352][233954] Min Reward on eval: -26.507718400191514[0m
[37m[1m[2023-07-11 13:18:16,352][233954] Mean Reward across all agents: 26.894477495295043[0m
[37m[1m[2023-07-11 13:18:16,352][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:18:16,354][233954] mean_value=-377.46377926061973, max_value=524.0468844945077[0m
[37m[1m[2023-07-11 13:18:16,357][233954] New mean coefficients: [[ 0.05643139 -0.0603743   0.03302697 -0.185209   -0.04909046 -0.38290858]][0m
[37m[1m[2023-07-11 13:18:16,358][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:18:25,328][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 13:18:25,328][233954] FPS: 428175.12[0m
[36m[2023-07-11 13:18:25,330][233954] itr=979, itrs=2000, Progress: 48.95%[0m
[36m[2023-07-11 13:18:36,970][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 13:18:36,970][233954] FPS: 332576.26[0m
[36m[2023-07-11 13:18:41,229][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:18:41,230][233954] Reward + Measures: [[27.93372586  0.12204966  0.146622    0.26988998  0.237509    0.3979384 ]][0m
[37m[1m[2023-07-11 13:18:41,230][233954] Max Reward on eval: 27.933725857867607[0m
[37m[1m[2023-07-11 13:18:41,230][233954] Min Reward on eval: 27.933725857867607[0m
[37m[1m[2023-07-11 13:18:41,231][233954] Mean Reward across all agents: 27.933725857867607[0m
[37m[1m[2023-07-11 13:18:41,231][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:18:46,261][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:18:46,262][233954] Reward + Measures: [[24.89012561  0.3788      0.4296      0.48289999  0.4632      0.36365065]
 [10.51172184  0.20030001  0.23739998  0.33270001  0.31560001  0.29054412]
 [27.51876155  0.19389999  0.14250001  0.35230002  0.30180001  0.39901617]
 ...
 [14.39335232  0.3748      0.24330001  0.50319999  0.4585      0.35486618]
 [25.14233816  0.2915      0.32309997  0.40610003  0.37980002  0.41922003]
 [22.90651767  0.1098      0.1419      0.25299999  0.2924      0.29682881]][0m
[37m[1m[2023-07-11 13:18:46,262][233954] Max Reward on eval: 131.16425026971848[0m
[37m[1m[2023-07-11 13:18:46,262][233954] Min Reward on eval: -20.464069343404844[0m
[37m[1m[2023-07-11 13:18:46,262][233954] Mean Reward across all agents: 25.48067480175825[0m
[37m[1m[2023-07-11 13:18:46,263][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:18:46,265][233954] mean_value=-353.58625553916687, max_value=301.2948480312782[0m
[37m[1m[2023-07-11 13:18:46,268][233954] New mean coefficients: [[ 0.00698905 -0.05648785  0.07527474 -0.15253603 -0.02627794 -0.31432268]][0m
[37m[1m[2023-07-11 13:18:46,268][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:18:55,356][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 13:18:55,357][233954] FPS: 422620.51[0m
[36m[2023-07-11 13:18:55,359][233954] itr=980, itrs=2000, Progress: 49.00%[0m
[37m[1m[2023-07-11 13:22:22,421][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000960[0m
[36m[2023-07-11 13:22:34,775][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 13:22:34,776][233954] FPS: 330308.63[0m
[36m[2023-07-11 13:22:38,914][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:22:38,914][233954] Reward + Measures: [[26.81806449  0.14013433  0.16284399  0.28139901  0.250348    0.39330339]][0m
[37m[1m[2023-07-11 13:22:38,915][233954] Max Reward on eval: 26.818064489546373[0m
[37m[1m[2023-07-11 13:22:38,915][233954] Min Reward on eval: 26.818064489546373[0m
[37m[1m[2023-07-11 13:22:38,915][233954] Mean Reward across all agents: 26.818064489546373[0m
[37m[1m[2023-07-11 13:22:38,915][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:22:43,870][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:22:43,871][233954] Reward + Measures: [[28.69232453  0.1098      0.163       0.23779999  0.18879999  0.45613033]
 [24.2838052   0.1987      0.25980002  0.35520002  0.30399999  0.30246168]
 [15.29938311  0.11009999  0.16049999  0.2388      0.21659999  0.42424449]
 ...
 [18.0458413   0.1997      0.23540001  0.30670002  0.31119999  0.34536538]
 [43.13777675  0.28569999  0.27170002  0.40240002  0.35090002  0.44604751]
 [29.39761735  0.1097      0.1754      0.29270002  0.2155      0.40703979]][0m
[37m[1m[2023-07-11 13:22:43,871][233954] Max Reward on eval: 91.70893597081303[0m
[37m[1m[2023-07-11 13:22:43,871][233954] Min Reward on eval: -31.865093310471273[0m
[37m[1m[2023-07-11 13:22:43,871][233954] Mean Reward across all agents: 25.14449636901439[0m
[37m[1m[2023-07-11 13:22:43,872][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:22:43,874][233954] mean_value=-151.52431631351521, max_value=59.92497929090335[0m
[37m[1m[2023-07-11 13:22:43,876][233954] New mean coefficients: [[ 0.03821401 -0.05806042  0.0343036  -0.15296187 -0.01024129 -0.29238084]][0m
[37m[1m[2023-07-11 13:22:43,877][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:22:52,897][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 13:22:52,897][233954] FPS: 425799.28[0m
[36m[2023-07-11 13:22:52,899][233954] itr=981, itrs=2000, Progress: 49.05%[0m
[36m[2023-07-11 13:23:04,762][233954] train() took 11.77 seconds to complete[0m
[36m[2023-07-11 13:23:04,762][233954] FPS: 326315.36[0m
[36m[2023-07-11 13:23:09,085][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:23:09,085][233954] Reward + Measures: [[26.25492717  0.12519434  0.15382166  0.24784033  0.22839001  0.39175975]][0m
[37m[1m[2023-07-11 13:23:09,086][233954] Max Reward on eval: 26.25492716702985[0m
[37m[1m[2023-07-11 13:23:09,086][233954] Min Reward on eval: 26.25492716702985[0m
[37m[1m[2023-07-11 13:23:09,086][233954] Mean Reward across all agents: 26.25492716702985[0m
[37m[1m[2023-07-11 13:23:09,086][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:23:14,114][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:23:14,114][233954] Reward + Measures: [[34.96848567  0.0238      0.0925      0.19700001  0.1138      0.38797441]
 [36.8923918   0.11270001  0.15740001  0.25819999  0.22790001  0.34608606]
 [27.95249688  0.20350002  0.2474      0.30310002  0.30490002  0.38262659]
 ...
 [30.24894828  0.0491      0.08180001  0.1902      0.1505      0.69666338]
 [52.74914626  0.33010003  0.34509999  0.44479999  0.4391      0.43157798]
 [25.87072527  0.2034      0.24229999  0.29089999  0.29669997  0.48790285]][0m
[37m[1m[2023-07-11 13:23:14,114][233954] Max Reward on eval: 252.3598379218951[0m
[37m[1m[2023-07-11 13:23:14,115][233954] Min Reward on eval: -6.890650429204106[0m
[37m[1m[2023-07-11 13:23:14,115][233954] Mean Reward across all agents: 32.8607828925684[0m
[37m[1m[2023-07-11 13:23:14,115][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:23:14,118][233954] mean_value=-300.44785109573326, max_value=475.9869945625635[0m
[37m[1m[2023-07-11 13:23:14,120][233954] New mean coefficients: [[ 0.07853959 -0.04641615 -0.01524426 -0.18937956 -0.03140147 -0.40967092]][0m
[37m[1m[2023-07-11 13:23:14,121][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:23:23,226][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 13:23:23,226][233954] FPS: 421836.96[0m
[36m[2023-07-11 13:23:23,229][233954] itr=982, itrs=2000, Progress: 49.10%[0m
[36m[2023-07-11 13:23:35,017][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 13:23:35,018][233954] FPS: 328587.88[0m
[36m[2023-07-11 13:23:39,314][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:23:39,315][233954] Reward + Measures: [[25.20396249  0.12266766  0.15182701  0.23793465  0.22700399  0.40473697]][0m
[37m[1m[2023-07-11 13:23:39,315][233954] Max Reward on eval: 25.203962486940064[0m
[37m[1m[2023-07-11 13:23:39,315][233954] Min Reward on eval: 25.203962486940064[0m
[37m[1m[2023-07-11 13:23:39,316][233954] Mean Reward across all agents: 25.203962486940064[0m
[37m[1m[2023-07-11 13:23:39,316][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:23:44,271][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:23:44,271][233954] Reward + Measures: [[55.27694601  0.11219998  0.0591      0.22230001  0.21159999  0.46457347]
 [14.93399274  0.09100001  0.21700001  0.23459999  0.17690001  0.40534791]
 [27.41746468  0.20179999  0.24620001  0.3188      0.29220003  0.39272091]
 ...
 [24.51160179  0.2872      0.23190002  0.36250004  0.35959998  0.37586033]
 [22.59622779  0.16010001  0.18480001  0.2168      0.2586      0.60534614]
 [29.83330198  0.19399999  0.26069999  0.32850003  0.3134      0.34290558]][0m
[37m[1m[2023-07-11 13:23:44,272][233954] Max Reward on eval: 114.2667432206741[0m
[37m[1m[2023-07-11 13:23:44,272][233954] Min Reward on eval: -4.9654319800436495[0m
[37m[1m[2023-07-11 13:23:44,272][233954] Mean Reward across all agents: 27.61717117744361[0m
[37m[1m[2023-07-11 13:23:44,272][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:23:44,275][233954] mean_value=-254.52420683960153, max_value=524.2999124888796[0m
[37m[1m[2023-07-11 13:23:44,277][233954] New mean coefficients: [[ 0.0546538  -0.05249181 -0.03349687 -0.20876041 -0.08045204 -0.5024426 ]][0m
[37m[1m[2023-07-11 13:23:44,278][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:23:53,272][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 13:23:53,272][233954] FPS: 427036.72[0m
[36m[2023-07-11 13:23:53,274][233954] itr=983, itrs=2000, Progress: 49.15%[0m
[36m[2023-07-11 13:24:05,062][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 13:24:05,062][233954] FPS: 328353.93[0m
[36m[2023-07-11 13:24:09,358][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:24:09,358][233954] Reward + Measures: [[25.96317544  0.12650067  0.14185633  0.22754666  0.229948    0.39499846]][0m
[37m[1m[2023-07-11 13:24:09,358][233954] Max Reward on eval: 25.963175440983104[0m
[37m[1m[2023-07-11 13:24:09,358][233954] Min Reward on eval: 25.963175440983104[0m
[37m[1m[2023-07-11 13:24:09,359][233954] Mean Reward across all agents: 25.963175440983104[0m
[37m[1m[2023-07-11 13:24:09,359][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:24:14,572][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:24:14,572][233954] Reward + Measures: [[11.32866302  0.1332      0.1384      0.16919999  0.2237      0.51225579]
 [25.20817455  0.3545      0.29180002  0.49769998  0.44270006  0.3886219 ]
 [19.53721007  0.19940001  0.26920003  0.31580001  0.2775      0.36301491]
 ...
 [21.06561848  0.1019      0.15560001  0.2059      0.2282      0.33072361]
 [55.81828696  0.13420001  0.09159999  0.1805      0.1881      0.58824646]
 [30.19411803  0.112       0.1825      0.26720002  0.21089999  0.36655781]][0m
[37m[1m[2023-07-11 13:24:14,572][233954] Max Reward on eval: 120.11067172465846[0m
[37m[1m[2023-07-11 13:24:14,573][233954] Min Reward on eval: 0.46023881384171544[0m
[37m[1m[2023-07-11 13:24:14,573][233954] Mean Reward across all agents: 28.115929205728072[0m
[37m[1m[2023-07-11 13:24:14,573][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:24:14,575][233954] mean_value=-331.32101078553376, max_value=175.27418065321658[0m
[37m[1m[2023-07-11 13:24:14,577][233954] New mean coefficients: [[ 0.04655676 -0.05490217 -0.04095076 -0.16998643 -0.06110826 -0.37896124]][0m
[37m[1m[2023-07-11 13:24:14,578][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:24:23,656][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 13:24:23,657][233954] FPS: 423074.87[0m
[36m[2023-07-11 13:24:23,659][233954] itr=984, itrs=2000, Progress: 49.20%[0m
[36m[2023-07-11 13:24:35,241][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 13:24:35,241][233954] FPS: 334294.09[0m
[36m[2023-07-11 13:24:39,569][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:24:39,569][233954] Reward + Measures: [[26.8966452   0.133324    0.15551101  0.24630965  0.23301399  0.38692912]][0m
[37m[1m[2023-07-11 13:24:39,570][233954] Max Reward on eval: 26.896645204263905[0m
[37m[1m[2023-07-11 13:24:39,570][233954] Min Reward on eval: 26.896645204263905[0m
[37m[1m[2023-07-11 13:24:39,570][233954] Mean Reward across all agents: 26.896645204263905[0m
[37m[1m[2023-07-11 13:24:39,570][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:24:44,605][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:24:44,606][233954] Reward + Measures: [[22.41648006  0.0168      0.0596      0.161       0.13410001  0.31813589]
 [15.67621077  0.27990001  0.32339999  0.36310002  0.3795      0.38248792]
 [20.490124    0.2877      0.27160001  0.3608      0.36540002  0.50349939]
 ...
 [28.22726832  0.11059999  0.14329998  0.2089      0.27169999  0.53671926]
 [29.40489007  0.0217      0.0439      0.15970001  0.18889999  0.30141813]
 [30.99542575  0.106       0.15440001  0.26750001  0.22880001  0.28364882]][0m
[37m[1m[2023-07-11 13:24:44,606][233954] Max Reward on eval: 143.0874263402191[0m
[37m[1m[2023-07-11 13:24:44,607][233954] Min Reward on eval: -46.0477750773789[0m
[37m[1m[2023-07-11 13:24:44,607][233954] Mean Reward across all agents: 25.730276592001314[0m
[37m[1m[2023-07-11 13:24:44,607][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:24:44,609][233954] mean_value=-352.7926028905619, max_value=336.82474881511087[0m
[37m[1m[2023-07-11 13:24:44,612][233954] New mean coefficients: [[-0.01394689 -0.09064914 -0.02360936 -0.2219247  -0.12557268 -0.44620943]][0m
[37m[1m[2023-07-11 13:24:44,613][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:24:53,686][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 13:24:53,687][233954] FPS: 423268.00[0m
[36m[2023-07-11 13:24:53,689][233954] itr=985, itrs=2000, Progress: 49.25%[0m
[36m[2023-07-11 13:25:05,571][233954] train() took 11.78 seconds to complete[0m
[36m[2023-07-11 13:25:05,576][233954] FPS: 325858.15[0m
[36m[2023-07-11 13:25:09,913][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:25:09,914][233954] Reward + Measures: [[26.19906275  0.14251666  0.171435    0.24334401  0.22961999  0.374955  ]][0m
[37m[1m[2023-07-11 13:25:09,914][233954] Max Reward on eval: 26.199062746860204[0m
[37m[1m[2023-07-11 13:25:09,914][233954] Min Reward on eval: 26.199062746860204[0m
[37m[1m[2023-07-11 13:25:09,915][233954] Mean Reward across all agents: 26.199062746860204[0m
[37m[1m[2023-07-11 13:25:09,915][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:25:14,891][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:25:14,892][233954] Reward + Measures: [[34.74802263  0.13079999  0.1772      0.26019999  0.22449999  0.40972301]
 [16.81215583  0.1129      0.1693      0.22930001  0.2244      0.62574834]
 [71.86451251  0.19950001  0.15519999  0.29179999  0.28990003  0.62763464]
 ...
 [29.1969579   0.36240003  0.37509999  0.46720004  0.4646      0.4433212 ]
 [64.64553074  0.30369997  0.1505      0.29809999  0.40289998  0.87159806]
 [15.99904504  0.0252      0.09860001  0.17850001  0.18980001  0.54165393]][0m
[37m[1m[2023-07-11 13:25:14,892][233954] Max Reward on eval: 124.34644474469823[0m
[37m[1m[2023-07-11 13:25:14,892][233954] Min Reward on eval: 2.779145343543496[0m
[37m[1m[2023-07-11 13:25:14,892][233954] Mean Reward across all agents: 29.591458242208017[0m
[37m[1m[2023-07-11 13:25:14,893][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:25:14,895][233954] mean_value=-273.4165408801549, max_value=76.37036359636053[0m
[37m[1m[2023-07-11 13:25:14,897][233954] New mean coefficients: [[-0.03198335 -0.06318344 -0.00400581 -0.17475855 -0.10987575 -0.4120406 ]][0m
[37m[1m[2023-07-11 13:25:14,898][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:25:23,875][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 13:25:23,876][233954] FPS: 427822.65[0m
[36m[2023-07-11 13:25:23,878][233954] itr=986, itrs=2000, Progress: 49.30%[0m
[36m[2023-07-11 13:25:35,606][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 13:25:35,606][233954] FPS: 330168.16[0m
[36m[2023-07-11 13:25:39,947][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:25:39,947][233954] Reward + Measures: [[26.89668533  0.12852934  0.16404267  0.23062602  0.21760368  0.36349478]][0m
[37m[1m[2023-07-11 13:25:39,948][233954] Max Reward on eval: 26.896685331716863[0m
[37m[1m[2023-07-11 13:25:39,948][233954] Min Reward on eval: 26.896685331716863[0m
[37m[1m[2023-07-11 13:25:39,948][233954] Mean Reward across all agents: 26.896685331716863[0m
[37m[1m[2023-07-11 13:25:39,948][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:25:44,998][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:25:44,999][233954] Reward + Measures: [[17.77083004  0.0161      0.1163      0.2084      0.12160001  0.39927259]
 [16.11197312  0.34459999  0.33590001  0.46409997  0.42739996  0.57284343]
 [33.5282242   0.024       0.0749      0.21260002  0.1716      0.32291946]
 ...
 [26.6497111   0.28659999  0.18180001  0.40720001  0.37420002  0.42059135]
 [61.99909855  0.35910001  0.25330001  0.44870001  0.46210003  0.59632653]
 [25.92278637  0.11260001  0.15049998  0.27169999  0.2349      0.30288425]][0m
[37m[1m[2023-07-11 13:25:44,999][233954] Max Reward on eval: 186.9252163568279[0m
[37m[1m[2023-07-11 13:25:44,999][233954] Min Reward on eval: -9.879918565670959[0m
[37m[1m[2023-07-11 13:25:44,999][233954] Mean Reward across all agents: 25.981106585050437[0m
[37m[1m[2023-07-11 13:25:45,000][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:25:45,002][233954] mean_value=-232.53922554570207, max_value=297.8393478369886[0m
[37m[1m[2023-07-11 13:25:45,004][233954] New mean coefficients: [[-0.08442776 -0.08748422  0.03372079 -0.17757879 -0.14456414 -0.49037808]][0m
[37m[1m[2023-07-11 13:25:45,005][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:25:54,100][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 13:25:54,100][233954] FPS: 422313.46[0m
[36m[2023-07-11 13:25:54,102][233954] itr=987, itrs=2000, Progress: 49.35%[0m
[36m[2023-07-11 13:26:05,879][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 13:26:05,879][233954] FPS: 328683.87[0m
[36m[2023-07-11 13:26:10,136][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:26:10,137][233954] Reward + Measures: [[26.09335255  0.12691733  0.16907534  0.24348134  0.22758867  0.34425652]][0m
[37m[1m[2023-07-11 13:26:10,137][233954] Max Reward on eval: 26.093352549993647[0m
[37m[1m[2023-07-11 13:26:10,137][233954] Min Reward on eval: 26.093352549993647[0m
[37m[1m[2023-07-11 13:26:10,137][233954] Mean Reward across all agents: 26.093352549993647[0m
[37m[1m[2023-07-11 13:26:10,138][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:26:15,123][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:26:15,123][233954] Reward + Measures: [[27.7041542   0.1076      0.17510001  0.2062      0.18990003  0.30594751]
 [24.29895733  0.1902      0.23179999  0.38060001  0.32970005  0.33624959]
 [23.1104802   0.11740001  0.13590001  0.19990002  0.21340001  0.41050887]
 ...
 [14.01938932  0.20379999  0.24270001  0.26789999  0.28940001  0.46277285]
 [16.22674612  0.1008      0.2138      0.31240001  0.22049999  0.32192758]
 [33.81708515  0.0286      0.08000001  0.22390001  0.15629999  0.49451849]][0m
[37m[1m[2023-07-11 13:26:15,124][233954] Max Reward on eval: 308.81263311960504[0m
[37m[1m[2023-07-11 13:26:15,124][233954] Min Reward on eval: -26.756743531348185[0m
[37m[1m[2023-07-11 13:26:15,124][233954] Mean Reward across all agents: 24.8700653771827[0m
[37m[1m[2023-07-11 13:26:15,124][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:26:15,126][233954] mean_value=-260.14672094627696, max_value=31.316430722109168[0m
[37m[1m[2023-07-11 13:26:15,129][233954] New mean coefficients: [[-0.07271413 -0.05541393  0.06954394 -0.06421605 -0.0584299  -0.30788305]][0m
[37m[1m[2023-07-11 13:26:15,130][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:26:24,136][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 13:26:24,136][233954] FPS: 426453.33[0m
[36m[2023-07-11 13:26:24,138][233954] itr=988, itrs=2000, Progress: 49.40%[0m
[36m[2023-07-11 13:26:35,745][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 13:26:35,745][233954] FPS: 333586.95[0m
[36m[2023-07-11 13:26:40,008][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:26:40,009][233954] Reward + Measures: [[26.66333848  0.12647     0.15428333  0.26083031  0.23575468  0.34932685]][0m
[37m[1m[2023-07-11 13:26:40,009][233954] Max Reward on eval: 26.663338482279514[0m
[37m[1m[2023-07-11 13:26:40,009][233954] Min Reward on eval: 26.663338482279514[0m
[37m[1m[2023-07-11 13:26:40,009][233954] Mean Reward across all agents: 26.663338482279514[0m
[37m[1m[2023-07-11 13:26:40,010][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:26:45,280][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:26:45,280][233954] Reward + Measures: [[19.47081287  0.2933      0.32530001  0.40599999  0.39490002  0.29569218]
 [22.54073548  0.11369999  0.1777      0.28169999  0.2098      0.38698012]
 [19.84203078  0.0296      0.087       0.1709      0.11490001  0.45018673]
 ...
 [28.89059716  0.28420001  0.31580001  0.38060004  0.41589999  0.3520968 ]
 [44.66566573  0.12190001  0.042       0.2536      0.32139999  0.45463416]
 [23.16126689  0.1067      0.16850001  0.26100001  0.2192      0.36318803]][0m
[37m[1m[2023-07-11 13:26:45,281][233954] Max Reward on eval: 260.6310947469436[0m
[37m[1m[2023-07-11 13:26:45,281][233954] Min Reward on eval: 0.3175837712362409[0m
[37m[1m[2023-07-11 13:26:45,281][233954] Mean Reward across all agents: 34.64016507257918[0m
[37m[1m[2023-07-11 13:26:45,281][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:26:45,284][233954] mean_value=-214.12711204312777, max_value=694.9865594932623[0m
[37m[1m[2023-07-11 13:26:45,286][233954] New mean coefficients: [[-0.10126559 -0.05668475  0.16585562 -0.05014461 -0.02089526 -0.22088131]][0m
[37m[1m[2023-07-11 13:26:45,287][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:26:54,261][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 13:26:54,261][233954] FPS: 427999.75[0m
[36m[2023-07-11 13:26:54,263][233954] itr=989, itrs=2000, Progress: 49.45%[0m
[36m[2023-07-11 13:27:05,828][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 13:27:05,828][233954] FPS: 334876.54[0m
[36m[2023-07-11 13:27:10,121][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:27:10,122][233954] Reward + Measures: [[24.80834947  0.12820067  0.16702367  0.25416034  0.23756032  0.3421241 ]][0m
[37m[1m[2023-07-11 13:27:10,122][233954] Max Reward on eval: 24.808349473328125[0m
[37m[1m[2023-07-11 13:27:10,122][233954] Min Reward on eval: 24.808349473328125[0m
[37m[1m[2023-07-11 13:27:10,122][233954] Mean Reward across all agents: 24.808349473328125[0m
[37m[1m[2023-07-11 13:27:10,123][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:27:15,117][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:27:15,117][233954] Reward + Measures: [[20.26908311  0.11080001  0.1663      0.26079997  0.2287      0.28332591]
 [26.33547954  0.0295      0.0648      0.1454      0.16860001  0.37695494]
 [20.8001967   0.18599999  0.25150001  0.27830002  0.27140003  0.36074325]
 ...
 [24.25922287  0.0179      0.0961      0.1928      0.10820001  0.44399306]
 [25.48157048  0.1858      0.22809999  0.29949999  0.2879      0.44100013]
 [23.19387282  0.20060001  0.1745      0.34260002  0.29530001  0.33045289]][0m
[37m[1m[2023-07-11 13:27:15,117][233954] Max Reward on eval: 102.38230895192828[0m
[37m[1m[2023-07-11 13:27:15,118][233954] Min Reward on eval: 0.8111328535946086[0m
[37m[1m[2023-07-11 13:27:15,118][233954] Mean Reward across all agents: 25.727945510623[0m
[37m[1m[2023-07-11 13:27:15,118][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:27:15,120][233954] mean_value=-215.77177325382, max_value=26.490177796611945[0m
[37m[1m[2023-07-11 13:27:15,123][233954] New mean coefficients: [[-0.0984458  -0.05886141  0.09687585 -0.08111331 -0.04594838 -0.1969864 ]][0m
[37m[1m[2023-07-11 13:27:15,124][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:27:24,093][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 13:27:24,093][233954] FPS: 428221.22[0m
[36m[2023-07-11 13:27:24,095][233954] itr=990, itrs=2000, Progress: 49.50%[0m
[37m[1m[2023-07-11 13:31:00,662][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000970[0m
[36m[2023-07-11 13:31:12,876][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 13:31:12,877][233954] FPS: 330301.91[0m
[36m[2023-07-11 13:31:17,172][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:31:17,173][233954] Reward + Measures: [[24.58870477  0.14191966  0.18410002  0.26342031  0.24772766  0.33156601]][0m
[37m[1m[2023-07-11 13:31:17,173][233954] Max Reward on eval: 24.588704768206775[0m
[37m[1m[2023-07-11 13:31:17,173][233954] Min Reward on eval: 24.588704768206775[0m
[37m[1m[2023-07-11 13:31:17,174][233954] Mean Reward across all agents: 24.588704768206775[0m
[37m[1m[2023-07-11 13:31:17,174][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:31:22,156][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:31:22,156][233954] Reward + Measures: [[22.85894071  0.20990001  0.2342      0.31300002  0.31500003  0.32532525]
 [45.15287593  0.055       0.0831      0.1513      0.13600001  0.44034195]
 [20.34784798  0.0219      0.071       0.14979999  0.1217      0.27618107]
 ...
 [10.31095526  0.0183      0.07520001  0.15820001  0.12730001  0.3641021 ]
 [18.07053504  0.206       0.22580002  0.32110003  0.32730001  0.26971111]
 [53.97399446  0.3838      0.34769997  0.46660003  0.4587      0.49137053]][0m
[37m[1m[2023-07-11 13:31:22,157][233954] Max Reward on eval: 97.20627171375091[0m
[37m[1m[2023-07-11 13:31:22,157][233954] Min Reward on eval: 4.928646302781999[0m
[37m[1m[2023-07-11 13:31:22,157][233954] Mean Reward across all agents: 26.776121010819857[0m
[37m[1m[2023-07-11 13:31:22,157][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:31:22,159][233954] mean_value=-201.95416595521579, max_value=17.260447224922615[0m
[37m[1m[2023-07-11 13:31:22,162][233954] New mean coefficients: [[-0.06374827 -0.03514206  0.10030565 -0.04327554 -0.02661737 -0.19830015]][0m
[37m[1m[2023-07-11 13:31:22,163][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:31:31,280][233954] train() took 9.12 seconds to complete[0m
[36m[2023-07-11 13:31:31,280][233954] FPS: 421248.69[0m
[36m[2023-07-11 13:31:31,283][233954] itr=991, itrs=2000, Progress: 49.55%[0m
[36m[2023-07-11 13:31:43,017][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 13:31:43,017][233954] FPS: 329874.52[0m
[36m[2023-07-11 13:31:47,348][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:31:47,348][233954] Reward + Measures: [[25.55210907  0.130689    0.16317734  0.26481536  0.24818166  0.34250769]][0m
[37m[1m[2023-07-11 13:31:47,349][233954] Max Reward on eval: 25.552109068200476[0m
[37m[1m[2023-07-11 13:31:47,349][233954] Min Reward on eval: 25.552109068200476[0m
[37m[1m[2023-07-11 13:31:47,349][233954] Mean Reward across all agents: 25.552109068200476[0m
[37m[1m[2023-07-11 13:31:47,349][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:31:52,304][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:31:52,305][233954] Reward + Measures: [[18.42424835  0.29200003  0.3488      0.42490003  0.3793      0.29777977]
 [25.94559312  0.0164      0.10860001  0.1601      0.13859999  0.36544466]
 [23.6555057   0.0222      0.042       0.1181      0.22289999  0.32340518]
 ...
 [45.86512264  0.0886      0.08030001  0.21439998  0.23020001  0.40611944]
 [11.43445947  0.2057      0.22850001  0.31659999  0.3346      0.33483896]
 [62.76684085  0.36460003  0.3996      0.37480003  0.47440004  0.44117528]][0m
[37m[1m[2023-07-11 13:31:52,305][233954] Max Reward on eval: 82.5563103231776[0m
[37m[1m[2023-07-11 13:31:52,305][233954] Min Reward on eval: -7.090624078805559[0m
[37m[1m[2023-07-11 13:31:52,306][233954] Mean Reward across all agents: 23.44953120123381[0m
[37m[1m[2023-07-11 13:31:52,306][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:31:52,308][233954] mean_value=-143.58614338232178, max_value=420.0980907127429[0m
[37m[1m[2023-07-11 13:31:52,311][233954] New mean coefficients: [[-0.09071772 -0.02861327  0.09623346 -0.11202908 -0.06586231 -0.329897  ]][0m
[37m[1m[2023-07-11 13:31:52,312][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:32:01,223][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 13:32:01,223][233954] FPS: 430994.23[0m
[36m[2023-07-11 13:32:01,225][233954] itr=992, itrs=2000, Progress: 49.60%[0m
[36m[2023-07-11 13:32:12,971][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 13:32:12,971][233954] FPS: 329579.33[0m
[36m[2023-07-11 13:32:17,174][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:32:17,175][233954] Reward + Measures: [[24.50024553  0.13541865  0.17363867  0.245456    0.243526    0.33237794]][0m
[37m[1m[2023-07-11 13:32:17,175][233954] Max Reward on eval: 24.50024553019896[0m
[37m[1m[2023-07-11 13:32:17,175][233954] Min Reward on eval: 24.50024553019896[0m
[37m[1m[2023-07-11 13:32:17,175][233954] Mean Reward across all agents: 24.50024553019896[0m
[37m[1m[2023-07-11 13:32:17,176][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:32:22,111][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:32:22,111][233954] Reward + Measures: [[59.87745883  0.47370002  0.43810001  0.55179995  0.537       0.41617915]
 [30.26112509  0.15270001  0.18090002  0.26610002  0.27330002  0.41740766]
 [15.28652703  0.36769998  0.30040002  0.46560001  0.43439999  0.33832714]
 ...
 [90.62939765  0.1059      0.14330001  0.29800001  0.33720002  0.61157197]
 [13.74541884  0.37079999  0.34689999  0.4337      0.43129998  0.36205727]
 [18.378089    0.29000002  0.31530002  0.36770001  0.38850003  0.254724  ]][0m
[37m[1m[2023-07-11 13:32:22,112][233954] Max Reward on eval: 251.1353002340533[0m
[37m[1m[2023-07-11 13:32:22,112][233954] Min Reward on eval: -3.5632377627305685[0m
[37m[1m[2023-07-11 13:32:22,112][233954] Mean Reward across all agents: 28.465598627075792[0m
[37m[1m[2023-07-11 13:32:22,112][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:32:22,115][233954] mean_value=-224.97349971429392, max_value=219.67819676130202[0m
[37m[1m[2023-07-11 13:32:22,117][233954] New mean coefficients: [[-0.13262796  0.0226664   0.10411849 -0.15865675 -0.11729587 -0.3624548 ]][0m
[37m[1m[2023-07-11 13:32:22,118][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:32:31,059][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 13:32:31,059][233954] FPS: 429556.40[0m
[36m[2023-07-11 13:32:31,061][233954] itr=993, itrs=2000, Progress: 49.65%[0m
[36m[2023-07-11 13:32:42,773][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 13:32:42,773][233954] FPS: 330489.96[0m
[36m[2023-07-11 13:32:47,124][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:32:47,125][233954] Reward + Measures: [[24.46358156  0.15269832  0.19074768  0.25260532  0.25348333  0.31764325]][0m
[37m[1m[2023-07-11 13:32:47,125][233954] Max Reward on eval: 24.46358155995206[0m
[37m[1m[2023-07-11 13:32:47,125][233954] Min Reward on eval: 24.46358155995206[0m
[37m[1m[2023-07-11 13:32:47,126][233954] Mean Reward across all agents: 24.46358155995206[0m
[37m[1m[2023-07-11 13:32:47,126][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:32:52,357][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:32:52,358][233954] Reward + Measures: [[72.00363428  0.1092      0.0467      0.1869      0.25979999  0.44387275]
 [ 2.58721686  0.1085      0.1666      0.21030001  0.2484      0.42853364]
 [25.06225054  0.1085      0.1402      0.23309998  0.21269999  0.23305233]
 ...
 [41.43524823  0.0563      0.0467      0.1381      0.2218      0.39842409]
 [15.17159313  0.0185      0.0542      0.1151      0.2414      0.33769366]
 [36.38056768  0.10780001  0.0402      0.17380001  0.27490002  0.5870083 ]][0m
[37m[1m[2023-07-11 13:32:52,358][233954] Max Reward on eval: 86.58098035654984[0m
[37m[1m[2023-07-11 13:32:52,358][233954] Min Reward on eval: 0.4854956648312509[0m
[37m[1m[2023-07-11 13:32:52,358][233954] Mean Reward across all agents: 22.104598550795206[0m
[37m[1m[2023-07-11 13:32:52,359][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:32:52,361][233954] mean_value=-110.20343741405784, max_value=517.875463958038[0m
[37m[1m[2023-07-11 13:32:52,363][233954] New mean coefficients: [[-0.11283083 -0.00665389  0.06254511 -0.11833215 -0.08527674 -0.45159635]][0m
[37m[1m[2023-07-11 13:32:52,364][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:33:01,289][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 13:33:01,289][233954] FPS: 430336.91[0m
[36m[2023-07-11 13:33:01,291][233954] itr=994, itrs=2000, Progress: 49.70%[0m
[36m[2023-07-11 13:33:13,035][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 13:33:13,035][233954] FPS: 329594.43[0m
[36m[2023-07-11 13:33:17,375][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:33:17,376][233954] Reward + Measures: [[22.383013    0.17491801  0.20921199  0.26917699  0.27434599  0.3108356 ]][0m
[37m[1m[2023-07-11 13:33:17,376][233954] Max Reward on eval: 22.38301300491486[0m
[37m[1m[2023-07-11 13:33:17,376][233954] Min Reward on eval: 22.38301300491486[0m
[37m[1m[2023-07-11 13:33:17,376][233954] Mean Reward across all agents: 22.38301300491486[0m
[37m[1m[2023-07-11 13:33:17,377][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:33:22,392][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:33:22,392][233954] Reward + Measures: [[33.56812284  0.0207      0.0558      0.1337      0.15810001  0.27203292]
 [15.75545286  0.18849999  0.27120003  0.26750001  0.25990003  0.50941938]
 [34.84758911  0.24390002  0.2615      0.31909999  0.35780001  0.36391824]
 ...
 [82.44048277  0.2895      0.2306      0.42030001  0.46510002  0.55504733]
 [76.38143421  0.1961      0.17659999  0.29340002  0.2719      0.4734222 ]
 [24.92340975  0.0955      0.15880002  0.20879999  0.19680002  0.44532844]][0m
[37m[1m[2023-07-11 13:33:22,393][233954] Max Reward on eval: 148.98059071898462[0m
[37m[1m[2023-07-11 13:33:22,393][233954] Min Reward on eval: -147.23312725150026[0m
[37m[1m[2023-07-11 13:33:22,393][233954] Mean Reward across all agents: 26.14652059896196[0m
[37m[1m[2023-07-11 13:33:22,393][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:33:22,396][233954] mean_value=-169.81610758793443, max_value=77.04828396565419[0m
[37m[1m[2023-07-11 13:33:22,398][233954] New mean coefficients: [[-0.12667722 -0.01550723  0.01992933 -0.15149465 -0.1139254  -0.2869023 ]][0m
[37m[1m[2023-07-11 13:33:22,399][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:33:31,383][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 13:33:31,383][233954] FPS: 427530.57[0m
[36m[2023-07-11 13:33:31,385][233954] itr=995, itrs=2000, Progress: 49.75%[0m
[36m[2023-07-11 13:33:43,087][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 13:33:43,087][233954] FPS: 330899.69[0m
[36m[2023-07-11 13:33:47,451][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:33:47,452][233954] Reward + Measures: [[23.16189927  0.16767167  0.19417667  0.26752099  0.26989999  0.30486116]][0m
[37m[1m[2023-07-11 13:33:47,452][233954] Max Reward on eval: 23.161899265118027[0m
[37m[1m[2023-07-11 13:33:47,452][233954] Min Reward on eval: 23.161899265118027[0m
[37m[1m[2023-07-11 13:33:47,452][233954] Mean Reward across all agents: 23.161899265118027[0m
[37m[1m[2023-07-11 13:33:47,453][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:33:52,443][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:33:52,444][233954] Reward + Measures: [[22.92987344  0.10630001  0.16320001  0.21859999  0.24460001  0.2641122 ]
 [21.6448456   0.198       0.25760001  0.30680001  0.3163      0.32800323]
 [27.81293418  0.021       0.0864      0.161       0.11980001  0.35569486]
 ...
 [13.31875706  0.28710002  0.36489999  0.37439999  0.37340006  0.36437073]
 [ 6.27237474  0.34740001  0.42959997  0.42160001  0.43810001  0.38637805]
 [22.1335279   0.18190001  0.1079      0.31729999  0.2701      0.33496103]][0m
[37m[1m[2023-07-11 13:33:52,444][233954] Max Reward on eval: 109.29818769468693[0m
[37m[1m[2023-07-11 13:33:52,444][233954] Min Reward on eval: -14.150538339780178[0m
[37m[1m[2023-07-11 13:33:52,444][233954] Mean Reward across all agents: 23.372339869825712[0m
[37m[1m[2023-07-11 13:33:52,444][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:33:52,447][233954] mean_value=-84.85644673697213, max_value=523.6096112037078[0m
[37m[1m[2023-07-11 13:33:52,449][233954] New mean coefficients: [[-0.1211184  -0.01369447  0.00909979 -0.15883064 -0.11124661 -0.41657513]][0m
[37m[1m[2023-07-11 13:33:52,450][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:34:01,520][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 13:34:01,520][233954] FPS: 423428.41[0m
[36m[2023-07-11 13:34:01,522][233954] itr=996, itrs=2000, Progress: 49.80%[0m
[36m[2023-07-11 13:34:13,262][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 13:34:13,262][233954] FPS: 329917.24[0m
[36m[2023-07-11 13:34:17,588][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:34:17,588][233954] Reward + Measures: [[24.09498427  0.18273234  0.20995434  0.273727    0.27388465  0.31054521]][0m
[37m[1m[2023-07-11 13:34:17,588][233954] Max Reward on eval: 24.09498427139413[0m
[37m[1m[2023-07-11 13:34:17,589][233954] Min Reward on eval: 24.09498427139413[0m
[37m[1m[2023-07-11 13:34:17,589][233954] Mean Reward across all agents: 24.09498427139413[0m
[37m[1m[2023-07-11 13:34:17,589][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:34:22,605][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:34:22,605][233954] Reward + Measures: [[30.3223674   0.0268      0.0551      0.12719999  0.1873      0.26092091]
 [32.11951926  0.25120002  0.1288      0.31580004  0.36230001  0.37979242]
 [18.86451112  0.12159999  0.18010001  0.20320001  0.20720001  0.71315718]
 ...
 [21.05326141  0.30140004  0.3348      0.35740003  0.40559998  0.32139716]
 [15.46407921  0.199       0.24319997  0.26270002  0.32659999  0.43498549]
 [12.3133176   0.1014      0.12630001  0.18359999  0.2304      0.27174094]][0m
[37m[1m[2023-07-11 13:34:22,606][233954] Max Reward on eval: 83.75982349559199[0m
[37m[1m[2023-07-11 13:34:22,606][233954] Min Reward on eval: -28.96655914853327[0m
[37m[1m[2023-07-11 13:34:22,606][233954] Mean Reward across all agents: 21.760816455546163[0m
[37m[1m[2023-07-11 13:34:22,606][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:34:22,609][233954] mean_value=-170.54027434004692, max_value=471.83833710813315[0m
[37m[1m[2023-07-11 13:34:22,611][233954] New mean coefficients: [[-0.09514838 -0.0042154  -0.01509273 -0.12558603 -0.08877196 -0.4046232 ]][0m
[37m[1m[2023-07-11 13:34:22,612][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:34:31,641][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 13:34:31,642][233954] FPS: 425363.37[0m
[36m[2023-07-11 13:34:31,644][233954] itr=997, itrs=2000, Progress: 49.85%[0m
[36m[2023-07-11 13:34:43,561][233954] train() took 11.82 seconds to complete[0m
[36m[2023-07-11 13:34:43,561][233954] FPS: 324835.38[0m
[36m[2023-07-11 13:34:47,895][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:34:47,895][233954] Reward + Measures: [[24.76412613  0.17280333  0.196513    0.27421933  0.26257232  0.30733448]][0m
[37m[1m[2023-07-11 13:34:47,896][233954] Max Reward on eval: 24.764126133635315[0m
[37m[1m[2023-07-11 13:34:47,896][233954] Min Reward on eval: 24.764126133635315[0m
[37m[1m[2023-07-11 13:34:47,896][233954] Mean Reward across all agents: 24.764126133635315[0m
[37m[1m[2023-07-11 13:34:47,896][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:34:52,925][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:34:52,926][233954] Reward + Measures: [[23.82314652  0.1908      0.22690001  0.26970002  0.34909996  0.27471051]
 [10.13277148  0.46170002  0.32210001  0.47569999  0.53180003  0.47918519]
 [23.12262817  0.17020002  0.2263      0.27790001  0.25530002  0.38971531]
 ...
 [23.42109083  0.24249998  0.2217      0.27130002  0.38410002  0.29735783]
 [35.60406001  0.16550002  0.0666      0.23480001  0.24850002  0.46871242]
 [21.92336048  0.1991      0.24800001  0.32000002  0.28330001  0.40525755]][0m
[37m[1m[2023-07-11 13:34:52,926][233954] Max Reward on eval: 84.4423435852863[0m
[37m[1m[2023-07-11 13:34:52,926][233954] Min Reward on eval: -32.30808703815565[0m
[37m[1m[2023-07-11 13:34:52,926][233954] Mean Reward across all agents: 22.776503828840212[0m
[37m[1m[2023-07-11 13:34:52,927][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:34:52,929][233954] mean_value=-100.29428317786773, max_value=496.30525243652517[0m
[37m[1m[2023-07-11 13:34:52,931][233954] New mean coefficients: [[-0.09288762  0.03406546 -0.04935586 -0.11193722 -0.0759996  -0.30611944]][0m
[37m[1m[2023-07-11 13:34:52,932][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:35:01,925][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 13:35:01,925][233954] FPS: 427087.43[0m
[36m[2023-07-11 13:35:01,928][233954] itr=998, itrs=2000, Progress: 49.90%[0m
[36m[2023-07-11 13:35:13,716][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 13:35:13,716][233954] FPS: 328361.31[0m
[36m[2023-07-11 13:35:18,024][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:35:18,024][233954] Reward + Measures: [[23.55955886  0.18357235  0.20508265  0.27616233  0.270082    0.31115755]][0m
[37m[1m[2023-07-11 13:35:18,025][233954] Max Reward on eval: 23.559558861225554[0m
[37m[1m[2023-07-11 13:35:18,025][233954] Min Reward on eval: 23.559558861225554[0m
[37m[1m[2023-07-11 13:35:18,025][233954] Mean Reward across all agents: 23.559558861225554[0m
[37m[1m[2023-07-11 13:35:18,025][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:35:22,962][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:35:22,963][233954] Reward + Measures: [[21.86845312  0.28330001  0.3213      0.35860002  0.38080001  0.34948805]
 [29.15801552  0.19319999  0.1675      0.29770002  0.27950001  0.23952194]
 [32.84052181  0.0259      0.066       0.11759999  0.18060002  0.29597095]
 ...
 [25.81256619  0.0159      0.0727      0.17129999  0.1163      0.2148952 ]
 [38.27152244  0.1939      0.1461      0.278       0.27350003  0.38916862]
 [13.67623928  0.1925      0.24090002  0.28110003  0.2816      0.25577193]][0m
[37m[1m[2023-07-11 13:35:22,963][233954] Max Reward on eval: 104.0729171450017[0m
[37m[1m[2023-07-11 13:35:22,963][233954] Min Reward on eval: -16.483185916487127[0m
[37m[1m[2023-07-11 13:35:22,964][233954] Mean Reward across all agents: 26.265313541656145[0m
[37m[1m[2023-07-11 13:35:22,964][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:35:22,966][233954] mean_value=-121.76030105387272, max_value=82.41619654936395[0m
[37m[1m[2023-07-11 13:35:22,969][233954] New mean coefficients: [[-0.06994176 -0.00545173 -0.10006273 -0.13334583 -0.10978235 -0.3769585 ]][0m
[37m[1m[2023-07-11 13:35:22,970][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:35:31,961][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 13:35:31,962][233954] FPS: 427135.45[0m
[36m[2023-07-11 13:35:31,964][233954] itr=999, itrs=2000, Progress: 49.95%[0m
[36m[2023-07-11 13:35:43,604][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 13:35:43,604][233954] FPS: 332602.86[0m
[36m[2023-07-11 13:35:47,903][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:35:47,904][233954] Reward + Measures: [[24.20715905  0.17407499  0.19606167  0.26469299  0.25919434  0.31281748]][0m
[37m[1m[2023-07-11 13:35:47,904][233954] Max Reward on eval: 24.207159052589816[0m
[37m[1m[2023-07-11 13:35:47,904][233954] Min Reward on eval: 24.207159052589816[0m
[37m[1m[2023-07-11 13:35:47,904][233954] Mean Reward across all agents: 24.207159052589816[0m
[37m[1m[2023-07-11 13:35:47,905][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:35:53,147][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:35:53,148][233954] Reward + Measures: [[20.90355972  0.20780002  0.2146      0.23639999  0.28550002  0.57177156]
 [ 3.52087388  0.37849998  0.34380004  0.46099997  0.42749998  0.37062582]
 [34.47312175  0.1219      0.15869999  0.19840001  0.22260001  0.44447389]
 ...
 [33.10383764  0.0251      0.0744      0.14400001  0.1437      0.33337888]
 [18.60617754  0.29410002  0.22830001  0.35680002  0.38900003  0.22330561]
 [17.74882881  0.2888      0.24609999  0.36070001  0.38150001  0.38409027]][0m
[37m[1m[2023-07-11 13:35:53,148][233954] Max Reward on eval: 115.87639946332202[0m
[37m[1m[2023-07-11 13:35:53,148][233954] Min Reward on eval: -36.27424741499126[0m
[37m[1m[2023-07-11 13:35:53,148][233954] Mean Reward across all agents: 23.211520192035696[0m
[37m[1m[2023-07-11 13:35:53,149][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:35:53,150][233954] mean_value=-255.57714697165258, max_value=177.8592353066365[0m
[37m[1m[2023-07-11 13:35:53,153][233954] New mean coefficients: [[-0.0553475   0.01768039 -0.11111803 -0.08369029 -0.04493868 -0.27702984]][0m
[37m[1m[2023-07-11 13:35:53,154][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:36:02,209][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 13:36:02,209][233954] FPS: 424138.45[0m
[36m[2023-07-11 13:36:02,211][233954] itr=1000, itrs=2000, Progress: 50.00%[0m
[37m[1m[2023-07-11 13:39:34,496][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000980[0m
[36m[2023-07-11 13:39:46,863][233954] train() took 11.76 seconds to complete[0m
[36m[2023-07-11 13:39:46,863][233954] FPS: 326668.55[0m
[36m[2023-07-11 13:39:51,065][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:39:51,066][233954] Reward + Measures: [[24.86560136  0.163395    0.18054901  0.24136132  0.25159898  0.31496605]][0m
[37m[1m[2023-07-11 13:39:51,066][233954] Max Reward on eval: 24.865601363158515[0m
[37m[1m[2023-07-11 13:39:51,066][233954] Min Reward on eval: 24.865601363158515[0m
[37m[1m[2023-07-11 13:39:51,067][233954] Mean Reward across all agents: 24.865601363158515[0m
[37m[1m[2023-07-11 13:39:51,067][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:39:55,950][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:39:55,951][233954] Reward + Measures: [[27.25698536  0.2062      0.20579998  0.2412      0.2735      0.56086874]
 [16.28082021  0.12560001  0.0586      0.1772      0.20259999  0.55992186]
 [27.50314551  0.3211      0.3583      0.38079998  0.38630003  0.40212676]
 ...
 [35.3863117   0.45150003  0.4147      0.49740002  0.491       0.50946254]
 [21.30070367  0.1129      0.15100001  0.21710001  0.222       0.35689607]
 [19.82143406  0.0767      0.0623      0.06840001  0.1323      0.65088272]][0m
[37m[1m[2023-07-11 13:39:55,951][233954] Max Reward on eval: 98.86031584378797[0m
[37m[1m[2023-07-11 13:39:55,951][233954] Min Reward on eval: -34.90326754163252[0m
[37m[1m[2023-07-11 13:39:55,952][233954] Mean Reward across all agents: 24.341915313282396[0m
[37m[1m[2023-07-11 13:39:55,952][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:39:55,954][233954] mean_value=-401.5854191010693, max_value=342.2133498332041[0m
[37m[1m[2023-07-11 13:39:55,956][233954] New mean coefficients: [[-0.0226496   0.03054685 -0.10127831 -0.0326015  -0.00701269 -0.25454208]][0m
[37m[1m[2023-07-11 13:39:55,957][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:40:04,832][233954] train() took 8.87 seconds to complete[0m
[36m[2023-07-11 13:40:04,832][233954] FPS: 432793.95[0m
[36m[2023-07-11 13:40:04,834][233954] itr=1001, itrs=2000, Progress: 50.05%[0m
[36m[2023-07-11 13:40:16,462][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 13:40:16,463][233954] FPS: 332893.45[0m
[36m[2023-07-11 13:40:20,717][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:40:20,717][233954] Reward + Measures: [[23.95239042  0.17018133  0.193903    0.25905702  0.26790732  0.29723904]][0m
[37m[1m[2023-07-11 13:40:20,718][233954] Max Reward on eval: 23.952390421832224[0m
[37m[1m[2023-07-11 13:40:20,718][233954] Min Reward on eval: 23.952390421832224[0m
[37m[1m[2023-07-11 13:40:20,718][233954] Mean Reward across all agents: 23.952390421832224[0m
[37m[1m[2023-07-11 13:40:20,718][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:40:25,703][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:40:25,703][233954] Reward + Measures: [[28.12914321  0.11010001  0.15530001  0.1964      0.21950002  0.36725923]
 [20.65309486  0.0993      0.14330001  0.1946      0.204       0.29184934]
 [15.2313417   0.211       0.2516      0.2965      0.31450003  0.24394999]
 ...
 [12.64366671  0.29290003  0.3242      0.36380002  0.3795      0.29330641]
 [26.93302226  0.09540001  0.15290001  0.1953      0.25760001  0.26549488]
 [33.7181753   0.38780001  0.42860004  0.43800002  0.44819999  0.39170933]][0m
[37m[1m[2023-07-11 13:40:25,703][233954] Max Reward on eval: 88.32009796034545[0m
[37m[1m[2023-07-11 13:40:25,704][233954] Min Reward on eval: -23.23576298020198[0m
[37m[1m[2023-07-11 13:40:25,704][233954] Mean Reward across all agents: 25.594563546177298[0m
[37m[1m[2023-07-11 13:40:25,704][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:40:25,706][233954] mean_value=-225.47868637851295, max_value=392.7339059953551[0m
[37m[1m[2023-07-11 13:40:25,709][233954] New mean coefficients: [[-0.00457012  0.02764552 -0.06051673 -0.00778601  0.0039857  -0.19295475]][0m
[37m[1m[2023-07-11 13:40:25,710][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:40:34,764][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 13:40:34,764][233954] FPS: 424192.01[0m
[36m[2023-07-11 13:40:34,767][233954] itr=1002, itrs=2000, Progress: 50.10%[0m
[36m[2023-07-11 13:40:46,413][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 13:40:46,413][233954] FPS: 332517.10[0m
[36m[2023-07-11 13:40:50,603][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:40:50,603][233954] Reward + Measures: [[25.5759862   0.17038065  0.19173633  0.25725168  0.26307935  0.30424938]][0m
[37m[1m[2023-07-11 13:40:50,603][233954] Max Reward on eval: 25.575986197995714[0m
[37m[1m[2023-07-11 13:40:50,604][233954] Min Reward on eval: 25.575986197995714[0m
[37m[1m[2023-07-11 13:40:50,604][233954] Mean Reward across all agents: 25.575986197995714[0m
[37m[1m[2023-07-11 13:40:50,604][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:40:55,576][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:40:55,577][233954] Reward + Measures: [[-7.35673722  0.54960006  0.59569997  0.57160002  0.62530005  0.35106099]
 [14.936386    0.19329999  0.24339998  0.29790002  0.27070001  0.26262823]
 [ 8.87086822  0.47269997  0.50050002  0.5226      0.53730005  0.29598281]
 ...
 [22.63795826  0.2172      0.26089999  0.289       0.2877      0.26028022]
 [12.12165543  0.30280003  0.34250003  0.36090001  0.3795      0.31881291]
 [ 7.26706412  0.1169      0.1564      0.2177      0.20400003  0.45295373]][0m
[37m[1m[2023-07-11 13:40:55,577][233954] Max Reward on eval: 91.30086266397265[0m
[37m[1m[2023-07-11 13:40:55,577][233954] Min Reward on eval: -26.892847484350206[0m
[37m[1m[2023-07-11 13:40:55,578][233954] Mean Reward across all agents: 23.439646156796563[0m
[37m[1m[2023-07-11 13:40:55,578][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:40:55,580][233954] mean_value=-165.17017943514114, max_value=269.40919124770494[0m
[37m[1m[2023-07-11 13:40:55,583][233954] New mean coefficients: [[ 0.0210138   0.03172957 -0.05959212  0.01154552  0.02454383 -0.23405191]][0m
[37m[1m[2023-07-11 13:40:55,584][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:41:04,568][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 13:41:04,569][233954] FPS: 427481.66[0m
[36m[2023-07-11 13:41:04,571][233954] itr=1003, itrs=2000, Progress: 50.15%[0m
[36m[2023-07-11 13:41:16,648][233954] train() took 11.98 seconds to complete[0m
[36m[2023-07-11 13:41:16,648][233954] FPS: 320430.60[0m
[36m[2023-07-11 13:41:20,918][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:41:20,918][233954] Reward + Measures: [[24.85797119  0.17608733  0.197311    0.273747    0.27707899  0.29894486]][0m
[37m[1m[2023-07-11 13:41:20,918][233954] Max Reward on eval: 24.857971185756174[0m
[37m[1m[2023-07-11 13:41:20,918][233954] Min Reward on eval: 24.857971185756174[0m
[37m[1m[2023-07-11 13:41:20,919][233954] Mean Reward across all agents: 24.857971185756174[0m
[37m[1m[2023-07-11 13:41:20,919][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:41:25,912][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:41:25,913][233954] Reward + Measures: [[ 23.75850932   0.49720001   0.49060002   0.4921       0.55900002
    0.64606971]
 [ 17.30191214   0.0223       0.0459       0.1437       0.1532
    0.18983082]
 [ 48.19982567   0.10359999   0.0599       0.20209999   0.22620001
    0.49210253]
 ...
 [ 16.08784329   0.20179999   0.24989998   0.2965       0.2974
    0.25568375]
 [-40.89537081   0.24029998   0.29270002   0.2369       0.33150002
    0.54038376]
 [-18.49084448   0.43930003   0.50369996   0.4571       0.51459998
    0.50484329]][0m
[37m[1m[2023-07-11 13:41:25,913][233954] Max Reward on eval: 128.14120899917324[0m
[37m[1m[2023-07-11 13:41:25,913][233954] Min Reward on eval: -47.379321114433694[0m
[37m[1m[2023-07-11 13:41:25,913][233954] Mean Reward across all agents: 22.87233123842385[0m
[37m[1m[2023-07-11 13:41:25,914][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:41:25,916][233954] mean_value=-97.19852441940027, max_value=425.97138640554294[0m
[37m[1m[2023-07-11 13:41:25,918][233954] New mean coefficients: [[ 0.03940832  0.0277197  -0.06431559  0.07672016  0.051181   -0.15595753]][0m
[37m[1m[2023-07-11 13:41:25,919][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:41:34,920][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 13:41:34,920][233954] FPS: 426717.18[0m
[36m[2023-07-11 13:41:34,922][233954] itr=1004, itrs=2000, Progress: 50.20%[0m
[36m[2023-07-11 13:41:46,686][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 13:41:46,686][233954] FPS: 329001.29[0m
[36m[2023-07-11 13:41:50,905][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:41:50,905][233954] Reward + Measures: [[26.53056924  0.16832933  0.18285766  0.27938667  0.27524298  0.30397522]][0m
[37m[1m[2023-07-11 13:41:50,905][233954] Max Reward on eval: 26.53056923672717[0m
[37m[1m[2023-07-11 13:41:50,905][233954] Min Reward on eval: 26.53056923672717[0m
[37m[1m[2023-07-11 13:41:50,906][233954] Mean Reward across all agents: 26.53056923672717[0m
[37m[1m[2023-07-11 13:41:50,906][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:41:55,882][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:41:55,883][233954] Reward + Measures: [[22.51584025  0.31300002  0.34140003  0.3716      0.37760001  0.38262293]
 [55.68638546  0.1867      0.23009999  0.24510001  0.26949999  0.55451536]
 [32.71121294  0.0237      0.10700001  0.1833      0.1123      0.24535421]
 ...
 [ 6.13721015  0.35530001  0.32250002  0.38530001  0.47470003  0.71381837]
 [20.72570788  0.12519999  0.12920001  0.18910001  0.34129998  0.35336137]
 [ 5.18501841  0.2888      0.34060001  0.40610003  0.3581      0.52017593]][0m
[37m[1m[2023-07-11 13:41:55,883][233954] Max Reward on eval: 90.46392510654405[0m
[37m[1m[2023-07-11 13:41:55,883][233954] Min Reward on eval: -13.70994894669857[0m
[37m[1m[2023-07-11 13:41:55,883][233954] Mean Reward across all agents: 24.925409768083682[0m
[37m[1m[2023-07-11 13:41:55,884][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:41:55,886][233954] mean_value=-101.20528648357828, max_value=521.060986252746[0m
[37m[1m[2023-07-11 13:41:55,888][233954] New mean coefficients: [[ 0.02979812  0.00911897 -0.03025893  0.04187527  0.04113092  0.00377913]][0m
[37m[1m[2023-07-11 13:41:55,889][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:42:04,845][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 13:42:04,846][233954] FPS: 428838.73[0m
[36m[2023-07-11 13:42:04,848][233954] itr=1005, itrs=2000, Progress: 50.25%[0m
[36m[2023-07-11 13:42:16,757][233954] train() took 11.81 seconds to complete[0m
[36m[2023-07-11 13:42:16,757][233954] FPS: 325033.96[0m
[36m[2023-07-11 13:42:21,122][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:42:21,122][233954] Reward + Measures: [[25.73208552  0.15808466  0.17181     0.27695134  0.27389333  0.29005453]][0m
[37m[1m[2023-07-11 13:42:21,123][233954] Max Reward on eval: 25.73208552460316[0m
[37m[1m[2023-07-11 13:42:21,123][233954] Min Reward on eval: 25.73208552460316[0m
[37m[1m[2023-07-11 13:42:21,123][233954] Mean Reward across all agents: 25.73208552460316[0m
[37m[1m[2023-07-11 13:42:21,123][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:42:26,387][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:42:26,387][233954] Reward + Measures: [[23.45093311  0.21120003  0.15519999  0.31530005  0.34449998  0.254825  ]
 [40.66125124  0.25040004  0.3046      0.36399999  0.33290002  0.33284757]
 [39.01529986  0.36419997  0.3459      0.42999998  0.41279998  0.47942454]
 ...
 [22.75816119  0.37670001  0.32210004  0.47220001  0.46989998  0.38078901]
 [30.95283428  0.11280002  0.2234      0.28029999  0.1891      0.34622169]
 [21.55533645  0.17219999  0.21069999  0.30060002  0.23900001  0.55026549]][0m
[37m[1m[2023-07-11 13:42:26,388][233954] Max Reward on eval: 80.59179503049236[0m
[37m[1m[2023-07-11 13:42:26,388][233954] Min Reward on eval: -1.765871228580363[0m
[37m[1m[2023-07-11 13:42:26,388][233954] Mean Reward across all agents: 24.33419597350662[0m
[37m[1m[2023-07-11 13:42:26,388][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:42:26,391][233954] mean_value=-100.66868075919001, max_value=317.04196156376616[0m
[37m[1m[2023-07-11 13:42:26,393][233954] New mean coefficients: [[-0.00398724  0.01982576  0.00544005  0.0301521   0.02420285 -0.03202792]][0m
[37m[1m[2023-07-11 13:42:26,394][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:42:35,494][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 13:42:35,494][233954] FPS: 422059.53[0m
[36m[2023-07-11 13:42:35,496][233954] itr=1006, itrs=2000, Progress: 50.30%[0m
[36m[2023-07-11 13:42:47,106][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 13:42:47,106][233954] FPS: 333594.86[0m
[36m[2023-07-11 13:42:51,449][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:42:51,450][233954] Reward + Measures: [[25.32705721  0.14497834  0.16800167  0.29374599  0.25778165  0.28634799]][0m
[37m[1m[2023-07-11 13:42:51,450][233954] Max Reward on eval: 25.32705721332316[0m
[37m[1m[2023-07-11 13:42:51,450][233954] Min Reward on eval: 25.32705721332316[0m
[37m[1m[2023-07-11 13:42:51,451][233954] Mean Reward across all agents: 25.32705721332316[0m
[37m[1m[2023-07-11 13:42:51,451][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:42:56,402][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:42:56,463][233954] Reward + Measures: [[25.37322619  0.12550001  0.14040001  0.2172      0.22390001  0.2793065 ]
 [15.47642598  0.2016      0.25689998  0.31469998  0.3073      0.25478777]
 [26.63005038  0.29620001  0.2277      0.39639997  0.3951      0.37875015]
 ...
 [24.76271265  0.0328      0.0679      0.1626      0.14050001  0.24912988]
 [20.16492418  0.11439999  0.18350001  0.2701      0.19759999  0.24780193]
 [36.30478892  0.28029999  0.28150001  0.39830002  0.3479      0.46466896]][0m
[37m[1m[2023-07-11 13:42:56,463][233954] Max Reward on eval: 89.92295346827014[0m
[37m[1m[2023-07-11 13:42:56,463][233954] Min Reward on eval: -10.536069348279852[0m
[37m[1m[2023-07-11 13:42:56,463][233954] Mean Reward across all agents: 25.11327039994404[0m
[37m[1m[2023-07-11 13:42:56,464][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:42:56,465][233954] mean_value=-82.68857500256395, max_value=511.2270340056857[0m
[37m[1m[2023-07-11 13:42:56,468][233954] New mean coefficients: [[-0.01717553  0.02867189  0.04974145  0.04522394  0.04204535 -0.01945291]][0m
[37m[1m[2023-07-11 13:42:56,469][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:43:05,465][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 13:43:05,465][233954] FPS: 426927.51[0m
[36m[2023-07-11 13:43:05,467][233954] itr=1007, itrs=2000, Progress: 50.35%[0m
[36m[2023-07-11 13:43:17,087][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 13:43:17,087][233954] FPS: 333172.47[0m
[36m[2023-07-11 13:43:21,337][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:43:21,338][233954] Reward + Measures: [[25.27613388  0.15397699  0.17542233  0.30754232  0.27016199  0.30162516]][0m
[37m[1m[2023-07-11 13:43:21,338][233954] Max Reward on eval: 25.276133881823743[0m
[37m[1m[2023-07-11 13:43:21,338][233954] Min Reward on eval: 25.276133881823743[0m
[37m[1m[2023-07-11 13:43:21,338][233954] Mean Reward across all agents: 25.276133881823743[0m
[37m[1m[2023-07-11 13:43:21,338][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:43:26,357][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:43:26,358][233954] Reward + Measures: [[11.37804327  0.29790002  0.32120001  0.3811      0.38329998  0.28861266]
 [12.95230008  0.2102      0.2368      0.30560002  0.34030002  0.38326934]
 [ 5.20655216  0.12279999  0.1321      0.18120001  0.19000001  0.46522257]
 ...
 [18.56342712  0.1902      0.2203      0.3019      0.36840001  0.2511321 ]
 [73.70717928  0.1139      0.25340003  0.29899999  0.30130002  0.64332873]
 [57.2262479   0.51160002  0.4323      0.55870003  0.59980005  0.49733534]][0m
[37m[1m[2023-07-11 13:43:26,358][233954] Max Reward on eval: 112.7776261602994[0m
[37m[1m[2023-07-11 13:43:26,358][233954] Min Reward on eval: -30.24209055772517[0m
[37m[1m[2023-07-11 13:43:26,358][233954] Mean Reward across all agents: 24.744018567034892[0m
[37m[1m[2023-07-11 13:43:26,359][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:43:26,361][233954] mean_value=-78.00447168580328, max_value=494.56454837543424[0m
[37m[1m[2023-07-11 13:43:26,364][233954] New mean coefficients: [[-0.01614572 -0.01527654  0.07658611  0.08385077  0.03748168 -0.0437406 ]][0m
[37m[1m[2023-07-11 13:43:26,365][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:43:35,319][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 13:43:35,319][233954] FPS: 428916.21[0m
[36m[2023-07-11 13:43:35,321][233954] itr=1008, itrs=2000, Progress: 50.40%[0m
[36m[2023-07-11 13:43:46,880][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 13:43:46,881][233954] FPS: 334924.13[0m
[36m[2023-07-11 13:43:51,112][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:43:51,112][233954] Reward + Measures: [[24.50047933  0.168898    0.19132733  0.34632266  0.280377    0.31312558]][0m
[37m[1m[2023-07-11 13:43:51,112][233954] Max Reward on eval: 24.5004793324529[0m
[37m[1m[2023-07-11 13:43:51,112][233954] Min Reward on eval: 24.5004793324529[0m
[37m[1m[2023-07-11 13:43:51,113][233954] Mean Reward across all agents: 24.5004793324529[0m
[37m[1m[2023-07-11 13:43:51,113][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:43:56,060][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:43:56,060][233954] Reward + Measures: [[20.9913924   0.10640001  0.17590001  0.28920001  0.20480001  0.40752718]
 [14.03887761  0.37920001  0.22930001  0.49110004  0.4876      0.26360282]
 [16.29935679  0.37060001  0.31259999  0.47849998  0.47470003  0.32368308]
 ...
 [23.50064932  0.1912      0.25240001  0.38799998  0.30449998  0.24254417]
 [25.31358601  0.19590001  0.1392      0.345       0.30599999  0.37687111]
 [66.69144691  0.2942      0.22649999  0.43619999  0.46520001  0.67653197]][0m
[37m[1m[2023-07-11 13:43:56,061][233954] Max Reward on eval: 133.39860591411124[0m
[37m[1m[2023-07-11 13:43:56,061][233954] Min Reward on eval: -41.466266650962645[0m
[37m[1m[2023-07-11 13:43:56,061][233954] Mean Reward across all agents: 27.768706708236465[0m
[37m[1m[2023-07-11 13:43:56,061][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:43:56,064][233954] mean_value=-37.18154386327284, max_value=322.40714671969056[0m
[37m[1m[2023-07-11 13:43:56,066][233954] New mean coefficients: [[-0.00232572 -0.02077088  0.00149284  0.1176029   0.03457012 -0.05244669]][0m
[37m[1m[2023-07-11 13:43:56,067][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:44:05,016][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 13:44:05,016][233954] FPS: 429185.22[0m
[36m[2023-07-11 13:44:05,019][233954] itr=1009, itrs=2000, Progress: 50.45%[0m
[36m[2023-07-11 13:44:16,669][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 13:44:16,670][233954] FPS: 332326.00[0m
[36m[2023-07-11 13:44:21,008][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:44:21,009][233954] Reward + Measures: [[27.91845972  0.16654333  0.17956066  0.37374699  0.268363    0.3158873 ]][0m
[37m[1m[2023-07-11 13:44:21,009][233954] Max Reward on eval: 27.918459721536202[0m
[37m[1m[2023-07-11 13:44:21,009][233954] Min Reward on eval: 27.918459721536202[0m
[37m[1m[2023-07-11 13:44:21,010][233954] Mean Reward across all agents: 27.918459721536202[0m
[37m[1m[2023-07-11 13:44:21,010][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:44:26,000][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:44:26,001][233954] Reward + Measures: [[-4.89796129  0.38659999  0.45000002  0.49419999  0.49440002  0.49430147]
 [16.17454625  0.27309999  0.21690002  0.39790002  0.4021      0.42405716]
 [26.97822385  0.11860001  0.18260001  0.31329998  0.2191      0.30111727]
 ...
 [29.43067682  0.28650001  0.2386      0.44580004  0.38439998  0.33577225]
 [21.49058393  0.28780001  0.31830001  0.44619998  0.39070001  0.31760505]
 [15.13729279  0.29170001  0.3425      0.4657      0.36830002  0.33511397]][0m
[37m[1m[2023-07-11 13:44:26,001][233954] Max Reward on eval: 250.83603046713398[0m
[37m[1m[2023-07-11 13:44:26,001][233954] Min Reward on eval: -11.323586341389454[0m
[37m[1m[2023-07-11 13:44:26,002][233954] Mean Reward across all agents: 28.22479498198212[0m
[37m[1m[2023-07-11 13:44:26,002][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:44:26,004][233954] mean_value=-70.51850337776793, max_value=251.60113203884512[0m
[37m[1m[2023-07-11 13:44:26,007][233954] New mean coefficients: [[ 0.01157299 -0.01974975  0.02234278  0.09972307  0.05417353  0.06681174]][0m
[37m[1m[2023-07-11 13:44:26,008][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:44:35,009][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 13:44:35,009][233954] FPS: 426699.67[0m
[36m[2023-07-11 13:44:35,011][233954] itr=1010, itrs=2000, Progress: 50.50%[0m
[37m[1m[2023-07-11 13:48:04,054][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000990[0m
[36m[2023-07-11 13:48:16,296][233954] train() took 11.74 seconds to complete[0m
[36m[2023-07-11 13:48:16,296][233954] FPS: 327129.75[0m
[36m[2023-07-11 13:48:20,513][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:48:20,513][233954] Reward + Measures: [[26.50903804  0.18033767  0.19475967  0.41275066  0.28973165  0.32865548]][0m
[37m[1m[2023-07-11 13:48:20,513][233954] Max Reward on eval: 26.509038038092744[0m
[37m[1m[2023-07-11 13:48:20,514][233954] Min Reward on eval: 26.509038038092744[0m
[37m[1m[2023-07-11 13:48:20,514][233954] Mean Reward across all agents: 26.509038038092744[0m
[37m[1m[2023-07-11 13:48:20,514][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:48:25,520][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:48:25,521][233954] Reward + Measures: [[27.62325448  0.10730001  0.0732      0.322       0.223       0.49273261]
 [27.96298622  0.0272      0.0523      0.25760001  0.17220001  0.52441275]
 [16.8865289   0.0255      0.0742      0.2854      0.16180001  0.23607381]
 ...
 [23.17453623  0.0444      0.0619      0.21300001  0.1699      0.6937477 ]
 [15.83712804  0.29360002  0.34639999  0.49110004  0.37580001  0.30409744]
 [29.62834798  0.14099999  0.1524      0.347       0.27090001  0.37978169]][0m
[37m[1m[2023-07-11 13:48:25,521][233954] Max Reward on eval: 148.9498264920432[0m
[37m[1m[2023-07-11 13:48:25,521][233954] Min Reward on eval: -105.97962640507612[0m
[37m[1m[2023-07-11 13:48:25,522][233954] Mean Reward across all agents: 26.649496114626128[0m
[37m[1m[2023-07-11 13:48:25,522][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:48:25,524][233954] mean_value=-103.11278112639404, max_value=265.7319266958257[0m
[37m[1m[2023-07-11 13:48:25,527][233954] New mean coefficients: [[ 0.02332982 -0.0459594   0.00949427  0.06790206  0.06684686  0.06347076]][0m
[37m[1m[2023-07-11 13:48:25,527][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:48:34,514][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 13:48:34,514][233954] FPS: 427387.99[0m
[36m[2023-07-11 13:48:34,516][233954] itr=1011, itrs=2000, Progress: 50.55%[0m
[36m[2023-07-11 13:48:46,182][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 13:48:46,182][233954] FPS: 331987.09[0m
[36m[2023-07-11 13:48:50,366][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:48:50,367][233954] Reward + Measures: [[26.87251035  0.17514032  0.19197901  0.41480032  0.29125834  0.3511661 ]][0m
[37m[1m[2023-07-11 13:48:50,367][233954] Max Reward on eval: 26.872510351732284[0m
[37m[1m[2023-07-11 13:48:50,367][233954] Min Reward on eval: 26.872510351732284[0m
[37m[1m[2023-07-11 13:48:50,368][233954] Mean Reward across all agents: 26.872510351732284[0m
[37m[1m[2023-07-11 13:48:50,368][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:48:55,538][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:48:55,544][233954] Reward + Measures: [[  7.55538      0.23190001   0.16070001   0.3759       0.34670001
    0.37843034]
 [ 72.11660004   0.12040001   0.17809999   0.32260001   0.30419999
    0.67013001]
 [ 10.10612164   0.1935       0.2263       0.37869999   0.33379999
    0.39142823]
 ...
 [139.71539407   0.37539998   0.51859999   0.59990001   0.62819999
    0.79608595]
 [ 72.9787897    0.3673       0.4068       0.54539996   0.56220001
    0.56748039]
 [ 27.12192717   0.3012       0.24109998   0.48160002   0.42280003
    0.38052288]][0m
[37m[1m[2023-07-11 13:48:55,544][233954] Max Reward on eval: 346.10889418683945[0m
[37m[1m[2023-07-11 13:48:55,545][233954] Min Reward on eval: -36.6790409778594[0m
[37m[1m[2023-07-11 13:48:55,545][233954] Mean Reward across all agents: 34.05557586667462[0m
[37m[1m[2023-07-11 13:48:55,545][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:48:55,548][233954] mean_value=-76.28167680465776, max_value=787.8690010349173[0m
[37m[1m[2023-07-11 13:48:55,551][233954] New mean coefficients: [[ 0.07883047 -0.04910932  0.05433264  0.13360323  0.11388671  0.05355556]][0m
[37m[1m[2023-07-11 13:48:55,552][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:49:04,457][233954] train() took 8.90 seconds to complete[0m
[36m[2023-07-11 13:49:04,457][233954] FPS: 431279.63[0m
[36m[2023-07-11 13:49:04,460][233954] itr=1012, itrs=2000, Progress: 50.60%[0m
[36m[2023-07-11 13:49:16,177][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 13:49:16,178][233954] FPS: 330435.80[0m
[36m[2023-07-11 13:49:20,474][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:49:20,475][233954] Reward + Measures: [[28.43385335  0.18438068  0.193974    0.44601765  0.30651665  0.35822093]][0m
[37m[1m[2023-07-11 13:49:20,475][233954] Max Reward on eval: 28.43385334833546[0m
[37m[1m[2023-07-11 13:49:20,475][233954] Min Reward on eval: 28.43385334833546[0m
[37m[1m[2023-07-11 13:49:20,475][233954] Mean Reward across all agents: 28.43385334833546[0m
[37m[1m[2023-07-11 13:49:20,476][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:49:25,448][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:49:25,448][233954] Reward + Measures: [[37.79551598  0.12160001  0.07210001  0.36859998  0.23840001  0.48401928]
 [14.95061686  0.46669999  0.40760002  0.62099999  0.57310003  0.31642756]
 [20.0326788   0.11669999  0.13520001  0.28010002  0.2626      0.46425459]
 ...
 [16.07164662  0.1973      0.24040003  0.39559999  0.32640001  0.28531024]
 [30.28319458  0.1141      0.1664      0.24759999  0.19990002  0.34567419]
 [87.45230436  0.1134      0.0422      0.24769998  0.2467      0.50189811]][0m
[37m[1m[2023-07-11 13:49:25,449][233954] Max Reward on eval: 92.54570656588767[0m
[37m[1m[2023-07-11 13:49:25,449][233954] Min Reward on eval: -40.474549299757925[0m
[37m[1m[2023-07-11 13:49:25,449][233954] Mean Reward across all agents: 25.607549290839177[0m
[37m[1m[2023-07-11 13:49:25,449][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:49:25,452][233954] mean_value=-81.17561069448762, max_value=556.0798687117756[0m
[37m[1m[2023-07-11 13:49:25,454][233954] New mean coefficients: [[ 0.04330594 -0.03778753  0.08815719  0.12240327  0.11128425 -0.12389414]][0m
[37m[1m[2023-07-11 13:49:25,455][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:49:34,459][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 13:49:34,459][233954] FPS: 426579.80[0m
[36m[2023-07-11 13:49:34,461][233954] itr=1013, itrs=2000, Progress: 50.65%[0m
[36m[2023-07-11 13:49:46,185][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 13:49:46,186][233954] FPS: 330337.30[0m
[36m[2023-07-11 13:49:50,415][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:49:50,415][233954] Reward + Measures: [[27.55506174  0.20047599  0.20586665  0.48024032  0.33116499  0.36112699]][0m
[37m[1m[2023-07-11 13:49:50,415][233954] Max Reward on eval: 27.555061744408086[0m
[37m[1m[2023-07-11 13:49:50,415][233954] Min Reward on eval: 27.555061744408086[0m
[37m[1m[2023-07-11 13:49:50,416][233954] Mean Reward across all agents: 27.555061744408086[0m
[37m[1m[2023-07-11 13:49:50,416][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:49:55,459][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:49:55,459][233954] Reward + Measures: [[20.07611007  0.19760001  0.22860001  0.47320005  0.347       0.25041974]
 [37.37447503  0.2597      0.26890001  0.52340001  0.35590002  0.39569971]
 [10.63727671  0.38000003  0.41710001  0.50630003  0.4664      0.39574122]
 ...
 [18.30509653  0.38820001  0.31090003  0.56269997  0.48969999  0.29833719]
 [ 8.7471728   0.26190001  0.25750002  0.43700001  0.39970002  0.70582789]
 [-0.33597397  0.71630001  0.75660002  0.7579      0.77790004  0.4505344 ]][0m
[37m[1m[2023-07-11 13:49:55,459][233954] Max Reward on eval: 128.83027611889412[0m
[37m[1m[2023-07-11 13:49:55,460][233954] Min Reward on eval: -54.295422269881236[0m
[37m[1m[2023-07-11 13:49:55,460][233954] Mean Reward across all agents: 24.26352550338945[0m
[37m[1m[2023-07-11 13:49:55,460][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:49:55,463][233954] mean_value=-54.794889453622126, max_value=473.1897797163483[0m
[37m[1m[2023-07-11 13:49:55,466][233954] New mean coefficients: [[ 0.02954846 -0.04348077  0.08202755  0.11094896  0.10129172 -0.23654632]][0m
[37m[1m[2023-07-11 13:49:55,467][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:50:04,505][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 13:50:04,505][233954] FPS: 424923.60[0m
[36m[2023-07-11 13:50:04,508][233954] itr=1014, itrs=2000, Progress: 50.70%[0m
[36m[2023-07-11 13:50:16,287][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 13:50:16,287][233954] FPS: 328643.75[0m
[36m[2023-07-11 13:50:20,604][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:50:20,604][233954] Reward + Measures: [[30.52804479  0.20821469  0.20817032  0.46227497  0.34322071  0.34598672]][0m
[37m[1m[2023-07-11 13:50:20,605][233954] Max Reward on eval: 30.528044785621507[0m
[37m[1m[2023-07-11 13:50:20,605][233954] Min Reward on eval: 30.528044785621507[0m
[37m[1m[2023-07-11 13:50:20,605][233954] Mean Reward across all agents: 30.528044785621507[0m
[37m[1m[2023-07-11 13:50:20,605][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:50:25,533][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:50:25,534][233954] Reward + Measures: [[20.89326963  0.1054      0.1347      0.4522      0.294       0.26767817]
 [20.63638216  0.20949998  0.1499      0.45289999  0.35979998  0.30742338]
 [13.28036884  0.271       0.32730004  0.43740001  0.43810001  0.47509342]
 ...
 [19.67196774  0.12349999  0.13610001  0.2985      0.34870002  0.34978998]
 [23.98797405  0.20100001  0.23020001  0.40180001  0.36559999  0.33332404]
 [12.96779689  0.1962      0.22720002  0.39740005  0.34550002  0.57607001]][0m
[37m[1m[2023-07-11 13:50:25,534][233954] Max Reward on eval: 140.28054655729792[0m
[37m[1m[2023-07-11 13:50:25,534][233954] Min Reward on eval: -28.666998061729828[0m
[37m[1m[2023-07-11 13:50:25,534][233954] Mean Reward across all agents: 26.136435845209924[0m
[37m[1m[2023-07-11 13:50:25,535][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:50:25,538][233954] mean_value=-32.304649908502434, max_value=309.83574473682893[0m
[37m[1m[2023-07-11 13:50:25,540][233954] New mean coefficients: [[ 0.00726526 -0.03645062  0.1006677   0.12152361  0.10189619 -0.1967739 ]][0m
[37m[1m[2023-07-11 13:50:25,541][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:50:34,448][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 13:50:34,449][233954] FPS: 431200.71[0m
[36m[2023-07-11 13:50:34,451][233954] itr=1015, itrs=2000, Progress: 50.75%[0m
[36m[2023-07-11 13:50:45,982][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 13:50:45,982][233954] FPS: 335735.40[0m
[36m[2023-07-11 13:50:50,214][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:50:50,215][233954] Reward + Measures: [[29.36371941  0.20361267  0.214803    0.46170598  0.34807795  0.33890235]][0m
[37m[1m[2023-07-11 13:50:50,215][233954] Max Reward on eval: 29.363719410372145[0m
[37m[1m[2023-07-11 13:50:50,215][233954] Min Reward on eval: 29.363719410372145[0m
[37m[1m[2023-07-11 13:50:50,216][233954] Mean Reward across all agents: 29.363719410372145[0m
[37m[1m[2023-07-11 13:50:50,216][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:50:55,212][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:50:55,212][233954] Reward + Measures: [[22.80238622  0.20290001  0.24800001  0.45830002  0.34580001  0.2637918 ]
 [13.72716661  0.1059      0.15890001  0.3944      0.2753      0.33185124]
 [13.65051737  0.35460004  0.41950002  0.53609997  0.48230001  0.34280062]
 ...
 [15.91384813  0.29359999  0.33459997  0.5273      0.41330004  0.29005605]
 [31.14254033  0.0521      0.1013      0.34779999  0.2228      0.32963547]
 [23.07480785  0.0186      0.1184      0.30759999  0.0843      0.28093889]][0m
[37m[1m[2023-07-11 13:50:55,212][233954] Max Reward on eval: 124.95822042096407[0m
[37m[1m[2023-07-11 13:50:55,218][233954] Min Reward on eval: -38.92637465908192[0m
[37m[1m[2023-07-11 13:50:55,218][233954] Mean Reward across all agents: 28.436092024513915[0m
[37m[1m[2023-07-11 13:50:55,219][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:50:55,222][233954] mean_value=-22.720439956204707, max_value=569.1823277486488[0m
[37m[1m[2023-07-11 13:50:55,225][233954] New mean coefficients: [[ 0.01902878 -0.0247107   0.01971634  0.1074347   0.09843603 -0.26895916]][0m
[37m[1m[2023-07-11 13:50:55,226][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:51:04,193][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 13:51:04,194][233954] FPS: 428286.10[0m
[36m[2023-07-11 13:51:04,196][233954] itr=1016, itrs=2000, Progress: 50.80%[0m
[36m[2023-07-11 13:51:15,971][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 13:51:15,971][233954] FPS: 328728.46[0m
[36m[2023-07-11 13:51:20,244][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:51:20,245][233954] Reward + Measures: [[29.60405286  0.205484    0.21701765  0.51302028  0.342875    0.3293789 ]][0m
[37m[1m[2023-07-11 13:51:20,245][233954] Max Reward on eval: 29.60405285570799[0m
[37m[1m[2023-07-11 13:51:20,245][233954] Min Reward on eval: 29.60405285570799[0m
[37m[1m[2023-07-11 13:51:20,245][233954] Mean Reward across all agents: 29.60405285570799[0m
[37m[1m[2023-07-11 13:51:20,246][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:51:25,244][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:51:25,245][233954] Reward + Measures: [[63.87411861  0.31380001  0.25079998  0.3942      0.37050003  0.69265372]
 [79.30045241  0.1964      0.13500001  0.4217      0.3671      0.49328041]
 [71.75914552  0.20640002  0.177       0.4377      0.35990003  0.53822035]
 ...
 [17.00984081  0.30679998  0.38530001  0.45720005  0.3646      0.35270309]
 [16.13098395  0.36600003  0.34660003  0.56669998  0.4894      0.32505393]
 [32.64285039  0.4729      0.43940002  0.51499999  0.53579998  0.80351967]][0m
[37m[1m[2023-07-11 13:51:25,245][233954] Max Reward on eval: 154.97175603739453[0m
[37m[1m[2023-07-11 13:51:25,245][233954] Min Reward on eval: -32.59808665603632[0m
[37m[1m[2023-07-11 13:51:25,245][233954] Mean Reward across all agents: 29.16347229588139[0m
[37m[1m[2023-07-11 13:51:25,246][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:51:25,249][233954] mean_value=-27.456154654495307, max_value=508.3798163175676[0m
[37m[1m[2023-07-11 13:51:25,252][233954] New mean coefficients: [[-0.0292502  -0.00403395  0.02704957  0.07974434  0.0737865  -0.28118023]][0m
[37m[1m[2023-07-11 13:51:25,253][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:51:34,287][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 13:51:34,287][233954] FPS: 425132.53[0m
[36m[2023-07-11 13:51:34,289][233954] itr=1017, itrs=2000, Progress: 50.85%[0m
[36m[2023-07-11 13:51:46,008][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 13:51:46,008][233954] FPS: 330381.36[0m
[36m[2023-07-11 13:51:50,301][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:51:50,301][233954] Reward + Measures: [[29.64678917  0.21817702  0.23100232  0.52269566  0.35440332  0.33318189]][0m
[37m[1m[2023-07-11 13:51:50,301][233954] Max Reward on eval: 29.646789174473145[0m
[37m[1m[2023-07-11 13:51:50,301][233954] Min Reward on eval: 29.646789174473145[0m
[37m[1m[2023-07-11 13:51:50,302][233954] Mean Reward across all agents: 29.646789174473145[0m
[37m[1m[2023-07-11 13:51:50,302][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:51:55,557][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:51:55,557][233954] Reward + Measures: [[22.1787889   0.18840002  0.24150001  0.49240002  0.3506      0.29032713]
 [89.11919589  0.46269998  0.50580001  0.62300003  0.62270004  0.60033906]
 [10.9573228   0.10530001  0.1928      0.45180002  0.233       0.29022345]
 ...
 [27.11446713  0.0183      0.1087      0.3267      0.16989999  0.33879778]
 [72.13242615  0.0233      0.1551      0.3249      0.31990001  0.47885156]
 [29.4135871   0.08580001  0.10420001  0.4113      0.24419999  0.33271688]][0m
[37m[1m[2023-07-11 13:51:55,558][233954] Max Reward on eval: 108.50073664220982[0m
[37m[1m[2023-07-11 13:51:55,558][233954] Min Reward on eval: -39.60470996063668[0m
[37m[1m[2023-07-11 13:51:55,558][233954] Mean Reward across all agents: 27.772344719413297[0m
[37m[1m[2023-07-11 13:51:55,558][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:51:55,563][233954] mean_value=-5.417712390988178, max_value=509.84471140378156[0m
[37m[1m[2023-07-11 13:51:55,566][233954] New mean coefficients: [[-0.04702536  0.04727855  0.02312077  0.09671606  0.06010665 -0.30275476]][0m
[37m[1m[2023-07-11 13:51:55,567][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:52:04,706][233954] train() took 9.14 seconds to complete[0m
[36m[2023-07-11 13:52:04,706][233954] FPS: 420256.81[0m
[36m[2023-07-11 13:52:04,708][233954] itr=1018, itrs=2000, Progress: 50.90%[0m
[36m[2023-07-11 13:52:16,486][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 13:52:16,486][233954] FPS: 328798.62[0m
[36m[2023-07-11 13:52:20,737][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:52:20,737][233954] Reward + Measures: [[26.69674773  0.19216467  0.21859267  0.52177036  0.338119    0.31568187]][0m
[37m[1m[2023-07-11 13:52:20,737][233954] Max Reward on eval: 26.696747730138412[0m
[37m[1m[2023-07-11 13:52:20,738][233954] Min Reward on eval: 26.696747730138412[0m
[37m[1m[2023-07-11 13:52:20,738][233954] Mean Reward across all agents: 26.696747730138412[0m
[37m[1m[2023-07-11 13:52:20,738][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:52:25,665][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:52:25,666][233954] Reward + Measures: [[ 20.76335211   0.19770001   0.25430003   0.3901       0.3809
    0.5221051 ]
 [ 20.32406431   0.38490003   0.43350002   0.6178       0.48989996
    0.24876831]
 [ 28.95185996   0.18380001   0.15089999   0.50940001   0.36809999
    0.29347935]
 ...
 [-11.55837682   0.36250001   0.31810001   0.53389996   0.4693
    0.44022724]
 [ 20.52395533   0.1096       0.15450001   0.32030001   0.27859998
    0.37671885]
 [ 24.89804989   0.2041       0.22579999   0.42090002   0.33140001
    0.34103361]][0m
[37m[1m[2023-07-11 13:52:25,666][233954] Max Reward on eval: 218.80021704686806[0m
[37m[1m[2023-07-11 13:52:25,666][233954] Min Reward on eval: -53.68052169904113[0m
[37m[1m[2023-07-11 13:52:25,666][233954] Mean Reward across all agents: 28.79604257158125[0m
[37m[1m[2023-07-11 13:52:25,667][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:52:25,670][233954] mean_value=-16.631687170201893, max_value=718.800217046868[0m
[37m[1m[2023-07-11 13:52:25,673][233954] New mean coefficients: [[-0.088282    0.07642953  0.06133717  0.06540112  0.05152188 -0.21539253]][0m
[37m[1m[2023-07-11 13:52:25,674][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:52:34,642][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 13:52:34,643][233954] FPS: 428248.08[0m
[36m[2023-07-11 13:52:34,645][233954] itr=1019, itrs=2000, Progress: 50.95%[0m
[36m[2023-07-11 13:52:46,327][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 13:52:46,328][233954] FPS: 331481.76[0m
[36m[2023-07-11 13:52:50,564][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:52:50,565][233954] Reward + Measures: [[27.45213852  0.21241935  0.22536966  0.55837071  0.36202928  0.31999707]][0m
[37m[1m[2023-07-11 13:52:50,565][233954] Max Reward on eval: 27.45213852046825[0m
[37m[1m[2023-07-11 13:52:50,565][233954] Min Reward on eval: 27.45213852046825[0m
[37m[1m[2023-07-11 13:52:50,566][233954] Mean Reward across all agents: 27.45213852046825[0m
[37m[1m[2023-07-11 13:52:50,566][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:52:55,509][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:52:55,510][233954] Reward + Measures: [[26.77998078  0.1073      0.0605      0.45650002  0.25750002  0.36489603]
 [12.71386241  0.1184      0.13000001  0.3795      0.30289999  0.40141898]
 [25.184038    0.1311      0.21360002  0.32230002  0.17290001  0.331673  ]
 ...
 [27.77244441  0.1091      0.1583      0.41630003  0.27140003  0.27125579]
 [23.58016337  0.19900002  0.24449997  0.49689999  0.3457      0.2930043 ]
 [12.0226276   0.21180001  0.2552      0.46540004  0.30630001  0.39887187]][0m
[37m[1m[2023-07-11 13:52:55,510][233954] Max Reward on eval: 196.68881922657602[0m
[37m[1m[2023-07-11 13:52:55,511][233954] Min Reward on eval: -36.71621879145969[0m
[37m[1m[2023-07-11 13:52:55,511][233954] Mean Reward across all agents: 26.164372771412946[0m
[37m[1m[2023-07-11 13:52:55,511][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:52:55,515][233954] mean_value=-31.76289831148441, max_value=522.7837424127152[0m
[37m[1m[2023-07-11 13:52:55,517][233954] New mean coefficients: [[-0.08438138  0.06544032  0.09670418  0.04264349  0.04592708 -0.15733072]][0m
[37m[1m[2023-07-11 13:52:55,518][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:53:04,519][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 13:53:04,519][233954] FPS: 426692.18[0m
[36m[2023-07-11 13:53:04,522][233954] itr=1020, itrs=2000, Progress: 51.00%[0m
[37m[1m[2023-07-11 13:56:42,969][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001000[0m
[36m[2023-07-11 13:56:55,168][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 13:56:55,169][233954] FPS: 332564.62[0m
[36m[2023-07-11 13:56:59,433][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:56:59,434][233954] Reward + Measures: [[25.5584216   0.21573634  0.23986067  0.53829497  0.36532634  0.31202477]][0m
[37m[1m[2023-07-11 13:56:59,434][233954] Max Reward on eval: 25.55842159503929[0m
[37m[1m[2023-07-11 13:56:59,434][233954] Min Reward on eval: 25.55842159503929[0m
[37m[1m[2023-07-11 13:56:59,435][233954] Mean Reward across all agents: 25.55842159503929[0m
[37m[1m[2023-07-11 13:56:59,435][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:57:04,349][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:57:04,350][233954] Reward + Measures: [[10.12239027  0.38300005  0.34330001  0.62159997  0.49690005  0.46386972]
 [20.45144335  0.29629999  0.34380001  0.46890002  0.40089998  0.27075934]
 [21.64890985  0.27880001  0.33050001  0.51050001  0.38890001  0.26773167]
 ...
 [20.86982544  0.30759999  0.34470001  0.42910001  0.41249999  0.46144709]
 [35.33689074  0.28280002  0.24639998  0.56100005  0.42670003  0.35191926]
 [ 7.249489    0.57380003  0.56740004  0.59540004  0.63030005  0.32578275]][0m
[37m[1m[2023-07-11 13:57:04,350][233954] Max Reward on eval: 180.1027197562158[0m
[37m[1m[2023-07-11 13:57:04,350][233954] Min Reward on eval: -33.17728741387837[0m
[37m[1m[2023-07-11 13:57:04,351][233954] Mean Reward across all agents: 26.47472113147762[0m
[37m[1m[2023-07-11 13:57:04,351][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:57:04,355][233954] mean_value=-15.146751661551962, max_value=532.8972846515389[0m
[37m[1m[2023-07-11 13:57:04,358][233954] New mean coefficients: [[-0.09378829  0.04762443 -0.00871126  0.01953437  0.03440915 -0.31271482]][0m
[37m[1m[2023-07-11 13:57:04,359][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:57:13,258][233954] train() took 8.90 seconds to complete[0m
[36m[2023-07-11 13:57:13,258][233954] FPS: 431588.51[0m
[36m[2023-07-11 13:57:13,260][233954] itr=1021, itrs=2000, Progress: 51.05%[0m
[36m[2023-07-11 13:57:24,759][233954] train() took 11.40 seconds to complete[0m
[36m[2023-07-11 13:57:24,759][233954] FPS: 336806.73[0m
[36m[2023-07-11 13:57:28,935][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:57:28,935][233954] Reward + Measures: [[25.33552959  0.20010166  0.22707634  0.54442233  0.35301766  0.30766836]][0m
[37m[1m[2023-07-11 13:57:28,936][233954] Max Reward on eval: 25.33552958509935[0m
[37m[1m[2023-07-11 13:57:28,936][233954] Min Reward on eval: 25.33552958509935[0m
[37m[1m[2023-07-11 13:57:28,936][233954] Mean Reward across all agents: 25.33552958509935[0m
[37m[1m[2023-07-11 13:57:28,936][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:57:33,806][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:57:33,807][233954] Reward + Measures: [[18.47968923  0.29179999  0.32979998  0.54610002  0.41929999  0.25265315]
 [12.65628261  0.1952      0.21939997  0.46089998  0.39920002  0.23525386]
 [25.37781032  0.0141      0.075       0.42280003  0.33550003  0.24877527]
 ...
 [31.04619516  0.019       0.0781      0.4501      0.2581      0.26305866]
 [ 7.26710832  0.1109      0.17999999  0.50599998  0.28590003  0.35098577]
 [18.22665865  0.19770001  0.22360002  0.47600004  0.41219997  0.27470508]][0m
[37m[1m[2023-07-11 13:57:33,807][233954] Max Reward on eval: 119.85215866608777[0m
[37m[1m[2023-07-11 13:57:33,807][233954] Min Reward on eval: -29.03242412796244[0m
[37m[1m[2023-07-11 13:57:33,807][233954] Mean Reward across all agents: 24.675028902237052[0m
[37m[1m[2023-07-11 13:57:33,808][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:57:33,812][233954] mean_value=-23.346329207961546, max_value=540.4395294538699[0m
[37m[1m[2023-07-11 13:57:33,815][233954] New mean coefficients: [[-0.12940517  0.05075751 -0.03391667  0.04268339  0.03162304 -0.2557514 ]][0m
[37m[1m[2023-07-11 13:57:33,815][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:57:42,750][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 13:57:42,750][233954] FPS: 429872.16[0m
[36m[2023-07-11 13:57:42,752][233954] itr=1022, itrs=2000, Progress: 51.10%[0m
[36m[2023-07-11 13:57:54,487][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 13:57:54,488][233954] FPS: 329886.68[0m
[36m[2023-07-11 13:57:58,737][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:57:58,738][233954] Reward + Measures: [[24.55440403  0.18506667  0.21498533  0.52034098  0.33611566  0.30584261]][0m
[37m[1m[2023-07-11 13:57:58,738][233954] Max Reward on eval: 24.554404027849824[0m
[37m[1m[2023-07-11 13:57:58,738][233954] Min Reward on eval: 24.554404027849824[0m
[37m[1m[2023-07-11 13:57:58,739][233954] Mean Reward across all agents: 24.554404027849824[0m
[37m[1m[2023-07-11 13:57:58,739][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:58:03,971][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:58:03,972][233954] Reward + Measures: [[35.983184    0.21700001  0.29930001  0.48270002  0.3651      0.33481023]
 [31.14897433  0.0275      0.0939      0.37369999  0.1999      0.50046933]
 [67.84478282  0.10860001  0.26910001  0.53479999  0.3996      0.43086806]
 ...
 [ 9.75193747  0.29190001  0.3285      0.59600002  0.51560003  0.27326921]
 [25.03005565  0.19680001  0.21299998  0.58499998  0.3777      0.20156141]
 [15.13716701  0.2802      0.3292      0.52730006  0.43050003  0.24364746]][0m
[37m[1m[2023-07-11 13:58:03,972][233954] Max Reward on eval: 163.5047895711381[0m
[37m[1m[2023-07-11 13:58:03,972][233954] Min Reward on eval: -20.172497606161052[0m
[37m[1m[2023-07-11 13:58:03,972][233954] Mean Reward across all agents: 28.822579715528168[0m
[37m[1m[2023-07-11 13:58:03,972][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:58:03,977][233954] mean_value=-8.003081580917746, max_value=525.5081537759257[0m
[37m[1m[2023-07-11 13:58:03,979][233954] New mean coefficients: [[-0.07337065  0.04415971 -0.01107967  0.12736395  0.06767234 -0.1626816 ]][0m
[37m[1m[2023-07-11 13:58:03,980][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:58:13,052][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 13:58:13,052][233954] FPS: 423375.25[0m
[36m[2023-07-11 13:58:13,055][233954] itr=1023, itrs=2000, Progress: 51.15%[0m
[36m[2023-07-11 13:58:24,877][233954] train() took 11.73 seconds to complete[0m
[36m[2023-07-11 13:58:24,877][233954] FPS: 327393.62[0m
[36m[2023-07-11 13:58:29,170][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:58:29,170][233954] Reward + Measures: [[24.06732208  0.19316365  0.22219332  0.54249603  0.35610035  0.30311748]][0m
[37m[1m[2023-07-11 13:58:29,170][233954] Max Reward on eval: 24.06732208319776[0m
[37m[1m[2023-07-11 13:58:29,171][233954] Min Reward on eval: 24.06732208319776[0m
[37m[1m[2023-07-11 13:58:29,171][233954] Mean Reward across all agents: 24.06732208319776[0m
[37m[1m[2023-07-11 13:58:29,171][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:58:34,144][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:58:34,145][233954] Reward + Measures: [[24.22790097  0.0188      0.06059999  0.465       0.22400001  0.19632111]
 [16.12548774  0.2852      0.3127      0.6049      0.49830005  0.2167182 ]
 [27.98170306  0.20180002  0.2287      0.51789999  0.3752      0.27690125]
 ...
 [19.7895706   0.1974      0.24240001  0.52609998  0.39539999  0.21330836]
 [-3.3044735   0.60830003  0.68570006  0.66779995  0.68360007  0.76548195]
 [28.92607198  0.43840003  0.44549999  0.5165      0.55849999  0.63708258]][0m
[37m[1m[2023-07-11 13:58:34,145][233954] Max Reward on eval: 143.99541915794833[0m
[37m[1m[2023-07-11 13:58:34,145][233954] Min Reward on eval: -120.11163728129468[0m
[37m[1m[2023-07-11 13:58:34,146][233954] Mean Reward across all agents: 12.725438874166043[0m
[37m[1m[2023-07-11 13:58:34,146][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:58:34,149][233954] mean_value=-25.600024992223112, max_value=555.2656713837757[0m
[37m[1m[2023-07-11 13:58:34,152][233954] New mean coefficients: [[-0.05168804  0.05067429 -0.02856128  0.1402154   0.04831028 -0.1521813 ]][0m
[37m[1m[2023-07-11 13:58:34,153][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:58:43,148][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 13:58:43,148][233954] FPS: 426990.16[0m
[36m[2023-07-11 13:58:43,150][233954] itr=1024, itrs=2000, Progress: 51.20%[0m
[36m[2023-07-11 13:58:55,022][233954] train() took 11.77 seconds to complete[0m
[36m[2023-07-11 13:58:55,022][233954] FPS: 326168.99[0m
[36m[2023-07-11 13:58:59,339][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:58:59,339][233954] Reward + Measures: [[22.6998487   0.19701967  0.217241    0.51529664  0.40724999  0.30037984]][0m
[37m[1m[2023-07-11 13:58:59,340][233954] Max Reward on eval: 22.69984869868347[0m
[37m[1m[2023-07-11 13:58:59,340][233954] Min Reward on eval: 22.69984869868347[0m
[37m[1m[2023-07-11 13:58:59,340][233954] Mean Reward across all agents: 22.69984869868347[0m
[37m[1m[2023-07-11 13:58:59,340][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:59:04,386][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:59:04,386][233954] Reward + Measures: [[28.03445808  0.11210001  0.0459      0.45880005  0.4052      0.30724362]
 [32.39036525  0.0252      0.0673      0.40030003  0.23310001  0.23719405]
 [14.2594081   0.37170002  0.4048      0.6688      0.56590003  0.30574772]
 ...
 [25.68601083  0.11040001  0.1568      0.52410001  0.37649998  0.24364953]
 [24.42000781  0.1023      0.12900001  0.48240003  0.37900004  0.23033543]
 [12.08137261  0.28659999  0.3328      0.50319999  0.40419999  0.23451321]][0m
[37m[1m[2023-07-11 13:59:04,386][233954] Max Reward on eval: 137.2612123820698[0m
[37m[1m[2023-07-11 13:59:04,387][233954] Min Reward on eval: -49.00296006352873[0m
[37m[1m[2023-07-11 13:59:04,387][233954] Mean Reward across all agents: 22.082222736561473[0m
[37m[1m[2023-07-11 13:59:04,387][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:59:04,391][233954] mean_value=-20.931195186150497, max_value=524.4096029019216[0m
[37m[1m[2023-07-11 13:59:04,393][233954] New mean coefficients: [[-0.04968011  0.05079145 -0.05805508  0.12266806  0.06511982 -0.08841958]][0m
[37m[1m[2023-07-11 13:59:04,394][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:59:13,465][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 13:59:13,466][233954] FPS: 423403.17[0m
[36m[2023-07-11 13:59:13,468][233954] itr=1025, itrs=2000, Progress: 51.25%[0m
[36m[2023-07-11 13:59:25,242][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 13:59:25,242][233954] FPS: 328795.31[0m
[36m[2023-07-11 13:59:29,523][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:59:29,523][233954] Reward + Measures: [[22.25686824  0.19944535  0.22053067  0.58110768  0.43996835  0.29790559]][0m
[37m[1m[2023-07-11 13:59:29,524][233954] Max Reward on eval: 22.25686824257291[0m
[37m[1m[2023-07-11 13:59:29,524][233954] Min Reward on eval: 22.25686824257291[0m
[37m[1m[2023-07-11 13:59:29,524][233954] Mean Reward across all agents: 22.25686824257291[0m
[37m[1m[2023-07-11 13:59:29,524][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:59:34,500][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:59:34,500][233954] Reward + Measures: [[35.10578991  0.2119      0.15650001  0.60699999  0.44560003  0.39422312]
 [36.86341002  0.0732      0.0787      0.46409997  0.3175      0.36300138]
 [30.08336902  0.1124      0.17420001  0.5104      0.34980002  0.30892575]
 ...
 [17.49918557  0.26820001  0.30870003  0.51719999  0.48329997  0.35782096]
 [24.79158276  0.1062      0.1542      0.58610004  0.41389999  0.20816545]
 [27.2161472   0.1934      0.29320002  0.5327      0.35300002  0.26973268]][0m
[37m[1m[2023-07-11 13:59:34,501][233954] Max Reward on eval: 84.83923542436678[0m
[37m[1m[2023-07-11 13:59:34,501][233954] Min Reward on eval: -34.43073873892426[0m
[37m[1m[2023-07-11 13:59:34,501][233954] Mean Reward across all agents: 21.430685487267787[0m
[37m[1m[2023-07-11 13:59:34,501][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 13:59:34,505][233954] mean_value=-3.4997896342196992, max_value=479.87332690673645[0m
[37m[1m[2023-07-11 13:59:34,508][233954] New mean coefficients: [[-0.01809961  0.04968851 -0.01491084  0.12199059  0.03934433 -0.01369869]][0m
[37m[1m[2023-07-11 13:59:34,509][233954] Moving the mean solution point...[0m
[36m[2023-07-11 13:59:43,566][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 13:59:43,566][233954] FPS: 424058.36[0m
[36m[2023-07-11 13:59:43,568][233954] itr=1026, itrs=2000, Progress: 51.30%[0m
[36m[2023-07-11 13:59:55,530][233954] train() took 11.87 seconds to complete[0m
[36m[2023-07-11 13:59:55,530][233954] FPS: 323633.00[0m
[36m[2023-07-11 13:59:59,903][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 13:59:59,903][233954] Reward + Measures: [[24.90083419  0.209104    0.22442733  0.612082    0.46897399  0.30694643]][0m
[37m[1m[2023-07-11 13:59:59,904][233954] Max Reward on eval: 24.900834193807405[0m
[37m[1m[2023-07-11 13:59:59,904][233954] Min Reward on eval: 24.900834193807405[0m
[37m[1m[2023-07-11 13:59:59,904][233954] Mean Reward across all agents: 24.900834193807405[0m
[37m[1m[2023-07-11 13:59:59,904][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:00:04,914][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:00:04,915][233954] Reward + Measures: [[ 22.38044408   0.1882       0.2086       0.61400002   0.46510002
    0.25220519]
 [ 52.4664197    0.28959998   0.1472       0.54170001   0.4474
    0.44271579]
 [ 93.40034524   0.19680001   0.24460001   0.373        0.40620002
    0.63917947]
 ...
 [ 28.95537911   0.2368       0.28080001   0.4632       0.3233
    0.32764101]
 [104.97391513   0.1127       0.22599998   0.37490001   0.24829999
    0.62798882]
 [ 17.00268203   0.20760003   0.26949999   0.3725       0.2561
    0.37700522]][0m
[37m[1m[2023-07-11 14:00:04,915][233954] Max Reward on eval: 149.69837904055603[0m
[37m[1m[2023-07-11 14:00:04,916][233954] Min Reward on eval: -28.182561793347123[0m
[37m[1m[2023-07-11 14:00:04,916][233954] Mean Reward across all agents: 28.014119690657207[0m
[37m[1m[2023-07-11 14:00:04,916][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:00:04,920][233954] mean_value=-23.525596349311574, max_value=627.5266825872939[0m
[37m[1m[2023-07-11 14:00:04,923][233954] New mean coefficients: [[ 0.03372417  0.06220565 -0.0467066   0.11081158  0.02701571  0.2141402 ]][0m
[37m[1m[2023-07-11 14:00:04,924][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:00:13,905][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 14:00:13,905][233954] FPS: 427645.59[0m
[36m[2023-07-11 14:00:13,908][233954] itr=1027, itrs=2000, Progress: 51.35%[0m
[36m[2023-07-11 14:00:25,543][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 14:00:25,544][233954] FPS: 332739.59[0m
[36m[2023-07-11 14:00:29,833][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:00:29,833][233954] Reward + Measures: [[25.16059961  0.31743202  0.20800532  0.22763534  0.42999667  0.59009194]][0m
[37m[1m[2023-07-11 14:00:29,833][233954] Max Reward on eval: 25.160599611202[0m
[37m[1m[2023-07-11 14:00:29,834][233954] Min Reward on eval: 25.160599611202[0m
[37m[1m[2023-07-11 14:00:29,834][233954] Mean Reward across all agents: 25.160599611202[0m
[37m[1m[2023-07-11 14:00:29,834][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:00:34,831][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:00:34,832][233954] Reward + Measures: [[16.51489221  0.1714      0.0681      0.0654      0.25440001  0.56985611]
 [74.71007649  0.32460004  0.13179998  0.2289      0.44500002  0.63038915]
 [16.25275135  0.51109999  0.39210001  0.40400001  0.47690001  0.54266256]
 ...
 [17.64553462  0.3867      0.30500001  0.29910001  0.47039995  0.54940623]
 [21.84092977  0.1085      0.0551      0.0644      0.2282      0.78408164]
 [17.65076331  0.41999999  0.30860001  0.2895      0.5309      0.53958756]][0m
[37m[1m[2023-07-11 14:00:34,832][233954] Max Reward on eval: 136.9326457085088[0m
[37m[1m[2023-07-11 14:00:34,832][233954] Min Reward on eval: -26.23796373842051[0m
[37m[1m[2023-07-11 14:00:34,833][233954] Mean Reward across all agents: 25.532684968095136[0m
[37m[1m[2023-07-11 14:00:34,833][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:00:34,835][233954] mean_value=-114.10479388826896, max_value=331.71268736201756[0m
[37m[1m[2023-07-11 14:00:34,838][233954] New mean coefficients: [[ 0.02666161  0.0665964  -0.01713042  0.146339    0.07742526  0.36984017]][0m
[37m[1m[2023-07-11 14:00:34,839][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:00:43,866][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 14:00:43,867][233954] FPS: 425425.60[0m
[36m[2023-07-11 14:00:43,869][233954] itr=1028, itrs=2000, Progress: 51.40%[0m
[36m[2023-07-11 14:00:55,576][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 14:00:55,576][233954] FPS: 330761.75[0m
[36m[2023-07-11 14:00:59,865][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:00:59,865][233954] Reward + Measures: [[25.86913561  0.28676534  0.22052999  0.24283834  0.37433133  0.86177862]][0m
[37m[1m[2023-07-11 14:00:59,866][233954] Max Reward on eval: 25.86913560757502[0m
[37m[1m[2023-07-11 14:00:59,866][233954] Min Reward on eval: 25.86913560757502[0m
[37m[1m[2023-07-11 14:00:59,866][233954] Mean Reward across all agents: 25.86913560757502[0m
[37m[1m[2023-07-11 14:00:59,866][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:01:05,109][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:01:05,109][233954] Reward + Measures: [[15.98816141  0.2392      0.2149      0.23539999  0.29230002  0.81950009]
 [57.64678907  0.22770002  0.33340001  0.3184      0.38299999  1.18391705]
 [68.86137241  0.40739998  0.29000002  0.3206      0.49630004  1.06497812]
 ...
 [12.62443761  0.34769997  0.29590002  0.31099999  0.40700004  0.83838254]
 [12.92134168  0.3425      0.31460002  0.28170002  0.43670002  0.85265082]
 [26.225437    0.35780001  0.2976      0.29490003  0.45490003  0.75410688]][0m
[37m[1m[2023-07-11 14:01:05,110][233954] Max Reward on eval: 140.6121960255783[0m
[37m[1m[2023-07-11 14:01:05,110][233954] Min Reward on eval: -61.068788361060435[0m
[37m[1m[2023-07-11 14:01:05,110][233954] Mean Reward across all agents: 30.47708804766006[0m
[37m[1m[2023-07-11 14:01:05,110][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:01:05,112][233954] mean_value=-495.6068659766104, max_value=56.398993266618426[0m
[37m[1m[2023-07-11 14:01:05,115][233954] New mean coefficients: [[ 0.05774548  0.0959651  -0.09030575  0.16619527  0.08999202  0.27175587]][0m
[37m[1m[2023-07-11 14:01:05,116][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:01:14,196][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 14:01:14,197][233954] FPS: 422947.82[0m
[36m[2023-07-11 14:01:14,199][233954] itr=1029, itrs=2000, Progress: 51.45%[0m
[36m[2023-07-11 14:01:25,843][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 14:01:25,844][233954] FPS: 332465.43[0m
[36m[2023-07-11 14:01:30,176][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:01:30,177][233954] Reward + Measures: [[25.70672715  0.28423932  0.20600866  0.24391899  0.34899268  0.94123048]][0m
[37m[1m[2023-07-11 14:01:30,177][233954] Max Reward on eval: 25.706727152374825[0m
[37m[1m[2023-07-11 14:01:30,177][233954] Min Reward on eval: 25.706727152374825[0m
[37m[1m[2023-07-11 14:01:30,177][233954] Mean Reward across all agents: 25.706727152374825[0m
[37m[1m[2023-07-11 14:01:30,178][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:01:35,214][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:01:35,214][233954] Reward + Measures: [[12.40661476  0.32099998  0.2678      0.32190001  0.33000001  1.10221171]
 [32.50825227  0.2728      0.1393      0.22870003  0.37069997  0.88189727]
 [46.87364723  0.32440001  0.18740001  0.2696      0.31209999  0.9992134 ]
 ...
 [19.16631134  0.34489998  0.18449999  0.28009999  0.43060002  0.84424603]
 [19.20540569  0.24539998  0.2062      0.20079999  0.3089      0.88004798]
 [20.12130387  0.16580001  0.12819999  0.13850001  0.32540002  0.93806553]][0m
[37m[1m[2023-07-11 14:01:35,214][233954] Max Reward on eval: 143.13487785874167[0m
[37m[1m[2023-07-11 14:01:35,215][233954] Min Reward on eval: -45.854242546483874[0m
[37m[1m[2023-07-11 14:01:35,215][233954] Mean Reward across all agents: 27.787757820665526[0m
[37m[1m[2023-07-11 14:01:35,215][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:01:35,217][233954] mean_value=-484.269105502875, max_value=348.6953508373708[0m
[37m[1m[2023-07-11 14:01:35,220][233954] New mean coefficients: [[ 0.06065909  0.10013337 -0.07834557  0.1848735   0.13437349  0.32205012]][0m
[37m[1m[2023-07-11 14:01:35,221][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:01:44,280][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 14:01:44,280][233954] FPS: 423945.72[0m
[36m[2023-07-11 14:01:44,283][233954] itr=1030, itrs=2000, Progress: 51.50%[0m
[37m[1m[2023-07-11 14:05:21,130][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001010[0m
[36m[2023-07-11 14:05:33,534][233954] train() took 11.72 seconds to complete[0m
[36m[2023-07-11 14:05:33,534][233954] FPS: 327508.29[0m
[36m[2023-07-11 14:05:37,737][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:05:37,737][233954] Reward + Measures: [[28.16974948  0.29741535  0.211147    0.25630066  0.34678698  1.00402093]][0m
[37m[1m[2023-07-11 14:05:37,738][233954] Max Reward on eval: 28.16974947684544[0m
[37m[1m[2023-07-11 14:05:37,738][233954] Min Reward on eval: 28.16974947684544[0m
[37m[1m[2023-07-11 14:05:37,738][233954] Mean Reward across all agents: 28.16974947684544[0m
[37m[1m[2023-07-11 14:05:37,738][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:05:42,672][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:05:42,678][233954] Reward + Measures: [[22.64236541  0.15810001  0.1102      0.14960001  0.21589999  1.08330524]
 [24.69648839  0.50089997  0.34        0.454       0.49289998  0.96570301]
 [31.47188252  0.4039      0.37399998  0.40990001  0.40550002  1.12738347]
 ...
 [27.69406367  0.4752      0.34489998  0.43809995  0.46680003  1.02834499]
 [47.74900545  0.25639999  0.1168      0.21729998  0.31600001  1.05666828]
 [19.80074632  0.35730001  0.25999999  0.28440002  0.398       1.01499462]][0m
[37m[1m[2023-07-11 14:05:42,678][233954] Max Reward on eval: 163.80008849593577[0m
[37m[1m[2023-07-11 14:05:42,679][233954] Min Reward on eval: -40.9739366971422[0m
[37m[1m[2023-07-11 14:05:42,679][233954] Mean Reward across all agents: 24.8634064170953[0m
[37m[1m[2023-07-11 14:05:42,679][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:05:42,682][233954] mean_value=-315.875354728501, max_value=530.6225278158065[0m
[37m[1m[2023-07-11 14:05:42,684][233954] New mean coefficients: [[ 0.08046533  0.10448818 -0.0600304   0.21571293  0.1405486   0.35088786]][0m
[37m[1m[2023-07-11 14:05:42,685][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:05:51,585][233954] train() took 8.90 seconds to complete[0m
[36m[2023-07-11 14:05:51,585][233954] FPS: 431564.27[0m
[36m[2023-07-11 14:05:51,587][233954] itr=1031, itrs=2000, Progress: 51.55%[0m
[36m[2023-07-11 14:06:03,149][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 14:06:03,149][233954] FPS: 334949.29[0m
[36m[2023-07-11 14:06:07,351][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:06:07,351][233954] Reward + Measures: [[28.64448419  0.29316732  0.21361566  0.25474602  0.33953032  1.05553734]][0m
[37m[1m[2023-07-11 14:06:07,352][233954] Max Reward on eval: 28.64448419246265[0m
[37m[1m[2023-07-11 14:06:07,352][233954] Min Reward on eval: 28.64448419246265[0m
[37m[1m[2023-07-11 14:06:07,352][233954] Mean Reward across all agents: 28.64448419246265[0m
[37m[1m[2023-07-11 14:06:07,353][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:06:12,295][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:06:12,301][233954] Reward + Measures: [[ 26.97537014   0.33580002   0.29460001   0.2931       0.3953
    0.98353618]
 [ 26.04828056   0.2494       0.10880001   0.22149999   0.28579998
    1.0442456 ]
 [ 23.27071958   0.32230002   0.19509999   0.28099999   0.30020002
    0.99586338]
 ...
 [ 11.96520115   0.40170002   0.303        0.36219999   0.36469999
    0.986911  ]
 [-14.49085366   0.67860001   0.54640001   0.60219997   0.55010003
    1.42668092]
 [ 10.17815941   0.27210003   0.2247       0.2221       0.36999997
    1.06992948]][0m
[37m[1m[2023-07-11 14:06:12,301][233954] Max Reward on eval: 109.4952645224752[0m
[37m[1m[2023-07-11 14:06:12,301][233954] Min Reward on eval: -82.80647812790703[0m
[37m[1m[2023-07-11 14:06:12,302][233954] Mean Reward across all agents: 20.28028893474302[0m
[37m[1m[2023-07-11 14:06:12,302][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:06:12,304][233954] mean_value=-272.83364058965856, max_value=399.07887992057437[0m
[37m[1m[2023-07-11 14:06:12,307][233954] New mean coefficients: [[ 0.0896244   0.11810075 -0.01732758  0.27990645  0.14552449  0.3663543 ]][0m
[37m[1m[2023-07-11 14:06:12,308][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:06:21,232][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 14:06:21,232][233954] FPS: 430386.60[0m
[36m[2023-07-11 14:06:21,234][233954] itr=1032, itrs=2000, Progress: 51.60%[0m
[36m[2023-07-11 14:06:33,078][233954] train() took 11.75 seconds to complete[0m
[36m[2023-07-11 14:06:33,079][233954] FPS: 326763.01[0m
[36m[2023-07-11 14:06:37,260][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:06:37,260][233954] Reward + Measures: [[33.67039773  0.292723    0.21623734  0.26002902  0.33451766  1.09436381]][0m
[37m[1m[2023-07-11 14:06:37,260][233954] Max Reward on eval: 33.67039772900386[0m
[37m[1m[2023-07-11 14:06:37,260][233954] Min Reward on eval: 33.67039772900386[0m
[37m[1m[2023-07-11 14:06:37,261][233954] Mean Reward across all agents: 33.67039772900386[0m
[37m[1m[2023-07-11 14:06:37,261][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:06:42,468][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:06:42,469][233954] Reward + Measures: [[15.56442043  0.41400003  0.39910004  0.4059      0.44449997  1.00532138]
 [17.289495    0.23889999  0.1418      0.22119999  0.2969      1.19342887]
 [16.1641679   0.33370003  0.1979      0.2825      0.37939999  1.07506979]
 ...
 [19.78111783  0.39210001  0.28480002  0.38710001  0.419       1.16223657]
 [32.97304619  0.2376      0.1904      0.2077      0.33899999  1.08868349]
 [37.86562393  0.25229999  0.20349999  0.20850001  0.30049998  1.11380041]][0m
[37m[1m[2023-07-11 14:06:42,469][233954] Max Reward on eval: 138.00553298266604[0m
[37m[1m[2023-07-11 14:06:42,469][233954] Min Reward on eval: -14.509024081908866[0m
[37m[1m[2023-07-11 14:06:42,469][233954] Mean Reward across all agents: 38.182665343313914[0m
[37m[1m[2023-07-11 14:06:42,470][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:06:42,472][233954] mean_value=-630.8035372557875, max_value=242.998824714783[0m
[37m[1m[2023-07-11 14:06:42,474][233954] New mean coefficients: [[0.08849087 0.11334166 0.00417993 0.28025204 0.15802673 0.41733506]][0m
[37m[1m[2023-07-11 14:06:42,475][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:06:51,366][233954] train() took 8.89 seconds to complete[0m
[36m[2023-07-11 14:06:51,366][233954] FPS: 431991.03[0m
[36m[2023-07-11 14:06:51,368][233954] itr=1033, itrs=2000, Progress: 51.65%[0m
[36m[2023-07-11 14:07:02,925][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 14:07:02,925][233954] FPS: 335014.44[0m
[36m[2023-07-11 14:07:07,189][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:07:07,189][233954] Reward + Measures: [[37.20653477  0.29562432  0.21936266  0.26864401  0.338274    1.14030349]][0m
[37m[1m[2023-07-11 14:07:07,189][233954] Max Reward on eval: 37.20653476882686[0m
[37m[1m[2023-07-11 14:07:07,190][233954] Min Reward on eval: 37.20653476882686[0m
[37m[1m[2023-07-11 14:07:07,190][233954] Mean Reward across all agents: 37.20653476882686[0m
[37m[1m[2023-07-11 14:07:07,190][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:07:12,192][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:07:12,193][233954] Reward + Measures: [[ 36.54819144   0.41739997   0.33419999   0.37900001   0.39739999
    1.39649284]
 [ 29.6804367    0.15689999   0.1096       0.12720001   0.21170001
    1.10553241]
 [ 18.82105423   0.39900002   0.33339998   0.3486       0.34690002
    1.12013614]
 ...
 [ 28.70297494   0.39030001   0.26630002   0.32510003   0.37920004
    1.11905444]
 [-13.47995706   0.42630002   0.38429999   0.37379998   0.3527
    1.13328469]
 [ 31.51619757   0.46149999   0.31009999   0.42510006   0.36050001
    1.11942565]][0m
[37m[1m[2023-07-11 14:07:12,193][233954] Max Reward on eval: 152.4665023435373[0m
[37m[1m[2023-07-11 14:07:12,193][233954] Min Reward on eval: -29.85384048051201[0m
[37m[1m[2023-07-11 14:07:12,194][233954] Mean Reward across all agents: 34.24651955468624[0m
[37m[1m[2023-07-11 14:07:12,194][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:07:12,196][233954] mean_value=-724.5989945410417, max_value=312.51672215165485[0m
[37m[1m[2023-07-11 14:07:12,198][233954] New mean coefficients: [[0.10189056 0.10037528 0.00497306 0.25098032 0.14582482 0.40235093]][0m
[37m[1m[2023-07-11 14:07:12,199][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:07:21,264][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 14:07:21,264][233954] FPS: 423693.73[0m
[36m[2023-07-11 14:07:21,267][233954] itr=1034, itrs=2000, Progress: 51.70%[0m
[36m[2023-07-11 14:07:33,051][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 14:07:33,052][233954] FPS: 328544.37[0m
[36m[2023-07-11 14:07:37,239][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:07:37,245][233954] Reward + Measures: [[43.23571464  0.28971165  0.22144935  0.269005    0.31690165  1.20746887]][0m
[37m[1m[2023-07-11 14:07:37,245][233954] Max Reward on eval: 43.235714639113134[0m
[37m[1m[2023-07-11 14:07:37,245][233954] Min Reward on eval: 43.235714639113134[0m
[37m[1m[2023-07-11 14:07:37,246][233954] Mean Reward across all agents: 43.235714639113134[0m
[37m[1m[2023-07-11 14:07:37,246][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:07:42,222][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:07:42,222][233954] Reward + Measures: [[24.63263484  0.36059999  0.1969      0.32600001  0.35390002  1.24953401]
 [53.91767229  0.44409999  0.31980002  0.42180005  0.43880001  1.29854989]
 [41.84530526  0.30309999  0.24430001  0.25990003  0.3558      1.15942276]
 ...
 [75.16113441  0.23559999  0.29260001  0.29210001  0.35230002  1.13289702]
 [14.96768339  0.4021      0.26610002  0.35720003  0.3757      1.10551059]
 [52.72495582  0.16140001  0.13340001  0.15020001  0.21570002  1.21577203]][0m
[37m[1m[2023-07-11 14:07:42,222][233954] Max Reward on eval: 164.6012961528264[0m
[37m[1m[2023-07-11 14:07:42,223][233954] Min Reward on eval: -52.64479942835169[0m
[37m[1m[2023-07-11 14:07:42,223][233954] Mean Reward across all agents: 41.05959443060795[0m
[37m[1m[2023-07-11 14:07:42,223][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:07:42,225][233954] mean_value=-502.39365249655367, max_value=360.7250918298955[0m
[37m[1m[2023-07-11 14:07:42,228][233954] New mean coefficients: [[0.06288233 0.11266785 0.0052114  0.27431095 0.14969851 0.305952  ]][0m
[37m[1m[2023-07-11 14:07:42,229][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:07:51,206][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 14:07:51,206][233954] FPS: 427841.11[0m
[36m[2023-07-11 14:07:51,208][233954] itr=1035, itrs=2000, Progress: 51.75%[0m
[36m[2023-07-11 14:08:02,853][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 14:08:02,854][233954] FPS: 332586.62[0m
[36m[2023-07-11 14:08:07,083][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:08:07,083][233954] Reward + Measures: [[51.06154671  0.30780298  0.25993368  0.29805502  0.34653068  1.25458562]][0m
[37m[1m[2023-07-11 14:08:07,083][233954] Max Reward on eval: 51.0615467085366[0m
[37m[1m[2023-07-11 14:08:07,083][233954] Min Reward on eval: 51.0615467085366[0m
[37m[1m[2023-07-11 14:08:07,084][233954] Mean Reward across all agents: 51.0615467085366[0m
[37m[1m[2023-07-11 14:08:07,084][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:08:12,037][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:08:12,038][233954] Reward + Measures: [[ 37.69698643   0.3197       0.23400001   0.31570002   0.37080002
    1.31857598]
 [ 43.05238898   0.31200001   0.27570003   0.29409999   0.35180002
    1.180179  ]
 [ 96.71839442   0.3028       0.3504       0.35930002   0.40260002
    1.24645841]
 ...
 [ 84.70518249   0.39369997   0.44340006   0.4542       0.52030009
    1.42501521]
 [103.34216405   0.32720003   0.20840001   0.3026       0.37439999
    1.29610562]
 [ 93.55501956   0.22650002   0.29720002   0.29159999   0.35499999
    1.33099771]][0m
[37m[1m[2023-07-11 14:08:12,038][233954] Max Reward on eval: 275.34435644592156[0m
[37m[1m[2023-07-11 14:08:12,038][233954] Min Reward on eval: -48.55769031518139[0m
[37m[1m[2023-07-11 14:08:12,038][233954] Mean Reward across all agents: 58.24670503069757[0m
[37m[1m[2023-07-11 14:08:12,039][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:08:12,041][233954] mean_value=-596.3683987438648, max_value=409.5215233658099[0m
[37m[1m[2023-07-11 14:08:12,044][233954] New mean coefficients: [[ 0.07068627  0.11652633 -0.03116762  0.30808187  0.1676992   0.21019652]][0m
[37m[1m[2023-07-11 14:08:12,045][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:08:21,051][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 14:08:21,051][233954] FPS: 426438.83[0m
[36m[2023-07-11 14:08:21,054][233954] itr=1036, itrs=2000, Progress: 51.80%[0m
[36m[2023-07-11 14:08:32,679][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 14:08:32,679][233954] FPS: 333133.79[0m
[36m[2023-07-11 14:08:36,967][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:08:36,967][233954] Reward + Measures: [[63.46282264  0.32253298  0.29967499  0.32690299  0.38638631  1.2887485 ]][0m
[37m[1m[2023-07-11 14:08:36,967][233954] Max Reward on eval: 63.46282263971415[0m
[37m[1m[2023-07-11 14:08:36,968][233954] Min Reward on eval: 63.46282263971415[0m
[37m[1m[2023-07-11 14:08:36,968][233954] Mean Reward across all agents: 63.46282263971415[0m
[37m[1m[2023-07-11 14:08:36,968][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:08:41,956][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:08:41,957][233954] Reward + Measures: [[24.34665975  0.55129999  0.50060004  0.51859999  0.52069998  1.15630686]
 [21.00290197  0.47049999  0.35390002  0.44379997  0.4535      1.14150298]
 [34.16888275  0.56269997  0.48409995  0.50910002  0.50780004  1.14770114]
 ...
 [71.23364053  0.46070001  0.43020001  0.44010001  0.46109995  1.23069692]
 [16.37004927  0.45269999  0.4375      0.4384      0.44759998  1.22516334]
 [34.57074488  0.54690003  0.45559999  0.50690001  0.47130004  1.17563522]][0m
[37m[1m[2023-07-11 14:08:41,957][233954] Max Reward on eval: 163.0411911101779[0m
[37m[1m[2023-07-11 14:08:41,957][233954] Min Reward on eval: -39.56089899064973[0m
[37m[1m[2023-07-11 14:08:41,957][233954] Mean Reward across all agents: 49.71725543410778[0m
[37m[1m[2023-07-11 14:08:41,958][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:08:41,960][233954] mean_value=-437.01337333843674, max_value=196.49170182214576[0m
[37m[1m[2023-07-11 14:08:41,963][233954] New mean coefficients: [[ 0.07586851  0.12767231 -0.0100661   0.30803004  0.16536683  0.19452818]][0m
[37m[1m[2023-07-11 14:08:41,964][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:08:50,960][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 14:08:50,960][233954] FPS: 426918.71[0m
[36m[2023-07-11 14:08:50,962][233954] itr=1037, itrs=2000, Progress: 51.85%[0m
[36m[2023-07-11 14:09:02,760][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 14:09:02,760][233954] FPS: 328175.21[0m
[36m[2023-07-11 14:09:07,044][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:09:07,044][233954] Reward + Measures: [[75.82112942  0.31973299  0.320862    0.33886799  0.403191    1.33935475]][0m
[37m[1m[2023-07-11 14:09:07,045][233954] Max Reward on eval: 75.82112941614946[0m
[37m[1m[2023-07-11 14:09:07,045][233954] Min Reward on eval: 75.82112941614946[0m
[37m[1m[2023-07-11 14:09:07,045][233954] Mean Reward across all agents: 75.82112941614946[0m
[37m[1m[2023-07-11 14:09:07,045][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:09:12,048][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:09:12,054][233954] Reward + Measures: [[ 16.72141116   0.31420001   0.29200003   0.2999       0.34979999
    1.28158033]
 [117.73415828   0.22230001   0.2235       0.30249998   0.38320002
    1.39143884]
 [ 13.85032666   0.2402       0.2349       0.24010001   0.32220003
    1.34929085]
 ...
 [ 49.91337071   0.3391       0.24609999   0.30250001   0.398
    1.367419  ]
 [ 93.08498425   0.46900001   0.54689997   0.52240002   0.58249998
    1.20233989]
 [ 63.33669519   0.34250003   0.30279997   0.34559998   0.49149999
    1.21729827]][0m
[37m[1m[2023-07-11 14:09:12,054][233954] Max Reward on eval: 276.17382273441183[0m
[37m[1m[2023-07-11 14:09:12,055][233954] Min Reward on eval: -14.363093077286612[0m
[37m[1m[2023-07-11 14:09:12,055][233954] Mean Reward across all agents: 72.29955346684879[0m
[37m[1m[2023-07-11 14:09:12,055][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:09:12,058][233954] mean_value=-482.9953480998035, max_value=78.96786026665028[0m
[37m[1m[2023-07-11 14:09:12,060][233954] New mean coefficients: [[0.07671729 0.12955557 0.00703578 0.32689598 0.1640433  0.06552534]][0m
[37m[1m[2023-07-11 14:09:12,061][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:09:21,044][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 14:09:21,044][233954] FPS: 427557.82[0m
[36m[2023-07-11 14:09:21,047][233954] itr=1038, itrs=2000, Progress: 51.90%[0m
[36m[2023-07-11 14:09:32,706][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 14:09:32,706][233954] FPS: 332141.24[0m
[36m[2023-07-11 14:09:37,055][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:09:37,055][233954] Reward + Measures: [[82.98627317  0.32198599  0.3462193   0.35470229  0.43065763  1.36238348]][0m
[37m[1m[2023-07-11 14:09:37,055][233954] Max Reward on eval: 82.98627317331277[0m
[37m[1m[2023-07-11 14:09:37,056][233954] Min Reward on eval: 82.98627317331277[0m
[37m[1m[2023-07-11 14:09:37,056][233954] Mean Reward across all agents: 82.98627317331277[0m
[37m[1m[2023-07-11 14:09:37,056][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:09:42,301][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:09:42,307][233954] Reward + Measures: [[ 37.0965986    0.1569       0.1454       0.1697       0.21160002
    1.23044419]
 [102.08725129   0.31         0.40290004   0.37509999   0.46190006
    1.48101699]
 [ 76.16323091   0.23380001   0.3089       0.31500003   0.39229998
    1.48294568]
 ...
 [123.29502247   0.32079998   0.31599998   0.37580001   0.45070001
    1.42361712]
 [163.1164899    0.27900001   0.4941       0.45699999   0.51960003
    1.54704273]
 [118.37162338   0.2543       0.3319       0.3091       0.3973
    1.39679658]][0m
[37m[1m[2023-07-11 14:09:42,308][233954] Max Reward on eval: 360.64535439827597[0m
[37m[1m[2023-07-11 14:09:42,308][233954] Min Reward on eval: -12.111010635219282[0m
[37m[1m[2023-07-11 14:09:42,308][233954] Mean Reward across all agents: 113.7173625803646[0m
[37m[1m[2023-07-11 14:09:42,308][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:09:42,312][233954] mean_value=-222.33725347335985, max_value=716.1079889660748[0m
[37m[1m[2023-07-11 14:09:42,315][233954] New mean coefficients: [[ 0.09730528  0.12505966 -0.00238579  0.36919463  0.19929467  0.01865618]][0m
[37m[1m[2023-07-11 14:09:42,316][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:09:51,327][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 14:09:51,328][233954] FPS: 426189.34[0m
[36m[2023-07-11 14:09:51,330][233954] itr=1039, itrs=2000, Progress: 51.95%[0m
[36m[2023-07-11 14:10:03,189][233954] train() took 11.76 seconds to complete[0m
[36m[2023-07-11 14:10:03,189][233954] FPS: 326406.14[0m
[36m[2023-07-11 14:10:07,418][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:10:07,419][233954] Reward + Measures: [[103.33837406   0.32194099   0.37865731   0.37865666   0.461335
    1.38635969]][0m
[37m[1m[2023-07-11 14:10:07,419][233954] Max Reward on eval: 103.3383740562148[0m
[37m[1m[2023-07-11 14:10:07,419][233954] Min Reward on eval: 103.3383740562148[0m
[37m[1m[2023-07-11 14:10:07,419][233954] Mean Reward across all agents: 103.3383740562148[0m
[37m[1m[2023-07-11 14:10:07,420][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:10:12,430][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:10:12,436][233954] Reward + Measures: [[ 85.5313015    0.23870002   0.139        0.23580003   0.31620002
    1.42735851]
 [196.00653633   0.14240001   0.41160002   0.32390001   0.48480001
    1.5051614 ]
 [178.43548617   0.38610002   0.5898       0.54030001   0.64360005
    1.42279375]
 ...
 [155.37251539   0.3053       0.40580001   0.43590003   0.55660003
    1.40150106]
 [ 66.54384668   0.13079999   0.23519997   0.20630001   0.33950001
    1.59782732]
 [144.03426987   0.24159999   0.39710003   0.36920002   0.4896
    1.49197352]][0m
[37m[1m[2023-07-11 14:10:12,436][233954] Max Reward on eval: 331.73751995489[0m
[37m[1m[2023-07-11 14:10:12,436][233954] Min Reward on eval: -27.338919216336215[0m
[37m[1m[2023-07-11 14:10:12,437][233954] Mean Reward across all agents: 109.39910474625518[0m
[37m[1m[2023-07-11 14:10:12,437][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:10:12,441][233954] mean_value=-338.38869691458825, max_value=617.7951726138626[0m
[37m[1m[2023-07-11 14:10:12,444][233954] New mean coefficients: [[0.10905418 0.1130595  0.11660587 0.40047455 0.25333622 0.06832629]][0m
[37m[1m[2023-07-11 14:10:12,444][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:10:21,378][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 14:10:21,378][233954] FPS: 429930.17[0m
[36m[2023-07-11 14:10:21,380][233954] itr=1040, itrs=2000, Progress: 52.00%[0m
[37m[1m[2023-07-11 14:13:53,384][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001020[0m
[36m[2023-07-11 14:14:05,827][233954] train() took 11.90 seconds to complete[0m
[36m[2023-07-11 14:14:05,827][233954] FPS: 322776.03[0m
[36m[2023-07-11 14:14:10,026][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:14:10,027][233954] Reward + Measures: [[126.52122522   0.33129466   0.43708795   0.42526901   0.51482171
    1.43319559]][0m
[37m[1m[2023-07-11 14:14:10,027][233954] Max Reward on eval: 126.52122521873879[0m
[37m[1m[2023-07-11 14:14:10,027][233954] Min Reward on eval: 126.52122521873879[0m
[37m[1m[2023-07-11 14:14:10,028][233954] Mean Reward across all agents: 126.52122521873879[0m
[37m[1m[2023-07-11 14:14:10,028][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:14:14,956][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:14:14,957][233954] Reward + Measures: [[ 37.50551035   0.72380006   0.7615       0.71210003   0.76500005
    1.29597366]
 [ 34.8771243    0.55840003   0.60299999   0.56039995   0.61840004
    1.23969853]
 [ 13.69443109   0.67410004   0.76609999   0.62989998   0.75599998
    1.29763293]
 ...
 [188.95775654   0.22350001   0.54750001   0.43770003   0.56770003
    1.59812617]
 [ 49.11280531   0.33399999   0.31579998   0.32170004   0.39920002
    1.10136878]
 [ 21.08531951   0.70679992   0.73850006   0.67540008   0.73850006
    1.04581726]][0m
[37m[1m[2023-07-11 14:14:14,957][233954] Max Reward on eval: 236.59826864432543[0m
[37m[1m[2023-07-11 14:14:14,958][233954] Min Reward on eval: -174.5105495397933[0m
[37m[1m[2023-07-11 14:14:14,958][233954] Mean Reward across all agents: 48.21295847166114[0m
[37m[1m[2023-07-11 14:14:14,958][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:14:14,961][233954] mean_value=-250.1469903580919, max_value=246.48314644795616[0m
[37m[1m[2023-07-11 14:14:14,963][233954] New mean coefficients: [[ 0.0993656   0.11695111  0.09971927  0.38428017  0.25168723 -0.04010998]][0m
[37m[1m[2023-07-11 14:14:14,964][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:14:23,913][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 14:14:23,914][233954] FPS: 429166.20[0m
[36m[2023-07-11 14:14:23,916][233954] itr=1041, itrs=2000, Progress: 52.05%[0m
[36m[2023-07-11 14:14:35,591][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 14:14:35,591][233954] FPS: 331606.52[0m
[36m[2023-07-11 14:14:39,805][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:14:39,805][233954] Reward + Measures: [[140.56423872   0.31063366   0.45427799   0.43333668   0.53178501
    1.45989096]][0m
[37m[1m[2023-07-11 14:14:39,805][233954] Max Reward on eval: 140.56423872435212[0m
[37m[1m[2023-07-11 14:14:39,805][233954] Min Reward on eval: 140.56423872435212[0m
[37m[1m[2023-07-11 14:14:39,806][233954] Mean Reward across all agents: 140.56423872435212[0m
[37m[1m[2023-07-11 14:14:39,806][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:14:44,795][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:14:44,795][233954] Reward + Measures: [[134.0362444    0.2246       0.30949998   0.29629999   0.43940002
    1.6294291 ]
 [-32.17277178   0.39790002   0.40949997   0.32260001   0.45210001
    1.36536157]
 [ 86.28742111   0.1012       0.16000001   0.14110002   0.25619999
    1.57882595]
 ...
 [ 58.82636758   0.33239999   0.3337       0.32300001   0.41060001
    1.3270036 ]
 [ 49.44370463   0.47930002   0.50819999   0.47760001   0.53909999
    1.2745589 ]
 [ 79.35946174   0.4285       0.39480001   0.44220001   0.52720004
    1.52736032]][0m
[37m[1m[2023-07-11 14:14:44,795][233954] Max Reward on eval: 275.1167577153072[0m
[37m[1m[2023-07-11 14:14:44,796][233954] Min Reward on eval: -46.195793315954504[0m
[37m[1m[2023-07-11 14:14:44,796][233954] Mean Reward across all agents: 95.25449709930903[0m
[37m[1m[2023-07-11 14:14:44,796][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:14:44,799][233954] mean_value=-287.37840942775864, max_value=568.207931454074[0m
[37m[1m[2023-07-11 14:14:44,802][233954] New mean coefficients: [[ 0.09562316  0.12068328  0.05759148  0.35601604  0.23177855 -0.08265245]][0m
[37m[1m[2023-07-11 14:14:44,803][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:14:53,716][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 14:14:53,716][233954] FPS: 430897.21[0m
[36m[2023-07-11 14:14:53,719][233954] itr=1042, itrs=2000, Progress: 52.10%[0m
[36m[2023-07-11 14:15:05,275][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 14:15:05,275][233954] FPS: 335040.55[0m
[36m[2023-07-11 14:15:09,562][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:15:09,563][233954] Reward + Measures: [[158.98180514   0.30806267   0.49535266   0.45990235   0.56727529
    1.46565521]][0m
[37m[1m[2023-07-11 14:15:09,563][233954] Max Reward on eval: 158.98180513522178[0m
[37m[1m[2023-07-11 14:15:09,563][233954] Min Reward on eval: 158.98180513522178[0m
[37m[1m[2023-07-11 14:15:09,563][233954] Mean Reward across all agents: 158.98180513522178[0m
[37m[1m[2023-07-11 14:15:09,564][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:15:14,752][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:15:14,753][233954] Reward + Measures: [[ 40.43511449   0.26320001   0.34619999   0.26940003   0.41630003
    1.37864971]
 [323.479774     0.35890001   0.86100006   0.74060005   0.86429995
    1.64254653]
 [174.40229713   0.17270002   0.41799998   0.34980002   0.45860001
    1.51998222]
 ...
 [217.14603258   0.43959999   0.59840006   0.62160003   0.70950001
    1.5652957 ]
 [101.96737019   0.2807       0.1847       0.27779999   0.34849998
    1.32342148]
 [253.11234047   0.13349999   0.37250003   0.3335       0.47540003
    1.63950825]][0m
[37m[1m[2023-07-11 14:15:14,753][233954] Max Reward on eval: 385.9112240806513[0m
[37m[1m[2023-07-11 14:15:14,754][233954] Min Reward on eval: -11.655322053236887[0m
[37m[1m[2023-07-11 14:15:14,754][233954] Mean Reward across all agents: 135.78810486733425[0m
[37m[1m[2023-07-11 14:15:14,754][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:15:14,759][233954] mean_value=-295.67489601803766, max_value=686.7701462289428[0m
[37m[1m[2023-07-11 14:15:14,762][233954] New mean coefficients: [[ 0.03861671  0.08580954  0.04098283  0.31603226  0.17705135 -0.09028558]][0m
[37m[1m[2023-07-11 14:15:14,763][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:15:23,776][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 14:15:23,776][233954] FPS: 426107.05[0m
[36m[2023-07-11 14:15:23,778][233954] itr=1043, itrs=2000, Progress: 52.15%[0m
[36m[2023-07-11 14:15:35,788][233954] train() took 11.92 seconds to complete[0m
[36m[2023-07-11 14:15:35,788][233954] FPS: 322260.52[0m
[36m[2023-07-11 14:15:39,980][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:15:39,980][233954] Reward + Measures: [[166.44021188   0.34648731   0.54738772   0.51148999   0.61693299
    1.43384373]][0m
[37m[1m[2023-07-11 14:15:39,980][233954] Max Reward on eval: 166.44021188186966[0m
[37m[1m[2023-07-11 14:15:39,981][233954] Min Reward on eval: 166.44021188186966[0m
[37m[1m[2023-07-11 14:15:39,981][233954] Mean Reward across all agents: 166.44021188186966[0m
[37m[1m[2023-07-11 14:15:39,981][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:15:44,941][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:15:44,941][233954] Reward + Measures: [[380.40260696   0.10380001   0.7051       0.56580001   0.73610002
    1.94398427]
 [105.41797558   0.39480004   0.49180004   0.47850004   0.59470004
    1.35407221]
 [134.29438553   0.1815       0.38519999   0.31469998   0.47079998
    1.46603775]
 ...
 [194.37215464   0.22750001   0.51879996   0.45580003   0.57160002
    1.54002118]
 [216.0865614    0.29630002   0.62110007   0.53460002   0.66340005
    1.59956646]
 [215.01082094   0.29449999   0.51080006   0.52750003   0.66499996
    1.54808581]][0m
[37m[1m[2023-07-11 14:15:44,942][233954] Max Reward on eval: 431.16307618292046[0m
[37m[1m[2023-07-11 14:15:44,942][233954] Min Reward on eval: 11.801406814699295[0m
[37m[1m[2023-07-11 14:15:44,942][233954] Mean Reward across all agents: 197.2037142184254[0m
[37m[1m[2023-07-11 14:15:44,942][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:15:44,947][233954] mean_value=-120.12573655249084, max_value=695.4930646458722[0m
[37m[1m[2023-07-11 14:15:44,950][233954] New mean coefficients: [[ 0.05626785  0.06470829  0.06028634  0.3280161   0.19058143 -0.07646011]][0m
[37m[1m[2023-07-11 14:15:44,951][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:15:53,931][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 14:15:53,931][233954] FPS: 427699.03[0m
[36m[2023-07-11 14:15:53,933][233954] itr=1044, itrs=2000, Progress: 52.20%[0m
[36m[2023-07-11 14:16:05,713][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 14:16:05,713][233954] FPS: 328652.17[0m
[36m[2023-07-11 14:16:09,967][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:16:09,967][233954] Reward + Measures: [[179.68074342   0.335309     0.56793863   0.52201968   0.63321632
    1.44699204]][0m
[37m[1m[2023-07-11 14:16:09,968][233954] Max Reward on eval: 179.6807434189253[0m
[37m[1m[2023-07-11 14:16:09,968][233954] Min Reward on eval: 179.6807434189253[0m
[37m[1m[2023-07-11 14:16:09,968][233954] Mean Reward across all agents: 179.6807434189253[0m
[37m[1m[2023-07-11 14:16:09,968][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:16:14,986][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:16:14,987][233954] Reward + Measures: [[270.51234305   0.22630003   0.43340001   0.45390001   0.58340001
    1.59874856]
 [111.132202     0.16069999   0.33239996   0.28309998   0.42319998
    1.42716014]
 [198.92318376   0.47930002   0.69600004   0.69710004   0.79450005
    1.39955389]
 ...
 [-20.3840997    0.39990002   0.60950005   0.40180001   0.60739994
    1.47670949]
 [198.21162196   0.20780002   0.53030008   0.47889996   0.55599999
    1.53081191]
 [300.69504961   0.2093       0.50959998   0.53080004   0.6649
    1.76287079]][0m
[37m[1m[2023-07-11 14:16:14,987][233954] Max Reward on eval: 346.99877685306126[0m
[37m[1m[2023-07-11 14:16:14,988][233954] Min Reward on eval: -50.40980979553424[0m
[37m[1m[2023-07-11 14:16:14,988][233954] Mean Reward across all agents: 139.85728371297566[0m
[37m[1m[2023-07-11 14:16:14,988][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:16:14,993][233954] mean_value=-113.91074210465463, max_value=438.493195550568[0m
[37m[1m[2023-07-11 14:16:14,996][233954] New mean coefficients: [[0.04885402 0.04036148 0.08465262 0.35071135 0.2150946  0.08250324]][0m
[37m[1m[2023-07-11 14:16:14,997][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:16:23,990][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 14:16:23,991][233954] FPS: 427040.50[0m
[36m[2023-07-11 14:16:23,993][233954] itr=1045, itrs=2000, Progress: 52.25%[0m
[36m[2023-07-11 14:16:35,533][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 14:16:35,533][233954] FPS: 335549.66[0m
[36m[2023-07-11 14:16:39,824][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:16:39,825][233954] Reward + Measures: [[188.45640391   0.33439368   0.58190203   0.52896535   0.646981
    1.47901213]][0m
[37m[1m[2023-07-11 14:16:39,825][233954] Max Reward on eval: 188.45640390868806[0m
[37m[1m[2023-07-11 14:16:39,825][233954] Min Reward on eval: 188.45640390868806[0m
[37m[1m[2023-07-11 14:16:39,826][233954] Mean Reward across all agents: 188.45640390868806[0m
[37m[1m[2023-07-11 14:16:39,826][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:16:44,757][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:16:44,758][233954] Reward + Measures: [[174.9045279    0.32769999   0.51389998   0.47209999   0.57969999
    1.38718748]
 [ 32.98722508   0.4752       0.49070001   0.47670004   0.54380006
    1.22984254]
 [143.85279657   0.47979999   0.69270003   0.63240004   0.69440001
    1.35657537]
 ...
 [ 94.89223845   0.3554       0.4585       0.40200001   0.52890009
    1.47322845]
 [286.01350687   0.21270001   0.61730003   0.52360004   0.64609998
    1.66360986]
 [188.10478613   0.21640001   0.42339998   0.38610002   0.51140004
    1.54792702]][0m
[37m[1m[2023-07-11 14:16:44,758][233954] Max Reward on eval: 469.0009617721662[0m
[37m[1m[2023-07-11 14:16:44,758][233954] Min Reward on eval: 15.438673359144014[0m
[37m[1m[2023-07-11 14:16:44,758][233954] Mean Reward across all agents: 202.08366622655973[0m
[37m[1m[2023-07-11 14:16:44,758][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:16:44,763][233954] mean_value=-35.55931140419136, max_value=584.5595824834189[0m
[37m[1m[2023-07-11 14:16:44,766][233954] New mean coefficients: [[0.03808919 0.04015042 0.05579958 0.34096324 0.219199   0.01564731]][0m
[37m[1m[2023-07-11 14:16:44,767][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:16:53,704][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 14:16:53,705][233954] FPS: 429742.47[0m
[36m[2023-07-11 14:16:53,707][233954] itr=1046, itrs=2000, Progress: 52.30%[0m
[36m[2023-07-11 14:17:05,353][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 14:17:05,354][233954] FPS: 332443.93[0m
[36m[2023-07-11 14:17:09,653][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:17:09,654][233954] Reward + Measures: [[197.90516188   0.36942968   0.64255565   0.58787835   0.6986987
    1.47398388]][0m
[37m[1m[2023-07-11 14:17:09,654][233954] Max Reward on eval: 197.9051618839465[0m
[37m[1m[2023-07-11 14:17:09,654][233954] Min Reward on eval: 197.9051618839465[0m
[37m[1m[2023-07-11 14:17:09,655][233954] Mean Reward across all agents: 197.9051618839465[0m
[37m[1m[2023-07-11 14:17:09,655][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:17:14,665][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:17:14,666][233954] Reward + Measures: [[127.37576702   0.39999998   0.41440001   0.47480002   0.59180003
    1.45198059]
 [109.2888309    0.47849998   0.59020007   0.55970001   0.63650006
    1.29400718]
 [169.23113684   0.46869999   0.58840001   0.6189       0.70410007
    1.47974229]
 ...
 [321.9970895    0.11980001   0.52020001   0.45510003   0.597
    1.7087965 ]
 [230.26954858   0.38420001   0.69080007   0.62410003   0.73660004
    1.47438371]
 [394.20451961   0.27239999   0.61840004   0.68589997   0.79970002
    1.89716113]][0m
[37m[1m[2023-07-11 14:17:14,666][233954] Max Reward on eval: 475.858634947869[0m
[37m[1m[2023-07-11 14:17:14,666][233954] Min Reward on eval: -2.521979413717054[0m
[37m[1m[2023-07-11 14:17:14,667][233954] Mean Reward across all agents: 176.94927302970217[0m
[37m[1m[2023-07-11 14:17:14,667][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:17:14,671][233954] mean_value=-32.695753622613715, max_value=551.099864964048[0m
[37m[1m[2023-07-11 14:17:14,674][233954] New mean coefficients: [[0.02767129 0.05639101 0.08625019 0.31503093 0.22151114 0.19505505]][0m
[37m[1m[2023-07-11 14:17:14,675][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:17:23,769][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 14:17:23,769][233954] FPS: 422336.34[0m
[36m[2023-07-11 14:17:23,772][233954] itr=1047, itrs=2000, Progress: 52.35%[0m
[36m[2023-07-11 14:17:35,981][233954] train() took 12.11 seconds to complete[0m
[36m[2023-07-11 14:17:35,981][233954] FPS: 317040.18[0m
[36m[2023-07-11 14:17:40,289][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:17:40,289][233954] Reward + Measures: [[202.42043447   0.36197001   0.6402033    0.58550835   0.69584197
    1.5268898 ]][0m
[37m[1m[2023-07-11 14:17:40,289][233954] Max Reward on eval: 202.4204344731612[0m
[37m[1m[2023-07-11 14:17:40,289][233954] Min Reward on eval: 202.4204344731612[0m
[37m[1m[2023-07-11 14:17:40,290][233954] Mean Reward across all agents: 202.4204344731612[0m
[37m[1m[2023-07-11 14:17:40,290][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:17:45,551][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:17:45,552][233954] Reward + Measures: [[256.73477682   0.37020001   0.60219997   0.59230006   0.73220003
    1.67542207]
 [169.98238584   0.29980001   0.49790001   0.45650002   0.58499998
    1.50823236]
 [236.06699847   0.37610003   0.50450003   0.51379997   0.64499998
    1.60904825]
 ...
 [279.19176212   0.2102       0.70609999   0.53430003   0.70699996
    1.76763916]
 [228.52765589   0.29860002   0.59969997   0.53830004   0.65980005
    1.70121598]
 [324.04331748   0.16299999   0.6735       0.55159998   0.70999998
    1.88397694]][0m
[37m[1m[2023-07-11 14:17:45,552][233954] Max Reward on eval: 477.8564125597826[0m
[37m[1m[2023-07-11 14:17:45,553][233954] Min Reward on eval: -6.472525592846796[0m
[37m[1m[2023-07-11 14:17:45,553][233954] Mean Reward across all agents: 200.87549031225538[0m
[37m[1m[2023-07-11 14:17:45,553][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:17:45,558][233954] mean_value=-13.708821242032894, max_value=776.2017449046512[0m
[37m[1m[2023-07-11 14:17:45,561][233954] New mean coefficients: [[-0.00643052  0.03461905  0.05379279  0.24564818  0.19696136  0.10825168]][0m
[37m[1m[2023-07-11 14:17:45,562][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:17:54,566][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 14:17:54,567][233954] FPS: 426526.50[0m
[36m[2023-07-11 14:17:54,569][233954] itr=1048, itrs=2000, Progress: 52.40%[0m
[36m[2023-07-11 14:18:06,114][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 14:18:06,115][233954] FPS: 335343.97[0m
[36m[2023-07-11 14:18:10,363][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:18:10,364][233954] Reward + Measures: [[208.94551236   0.36422297   0.66278362   0.60186529   0.71278566
    1.55025387]][0m
[37m[1m[2023-07-11 14:18:10,364][233954] Max Reward on eval: 208.94551236254964[0m
[37m[1m[2023-07-11 14:18:10,364][233954] Min Reward on eval: 208.94551236254964[0m
[37m[1m[2023-07-11 14:18:10,365][233954] Mean Reward across all agents: 208.94551236254964[0m
[37m[1m[2023-07-11 14:18:10,365][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:18:15,354][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:18:15,355][233954] Reward + Measures: [[339.06305269   0.20410001   0.7001       0.58939999   0.72450006
    1.81911933]
 [242.03575261   0.1229       0.51490003   0.45670006   0.56910002
    1.73492742]
 [247.04807391   0.36469999   0.75749999   0.66420001   0.78250003
    1.69760072]
 ...
 [ 99.15264065   0.3513       0.49980003   0.42480001   0.56529999
    1.26555574]
 [387.43656419   0.2877       0.88859999   0.76950002   0.89099997
    1.80806255]
 [172.11918987   0.38320002   0.59580004   0.55629998   0.64410001
    1.52938831]][0m
[37m[1m[2023-07-11 14:18:15,355][233954] Max Reward on eval: 461.78171344934964[0m
[37m[1m[2023-07-11 14:18:15,355][233954] Min Reward on eval: -47.23173212246038[0m
[37m[1m[2023-07-11 14:18:15,356][233954] Mean Reward across all agents: 192.1039855358963[0m
[37m[1m[2023-07-11 14:18:15,356][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:18:15,360][233954] mean_value=-31.21306391132574, max_value=573.7391378698178[0m
[37m[1m[2023-07-11 14:18:15,363][233954] New mean coefficients: [[-0.01751747  0.03770364  0.0838721   0.2334226   0.20010033  0.11988588]][0m
[37m[1m[2023-07-11 14:18:15,364][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:18:24,331][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 14:18:24,331][233954] FPS: 428320.25[0m
[36m[2023-07-11 14:18:24,333][233954] itr=1049, itrs=2000, Progress: 52.45%[0m
[36m[2023-07-11 14:18:35,881][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 14:18:35,881][233954] FPS: 335321.42[0m
[36m[2023-07-11 14:18:40,175][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:18:40,175][233954] Reward + Measures: [[214.03205556   0.39267734   0.71786326   0.64640665   0.75737059
    1.57697606]][0m
[37m[1m[2023-07-11 14:18:40,176][233954] Max Reward on eval: 214.03205555526966[0m
[37m[1m[2023-07-11 14:18:40,176][233954] Min Reward on eval: 214.03205555526966[0m
[37m[1m[2023-07-11 14:18:40,176][233954] Mean Reward across all agents: 214.03205555526966[0m
[37m[1m[2023-07-11 14:18:40,176][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:18:45,130][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:18:45,130][233954] Reward + Measures: [[241.41182176   0.45620003   0.87690002   0.78380007   0.88679999
    1.61778796]
 [ 74.67254977   0.3091       0.33770001   0.38569999   0.4858
    1.36081541]
 [314.90608312   0.2115       0.7141       0.60299999   0.74110001
    1.77144456]
 ...
 [180.93028625   0.21829998   0.52510005   0.46960002   0.59450001
    1.65120888]
 [ 84.16365668   0.34779999   0.50389999   0.45539999   0.5262
    1.43528807]
 [178.67105436   0.454        0.77060002   0.70389998   0.78719997
    1.60465086]][0m
[37m[1m[2023-07-11 14:18:45,131][233954] Max Reward on eval: 478.1119051111862[0m
[37m[1m[2023-07-11 14:18:45,131][233954] Min Reward on eval: -46.481609594961625[0m
[37m[1m[2023-07-11 14:18:45,131][233954] Mean Reward across all agents: 189.17673396013154[0m
[37m[1m[2023-07-11 14:18:45,131][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:18:45,136][233954] mean_value=-12.390125809577603, max_value=755.6269513605174[0m
[37m[1m[2023-07-11 14:18:45,138][233954] New mean coefficients: [[-0.0239692   0.03743409  0.08634054  0.20380673  0.18173695  0.21533312]][0m
[37m[1m[2023-07-11 14:18:45,139][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:18:54,116][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 14:18:54,117][233954] FPS: 427823.87[0m
[36m[2023-07-11 14:18:54,119][233954] itr=1050, itrs=2000, Progress: 52.50%[0m
[37m[1m[2023-07-11 14:22:22,724][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001030[0m
[36m[2023-07-11 14:22:35,284][233954] train() took 11.89 seconds to complete[0m
[36m[2023-07-11 14:22:35,285][233954] FPS: 323033.46[0m
[36m[2023-07-11 14:22:39,506][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:22:39,507][233954] Reward + Measures: [[213.56523459   0.40644896   0.73102432   0.65610605   0.76664728
    1.59850776]][0m
[37m[1m[2023-07-11 14:22:39,507][233954] Max Reward on eval: 213.5652345925176[0m
[37m[1m[2023-07-11 14:22:39,507][233954] Min Reward on eval: 213.5652345925176[0m
[37m[1m[2023-07-11 14:22:39,508][233954] Mean Reward across all agents: 213.5652345925176[0m
[37m[1m[2023-07-11 14:22:39,508][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:22:44,505][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:22:44,506][233954] Reward + Measures: [[175.90309895   0.37190002   0.68300003   0.56450003   0.69920003
    1.55187416]
 [262.29509109   0.29050002   0.6893       0.59770006   0.72209996
    1.66183341]
 [154.85139302   0.42750001   0.6857       0.60639995   0.70690006
    1.52968562]
 ...
 [364.82209554   0.329        0.97110003   0.81639999   0.96140003
    1.86336935]
 [192.01819374   0.29630002   0.42020002   0.47870001   0.5765
    1.5402844 ]
 [132.13461882   0.3109       0.50730002   0.44239998   0.53220004
    1.38131344]][0m
[37m[1m[2023-07-11 14:22:44,506][233954] Max Reward on eval: 501.5209560229443[0m
[37m[1m[2023-07-11 14:22:44,506][233954] Min Reward on eval: -28.315340875834227[0m
[37m[1m[2023-07-11 14:22:44,506][233954] Mean Reward across all agents: 205.1572219136492[0m
[37m[1m[2023-07-11 14:22:44,506][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:22:44,512][233954] mean_value=-16.063630374984097, max_value=711.3696352285009[0m
[37m[1m[2023-07-11 14:22:44,515][233954] New mean coefficients: [[-0.01427829  0.02545138  0.07554504  0.2061443   0.16829686 -0.01910187]][0m
[37m[1m[2023-07-11 14:22:44,516][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:22:53,489][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 14:22:53,489][233954] FPS: 428012.54[0m
[36m[2023-07-11 14:22:53,492][233954] itr=1051, itrs=2000, Progress: 52.55%[0m
[36m[2023-07-11 14:23:05,118][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 14:23:05,118][233954] FPS: 333021.83[0m
[36m[2023-07-11 14:23:09,467][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:23:09,468][233954] Reward + Measures: [[212.36352819   0.41160002   0.74407405   0.66284734   0.77830124
    1.59016156]][0m
[37m[1m[2023-07-11 14:23:09,468][233954] Max Reward on eval: 212.3635281880269[0m
[37m[1m[2023-07-11 14:23:09,468][233954] Min Reward on eval: 212.3635281880269[0m
[37m[1m[2023-07-11 14:23:09,468][233954] Mean Reward across all agents: 212.3635281880269[0m
[37m[1m[2023-07-11 14:23:09,469][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:23:14,471][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:23:14,472][233954] Reward + Measures: [[ 91.2882893    0.65270001   0.77740002   0.72480005   0.76880002
    1.28588808]
 [228.63643483   0.38030002   0.7823       0.68700004   0.79800004
    1.63843179]
 [226.90480623   0.22809999   0.37640002   0.4258       0.62340003
    2.00810122]
 ...
 [264.90715052   0.12740001   0.42449999   0.3698       0.5381
    1.82899094]
 [216.83212806   0.38100001   0.69309998   0.5873       0.74650002
    1.65666771]
 [250.09063258   0.27200001   0.70600003   0.5571       0.72369999
    1.77170217]][0m
[37m[1m[2023-07-11 14:23:14,472][233954] Max Reward on eval: 460.30142283057796[0m
[37m[1m[2023-07-11 14:23:14,472][233954] Min Reward on eval: -5.926727493433281[0m
[37m[1m[2023-07-11 14:23:14,473][233954] Mean Reward across all agents: 201.9506177580323[0m
[37m[1m[2023-07-11 14:23:14,473][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:23:14,477][233954] mean_value=-25.87877147258522, max_value=645.6485389875306[0m
[37m[1m[2023-07-11 14:23:14,480][233954] New mean coefficients: [[ 0.01866266  0.03674912  0.06253245  0.20206572  0.1819698  -0.08949704]][0m
[37m[1m[2023-07-11 14:23:14,481][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:23:23,528][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 14:23:23,529][233954] FPS: 424517.92[0m
[36m[2023-07-11 14:23:23,531][233954] itr=1052, itrs=2000, Progress: 52.60%[0m
[36m[2023-07-11 14:23:35,228][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 14:23:35,228][233954] FPS: 330944.31[0m
[36m[2023-07-11 14:23:39,473][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:23:39,474][233954] Reward + Measures: [[211.72949238   0.42752001   0.76003867   0.67378497   0.79201937
    1.58225906]][0m
[37m[1m[2023-07-11 14:23:39,474][233954] Max Reward on eval: 211.72949237546527[0m
[37m[1m[2023-07-11 14:23:39,474][233954] Min Reward on eval: 211.72949237546527[0m
[37m[1m[2023-07-11 14:23:39,475][233954] Mean Reward across all agents: 211.72949237546527[0m
[37m[1m[2023-07-11 14:23:39,475][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:23:44,648][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:23:44,649][233954] Reward + Measures: [[201.68072943   0.3576       0.78080004   0.6024       0.80690002
    1.7527138 ]
 [ 76.49987018   0.63889998   0.76739997   0.65690005   0.77240002
    1.30582523]
 [225.23599097   0.3048       0.59570009   0.52469999   0.64560002
    1.5471704 ]
 ...
 [278.71044878   0.37470001   0.71090001   0.67989999   0.78549999
    1.62008286]
 [261.0296716    0.44650003   0.87180007   0.76889998   0.87309998
    1.63342726]
 [226.04908537   0.57279998   0.95529997   0.76950002   0.93239993
    1.60008967]][0m
[37m[1m[2023-07-11 14:23:44,649][233954] Max Reward on eval: 435.9957825192716[0m
[37m[1m[2023-07-11 14:23:44,650][233954] Min Reward on eval: 5.250614956230857[0m
[37m[1m[2023-07-11 14:23:44,650][233954] Mean Reward across all agents: 195.03485106203283[0m
[37m[1m[2023-07-11 14:23:44,650][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:23:44,655][233954] mean_value=-15.408727473421898, max_value=706.9138529704464[0m
[37m[1m[2023-07-11 14:23:44,657][233954] New mean coefficients: [[ 0.0026006   0.03085607  0.00278585  0.17320195  0.1747477  -0.23099741]][0m
[37m[1m[2023-07-11 14:23:44,658][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:23:53,548][233954] train() took 8.89 seconds to complete[0m
[36m[2023-07-11 14:23:53,548][233954] FPS: 432044.88[0m
[36m[2023-07-11 14:23:53,550][233954] itr=1053, itrs=2000, Progress: 52.65%[0m
[36m[2023-07-11 14:24:05,389][233954] train() took 11.75 seconds to complete[0m
[36m[2023-07-11 14:24:05,390][233954] FPS: 326944.28[0m
[36m[2023-07-11 14:24:09,626][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:24:09,627][233954] Reward + Measures: [[208.9014224    0.43876636   0.76001269   0.677957     0.79313266
    1.54732633]][0m
[37m[1m[2023-07-11 14:24:09,627][233954] Max Reward on eval: 208.90142239968765[0m
[37m[1m[2023-07-11 14:24:09,627][233954] Min Reward on eval: 208.90142239968765[0m
[37m[1m[2023-07-11 14:24:09,627][233954] Mean Reward across all agents: 208.90142239968765[0m
[37m[1m[2023-07-11 14:24:09,628][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:24:14,532][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:24:14,532][233954] Reward + Measures: [[ 24.48880999   0.82610005   0.85140002   0.82100004   0.84279996
    1.09512579]
 [226.32768778   0.49829999   0.95260012   0.79700005   0.93330002
    1.58680618]
 [134.81811884   0.6365       0.86260003   0.79520005   0.85710001
    1.38897157]
 ...
 [237.71865911   0.13329999   0.51920003   0.44639999   0.58669996
    1.65947115]
 [207.68386246   0.4429       0.67820007   0.5952       0.72229999
    1.57093823]
 [220.98635493   0.44229999   0.72290003   0.65749997   0.82100004
    1.64709151]][0m
[37m[1m[2023-07-11 14:24:14,533][233954] Max Reward on eval: 452.26503700757746[0m
[37m[1m[2023-07-11 14:24:14,533][233954] Min Reward on eval: -119.84778395895846[0m
[37m[1m[2023-07-11 14:24:14,533][233954] Mean Reward across all agents: 217.84303569384855[0m
[37m[1m[2023-07-11 14:24:14,534][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:24:14,538][233954] mean_value=-17.54050892950622, max_value=543.3850311311347[0m
[37m[1m[2023-07-11 14:24:14,541][233954] New mean coefficients: [[-0.03679476  0.03429713  0.03670731  0.15933408  0.14910278 -0.2982674 ]][0m
[37m[1m[2023-07-11 14:24:14,542][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:24:23,479][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 14:24:23,479][233954] FPS: 429723.14[0m
[36m[2023-07-11 14:24:23,482][233954] itr=1054, itrs=2000, Progress: 52.70%[0m
[36m[2023-07-11 14:24:35,343][233954] train() took 11.76 seconds to complete[0m
[36m[2023-07-11 14:24:35,343][233954] FPS: 326397.23[0m
[36m[2023-07-11 14:24:39,630][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:24:39,630][233954] Reward + Measures: [[212.27320889   0.43146166   0.76779771   0.68426096   0.79742128
    1.51819289]][0m
[37m[1m[2023-07-11 14:24:39,630][233954] Max Reward on eval: 212.27320889008118[0m
[37m[1m[2023-07-11 14:24:39,630][233954] Min Reward on eval: 212.27320889008118[0m
[37m[1m[2023-07-11 14:24:39,631][233954] Mean Reward across all agents: 212.27320889008118[0m
[37m[1m[2023-07-11 14:24:39,631][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:24:44,641][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:24:44,641][233954] Reward + Measures: [[240.72833981   0.2095       0.51200002   0.45140001   0.60830003
    1.76549947]
 [ 15.48903367   0.74800003   0.76169997   0.72479999   0.77570003
    0.98938972]
 [119.00378513   0.33399999   0.42109999   0.39290002   0.51929998
    1.34228301]
 ...
 [162.81211101   0.64880008   0.67730004   0.73119998   0.79790002
    1.35734963]
 [379.25715893   0.19180001   0.80269998   0.63940001   0.81589997
    1.80260873]
 [181.13055592   0.24340002   0.2404       0.32870001   0.44229999
    1.57525098]][0m
[37m[1m[2023-07-11 14:24:44,642][233954] Max Reward on eval: 531.2475814922713[0m
[37m[1m[2023-07-11 14:24:44,642][233954] Min Reward on eval: -43.80193607788533[0m
[37m[1m[2023-07-11 14:24:44,642][233954] Mean Reward across all agents: 186.7722460830496[0m
[37m[1m[2023-07-11 14:24:44,642][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:24:44,647][233954] mean_value=-21.496809617292865, max_value=503.6734790898803[0m
[37m[1m[2023-07-11 14:24:44,649][233954] New mean coefficients: [[-0.02306144  0.06094652  0.01520037  0.15638383  0.14931078 -0.3510244 ]][0m
[37m[1m[2023-07-11 14:24:44,650][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:24:53,675][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 14:24:53,675][233954] FPS: 425555.02[0m
[36m[2023-07-11 14:24:53,678][233954] itr=1055, itrs=2000, Progress: 52.75%[0m
[36m[2023-07-11 14:25:05,361][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 14:25:05,361][233954] FPS: 331482.20[0m
[36m[2023-07-11 14:25:09,626][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:25:09,627][233954] Reward + Measures: [[189.61579205   0.45900497   0.76709831   0.6898244    0.79643035
    1.42432976]][0m
[37m[1m[2023-07-11 14:25:09,627][233954] Max Reward on eval: 189.61579205237112[0m
[37m[1m[2023-07-11 14:25:09,627][233954] Min Reward on eval: 189.61579205237112[0m
[37m[1m[2023-07-11 14:25:09,628][233954] Mean Reward across all agents: 189.61579205237112[0m
[37m[1m[2023-07-11 14:25:09,628][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:25:14,636][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:25:14,637][233954] Reward + Measures: [[348.06844403   0.27430001   0.79300004   0.68489999   0.82979995
    1.82224119]
 [211.43263582   0.26100001   0.70480001   0.56690001   0.72500002
    1.65473652]
 [106.11799909   0.4571       0.64670002   0.54159999   0.70550007
    1.33623588]
 ...
 [310.4358256    0.3594       0.87360001   0.75150007   0.86910003
    1.67669928]
 [113.37002399   0.51170003   0.77689999   0.64820004   0.76379997
    1.40926731]
 [309.24620448   0.28510001   0.79589999   0.68149996   0.80800003
    1.67200112]][0m
[37m[1m[2023-07-11 14:25:14,637][233954] Max Reward on eval: 517.1362527613528[0m
[37m[1m[2023-07-11 14:25:14,637][233954] Min Reward on eval: -38.196777420910074[0m
[37m[1m[2023-07-11 14:25:14,637][233954] Mean Reward across all agents: 191.08900559300042[0m
[37m[1m[2023-07-11 14:25:14,637][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:25:14,642][233954] mean_value=-11.910123950729842, max_value=526.543652129149[0m
[37m[1m[2023-07-11 14:25:14,645][233954] New mean coefficients: [[-0.03868468  0.04449733  0.00285626  0.11789668  0.10175554 -0.5964541 ]][0m
[37m[1m[2023-07-11 14:25:14,646][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:25:23,655][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 14:25:23,656][233954] FPS: 426304.22[0m
[36m[2023-07-11 14:25:23,658][233954] itr=1056, itrs=2000, Progress: 52.80%[0m
[36m[2023-07-11 14:25:35,226][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 14:25:35,226][233954] FPS: 334671.46[0m
[36m[2023-07-11 14:25:39,487][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:25:39,488][233954] Reward + Measures: [[190.93542256   0.46419099   0.77368796   0.70243335   0.80176938
    1.32823563]][0m
[37m[1m[2023-07-11 14:25:39,488][233954] Max Reward on eval: 190.93542255552882[0m
[37m[1m[2023-07-11 14:25:39,488][233954] Min Reward on eval: 190.93542255552882[0m
[37m[1m[2023-07-11 14:25:39,488][233954] Mean Reward across all agents: 190.93542255552882[0m
[37m[1m[2023-07-11 14:25:39,489][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:25:44,477][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:25:44,478][233954] Reward + Measures: [[282.0594159    0.37650001   0.88420004   0.78920001   0.89399999
    1.54132879]
 [106.86590734   0.398        0.59569997   0.50700003   0.64390004
    1.32542133]
 [134.52596815   0.47979999   0.69980001   0.64429998   0.72139996
    1.11556792]
 ...
 [109.70590144   0.49480006   0.77890009   0.64309996   0.78490001
    1.21250403]
 [243.62830198   0.38400003   0.70139998   0.63290006   0.74470007
    1.47308171]
 [123.98787616   0.5          0.78109998   0.69990009   0.78120005
    1.2614553 ]][0m
[37m[1m[2023-07-11 14:25:44,478][233954] Max Reward on eval: 439.33298468286523[0m
[37m[1m[2023-07-11 14:25:44,478][233954] Min Reward on eval: -17.75703859180212[0m
[37m[1m[2023-07-11 14:25:44,478][233954] Mean Reward across all agents: 193.57514133423544[0m
[37m[1m[2023-07-11 14:25:44,479][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:25:44,485][233954] mean_value=58.625418614361514, max_value=591.8817466631541[0m
[37m[1m[2023-07-11 14:25:44,488][233954] New mean coefficients: [[-0.05318845  0.04206305  0.06832834  0.15541774  0.08712943 -0.7603411 ]][0m
[37m[1m[2023-07-11 14:25:44,489][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:25:53,469][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 14:25:53,470][233954] FPS: 427677.69[0m
[36m[2023-07-11 14:25:53,472][233954] itr=1057, itrs=2000, Progress: 52.85%[0m
[36m[2023-07-11 14:26:05,025][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 14:26:05,025][233954] FPS: 335178.29[0m
[36m[2023-07-11 14:26:09,283][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:26:09,284][233954] Reward + Measures: [[192.13251682   0.4768863    0.78753197   0.71565598   0.81184196
    1.2539686 ]][0m
[37m[1m[2023-07-11 14:26:09,284][233954] Max Reward on eval: 192.13251681676041[0m
[37m[1m[2023-07-11 14:26:09,284][233954] Min Reward on eval: 192.13251681676041[0m
[37m[1m[2023-07-11 14:26:09,285][233954] Mean Reward across all agents: 192.13251681676041[0m
[37m[1m[2023-07-11 14:26:09,285][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:26:14,308][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:26:14,309][233954] Reward + Measures: [[181.11962209   0.40349999   0.59689999   0.55680001   0.65570003
    1.28022194]
 [434.27006758   0.13680001   0.77890003   0.66500008   0.82609999
    1.88190544]
 [153.59659208   0.4772       0.68230003   0.63690007   0.7155
    1.29553497]
 ...
 [150.12496615   0.43969998   0.77610004   0.62169999   0.80590004
    1.27289426]
 [192.58407792   0.46450001   0.78079998   0.71869999   0.78890002
    1.28031087]
 [218.73121015   0.55030006   0.86709994   0.79500008   0.87199992
    1.20398545]][0m
[37m[1m[2023-07-11 14:26:14,309][233954] Max Reward on eval: 484.3703260361799[0m
[37m[1m[2023-07-11 14:26:14,309][233954] Min Reward on eval: -1.0790593488374725[0m
[37m[1m[2023-07-11 14:26:14,310][233954] Mean Reward across all agents: 181.19087576283883[0m
[37m[1m[2023-07-11 14:26:14,310][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:26:14,317][233954] mean_value=70.75669661183088, max_value=706.6472447261215[0m
[37m[1m[2023-07-11 14:26:14,319][233954] New mean coefficients: [[-0.0512341   0.05217655  0.1098229   0.15460894  0.11680683 -0.73502976]][0m
[37m[1m[2023-07-11 14:26:14,320][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:26:23,300][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 14:26:23,300][233954] FPS: 427736.74[0m
[36m[2023-07-11 14:26:23,302][233954] itr=1058, itrs=2000, Progress: 52.90%[0m
[36m[2023-07-11 14:26:35,163][233954] train() took 11.76 seconds to complete[0m
[36m[2023-07-11 14:26:35,163][233954] FPS: 326407.43[0m
[36m[2023-07-11 14:26:39,470][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:26:39,470][233954] Reward + Measures: [[169.92924992   0.49326497   0.76790643   0.70860368   0.79383302
    1.09981608]][0m
[37m[1m[2023-07-11 14:26:39,471][233954] Max Reward on eval: 169.92924991695207[0m
[37m[1m[2023-07-11 14:26:39,471][233954] Min Reward on eval: 169.92924991695207[0m
[37m[1m[2023-07-11 14:26:39,471][233954] Mean Reward across all agents: 169.92924991695207[0m
[37m[1m[2023-07-11 14:26:39,471][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:26:44,750][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:26:44,751][233954] Reward + Measures: [[331.67285751   0.33880001   0.87930006   0.76960003   0.88280004
    1.51996958]
 [146.37659796   0.72060007   0.95570004   0.88160002   0.94340003
    1.1183356 ]
 [303.77234877   0.3748       0.79350001   0.68399996   0.80579996
    1.65114427]
 ...
 [246.13806207   0.47049999   0.61270005   0.66870004   0.70780003
    1.26007044]
 [112.53313317   0.38960001   0.60689998   0.54180002   0.6656
    1.04351366]
 [329.43379302   0.28430003   0.79220003   0.70910001   0.83170003
    1.53520942]][0m
[37m[1m[2023-07-11 14:26:44,751][233954] Max Reward on eval: 480.55665866896743[0m
[37m[1m[2023-07-11 14:26:44,751][233954] Min Reward on eval: -29.489512626081705[0m
[37m[1m[2023-07-11 14:26:44,751][233954] Mean Reward across all agents: 213.93001016516558[0m
[37m[1m[2023-07-11 14:26:44,752][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:26:44,760][233954] mean_value=116.91291186104756, max_value=746.1380620719167[0m
[37m[1m[2023-07-11 14:26:44,762][233954] New mean coefficients: [[-0.04686356  0.0348357   0.07039125  0.14330734  0.0981319  -0.8229357 ]][0m
[37m[1m[2023-07-11 14:26:44,763][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:26:53,765][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 14:26:53,765][233954] FPS: 426666.60[0m
[36m[2023-07-11 14:26:53,767][233954] itr=1059, itrs=2000, Progress: 52.95%[0m
[36m[2023-07-11 14:27:05,642][233954] train() took 11.78 seconds to complete[0m
[36m[2023-07-11 14:27:05,642][233954] FPS: 325968.43[0m
[36m[2023-07-11 14:27:09,934][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:27:09,934][233954] Reward + Measures: [[175.67426195   0.48120466   0.76276499   0.71004033   0.78934991
    1.07341254]][0m
[37m[1m[2023-07-11 14:27:09,934][233954] Max Reward on eval: 175.67426195086895[0m
[37m[1m[2023-07-11 14:27:09,935][233954] Min Reward on eval: 175.67426195086895[0m
[37m[1m[2023-07-11 14:27:09,935][233954] Mean Reward across all agents: 175.67426195086895[0m
[37m[1m[2023-07-11 14:27:09,935][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:27:14,904][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:27:14,905][233954] Reward + Measures: [[193.47236336   0.56010002   0.69240004   0.73070002   0.79940003
    1.16282356]
 [205.06569834   0.39070001   0.8064       0.71320003   0.79409999
    1.30793035]
 [124.68091051   0.64630002   0.6785       0.73439997   0.7877
    0.98749512]
 ...
 [311.42052605   0.21789999   0.70120007   0.62129998   0.76140004
    1.46809018]
 [269.81168016   0.2818       0.71449995   0.6292001    0.7295
    1.51127315]
 [298.91423683   0.46250001   0.97310013   0.88099998   0.96619999
    1.33218563]][0m
[37m[1m[2023-07-11 14:27:14,905][233954] Max Reward on eval: 431.3373669736553[0m
[37m[1m[2023-07-11 14:27:14,905][233954] Min Reward on eval: -26.085778943425975[0m
[37m[1m[2023-07-11 14:27:14,906][233954] Mean Reward across all agents: 194.3630996416081[0m
[37m[1m[2023-07-11 14:27:14,906][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:27:14,913][233954] mean_value=27.71773581356304, max_value=557.7776689811014[0m
[37m[1m[2023-07-11 14:27:14,915][233954] New mean coefficients: [[-0.07772292  0.00668972  0.0671017   0.08773771  0.06709775 -0.8308509 ]][0m
[37m[1m[2023-07-11 14:27:14,916][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:27:23,870][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 14:27:23,870][233954] FPS: 428937.01[0m
[36m[2023-07-11 14:27:23,873][233954] itr=1060, itrs=2000, Progress: 53.00%[0m
[37m[1m[2023-07-11 14:31:03,311][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001040[0m
[36m[2023-07-11 14:31:15,435][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 14:31:15,435][233954] FPS: 332292.62[0m
[36m[2023-07-11 14:31:19,645][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:31:19,646][233954] Reward + Measures: [[179.44180216   0.47043097   0.76060271   0.70619529   0.78402829
    1.07311237]][0m
[37m[1m[2023-07-11 14:31:19,646][233954] Max Reward on eval: 179.4418021597112[0m
[37m[1m[2023-07-11 14:31:19,646][233954] Min Reward on eval: 179.4418021597112[0m
[37m[1m[2023-07-11 14:31:19,646][233954] Mean Reward across all agents: 179.4418021597112[0m
[37m[1m[2023-07-11 14:31:19,647][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:31:24,637][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:31:24,698][233954] Reward + Measures: [[175.06714375   0.57050002   0.8761       0.80719995   0.89470005
    0.91290563]
 [ 66.77403449   0.60210001   0.77939999   0.70630002   0.78620005
    0.83827811]
 [228.24340643   0.373        0.71260005   0.65609998   0.72819996
    1.1475786 ]
 ...
 [168.92414814   0.4152       0.48839998   0.47930002   0.63350004
    1.25172424]
 [308.62988051   0.30000001   0.79649997   0.68720001   0.82130003
    1.38763773]
 [166.89954578   0.28670001   0.4738       0.47530004   0.55400008
    1.24338663]][0m
[37m[1m[2023-07-11 14:31:24,698][233954] Max Reward on eval: 462.26472946238937[0m
[37m[1m[2023-07-11 14:31:24,698][233954] Min Reward on eval: -49.069473719084634[0m
[37m[1m[2023-07-11 14:31:24,699][233954] Mean Reward across all agents: 166.47619170271514[0m
[37m[1m[2023-07-11 14:31:24,699][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:31:24,707][233954] mean_value=36.713553400768795, max_value=590.4356498752162[0m
[37m[1m[2023-07-11 14:31:24,710][233954] New mean coefficients: [[-0.06474628  0.03304982  0.05063618  0.06356185  0.07926319 -0.82088965]][0m
[37m[1m[2023-07-11 14:31:24,711][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:31:33,828][233954] train() took 9.12 seconds to complete[0m
[36m[2023-07-11 14:31:33,828][233954] FPS: 421244.19[0m
[36m[2023-07-11 14:31:33,831][233954] itr=1061, itrs=2000, Progress: 53.05%[0m
[36m[2023-07-11 14:31:45,437][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 14:31:45,437][233954] FPS: 333690.91[0m
[36m[2023-07-11 14:31:49,681][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:31:49,682][233954] Reward + Measures: [[172.12741074   0.47235864   0.74800599   0.702604     0.78092462
    1.04569304]][0m
[37m[1m[2023-07-11 14:31:49,682][233954] Max Reward on eval: 172.12741073669815[0m
[37m[1m[2023-07-11 14:31:49,682][233954] Min Reward on eval: 172.12741073669815[0m
[37m[1m[2023-07-11 14:31:49,683][233954] Mean Reward across all agents: 172.12741073669815[0m
[37m[1m[2023-07-11 14:31:49,683][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:31:54,626][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:31:54,627][233954] Reward + Measures: [[171.09012894   0.47849998   0.80550003   0.72990006   0.79549998
    0.98892063]
 [124.22238491   0.70740002   0.96239996   0.88029999   0.94999999
    0.7904278 ]
 [138.46232029   0.55689996   0.80039996   0.74800009   0.78260005
    0.86983818]
 ...
 [188.98106296   0.31520003   0.61089998   0.53839999   0.65209997
    1.21015882]
 [ 87.96044776   0.32479998   0.46869999   0.45679998   0.50220001
    0.93255579]
 [182.5398823    0.48400003   0.78870004   0.72540003   0.82880002
    1.05792797]][0m
[37m[1m[2023-07-11 14:31:54,627][233954] Max Reward on eval: 385.21130503646566[0m
[37m[1m[2023-07-11 14:31:54,627][233954] Min Reward on eval: -68.05234607784078[0m
[37m[1m[2023-07-11 14:31:54,627][233954] Mean Reward across all agents: 148.42473469937292[0m
[37m[1m[2023-07-11 14:31:54,627][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:31:54,635][233954] mean_value=20.225905206468706, max_value=627.580422188607[0m
[37m[1m[2023-07-11 14:31:54,637][233954] New mean coefficients: [[-0.08183478  0.02077435  0.05887824  0.07430554  0.06331158 -0.7321709 ]][0m
[37m[1m[2023-07-11 14:31:54,638][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:32:03,654][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 14:32:03,654][233954] FPS: 426023.79[0m
[36m[2023-07-11 14:32:03,656][233954] itr=1062, itrs=2000, Progress: 53.10%[0m
[36m[2023-07-11 14:32:15,443][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 14:32:15,443][233954] FPS: 328556.89[0m
[36m[2023-07-11 14:32:19,826][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:32:19,827][233954] Reward + Measures: [[158.79785312   0.48684233   0.73773235   0.69678175   0.7700026
    0.98728526]][0m
[37m[1m[2023-07-11 14:32:19,827][233954] Max Reward on eval: 158.7978531158487[0m
[37m[1m[2023-07-11 14:32:19,827][233954] Min Reward on eval: 158.7978531158487[0m
[37m[1m[2023-07-11 14:32:19,827][233954] Mean Reward across all agents: 158.7978531158487[0m
[37m[1m[2023-07-11 14:32:19,828][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:32:25,042][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:32:25,043][233954] Reward + Measures: [[254.86181409   0.38100001   0.78940004   0.71899998   0.80629998
    1.32646596]
 [181.11186226   0.5582       0.86590004   0.78299999   0.87740004
    0.99749488]
 [120.21582828   0.40770003   0.64210004   0.60479999   0.6534
    0.83624613]
 ...
 [208.45166048   0.39910004   0.78850001   0.65690005   0.82120001
    1.31712782]
 [116.34183959   0.48209998   0.69709998   0.64059997   0.71810001
    0.99553967]
 [220.23646108   0.38330004   0.51560003   0.57160002   0.63499999
    1.2720741 ]][0m
[37m[1m[2023-07-11 14:32:25,044][233954] Max Reward on eval: 454.76561658559365[0m
[37m[1m[2023-07-11 14:32:25,044][233954] Min Reward on eval: -18.328231046628208[0m
[37m[1m[2023-07-11 14:32:25,044][233954] Mean Reward across all agents: 163.42848556987568[0m
[37m[1m[2023-07-11 14:32:25,044][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:32:25,051][233954] mean_value=23.63082535637629, max_value=467.7917611473134[0m
[37m[1m[2023-07-11 14:32:25,054][233954] New mean coefficients: [[-0.05047757  0.02338805  0.07067201  0.10458422  0.07745337 -0.7428064 ]][0m
[37m[1m[2023-07-11 14:32:25,055][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:32:34,094][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 14:32:34,094][233954] FPS: 424907.28[0m
[36m[2023-07-11 14:32:34,097][233954] itr=1063, itrs=2000, Progress: 53.15%[0m
[36m[2023-07-11 14:32:45,689][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 14:32:45,689][233954] FPS: 334086.22[0m
[36m[2023-07-11 14:32:49,916][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:32:49,917][233954] Reward + Measures: [[149.10418072   0.486238     0.73180199   0.68778336   0.75853664
    0.93784714]][0m
[37m[1m[2023-07-11 14:32:49,917][233954] Max Reward on eval: 149.10418072480354[0m
[37m[1m[2023-07-11 14:32:49,917][233954] Min Reward on eval: 149.10418072480354[0m
[37m[1m[2023-07-11 14:32:49,918][233954] Mean Reward across all agents: 149.10418072480354[0m
[37m[1m[2023-07-11 14:32:49,918][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:32:54,863][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:32:54,863][233954] Reward + Measures: [[106.40300192   0.4025       0.53249997   0.49169999   0.5923
    0.89619809]
 [ -1.56787579   0.81949997   0.85529995   0.78600001   0.85610002
    0.59328049]
 [ 67.10926806   0.74739999   0.87370008   0.83249998   0.86219996
    0.65149671]
 ...
 [ 60.69513514   0.48839998   0.61449999   0.59009999   0.62309998
    0.84600848]
 [164.00700869   0.43790004   0.60670006   0.5643       0.69349998
    0.91060591]
 [  9.84492281   0.58390009   0.71320003   0.55109996   0.74090004
    0.88959616]][0m
[37m[1m[2023-07-11 14:32:54,863][233954] Max Reward on eval: 410.24110812088475[0m
[37m[1m[2023-07-11 14:32:54,864][233954] Min Reward on eval: -68.49243569020764[0m
[37m[1m[2023-07-11 14:32:54,864][233954] Mean Reward across all agents: 141.6433322936694[0m
[37m[1m[2023-07-11 14:32:54,864][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:32:54,870][233954] mean_value=16.662539226879918, max_value=625.6720898811473[0m
[37m[1m[2023-07-11 14:32:54,873][233954] New mean coefficients: [[-0.06069779  0.04227455  0.02611825  0.06480432  0.04636971 -0.6207596 ]][0m
[37m[1m[2023-07-11 14:32:54,874][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:33:03,805][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 14:33:03,805][233954] FPS: 430028.60[0m
[36m[2023-07-11 14:33:03,807][233954] itr=1064, itrs=2000, Progress: 53.20%[0m
[36m[2023-07-11 14:33:15,682][233954] train() took 11.78 seconds to complete[0m
[36m[2023-07-11 14:33:15,683][233954] FPS: 325957.00[0m
[36m[2023-07-11 14:33:20,011][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:33:20,012][233954] Reward + Measures: [[151.65711208   0.48213667   0.71814966   0.67895663   0.75630933
    0.95232618]][0m
[37m[1m[2023-07-11 14:33:20,012][233954] Max Reward on eval: 151.657112075001[0m
[37m[1m[2023-07-11 14:33:20,012][233954] Min Reward on eval: 151.657112075001[0m
[37m[1m[2023-07-11 14:33:20,012][233954] Mean Reward across all agents: 151.657112075001[0m
[37m[1m[2023-07-11 14:33:20,013][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:33:24,972][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:33:24,973][233954] Reward + Measures: [[191.215912     0.4436       0.8193       0.72220004   0.80070001
    1.0724895 ]
 [ 68.35435689   0.48400003   0.50580001   0.47090003   0.62860006
    0.6284247 ]
 [345.00423012   0.2929       0.63230002   0.64340001   0.7374
    1.51729083]
 ...
 [171.8207457    0.63859999   0.72629994   0.83660001   0.83500004
    0.86833698]
 [227.22221066   0.36059999   0.81920004   0.70090002   0.77929997
    1.28364885]
 [190.08353265   0.3856       0.7202       0.6692       0.71960002
    1.17541862]][0m
[37m[1m[2023-07-11 14:33:24,973][233954] Max Reward on eval: 427.808138562541[0m
[37m[1m[2023-07-11 14:33:24,973][233954] Min Reward on eval: -51.6189496695064[0m
[37m[1m[2023-07-11 14:33:24,974][233954] Mean Reward across all agents: 144.83652473921694[0m
[37m[1m[2023-07-11 14:33:24,974][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:33:24,981][233954] mean_value=6.416892220070631, max_value=526.9009604624998[0m
[37m[1m[2023-07-11 14:33:24,983][233954] New mean coefficients: [[-0.07917567  0.03112626 -0.01604931  0.04168005  0.03110869 -0.66540927]][0m
[37m[1m[2023-07-11 14:33:24,984][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:33:34,018][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 14:33:34,018][233954] FPS: 425149.97[0m
[36m[2023-07-11 14:33:34,020][233954] itr=1065, itrs=2000, Progress: 53.25%[0m
[36m[2023-07-11 14:33:45,778][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 14:33:45,778][233954] FPS: 329259.26[0m
[36m[2023-07-11 14:33:50,057][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:33:50,058][233954] Reward + Measures: [[135.12484383   0.52073699   0.7329846    0.69543475   0.77162403
    0.87186915]][0m
[37m[1m[2023-07-11 14:33:50,058][233954] Max Reward on eval: 135.12484382897804[0m
[37m[1m[2023-07-11 14:33:50,058][233954] Min Reward on eval: 135.12484382897804[0m
[37m[1m[2023-07-11 14:33:50,058][233954] Mean Reward across all agents: 135.12484382897804[0m
[37m[1m[2023-07-11 14:33:50,059][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:33:55,011][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:33:55,011][233954] Reward + Measures: [[125.19552913   0.648        0.88500005   0.82670003   0.86200011
    0.71076649]
 [ 57.66269605   0.49700004   0.62300003   0.58470005   0.68580008
    0.83098632]
 [112.02368531   0.38         0.73870003   0.58399999   0.72170001
    1.2513063 ]
 ...
 [218.05072972   0.40559998   0.53049999   0.59219998   0.67740005
    1.2744478 ]
 [230.04842235   0.32869998   0.70300001   0.60930002   0.77010006
    1.18126738]
 [ 37.22016007   0.58549994   0.40839997   0.45629999   0.57790005
    0.51523632]][0m
[37m[1m[2023-07-11 14:33:55,012][233954] Max Reward on eval: 364.2906195229385[0m
[37m[1m[2023-07-11 14:33:55,012][233954] Min Reward on eval: -47.110859011276624[0m
[37m[1m[2023-07-11 14:33:55,012][233954] Mean Reward across all agents: 144.5026211352455[0m
[37m[1m[2023-07-11 14:33:55,012][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:33:55,019][233954] mean_value=7.425587878280567, max_value=529.0687301544434[0m
[37m[1m[2023-07-11 14:33:55,022][233954] New mean coefficients: [[-0.08629863  0.02927288 -0.02725028  0.03244791  0.02316649 -0.7147957 ]][0m
[37m[1m[2023-07-11 14:33:55,023][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:34:04,111][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 14:34:04,111][233954] FPS: 422630.24[0m
[36m[2023-07-11 14:34:04,113][233954] itr=1066, itrs=2000, Progress: 53.30%[0m
[36m[2023-07-11 14:34:15,851][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 14:34:15,851][233954] FPS: 329840.88[0m
[36m[2023-07-11 14:34:20,205][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:34:20,206][233954] Reward + Measures: [[134.92035546   0.51751566   0.72354901   0.68848997   0.76745963
    0.86975086]][0m
[37m[1m[2023-07-11 14:34:20,206][233954] Max Reward on eval: 134.92035545865156[0m
[37m[1m[2023-07-11 14:34:20,206][233954] Min Reward on eval: 134.92035545865156[0m
[37m[1m[2023-07-11 14:34:20,206][233954] Mean Reward across all agents: 134.92035545865156[0m
[37m[1m[2023-07-11 14:34:20,207][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:34:25,217][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:34:25,218][233954] Reward + Measures: [[ 58.0207207    0.54520005   0.70000005   0.64210004   0.7373001
    0.76141906]
 [239.21428394   0.2938       0.72030002   0.64700001   0.7518
    1.32327294]
 [188.63955462   0.4605       0.80459994   0.73010004   0.79770005
    0.98042834]
 ...
 [  6.72594206   0.7608       0.76420003   0.74580002   0.79500002
    0.49189836]
 [  7.39539884   0.77509999   0.75529999   0.73369998   0.82490009
    0.29758132]
 [  2.8935289    0.92550004   0.95080006   0.91839999   0.94209999
    0.41176367]][0m
[37m[1m[2023-07-11 14:34:25,218][233954] Max Reward on eval: 371.9013591229217[0m
[37m[1m[2023-07-11 14:34:25,218][233954] Min Reward on eval: -36.4271995106712[0m
[37m[1m[2023-07-11 14:34:25,218][233954] Mean Reward across all agents: 124.27725624315342[0m
[37m[1m[2023-07-11 14:34:25,219][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:34:25,224][233954] mean_value=3.2860689444605273, max_value=558.2925762398053[0m
[37m[1m[2023-07-11 14:34:25,227][233954] New mean coefficients: [[-0.09637596  0.02235441 -0.01367348  0.02284681  0.02312751 -0.74402314]][0m
[37m[1m[2023-07-11 14:34:25,228][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:34:34,191][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 14:34:34,191][233954] FPS: 428483.14[0m
[36m[2023-07-11 14:34:34,193][233954] itr=1067, itrs=2000, Progress: 53.35%[0m
[36m[2023-07-11 14:34:45,987][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 14:34:45,987][233954] FPS: 328396.88[0m
[36m[2023-07-11 14:34:50,267][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:34:50,267][233954] Reward + Measures: [[136.22482519   0.51264131   0.72470498   0.68557364   0.76913804
    0.86048615]][0m
[37m[1m[2023-07-11 14:34:50,268][233954] Max Reward on eval: 136.22482519352266[0m
[37m[1m[2023-07-11 14:34:50,268][233954] Min Reward on eval: 136.22482519352266[0m
[37m[1m[2023-07-11 14:34:50,268][233954] Mean Reward across all agents: 136.22482519352266[0m
[37m[1m[2023-07-11 14:34:50,268][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:34:55,298][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:34:55,299][233954] Reward + Measures: [[219.50054884   0.31090003   0.70460004   0.61439997   0.74720001
    1.21642101]
 [162.8049367    0.47779998   0.80199999   0.73150003   0.81370002
    1.08839881]
 [211.46849678   0.3098       0.63060009   0.56380004   0.66670001
    1.14203966]
 ...
 [ 31.72729031   0.48240003   0.51349998   0.49969998   0.5643
    0.66412908]
 [181.84827105   0.52800006   0.87960005   0.78670007   0.88290006
    0.98735267]
 [ 77.78023832   0.43840003   0.52249998   0.49680001   0.56400001
    0.7281909 ]][0m
[37m[1m[2023-07-11 14:34:55,299][233954] Max Reward on eval: 361.4605172452808[0m
[37m[1m[2023-07-11 14:34:55,299][233954] Min Reward on eval: -28.392248384607957[0m
[37m[1m[2023-07-11 14:34:55,300][233954] Mean Reward across all agents: 117.41770244354761[0m
[37m[1m[2023-07-11 14:34:55,300][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:34:55,305][233954] mean_value=-5.021037944593651, max_value=564.6622136013815[0m
[37m[1m[2023-07-11 14:34:55,308][233954] New mean coefficients: [[-0.07835004  0.01704205  0.00503254  0.03841497  0.02579362 -0.63426566]][0m
[37m[1m[2023-07-11 14:34:55,309][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:35:04,242][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 14:35:04,243][233954] FPS: 429905.64[0m
[36m[2023-07-11 14:35:04,245][233954] itr=1068, itrs=2000, Progress: 53.40%[0m
[36m[2023-07-11 14:35:15,829][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 14:35:15,834][233954] FPS: 334235.00[0m
[36m[2023-07-11 14:35:20,043][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:35:20,044][233954] Reward + Measures: [[121.08493853   0.52772731   0.71620929   0.6792143    0.76664859
    0.79966521]][0m
[37m[1m[2023-07-11 14:35:20,044][233954] Max Reward on eval: 121.08493852630765[0m
[37m[1m[2023-07-11 14:35:20,044][233954] Min Reward on eval: 121.08493852630765[0m
[37m[1m[2023-07-11 14:35:20,044][233954] Mean Reward across all agents: 121.08493852630765[0m
[37m[1m[2023-07-11 14:35:20,045][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:35:25,315][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:35:25,315][233954] Reward + Measures: [[  9.2247289    0.65469998   0.70300001   0.68119997   0.71210003
    0.47633362]
 [225.49622959   0.42500001   0.79409999   0.70990002   0.86280006
    1.08220804]
 [129.14273159   0.43309999   0.59790003   0.54900002   0.66979998
    0.7891925 ]
 ...
 [140.89671508   0.55899996   0.77649999   0.70880002   0.81090003
    0.88066322]
 [ 87.57135823   0.41300002   0.45630002   0.48919997   0.58670002
    0.727539  ]
 [208.4578889    0.52170002   0.85319996   0.76700002   0.87419999
    1.12654245]][0m
[37m[1m[2023-07-11 14:35:25,316][233954] Max Reward on eval: 366.1279212373309[0m
[37m[1m[2023-07-11 14:35:25,316][233954] Min Reward on eval: -11.890929396264255[0m
[37m[1m[2023-07-11 14:35:25,316][233954] Mean Reward across all agents: 124.77691149467898[0m
[37m[1m[2023-07-11 14:35:25,316][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:35:25,322][233954] mean_value=-8.110916950139679, max_value=554.5825189159473[0m
[37m[1m[2023-07-11 14:35:25,325][233954] New mean coefficients: [[-0.07551671  0.00370459  0.00943898  0.03985348  0.01769336 -0.6697739 ]][0m
[37m[1m[2023-07-11 14:35:25,326][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:35:34,295][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 14:35:34,295][233954] FPS: 428209.82[0m
[36m[2023-07-11 14:35:34,298][233954] itr=1069, itrs=2000, Progress: 53.45%[0m
[36m[2023-07-11 14:35:46,171][233954] train() took 11.77 seconds to complete[0m
[36m[2023-07-11 14:35:46,171][233954] FPS: 326114.86[0m
[36m[2023-07-11 14:35:50,512][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:35:50,512][233954] Reward + Measures: [[125.81175634   0.52840203   0.71707439   0.68000633   0.77255434
    0.80636406]][0m
[37m[1m[2023-07-11 14:35:50,512][233954] Max Reward on eval: 125.81175633518168[0m
[37m[1m[2023-07-11 14:35:50,513][233954] Min Reward on eval: 125.81175633518168[0m
[37m[1m[2023-07-11 14:35:50,513][233954] Mean Reward across all agents: 125.81175633518168[0m
[37m[1m[2023-07-11 14:35:50,513][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:35:55,489][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:35:55,489][233954] Reward + Measures: [[177.13805927   0.3863       0.7287001    0.66249996   0.72360003
    0.98341763]
 [ 99.02151161   0.5747       0.77719998   0.71619999   0.8021
    0.72545689]
 [ 18.36393168   0.60900003   0.58930004   0.57339996   0.67030001
    0.49871618]
 ...
 [195.21709033   0.38979998   0.68609995   0.61869997   0.74790001
    1.17285848]
 [184.19407067   0.49699998   0.78830004   0.71390003   0.82709998
    0.89671057]
 [103.14487772   0.57450002   0.73089999   0.68950003   0.77149999
    0.79088217]][0m
[37m[1m[2023-07-11 14:35:55,490][233954] Max Reward on eval: 366.00398529884404[0m
[37m[1m[2023-07-11 14:35:55,490][233954] Min Reward on eval: -24.005069134570657[0m
[37m[1m[2023-07-11 14:35:55,490][233954] Mean Reward across all agents: 124.35958739289791[0m
[37m[1m[2023-07-11 14:35:55,490][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:35:55,495][233954] mean_value=-5.351772792970363, max_value=352.180010395339[0m
[37m[1m[2023-07-11 14:35:55,498][233954] New mean coefficients: [[-0.06672516 -0.00066332  0.01809924  0.05073782  0.04427583 -0.5757779 ]][0m
[37m[1m[2023-07-11 14:35:55,499][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:36:04,479][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 14:36:04,479][233954] FPS: 427705.16[0m
[36m[2023-07-11 14:36:04,481][233954] itr=1070, itrs=2000, Progress: 53.50%[0m
[37m[1m[2023-07-11 14:39:40,752][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001050[0m
[36m[2023-07-11 14:39:53,433][233954] train() took 12.03 seconds to complete[0m
[36m[2023-07-11 14:39:53,433][233954] FPS: 319215.03[0m
[36m[2023-07-11 14:39:57,661][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:39:57,662][233954] Reward + Measures: [[117.15221637   0.56544501   0.75259161   0.70752066   0.79606199
    0.76271915]][0m
[37m[1m[2023-07-11 14:39:57,662][233954] Max Reward on eval: 117.15221636558063[0m
[37m[1m[2023-07-11 14:39:57,662][233954] Min Reward on eval: 117.15221636558063[0m
[37m[1m[2023-07-11 14:39:57,663][233954] Mean Reward across all agents: 117.15221636558063[0m
[37m[1m[2023-07-11 14:39:57,663][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:40:02,618][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:40:02,619][233954] Reward + Measures: [[ 68.27069736   0.55300003   0.70320004   0.67629999   0.69659996
    0.83210701]
 [ 88.82177521   0.5291       0.69550008   0.61800003   0.76420003
    0.72041202]
 [110.81246604   0.64670002   0.86339998   0.79540008   0.87080002
    0.78244346]
 ...
 [ 62.54515239   0.52850002   0.50489998   0.55619997   0.6947
    0.59546709]
 [116.73330149   0.44029999   0.59689999   0.5262       0.69859999
    0.82324344]
 [118.42893081   0.73510009   0.94750005   0.89910001   0.94690001
    0.71409792]][0m
[37m[1m[2023-07-11 14:40:02,619][233954] Max Reward on eval: 324.64118953983416[0m
[37m[1m[2023-07-11 14:40:02,619][233954] Min Reward on eval: -13.619766553808585[0m
[37m[1m[2023-07-11 14:40:02,619][233954] Mean Reward across all agents: 115.84332510574012[0m
[37m[1m[2023-07-11 14:40:02,620][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:40:02,624][233954] mean_value=-5.172971602384047, max_value=508.00886151796715[0m
[37m[1m[2023-07-11 14:40:02,627][233954] New mean coefficients: [[-0.07347313  0.00856135 -0.0016445   0.04072588  0.01328509 -0.48208216]][0m
[37m[1m[2023-07-11 14:40:02,628][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:40:11,571][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 14:40:11,571][233954] FPS: 429468.28[0m
[36m[2023-07-11 14:40:11,573][233954] itr=1071, itrs=2000, Progress: 53.55%[0m
[36m[2023-07-11 14:40:23,121][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 14:40:23,122][233954] FPS: 335285.21[0m
[36m[2023-07-11 14:40:27,331][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:40:27,336][233954] Reward + Measures: [[117.63845525   0.57694167   0.75717002   0.71389598   0.80515593
    0.73936278]][0m
[37m[1m[2023-07-11 14:40:27,336][233954] Max Reward on eval: 117.63845525104607[0m
[37m[1m[2023-07-11 14:40:27,337][233954] Min Reward on eval: 117.63845525104607[0m
[37m[1m[2023-07-11 14:40:27,337][233954] Mean Reward across all agents: 117.63845525104607[0m
[37m[1m[2023-07-11 14:40:27,337][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:40:32,292][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:40:32,293][233954] Reward + Measures: [[ 99.46246674   0.64379996   0.96069998   0.83719999   0.93720007
    0.8949194 ]
 [166.69488296   0.62580001   0.93000001   0.85570002   0.93540001
    0.92423886]
 [ 10.6001565    0.70839995   0.68469995   0.65829998   0.75660002
    0.35863924]
 ...
 [199.60898861   0.35310003   0.7094       0.66149998   0.75940001
    1.2965883 ]
 [129.0798519    0.59200001   0.79500002   0.72409999   0.83759993
    0.72659022]
 [  7.77904536   0.66259998   0.71530002   0.67180002   0.69880003
    0.49321255]][0m
[37m[1m[2023-07-11 14:40:32,293][233954] Max Reward on eval: 342.1042877702042[0m
[37m[1m[2023-07-11 14:40:32,293][233954] Min Reward on eval: -12.007444681727794[0m
[37m[1m[2023-07-11 14:40:32,293][233954] Mean Reward across all agents: 118.21515276052673[0m
[37m[1m[2023-07-11 14:40:32,294][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:40:32,298][233954] mean_value=-10.357753587979559, max_value=571.4327824113634[0m
[37m[1m[2023-07-11 14:40:32,301][233954] New mean coefficients: [[-0.05586717 -0.01335013 -0.00457279  0.03404259  0.03371066 -0.38762757]][0m
[37m[1m[2023-07-11 14:40:32,302][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:40:41,295][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 14:40:41,295][233954] FPS: 427076.55[0m
[36m[2023-07-11 14:40:41,297][233954] itr=1072, itrs=2000, Progress: 53.60%[0m
[36m[2023-07-11 14:40:53,045][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 14:40:53,045][233954] FPS: 329538.96[0m
[36m[2023-07-11 14:40:57,274][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:40:57,274][233954] Reward + Measures: [[115.00209426   0.56605631   0.74504101   0.69629067   0.795138
    0.73067665]][0m
[37m[1m[2023-07-11 14:40:57,275][233954] Max Reward on eval: 115.00209426477842[0m
[37m[1m[2023-07-11 14:40:57,275][233954] Min Reward on eval: 115.00209426477842[0m
[37m[1m[2023-07-11 14:40:57,275][233954] Mean Reward across all agents: 115.00209426477842[0m
[37m[1m[2023-07-11 14:40:57,275][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:41:02,523][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:41:02,524][233954] Reward + Measures: [[ 58.66746603   0.6627       0.71039999   0.74490005   0.79759997
    0.52878428]
 [ 63.39652982   0.53369999   0.62330002   0.5873       0.69129997
    0.68351549]
 [  6.80929286   0.67740005   0.68040001   0.63810009   0.74139994
    0.41618034]
 ...
 [ 56.29726809   0.66760004   0.76459998   0.72130007   0.80709994
    0.59001291]
 [186.30538141   0.51480001   0.7816       0.71819997   0.85159999
    0.91215193]
 [152.59350447   0.42340001   0.6261       0.57919997   0.67890006
    0.88295013]][0m
[37m[1m[2023-07-11 14:41:02,524][233954] Max Reward on eval: 318.21690216334537[0m
[37m[1m[2023-07-11 14:41:02,524][233954] Min Reward on eval: -13.676902474206873[0m
[37m[1m[2023-07-11 14:41:02,525][233954] Mean Reward across all agents: 114.74649966986283[0m
[37m[1m[2023-07-11 14:41:02,525][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:41:02,530][233954] mean_value=-2.4393918044305396, max_value=495.4245842364537[0m
[37m[1m[2023-07-11 14:41:02,533][233954] New mean coefficients: [[-0.05921029 -0.0087885  -0.02844329  0.01302558  0.02081316 -0.37932426]][0m
[37m[1m[2023-07-11 14:41:02,534][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:41:11,577][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 14:41:11,577][233954] FPS: 424707.04[0m
[36m[2023-07-11 14:41:11,580][233954] itr=1073, itrs=2000, Progress: 53.65%[0m
[36m[2023-07-11 14:41:23,362][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 14:41:23,363][233954] FPS: 328561.21[0m
[36m[2023-07-11 14:41:27,445][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:41:27,446][233954] Reward + Measures: [[117.39576709   0.55474168   0.73892236   0.68708968   0.79168475
    0.73106325]][0m
[37m[1m[2023-07-11 14:41:27,446][233954] Max Reward on eval: 117.39576709013133[0m
[37m[1m[2023-07-11 14:41:27,446][233954] Min Reward on eval: 117.39576709013133[0m
[37m[1m[2023-07-11 14:41:27,446][233954] Mean Reward across all agents: 117.39576709013133[0m
[37m[1m[2023-07-11 14:41:27,447][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:41:32,179][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:41:32,180][233954] Reward + Measures: [[192.02531937   0.62770003   0.96789998   0.84829998   0.954
    0.90830129]
 [191.87744367   0.48130003   0.69120002   0.69699997   0.82770008
    0.9210481 ]
 [147.10668232   0.66650003   0.78280002   0.81800002   0.88630003
    0.75949484]
 ...
 [118.22711912   0.54120004   0.53470004   0.57819998   0.72150004
    0.63110846]
 [248.31653412   0.23619998   0.61339998   0.50960004   0.70240003
    1.20364785]
 [ 66.37608479   0.75319999   0.77679998   0.81870002   0.87479991
    0.50104135]][0m
[37m[1m[2023-07-11 14:41:32,180][233954] Max Reward on eval: 294.07435011921916[0m
[37m[1m[2023-07-11 14:41:32,180][233954] Min Reward on eval: -22.80407473021187[0m
[37m[1m[2023-07-11 14:41:32,180][233954] Mean Reward across all agents: 115.31991768346178[0m
[37m[1m[2023-07-11 14:41:32,181][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:41:32,185][233954] mean_value=-3.220599867440263, max_value=600.3402403540696[0m
[37m[1m[2023-07-11 14:41:32,188][233954] New mean coefficients: [[-0.06495676 -0.00848381  0.01571663  0.01676183  0.03040529 -0.35249168]][0m
[37m[1m[2023-07-11 14:41:32,189][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:41:40,736][233954] train() took 8.54 seconds to complete[0m
[36m[2023-07-11 14:41:40,736][233954] FPS: 449395.19[0m
[36m[2023-07-11 14:41:40,738][233954] itr=1074, itrs=2000, Progress: 53.70%[0m
[36m[2023-07-11 14:41:52,546][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 14:41:52,546][233954] FPS: 327898.62[0m
[36m[2023-07-11 14:41:56,808][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:41:56,808][233954] Reward + Measures: [[107.16133733   0.55417931   0.70664304   0.66617298   0.77659798
    0.6933434 ]][0m
[37m[1m[2023-07-11 14:41:56,809][233954] Max Reward on eval: 107.16133732633557[0m
[37m[1m[2023-07-11 14:41:56,809][233954] Min Reward on eval: 107.16133732633557[0m
[37m[1m[2023-07-11 14:41:56,809][233954] Mean Reward across all agents: 107.16133732633557[0m
[37m[1m[2023-07-11 14:41:56,809][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:42:01,809][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:42:01,870][233954] Reward + Measures: [[228.11720067   0.37420002   0.78060001   0.68610001   0.81120008
    1.14457643]
 [ 43.88490726   0.75929999   0.86920005   0.81010002   0.88700002
    0.46864986]
 [187.27937451   0.57450002   0.87559998   0.80489999   0.88840008
    0.88521814]
 ...
 [236.16412944   0.39000002   0.81739998   0.72960001   0.80430013
    1.03809297]
 [141.73820746   0.40279999   0.51230001   0.46290001   0.68190002
    0.73136955]
 [ 10.59361627   0.55690002   0.49429998   0.47340003   0.64169997
    0.34723583]][0m
[37m[1m[2023-07-11 14:42:01,870][233954] Max Reward on eval: 328.1939596289652[0m
[37m[1m[2023-07-11 14:42:01,870][233954] Min Reward on eval: -40.38180060801096[0m
[37m[1m[2023-07-11 14:42:01,870][233954] Mean Reward across all agents: 91.79961239823415[0m
[37m[1m[2023-07-11 14:42:01,871][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:42:01,875][233954] mean_value=-17.731632605060422, max_value=360.6728210026225[0m
[37m[1m[2023-07-11 14:42:01,878][233954] New mean coefficients: [[-0.05882233 -0.03113719  0.00128299  0.02637453  0.04987244 -0.4376003 ]][0m
[37m[1m[2023-07-11 14:42:01,879][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:42:10,862][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 14:42:10,862][233954] FPS: 427577.83[0m
[36m[2023-07-11 14:42:10,865][233954] itr=1075, itrs=2000, Progress: 53.75%[0m
[36m[2023-07-11 14:42:22,583][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 14:42:22,584][233954] FPS: 330370.01[0m
[36m[2023-07-11 14:42:26,847][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:42:26,847][233954] Reward + Measures: [[115.1722388    0.51214331   0.66826236   0.62641805   0.74851763
    0.71791822]][0m
[37m[1m[2023-07-11 14:42:26,847][233954] Max Reward on eval: 115.17223879670712[0m
[37m[1m[2023-07-11 14:42:26,848][233954] Min Reward on eval: 115.17223879670712[0m
[37m[1m[2023-07-11 14:42:26,848][233954] Mean Reward across all agents: 115.17223879670712[0m
[37m[1m[2023-07-11 14:42:26,848][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:42:31,837][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:42:31,837][233954] Reward + Measures: [[165.68019521   0.49070001   0.80859995   0.71020001   0.81440002
    0.87135237]
 [ 56.62139029   0.63999999   0.68290001   0.64550006   0.81380004
    0.47185364]
 [ 59.80385516   0.74870002   0.86709994   0.81409997   0.87140006
    0.46983752]
 ...
 [ 50.61516288   0.45759997   0.52700001   0.47790003   0.65860003
    0.63062125]
 [190.56877694   0.36579999   0.60180002   0.51330006   0.73380005
    0.89125484]
 [208.57206547   0.40550002   0.7105       0.60729998   0.7324
    0.98294801]][0m
[37m[1m[2023-07-11 14:42:31,838][233954] Max Reward on eval: 313.0027625766117[0m
[37m[1m[2023-07-11 14:42:31,838][233954] Min Reward on eval: -51.635434871527835[0m
[37m[1m[2023-07-11 14:42:31,838][233954] Mean Reward across all agents: 98.40886170834463[0m
[37m[1m[2023-07-11 14:42:31,838][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:42:31,844][233954] mean_value=-7.352010527782004, max_value=279.78327898037156[0m
[37m[1m[2023-07-11 14:42:31,846][233954] New mean coefficients: [[-0.07813413 -0.0253999   0.02515967  0.04645472  0.0536041  -0.32819378]][0m
[37m[1m[2023-07-11 14:42:31,847][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:42:40,860][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 14:42:40,860][233954] FPS: 426156.40[0m
[36m[2023-07-11 14:42:40,862][233954] itr=1076, itrs=2000, Progress: 53.80%[0m
[36m[2023-07-11 14:42:52,856][233954] train() took 11.90 seconds to complete[0m
[36m[2023-07-11 14:42:52,856][233954] FPS: 322741.75[0m
[36m[2023-07-11 14:42:57,077][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:42:57,077][233954] Reward + Measures: [[116.06938809   0.53682601   0.68980372   0.64806736   0.77522135
    0.70374447]][0m
[37m[1m[2023-07-11 14:42:57,078][233954] Max Reward on eval: 116.06938809219864[0m
[37m[1m[2023-07-11 14:42:57,078][233954] Min Reward on eval: 116.06938809219864[0m
[37m[1m[2023-07-11 14:42:57,078][233954] Mean Reward across all agents: 116.06938809219864[0m
[37m[1m[2023-07-11 14:42:57,078][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:43:02,080][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:43:02,080][233954] Reward + Measures: [[ 79.25044928   0.4619       0.52540004   0.4804       0.6426
    0.53705549]
 [150.12453429   0.34170002   0.53360003   0.47259998   0.62129998
    0.87296677]
 [ 22.88113557   0.52790004   0.52869999   0.4849       0.63160002
    0.51788914]
 ...
 [ 67.82122156   0.76490003   0.86289996   0.8186       0.89349997
    0.45678741]
 [197.80566486   0.46799999   0.72120005   0.70190006   0.77160001
    1.04871774]
 [  4.1508028    0.83030003   0.79720002   0.8488       0.83939999
    0.29012406]][0m
[37m[1m[2023-07-11 14:43:02,080][233954] Max Reward on eval: 316.99128585946744[0m
[37m[1m[2023-07-11 14:43:02,081][233954] Min Reward on eval: -55.39437429213431[0m
[37m[1m[2023-07-11 14:43:02,081][233954] Mean Reward across all agents: 109.03936005971781[0m
[37m[1m[2023-07-11 14:43:02,081][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:43:02,087][233954] mean_value=-4.4553790999630865, max_value=647.3388204484596[0m
[37m[1m[2023-07-11 14:43:02,090][233954] New mean coefficients: [[-0.05456329 -0.03083394  0.05343856  0.04284628  0.0695281  -0.2783294 ]][0m
[37m[1m[2023-07-11 14:43:02,090][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:43:11,081][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 14:43:11,081][233954] FPS: 427210.88[0m
[36m[2023-07-11 14:43:11,083][233954] itr=1077, itrs=2000, Progress: 53.85%[0m
[36m[2023-07-11 14:43:22,827][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 14:43:22,827][233954] FPS: 329753.05[0m
[36m[2023-07-11 14:43:27,049][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:43:27,050][233954] Reward + Measures: [[112.1078058    0.54942632   0.69055337   0.656982     0.77731675
    0.69336402]][0m
[37m[1m[2023-07-11 14:43:27,050][233954] Max Reward on eval: 112.10780579816162[0m
[37m[1m[2023-07-11 14:43:27,050][233954] Min Reward on eval: 112.10780579816162[0m
[37m[1m[2023-07-11 14:43:27,050][233954] Mean Reward across all agents: 112.10780579816162[0m
[37m[1m[2023-07-11 14:43:27,051][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:43:31,984][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:43:31,984][233954] Reward + Measures: [[105.73430124   0.77150005   0.78280002   0.82510006   0.90690005
    0.63016868]
 [ 57.76453603   0.68090004   0.77540004   0.73120004   0.82320005
    0.64333838]
 [ 12.04240261   0.3468       0.34909999   0.33300003   0.41879997
    0.62820238]
 ...
 [119.19375605   0.4217       0.62250006   0.53840005   0.69480002
    0.78006577]
 [ 65.35158772   0.7744       0.86669999   0.82410002   0.90109998
    0.48628256]
 [  0.25853176   0.67339998   0.59530002   0.64470005   0.72160006
    0.49256048]][0m
[37m[1m[2023-07-11 14:43:31,985][233954] Max Reward on eval: 380.5886412926135[0m
[37m[1m[2023-07-11 14:43:31,985][233954] Min Reward on eval: -55.142884831107224[0m
[37m[1m[2023-07-11 14:43:31,985][233954] Mean Reward across all agents: 89.13689897566486[0m
[37m[1m[2023-07-11 14:43:31,985][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:43:31,991][233954] mean_value=-12.843046388294324, max_value=563.0629559764623[0m
[37m[1m[2023-07-11 14:43:31,993][233954] New mean coefficients: [[-0.03690222 -0.01027093  0.02762695  0.03525621  0.06899127 -0.32375684]][0m
[37m[1m[2023-07-11 14:43:31,994][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:43:40,935][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 14:43:40,940][233954] FPS: 429591.89[0m
[36m[2023-07-11 14:43:40,943][233954] itr=1078, itrs=2000, Progress: 53.90%[0m
[36m[2023-07-11 14:43:52,575][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 14:43:52,575][233954] FPS: 332937.04[0m
[36m[2023-07-11 14:43:56,830][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:43:56,831][233954] Reward + Measures: [[107.74991946   0.55279899   0.691378     0.652327     0.78041095
    0.68063664]][0m
[37m[1m[2023-07-11 14:43:56,831][233954] Max Reward on eval: 107.7499194551978[0m
[37m[1m[2023-07-11 14:43:56,831][233954] Min Reward on eval: 107.7499194551978[0m
[37m[1m[2023-07-11 14:43:56,832][233954] Mean Reward across all agents: 107.7499194551978[0m
[37m[1m[2023-07-11 14:43:56,832][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:44:02,093][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:44:02,094][233954] Reward + Measures: [[121.6832299    0.66250008   0.86969995   0.79630005   0.88199997
    0.7032221 ]
 [ 73.40317309   0.44310004   0.53529996   0.49060002   0.63000005
    0.55400598]
 [ 91.51787431   0.60910004   0.6825       0.64319998   0.77780002
    0.63730228]
 ...
 [ 73.49493919   0.45909998   0.54190004   0.47890002   0.66850001
    0.79080504]
 [204.97244747   0.49560004   0.6965       0.70370001   0.83460009
    0.96725768]
 [168.45566295   0.55199999   0.89539999   0.81470007   0.86130011
    0.86846411]][0m
[37m[1m[2023-07-11 14:44:02,094][233954] Max Reward on eval: 372.58908381231595[0m
[37m[1m[2023-07-11 14:44:02,094][233954] Min Reward on eval: -49.713799982413185[0m
[37m[1m[2023-07-11 14:44:02,094][233954] Mean Reward across all agents: 94.71673255130062[0m
[37m[1m[2023-07-11 14:44:02,094][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:44:02,099][233954] mean_value=-10.891594543212307, max_value=648.5444360661377[0m
[37m[1m[2023-07-11 14:44:02,102][233954] New mean coefficients: [[-0.05553333 -0.00006128 -0.01026372  0.01927432  0.04445285 -0.4410845 ]][0m
[37m[1m[2023-07-11 14:44:02,103][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:44:11,037][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 14:44:11,043][233954] FPS: 429895.89[0m
[36m[2023-07-11 14:44:11,049][233954] itr=1079, itrs=2000, Progress: 53.95%[0m
[36m[2023-07-11 14:44:22,828][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 14:44:22,828][233954] FPS: 329000.91[0m
[36m[2023-07-11 14:44:27,161][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:44:27,162][233954] Reward + Measures: [[95.30078882  0.56720704  0.69470572  0.65419734  0.77406329  0.63426393]][0m
[37m[1m[2023-07-11 14:44:27,162][233954] Max Reward on eval: 95.30078881901153[0m
[37m[1m[2023-07-11 14:44:27,162][233954] Min Reward on eval: 95.30078881901153[0m
[37m[1m[2023-07-11 14:44:27,162][233954] Mean Reward across all agents: 95.30078881901153[0m
[37m[1m[2023-07-11 14:44:27,163][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:44:32,172][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:44:32,172][233954] Reward + Measures: [[ 58.32018837   0.23280001   0.35069999   0.31780002   0.4039
    0.86718911]
 [ 70.28924537   0.35590002   0.47610003   0.44239998   0.53190005
    0.63131016]
 [ 72.34563423   0.58270001   0.59650004   0.55830002   0.7902
    0.46508113]
 ...
 [ 79.96690897   0.54320002   0.59240001   0.55669999   0.71350002
    0.48732695]
 [118.57406962   0.40249997   0.59729999   0.5323       0.67070001
    0.93619311]
 [ 75.63976941   0.51200002   0.50319999   0.46739998   0.7353
    0.49968025]][0m
[37m[1m[2023-07-11 14:44:32,172][233954] Max Reward on eval: 312.9957258346491[0m
[37m[1m[2023-07-11 14:44:32,173][233954] Min Reward on eval: -79.51361016257434[0m
[37m[1m[2023-07-11 14:44:32,173][233954] Mean Reward across all agents: 104.27790261213008[0m
[37m[1m[2023-07-11 14:44:32,173][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:44:32,178][233954] mean_value=-18.825588170289514, max_value=656.1233639120823[0m
[37m[1m[2023-07-11 14:44:32,180][233954] New mean coefficients: [[-0.0531441  -0.00907438 -0.01179923  0.03244359  0.07815099 -0.25728363]][0m
[37m[1m[2023-07-11 14:44:32,181][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:44:41,181][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 14:44:41,181][233954] FPS: 426761.99[0m
[36m[2023-07-11 14:44:41,184][233954] itr=1080, itrs=2000, Progress: 54.00%[0m
[37m[1m[2023-07-11 14:48:14,630][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001060[0m
[36m[2023-07-11 14:48:26,979][233954] train() took 11.75 seconds to complete[0m
[36m[2023-07-11 14:48:26,980][233954] FPS: 326795.79[0m
[36m[2023-07-11 14:48:31,164][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:48:31,165][233954] Reward + Measures: [[93.92248061  0.57372296  0.69929403  0.65935832  0.77584499  0.62947279]][0m
[37m[1m[2023-07-11 14:48:31,165][233954] Max Reward on eval: 93.92248060711161[0m
[37m[1m[2023-07-11 14:48:31,165][233954] Min Reward on eval: 93.92248060711161[0m
[37m[1m[2023-07-11 14:48:31,166][233954] Mean Reward across all agents: 93.92248060711161[0m
[37m[1m[2023-07-11 14:48:31,166][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:48:36,089][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:48:36,089][233954] Reward + Measures: [[178.04462668   0.29879999   0.52880001   0.45229998   0.68099999
    0.92517895]
 [ 47.02479883   0.5607       0.74379998   0.67309999   0.68559998
    0.65512079]
 [ 52.04698486   0.68590003   0.7841       0.75010008   0.81940001
    0.52833694]
 ...
 [ 63.28618972   0.68610001   0.77209997   0.72220004   0.82870001
    0.48718381]
 [ 62.30237752   0.76730007   0.87029999   0.80940002   0.8908
    0.47920379]
 [ 90.25937248   0.5557       0.7191       0.64749998   0.73769999
    0.6883921 ]][0m
[37m[1m[2023-07-11 14:48:36,089][233954] Max Reward on eval: 420.3906669189222[0m
[37m[1m[2023-07-11 14:48:36,090][233954] Min Reward on eval: -61.2752262338181[0m
[37m[1m[2023-07-11 14:48:36,090][233954] Mean Reward across all agents: 74.44241843396864[0m
[37m[1m[2023-07-11 14:48:36,090][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:48:36,094][233954] mean_value=-18.219272094867666, max_value=544.3105477642862[0m
[37m[1m[2023-07-11 14:48:36,097][233954] New mean coefficients: [[-0.04742207 -0.01773032  0.04017583  0.03487924  0.07310595 -0.2976286 ]][0m
[37m[1m[2023-07-11 14:48:36,098][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:48:45,094][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 14:48:45,094][233954] FPS: 426937.25[0m
[36m[2023-07-11 14:48:45,096][233954] itr=1081, itrs=2000, Progress: 54.05%[0m
[36m[2023-07-11 14:48:56,660][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 14:48:56,661][233954] FPS: 334763.37[0m
[36m[2023-07-11 14:49:00,860][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:49:00,860][233954] Reward + Measures: [[104.17661196   0.57615066   0.7213133    0.66930437   0.79346037
    0.65269178]][0m
[37m[1m[2023-07-11 14:49:00,860][233954] Max Reward on eval: 104.17661195942195[0m
[37m[1m[2023-07-11 14:49:00,861][233954] Min Reward on eval: 104.17661195942195[0m
[37m[1m[2023-07-11 14:49:00,861][233954] Mean Reward across all agents: 104.17661195942195[0m
[37m[1m[2023-07-11 14:49:00,861][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:49:05,840][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:49:05,840][233954] Reward + Measures: [[137.96115907   0.65110004   0.78789997   0.79479998   0.85210001
    0.943165  ]
 [121.31663544   0.57599998   0.78850001   0.70920002   0.79899997
    0.75984746]
 [ 26.22410288   0.76789999   0.78200001   0.72350001   0.84079999
    0.33083889]
 ...
 [ 57.27139264   0.5474       0.50310004   0.5467       0.71960002
    0.51321596]
 [204.53850581   0.59170002   0.73050004   0.74470001   0.86870003
    0.91820633]
 [ -3.56514829   0.85200006   0.86860001   0.83020002   0.87950003
    0.30960733]][0m
[37m[1m[2023-07-11 14:49:05,841][233954] Max Reward on eval: 439.83312368805053[0m
[37m[1m[2023-07-11 14:49:05,841][233954] Min Reward on eval: -43.972016722848636[0m
[37m[1m[2023-07-11 14:49:05,841][233954] Mean Reward across all agents: 97.64365289046509[0m
[37m[1m[2023-07-11 14:49:05,841][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:49:05,846][233954] mean_value=-9.46233693332578, max_value=555.4312289693257[0m
[37m[1m[2023-07-11 14:49:05,849][233954] New mean coefficients: [[-0.05328283 -0.00888612  0.04964826  0.01961098  0.07021706 -0.2498264 ]][0m
[37m[1m[2023-07-11 14:49:05,850][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:49:14,810][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 14:49:14,811][233954] FPS: 428639.84[0m
[36m[2023-07-11 14:49:14,813][233954] itr=1082, itrs=2000, Progress: 54.10%[0m
[36m[2023-07-11 14:49:26,415][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 14:49:26,416][233954] FPS: 333783.36[0m
[36m[2023-07-11 14:49:30,670][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:49:30,670][233954] Reward + Measures: [[47.90910173  0.63845897  0.68227369  0.64274001  0.76797998  0.47928929]][0m
[37m[1m[2023-07-11 14:49:30,671][233954] Max Reward on eval: 47.90910173432755[0m
[37m[1m[2023-07-11 14:49:30,671][233954] Min Reward on eval: 47.90910173432755[0m
[37m[1m[2023-07-11 14:49:30,671][233954] Mean Reward across all agents: 47.90910173432755[0m
[37m[1m[2023-07-11 14:49:30,671][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:49:35,701][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:49:35,702][233954] Reward + Measures: [[32.74471999  0.57880002  0.62460005  0.55419999  0.65630001  0.47101289]
 [-0.94655408  0.60540003  0.70180005  0.61080003  0.68850005  0.65425235]
 [ 7.10120698  0.79799998  0.7827      0.79560006  0.87130004  0.47632924]
 ...
 [ 1.81738509  0.60339999  0.634       0.59799999  0.63730001  0.42881128]
 [20.11244793  0.55080003  0.51820004  0.47729999  0.68169999  0.50428343]
 [ 8.04884682  0.83260006  0.86330003  0.80849999  0.88690007  0.36810592]][0m
[37m[1m[2023-07-11 14:49:35,702][233954] Max Reward on eval: 375.5858930833521[0m
[37m[1m[2023-07-11 14:49:35,702][233954] Min Reward on eval: -49.46626365524717[0m
[37m[1m[2023-07-11 14:49:35,702][233954] Mean Reward across all agents: 38.015707454787226[0m
[37m[1m[2023-07-11 14:49:35,703][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:49:35,707][233954] mean_value=-16.885240841542664, max_value=472.51518290273265[0m
[37m[1m[2023-07-11 14:49:35,709][233954] New mean coefficients: [[-0.06605686 -0.02041514  0.02523791 -0.01711879  0.04472139 -0.3034407 ]][0m
[37m[1m[2023-07-11 14:49:35,710][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:49:44,837][233954] train() took 9.12 seconds to complete[0m
[36m[2023-07-11 14:49:44,837][233954] FPS: 420841.26[0m
[36m[2023-07-11 14:49:44,839][233954] itr=1083, itrs=2000, Progress: 54.15%[0m
[36m[2023-07-11 14:49:56,366][233954] train() took 11.43 seconds to complete[0m
[36m[2023-07-11 14:49:56,366][233954] FPS: 336038.40[0m
[36m[2023-07-11 14:50:00,651][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:50:00,651][233954] Reward + Measures: [[16.63210153  0.69468302  0.68740004  0.65478301  0.77149034  0.37117708]][0m
[37m[1m[2023-07-11 14:50:00,651][233954] Max Reward on eval: 16.632101527727308[0m
[37m[1m[2023-07-11 14:50:00,652][233954] Min Reward on eval: 16.632101527727308[0m
[37m[1m[2023-07-11 14:50:00,652][233954] Mean Reward across all agents: 16.632101527727308[0m
[37m[1m[2023-07-11 14:50:00,652][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:50:05,928][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:50:05,929][233954] Reward + Measures: [[57.10530756  0.59700006  0.60210007  0.63330001  0.74680001  0.60857123]
 [23.17919985  0.6674      0.60820001  0.56709999  0.75699997  0.38370523]
 [30.46204407  0.65869999  0.71450001  0.65710002  0.74780005  0.37229595]
 ...
 [-0.60139007  0.83719999  0.85049993  0.8132      0.87830001  0.30382898]
 [18.078524    0.65139997  0.71209997  0.66710001  0.71600002  0.42209044]
 [11.00785219  0.65869999  0.70290005  0.65600008  0.72000003  0.37900764]][0m
[37m[1m[2023-07-11 14:50:05,929][233954] Max Reward on eval: 323.18841141992016[0m
[37m[1m[2023-07-11 14:50:05,929][233954] Min Reward on eval: -58.256006166932636[0m
[37m[1m[2023-07-11 14:50:05,930][233954] Mean Reward across all agents: 30.051256311709462[0m
[37m[1m[2023-07-11 14:50:05,930][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:50:05,934][233954] mean_value=-15.891218567553855, max_value=374.34713626584085[0m
[37m[1m[2023-07-11 14:50:05,936][233954] New mean coefficients: [[-0.06539815 -0.01308679  0.00750539 -0.02163555  0.03712291 -0.2809088 ]][0m
[37m[1m[2023-07-11 14:50:05,937][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:50:15,070][233954] train() took 9.13 seconds to complete[0m
[36m[2023-07-11 14:50:15,071][233954] FPS: 420525.54[0m
[36m[2023-07-11 14:50:15,073][233954] itr=1084, itrs=2000, Progress: 54.20%[0m
[36m[2023-07-11 14:50:26,808][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 14:50:26,808][233954] FPS: 329914.74[0m
[36m[2023-07-11 14:50:31,121][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:50:31,121][233954] Reward + Measures: [[11.98291892  0.68815362  0.6733523   0.64475602  0.76191503  0.36676615]][0m
[37m[1m[2023-07-11 14:50:31,122][233954] Max Reward on eval: 11.982918918857132[0m
[37m[1m[2023-07-11 14:50:31,122][233954] Min Reward on eval: 11.982918918857132[0m
[37m[1m[2023-07-11 14:50:31,122][233954] Mean Reward across all agents: 11.982918918857132[0m
[37m[1m[2023-07-11 14:50:31,122][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:50:36,110][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:50:36,110][233954] Reward + Measures: [[ 0.11940928  0.75240004  0.78490001  0.74009997  0.81709999  0.52930468]
 [ 6.08291395  0.61809999  0.65140003  0.59819996  0.67810005  0.37438998]
 [-7.06042275  0.89639997  0.95300001  0.89630002  0.93169993  0.28604054]
 ...
 [ 9.99096994  0.68740004  0.69680005  0.6627      0.72980005  0.31484047]
 [26.09542661  0.74239999  0.77360004  0.71460003  0.84840006  0.42660281]
 [-0.45635397  0.76159996  0.78279996  0.74040002  0.8136      0.31286541]][0m
[37m[1m[2023-07-11 14:50:36,110][233954] Max Reward on eval: 247.4543977867579[0m
[37m[1m[2023-07-11 14:50:36,111][233954] Min Reward on eval: -57.06581367609324[0m
[37m[1m[2023-07-11 14:50:36,111][233954] Mean Reward across all agents: 27.727067608493634[0m
[37m[1m[2023-07-11 14:50:36,111][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:50:36,115][233954] mean_value=-17.47184334884416, max_value=495.71725046270296[0m
[37m[1m[2023-07-11 14:50:36,117][233954] New mean coefficients: [[-0.07192223 -0.02081854 -0.02090497 -0.0584791   0.05098197 -0.1649448 ]][0m
[37m[1m[2023-07-11 14:50:36,118][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:50:45,169][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 14:50:45,169][233954] FPS: 424351.38[0m
[36m[2023-07-11 14:50:45,172][233954] itr=1085, itrs=2000, Progress: 54.25%[0m
[36m[2023-07-11 14:50:57,098][233954] train() took 11.83 seconds to complete[0m
[36m[2023-07-11 14:50:57,098][233954] FPS: 324705.33[0m
[36m[2023-07-11 14:51:01,383][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:51:01,383][233954] Reward + Measures: [[9.6602882  0.71420264 0.6907903  0.66354764 0.7825917  0.3461225 ]][0m
[37m[1m[2023-07-11 14:51:01,383][233954] Max Reward on eval: 9.660288199026905[0m
[37m[1m[2023-07-11 14:51:01,384][233954] Min Reward on eval: 9.660288199026905[0m
[37m[1m[2023-07-11 14:51:01,384][233954] Mean Reward across all agents: 9.660288199026905[0m
[37m[1m[2023-07-11 14:51:01,384][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:51:06,294][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:51:06,294][233954] Reward + Measures: [[  9.39812115   0.65259999   0.58199996   0.54949999   0.75510001
    0.33289918]
 [ 14.74651043   0.63650006   0.61129999   0.57460004   0.70679998
    0.43932801]
 [  5.09774581   0.53369999   0.53209996   0.49110004   0.60159999
    0.46148768]
 ...
 [ 13.10168499   0.55269998   0.42309999   0.47610003   0.66769999
    0.48142037]
 [-63.3127019    0.7026       0.68750006   0.55300003   0.77559996
    0.50575364]
 [ -2.21239177   0.84780008   0.85589999   0.82490009   0.87900001
    0.25054842]][0m
[37m[1m[2023-07-11 14:51:06,295][233954] Max Reward on eval: 257.5389510577079[0m
[37m[1m[2023-07-11 14:51:06,295][233954] Min Reward on eval: -63.31270190151408[0m
[37m[1m[2023-07-11 14:51:06,295][233954] Mean Reward across all agents: 14.174525622908568[0m
[37m[1m[2023-07-11 14:51:06,295][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:51:06,298][233954] mean_value=-20.23805252420294, max_value=486.1762031138642[0m
[37m[1m[2023-07-11 14:51:06,301][233954] New mean coefficients: [[-0.07448035 -0.02475502 -0.03917658 -0.04549399  0.06260766 -0.0269655 ]][0m
[37m[1m[2023-07-11 14:51:06,302][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:51:15,314][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 14:51:15,315][233954] FPS: 426153.54[0m
[36m[2023-07-11 14:51:15,317][233954] itr=1086, itrs=2000, Progress: 54.30%[0m
[36m[2023-07-11 14:51:27,051][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 14:51:27,052][233954] FPS: 329911.30[0m
[36m[2023-07-11 14:51:31,315][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:51:31,315][233954] Reward + Measures: [[9.17262004 0.7309956  0.70852131 0.68141204 0.79534292 0.34151757]][0m
[37m[1m[2023-07-11 14:51:31,316][233954] Max Reward on eval: 9.172620044739949[0m
[37m[1m[2023-07-11 14:51:31,316][233954] Min Reward on eval: 9.172620044739949[0m
[37m[1m[2023-07-11 14:51:31,316][233954] Mean Reward across all agents: 9.172620044739949[0m
[37m[1m[2023-07-11 14:51:31,316][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:51:36,332][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:51:36,332][233954] Reward + Measures: [[123.22006251   0.69930005   0.68449998   0.73200005   0.84469998
    0.69226563]
 [ -2.03067601   0.625        0.60830003   0.58149999   0.69119996
    0.51475739]
 [  4.78563133   0.80900002   0.77019995   0.73750001   0.87069988
    0.31456289]
 ...
 [  6.80607691   0.71070004   0.68370003   0.64799994   0.79570001
    0.29488665]
 [  3.38874346   0.83809996   0.86590004   0.81409997   0.8865
    0.36184272]
 [  2.84393587   0.85500002   0.86500007   0.82919997   0.89300007
    0.28904003]][0m
[37m[1m[2023-07-11 14:51:36,332][233954] Max Reward on eval: 124.80879880054854[0m
[37m[1m[2023-07-11 14:51:36,333][233954] Min Reward on eval: -40.54084681821987[0m
[37m[1m[2023-07-11 14:51:36,333][233954] Mean Reward across all agents: 8.21626840346483[0m
[37m[1m[2023-07-11 14:51:36,333][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:51:36,336][233954] mean_value=-19.528736981432896, max_value=405.7014788713809[0m
[37m[1m[2023-07-11 14:51:36,339][233954] New mean coefficients: [[-0.09211697 -0.01488784 -0.03585234 -0.0493327   0.04861809 -0.0932712 ]][0m
[37m[1m[2023-07-11 14:51:36,339][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:51:45,409][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 14:51:45,415][233954] FPS: 423457.54[0m
[36m[2023-07-11 14:51:45,418][233954] itr=1087, itrs=2000, Progress: 54.35%[0m
[36m[2023-07-11 14:51:57,118][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 14:51:57,118][233954] FPS: 331003.88[0m
[36m[2023-07-11 14:52:01,387][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:52:01,388][233954] Reward + Measures: [[6.92359239 0.72681296 0.70357096 0.67191869 0.79125196 0.33753285]][0m
[37m[1m[2023-07-11 14:52:01,388][233954] Max Reward on eval: 6.923592393919209[0m
[37m[1m[2023-07-11 14:52:01,388][233954] Min Reward on eval: 6.923592393919209[0m
[37m[1m[2023-07-11 14:52:01,388][233954] Mean Reward across all agents: 6.923592393919209[0m
[37m[1m[2023-07-11 14:52:01,389][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:52:06,349][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:52:06,349][233954] Reward + Measures: [[113.25134228   0.67989999   0.86470002   0.79630005   0.9073
    0.71056169]
 [ 13.67580311   0.74809998   0.68920004   0.65150005   0.8143
    0.28484073]
 [  4.42280772   0.70500004   0.67839998   0.63850003   0.75870001
    0.32768375]
 ...
 [ 17.84123384   0.63609999   0.63140005   0.58969998   0.6918
    0.25656959]
 [ 27.62086679   0.44980001   0.44119999   0.3928       0.53669995
    0.51135105]
 [ -5.1166617    0.82489997   0.78450006   0.82359999   0.84870005
    0.37767178]][0m
[37m[1m[2023-07-11 14:52:06,350][233954] Max Reward on eval: 186.92477620048447[0m
[37m[1m[2023-07-11 14:52:06,350][233954] Min Reward on eval: -62.6273268883815[0m
[37m[1m[2023-07-11 14:52:06,350][233954] Mean Reward across all agents: 14.18520800510538[0m
[37m[1m[2023-07-11 14:52:06,350][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:52:06,353][233954] mean_value=-15.391620683277797, max_value=575.4946941017231[0m
[37m[1m[2023-07-11 14:52:06,356][233954] New mean coefficients: [[-0.09236733 -0.01169585 -0.01315776 -0.05956787  0.0366829  -0.02587266]][0m
[37m[1m[2023-07-11 14:52:06,357][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:52:15,319][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 14:52:15,319][233954] FPS: 428541.82[0m
[36m[2023-07-11 14:52:15,321][233954] itr=1088, itrs=2000, Progress: 54.40%[0m
[36m[2023-07-11 14:52:26,982][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 14:52:26,982][233954] FPS: 332056.36[0m
[36m[2023-07-11 14:52:31,290][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:52:31,290][233954] Reward + Measures: [[5.26994569 0.74396706 0.73017526 0.69506073 0.80093503 0.33508766]][0m
[37m[1m[2023-07-11 14:52:31,290][233954] Max Reward on eval: 5.269945694240368[0m
[37m[1m[2023-07-11 14:52:31,291][233954] Min Reward on eval: 5.269945694240368[0m
[37m[1m[2023-07-11 14:52:31,291][233954] Mean Reward across all agents: 5.269945694240368[0m
[37m[1m[2023-07-11 14:52:31,291][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:52:36,528][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:52:36,528][233954] Reward + Measures: [[ -1.73717831   0.65830004   0.7105       0.64740002   0.70730007
    0.47370419]
 [ -1.49827039   0.7604       0.7676       0.72490007   0.80520004
    0.37880465]
 [ -1.35323154   0.68940002   0.71469998   0.67150003   0.71530002
    0.46703741]
 ...
 [  2.31349334   0.69580001   0.69090003   0.65270007   0.75690001
    0.3595067 ]
 [-77.23797574   0.71719998   0.80880004   0.6189       0.79519999
    0.60242814]
 [-15.74410513   0.85839999   0.86390001   0.829        0.86360008
    0.25797966]][0m
[37m[1m[2023-07-11 14:52:36,529][233954] Max Reward on eval: 72.90659704974387[0m
[37m[1m[2023-07-11 14:52:36,529][233954] Min Reward on eval: -210.96016119485722[0m
[37m[1m[2023-07-11 14:52:36,529][233954] Mean Reward across all agents: -1.4875845704271486[0m
[37m[1m[2023-07-11 14:52:36,529][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:52:36,532][233954] mean_value=-32.65875284936653, max_value=449.1480279309116[0m
[37m[1m[2023-07-11 14:52:36,535][233954] New mean coefficients: [[-0.0894228  -0.01334655 -0.02000736 -0.06454294  0.04833928  0.06198099]][0m
[37m[1m[2023-07-11 14:52:36,536][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:52:45,552][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 14:52:45,552][233954] FPS: 425974.77[0m
[36m[2023-07-11 14:52:45,555][233954] itr=1089, itrs=2000, Progress: 54.45%[0m
[36m[2023-07-11 14:52:57,147][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 14:52:57,147][233954] FPS: 334041.40[0m
[36m[2023-07-11 14:53:01,449][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:53:01,455][233954] Reward + Measures: [[1.55933402 0.75662369 0.74638265 0.70627129 0.81456703 0.34931427]][0m
[37m[1m[2023-07-11 14:53:01,455][233954] Max Reward on eval: 1.5593340232039337[0m
[37m[1m[2023-07-11 14:53:01,456][233954] Min Reward on eval: 1.5593340232039337[0m
[37m[1m[2023-07-11 14:53:01,456][233954] Mean Reward across all agents: 1.5593340232039337[0m
[37m[1m[2023-07-11 14:53:01,456][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:53:06,433][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:53:06,433][233954] Reward + Measures: [[ 10.96523176   0.83590001   0.90259999   0.84499997   0.85519999
    0.24438524]
 [ -1.1437148    0.76730001   0.7762       0.72229999   0.82120001
    0.36885378]
 [-12.33129363   0.66730005   0.67690003   0.63149995   0.7658
    0.48146486]
 ...
 [  8.05948969   0.68849993   0.70659995   0.66040003   0.7349
    0.35745043]
 [ 17.81971984   0.764        0.76960003   0.72220004   0.81779999
    0.38860169]
 [ 11.82191357   0.68180001   0.727        0.67440003   0.71120006
    0.2934939 ]][0m
[37m[1m[2023-07-11 14:53:06,434][233954] Max Reward on eval: 144.89669668071437[0m
[37m[1m[2023-07-11 14:53:06,434][233954] Min Reward on eval: -145.6074923459324[0m
[37m[1m[2023-07-11 14:53:06,434][233954] Mean Reward across all agents: 2.599066315551411[0m
[37m[1m[2023-07-11 14:53:06,434][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:53:06,437][233954] mean_value=-37.05873319847908, max_value=365.5455380029374[0m
[37m[1m[2023-07-11 14:53:06,440][233954] New mean coefficients: [[-0.10558036 -0.00856862  0.00917046 -0.07534382  0.0038088   0.0348531 ]][0m
[37m[1m[2023-07-11 14:53:06,441][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:53:15,459][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 14:53:15,460][233954] FPS: 425869.07[0m
[36m[2023-07-11 14:53:15,462][233954] itr=1090, itrs=2000, Progress: 54.50%[0m
[37m[1m[2023-07-11 14:56:45,151][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001070[0m
[36m[2023-07-11 14:56:57,163][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 14:56:57,163][233954] FPS: 332869.79[0m
[36m[2023-07-11 14:57:01,326][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:57:01,326][233954] Reward + Measures: [[15.51536529  0.90925765  0.82648134  0.86672533  0.90601629  1.50249219]][0m
[37m[1m[2023-07-11 14:57:01,327][233954] Max Reward on eval: 15.51536528685283[0m
[37m[1m[2023-07-11 14:57:01,327][233954] Min Reward on eval: 15.51536528685283[0m
[37m[1m[2023-07-11 14:57:01,327][233954] Mean Reward across all agents: 15.51536528685283[0m
[37m[1m[2023-07-11 14:57:01,327][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:57:06,276][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:57:06,277][233954] Reward + Measures: [[ 45.34508456   0.88050002   0.72960001   0.84499997   0.89000005
    1.42644668]
 [ 37.89975615   0.92389995   0.75670004   0.88319999   0.92449999
    1.50994134]
 [ 18.34589407   0.91750002   0.79400009   0.87859994   0.91530001
    1.46487617]
 ...
 [  8.80632962   0.91149998   0.8545       0.86469996   0.90429991
    1.42985713]
 [ 83.16770932   0.90930003   0.6548       0.866        0.90620005
    1.62166631]
 [-10.33001161   0.9199999    0.90319997   0.87550002   0.91590005
    1.40002167]][0m
[37m[1m[2023-07-11 14:57:06,277][233954] Max Reward on eval: 90.40706778299064[0m
[37m[1m[2023-07-11 14:57:06,277][233954] Min Reward on eval: -36.59817182831466[0m
[37m[1m[2023-07-11 14:57:06,278][233954] Mean Reward across all agents: 11.711951034786257[0m
[37m[1m[2023-07-11 14:57:06,278][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:57:06,281][233954] mean_value=-17.18465068568349, max_value=583.9123701696284[0m
[37m[1m[2023-07-11 14:57:06,284][233954] New mean coefficients: [[-0.11052801  0.01594865  0.13884471 -0.01682731  0.05413081  0.18715796]][0m
[37m[1m[2023-07-11 14:57:06,285][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:57:15,180][233954] train() took 8.89 seconds to complete[0m
[36m[2023-07-11 14:57:15,180][233954] FPS: 431807.94[0m
[36m[2023-07-11 14:57:15,182][233954] itr=1091, itrs=2000, Progress: 54.55%[0m
[36m[2023-07-11 14:57:26,977][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 14:57:26,978][233954] FPS: 328175.01[0m
[36m[2023-07-11 14:57:31,227][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:57:31,227][233954] Reward + Measures: [[-48.44033527   0.72582924   0.82333738   0.66926003   0.78378302
    1.84158492]][0m
[37m[1m[2023-07-11 14:57:31,227][233954] Max Reward on eval: -48.44033526860792[0m
[37m[1m[2023-07-11 14:57:31,228][233954] Min Reward on eval: -48.44033526860792[0m
[37m[1m[2023-07-11 14:57:31,228][233954] Mean Reward across all agents: -48.44033526860792[0m
[37m[1m[2023-07-11 14:57:31,228][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:57:36,198][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:57:36,199][233954] Reward + Measures: [[-60.64713206   0.72030002   0.82670003   0.60409999   0.78740001
    1.7657212 ]
 [ -3.82885992   0.79570001   0.87019998   0.76659995   0.82480001
    2.07864332]
 [ -7.30101672   0.71030003   0.76999998   0.68180001   0.75029999
    1.47185838]
 ...
 [-10.16770274   0.69630003   0.77179998   0.68460006   0.71820003
    1.62253881]
 [-39.1057395    0.68020004   0.77060002   0.62099999   0.7439
    1.7019335 ]
 [ 45.79577999   0.58980006   0.68189996   0.51880002   0.65639997
    1.52866852]][0m
[37m[1m[2023-07-11 14:57:36,199][233954] Max Reward on eval: 53.26017921217717[0m
[37m[1m[2023-07-11 14:57:36,199][233954] Min Reward on eval: -142.70624174932018[0m
[37m[1m[2023-07-11 14:57:36,200][233954] Mean Reward across all agents: -36.25806133546259[0m
[37m[1m[2023-07-11 14:57:36,200][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:57:36,202][233954] mean_value=-135.52885227373395, max_value=170.93278976409147[0m
[37m[1m[2023-07-11 14:57:36,204][233954] New mean coefficients: [[-0.11904936  0.01689613  0.15975676  0.01688804  0.0629015   0.03833553]][0m
[37m[1m[2023-07-11 14:57:36,205][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:57:45,186][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 14:57:45,187][233954] FPS: 427630.63[0m
[36m[2023-07-11 14:57:45,189][233954] itr=1092, itrs=2000, Progress: 54.60%[0m
[36m[2023-07-11 14:57:56,782][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 14:57:56,782][233954] FPS: 334121.63[0m
[36m[2023-07-11 14:58:00,996][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:58:00,997][233954] Reward + Measures: [[-71.65443103   0.80627161   0.88539064   0.72791064   0.84821868
    1.97787416]][0m
[37m[1m[2023-07-11 14:58:00,997][233954] Max Reward on eval: -71.65443102669502[0m
[37m[1m[2023-07-11 14:58:00,997][233954] Min Reward on eval: -71.65443102669502[0m
[37m[1m[2023-07-11 14:58:00,997][233954] Mean Reward across all agents: -71.65443102669502[0m
[37m[1m[2023-07-11 14:58:00,998][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:58:05,963][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:58:05,964][233954] Reward + Measures: [[ -34.82491743    0.85120004    0.88770002    0.82310003    0.86059999
     2.04192209]
 [ -56.29877499    0.83179998    0.88290006    0.76540005    0.84189999
     1.94413877]
 [ -33.0644293     0.8308        0.90550005    0.79100001    0.86320001
     2.00808764]
 ...
 [-105.09535336    0.74290001    0.82950002    0.63620007    0.7931
     1.91961801]
 [-135.55159394    0.84229994    0.94950008    0.73079997    0.91359997
     2.17274332]
 [ -53.62090031    0.82310003    0.875         0.80629998    0.83500004
     1.95068765]][0m
[37m[1m[2023-07-11 14:58:05,964][233954] Max Reward on eval: 8.934506111359223[0m
[37m[1m[2023-07-11 14:58:05,964][233954] Min Reward on eval: -231.12645590249448[0m
[37m[1m[2023-07-11 14:58:05,964][233954] Mean Reward across all agents: -70.46225753259482[0m
[37m[1m[2023-07-11 14:58:05,964][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:58:05,966][233954] mean_value=-174.55905256415372, max_value=227.96827199539905[0m
[37m[1m[2023-07-11 14:58:05,969][233954] New mean coefficients: [[-0.131425   -0.00805982  0.17081837  0.02845788  0.05754049  0.03354789]][0m
[37m[1m[2023-07-11 14:58:05,970][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:58:14,976][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 14:58:14,976][233954] FPS: 426446.21[0m
[36m[2023-07-11 14:58:14,978][233954] itr=1093, itrs=2000, Progress: 54.65%[0m
[36m[2023-07-11 14:58:26,699][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 14:58:26,699][233954] FPS: 330305.27[0m
[36m[2023-07-11 14:58:30,900][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:58:30,901][233954] Reward + Measures: [[-91.40353098   0.82618231   0.90495098   0.71897101   0.87088597
    2.01233387]][0m
[37m[1m[2023-07-11 14:58:30,901][233954] Max Reward on eval: -91.40353098295515[0m
[37m[1m[2023-07-11 14:58:30,901][233954] Min Reward on eval: -91.40353098295515[0m
[37m[1m[2023-07-11 14:58:30,902][233954] Mean Reward across all agents: -91.40353098295515[0m
[37m[1m[2023-07-11 14:58:30,902][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:58:36,142][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:58:36,143][233954] Reward + Measures: [[ -58.17692343    0.89209998    0.95559996    0.82639998    0.92449999
     2.01980948]
 [ -55.62296474    0.91510004    0.95800012    0.87369996    0.91100007
     2.01683927]
 [ -38.26143321    0.75159997    0.85039997    0.70410001    0.76660001
     1.88927114]
 ...
 [ -65.09122185    0.8427        0.93620008    0.76200002    0.87880003
     1.99155176]
 [ -25.44584384    0.8082        0.91800004    0.77930003    0.79760003
     2.12773132]
 [-102.82920307    0.84829998    0.91219997    0.72730005    0.84960002
     1.99596715]][0m
[37m[1m[2023-07-11 14:58:36,143][233954] Max Reward on eval: -6.83907558741048[0m
[37m[1m[2023-07-11 14:58:36,143][233954] Min Reward on eval: -338.2984197875252[0m
[37m[1m[2023-07-11 14:58:36,143][233954] Mean Reward across all agents: -92.38061452615432[0m
[37m[1m[2023-07-11 14:58:36,143][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:58:36,145][233954] mean_value=-185.45989210581544, max_value=111.776941942689[0m
[37m[1m[2023-07-11 14:58:36,148][233954] New mean coefficients: [[-0.16376045 -0.03437246  0.12775283  0.00732883  0.02438085 -0.11924877]][0m
[37m[1m[2023-07-11 14:58:36,149][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:58:45,200][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 14:58:45,200][233954] FPS: 424332.00[0m
[36m[2023-07-11 14:58:45,202][233954] itr=1094, itrs=2000, Progress: 54.70%[0m
[36m[2023-07-11 14:58:57,073][233954] train() took 11.78 seconds to complete[0m
[36m[2023-07-11 14:58:57,073][233954] FPS: 326103.11[0m
[36m[2023-07-11 14:59:01,298][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:59:01,298][233954] Reward + Measures: [[-87.8302157    0.84654933   0.91477603   0.7369523    0.88162071
    1.9997865 ]][0m
[37m[1m[2023-07-11 14:59:01,299][233954] Max Reward on eval: -87.8302156965156[0m
[37m[1m[2023-07-11 14:59:01,299][233954] Min Reward on eval: -87.8302156965156[0m
[37m[1m[2023-07-11 14:59:01,299][233954] Mean Reward across all agents: -87.8302156965156[0m
[37m[1m[2023-07-11 14:59:01,300][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:59:06,302][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:59:06,302][233954] Reward + Measures: [[ -76.57340657    0.93220007    0.97210008    0.90170002    0.91989994
     2.19873571]
 [-111.60456988    0.83010006    0.89419997    0.69230002    0.8125
     2.00892711]
 [ -77.13454424    0.88920003    0.95629996    0.7723        0.91359997
     2.1445241 ]
 ...
 [ -31.98712968    0.8351        0.90429991    0.80030006    0.80929995
     2.03507686]
 [ -76.60428303    0.83520001    0.90710002    0.77530003    0.88090003
     2.19331408]
 [ -80.12359345    0.88520002    0.93129998    0.7938        0.88029999
     2.04307914]][0m
[37m[1m[2023-07-11 14:59:06,303][233954] Max Reward on eval: 12.999775194888935[0m
[37m[1m[2023-07-11 14:59:06,303][233954] Min Reward on eval: -276.3655653081834[0m
[37m[1m[2023-07-11 14:59:06,303][233954] Mean Reward across all agents: -72.8363893318021[0m
[37m[1m[2023-07-11 14:59:06,303][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:59:06,305][233954] mean_value=-187.21057686719448, max_value=174.39131069835645[0m
[37m[1m[2023-07-11 14:59:06,307][233954] New mean coefficients: [[-0.17978674 -0.00239422  0.11947882  0.04651585  0.04601633 -0.1370484 ]][0m
[37m[1m[2023-07-11 14:59:06,308][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:59:15,368][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 14:59:15,368][233954] FPS: 423925.44[0m
[36m[2023-07-11 14:59:15,371][233954] itr=1095, itrs=2000, Progress: 54.75%[0m
[36m[2023-07-11 14:59:27,312][233954] train() took 11.84 seconds to complete[0m
[36m[2023-07-11 14:59:27,312][233954] FPS: 324263.75[0m
[36m[2023-07-11 14:59:31,630][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:59:31,630][233954] Reward + Measures: [[-75.05324054   0.89671725   0.94693536   0.80212241   0.91968697
    1.99069464]][0m
[37m[1m[2023-07-11 14:59:31,630][233954] Max Reward on eval: -75.05324053813989[0m
[37m[1m[2023-07-11 14:59:31,631][233954] Min Reward on eval: -75.05324053813989[0m
[37m[1m[2023-07-11 14:59:31,631][233954] Mean Reward across all agents: -75.05324053813989[0m
[37m[1m[2023-07-11 14:59:31,631][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:59:36,646][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 14:59:36,647][233954] Reward + Measures: [[-166.48639512    0.91709995    0.97439998    0.72049999    0.94890004
     2.09401655]
 [ -81.58599281    0.89390004    0.9479        0.80739993    0.91820002
     2.05353737]
 [ -56.89584041    0.86730003    0.9346        0.81389999    0.90130007
     2.03165221]
 ...
 [ -43.70025674    0.9375        0.97250003    0.90510005    0.94400007
     2.03622508]
 [ -66.16838277    0.93909997    0.96480006    0.86499995    0.93790001
     2.00905085]
 [ -95.41741464    0.77710003    0.8459        0.67440003    0.8023001
     1.97327161]][0m
[37m[1m[2023-07-11 14:59:36,647][233954] Max Reward on eval: 6.1356983974110335[0m
[37m[1m[2023-07-11 14:59:36,647][233954] Min Reward on eval: -302.1430486505618[0m
[37m[1m[2023-07-11 14:59:36,648][233954] Mean Reward across all agents: -77.22200587342192[0m
[37m[1m[2023-07-11 14:59:36,648][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 14:59:36,650][233954] mean_value=-196.86311685605412, max_value=119.31548781554784[0m
[37m[1m[2023-07-11 14:59:36,652][233954] New mean coefficients: [[-0.16647992 -0.00922866  0.11912569  0.03846405  0.04050535  0.0019777 ]][0m
[37m[1m[2023-07-11 14:59:36,653][233954] Moving the mean solution point...[0m
[36m[2023-07-11 14:59:45,735][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 14:59:45,735][233954] FPS: 422889.33[0m
[36m[2023-07-11 14:59:45,738][233954] itr=1096, itrs=2000, Progress: 54.80%[0m
[36m[2023-07-11 14:59:57,676][233954] train() took 11.84 seconds to complete[0m
[36m[2023-07-11 14:59:57,681][233954] FPS: 324242.21[0m
[36m[2023-07-11 15:00:01,969][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:00:01,969][233954] Reward + Measures: [[-76.92550057   0.91202962   0.9546743    0.82835996   0.92912334
    2.0004425 ]][0m
[37m[1m[2023-07-11 15:00:01,969][233954] Max Reward on eval: -76.92550056885156[0m
[37m[1m[2023-07-11 15:00:01,970][233954] Min Reward on eval: -76.92550056885156[0m
[37m[1m[2023-07-11 15:00:01,970][233954] Mean Reward across all agents: -76.92550056885156[0m
[37m[1m[2023-07-11 15:00:01,970][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:00:06,997][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:00:06,998][233954] Reward + Measures: [[-168.38510679    0.81129998    0.81300002    0.60560006    0.83290005
     1.91660154]
 [-139.21982906    0.92010003    0.97360003    0.76179999    0.94569999
     2.13096094]
 [ -35.6642642     0.87629998    0.9127        0.84639996    0.88239998
     1.94911802]
 ...
 [ -31.69255766    0.94419998    0.97390002    0.91790003    0.94690001
     2.04842925]
 [ -64.7915039     0.92060006    0.94470006    0.89650005    0.91920006
     2.03908134]
 [-116.15892507    0.90529996    0.96219999    0.77520001    0.93889999
     2.11573243]][0m
[37m[1m[2023-07-11 15:00:06,998][233954] Max Reward on eval: -13.294844717811793[0m
[37m[1m[2023-07-11 15:00:06,998][233954] Min Reward on eval: -256.3125890554162[0m
[37m[1m[2023-07-11 15:00:06,999][233954] Mean Reward across all agents: -75.4932689263056[0m
[37m[1m[2023-07-11 15:00:06,999][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:00:07,000][233954] mean_value=-213.6105480123958, max_value=275.9442732116207[0m
[37m[1m[2023-07-11 15:00:07,002][233954] New mean coefficients: [[-0.12186927 -0.02730767  0.15203494  0.05240361  0.06288826  0.17429529]][0m
[37m[1m[2023-07-11 15:00:07,003][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:00:15,979][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 15:00:15,979][233954] FPS: 427895.37[0m
[36m[2023-07-11 15:00:15,982][233954] itr=1097, itrs=2000, Progress: 54.85%[0m
[36m[2023-07-11 15:00:27,662][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 15:00:27,667][233954] FPS: 331457.35[0m
[36m[2023-07-11 15:00:32,021][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:00:32,021][233954] Reward + Measures: [[-53.42288292   0.91784161   0.95279896   0.83170199   0.91333073
    2.00034666]][0m
[37m[1m[2023-07-11 15:00:32,021][233954] Max Reward on eval: -53.42288291802391[0m
[37m[1m[2023-07-11 15:00:32,022][233954] Min Reward on eval: -53.42288291802391[0m
[37m[1m[2023-07-11 15:00:32,022][233954] Mean Reward across all agents: -53.42288291802391[0m
[37m[1m[2023-07-11 15:00:32,022][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:00:37,100][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:00:37,101][233954] Reward + Measures: [[ -63.97676099    0.94849998    0.97490007    0.90129995    0.89780009
     2.18240047]
 [-136.65698324    0.90269995    0.97130007    0.72410005    0.91530001
     2.07928777]
 [ -25.03029031    0.89280003    0.92329997    0.83920002    0.87389994
     2.06936884]
 ...
 [ -35.98357235    0.94279999    0.97600001    0.84190005    0.93190002
     1.98047912]
 [-233.468765      0.91210002    0.95670003    0.60579997    0.92100012
     2.1381619 ]
 [-174.31637939    0.91970009    0.96480006    0.68219995    0.91040003
     2.06712151]][0m
[37m[1m[2023-07-11 15:00:37,101][233954] Max Reward on eval: 14.576202984317206[0m
[37m[1m[2023-07-11 15:00:37,102][233954] Min Reward on eval: -369.98217344880106[0m
[37m[1m[2023-07-11 15:00:37,102][233954] Mean Reward across all agents: -72.10064932391467[0m
[37m[1m[2023-07-11 15:00:37,102][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:00:37,104][233954] mean_value=-192.56660824246165, max_value=390.57986704297366[0m
[37m[1m[2023-07-11 15:00:37,106][233954] New mean coefficients: [[-0.13714851 -0.04048941  0.10463464  0.02426454  0.06427917  0.31557512]][0m
[37m[1m[2023-07-11 15:00:37,107][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:00:46,246][233954] train() took 9.14 seconds to complete[0m
[36m[2023-07-11 15:00:46,247][233954] FPS: 420257.75[0m
[36m[2023-07-11 15:00:46,249][233954] itr=1098, itrs=2000, Progress: 54.90%[0m
[36m[2023-07-11 15:00:58,106][233954] train() took 11.76 seconds to complete[0m
[36m[2023-07-11 15:00:58,106][233954] FPS: 326571.95[0m
[36m[2023-07-11 15:01:02,445][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:01:02,446][233954] Reward + Measures: [[-105.47043178    0.87973732    0.9372223     0.83027327    0.82313597
     2.48045087]][0m
[37m[1m[2023-07-11 15:01:02,446][233954] Max Reward on eval: -105.47043178272062[0m
[37m[1m[2023-07-11 15:01:02,446][233954] Min Reward on eval: -105.47043178272062[0m
[37m[1m[2023-07-11 15:01:02,446][233954] Mean Reward across all agents: -105.47043178272062[0m
[37m[1m[2023-07-11 15:01:02,447][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:01:07,449][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:01:07,450][233954] Reward + Measures: [[ -77.33936086    0.86940002    0.91670001    0.84890002    0.7938
     2.40660763]
 [ -70.78397738    0.84849995    0.91450006    0.83570004    0.77890003
     2.41176963]
 [-154.74489191    0.83090001    0.92109996    0.71130002    0.8143999
     2.47578621]
 ...
 [-260.2366557     0.91549999    0.97220004    0.70880002    0.88370001
     2.55138302]
 [ -63.76319993    0.84020007    0.91920006    0.7877        0.83309996
     2.37267518]
 [ -11.17719675    0.90679997    0.94499999    0.89270002    0.87009996
     2.43374634]][0m
[37m[1m[2023-07-11 15:01:07,450][233954] Max Reward on eval: 33.34238123690011[0m
[37m[1m[2023-07-11 15:01:07,450][233954] Min Reward on eval: -385.58478450533005[0m
[37m[1m[2023-07-11 15:01:07,450][233954] Mean Reward across all agents: -85.1095434306007[0m
[37m[1m[2023-07-11 15:01:07,451][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:01:07,452][233954] mean_value=-244.67349627659982, max_value=306.52621397373036[0m
[37m[1m[2023-07-11 15:01:07,455][233954] New mean coefficients: [[-0.15292719 -0.05655584  0.1053364   0.0389168   0.04380454  0.10183053]][0m
[37m[1m[2023-07-11 15:01:07,456][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:01:16,466][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 15:01:16,466][233954] FPS: 426264.29[0m
[36m[2023-07-11 15:01:16,469][233954] itr=1099, itrs=2000, Progress: 54.95%[0m
[36m[2023-07-11 15:01:28,152][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 15:01:28,152][233954] FPS: 331461.43[0m
[36m[2023-07-11 15:01:32,414][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:01:32,414][233954] Reward + Measures: [[-111.46125688    0.90299934    0.95202529    0.8742553     0.83156401
     2.55609703]][0m
[37m[1m[2023-07-11 15:01:32,415][233954] Max Reward on eval: -111.46125688263099[0m
[37m[1m[2023-07-11 15:01:32,415][233954] Min Reward on eval: -111.46125688263099[0m
[37m[1m[2023-07-11 15:01:32,415][233954] Mean Reward across all agents: -111.46125688263099[0m
[37m[1m[2023-07-11 15:01:32,415][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:01:37,645][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:01:37,646][233954] Reward + Measures: [[-104.64513534    0.92480004    0.97500002    0.90980005    0.85039997
     2.64510942]
 [ -96.26585578    0.93339998    0.96870005    0.91530007    0.86229992
     2.57097721]
 [ -70.8994534     0.88140005    0.94349998    0.83880007    0.82940006
     2.48876476]
 ...
 [ -99.47460556    0.88910007    0.94950002    0.8707        0.81400007
     2.55575562]
 [-134.19289493    0.91820002    0.96330005    0.90399998    0.8391
     2.60748267]
 [-159.15624949    0.91129988    0.95920002    0.83400005    0.84549999
     2.57107973]][0m
[37m[1m[2023-07-11 15:01:37,646][233954] Max Reward on eval: -40.01133497748524[0m
[37m[1m[2023-07-11 15:01:37,646][233954] Min Reward on eval: -213.3095101162791[0m
[37m[1m[2023-07-11 15:01:37,646][233954] Mean Reward across all agents: -103.91858551414879[0m
[37m[1m[2023-07-11 15:01:37,647][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:01:37,648][233954] mean_value=-252.39002929294142, max_value=246.43053658441295[0m
[37m[1m[2023-07-11 15:01:37,650][233954] New mean coefficients: [[-0.1456932  -0.08153504  0.07557599 -0.00809891  0.01039631  0.35612053]][0m
[37m[1m[2023-07-11 15:01:37,651][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:01:46,701][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 15:01:46,702][233954] FPS: 424369.64[0m
[36m[2023-07-11 15:01:46,704][233954] itr=1100, itrs=2000, Progress: 55.00%[0m
[37m[1m[2023-07-11 15:05:24,446][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001080[0m
[36m[2023-07-11 15:05:36,703][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 15:05:36,703][233954] FPS: 329072.03[0m
[36m[2023-07-11 15:05:40,913][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:05:40,914][233954] Reward + Measures: [[-113.21528416    0.91583139    0.96068406    0.89852798    0.83364761
     2.63135672]][0m
[37m[1m[2023-07-11 15:05:40,914][233954] Max Reward on eval: -113.21528415922054[0m
[37m[1m[2023-07-11 15:05:40,914][233954] Min Reward on eval: -113.21528415922054[0m
[37m[1m[2023-07-11 15:05:40,915][233954] Mean Reward across all agents: -113.21528415922054[0m
[37m[1m[2023-07-11 15:05:40,915][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:05:45,880][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:05:45,880][233954] Reward + Measures: [[-119.04855107    0.89810002    0.93889999    0.89449996    0.83140004
     2.60592222]
 [ -96.37769173    0.92630005    0.97269994    0.9127        0.824
     2.61302066]
 [-100.15986632    0.90799999    0.9533        0.87880003    0.82989997
     2.61498809]
 ...
 [ -91.37111266    0.87929994    0.95079994    0.86550009    0.81160003
     2.58156013]
 [-128.97359039    0.9471001     0.9806        0.93809998    0.89840001
     2.70713115]
 [-135.49583051    0.94659996    0.97970003    0.93920004    0.85979998
     2.69228816]][0m
[37m[1m[2023-07-11 15:05:45,880][233954] Max Reward on eval: -56.67689861091785[0m
[37m[1m[2023-07-11 15:05:45,881][233954] Min Reward on eval: -206.3578743980266[0m
[37m[1m[2023-07-11 15:05:45,881][233954] Mean Reward across all agents: -109.23914118181764[0m
[37m[1m[2023-07-11 15:05:45,881][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:05:45,883][233954] mean_value=-230.88918992726585, max_value=-16.63662275590619[0m
[36m[2023-07-11 15:05:45,893][233954] XNES is restarting with a new solution whose measures are [0.50430006 0.88210005 0.24490002 0.86210006 3.47957873] and objective is -137.5975685004145[0m
[36m[2023-07-11 15:05:45,921][233954] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-11 15:05:45,924][233954] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-11 15:05:45,925][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:05:54,905][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 15:05:54,905][233954] FPS: 427675.86[0m
[36m[2023-07-11 15:05:54,908][233954] itr=1101, itrs=2000, Progress: 55.05%[0m
[36m[2023-07-11 15:06:06,595][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 15:06:06,595][233954] FPS: 331353.31[0m
[36m[2023-07-11 15:06:10,823][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:06:10,823][233954] Reward + Measures: [[-64.4613725    0.41694766   0.83672071   0.26670766   0.81786937
    3.40476561]][0m
[37m[1m[2023-07-11 15:06:10,823][233954] Max Reward on eval: -64.46137249595562[0m
[37m[1m[2023-07-11 15:06:10,824][233954] Min Reward on eval: -64.46137249595562[0m
[37m[1m[2023-07-11 15:06:10,824][233954] Mean Reward across all agents: -64.46137249595562[0m
[37m[1m[2023-07-11 15:06:10,824][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:06:15,773][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:06:15,774][233954] Reward + Measures: [[  72.02033423    0.33139998    0.80190003    0.31470001    0.77990001
     3.47434163]
 [  67.70738936    0.31040001    0.79189998    0.30230001    0.78839999
     3.57905841]
 [-260.17592668    0.56739998    0.86470002    0.16720001    0.83640003
     3.41267586]
 ...
 [-487.66635562    0.68880004    0.78359997    0.0542        0.77590007
     3.10592151]
 [ 300.01268644    0.0876        0.51800007    0.31350002    0.51810002
     3.25277138]
 [ 257.93963594    0.29660001    0.96160001    0.42389998    0.9375
     3.59931922]][0m
[37m[1m[2023-07-11 15:06:15,774][233954] Max Reward on eval: 656.6025262985379[0m
[37m[1m[2023-07-11 15:06:15,774][233954] Min Reward on eval: -642.1222032006132[0m
[37m[1m[2023-07-11 15:06:15,775][233954] Mean Reward across all agents: 46.287958252891556[0m
[37m[1m[2023-07-11 15:06:15,775][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:06:15,778][233954] mean_value=-210.39009241071864, max_value=341.78411690731684[0m
[37m[1m[2023-07-11 15:06:15,780][233954] New mean coefficients: [[ 0.5920004  -1.3662598  -0.62209696 -2.3441088  -1.517091   -1.1785736 ]][0m
[37m[1m[2023-07-11 15:06:15,781][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:06:24,725][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 15:06:24,725][233954] FPS: 429437.03[0m
[36m[2023-07-11 15:06:24,727][233954] itr=1102, itrs=2000, Progress: 55.10%[0m
[36m[2023-07-11 15:06:36,288][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 15:06:36,288][233954] FPS: 334880.06[0m
[36m[2023-07-11 15:06:40,455][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:06:40,456][233954] Reward + Measures: [[-33.06339329   0.38910934   0.81304735   0.26944998   0.79451698
    3.3908639 ]][0m
[37m[1m[2023-07-11 15:06:40,456][233954] Max Reward on eval: -33.06339329420645[0m
[37m[1m[2023-07-11 15:06:40,456][233954] Min Reward on eval: -33.06339329420645[0m
[37m[1m[2023-07-11 15:06:40,456][233954] Mean Reward across all agents: -33.06339329420645[0m
[37m[1m[2023-07-11 15:06:40,456][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:06:45,493][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:06:45,493][233954] Reward + Measures: [[  85.50820517    0.2994        0.79980004    0.31330001    0.7802
     3.45234418]
 [  -9.98506733    0.493         0.98579997    0.30960003    0.97420007
     3.52492571]
 [ -52.92746447    0.49240002    0.87880003    0.35640001    0.92989999
     3.44773149]
 ...
 [-368.06488166    0.57800001    0.82489997    0.13950001    0.77980006
     3.39676833]
 [ 223.41794972    0.29660001    0.9679001     0.42210004    0.95230001
     3.62538123]
 [ 130.65141389    0.32570001    0.917         0.35450003    0.88360006
     3.56373262]][0m
[37m[1m[2023-07-11 15:06:45,493][233954] Max Reward on eval: 535.2372551146894[0m
[37m[1m[2023-07-11 15:06:45,494][233954] Min Reward on eval: -682.985202825116[0m
[37m[1m[2023-07-11 15:06:45,494][233954] Mean Reward across all agents: 13.09646259561824[0m
[37m[1m[2023-07-11 15:06:45,494][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:06:45,496][233954] mean_value=-211.44237765308796, max_value=207.08919368310652[0m
[37m[1m[2023-07-11 15:06:45,499][233954] New mean coefficients: [[ 0.33063102 -2.081613   -0.6064263  -3.2751765  -1.405626   -0.69841886]][0m
[37m[1m[2023-07-11 15:06:45,500][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:06:54,550][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 15:06:54,550][233954] FPS: 424380.98[0m
[36m[2023-07-11 15:06:54,552][233954] itr=1103, itrs=2000, Progress: 55.15%[0m
[36m[2023-07-11 15:07:06,159][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 15:07:06,159][233954] FPS: 333675.98[0m
[36m[2023-07-11 15:07:10,441][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:07:10,442][233954] Reward + Measures: [[-38.11075915   0.3887563    0.81404465   0.26649666   0.79426873
    3.38938856]][0m
[37m[1m[2023-07-11 15:07:10,442][233954] Max Reward on eval: -38.11075914513359[0m
[37m[1m[2023-07-11 15:07:10,442][233954] Min Reward on eval: -38.11075914513359[0m
[37m[1m[2023-07-11 15:07:10,442][233954] Mean Reward across all agents: -38.11075914513359[0m
[37m[1m[2023-07-11 15:07:10,443][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:07:15,435][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:07:15,435][233954] Reward + Measures: [[ 128.12900349    0.27670002    0.90620005    0.43409997    0.88590002
     3.1040616 ]
 [ -45.31907651    0.37180001    0.83780003    0.28909999    0.8096
     3.2177453 ]
 [-105.32466814    0.3281        0.34169999    0.0503        0.33989999
     3.29340672]
 ...
 [ 437.4318209     0.0524        0.80489999    0.44649997    0.78680003
     3.3373456 ]
 [ 388.56060258    0.1823        0.89399999    0.46529999    0.88160002
     3.52569818]
 [-236.52134227    0.4436        0.94919997    0.17609999    0.94110006
     3.04907727]][0m
[37m[1m[2023-07-11 15:07:15,435][233954] Max Reward on eval: 690.4900970675051[0m
[37m[1m[2023-07-11 15:07:15,436][233954] Min Reward on eval: -611.9447522374801[0m
[37m[1m[2023-07-11 15:07:15,436][233954] Mean Reward across all agents: 108.10668121419728[0m
[37m[1m[2023-07-11 15:07:15,436][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:07:15,439][233954] mean_value=-182.88606325456715, max_value=514.6869010506358[0m
[37m[1m[2023-07-11 15:07:15,442][233954] New mean coefficients: [[ 1.2950323 -3.0636334 -1.046844  -3.1165164 -1.7847338 -0.9349213]][0m
[37m[1m[2023-07-11 15:07:15,443][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:07:24,492][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 15:07:24,498][233954] FPS: 424414.95[0m
[36m[2023-07-11 15:07:24,501][233954] itr=1104, itrs=2000, Progress: 55.20%[0m
[36m[2023-07-11 15:07:36,241][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 15:07:36,241][233954] FPS: 329820.21[0m
[36m[2023-07-11 15:07:40,569][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:07:40,574][233954] Reward + Measures: [[-14.81842244   0.36966801   0.78952366   0.26434368   0.76990527
    3.36850262]][0m
[37m[1m[2023-07-11 15:07:40,574][233954] Max Reward on eval: -14.818422437681361[0m
[37m[1m[2023-07-11 15:07:40,574][233954] Min Reward on eval: -14.818422437681361[0m
[37m[1m[2023-07-11 15:07:40,575][233954] Mean Reward across all agents: -14.818422437681361[0m
[37m[1m[2023-07-11 15:07:40,575][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:07:45,512][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:07:45,512][233954] Reward + Measures: [[ 22.34762904   0.40640002   0.61549997   0.30520001   0.68990004
    3.65856719]
 [-16.10913296   0.24200001   0.4316       0.16949999   0.40950003
    3.62895942]
 [ 37.91910598   0.1921       0.50599998   0.21700001   0.50209999
    2.72010088]
 ...
 [114.24068797   0.28590003   0.76770002   0.33860001   0.75710005
    2.87132692]
 [ 61.9928369    0.0689       0.25570002   0.2088       0.25620002
    3.63615417]
 [ 53.6235096    0.16989999   0.2483       0.09850001   0.26269999
    3.53449631]][0m
[37m[1m[2023-07-11 15:07:45,513][233954] Max Reward on eval: 604.9277687124908[0m
[37m[1m[2023-07-11 15:07:45,513][233954] Min Reward on eval: -578.6986818626523[0m
[37m[1m[2023-07-11 15:07:45,513][233954] Mean Reward across all agents: 44.36721637627104[0m
[37m[1m[2023-07-11 15:07:45,513][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:07:45,518][233954] mean_value=-100.71621106630768, max_value=628.0579530900341[0m
[37m[1m[2023-07-11 15:07:45,520][233954] New mean coefficients: [[ 1.1566912 -5.1105986 -1.0139416 -3.319202  -2.3060868 -0.3760829]][0m
[37m[1m[2023-07-11 15:07:45,521][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:07:54,509][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 15:07:54,514][233954] FPS: 427327.50[0m
[36m[2023-07-11 15:07:54,517][233954] itr=1105, itrs=2000, Progress: 55.25%[0m
[36m[2023-07-11 15:08:06,192][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 15:08:06,192][233954] FPS: 331570.69[0m
[36m[2023-07-11 15:08:10,467][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:08:10,472][233954] Reward + Measures: [[-6.26085578  0.33874533  0.74635303  0.255786    0.72692496  3.36026096]][0m
[37m[1m[2023-07-11 15:08:10,473][233954] Max Reward on eval: -6.260855781980218[0m
[37m[1m[2023-07-11 15:08:10,473][233954] Min Reward on eval: -6.260855781980218[0m
[37m[1m[2023-07-11 15:08:10,473][233954] Mean Reward across all agents: -6.260855781980218[0m
[37m[1m[2023-07-11 15:08:10,473][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:08:15,718][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:08:15,719][233954] Reward + Measures: [[ 63.67856618   0.264        0.70970005   0.22400001   0.71320003
    2.78028846]
 [248.84022618   0.27599999   0.87919998   0.39249998   0.86310005
    3.25751877]
 [114.36357015   0.17480001   0.87950003   0.3793       0.87400001
    2.8012414 ]
 ...
 [-47.81299706   0.24720001   0.46569997   0.27560002   0.45250002
    2.03882623]
 [-51.20109039   0.30649999   0.84420007   0.25649998   0.82460004
    2.92600226]
 [128.69635484   0.14380001   0.69270003   0.25060001   0.63499999
    2.36378098]][0m
[37m[1m[2023-07-11 15:08:15,719][233954] Max Reward on eval: 545.1977577475831[0m
[37m[1m[2023-07-11 15:08:15,719][233954] Min Reward on eval: -255.02114971014672[0m
[37m[1m[2023-07-11 15:08:15,720][233954] Mean Reward across all agents: 85.90601765431691[0m
[37m[1m[2023-07-11 15:08:15,720][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:08:15,721][233954] mean_value=-248.89039836382764, max_value=90.687715636865[0m
[37m[1m[2023-07-11 15:08:15,723][233954] New mean coefficients: [[ 2.1267128 -3.4477773 -1.2179222 -3.3846884 -2.701559  -0.3112874]][0m
[37m[1m[2023-07-11 15:08:15,724][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:08:24,733][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 15:08:24,739][233954] FPS: 426316.45[0m
[36m[2023-07-11 15:08:24,745][233954] itr=1106, itrs=2000, Progress: 55.30%[0m
[36m[2023-07-11 15:08:36,368][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 15:08:36,368][233954] FPS: 333456.83[0m
[36m[2023-07-11 15:08:40,566][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:08:40,567][233954] Reward + Measures: [[13.19643525  0.32174167  0.70429164  0.244109    0.68423533  3.33483171]][0m
[37m[1m[2023-07-11 15:08:40,567][233954] Max Reward on eval: 13.196435254611357[0m
[37m[1m[2023-07-11 15:08:40,567][233954] Min Reward on eval: 13.196435254611357[0m
[37m[1m[2023-07-11 15:08:40,568][233954] Mean Reward across all agents: 13.196435254611357[0m
[37m[1m[2023-07-11 15:08:40,568][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:08:45,510][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:08:45,510][233954] Reward + Measures: [[ 333.74203063    0.1283        0.87709999    0.48709998    0.86690009
     3.37841964]
 [  75.97239996    0.20060001    0.62040001    0.2987        0.60710001
     3.21391678]
 [-166.85858248    0.50910002    0.73710001    0.0953        0.69129997
     3.342273  ]
 ...
 [ -57.66028226    0.0808        0.1904        0.11000001    0.14549999
     3.14350772]
 [  78.02404353    0.30050001    0.89350003    0.3953        0.88450015
     3.4060936 ]
 [ 218.13756752    0.19760001    0.84370005    0.39110002    0.81879997
     3.25263762]][0m
[37m[1m[2023-07-11 15:08:45,511][233954] Max Reward on eval: 589.5375442351215[0m
[37m[1m[2023-07-11 15:08:45,511][233954] Min Reward on eval: -401.6845893378835[0m
[37m[1m[2023-07-11 15:08:45,511][233954] Mean Reward across all agents: 99.92404654032785[0m
[37m[1m[2023-07-11 15:08:45,511][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:08:45,513][233954] mean_value=-236.57102715044093, max_value=312.0150591084586[0m
[37m[1m[2023-07-11 15:08:45,516][233954] New mean coefficients: [[ 3.189653   -2.8740335  -0.7582014  -2.1888208  -2.3383243  -0.23549153]][0m
[37m[1m[2023-07-11 15:08:45,517][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:08:54,472][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 15:08:54,473][233954] FPS: 428866.46[0m
[36m[2023-07-11 15:08:54,475][233954] itr=1107, itrs=2000, Progress: 55.35%[0m
[36m[2023-07-11 15:09:06,281][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 15:09:06,282][233954] FPS: 327927.81[0m
[36m[2023-07-11 15:09:10,569][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:09:10,570][233954] Reward + Measures: [[85.8801234   0.28131667  0.70277971  0.27810434  0.68487036  3.34934306]][0m
[37m[1m[2023-07-11 15:09:10,570][233954] Max Reward on eval: 85.8801233977117[0m
[37m[1m[2023-07-11 15:09:10,570][233954] Min Reward on eval: 85.8801233977117[0m
[37m[1m[2023-07-11 15:09:10,570][233954] Mean Reward across all agents: 85.8801233977117[0m
[37m[1m[2023-07-11 15:09:10,571][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:09:15,593][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:09:15,594][233954] Reward + Measures: [[  45.47580525    0.2098        0.70700002    0.30770001    0.6778
     3.47978854]
 [ 223.37962246    0.23249999    0.79970008    0.3881        0.77170002
     3.70570993]
 [  47.69576523    0.20319998    0.33830005    0.19069999    0.33670002
     3.45657349]
 ...
 [-218.41599886    0.47809997    0.87460005    0.24249999    0.88220006
     3.18871856]
 [ 127.64941092    0.14670001    0.1436        0.0692        0.1609
     3.56617618]
 [ 119.39355828    0.37720001    0.74970001    0.35530001    0.77429992
     3.80186319]][0m
[37m[1m[2023-07-11 15:09:15,594][233954] Max Reward on eval: 559.3411593099357[0m
[37m[1m[2023-07-11 15:09:15,595][233954] Min Reward on eval: -508.1200384968077[0m
[37m[1m[2023-07-11 15:09:15,595][233954] Mean Reward across all agents: 87.36232920076262[0m
[37m[1m[2023-07-11 15:09:15,595][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:09:15,600][233954] mean_value=-108.93681743624079, max_value=710.9302485281871[0m
[37m[1m[2023-07-11 15:09:15,602][233954] New mean coefficients: [[ 3.0188046 -5.5929193 -1.3000487 -2.1902812 -2.2467008  0.0364594]][0m
[37m[1m[2023-07-11 15:09:15,603][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:09:24,729][233954] train() took 9.12 seconds to complete[0m
[36m[2023-07-11 15:09:24,729][233954] FPS: 420865.16[0m
[36m[2023-07-11 15:09:24,731][233954] itr=1108, itrs=2000, Progress: 55.40%[0m
[36m[2023-07-11 15:09:36,605][233954] train() took 11.78 seconds to complete[0m
[36m[2023-07-11 15:09:36,606][233954] FPS: 326007.78[0m
[36m[2023-07-11 15:09:40,955][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:09:40,956][233954] Reward + Measures: [[150.06498028   0.24443664   0.72608066   0.32024866   0.7068994
    3.3631146 ]][0m
[37m[1m[2023-07-11 15:09:40,956][233954] Max Reward on eval: 150.0649802840379[0m
[37m[1m[2023-07-11 15:09:40,956][233954] Min Reward on eval: 150.0649802840379[0m
[37m[1m[2023-07-11 15:09:40,957][233954] Mean Reward across all agents: 150.0649802840379[0m
[37m[1m[2023-07-11 15:09:40,957][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:09:45,963][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:09:45,969][233954] Reward + Measures: [[ 369.71849015    0.20150001    0.93599999    0.46880004    0.92649996
     3.307441  ]
 [ 392.81714965    0.22939999    0.88230002    0.37639999    0.85050005
     3.39833236]
 [  63.40374099    0.29140002    0.86409998    0.41770002    0.86259997
     3.34515381]
 ...
 [ -94.37584911    0.28420001    0.53389996    0.1513        0.52170002
     3.05913162]
 [  29.98906974    0.33770001    0.78670001    0.29859999    0.76669997
     3.31891942]
 [-145.47122004    0.48850003    0.96569997    0.3321        0.96130002
     3.3678844 ]][0m
[37m[1m[2023-07-11 15:09:45,969][233954] Max Reward on eval: 606.7473964666948[0m
[37m[1m[2023-07-11 15:09:45,970][233954] Min Reward on eval: -392.63244059723803[0m
[37m[1m[2023-07-11 15:09:45,970][233954] Mean Reward across all agents: 176.70010419104506[0m
[37m[1m[2023-07-11 15:09:45,970][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:09:45,973][233954] mean_value=-155.52863077840854, max_value=348.4019713769282[0m
[37m[1m[2023-07-11 15:09:45,975][233954] New mean coefficients: [[ 2.4525483 -5.435446  -0.5537008 -2.9281857 -2.0097046  0.5430452]][0m
[37m[1m[2023-07-11 15:09:45,976][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:09:54,957][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 15:09:54,958][233954] FPS: 427635.94[0m
[36m[2023-07-11 15:09:54,960][233954] itr=1109, itrs=2000, Progress: 55.45%[0m
[36m[2023-07-11 15:10:06,785][233954] train() took 11.73 seconds to complete[0m
[36m[2023-07-11 15:10:06,785][233954] FPS: 327475.72[0m
[36m[2023-07-11 15:10:11,114][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:10:11,114][233954] Reward + Measures: [[166.59971784   0.24541634   0.71010077   0.31029135   0.69075632
    3.35852742]][0m
[37m[1m[2023-07-11 15:10:11,115][233954] Max Reward on eval: 166.59971784202534[0m
[37m[1m[2023-07-11 15:10:11,115][233954] Min Reward on eval: 166.59971784202534[0m
[37m[1m[2023-07-11 15:10:11,115][233954] Mean Reward across all agents: 166.59971784202534[0m
[37m[1m[2023-07-11 15:10:11,115][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:10:16,158][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:10:16,159][233954] Reward + Measures: [[215.72058965   0.2737       0.68850005   0.31900001   0.67640001
    3.20718169]
 [155.33904459   0.15989999   0.7209       0.4237       0.70349997
    3.22528386]
 [-69.45994806   0.45180002   0.79650003   0.24600001   0.77940005
    3.16335869]
 ...
 [  3.26764193   0.37579998   0.83789998   0.28350002   0.8179
    3.39090776]
 [230.29512833   0.20840001   0.8150999    0.43420002   0.80219996
    3.29720616]
 [-40.50811031   0.34259999   0.71140003   0.27430001   0.69690007
    3.68643951]][0m
[37m[1m[2023-07-11 15:10:16,159][233954] Max Reward on eval: 361.3387066856027[0m
[37m[1m[2023-07-11 15:10:16,159][233954] Min Reward on eval: -253.50363736655564[0m
[37m[1m[2023-07-11 15:10:16,159][233954] Mean Reward across all agents: 137.43162604676624[0m
[37m[1m[2023-07-11 15:10:16,159][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:10:16,161][233954] mean_value=-226.2257815576254, max_value=88.62034325542514[0m
[37m[1m[2023-07-11 15:10:16,164][233954] New mean coefficients: [[ 1.3321624  -5.1687512  -0.27064142 -3.0184379  -2.1357074   0.89451873]][0m
[37m[1m[2023-07-11 15:10:16,165][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:10:25,203][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 15:10:25,203][233954] FPS: 424945.19[0m
[36m[2023-07-11 15:10:25,205][233954] itr=1110, itrs=2000, Progress: 55.50%[0m
[37m[1m[2023-07-11 15:14:02,222][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001090[0m
[36m[2023-07-11 15:14:14,469][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 15:14:14,469][233954] FPS: 329071.87[0m
[36m[2023-07-11 15:14:18,678][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:14:18,679][233954] Reward + Measures: [[200.31283081   0.21222533   0.68091333   0.31419131   0.66166234
    3.35198426]][0m
[37m[1m[2023-07-11 15:14:18,679][233954] Max Reward on eval: 200.3128308087527[0m
[37m[1m[2023-07-11 15:14:18,679][233954] Min Reward on eval: 200.3128308087527[0m
[37m[1m[2023-07-11 15:14:18,679][233954] Mean Reward across all agents: 200.3128308087527[0m
[37m[1m[2023-07-11 15:14:18,680][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:14:23,605][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:14:23,606][233954] Reward + Measures: [[-35.66150801   0.29840001   0.60549998   0.1886       0.58670002
    3.28237009]
 [273.69271453   0.1653       0.6239       0.36040002   0.60759997
    3.28709412]
 [317.80001378   0.34660003   0.73990005   0.20349999   0.70170003
    3.20079732]
 ...
 [144.98827907   0.1865       0.44079995   0.1533       0.41999999
    3.11719012]
 [-47.50911045   0.37709999   0.6142       0.16790001   0.62490004
    3.35585856]
 [204.88956807   0.103        0.4357       0.22659998   0.42379999
    3.19579005]][0m
[37m[1m[2023-07-11 15:14:23,606][233954] Max Reward on eval: 524.8560972693376[0m
[37m[1m[2023-07-11 15:14:23,606][233954] Min Reward on eval: -544.5568045802414[0m
[37m[1m[2023-07-11 15:14:23,607][233954] Mean Reward across all agents: 199.67200895843257[0m
[37m[1m[2023-07-11 15:14:23,607][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:14:23,610][233954] mean_value=-113.94062620602418, max_value=316.84068047006235[0m
[37m[1m[2023-07-11 15:14:23,612][233954] New mean coefficients: [[ 1.4977623  -6.588111   -0.51158094 -2.8423223  -2.2543485   0.866676  ]][0m
[37m[1m[2023-07-11 15:14:23,613][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:14:32,569][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 15:14:32,569][233954] FPS: 428856.80[0m
[36m[2023-07-11 15:14:32,572][233954] itr=1111, itrs=2000, Progress: 55.55%[0m
[36m[2023-07-11 15:14:44,303][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 15:14:44,303][233954] FPS: 330112.53[0m
[36m[2023-07-11 15:14:48,556][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:14:48,556][233954] Reward + Measures: [[193.38664324   0.21884033   0.67848331   0.3063513    0.656317
    3.34744716]][0m
[37m[1m[2023-07-11 15:14:48,556][233954] Max Reward on eval: 193.38664324069637[0m
[37m[1m[2023-07-11 15:14:48,557][233954] Min Reward on eval: 193.38664324069637[0m
[37m[1m[2023-07-11 15:14:48,557][233954] Mean Reward across all agents: 193.38664324069637[0m
[37m[1m[2023-07-11 15:14:48,557][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:14:53,755][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:14:53,762][233954] Reward + Measures: [[ 426.93630171    0.0416        0.62239999    0.43879995    0.60549998
     3.32101107]
 [-228.17403987    0.31570002    0.35640001    0.054         0.32690001
     3.22869921]
 [-468.5739205     0.55830002    0.61079997    0.0454        0.62019998
     3.3450706 ]
 ...
 [-233.17024274    0.47760001    0.70679998    0.1436        0.70189995
     3.3657639 ]
 [ 182.61595854    0.29589999    0.7051        0.2775        0.68510002
     3.42822957]
 [ 182.26220075    0.0582        0.45630002    0.26369998    0.4303
     3.28958106]][0m
[37m[1m[2023-07-11 15:14:53,762][233954] Max Reward on eval: 552.3963303775527[0m
[37m[1m[2023-07-11 15:14:53,762][233954] Min Reward on eval: -468.5739204988815[0m
[37m[1m[2023-07-11 15:14:53,763][233954] Mean Reward across all agents: 116.16840260335047[0m
[37m[1m[2023-07-11 15:14:53,763][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:14:53,765][233954] mean_value=-194.19326540212393, max_value=305.31684454220834[0m
[37m[1m[2023-07-11 15:14:53,768][233954] New mean coefficients: [[ 1.2820054 -7.0093317 -0.5082373 -1.6203134 -1.6548295  0.7251864]][0m
[37m[1m[2023-07-11 15:14:53,769][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:15:02,644][233954] train() took 8.87 seconds to complete[0m
[36m[2023-07-11 15:15:02,644][233954] FPS: 432741.77[0m
[36m[2023-07-11 15:15:02,647][233954] itr=1112, itrs=2000, Progress: 55.60%[0m
[36m[2023-07-11 15:15:14,314][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 15:15:14,314][233954] FPS: 331968.63[0m
[36m[2023-07-11 15:15:18,489][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:15:18,490][233954] Reward + Measures: [[199.83251841   0.20960733   0.66448134   0.30374667   0.64363867
    3.36081243]][0m
[37m[1m[2023-07-11 15:15:18,490][233954] Max Reward on eval: 199.832518406116[0m
[37m[1m[2023-07-11 15:15:18,490][233954] Min Reward on eval: 199.832518406116[0m
[37m[1m[2023-07-11 15:15:18,491][233954] Mean Reward across all agents: 199.832518406116[0m
[37m[1m[2023-07-11 15:15:18,491][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:15:23,467][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:15:23,468][233954] Reward + Measures: [[351.26518563   0.20390001   0.80030006   0.4059       0.78570002
    3.44165778]
 [516.86787442   0.0295       0.71540004   0.51940006   0.70870006
    3.49012446]
 [580.65286591   0.0891       0.79339999   0.50319999   0.79409999
    3.40789032]
 ...
 [318.25439587   0.0563       0.5399       0.3515       0.51569998
    3.26647544]
 [ 62.90346051   0.34590003   0.79279995   0.27089998   0.77030003
    3.42682624]
 [340.52415514   0.1857       0.88660002   0.40270001   0.87340003
    3.55516696]][0m
[37m[1m[2023-07-11 15:15:23,468][233954] Max Reward on eval: 615.1329181276262[0m
[37m[1m[2023-07-11 15:15:23,468][233954] Min Reward on eval: -403.50731658246366[0m
[37m[1m[2023-07-11 15:15:23,469][233954] Mean Reward across all agents: 235.07188170438215[0m
[37m[1m[2023-07-11 15:15:23,469][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:15:23,471][233954] mean_value=-156.39809001090472, max_value=251.2278191982562[0m
[37m[1m[2023-07-11 15:15:23,474][233954] New mean coefficients: [[ 1.1423485  -7.527463    0.07176262 -1.2849143  -1.6797006   0.62417936]][0m
[37m[1m[2023-07-11 15:15:23,475][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:15:32,440][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 15:15:32,441][233954] FPS: 428375.47[0m
[36m[2023-07-11 15:15:32,443][233954] itr=1113, itrs=2000, Progress: 55.65%[0m
[36m[2023-07-11 15:15:44,106][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 15:15:44,107][233954] FPS: 331985.08[0m
[36m[2023-07-11 15:15:48,336][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:15:48,337][233954] Reward + Measures: [[211.41979937   0.20545533   0.66880763   0.30860201   0.64581728
    3.35778451]][0m
[37m[1m[2023-07-11 15:15:48,337][233954] Max Reward on eval: 211.41979937246674[0m
[37m[1m[2023-07-11 15:15:48,337][233954] Min Reward on eval: 211.41979937246674[0m
[37m[1m[2023-07-11 15:15:48,338][233954] Mean Reward across all agents: 211.41979937246674[0m
[37m[1m[2023-07-11 15:15:48,338][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:15:53,258][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:15:53,259][233954] Reward + Measures: [[ 72.93695491   0.23600002   0.67799997   0.2307       0.65860003
    3.16270614]
 [275.40765502   0.14400001   0.72330004   0.42039999   0.70429999
    3.27210307]
 [449.62948175   0.0942       0.77999997   0.46670005   0.77160001
    3.30093741]
 ...
 [295.29265826   0.1245       0.72289997   0.43090001   0.70929998
    3.33528328]
 [300.31479255   0.27779999   0.87670004   0.37490001   0.84759998
    3.4613297 ]
 [527.68767831   0.0954       0.9716       0.63169998   0.95710003
    3.51861382]][0m
[37m[1m[2023-07-11 15:15:53,259][233954] Max Reward on eval: 753.0056533407885[0m
[37m[1m[2023-07-11 15:15:53,259][233954] Min Reward on eval: -629.4597053327132[0m
[37m[1m[2023-07-11 15:15:53,260][233954] Mean Reward across all agents: 188.0682329923811[0m
[37m[1m[2023-07-11 15:15:53,260][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:15:53,262][233954] mean_value=-201.9191026433429, max_value=154.74894844162617[0m
[37m[1m[2023-07-11 15:15:53,265][233954] New mean coefficients: [[ 1.6619319  -7.2791057   0.51446664 -0.8331006  -2.3263602   0.9117067 ]][0m
[37m[1m[2023-07-11 15:15:53,266][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:16:02,254][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 15:16:02,254][233954] FPS: 427306.70[0m
[36m[2023-07-11 15:16:02,256][233954] itr=1114, itrs=2000, Progress: 55.70%[0m
[36m[2023-07-11 15:16:14,130][233954] train() took 11.78 seconds to complete[0m
[36m[2023-07-11 15:16:14,130][233954] FPS: 326038.02[0m
[36m[2023-07-11 15:16:18,417][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:16:18,417][233954] Reward + Measures: [[231.43879998   0.19252466   0.67370766   0.32146633   0.649773
    3.3610692 ]][0m
[37m[1m[2023-07-11 15:16:18,418][233954] Max Reward on eval: 231.4387999782018[0m
[37m[1m[2023-07-11 15:16:18,418][233954] Min Reward on eval: 231.4387999782018[0m
[37m[1m[2023-07-11 15:16:18,418][233954] Mean Reward across all agents: 231.4387999782018[0m
[37m[1m[2023-07-11 15:16:18,418][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:16:23,424][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:16:23,424][233954] Reward + Measures: [[-230.76656338    0.41140005    0.68239999    0.1478        0.6354
     3.31836486]
 [  49.60698993    0.4402        0.88340008    0.2811        0.861
     3.4505055 ]
 [ 116.14108654    0.37180004    0.9806        0.36380002    0.96530002
     3.5136106 ]
 ...
 [ 258.70470374    0.2168        0.6965        0.30810001    0.67739999
     3.40668654]
 [-417.79612068    0.69420004    0.95819998    0.12280001    0.92400008
     3.62047696]
 [-149.4035431     0.3901        0.7554        0.2           0.71830004
     3.33803606]][0m
[37m[1m[2023-07-11 15:16:23,424][233954] Max Reward on eval: 496.7411031885073[0m
[37m[1m[2023-07-11 15:16:23,425][233954] Min Reward on eval: -622.8672031593509[0m
[37m[1m[2023-07-11 15:16:23,425][233954] Mean Reward across all agents: -14.461355673444334[0m
[37m[1m[2023-07-11 15:16:23,425][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:16:23,427][233954] mean_value=-247.783509701001, max_value=223.57967360564493[0m
[37m[1m[2023-07-11 15:16:23,430][233954] New mean coefficients: [[ 1.5534484 -6.908407   0.7477554 -0.7087462 -2.017752   0.7644932]][0m
[37m[1m[2023-07-11 15:16:23,430][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:16:32,467][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 15:16:32,467][233954] FPS: 425039.85[0m
[36m[2023-07-11 15:16:32,469][233954] itr=1115, itrs=2000, Progress: 55.75%[0m
[36m[2023-07-11 15:16:44,125][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 15:16:44,125][233954] FPS: 332230.73[0m
[36m[2023-07-11 15:16:48,334][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:16:48,334][233954] Reward + Measures: [[234.65661593   0.19396433   0.6769653    0.32592565   0.65749496
    3.36844325]][0m
[37m[1m[2023-07-11 15:16:48,335][233954] Max Reward on eval: 234.65661592879357[0m
[37m[1m[2023-07-11 15:16:48,335][233954] Min Reward on eval: 234.65661592879357[0m
[37m[1m[2023-07-11 15:16:48,335][233954] Mean Reward across all agents: 234.65661592879357[0m
[37m[1m[2023-07-11 15:16:48,335][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:16:53,301][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:16:53,301][233954] Reward + Measures: [[ 86.69540436   0.22850001   0.50599998   0.1876       0.46890002
    3.32500768]
 [ 22.78801754   0.26530001   0.71500009   0.27540001   0.69499999
    3.40009236]
 [209.43625459   0.14250001   0.64480001   0.31250003   0.60879999
    3.41803598]
 ...
 [299.90584042   0.294        0.77330005   0.48179999   0.82779998
    3.53496027]
 [-32.51373104   0.3583       0.94200003   0.37350002   0.91440004
    3.55621457]
 [396.19272237   0.0422       0.70300001   0.49349999   0.6904
    3.50284886]][0m
[37m[1m[2023-07-11 15:16:53,301][233954] Max Reward on eval: 587.743656652607[0m
[37m[1m[2023-07-11 15:16:53,302][233954] Min Reward on eval: -369.9645385602256[0m
[37m[1m[2023-07-11 15:16:53,302][233954] Mean Reward across all agents: 173.86652535153183[0m
[37m[1m[2023-07-11 15:16:53,302][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:16:53,305][233954] mean_value=-140.1304669970758, max_value=268.2698783476394[0m
[37m[1m[2023-07-11 15:16:53,307][233954] New mean coefficients: [[ 1.5331134  -7.29466     0.54867274 -1.1485941  -2.5116801   1.2989881 ]][0m
[37m[1m[2023-07-11 15:16:53,308][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:17:02,219][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 15:17:02,219][233954] FPS: 431026.26[0m
[36m[2023-07-11 15:17:02,221][233954] itr=1116, itrs=2000, Progress: 55.80%[0m
[36m[2023-07-11 15:17:13,990][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 15:17:13,990][233954] FPS: 329063.44[0m
[36m[2023-07-11 15:17:18,260][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:17:18,261][233954] Reward + Measures: [[246.3277356    0.18204166   0.6596517    0.32499799   0.63813436
    3.35572028]][0m
[37m[1m[2023-07-11 15:17:18,261][233954] Max Reward on eval: 246.32773560245792[0m
[37m[1m[2023-07-11 15:17:18,261][233954] Min Reward on eval: 246.32773560245792[0m
[37m[1m[2023-07-11 15:17:18,261][233954] Mean Reward across all agents: 246.32773560245792[0m
[37m[1m[2023-07-11 15:17:18,262][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:17:23,237][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:17:23,243][233954] Reward + Measures: [[ 459.12031675    0.0716        0.74310005    0.42799997    0.72909993
     3.37457705]
 [ 496.6355729     0.0407        0.70999998    0.44689998    0.70099998
     3.48508334]
 [ 268.77533899    0.18670002    0.73350006    0.3585        0.70520002
     3.40733719]
 ...
 [ 371.83952782    0.12809999    0.74900001    0.40670004    0.72610003
     3.50544524]
 [ 440.55447296    0.161         0.88139993    0.43520004    0.86779994
     3.52219844]
 [-197.55846025    0.50699997    0.96820003    0.28340003    0.96649998
     3.46313715]][0m
[37m[1m[2023-07-11 15:17:23,243][233954] Max Reward on eval: 745.0332841849886[0m
[37m[1m[2023-07-11 15:17:23,243][233954] Min Reward on eval: -256.0010623543523[0m
[37m[1m[2023-07-11 15:17:23,243][233954] Mean Reward across all agents: 252.55891090027671[0m
[37m[1m[2023-07-11 15:17:23,244][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:17:23,246][233954] mean_value=-145.2411252026628, max_value=353.5112855749741[0m
[37m[1m[2023-07-11 15:17:23,249][233954] New mean coefficients: [[ 1.830445   -7.2911677   0.36583513 -1.7326641  -2.050582    1.4590461 ]][0m
[37m[1m[2023-07-11 15:17:23,250][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:17:32,204][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 15:17:32,204][233954] FPS: 428939.78[0m
[36m[2023-07-11 15:17:32,206][233954] itr=1117, itrs=2000, Progress: 55.85%[0m
[36m[2023-07-11 15:17:43,801][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 15:17:43,801][233954] FPS: 333950.60[0m
[36m[2023-07-11 15:17:48,098][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:17:48,098][233954] Reward + Measures: [[259.24314058   0.17267168   0.67580128   0.33688167   0.65272629
    3.36283827]][0m
[37m[1m[2023-07-11 15:17:48,098][233954] Max Reward on eval: 259.243140581122[0m
[37m[1m[2023-07-11 15:17:48,099][233954] Min Reward on eval: 259.243140581122[0m
[37m[1m[2023-07-11 15:17:48,099][233954] Mean Reward across all agents: 259.243140581122[0m
[37m[1m[2023-07-11 15:17:48,099][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:17:53,310][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:17:53,311][233954] Reward + Measures: [[ 126.66348627    0.1389        0.63530004    0.31850001    0.62640005
     3.28713846]
 [ 246.21163451    0.19789998    0.70830005    0.32560003    0.66790003
     3.4429028 ]
 [ 205.56561848    0.14400001    0.61799997    0.294         0.579
     3.11506629]
 ...
 [-184.02883786    0.39160001    0.44819999    0.0472        0.41630003
     3.18591094]
 [  67.29787401    0.2933        0.63960004    0.22839999    0.61429995
     3.40059471]
 [ -60.77303509    0.40400001    0.78459996    0.2678        0.77200001
     3.3559711 ]][0m
[37m[1m[2023-07-11 15:17:53,311][233954] Max Reward on eval: 545.5673004182056[0m
[37m[1m[2023-07-11 15:17:53,311][233954] Min Reward on eval: -233.3100941196084[0m
[37m[1m[2023-07-11 15:17:53,311][233954] Mean Reward across all agents: 86.53856068335517[0m
[37m[1m[2023-07-11 15:17:53,311][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:17:53,314][233954] mean_value=-201.2039472905661, max_value=220.4911490632236[0m
[37m[1m[2023-07-11 15:17:53,316][233954] New mean coefficients: [[ 1.8512279  -6.652475    0.01560414 -1.7514316  -2.2778008   0.9414874 ]][0m
[37m[1m[2023-07-11 15:17:53,317][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:18:02,379][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 15:18:02,379][233954] FPS: 423839.67[0m
[36m[2023-07-11 15:18:02,381][233954] itr=1118, itrs=2000, Progress: 55.90%[0m
[36m[2023-07-11 15:18:13,955][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 15:18:13,955][233954] FPS: 334658.26[0m
[36m[2023-07-11 15:18:18,231][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:18:18,232][233954] Reward + Measures: [[260.32866518   0.18144166   0.68484765   0.33955732   0.66433364
    3.36562109]][0m
[37m[1m[2023-07-11 15:18:18,232][233954] Max Reward on eval: 260.3286651835673[0m
[37m[1m[2023-07-11 15:18:18,232][233954] Min Reward on eval: 260.3286651835673[0m
[37m[1m[2023-07-11 15:18:18,232][233954] Mean Reward across all agents: 260.3286651835673[0m
[37m[1m[2023-07-11 15:18:18,233][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:18:23,236][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:18:23,237][233954] Reward + Measures: [[303.24661686   0.1876       0.7985       0.37040001   0.76590002
    3.27825975]
 [  6.92095757   0.29280001   0.62159997   0.2095       0.61079997
    3.36217618]
 [317.29870562   0.26190001   0.88479996   0.38869998   0.85420001
    3.3457768 ]
 ...
 [156.60013604   0.2965       0.63860005   0.22120002   0.64850003
    3.29572678]
 [-59.27391946   0.42500001   0.7166       0.29660001   0.75020003
    3.40362024]
 [164.26215887   0.48410001   0.93099993   0.2149       0.92380011
    3.29099441]][0m
[37m[1m[2023-07-11 15:18:23,237][233954] Max Reward on eval: 624.4954529162496[0m
[37m[1m[2023-07-11 15:18:23,237][233954] Min Reward on eval: -277.721500163991[0m
[37m[1m[2023-07-11 15:18:23,237][233954] Mean Reward across all agents: 194.8627673451715[0m
[37m[1m[2023-07-11 15:18:23,238][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:18:23,241][233954] mean_value=-167.1188370405488, max_value=255.44101298611162[0m
[37m[1m[2023-07-11 15:18:23,243][233954] New mean coefficients: [[ 1.254359   -7.148531    0.09842806 -2.3495908  -2.3005698   1.0539382 ]][0m
[37m[1m[2023-07-11 15:18:23,244][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:18:32,311][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 15:18:32,312][233954] FPS: 423584.84[0m
[36m[2023-07-11 15:18:32,314][233954] itr=1119, itrs=2000, Progress: 55.95%[0m
[36m[2023-07-11 15:18:44,024][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 15:18:44,024][233954] FPS: 330787.18[0m
[36m[2023-07-11 15:18:48,271][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:18:48,272][233954] Reward + Measures: [[285.23156433   0.17093669   0.70241398   0.35673869   0.68009764
    3.38231826]][0m
[37m[1m[2023-07-11 15:18:48,272][233954] Max Reward on eval: 285.231564333523[0m
[37m[1m[2023-07-11 15:18:48,272][233954] Min Reward on eval: 285.231564333523[0m
[37m[1m[2023-07-11 15:18:48,273][233954] Mean Reward across all agents: 285.231564333523[0m
[37m[1m[2023-07-11 15:18:48,273][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:18:53,242][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:18:53,243][233954] Reward + Measures: [[340.04936074   0.1141       0.80240005   0.49860001   0.78280002
    3.39315915]
 [175.62042334   0.23940001   0.96509999   0.48839998   0.95489997
    3.43714213]
 [276.35428935   0.1901       0.86470002   0.44380003   0.8405
    3.29246449]
 ...
 [371.19491008   0.1823       0.78110003   0.4219       0.77000004
    3.45084071]
 [395.99222994   0.08079999   0.88729995   0.58109999   0.875
    3.40279627]
 [ 91.93150329   0.34540004   0.88370001   0.36229998   0.87389994
    3.44389033]][0m
[37m[1m[2023-07-11 15:18:53,243][233954] Max Reward on eval: 618.7799411305808[0m
[37m[1m[2023-07-11 15:18:53,243][233954] Min Reward on eval: -340.7994566142559[0m
[37m[1m[2023-07-11 15:18:53,244][233954] Mean Reward across all agents: 194.8943488925222[0m
[37m[1m[2023-07-11 15:18:53,244][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:18:53,246][233954] mean_value=-209.14676617498125, max_value=281.9189424478113[0m
[37m[1m[2023-07-11 15:18:53,248][233954] New mean coefficients: [[ 1.6622643 -6.8025584 -0.4972452 -2.2578144 -2.1624334  0.6711941]][0m
[37m[1m[2023-07-11 15:18:53,249][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:19:02,289][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 15:19:02,289][233954] FPS: 424853.62[0m
[36m[2023-07-11 15:19:02,292][233954] itr=1120, itrs=2000, Progress: 56.00%[0m
[37m[1m[2023-07-11 15:22:32,983][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001100[0m
[36m[2023-07-11 15:22:45,387][233954] train() took 11.73 seconds to complete[0m
[36m[2023-07-11 15:22:45,387][233954] FPS: 327258.92[0m
[36m[2023-07-11 15:22:49,571][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:22:49,576][233954] Reward + Measures: [[285.51388566   0.16431533   0.70472932   0.36194798   0.68174905
    3.3809154 ]][0m
[37m[1m[2023-07-11 15:22:49,576][233954] Max Reward on eval: 285.51388566189854[0m
[37m[1m[2023-07-11 15:22:49,576][233954] Min Reward on eval: 285.51388566189854[0m
[37m[1m[2023-07-11 15:22:49,577][233954] Mean Reward across all agents: 285.51388566189854[0m
[37m[1m[2023-07-11 15:22:49,577][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:22:54,550][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:22:54,551][233954] Reward + Measures: [[-49.02924501   0.53570002   0.54960001   0.53350002   0.55419999
    3.45049453]
 [-29.78434132   0.69069999   0.69919997   0.66930002   0.68690008
    3.69518161]
 [-33.49764228   0.48899999   0.50819999   0.4896       0.50159997
    3.46406484]
 ...
 [ 44.62545948   0.66670001   0.69300002   0.66210002   0.68260002
    3.60833859]
 [-63.06525353   0.39749998   0.72310001   0.23590003   0.72210002
    3.4804306 ]
 [ -5.53408119   0.4808       0.51530004   0.4567       0.50920004
    3.28558397]][0m
[37m[1m[2023-07-11 15:22:54,551][233954] Max Reward on eval: 488.07285915035754[0m
[37m[1m[2023-07-11 15:22:54,551][233954] Min Reward on eval: -415.33953393967823[0m
[37m[1m[2023-07-11 15:22:54,552][233954] Mean Reward across all agents: -16.298633862016157[0m
[37m[1m[2023-07-11 15:22:54,552][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:22:54,554][233954] mean_value=-179.9491209470012, max_value=118.08335687333526[0m
[37m[1m[2023-07-11 15:22:54,557][233954] New mean coefficients: [[ 1.6310542  -9.913104   -0.65426284 -2.2331903  -1.5905458   0.17020273]][0m
[37m[1m[2023-07-11 15:22:54,558][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:23:03,616][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 15:23:03,616][233954] FPS: 424015.27[0m
[36m[2023-07-11 15:23:03,618][233954] itr=1121, itrs=2000, Progress: 56.05%[0m
[36m[2023-07-11 15:23:15,487][233954] train() took 11.77 seconds to complete[0m
[36m[2023-07-11 15:23:15,488][233954] FPS: 326240.38[0m
[36m[2023-07-11 15:23:19,690][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:23:19,691][233954] Reward + Measures: [[279.35406233   0.16201133   0.67889297   0.35164768   0.65768099
    3.36530137]][0m
[37m[1m[2023-07-11 15:23:19,691][233954] Max Reward on eval: 279.3540623268367[0m
[37m[1m[2023-07-11 15:23:19,691][233954] Min Reward on eval: 279.3540623268367[0m
[37m[1m[2023-07-11 15:23:19,691][233954] Mean Reward across all agents: 279.3540623268367[0m
[37m[1m[2023-07-11 15:23:19,692][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:23:24,614][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:23:24,615][233954] Reward + Measures: [[360.50541546   0.11790001   0.70320004   0.41600004   0.6979
    3.41467357]
 [292.67209314   0.0597       0.70950007   0.4271       0.6979
    3.3615768 ]
 [181.64011186   0.13150001   0.42300001   0.21460001   0.41290003
    3.11567855]
 ...
 [-51.27500214   0.16500001   0.27019998   0.10879999   0.26539999
    3.13721991]
 [180.66340006   0.058        0.37850001   0.25560001   0.35380003
    3.13583994]
 [120.66330262   0.25530002   0.69980007   0.28749999   0.69199997
    3.25080419]][0m
[37m[1m[2023-07-11 15:23:24,615][233954] Max Reward on eval: 651.6569905335084[0m
[37m[1m[2023-07-11 15:23:24,615][233954] Min Reward on eval: -51.275002136919646[0m
[37m[1m[2023-07-11 15:23:24,615][233954] Mean Reward across all agents: 258.0328306763764[0m
[37m[1m[2023-07-11 15:23:24,616][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:23:24,618][233954] mean_value=-138.63243231632862, max_value=288.5810851255013[0m
[37m[1m[2023-07-11 15:23:24,621][233954] New mean coefficients: [[ 1.7016296  -9.629331    0.15230674 -2.0058239  -1.5104858  -0.44550306]][0m
[37m[1m[2023-07-11 15:23:24,622][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:23:33,585][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 15:23:33,586][233954] FPS: 428479.09[0m
[36m[2023-07-11 15:23:33,588][233954] itr=1122, itrs=2000, Progress: 56.10%[0m
[36m[2023-07-11 15:23:45,448][233954] train() took 11.76 seconds to complete[0m
[36m[2023-07-11 15:23:45,448][233954] FPS: 326472.02[0m
[36m[2023-07-11 15:23:49,736][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:23:49,741][233954] Reward + Measures: [[291.59524916   0.15503867   0.68071234   0.35939899   0.65878063
    3.37121964]][0m
[37m[1m[2023-07-11 15:23:49,741][233954] Max Reward on eval: 291.595249161205[0m
[37m[1m[2023-07-11 15:23:49,742][233954] Min Reward on eval: 291.595249161205[0m
[37m[1m[2023-07-11 15:23:49,742][233954] Mean Reward across all agents: 291.595249161205[0m
[37m[1m[2023-07-11 15:23:49,742][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:23:54,906][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:23:54,913][233954] Reward + Measures: [[ -2.06792783   0.37620002   0.88789999   0.3215       0.87770003
    3.38911366]
 [ 95.79233077   0.30280003   0.68889999   0.2638       0.66820002
    3.20903015]
 [ 24.05291705   0.36160001   0.87789994   0.31690001   0.85949993
    3.4745481 ]
 ...
 [226.77592015   0.206        0.62950003   0.32119998   0.61829996
    3.32678986]
 [225.86294832   0.12820001   0.64099997   0.38069999   0.63410008
    3.32855582]
 [182.28510478   0.20550001   0.71110004   0.259        0.69600004
    3.32446218]][0m
[37m[1m[2023-07-11 15:23:54,914][233954] Max Reward on eval: 589.4262499467644[0m
[37m[1m[2023-07-11 15:23:54,914][233954] Min Reward on eval: -351.63381452478933[0m
[37m[1m[2023-07-11 15:23:54,914][233954] Mean Reward across all agents: 170.04653142058413[0m
[37m[1m[2023-07-11 15:23:54,915][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:23:54,917][233954] mean_value=-214.6875283708416, max_value=306.00964467809655[0m
[37m[1m[2023-07-11 15:23:54,919][233954] New mean coefficients: [[ 0.9547298  -9.961402    0.42229816 -2.7342622  -0.70159346 -0.6779084 ]][0m
[37m[1m[2023-07-11 15:23:54,920][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:24:03,793][233954] train() took 8.87 seconds to complete[0m
[36m[2023-07-11 15:24:03,793][233954] FPS: 432877.74[0m
[36m[2023-07-11 15:24:03,795][233954] itr=1123, itrs=2000, Progress: 56.15%[0m
[36m[2023-07-11 15:24:15,368][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 15:24:15,368][233954] FPS: 334578.26[0m
[36m[2023-07-11 15:24:19,667][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:24:19,667][233954] Reward + Measures: [[255.26104191   0.16715233   0.65612698   0.32987034   0.63254571
    3.35542059]][0m
[37m[1m[2023-07-11 15:24:19,667][233954] Max Reward on eval: 255.26104191092736[0m
[37m[1m[2023-07-11 15:24:19,668][233954] Min Reward on eval: 255.26104191092736[0m
[37m[1m[2023-07-11 15:24:19,668][233954] Mean Reward across all agents: 255.26104191092736[0m
[37m[1m[2023-07-11 15:24:19,668][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:24:24,663][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:24:24,664][233954] Reward + Measures: [[ 96.93276496   0.16070001   0.45770001   0.17559999   0.42560002
    3.06422472]
 [204.72884044   0.1429       0.51440001   0.33080003   0.55170006
    3.19425559]
 [203.23661325   0.20910001   0.61980003   0.2658       0.60950005
    3.28249907]
 ...
 [139.57578509   0.1839       0.47209999   0.16880001   0.46949998
    3.02519035]
 [299.50074143   0.20009999   0.63519996   0.41230002   0.70199996
    3.24993014]
 [ 66.22161013   0.27659997   0.6742       0.28260002   0.67210001
    3.24967432]][0m
[37m[1m[2023-07-11 15:24:24,664][233954] Max Reward on eval: 652.9913654409349[0m
[37m[1m[2023-07-11 15:24:24,664][233954] Min Reward on eval: -260.35234205839225[0m
[37m[1m[2023-07-11 15:24:24,664][233954] Mean Reward across all agents: 185.74459552411741[0m
[37m[1m[2023-07-11 15:24:24,665][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:24:24,667][233954] mean_value=-156.48239995283012, max_value=135.76303942878485[0m
[37m[1m[2023-07-11 15:24:24,669][233954] New mean coefficients: [[ 1.828413   -9.426196    0.22325447 -1.4983803  -1.0784361  -0.46197337]][0m
[37m[1m[2023-07-11 15:24:24,670][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:24:33,758][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 15:24:33,758][233954] FPS: 422629.45[0m
[36m[2023-07-11 15:24:33,760][233954] itr=1124, itrs=2000, Progress: 56.20%[0m
[36m[2023-07-11 15:24:45,569][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 15:24:45,569][233954] FPS: 327838.54[0m
[36m[2023-07-11 15:24:49,860][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:24:49,860][233954] Reward + Measures: [[285.17955164   0.14113399   0.65263897   0.34545365   0.626921
    3.3555963 ]][0m
[37m[1m[2023-07-11 15:24:49,861][233954] Max Reward on eval: 285.1795516426233[0m
[37m[1m[2023-07-11 15:24:49,861][233954] Min Reward on eval: 285.1795516426233[0m
[37m[1m[2023-07-11 15:24:49,861][233954] Mean Reward across all agents: 285.1795516426233[0m
[37m[1m[2023-07-11 15:24:49,862][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:24:54,839][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:24:54,840][233954] Reward + Measures: [[115.42265846   0.076        0.33880001   0.20819998   0.35310003
    3.23372197]
 [467.95534322   0.1998       0.96540004   0.47510001   0.94610006
    3.46224666]
 [358.98676817   0.122        0.81520003   0.46709999   0.80689996
    3.41622329]
 ...
 [245.53915587   0.2362       0.88170004   0.3732       0.86059999
    3.34007812]
 [482.24598451   0.1444       0.88199997   0.46149999   0.8731001
    3.51856589]
 [145.82703283   0.13960001   0.48000002   0.23500001   0.433
    3.19202971]][0m
[37m[1m[2023-07-11 15:24:54,840][233954] Max Reward on eval: 662.8194100273307[0m
[37m[1m[2023-07-11 15:24:54,840][233954] Min Reward on eval: -198.51708123935387[0m
[37m[1m[2023-07-11 15:24:54,841][233954] Mean Reward across all agents: 216.50640429112553[0m
[37m[1m[2023-07-11 15:24:54,841][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:24:54,843][233954] mean_value=-182.7551368319585, max_value=206.19439433051247[0m
[37m[1m[2023-07-11 15:24:54,845][233954] New mean coefficients: [[ 1.567558   -8.753862    0.58461046 -1.4795829  -1.4604564   0.20021993]][0m
[37m[1m[2023-07-11 15:24:54,846][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:25:03,826][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 15:25:03,826][233954] FPS: 427719.31[0m
[36m[2023-07-11 15:25:03,828][233954] itr=1125, itrs=2000, Progress: 56.25%[0m
[36m[2023-07-11 15:25:15,913][233954] train() took 11.99 seconds to complete[0m
[36m[2023-07-11 15:25:15,914][233954] FPS: 320353.15[0m
[36m[2023-07-11 15:25:20,266][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:25:20,266][233954] Reward + Measures: [[292.91158872   0.143784     0.65434164   0.34655231   0.62932235
    3.35785222]][0m
[37m[1m[2023-07-11 15:25:20,266][233954] Max Reward on eval: 292.9115887205282[0m
[37m[1m[2023-07-11 15:25:20,266][233954] Min Reward on eval: 292.9115887205282[0m
[37m[1m[2023-07-11 15:25:20,267][233954] Mean Reward across all agents: 292.9115887205282[0m
[37m[1m[2023-07-11 15:25:20,267][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:25:25,310][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:25:25,316][233954] Reward + Measures: [[ 73.23046099   0.2938       0.52789998   0.16700001   0.51139998
    3.22538877]
 [238.26764635   0.13070001   0.63010001   0.34590003   0.611
    3.33854651]
 [115.13159557   0.09230001   0.41669998   0.23169999   0.39320001
    3.25612831]
 ...
 [ 38.34647297   0.1495       0.36789998   0.20710002   0.35330001
    3.29949999]
 [ 41.01183186   0.2951       0.62079996   0.2172       0.60780001
    3.32317424]
 [ 92.79195595   0.30329999   0.60680002   0.23790002   0.61180001
    3.35257339]][0m
[37m[1m[2023-07-11 15:25:25,316][233954] Max Reward on eval: 446.1383572645485[0m
[37m[1m[2023-07-11 15:25:25,317][233954] Min Reward on eval: -243.95702367662452[0m
[37m[1m[2023-07-11 15:25:25,317][233954] Mean Reward across all agents: 127.11328455859919[0m
[37m[1m[2023-07-11 15:25:25,317][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:25:25,319][233954] mean_value=-172.80385843381183, max_value=783.3826904401649[0m
[37m[1m[2023-07-11 15:25:25,322][233954] New mean coefficients: [[ 1.078795   -7.860633    0.59737617 -1.5488778  -1.4827209   0.78674906]][0m
[37m[1m[2023-07-11 15:25:25,323][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:25:34,311][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 15:25:34,311][233954] FPS: 427309.01[0m
[36m[2023-07-11 15:25:34,313][233954] itr=1126, itrs=2000, Progress: 56.30%[0m
[36m[2023-07-11 15:25:46,075][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 15:25:46,076][233954] FPS: 329250.51[0m
[36m[2023-07-11 15:25:50,394][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:25:50,395][233954] Reward + Measures: [[294.32351985   0.14330766   0.66536367   0.3500323    0.6376996
    3.36517406]][0m
[37m[1m[2023-07-11 15:25:50,395][233954] Max Reward on eval: 294.3235198507681[0m
[37m[1m[2023-07-11 15:25:50,395][233954] Min Reward on eval: 294.3235198507681[0m
[37m[1m[2023-07-11 15:25:50,396][233954] Mean Reward across all agents: 294.3235198507681[0m
[37m[1m[2023-07-11 15:25:50,396][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:25:55,354][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:25:55,354][233954] Reward + Measures: [[116.17986085   0.30609998   0.63879997   0.2149       0.61350006
    3.32868838]
 [ 55.56592128   0.31360003   0.71560001   0.21780001   0.69410002
    3.3318727 ]
 [311.04155563   0.139        0.58020002   0.26980001   0.52920002
    3.24359298]
 ...
 [315.71257838   0.0423       0.52509999   0.3416       0.53979999
    3.37756538]
 [546.01247478   0.1006       0.8071       0.49180004   0.7938
    3.43775988]
 [ 37.14628589   0.23239999   0.46269998   0.16860001   0.42129999
    3.17840695]][0m
[37m[1m[2023-07-11 15:25:55,354][233954] Max Reward on eval: 646.9884486543481[0m
[37m[1m[2023-07-11 15:25:55,355][233954] Min Reward on eval: -94.45045735165476[0m
[37m[1m[2023-07-11 15:25:55,355][233954] Mean Reward across all agents: 311.4637780288099[0m
[37m[1m[2023-07-11 15:25:55,355][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:25:55,358][233954] mean_value=-99.23880156176217, max_value=674.659158792663[0m
[37m[1m[2023-07-11 15:25:55,361][233954] New mean coefficients: [[ 1.3638973 -7.634053   1.1119568 -1.9536707 -2.0644317  0.584388 ]][0m
[37m[1m[2023-07-11 15:25:55,362][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:26:04,421][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 15:26:04,421][233954] FPS: 423992.47[0m
[36m[2023-07-11 15:26:04,423][233954] itr=1127, itrs=2000, Progress: 56.35%[0m
[36m[2023-07-11 15:26:16,116][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 15:26:16,116][233954] FPS: 331259.44[0m
[36m[2023-07-11 15:26:20,455][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:26:20,456][233954] Reward + Measures: [[300.78246267   0.140769     0.66660768   0.34934399   0.63705164
    3.37069821]][0m
[37m[1m[2023-07-11 15:26:20,456][233954] Max Reward on eval: 300.7824626677545[0m
[37m[1m[2023-07-11 15:26:20,456][233954] Min Reward on eval: 300.7824626677545[0m
[37m[1m[2023-07-11 15:26:20,456][233954] Mean Reward across all agents: 300.7824626677545[0m
[37m[1m[2023-07-11 15:26:20,456][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:26:25,708][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:26:25,709][233954] Reward + Measures: [[219.88707556   0.13940001   0.74119997   0.35790002   0.71890002
    3.42405391]
 [452.94039291   0.0658       0.79180002   0.46760002   0.77079999
    3.38233638]
 [182.63324181   0.29889998   0.61740005   0.2201       0.62099999
    3.38361287]
 ...
 [318.05290949   0.3644       0.87169999   0.2942       0.84680003
    3.35859489]
 [229.78861786   0.27610001   0.7712       0.30749997   0.75389999
    3.35998011]
 [209.92164513   0.26800001   0.88249999   0.38270003   0.86709994
    3.41392493]][0m
[37m[1m[2023-07-11 15:26:25,709][233954] Max Reward on eval: 653.7137088310031[0m
[37m[1m[2023-07-11 15:26:25,710][233954] Min Reward on eval: -240.1459818721167[0m
[37m[1m[2023-07-11 15:26:25,710][233954] Mean Reward across all agents: 215.1636906543373[0m
[37m[1m[2023-07-11 15:26:25,710][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:26:25,713][233954] mean_value=-170.1012327276575, max_value=621.0082619168849[0m
[37m[1m[2023-07-11 15:26:25,715][233954] New mean coefficients: [[ 2.0675097  -7.2836847   0.94256353 -1.3326547  -2.374154    0.5797889 ]][0m
[37m[1m[2023-07-11 15:26:25,716][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:26:34,791][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 15:26:34,792][233954] FPS: 423207.38[0m
[36m[2023-07-11 15:26:34,794][233954] itr=1128, itrs=2000, Progress: 56.40%[0m
[36m[2023-07-11 15:26:46,577][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 15:26:46,578][233954] FPS: 328563.43[0m
[36m[2023-07-11 15:26:50,815][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:26:50,820][233954] Reward + Measures: [[293.99745295   0.13373899   0.63523966   0.33521932   0.60415
    3.36290288]][0m
[37m[1m[2023-07-11 15:26:50,821][233954] Max Reward on eval: 293.9974529462749[0m
[37m[1m[2023-07-11 15:26:50,821][233954] Min Reward on eval: 293.9974529462749[0m
[37m[1m[2023-07-11 15:26:50,821][233954] Mean Reward across all agents: 293.9974529462749[0m
[37m[1m[2023-07-11 15:26:50,822][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:26:55,792][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:26:55,792][233954] Reward + Measures: [[-35.42933734   0.31370002   0.53740001   0.18430001   0.53640002
    3.38888407]
 [308.93474987   0.11530001   0.63609999   0.33199999   0.59090006
    3.2757833 ]
 [381.29874955   0.0515       0.6275       0.38120002   0.60299999
    3.29591036]
 ...
 [181.11567162   0.21729998   0.70649999   0.31080002   0.67720002
    3.49525619]
 [140.57009744   0.30460003   0.79640001   0.29500002   0.77880001
    3.44199634]
 [285.82730736   0.2642       0.68660003   0.35640001   0.72420001
    3.39870143]][0m
[37m[1m[2023-07-11 15:26:55,792][233954] Max Reward on eval: 635.5749140155967[0m
[37m[1m[2023-07-11 15:26:55,793][233954] Min Reward on eval: -199.04824941400437[0m
[37m[1m[2023-07-11 15:26:55,793][233954] Mean Reward across all agents: 238.7886670871145[0m
[37m[1m[2023-07-11 15:26:55,793][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:26:55,795][233954] mean_value=-161.1026504399863, max_value=550.2114954043507[0m
[37m[1m[2023-07-11 15:26:55,798][233954] New mean coefficients: [[ 1.7683387 -6.395912   0.7360002 -1.1634037 -2.9311876  0.9222863]][0m
[37m[1m[2023-07-11 15:26:55,799][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:27:04,812][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 15:27:04,812][233954] FPS: 426125.38[0m
[36m[2023-07-11 15:27:04,814][233954] itr=1129, itrs=2000, Progress: 56.45%[0m
[36m[2023-07-11 15:27:16,446][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 15:27:16,446][233954] FPS: 332891.37[0m
[36m[2023-07-11 15:27:20,788][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:27:20,794][233954] Reward + Measures: [[303.50160832   0.13265967   0.63928032   0.33497563   0.60667235
    3.37318802]][0m
[37m[1m[2023-07-11 15:27:20,794][233954] Max Reward on eval: 303.50160832225794[0m
[37m[1m[2023-07-11 15:27:20,795][233954] Min Reward on eval: 303.50160832225794[0m
[37m[1m[2023-07-11 15:27:20,795][233954] Mean Reward across all agents: 303.50160832225794[0m
[37m[1m[2023-07-11 15:27:20,795][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:27:25,754][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:27:25,754][233954] Reward + Measures: [[-48.79503991   0.40899998   0.87419999   0.27759999   0.86390001
    3.39692926]
 [209.58125113   0.3064       0.6638       0.22930001   0.64749998
    3.38164973]
 [401.86126353   0.15899999   0.88849992   0.47880003   0.87210006
    3.4629457 ]
 ...
 [325.11111359   0.15539999   0.70570004   0.3633       0.7051
    3.46754265]
 [ 99.69158875   0.29659998   0.89820004   0.36840001   0.86100006
    3.48921514]
 [448.32669687   0.0308       0.66440004   0.42420003   0.63880002
    3.44429326]][0m
[37m[1m[2023-07-11 15:27:25,755][233954] Max Reward on eval: 734.4296255219728[0m
[37m[1m[2023-07-11 15:27:25,755][233954] Min Reward on eval: -292.7450819134712[0m
[37m[1m[2023-07-11 15:27:25,755][233954] Mean Reward across all agents: 272.8763946046262[0m
[37m[1m[2023-07-11 15:27:25,755][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:27:25,758][233954] mean_value=-158.76719167653312, max_value=285.6715484070313[0m
[37m[1m[2023-07-11 15:27:25,760][233954] New mean coefficients: [[ 1.4082754  -6.0066404   0.25639063 -1.089176   -2.7740211   0.71727335]][0m
[37m[1m[2023-07-11 15:27:25,761][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:27:34,796][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 15:27:34,796][233954] FPS: 425107.70[0m
[36m[2023-07-11 15:27:34,798][233954] itr=1130, itrs=2000, Progress: 56.50%[0m
[37m[1m[2023-07-11 15:31:04,848][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001110[0m
[36m[2023-07-11 15:31:17,189][233954] train() took 11.77 seconds to complete[0m
[36m[2023-07-11 15:31:17,190][233954] FPS: 326187.21[0m
[36m[2023-07-11 15:31:21,417][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:31:21,418][233954] Reward + Measures: [[307.11250881   0.13368201   0.63547063   0.333258     0.60364968
    3.37620544]][0m
[37m[1m[2023-07-11 15:31:21,418][233954] Max Reward on eval: 307.11250880874326[0m
[37m[1m[2023-07-11 15:31:21,418][233954] Min Reward on eval: 307.11250880874326[0m
[37m[1m[2023-07-11 15:31:21,418][233954] Mean Reward across all agents: 307.11250880874326[0m
[37m[1m[2023-07-11 15:31:21,419][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:31:26,402][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:31:26,403][233954] Reward + Measures: [[ 81.11348012   0.1943       0.491        0.2191       0.48129997
    3.18301845]
 [365.6686329    0.0396       0.6135       0.3856       0.60200006
    3.3140161 ]
 [290.25950109   0.1061       0.64240003   0.3527       0.59700006
    3.43606043]
 ...
 [274.55074739   0.0958       0.54369998   0.30150002   0.51920003
    3.20371509]
 [298.33467225   0.1613       0.79360002   0.42870003   0.78060001
    3.38903213]
 [378.43982163   0.28909999   0.77389997   0.39069998   0.79539996
    3.31226707]][0m
[37m[1m[2023-07-11 15:31:26,403][233954] Max Reward on eval: 644.2245857156813[0m
[37m[1m[2023-07-11 15:31:26,404][233954] Min Reward on eval: -254.11053712330758[0m
[37m[1m[2023-07-11 15:31:26,404][233954] Mean Reward across all agents: 227.06275741942508[0m
[37m[1m[2023-07-11 15:31:26,404][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:31:26,407][233954] mean_value=-155.68119185261932, max_value=174.44071111061754[0m
[37m[1m[2023-07-11 15:31:26,409][233954] New mean coefficients: [[ 1.3098534  -4.0646496   0.68777543 -1.4265169  -2.6352026   0.5154455 ]][0m
[37m[1m[2023-07-11 15:31:26,410][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:31:35,317][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 15:31:35,323][233954] FPS: 431166.56[0m
[36m[2023-07-11 15:31:35,329][233954] itr=1131, itrs=2000, Progress: 56.55%[0m
[36m[2023-07-11 15:31:46,957][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 15:31:46,957][233954] FPS: 332958.12[0m
[36m[2023-07-11 15:31:51,219][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:31:51,220][233954] Reward + Measures: [[335.87257473   0.11404233   0.65234232   0.35401163   0.61838329
    3.38920307]][0m
[37m[1m[2023-07-11 15:31:51,220][233954] Max Reward on eval: 335.87257472927354[0m
[37m[1m[2023-07-11 15:31:51,220][233954] Min Reward on eval: 335.87257472927354[0m
[37m[1m[2023-07-11 15:31:51,221][233954] Mean Reward across all agents: 335.87257472927354[0m
[37m[1m[2023-07-11 15:31:51,221][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:31:56,199][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:31:56,199][233954] Reward + Measures: [[ 430.35908363    0.1217        0.73080009    0.38870001    0.69020003
     3.42303586]
 [  -7.40966864    0.15179999    0.34669998    0.1425        0.32319999
     3.28847623]
 [ 119.45693259    0.0627        0.28010002    0.1732        0.25749999
     3.23606849]
 ...
 [ 313.13639684    0.21710001    0.54000008    0.23130003    0.50650001
     3.29882669]
 [ 227.50982474    0.1362        0.4553        0.23200002    0.41290003
     3.20147014]
 [-151.00404827    0.24889998    0.32049999    0.0675        0.27250001
     3.19737697]][0m
[37m[1m[2023-07-11 15:31:56,199][233954] Max Reward on eval: 643.7049196375999[0m
[37m[1m[2023-07-11 15:31:56,200][233954] Min Reward on eval: -280.2922249595402[0m
[37m[1m[2023-07-11 15:31:56,200][233954] Mean Reward across all agents: 197.51329710309585[0m
[37m[1m[2023-07-11 15:31:56,200][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:31:56,203][233954] mean_value=-149.46628714677695, max_value=349.27920833977936[0m
[37m[1m[2023-07-11 15:31:56,205][233954] New mean coefficients: [[ 1.2289107  -2.6349852   0.38674253 -1.3127843  -2.6974528   0.37626034]][0m
[37m[1m[2023-07-11 15:31:56,206][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:32:05,224][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 15:32:05,225][233954] FPS: 425864.90[0m
[36m[2023-07-11 15:32:05,227][233954] itr=1132, itrs=2000, Progress: 56.60%[0m
[36m[2023-07-11 15:32:17,046][233954] train() took 11.72 seconds to complete[0m
[36m[2023-07-11 15:32:17,046][233954] FPS: 327554.29[0m
[36m[2023-07-11 15:32:21,267][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:32:21,268][233954] Reward + Measures: [[308.55460216   0.11428833   0.62022233   0.33778802   0.58710301
    3.38052106]][0m
[37m[1m[2023-07-11 15:32:21,268][233954] Max Reward on eval: 308.5546021590353[0m
[37m[1m[2023-07-11 15:32:21,268][233954] Min Reward on eval: 308.5546021590353[0m
[37m[1m[2023-07-11 15:32:21,269][233954] Mean Reward across all agents: 308.5546021590353[0m
[37m[1m[2023-07-11 15:32:21,269][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:32:26,182][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:32:26,189][233954] Reward + Measures: [[343.4431613    0.1178       0.64600003   0.384        0.62010002
    3.40600371]
 [428.65821122   0.0457       0.53689998   0.33680001   0.53150004
    3.45376754]
 [111.81960583   0.26029998   0.60799998   0.24660002   0.60550004
    3.2988708 ]
 ...
 [194.25874073   0.13540001   0.54630005   0.26180002   0.51639998
    3.254704  ]
 [203.62448203   0.1541       0.329        0.1681       0.33090001
    3.16127372]
 [151.94204232   0.0777       0.36479998   0.22500001   0.34940001
    3.23906517]][0m
[37m[1m[2023-07-11 15:32:26,189][233954] Max Reward on eval: 719.6663226952776[0m
[37m[1m[2023-07-11 15:32:26,190][233954] Min Reward on eval: -123.71903162617237[0m
[37m[1m[2023-07-11 15:32:26,190][233954] Mean Reward across all agents: 303.5926942637325[0m
[37m[1m[2023-07-11 15:32:26,191][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:32:26,198][233954] mean_value=-90.8181247624458, max_value=471.1080054175509[0m
[37m[1m[2023-07-11 15:32:26,202][233954] New mean coefficients: [[ 0.5618361  -2.9278493   0.25117207 -1.239648   -2.5224404   0.6102217 ]][0m
[37m[1m[2023-07-11 15:32:26,204][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:32:35,113][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 15:32:35,113][233954] FPS: 431106.46[0m
[36m[2023-07-11 15:32:35,115][233954] itr=1133, itrs=2000, Progress: 56.65%[0m
[36m[2023-07-11 15:32:46,789][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 15:32:46,789][233954] FPS: 331626.32[0m
[36m[2023-07-11 15:32:51,099][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:32:51,100][233954] Reward + Measures: [[330.98241217   0.10994501   0.63061863   0.34566033   0.59506232
    3.38555098]][0m
[37m[1m[2023-07-11 15:32:51,100][233954] Max Reward on eval: 330.9824121654707[0m
[37m[1m[2023-07-11 15:32:51,100][233954] Min Reward on eval: 330.9824121654707[0m
[37m[1m[2023-07-11 15:32:51,100][233954] Mean Reward across all agents: 330.9824121654707[0m
[37m[1m[2023-07-11 15:32:51,101][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:32:56,340][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:32:56,346][233954] Reward + Measures: [[289.71141755   0.17749999   0.62260002   0.30370003   0.6038
    3.31083417]
 [275.93858933   0.0574       0.41660005   0.26310003   0.40740004
    3.33980227]
 [421.34488684   0.17920001   0.69839996   0.33290002   0.67650002
    3.44361377]
 ...
 [283.60429586   0.04         0.44600001   0.29260001   0.45039997
    3.37260294]
 [309.58813357   0.0332       0.48270002   0.28999999   0.43389997
    3.31399155]
 [352.81150732   0.0388       0.54930001   0.35470003   0.51749998
    3.38569236]][0m
[37m[1m[2023-07-11 15:32:56,346][233954] Max Reward on eval: 633.8442507924046[0m
[37m[1m[2023-07-11 15:32:56,347][233954] Min Reward on eval: -105.22545960135758[0m
[37m[1m[2023-07-11 15:32:56,347][233954] Mean Reward across all agents: 246.18104271840426[0m
[37m[1m[2023-07-11 15:32:56,347][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:32:56,350][233954] mean_value=-113.63540866856403, max_value=348.82001622167445[0m
[37m[1m[2023-07-11 15:32:56,352][233954] New mean coefficients: [[ 1.000608   -2.4642596   0.49119565 -1.0373387  -2.2986784   0.34004053]][0m
[37m[1m[2023-07-11 15:32:56,353][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:33:05,383][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 15:33:05,384][233954] FPS: 425325.28[0m
[36m[2023-07-11 15:33:05,386][233954] itr=1134, itrs=2000, Progress: 56.70%[0m
[36m[2023-07-11 15:33:17,292][233954] train() took 11.81 seconds to complete[0m
[36m[2023-07-11 15:33:17,292][233954] FPS: 325152.35[0m
[36m[2023-07-11 15:33:21,566][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:33:21,566][233954] Reward + Measures: [[318.21376213   0.10417533   0.61084127   0.33843264   0.5756467
    3.37705278]][0m
[37m[1m[2023-07-11 15:33:21,566][233954] Max Reward on eval: 318.21376213210897[0m
[37m[1m[2023-07-11 15:33:21,567][233954] Min Reward on eval: 318.21376213210897[0m
[37m[1m[2023-07-11 15:33:21,567][233954] Mean Reward across all agents: 318.21376213210897[0m
[37m[1m[2023-07-11 15:33:21,567][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:33:26,610][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:33:26,610][233954] Reward + Measures: [[416.67673295   0.09980001   0.64200002   0.36200002   0.61519998
    3.31829453]
 [424.00791358   0.1053       0.80559999   0.47199997   0.79299998
    3.44516921]
 [324.41667696   0.1071       0.70180005   0.3759       0.67640001
    3.31318331]
 ...
 [123.4194562    0.11830001   0.55670005   0.31389999   0.52770001
    3.29472804]
 [116.01901601   0.14119999   0.32440001   0.13880001   0.28190002
    3.29659891]
 [361.42514636   0.0779       0.61629999   0.38859999   0.6002
    3.39896822]][0m
[37m[1m[2023-07-11 15:33:26,611][233954] Max Reward on eval: 634.2224503187463[0m
[37m[1m[2023-07-11 15:33:26,611][233954] Min Reward on eval: -163.36140546910465[0m
[37m[1m[2023-07-11 15:33:26,611][233954] Mean Reward across all agents: 208.75696622796906[0m
[37m[1m[2023-07-11 15:33:26,611][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:33:26,614][233954] mean_value=-153.5450700566655, max_value=215.73959948328627[0m
[37m[1m[2023-07-11 15:33:26,616][233954] New mean coefficients: [[ 0.12050676 -2.4060006   0.9191867  -1.1579534  -2.1636322  -0.05344778]][0m
[37m[1m[2023-07-11 15:33:26,617][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:33:35,695][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 15:33:35,695][233954] FPS: 423095.11[0m
[36m[2023-07-11 15:33:35,697][233954] itr=1135, itrs=2000, Progress: 56.75%[0m
[36m[2023-07-11 15:33:47,469][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 15:33:47,469][233954] FPS: 328900.80[0m
[36m[2023-07-11 15:33:51,778][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:33:51,783][233954] Reward + Measures: [[329.2449957    0.10926432   0.61529332   0.34172803   0.583924
    3.38524604]][0m
[37m[1m[2023-07-11 15:33:51,784][233954] Max Reward on eval: 329.2449956980541[0m
[37m[1m[2023-07-11 15:33:51,784][233954] Min Reward on eval: 329.2449956980541[0m
[37m[1m[2023-07-11 15:33:51,784][233954] Mean Reward across all agents: 329.2449956980541[0m
[37m[1m[2023-07-11 15:33:51,785][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:33:56,833][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:33:56,838][233954] Reward + Measures: [[259.32410338   0.1042       0.67650002   0.40980002   0.6645
    3.41044807]
 [ 77.13543822   0.34709999   0.80380005   0.30860001   0.78600001
    3.48880458]
 [346.96578026   0.20609999   0.73330003   0.3712       0.72909999
    3.45218205]
 ...
 [219.26596411   0.12409999   0.64539999   0.37490001   0.61570001
    3.33083463]
 [331.40813922   0.1947       0.88999999   0.44460002   0.86350006
    3.55701876]
 [360.25458314   0.0605       0.63009995   0.39140001   0.59750003
    3.30474091]][0m
[37m[1m[2023-07-11 15:33:56,838][233954] Max Reward on eval: 606.1564197355881[0m
[37m[1m[2023-07-11 15:33:56,838][233954] Min Reward on eval: -301.7985296273604[0m
[37m[1m[2023-07-11 15:33:56,839][233954] Mean Reward across all agents: 263.6081376811201[0m
[37m[1m[2023-07-11 15:33:56,839][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:33:56,841][233954] mean_value=-154.36913259654028, max_value=395.8613587114286[0m
[37m[1m[2023-07-11 15:33:56,844][233954] New mean coefficients: [[ 0.26671493 -0.7121233   1.0161283  -0.9931098  -2.4250467   0.09798042]][0m
[37m[1m[2023-07-11 15:33:56,845][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:34:05,886][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 15:34:05,886][233954] FPS: 424825.13[0m
[36m[2023-07-11 15:34:05,888][233954] itr=1136, itrs=2000, Progress: 56.80%[0m
[36m[2023-07-11 15:34:17,709][233954] train() took 11.72 seconds to complete[0m
[36m[2023-07-11 15:34:17,709][233954] FPS: 327540.98[0m
[36m[2023-07-11 15:34:21,957][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:34:21,958][233954] Reward + Measures: [[312.20686589   0.10331699   0.58796901   0.32646468   0.55216867
    3.36759067]][0m
[37m[1m[2023-07-11 15:34:21,958][233954] Max Reward on eval: 312.2068658879005[0m
[37m[1m[2023-07-11 15:34:21,958][233954] Min Reward on eval: 312.2068658879005[0m
[37m[1m[2023-07-11 15:34:21,959][233954] Mean Reward across all agents: 312.2068658879005[0m
[37m[1m[2023-07-11 15:34:21,959][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:34:26,936][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:34:26,936][233954] Reward + Measures: [[214.45156472   0.10220001   0.77319998   0.3849       0.76179999
    3.23833394]
 [122.46870168   0.0969       0.3858       0.18679999   0.31209999
    3.27366519]
 [281.64651907   0.204        0.7263       0.3353       0.69380003
    3.40051389]
 ...
 [ 19.26633033   0.24439998   0.51189995   0.1912       0.51500005
    3.38620734]
 [174.11872477   0.21330002   0.54390001   0.25870001   0.52210003
    3.33885694]
 [142.94987395   0.13610001   0.5025       0.24910001   0.44770002
    3.30636764]][0m
[37m[1m[2023-07-11 15:34:26,937][233954] Max Reward on eval: 650.5489239246584[0m
[37m[1m[2023-07-11 15:34:26,937][233954] Min Reward on eval: -111.17823969582096[0m
[37m[1m[2023-07-11 15:34:26,937][233954] Mean Reward across all agents: 236.107181342873[0m
[37m[1m[2023-07-11 15:34:26,937][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:34:26,939][233954] mean_value=-146.28261789839556, max_value=273.3400960146711[0m
[37m[1m[2023-07-11 15:34:26,942][233954] New mean coefficients: [[ 0.4519682  -0.9540564   0.39916068 -0.9315365  -2.3219209  -0.19557573]][0m
[37m[1m[2023-07-11 15:34:26,943][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:34:35,966][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 15:34:35,966][233954] FPS: 425658.20[0m
[36m[2023-07-11 15:34:35,968][233954] itr=1137, itrs=2000, Progress: 56.85%[0m
[36m[2023-07-11 15:34:47,731][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 15:34:47,731][233954] FPS: 329276.27[0m
[36m[2023-07-11 15:34:52,040][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:34:52,040][233954] Reward + Measures: [[286.70759518   0.109215     0.56640434   0.30976701   0.52885866
    3.35434437]][0m
[37m[1m[2023-07-11 15:34:52,041][233954] Max Reward on eval: 286.7075951783853[0m
[37m[1m[2023-07-11 15:34:52,041][233954] Min Reward on eval: 286.7075951783853[0m
[37m[1m[2023-07-11 15:34:52,041][233954] Mean Reward across all agents: 286.7075951783853[0m
[37m[1m[2023-07-11 15:34:52,041][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:34:56,988][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:34:56,988][233954] Reward + Measures: [[198.04005363   0.0384       0.36410001   0.2149       0.30759999
    3.26778722]
 [ -6.58468626   0.14430001   0.3827       0.17839999   0.361
    3.29469466]
 [486.1532316    0.1429       0.74590003   0.46000004   0.76200002
    3.48193598]
 ...
 [365.82885882   0.0479       0.8818       0.57110006   0.86710006
    3.46476531]
 [223.31038506   0.0355       0.40630004   0.2192       0.32690001
    3.23234248]
 [ 22.87185068   0.30399999   0.45700002   0.11360001   0.42810002
    3.32009196]][0m
[37m[1m[2023-07-11 15:34:56,988][233954] Max Reward on eval: 626.281683219783[0m
[37m[1m[2023-07-11 15:34:56,989][233954] Min Reward on eval: -301.3829698326299[0m
[37m[1m[2023-07-11 15:34:56,989][233954] Mean Reward across all agents: 224.0054744205053[0m
[37m[1m[2023-07-11 15:34:56,989][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:34:56,992][233954] mean_value=-147.4500907118928, max_value=235.17636226993596[0m
[37m[1m[2023-07-11 15:34:56,994][233954] New mean coefficients: [[ 0.78986955 -1.147045    0.8935163  -1.035675   -2.3164718  -0.04015915]][0m
[37m[1m[2023-07-11 15:34:56,995][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:35:06,011][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 15:35:06,011][233954] FPS: 425991.91[0m
[36m[2023-07-11 15:35:06,013][233954] itr=1138, itrs=2000, Progress: 56.90%[0m
[36m[2023-07-11 15:35:17,700][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 15:35:17,701][233954] FPS: 331252.01[0m
[36m[2023-07-11 15:35:21,967][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:35:21,967][233954] Reward + Measures: [[274.60484891   0.10436999   0.54577434   0.29660833   0.50448501
    3.33830476]][0m
[37m[1m[2023-07-11 15:35:21,967][233954] Max Reward on eval: 274.6048489141126[0m
[37m[1m[2023-07-11 15:35:21,968][233954] Min Reward on eval: 274.6048489141126[0m
[37m[1m[2023-07-11 15:35:21,968][233954] Mean Reward across all agents: 274.6048489141126[0m
[37m[1m[2023-07-11 15:35:21,968][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:35:27,158][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:35:27,158][233954] Reward + Measures: [[431.250309     0.11589999   0.80109996   0.47480002   0.78389996
    3.50279236]
 [142.81060453   0.1664       0.6929       0.30109999   0.67190003
    3.32727218]
 [ 93.20587489   0.1286       0.41219997   0.1997       0.36900002
    3.21537662]
 ...
 [143.63384726   0.34419999   0.78229994   0.30670002   0.77509993
    3.36000705]
 [224.91150109   0.0871       0.61730003   0.33449998   0.59230006
    3.28486037]
 [ 30.7706981    0.29410002   0.62880003   0.19850001   0.58700001
    3.29244614]][0m
[37m[1m[2023-07-11 15:35:27,159][233954] Max Reward on eval: 628.5122657436877[0m
[37m[1m[2023-07-11 15:35:27,159][233954] Min Reward on eval: -244.85145042054356[0m
[37m[1m[2023-07-11 15:35:27,159][233954] Mean Reward across all agents: 251.8078724957858[0m
[37m[1m[2023-07-11 15:35:27,159][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:35:27,162][233954] mean_value=-144.3166688014868, max_value=194.82661767668577[0m
[37m[1m[2023-07-11 15:35:27,164][233954] New mean coefficients: [[ 1.0949132  -1.2458727   1.339102   -0.6241053  -1.8646078  -0.31116378]][0m
[37m[1m[2023-07-11 15:35:27,165][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:35:36,177][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 15:35:36,177][233954] FPS: 426169.39[0m
[36m[2023-07-11 15:35:36,179][233954] itr=1139, itrs=2000, Progress: 56.95%[0m
[36m[2023-07-11 15:35:47,862][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 15:35:47,863][233954] FPS: 331442.89[0m
[36m[2023-07-11 15:35:52,140][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:35:52,140][233954] Reward + Measures: [[245.52661868   0.109158     0.52067763   0.27742168   0.47516301
    3.31581926]][0m
[37m[1m[2023-07-11 15:35:52,141][233954] Max Reward on eval: 245.5266186824725[0m
[37m[1m[2023-07-11 15:35:52,141][233954] Min Reward on eval: 245.5266186824725[0m
[37m[1m[2023-07-11 15:35:52,141][233954] Mean Reward across all agents: 245.5266186824725[0m
[37m[1m[2023-07-11 15:35:52,141][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:35:57,170][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:35:57,175][233954] Reward + Measures: [[177.95988672   0.26210001   0.65240002   0.24569999   0.616
    3.27900386]
 [291.9454363    0.1346       0.76620001   0.4025       0.72310001
    3.44003487]
 [ 87.01764753   0.21350001   0.55619997   0.2253       0.53479999
    3.35681224]
 ...
 [ 77.25677347   0.13499999   0.32680002   0.13829999   0.2933
    3.32007408]
 [282.93170702   0.1058       0.54659998   0.30520001   0.51820004
    3.22751808]
 [ 95.07709901   0.07770001   0.26540002   0.1973       0.2472
    3.04878306]][0m
[37m[1m[2023-07-11 15:35:57,176][233954] Max Reward on eval: 625.453481664788[0m
[37m[1m[2023-07-11 15:35:57,176][233954] Min Reward on eval: -117.73461031597108[0m
[37m[1m[2023-07-11 15:35:57,176][233954] Mean Reward across all agents: 219.81567445953678[0m
[37m[1m[2023-07-11 15:35:57,177][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:35:57,179][233954] mean_value=-145.0992859410414, max_value=169.57691633716797[0m
[37m[1m[2023-07-11 15:35:57,182][233954] New mean coefficients: [[ 1.3285198  -1.4214256   1.0194939  -0.565325   -2.4207242  -0.12300749]][0m
[37m[1m[2023-07-11 15:35:57,183][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:36:06,298][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 15:36:06,298][233954] FPS: 421357.55[0m
[36m[2023-07-11 15:36:06,300][233954] itr=1140, itrs=2000, Progress: 57.00%[0m
[37m[1m[2023-07-11 15:39:35,454][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001120[0m
[36m[2023-07-11 15:39:47,706][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 15:39:47,707][233954] FPS: 329454.57[0m
[36m[2023-07-11 15:39:51,918][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:39:51,919][233954] Reward + Measures: [[270.78306873   0.09309533   0.52328098   0.29352367   0.48043534
    3.31392384]][0m
[37m[1m[2023-07-11 15:39:51,919][233954] Max Reward on eval: 270.78306873155714[0m
[37m[1m[2023-07-11 15:39:51,919][233954] Min Reward on eval: 270.78306873155714[0m
[37m[1m[2023-07-11 15:39:51,919][233954] Mean Reward across all agents: 270.78306873155714[0m
[37m[1m[2023-07-11 15:39:51,920][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:39:56,886][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:39:56,886][233954] Reward + Measures: [[ 81.43510971   0.15090001   0.37139997   0.15709999   0.34180003
    3.26856041]
 [275.83680418   0.12730001   0.55669999   0.29769999   0.53050005
    3.3098619 ]
 [122.07776584   0.19320001   0.54659998   0.20190001   0.49239999
    3.32208991]
 ...
 [181.28236179   0.23889999   0.63490003   0.2581       0.61159998
    3.41728592]
 [384.973413     0.0711       0.71110004   0.38159999   0.70210004
    3.47491908]
 [ 88.21167904   0.2271       0.55800003   0.21370001   0.49689999
    3.29879236]][0m
[37m[1m[2023-07-11 15:39:56,887][233954] Max Reward on eval: 560.7615335439798[0m
[37m[1m[2023-07-11 15:39:56,887][233954] Min Reward on eval: -208.68007564451545[0m
[37m[1m[2023-07-11 15:39:56,887][233954] Mean Reward across all agents: 207.6598489729441[0m
[37m[1m[2023-07-11 15:39:56,887][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:39:56,890][233954] mean_value=-146.01202560250584, max_value=93.08287735466519[0m
[37m[1m[2023-07-11 15:39:56,892][233954] New mean coefficients: [[ 0.55553776 -1.0293345   0.8121718  -0.3959368  -2.327003    0.33439463]][0m
[37m[1m[2023-07-11 15:39:56,893][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:40:05,982][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 15:40:05,983][233954] FPS: 422539.08[0m
[36m[2023-07-11 15:40:05,985][233954] itr=1141, itrs=2000, Progress: 57.05%[0m
[36m[2023-07-11 15:40:17,551][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 15:40:17,551][233954] FPS: 334744.69[0m
[36m[2023-07-11 15:40:21,803][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:40:21,804][233954] Reward + Measures: [[258.81578965   0.097019     0.5046463    0.28073633   0.45799133
    3.2972312 ]][0m
[37m[1m[2023-07-11 15:40:21,804][233954] Max Reward on eval: 258.81578965078467[0m
[37m[1m[2023-07-11 15:40:21,804][233954] Min Reward on eval: 258.81578965078467[0m
[37m[1m[2023-07-11 15:40:21,804][233954] Mean Reward across all agents: 258.81578965078467[0m
[37m[1m[2023-07-11 15:40:21,805][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:40:26,724][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:40:26,725][233954] Reward + Measures: [[ -11.5050354     0.51910001    0.92889994    0.2079        0.90410006
     3.04086018]
 [  97.29698279    0.14000002    0.48289999    0.23220001    0.42799997
     3.10428214]
 [-119.46789943    0.3488        0.45989999    0.1356        0.4743
     3.07661366]
 ...
 [ 271.58476922    0.15869999    0.653         0.62010002    0.73400003
     3.21258736]
 [ 301.56045437    0.0642        0.76400006    0.61530006    0.77639997
     3.20785689]
 [ 342.88801287    0.1657        0.75319999    0.57849997    0.77200001
     3.10269475]][0m
[37m[1m[2023-07-11 15:40:26,725][233954] Max Reward on eval: 556.6719593622722[0m
[37m[1m[2023-07-11 15:40:26,725][233954] Min Reward on eval: -166.36389182789716[0m
[37m[1m[2023-07-11 15:40:26,726][233954] Mean Reward across all agents: 167.74225666599366[0m
[37m[1m[2023-07-11 15:40:26,726][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:40:26,728][233954] mean_value=-155.87374362995047, max_value=172.42857368235747[0m
[37m[1m[2023-07-11 15:40:26,730][233954] New mean coefficients: [[ 0.81416094 -1.1232158   0.95088094 -0.67514485 -2.6930194   0.25531584]][0m
[37m[1m[2023-07-11 15:40:26,731][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:40:35,710][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 15:40:35,711][233954] FPS: 427739.45[0m
[36m[2023-07-11 15:40:35,713][233954] itr=1142, itrs=2000, Progress: 57.10%[0m
[36m[2023-07-11 15:40:47,348][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 15:40:47,349][233954] FPS: 332836.50[0m
[36m[2023-07-11 15:40:51,665][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:40:51,665][233954] Reward + Measures: [[236.51011708   0.09690667   0.49011165   0.26490033   0.43856499
    3.2898531 ]][0m
[37m[1m[2023-07-11 15:40:51,666][233954] Max Reward on eval: 236.51011707922086[0m
[37m[1m[2023-07-11 15:40:51,666][233954] Min Reward on eval: 236.51011707922086[0m
[37m[1m[2023-07-11 15:40:51,666][233954] Mean Reward across all agents: 236.51011707922086[0m
[37m[1m[2023-07-11 15:40:51,667][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:40:56,927][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:40:56,928][233954] Reward + Measures: [[ 76.6733973    0.0797       0.2872       0.14399999   0.24530001
    3.12057686]
 [399.76706645   0.0346       0.55490005   0.35179999   0.5183
    3.33460093]
 [494.479722     0.0227       0.65999997   0.43470001   0.6124
    3.43017077]
 ...
 [  7.16626535   0.1452       0.48160002   0.19649999   0.4032
    3.20740294]
 [174.07921649   0.17309999   0.42379999   0.16610001   0.39130002
    3.20891356]
 [234.20116674   0.0485       0.39739999   0.2703       0.33950001
    3.2013104 ]][0m
[37m[1m[2023-07-11 15:40:56,928][233954] Max Reward on eval: 502.8971922399011[0m
[37m[1m[2023-07-11 15:40:56,928][233954] Min Reward on eval: -254.98061300066766[0m
[37m[1m[2023-07-11 15:40:56,929][233954] Mean Reward across all agents: 191.05658362401914[0m
[37m[1m[2023-07-11 15:40:56,929][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:40:56,931][233954] mean_value=-120.84723070946144, max_value=117.50324661752325[0m
[37m[1m[2023-07-11 15:40:56,934][233954] New mean coefficients: [[ 0.6168638  -1.5686002   0.9499219  -0.97376645 -2.5276198   0.20434411]][0m
[37m[1m[2023-07-11 15:40:56,935][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:41:05,880][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 15:41:05,880][233954] FPS: 429349.42[0m
[36m[2023-07-11 15:41:05,883][233954] itr=1143, itrs=2000, Progress: 57.15%[0m
[36m[2023-07-11 15:41:17,530][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 15:41:17,530][233954] FPS: 332516.44[0m
[36m[2023-07-11 15:41:21,825][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:41:21,826][233954] Reward + Measures: [[240.58263839   0.09036932   0.4646377    0.25904298   0.41611001
    3.2872386 ]][0m
[37m[1m[2023-07-11 15:41:21,826][233954] Max Reward on eval: 240.5826383941734[0m
[37m[1m[2023-07-11 15:41:21,826][233954] Min Reward on eval: 240.5826383941734[0m
[37m[1m[2023-07-11 15:41:21,826][233954] Mean Reward across all agents: 240.5826383941734[0m
[37m[1m[2023-07-11 15:41:21,827][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:41:26,829][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:41:26,830][233954] Reward + Measures: [[ 21.39928897   0.11210001   0.2034       0.10480001   0.20560001
    3.19244671]
 [182.2983328    0.132        0.46300003   0.23380001   0.42860004
    3.24970984]
 [408.11616562   0.0322       0.54790002   0.34840003   0.53070003
    3.38295245]
 ...
 [459.6499876    0.0283       0.72850007   0.47209999   0.69960004
    3.41347957]
 [162.27054805   0.12819999   0.62290001   0.30389997   0.60589999
    3.37402511]
 [ 97.1904281    0.16080001   0.4377       0.1955       0.40310001
    3.24148536]][0m
[37m[1m[2023-07-11 15:41:26,830][233954] Max Reward on eval: 548.3574701751583[0m
[37m[1m[2023-07-11 15:41:26,831][233954] Min Reward on eval: -204.3323142172303[0m
[37m[1m[2023-07-11 15:41:26,831][233954] Mean Reward across all agents: 201.89512562080603[0m
[37m[1m[2023-07-11 15:41:26,831][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:41:26,833][233954] mean_value=-143.83744267792176, max_value=155.36528579806165[0m
[37m[1m[2023-07-11 15:41:26,836][233954] New mean coefficients: [[ 0.25086197 -1.2707953   0.8489425  -0.5547464  -1.910187    0.24576475]][0m
[37m[1m[2023-07-11 15:41:26,837][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:41:35,855][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 15:41:35,856][233954] FPS: 425848.80[0m
[36m[2023-07-11 15:41:35,858][233954] itr=1144, itrs=2000, Progress: 57.20%[0m
[36m[2023-07-11 15:41:47,585][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 15:41:47,585][233954] FPS: 330166.40[0m
[36m[2023-07-11 15:41:51,818][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:41:51,818][233954] Reward + Measures: [[217.50263902   0.09078499   0.44999099   0.24345365   0.39474797
    3.2838974 ]][0m
[37m[1m[2023-07-11 15:41:51,819][233954] Max Reward on eval: 217.50263901552935[0m
[37m[1m[2023-07-11 15:41:51,819][233954] Min Reward on eval: 217.50263901552935[0m
[37m[1m[2023-07-11 15:41:51,819][233954] Mean Reward across all agents: 217.50263901552935[0m
[37m[1m[2023-07-11 15:41:51,820][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:41:56,828][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:41:56,829][233954] Reward + Measures: [[106.78400567   0.22670002   0.71900004   0.30230001   0.66180003
    2.86015534]
 [ -8.25999459   0.16159999   0.2757       0.0944       0.22000001
    3.26013374]
 [-10.67460043   0.22970001   0.37799999   0.1148       0.32399997
    3.20230174]
 ...
 [161.55230096   0.26370001   0.69240004   0.2448       0.65830004
    2.85866022]
 [  3.76499333   0.12840001   0.36309999   0.16949999   0.34090003
    3.4161911 ]
 [-67.18499674   0.1416       0.20720001   0.06260001   0.1445
    3.12655258]][0m
[37m[1m[2023-07-11 15:41:56,829][233954] Max Reward on eval: 534.2650540061295[0m
[37m[1m[2023-07-11 15:41:56,830][233954] Min Reward on eval: -251.96646071393042[0m
[37m[1m[2023-07-11 15:41:56,830][233954] Mean Reward across all agents: 150.9353673071565[0m
[37m[1m[2023-07-11 15:41:56,830][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:41:56,832][233954] mean_value=-169.84033664837312, max_value=282.7781470440051[0m
[37m[1m[2023-07-11 15:41:56,835][233954] New mean coefficients: [[ 0.09669934  0.3914982   0.7891529  -0.4270527  -1.8711995   0.09756811]][0m
[37m[1m[2023-07-11 15:41:56,836][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:42:05,901][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 15:42:05,901][233954] FPS: 423666.32[0m
[36m[2023-07-11 15:42:05,903][233954] itr=1145, itrs=2000, Progress: 57.25%[0m
[36m[2023-07-11 15:42:17,592][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 15:42:17,592][233954] FPS: 331257.55[0m
[36m[2023-07-11 15:42:21,897][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:42:21,897][233954] Reward + Measures: [[219.89711712   0.09441867   0.45486131   0.24018733   0.39641967
    3.281147  ]][0m
[37m[1m[2023-07-11 15:42:21,897][233954] Max Reward on eval: 219.8971171175834[0m
[37m[1m[2023-07-11 15:42:21,898][233954] Min Reward on eval: 219.8971171175834[0m
[37m[1m[2023-07-11 15:42:21,898][233954] Mean Reward across all agents: 219.8971171175834[0m
[37m[1m[2023-07-11 15:42:21,898][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:42:26,858][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:42:26,859][233954] Reward + Measures: [[176.87464419   0.12290001   0.30560002   0.1401       0.26930001
    3.18114614]
 [344.36902847   0.1428       0.54170007   0.3872       0.59980005
    3.33904958]
 [140.22695085   0.1025       0.38249999   0.1743       0.31850001
    3.05754733]
 ...
 [ 91.55927546   0.056        0.3062       0.1816       0.23820002
    3.20815778]
 [-65.98322305   0.23629999   0.44490004   0.15890001   0.43479997
    3.26510692]
 [223.73256026   0.0643       0.36450002   0.27860001   0.35999998
    3.29897928]][0m
[37m[1m[2023-07-11 15:42:26,859][233954] Max Reward on eval: 647.1719848908484[0m
[37m[1m[2023-07-11 15:42:26,860][233954] Min Reward on eval: -65.98322305390612[0m
[37m[1m[2023-07-11 15:42:26,860][233954] Mean Reward across all agents: 188.17467871480684[0m
[37m[1m[2023-07-11 15:42:26,860][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:42:26,862][233954] mean_value=-131.9407065928207, max_value=136.64987939545165[0m
[37m[1m[2023-07-11 15:42:26,865][233954] New mean coefficients: [[ 0.06652032  0.11321679  1.1229355  -0.60642695 -2.0532618  -0.09976497]][0m
[37m[1m[2023-07-11 15:42:26,866][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:42:35,797][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 15:42:35,797][233954] FPS: 430020.54[0m
[36m[2023-07-11 15:42:35,800][233954] itr=1146, itrs=2000, Progress: 57.30%[0m
[36m[2023-07-11 15:42:47,391][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 15:42:47,391][233954] FPS: 334090.12[0m
[36m[2023-07-11 15:42:51,752][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:42:51,752][233954] Reward + Measures: [[198.33625197   0.09515434   0.43856135   0.23114033   0.37971199
    3.2714231 ]][0m
[37m[1m[2023-07-11 15:42:51,753][233954] Max Reward on eval: 198.33625196807444[0m
[37m[1m[2023-07-11 15:42:51,753][233954] Min Reward on eval: 198.33625196807444[0m
[37m[1m[2023-07-11 15:42:51,753][233954] Mean Reward across all agents: 198.33625196807444[0m
[37m[1m[2023-07-11 15:42:51,753][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:42:56,766][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:42:56,767][233954] Reward + Measures: [[531.2396811    0.021        0.68480003   0.45770001   0.66889995
    3.43360758]
 [ 42.83978099   0.0894       0.32780001   0.1507       0.2669
    3.23732948]
 [ 53.57635019   0.13880001   0.27310002   0.1041       0.22650002
    3.29572749]
 ...
 [140.38485677   0.11130001   0.41589999   0.1883       0.37149999
    3.18228149]
 [198.59189147   0.12810001   0.39189997   0.26139998   0.3628
    3.24908566]
 [226.13179875   0.0641       0.46690002   0.2572       0.396
    3.24131656]][0m
[37m[1m[2023-07-11 15:42:56,767][233954] Max Reward on eval: 546.4806421477348[0m
[37m[1m[2023-07-11 15:42:56,767][233954] Min Reward on eval: -174.68585297297687[0m
[37m[1m[2023-07-11 15:42:56,767][233954] Mean Reward across all agents: 180.08229853339978[0m
[37m[1m[2023-07-11 15:42:56,768][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:42:56,770][233954] mean_value=-138.7440992356069, max_value=238.9436345388114[0m
[37m[1m[2023-07-11 15:42:56,772][233954] New mean coefficients: [[-0.8797659   0.93086386  1.1063668  -0.48957723 -2.0497723   0.03797597]][0m
[37m[1m[2023-07-11 15:42:56,773][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:43:05,727][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 15:43:05,728][233954] FPS: 428928.91[0m
[36m[2023-07-11 15:43:05,730][233954] itr=1147, itrs=2000, Progress: 57.35%[0m
[36m[2023-07-11 15:43:17,309][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 15:43:17,309][233954] FPS: 334365.67[0m
[36m[2023-07-11 15:43:21,547][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:43:21,548][233954] Reward + Measures: [[169.29249589   0.09086701   0.40689468   0.21252465   0.34178999
    3.25428963]][0m
[37m[1m[2023-07-11 15:43:21,548][233954] Max Reward on eval: 169.2924958912748[0m
[37m[1m[2023-07-11 15:43:21,548][233954] Min Reward on eval: 169.2924958912748[0m
[37m[1m[2023-07-11 15:43:21,548][233954] Mean Reward across all agents: 169.2924958912748[0m
[37m[1m[2023-07-11 15:43:21,549][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:43:26,509][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:43:26,510][233954] Reward + Measures: [[353.19983929   0.06789999   0.50450003   0.34619999   0.50029999
    3.29392624]
 [309.99588117   0.05519999   0.51179999   0.324        0.5097
    3.33148813]
 [226.99253885   0.1128       0.51599997   0.25260001   0.41420004
    3.26357889]
 ...
 [235.7039734    0.20710002   0.61830002   0.29480001   0.62340003
    3.33815265]
 [ 36.74928522   0.12670001   0.33580002   0.117        0.234
    3.17983031]
 [208.98978525   0.16510002   0.40229997   0.21279998   0.3867
    3.33710408]][0m
[37m[1m[2023-07-11 15:43:26,510][233954] Max Reward on eval: 557.8933452089783[0m
[37m[1m[2023-07-11 15:43:26,511][233954] Min Reward on eval: -142.27771737670992[0m
[37m[1m[2023-07-11 15:43:26,511][233954] Mean Reward across all agents: 181.78647648939256[0m
[37m[1m[2023-07-11 15:43:26,511][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:43:26,513][233954] mean_value=-149.12740312371932, max_value=226.46197414617382[0m
[37m[1m[2023-07-11 15:43:26,515][233954] New mean coefficients: [[-1.0549273   0.9579065   0.9745542  -1.0691704  -2.4346197   0.30993688]][0m
[37m[1m[2023-07-11 15:43:26,516][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:43:35,496][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 15:43:35,501][233954] FPS: 427731.21[0m
[36m[2023-07-11 15:43:35,504][233954] itr=1148, itrs=2000, Progress: 57.40%[0m
[36m[2023-07-11 15:43:47,171][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 15:43:47,171][233954] FPS: 331799.47[0m
[36m[2023-07-11 15:43:51,516][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:43:51,516][233954] Reward + Measures: [[167.14523198   0.09585533   0.40694267   0.21153434   0.34329167
    3.25861907]][0m
[37m[1m[2023-07-11 15:43:51,516][233954] Max Reward on eval: 167.1452319819167[0m
[37m[1m[2023-07-11 15:43:51,517][233954] Min Reward on eval: 167.1452319819167[0m
[37m[1m[2023-07-11 15:43:51,517][233954] Mean Reward across all agents: 167.1452319819167[0m
[37m[1m[2023-07-11 15:43:51,517][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:43:56,832][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:43:56,833][233954] Reward + Measures: [[166.72540186   0.09959999   0.41019997   0.1954       0.35670003
    3.17482972]
 [ 38.57620178   0.2902       0.34709999   0.0498       0.31420001
    3.10400987]
 [104.25179624   0.13689999   0.3479       0.1638       0.30450001
    3.22556233]
 ...
 [154.03061067   0.15000001   0.46080002   0.22719999   0.43100005
    3.19769692]
 [242.99345261   0.126        0.46510002   0.2174       0.40769997
    3.20796514]
 [205.46951114   0.1305       0.59219998   0.2393       0.53369999
    3.23900986]][0m
[37m[1m[2023-07-11 15:43:56,833][233954] Max Reward on eval: 600.7329564911081[0m
[37m[1m[2023-07-11 15:43:56,833][233954] Min Reward on eval: -185.12494393838568[0m
[37m[1m[2023-07-11 15:43:56,833][233954] Mean Reward across all agents: 145.42776431181068[0m
[37m[1m[2023-07-11 15:43:56,834][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:43:56,836][233954] mean_value=-171.48279990457996, max_value=88.99033305148441[0m
[37m[1m[2023-07-11 15:43:56,838][233954] New mean coefficients: [[-0.7951313   1.1366762   0.8212044  -1.2802306  -1.9742814   0.01115748]][0m
[37m[1m[2023-07-11 15:43:56,839][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:44:05,964][233954] train() took 9.12 seconds to complete[0m
[36m[2023-07-11 15:44:05,964][233954] FPS: 420936.54[0m
[36m[2023-07-11 15:44:05,966][233954] itr=1149, itrs=2000, Progress: 57.45%[0m
[36m[2023-07-11 15:44:17,802][233954] train() took 11.74 seconds to complete[0m
[36m[2023-07-11 15:44:17,802][233954] FPS: 327072.79[0m
[36m[2023-07-11 15:44:22,092][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:44:22,093][233954] Reward + Measures: [[135.54483204   0.10039467   0.384168     0.19154932   0.31929967
    3.24907613]][0m
[37m[1m[2023-07-11 15:44:22,093][233954] Max Reward on eval: 135.54483203966558[0m
[37m[1m[2023-07-11 15:44:22,093][233954] Min Reward on eval: 135.54483203966558[0m
[37m[1m[2023-07-11 15:44:22,093][233954] Mean Reward across all agents: 135.54483203966558[0m
[37m[1m[2023-07-11 15:44:22,094][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:44:27,115][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:44:27,115][233954] Reward + Measures: [[ 54.03173654   0.1508       0.39899999   0.16610001   0.3351
    3.27778983]
 [289.16875324   0.141        0.46119997   0.20560001   0.40270001
    3.35790801]
 [198.6806781    0.073        0.5668       0.29070002   0.53240001
    3.28919864]
 ...
 [ 84.36318351   0.1051       0.43430001   0.1969       0.41680002
    3.15885234]
 [154.80717284   0.0541       0.28830001   0.1979       0.24680002
    3.26647735]
 [292.74576037   0.0513       0.47300002   0.31990001   0.44789997
    3.34136438]][0m
[37m[1m[2023-07-11 15:44:27,115][233954] Max Reward on eval: 560.8222775056959[0m
[37m[1m[2023-07-11 15:44:27,116][233954] Min Reward on eval: -116.09016380710527[0m
[37m[1m[2023-07-11 15:44:27,116][233954] Mean Reward across all agents: 159.27618520144767[0m
[37m[1m[2023-07-11 15:44:27,116][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:44:27,119][233954] mean_value=-140.15821685547763, max_value=131.21108692196816[0m
[37m[1m[2023-07-11 15:44:27,121][233954] New mean coefficients: [[-0.72574973  1.7429988   0.14629602 -1.036256   -2.3799503   0.3203151 ]][0m
[37m[1m[2023-07-11 15:44:27,122][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:44:36,157][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 15:44:36,157][233954] FPS: 425105.87[0m
[36m[2023-07-11 15:44:36,159][233954] itr=1150, itrs=2000, Progress: 57.50%[0m
[37m[1m[2023-07-11 15:48:12,391][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001130[0m
[36m[2023-07-11 15:48:24,723][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 15:48:24,724][233954] FPS: 332341.38[0m
[36m[2023-07-11 15:48:28,933][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:48:28,933][233954] Reward + Measures: [[111.2131503    0.09869833   0.36389399   0.17951798   0.29325834
    3.24934816]][0m
[37m[1m[2023-07-11 15:48:28,933][233954] Max Reward on eval: 111.21315029716922[0m
[37m[1m[2023-07-11 15:48:28,934][233954] Min Reward on eval: 111.21315029716922[0m
[37m[1m[2023-07-11 15:48:28,934][233954] Mean Reward across all agents: 111.21315029716922[0m
[37m[1m[2023-07-11 15:48:28,934][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:48:33,873][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:48:33,874][233954] Reward + Measures: [[219.18945413   0.0795       0.47160003   0.28780001   0.41890001
    3.31353045]
 [254.73843244   0.0311       0.48280001   0.28520003   0.42120001
    3.30098128]
 [225.1623092    0.0717       0.54039997   0.31209999   0.51859999
    3.34474683]
 ...
 [232.72525469   0.24480002   0.17110001   0.2969       0.34209999
    3.27501154]
 [134.11160794   0.0554       0.28569999   0.1663       0.21950002
    3.21839404]
 [ 86.19576849   0.0903       0.19239999   0.1323       0.1575
    3.114115  ]][0m
[37m[1m[2023-07-11 15:48:33,874][233954] Max Reward on eval: 533.2896835212596[0m
[37m[1m[2023-07-11 15:48:33,874][233954] Min Reward on eval: -142.634129330609[0m
[37m[1m[2023-07-11 15:48:33,875][233954] Mean Reward across all agents: 116.73101943377876[0m
[37m[1m[2023-07-11 15:48:33,875][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:48:33,877][233954] mean_value=-164.9034238688353, max_value=670.1886467157863[0m
[37m[1m[2023-07-11 15:48:33,879][233954] New mean coefficients: [[-0.32535106  1.7903491   0.4268374  -0.89065456 -2.1852274   0.34488088]][0m
[37m[1m[2023-07-11 15:48:33,880][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:48:42,933][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 15:48:42,934][233954] FPS: 424231.43[0m
[36m[2023-07-11 15:48:42,936][233954] itr=1151, itrs=2000, Progress: 57.55%[0m
[36m[2023-07-11 15:48:55,034][233954] train() took 12.00 seconds to complete[0m
[36m[2023-07-11 15:48:55,034][233954] FPS: 320042.65[0m
[36m[2023-07-11 15:48:59,290][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:48:59,291][233954] Reward + Measures: [[110.74648068   0.09396067   0.36389866   0.18165901   0.29168567
    3.25937104]][0m
[37m[1m[2023-07-11 15:48:59,291][233954] Max Reward on eval: 110.74648068059054[0m
[37m[1m[2023-07-11 15:48:59,291][233954] Min Reward on eval: 110.74648068059054[0m
[37m[1m[2023-07-11 15:48:59,291][233954] Mean Reward across all agents: 110.74648068059054[0m
[37m[1m[2023-07-11 15:48:59,292][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:49:04,236][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:49:04,236][233954] Reward + Measures: [[-11.53698842   0.0626       0.1393       0.0686       0.0678
    3.16962051]
 [147.11121504   0.1245       0.2811       0.21980003   0.29160005
    3.19064426]
 [102.22510835   0.1435       0.2924       0.13059999   0.26100001
    3.10732245]
 ...
 [266.77443715   0.0918       0.52899998   0.27040002   0.50189996
    3.35946894]
 [ 51.18472416   0.0779       0.36149999   0.17410001   0.32759997
    3.15719414]
 [-17.99345607   0.14060001   0.2457       0.08540001   0.19340001
    3.01171756]][0m
[37m[1m[2023-07-11 15:49:04,237][233954] Max Reward on eval: 530.331478862185[0m
[37m[1m[2023-07-11 15:49:04,237][233954] Min Reward on eval: -231.47108171018772[0m
[37m[1m[2023-07-11 15:49:04,237][233954] Mean Reward across all agents: 120.13468352513111[0m
[37m[1m[2023-07-11 15:49:04,237][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:49:04,239][233954] mean_value=-180.82312103400528, max_value=117.76394956027184[0m
[37m[1m[2023-07-11 15:49:04,242][233954] New mean coefficients: [[-0.1669316   1.3171332   0.65399075 -0.7057927  -2.3885386   0.23321536]][0m
[37m[1m[2023-07-11 15:49:04,243][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:49:13,311][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 15:49:13,311][233954] FPS: 423539.91[0m
[36m[2023-07-11 15:49:13,313][233954] itr=1152, itrs=2000, Progress: 57.60%[0m
[36m[2023-07-11 15:49:25,085][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 15:49:25,085][233954] FPS: 328960.01[0m
[36m[2023-07-11 15:49:29,329][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:49:29,330][233954] Reward + Measures: [[98.66938942  0.09532999  0.34458601  0.17132364  0.27178866  3.24009037]][0m
[37m[1m[2023-07-11 15:49:29,330][233954] Max Reward on eval: 98.66938941776792[0m
[37m[1m[2023-07-11 15:49:29,330][233954] Min Reward on eval: 98.66938941776792[0m
[37m[1m[2023-07-11 15:49:29,330][233954] Mean Reward across all agents: 98.66938941776792[0m
[37m[1m[2023-07-11 15:49:29,331][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:49:34,583][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:49:34,583][233954] Reward + Measures: [[198.04781265   0.1358       0.51220006   0.23099999   0.45290002
    3.35198951]
 [ 70.82896336   0.1235       0.37600002   0.1699       0.3303
    3.23435593]
 [220.02922177   0.10600001   0.46419999   0.22409998   0.41729999
    3.2767427 ]
 ...
 [  1.20520786   0.06170001   0.11970001   0.06240001   0.0573
    3.0082128 ]
 [  1.55695895   0.0717       0.1944       0.08670001   0.11310001
    3.15355849]
 [ 75.24006161   0.05         0.296        0.14910001   0.18880001
    3.21673822]][0m
[37m[1m[2023-07-11 15:49:34,584][233954] Max Reward on eval: 481.4826794880908[0m
[37m[1m[2023-07-11 15:49:34,584][233954] Min Reward on eval: -171.1387421294581[0m
[37m[1m[2023-07-11 15:49:34,584][233954] Mean Reward across all agents: 119.11879264655276[0m
[37m[1m[2023-07-11 15:49:34,584][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:49:34,586][233954] mean_value=-161.9114203462997, max_value=110.92367856896274[0m
[37m[1m[2023-07-11 15:49:34,589][233954] New mean coefficients: [[-0.51342446  1.2213286   0.4168899   0.18214041 -2.2389193   0.47900975]][0m
[37m[1m[2023-07-11 15:49:34,590][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:49:43,535][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 15:49:43,536][233954] FPS: 429335.53[0m
[36m[2023-07-11 15:49:43,538][233954] itr=1153, itrs=2000, Progress: 57.65%[0m
[36m[2023-07-11 15:49:55,591][233954] train() took 11.95 seconds to complete[0m
[36m[2023-07-11 15:49:55,591][233954] FPS: 321251.35[0m
[36m[2023-07-11 15:49:59,792][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:49:59,792][233954] Reward + Measures: [[90.03052727  0.09445233  0.33057001  0.16481067  0.26057801  3.2367084 ]][0m
[37m[1m[2023-07-11 15:49:59,792][233954] Max Reward on eval: 90.03052727294306[0m
[37m[1m[2023-07-11 15:49:59,793][233954] Min Reward on eval: 90.03052727294306[0m
[37m[1m[2023-07-11 15:49:59,793][233954] Mean Reward across all agents: 90.03052727294306[0m
[37m[1m[2023-07-11 15:49:59,793][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:50:04,665][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:50:04,666][233954] Reward + Measures: [[ 111.12928968    0.048         0.31169999    0.16900001    0.23629999
     3.26111722]
 [  68.56698465    0.2026        0.52130002    0.20279999    0.5018
     3.30697131]
 [ -51.02155856    0.24010001    0.52930003    0.17730002    0.49700004
     3.29179883]
 ...
 [ -29.09586119    0.0657        0.14860001    0.0622        0.0577
     3.09363794]
 [  73.81420994    0.0464        0.42280003    0.2429        0.32640001
     3.33941889]
 [-199.50522918    0.41429996    0.56029999    0.069         0.49419999
     3.31420898]][0m
[37m[1m[2023-07-11 15:50:04,666][233954] Max Reward on eval: 410.2813429157948[0m
[37m[1m[2023-07-11 15:50:04,666][233954] Min Reward on eval: -221.32853520866485[0m
[37m[1m[2023-07-11 15:50:04,666][233954] Mean Reward across all agents: 96.54691844005653[0m
[37m[1m[2023-07-11 15:50:04,666][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:50:04,668][233954] mean_value=-189.3434356476991, max_value=94.44730039206677[0m
[37m[1m[2023-07-11 15:50:04,670][233954] New mean coefficients: [[-0.52942     1.5142183   0.7664187   0.00847889 -1.7659173   0.60287565]][0m
[37m[1m[2023-07-11 15:50:04,671][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:50:13,678][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 15:50:13,678][233954] FPS: 426438.26[0m
[36m[2023-07-11 15:50:13,680][233954] itr=1154, itrs=2000, Progress: 57.70%[0m
[36m[2023-07-11 15:50:25,729][233954] train() took 11.95 seconds to complete[0m
[36m[2023-07-11 15:50:25,729][233954] FPS: 321328.03[0m
[36m[2023-07-11 15:50:30,098][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:50:30,098][233954] Reward + Measures: [[82.55059818  0.09530433  0.34381899  0.16729167  0.26853433  3.24940109]][0m
[37m[1m[2023-07-11 15:50:30,099][233954] Max Reward on eval: 82.55059818286291[0m
[37m[1m[2023-07-11 15:50:30,099][233954] Min Reward on eval: 82.55059818286291[0m
[37m[1m[2023-07-11 15:50:30,099][233954] Mean Reward across all agents: 82.55059818286291[0m
[37m[1m[2023-07-11 15:50:30,099][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:50:35,217][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:50:35,218][233954] Reward + Measures: [[184.8379017    0.05250001   0.3057       0.17309999   0.2568
    3.06339121]
 [363.85041423   0.0396       0.54439998   0.31900001   0.52209997
    3.30601954]
 [ 24.6561324    0.20479999   0.37369999   0.10390001   0.30930001
    3.28449416]
 ...
 [ 23.38156631   0.0579       0.23700002   0.1243       0.141
    3.20109415]
 [167.40242982   0.1495       0.49100003   0.22760001   0.43959999
    3.34376884]
 [315.15042909   0.0483       0.4686       0.28700003   0.42139998
    3.26060104]][0m
[37m[1m[2023-07-11 15:50:35,218][233954] Max Reward on eval: 513.0849903624854[0m
[37m[1m[2023-07-11 15:50:35,218][233954] Min Reward on eval: -142.35745450556277[0m
[37m[1m[2023-07-11 15:50:35,218][233954] Mean Reward across all agents: 138.43078717982021[0m
[37m[1m[2023-07-11 15:50:35,219][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:50:35,221][233954] mean_value=-151.65836635508919, max_value=629.8244920945074[0m
[37m[1m[2023-07-11 15:50:35,223][233954] New mean coefficients: [[-0.9578327   1.4353777   0.7618927  -0.19190127 -1.88597     0.66582376]][0m
[37m[1m[2023-07-11 15:50:35,224][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:50:44,231][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 15:50:44,231][233954] FPS: 426420.02[0m
[36m[2023-07-11 15:50:44,233][233954] itr=1155, itrs=2000, Progress: 57.75%[0m
[36m[2023-07-11 15:50:55,875][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 15:50:55,875][233954] FPS: 332685.64[0m
[36m[2023-07-11 15:51:00,194][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:51:00,194][233954] Reward + Measures: [[70.91913709  0.09991567  0.32230932  0.15820968  0.25460935  3.23507524]][0m
[37m[1m[2023-07-11 15:51:00,195][233954] Max Reward on eval: 70.91913708530709[0m
[37m[1m[2023-07-11 15:51:00,195][233954] Min Reward on eval: 70.91913708530709[0m
[37m[1m[2023-07-11 15:51:00,195][233954] Mean Reward across all agents: 70.91913708530709[0m
[37m[1m[2023-07-11 15:51:00,195][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:51:05,219][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:51:05,225][233954] Reward + Measures: [[-45.15260685   0.26820001   0.47709998   0.15079999   0.44179997
    3.09524393]
 [-25.52811249   0.13079999   0.28700003   0.1155       0.25890002
    3.2559464 ]
 [ 28.30208336   0.1049       0.24330001   0.1193       0.20920001
    2.90616155]
 ...
 [ -1.47391742   0.1992       0.39160001   0.1471       0.3355
    3.17769217]
 [  1.77570603   0.13170001   0.21269999   0.07309999   0.17680001
    3.13165331]
 [ 67.50569617   0.11700001   0.32380003   0.11700001   0.22089998
    3.17528319]][0m
[37m[1m[2023-07-11 15:51:05,225][233954] Max Reward on eval: 464.4156590660103[0m
[37m[1m[2023-07-11 15:51:05,226][233954] Min Reward on eval: -223.78152899164706[0m
[37m[1m[2023-07-11 15:51:05,226][233954] Mean Reward across all agents: 69.53317772943659[0m
[37m[1m[2023-07-11 15:51:05,226][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:51:05,228][233954] mean_value=-195.9793499274546, max_value=126.3301256392723[0m
[37m[1m[2023-07-11 15:51:05,230][233954] New mean coefficients: [[-0.7165385   1.5469683   0.9512076   0.06914774 -1.9872346   0.21549481]][0m
[37m[1m[2023-07-11 15:51:05,231][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:51:14,255][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 15:51:14,256][233954] FPS: 425586.88[0m
[36m[2023-07-11 15:51:14,258][233954] itr=1156, itrs=2000, Progress: 57.80%[0m
[36m[2023-07-11 15:51:25,934][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 15:51:25,934][233954] FPS: 331651.63[0m
[36m[2023-07-11 15:51:30,310][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:51:30,310][233954] Reward + Measures: [[60.5042539   0.10317867  0.31973833  0.15327699  0.24880201  3.23087311]][0m
[37m[1m[2023-07-11 15:51:30,310][233954] Max Reward on eval: 60.50425390071417[0m
[37m[1m[2023-07-11 15:51:30,311][233954] Min Reward on eval: 60.50425390071417[0m
[37m[1m[2023-07-11 15:51:30,311][233954] Mean Reward across all agents: 60.50425390071417[0m
[37m[1m[2023-07-11 15:51:30,311][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:51:35,324][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:51:35,324][233954] Reward + Measures: [[238.35712266   0.122        0.66149998   0.32469997   0.63900006
    3.32140517]
 [ 93.19879982   0.1048       0.19760001   0.15980001   0.18599999
    3.15822482]
 [131.05105903   0.07699999   0.33590001   0.2138       0.2809
    3.25699091]
 ...
 [ 63.28950579   0.0575       0.34760004   0.1596       0.27000001
    3.22091842]
 [246.30430217   0.15350001   0.62709999   0.2949       0.58840001
    3.30032921]
 [223.10809572   0.1033       0.63770002   0.3321       0.59140003
    3.33933115]][0m
[37m[1m[2023-07-11 15:51:35,325][233954] Max Reward on eval: 568.1576860260218[0m
[37m[1m[2023-07-11 15:51:35,325][233954] Min Reward on eval: -237.23365943003446[0m
[37m[1m[2023-07-11 15:51:35,325][233954] Mean Reward across all agents: 145.03947654011637[0m
[37m[1m[2023-07-11 15:51:35,325][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:51:35,327][233954] mean_value=-156.1211200388412, max_value=339.23372239726467[0m
[37m[1m[2023-07-11 15:51:35,330][233954] New mean coefficients: [[-0.14985186  1.7744083   0.95446754  0.61904657 -2.1678178   0.3044966 ]][0m
[37m[1m[2023-07-11 15:51:35,331][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:51:44,331][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 15:51:44,332][233954] FPS: 426715.91[0m
[36m[2023-07-11 15:51:44,334][233954] itr=1157, itrs=2000, Progress: 57.85%[0m
[36m[2023-07-11 15:51:56,025][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 15:51:56,025][233954] FPS: 331276.12[0m
[36m[2023-07-11 15:52:00,292][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:52:00,293][233954] Reward + Measures: [[40.71671293  0.099342    0.29664934  0.14204834  0.22584534  3.21753621]][0m
[37m[1m[2023-07-11 15:52:00,293][233954] Max Reward on eval: 40.71671293320401[0m
[37m[1m[2023-07-11 15:52:00,293][233954] Min Reward on eval: 40.71671293320401[0m
[37m[1m[2023-07-11 15:52:00,294][233954] Mean Reward across all agents: 40.71671293320401[0m
[37m[1m[2023-07-11 15:52:00,294][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:52:05,254][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:52:05,255][233954] Reward + Measures: [[-50.6869299    0.14130001   0.28080001   0.1183       0.24519999
    3.18219686]
 [229.13054443   0.1219       0.45870003   0.20820001   0.40459999
    3.21264648]
 [226.05833196   0.1041       0.56230003   0.26250002   0.48570004
    3.35001493]
 ...
 [  0.45010902   0.17240001   0.35119998   0.1182       0.31760001
    3.1923275 ]
 [194.8498994    0.0533       0.46470004   0.34810001   0.43169999
    3.27376795]
 [121.41509177   0.0549       0.3662       0.19450001   0.27430001
    3.25761294]][0m
[37m[1m[2023-07-11 15:52:05,255][233954] Max Reward on eval: 454.7914699923247[0m
[37m[1m[2023-07-11 15:52:05,255][233954] Min Reward on eval: -215.3593154963106[0m
[37m[1m[2023-07-11 15:52:05,256][233954] Mean Reward across all agents: 93.44349309873074[0m
[37m[1m[2023-07-11 15:52:05,256][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:52:05,257][233954] mean_value=-179.10741143031757, max_value=88.06516484392455[0m
[37m[1m[2023-07-11 15:52:05,260][233954] New mean coefficients: [[-0.44626027  1.8853965   1.230729    0.00503296 -2.3277853   0.37945634]][0m
[37m[1m[2023-07-11 15:52:05,261][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:52:14,295][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 15:52:14,296][233954] FPS: 425114.23[0m
[36m[2023-07-11 15:52:14,298][233954] itr=1158, itrs=2000, Progress: 57.90%[0m
[36m[2023-07-11 15:52:26,230][233954] train() took 11.83 seconds to complete[0m
[36m[2023-07-11 15:52:26,230][233954] FPS: 324550.06[0m
[36m[2023-07-11 15:52:30,510][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:52:30,510][233954] Reward + Measures: [[26.4047828   0.10572066  0.27913034  0.13209566  0.21123034  3.21351504]][0m
[37m[1m[2023-07-11 15:52:30,511][233954] Max Reward on eval: 26.404782801143863[0m
[37m[1m[2023-07-11 15:52:30,511][233954] Min Reward on eval: 26.404782801143863[0m
[37m[1m[2023-07-11 15:52:30,511][233954] Mean Reward across all agents: 26.404782801143863[0m
[37m[1m[2023-07-11 15:52:30,511][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:52:35,776][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:52:35,777][233954] Reward + Measures: [[ 297.14291625    0.1322        0.71850002    0.36090001    0.68269998
     3.16241503]
 [  57.07042237    0.054         0.2175        0.13850001    0.15440001
     3.36558771]
 [  59.96819117    0.20940001    0.73089999    0.26150003    0.67230004
     3.04345822]
 ...
 [-101.91303445    0.2933        0.50410002    0.13380001    0.4621
     3.07171226]
 [ 241.9764384     0.0506        0.4738        0.30450001    0.40920001
     3.34428453]
 [  -3.60256742    0.0691        0.12570001    0.0758        0.0848
     3.00974727]][0m
[37m[1m[2023-07-11 15:52:35,777][233954] Max Reward on eval: 436.87705610403793[0m
[37m[1m[2023-07-11 15:52:35,777][233954] Min Reward on eval: -664.2643318140414[0m
[37m[1m[2023-07-11 15:52:35,777][233954] Mean Reward across all agents: 65.38709643010434[0m
[37m[1m[2023-07-11 15:52:35,777][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:52:35,779][233954] mean_value=-206.10771824572723, max_value=70.90685796143501[0m
[37m[1m[2023-07-11 15:52:35,782][233954] New mean coefficients: [[-0.7945638   1.4155986   1.4429048  -0.4590757  -2.9413733   0.45397222]][0m
[37m[1m[2023-07-11 15:52:35,783][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:52:44,803][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 15:52:44,803][233954] FPS: 425834.38[0m
[36m[2023-07-11 15:52:44,805][233954] itr=1159, itrs=2000, Progress: 57.95%[0m
[36m[2023-07-11 15:52:56,588][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 15:52:56,589][233954] FPS: 328547.54[0m
[36m[2023-07-11 15:53:00,843][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:53:00,844][233954] Reward + Measures: [[31.89370344  0.10864066  0.29172066  0.13584366  0.22329468  3.21266603]][0m
[37m[1m[2023-07-11 15:53:00,844][233954] Max Reward on eval: 31.893703440781422[0m
[37m[1m[2023-07-11 15:53:00,844][233954] Min Reward on eval: 31.893703440781422[0m
[37m[1m[2023-07-11 15:53:00,845][233954] Mean Reward across all agents: 31.893703440781422[0m
[37m[1m[2023-07-11 15:53:00,845][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:53:05,811][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:53:05,812][233954] Reward + Measures: [[436.82510137   0.0959       0.76319999   0.44399998   0.75010002
    3.48848772]
 [171.08383102   0.0523       0.30110002   0.1867       0.25550002
    3.38161469]
 [ 16.60700328   0.1383       0.33319998   0.12310001   0.24590002
    3.19855309]
 ...
 [ 63.1176903    0.10170001   0.28830001   0.11229999   0.23020001
    3.07328773]
 [-49.750526     0.0475       0.1362       0.0647       0.0558
    3.06983995]
 [  4.40028922   0.09280001   0.19050001   0.0679       0.1222
    2.99025297]][0m
[37m[1m[2023-07-11 15:53:05,812][233954] Max Reward on eval: 528.0958566138521[0m
[37m[1m[2023-07-11 15:53:05,812][233954] Min Reward on eval: -223.38466689577325[0m
[37m[1m[2023-07-11 15:53:05,813][233954] Mean Reward across all agents: 92.91247049784953[0m
[37m[1m[2023-07-11 15:53:05,813][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:53:05,815][233954] mean_value=-188.00566745212768, max_value=37.25342189868627[0m
[37m[1m[2023-07-11 15:53:05,817][233954] New mean coefficients: [[-0.78193396  2.640194    1.6209347  -0.47916648 -2.9171271   0.5253693 ]][0m
[37m[1m[2023-07-11 15:53:05,818][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:53:14,806][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 15:53:14,806][233954] FPS: 427330.91[0m
[36m[2023-07-11 15:53:14,808][233954] itr=1160, itrs=2000, Progress: 58.00%[0m
[37m[1m[2023-07-11 15:56:49,790][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001140[0m
[36m[2023-07-11 15:57:01,844][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 15:57:01,844][233954] FPS: 335259.29[0m
[36m[2023-07-11 15:57:06,055][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:57:06,055][233954] Reward + Measures: [[26.01199086  0.10344299  0.28890032  0.13482399  0.21631633  3.20722651]][0m
[37m[1m[2023-07-11 15:57:06,056][233954] Max Reward on eval: 26.01199086073373[0m
[37m[1m[2023-07-11 15:57:06,056][233954] Min Reward on eval: 26.01199086073373[0m
[37m[1m[2023-07-11 15:57:06,056][233954] Mean Reward across all agents: 26.01199086073373[0m
[37m[1m[2023-07-11 15:57:06,056][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:57:11,036][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:57:11,037][233954] Reward + Measures: [[ 49.82973074   0.20320001   0.25650001   0.1621       0.25840002
    3.03842974]
 [155.59083531   0.1265       0.46750003   0.2349       0.43140003
    3.32528234]
 [ 62.79275893   0.12050001   0.40060002   0.16600001   0.308
    3.24214363]
 ...
 [ 27.22486183   0.0487       0.25710002   0.13600002   0.15550001
    3.28940773]
 [131.42902195   0.0513       0.32940003   0.20200002   0.24730001
    3.24387789]
 [ 33.81277402   0.0615       0.25929999   0.1593       0.19820002
    3.19460845]][0m
[37m[1m[2023-07-11 15:57:11,037][233954] Max Reward on eval: 427.85127459596845[0m
[37m[1m[2023-07-11 15:57:11,037][233954] Min Reward on eval: -170.40524488247465[0m
[37m[1m[2023-07-11 15:57:11,037][233954] Mean Reward across all agents: 66.3483345696584[0m
[37m[1m[2023-07-11 15:57:11,038][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:57:11,039][233954] mean_value=-197.6514950590259, max_value=58.634395040170546[0m
[37m[1m[2023-07-11 15:57:11,042][233954] New mean coefficients: [[-0.3376033   2.3906586   2.389214   -0.04144615 -2.8084645   0.10661703]][0m
[37m[1m[2023-07-11 15:57:11,042][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:57:19,974][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 15:57:19,974][233954] FPS: 430034.35[0m
[36m[2023-07-11 15:57:19,976][233954] itr=1161, itrs=2000, Progress: 58.05%[0m
[36m[2023-07-11 15:57:31,640][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 15:57:31,640][233954] FPS: 331889.74[0m
[36m[2023-07-11 15:57:35,864][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:57:35,864][233954] Reward + Measures: [[18.10147461  0.10861     0.29188034  0.129824    0.21577732  3.20759773]][0m
[37m[1m[2023-07-11 15:57:35,865][233954] Max Reward on eval: 18.10147461266505[0m
[37m[1m[2023-07-11 15:57:35,865][233954] Min Reward on eval: 18.10147461266505[0m
[37m[1m[2023-07-11 15:57:35,865][233954] Mean Reward across all agents: 18.10147461266505[0m
[37m[1m[2023-07-11 15:57:35,865][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:57:40,748][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:57:40,749][233954] Reward + Measures: [[117.31879139   0.0675       0.3725       0.20440002   0.3328
    3.30230951]
 [ 70.41159215   0.1446       0.37719998   0.16019998   0.31299996
    3.27290273]
 [108.89768716   0.15140001   0.41700003   0.2402       0.40580001
    3.27561927]
 ...
 [227.63573199   0.0458       0.4894       0.30609998   0.45120001
    3.26658607]
 [ 52.98542432   0.0808       0.3768       0.1876       0.3071
    3.22398686]
 [251.21571967   0.15440002   0.61320007   0.40270004   0.64969999
    3.43647122]][0m
[37m[1m[2023-07-11 15:57:40,749][233954] Max Reward on eval: 601.9190630927682[0m
[37m[1m[2023-07-11 15:57:40,749][233954] Min Reward on eval: -187.14601965192705[0m
[37m[1m[2023-07-11 15:57:40,749][233954] Mean Reward across all agents: 139.24639246001044[0m
[37m[1m[2023-07-11 15:57:40,750][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:57:40,751][233954] mean_value=-173.7826340044027, max_value=137.9196599024548[0m
[37m[1m[2023-07-11 15:57:40,754][233954] New mean coefficients: [[-0.5990439   2.6895616   2.2409441   0.26429674 -2.6158552   0.3452487 ]][0m
[37m[1m[2023-07-11 15:57:40,755][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:57:49,684][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 15:57:49,684][233954] FPS: 430164.92[0m
[36m[2023-07-11 15:57:49,686][233954] itr=1162, itrs=2000, Progress: 58.10%[0m
[36m[2023-07-11 15:58:01,588][233954] train() took 11.80 seconds to complete[0m
[36m[2023-07-11 15:58:01,589][233954] FPS: 325309.76[0m
[36m[2023-07-11 15:58:05,838][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:58:05,838][233954] Reward + Measures: [[16.48520322  0.11062367  0.291756    0.12843201  0.215593    3.20271683]][0m
[37m[1m[2023-07-11 15:58:05,839][233954] Max Reward on eval: 16.485203217935137[0m
[37m[1m[2023-07-11 15:58:05,839][233954] Min Reward on eval: 16.485203217935137[0m
[37m[1m[2023-07-11 15:58:05,839][233954] Mean Reward across all agents: 16.485203217935137[0m
[37m[1m[2023-07-11 15:58:05,839][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:58:10,782][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:58:10,783][233954] Reward + Measures: [[-38.94656469   0.0816       0.25190002   0.1043       0.14929999
    3.18236208]
 [ 95.56440887   0.1207       0.33080003   0.125        0.25409999
    3.18126011]
 [259.32300099   0.0982       0.32800001   0.26159999   0.36020002
    3.26660728]
 ...
 [183.31144662   0.0325       0.49939999   0.29769999   0.41139999
    3.37445331]
 [-25.89832637   0.17580001   0.36230001   0.12630001   0.32790002
    3.16322851]
 [130.38522864   0.12910001   0.37690002   0.18320002   0.3249
    3.25286651]][0m
[37m[1m[2023-07-11 15:58:10,783][233954] Max Reward on eval: 522.2637076159939[0m
[37m[1m[2023-07-11 15:58:10,783][233954] Min Reward on eval: -149.86910443902016[0m
[37m[1m[2023-07-11 15:58:10,784][233954] Mean Reward across all agents: 108.9492617273684[0m
[37m[1m[2023-07-11 15:58:10,784][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:58:10,786][233954] mean_value=-184.3811420422331, max_value=33.431465140192245[0m
[37m[1m[2023-07-11 15:58:10,788][233954] New mean coefficients: [[-1.0339268   3.019154    2.1270213   0.12554389 -2.9220548   0.69842637]][0m
[37m[1m[2023-07-11 15:58:10,789][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:58:19,793][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 15:58:19,793][233954] FPS: 426565.56[0m
[36m[2023-07-11 15:58:19,795][233954] itr=1163, itrs=2000, Progress: 58.15%[0m
[36m[2023-07-11 15:58:31,618][233954] train() took 11.73 seconds to complete[0m
[36m[2023-07-11 15:58:31,618][233954] FPS: 327497.99[0m
[36m[2023-07-11 15:58:35,922][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:58:35,922][233954] Reward + Measures: [[12.3903946   0.108495    0.29894567  0.12887798  0.21752034  3.20858169]][0m
[37m[1m[2023-07-11 15:58:35,922][233954] Max Reward on eval: 12.390394600729271[0m
[37m[1m[2023-07-11 15:58:35,923][233954] Min Reward on eval: 12.390394600729271[0m
[37m[1m[2023-07-11 15:58:35,923][233954] Mean Reward across all agents: 12.390394600729271[0m
[37m[1m[2023-07-11 15:58:35,923][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:58:41,125][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:58:41,126][233954] Reward + Measures: [[ 190.72358824    0.0399        0.37990001    0.22370003    0.32610002
     3.31265044]
 [ -81.35180447    0.1574        0.22449999    0.0677        0.1611
     3.11368895]
 [  90.64206172    0.20850001    0.3917        0.1305        0.35890001
     3.26915479]
 ...
 [  85.4192112     0.1961        0.43379998    0.14870001    0.41909996
     3.30465388]
 [ 197.4247432     0.0408        0.46430001    0.25820002    0.40400001
     3.34882212]
 [-204.3078787     0.34150001    0.51770002    0.11520001    0.50610006
     3.15052581]][0m
[37m[1m[2023-07-11 15:58:41,126][233954] Max Reward on eval: 366.8605694601312[0m
[37m[1m[2023-07-11 15:58:41,126][233954] Min Reward on eval: -266.33923890218136[0m
[37m[1m[2023-07-11 15:58:41,127][233954] Mean Reward across all agents: 29.745765181354425[0m
[37m[1m[2023-07-11 15:58:41,127][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:58:41,128][233954] mean_value=-219.6328875185007, max_value=-10.588637307712844[0m
[36m[2023-07-11 15:58:41,146][233954] XNES is restarting with a new solution whose measures are [0.52890003 0.50089997 0.51270002 0.57690001 0.55444449] and objective is 84.51118420595303[0m
[36m[2023-07-11 15:58:41,148][233954] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-11 15:58:41,151][233954] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-11 15:58:41,152][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:58:50,100][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 15:58:50,100][233954] FPS: 429204.96[0m
[36m[2023-07-11 15:58:50,102][233954] itr=1164, itrs=2000, Progress: 58.20%[0m
[36m[2023-07-11 15:59:01,670][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 15:59:01,671][233954] FPS: 334701.46[0m
[36m[2023-07-11 15:59:05,967][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:59:05,968][233954] Reward + Measures: [[67.14504987  0.52926368  0.52992862  0.50534564  0.68254304  0.64987689]][0m
[37m[1m[2023-07-11 15:59:05,968][233954] Max Reward on eval: 67.14504986582064[0m
[37m[1m[2023-07-11 15:59:05,968][233954] Min Reward on eval: 67.14504986582064[0m
[37m[1m[2023-07-11 15:59:05,969][233954] Mean Reward across all agents: 67.14504986582064[0m
[37m[1m[2023-07-11 15:59:05,969][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:59:11,042][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:59:11,043][233954] Reward + Measures: [[-67.63891724   0.66130006   0.54190004   0.5011       0.2139
    1.79038334]
 [ 11.52641804   0.1989       0.22909999   0.1129       0.25909999
    2.42957234]
 [ 63.1904336    0.1008       0.0972       0.0645       0.11860001
    2.21440053]
 ...
 [191.36427497   0.0772       0.96429998   0.60089999   0.93629998
    2.14171028]
 [-23.67011771   0.4443       0.66590005   0.50940001   0.70180005
    2.19873023]
 [ 21.51079451   0.27040002   0.2421       0.25130001   0.34020001
    2.46250582]][0m
[37m[1m[2023-07-11 15:59:11,043][233954] Max Reward on eval: 623.4713897451759[0m
[37m[1m[2023-07-11 15:59:11,043][233954] Min Reward on eval: -162.6959738618927[0m
[37m[1m[2023-07-11 15:59:11,044][233954] Mean Reward across all agents: 28.46532670576271[0m
[37m[1m[2023-07-11 15:59:11,044][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:59:11,047][233954] mean_value=-261.0660996297005, max_value=486.39510283400625[0m
[37m[1m[2023-07-11 15:59:11,049][233954] New mean coefficients: [[-0.83249485 -0.24791205 -1.1935979  -1.0841024  -1.1291862  -0.1783899 ]][0m
[37m[1m[2023-07-11 15:59:11,050][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:59:20,123][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 15:59:20,123][233954] FPS: 423317.26[0m
[36m[2023-07-11 15:59:20,126][233954] itr=1165, itrs=2000, Progress: 58.25%[0m
[36m[2023-07-11 15:59:31,980][233954] train() took 11.76 seconds to complete[0m
[36m[2023-07-11 15:59:31,980][233954] FPS: 326509.39[0m
[36m[2023-07-11 15:59:36,294][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:59:36,295][233954] Reward + Measures: [[27.32158533  0.54042667  0.47325867  0.4669973   0.64190137  0.53943598]][0m
[37m[1m[2023-07-11 15:59:36,295][233954] Max Reward on eval: 27.321585328266714[0m
[37m[1m[2023-07-11 15:59:36,295][233954] Min Reward on eval: 27.321585328266714[0m
[37m[1m[2023-07-11 15:59:36,296][233954] Mean Reward across all agents: 27.321585328266714[0m
[37m[1m[2023-07-11 15:59:36,296][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:59:41,284][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 15:59:41,285][233954] Reward + Measures: [[-49.55680308   0.27420002   0.23930001   0.26159999   0.25840002
    2.4394834 ]
 [489.43631173   0.0241       0.96180004   0.67880005   0.96130002
    2.75925088]
 [ 40.31419155   0.1102       0.2297       0.15110001   0.2448
    3.19493794]
 ...
 [200.01730797   0.0932       0.73630005   0.33850002   0.68690002
    2.99269843]
 [ 93.51143056   0.34239998   0.2588       0.30949998   0.2192
    2.47196579]
 [ 52.38615538   0.32570001   0.1468       0.33660001   0.35080001
    2.26504707]][0m
[37m[1m[2023-07-11 15:59:41,285][233954] Max Reward on eval: 617.3888931402005[0m
[37m[1m[2023-07-11 15:59:41,285][233954] Min Reward on eval: -139.74715186614776[0m
[37m[1m[2023-07-11 15:59:41,285][233954] Mean Reward across all agents: 81.23483003863791[0m
[37m[1m[2023-07-11 15:59:41,285][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 15:59:41,288][233954] mean_value=-248.7104974890867, max_value=460.71044752891845[0m
[37m[1m[2023-07-11 15:59:41,291][233954] New mean coefficients: [[-0.44421864  0.3881629  -0.5263045  -0.816823    0.5384059   0.0096429 ]][0m
[37m[1m[2023-07-11 15:59:41,292][233954] Moving the mean solution point...[0m
[36m[2023-07-11 15:59:50,299][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 15:59:50,299][233954] FPS: 426406.66[0m
[36m[2023-07-11 15:59:50,301][233954] itr=1166, itrs=2000, Progress: 58.30%[0m
[36m[2023-07-11 16:00:02,039][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 16:00:02,039][233954] FPS: 329784.84[0m
[36m[2023-07-11 16:00:06,367][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:00:06,367][233954] Reward + Measures: [[19.94526732  0.51991636  0.43838868  0.4345943   0.61565     0.55358148]][0m
[37m[1m[2023-07-11 16:00:06,367][233954] Max Reward on eval: 19.945267316066424[0m
[37m[1m[2023-07-11 16:00:06,368][233954] Min Reward on eval: 19.945267316066424[0m
[37m[1m[2023-07-11 16:00:06,368][233954] Mean Reward across all agents: 19.945267316066424[0m
[37m[1m[2023-07-11 16:00:06,368][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:00:11,367][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:00:11,368][233954] Reward + Measures: [[172.47503529   0.23190001   0.32950002   0.31         0.44250003
    2.57787132]
 [-40.17940138   0.23119998   0.4127       0.28660002   0.34730002
    2.61514258]
 [271.67536927   0.32600001   0.77540004   0.54000002   0.68260008
    2.1870513 ]
 ...
 [ 63.69551414   0.14219999   0.15570001   0.19810002   0.2277
    2.53400636]
 [ -5.1330789    0.71889997   0.75080001   0.70099998   0.70090002
    2.51386213]
 [ 22.71055875   0.48410001   0.4614       0.4323       0.49899998
    2.42285109]][0m
[37m[1m[2023-07-11 16:00:11,368][233954] Max Reward on eval: 493.81666948758067[0m
[37m[1m[2023-07-11 16:00:11,368][233954] Min Reward on eval: -279.7023221391719[0m
[37m[1m[2023-07-11 16:00:11,368][233954] Mean Reward across all agents: 76.66926119503655[0m
[37m[1m[2023-07-11 16:00:11,369][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:00:11,372][233954] mean_value=-190.21411976365528, max_value=597.781175872901[0m
[37m[1m[2023-07-11 16:00:11,375][233954] New mean coefficients: [[ 0.08885062  0.5211787   0.60100996 -0.5459379   0.9532616   0.30838847]][0m
[37m[1m[2023-07-11 16:00:11,376][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:00:20,438][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 16:00:20,438][233954] FPS: 423814.31[0m
[36m[2023-07-11 16:00:20,440][233954] itr=1167, itrs=2000, Progress: 58.35%[0m
[36m[2023-07-11 16:00:32,156][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 16:00:32,161][233954] FPS: 330476.69[0m
[36m[2023-07-11 16:00:36,450][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:00:36,451][233954] Reward + Measures: [[27.92017602  0.49510303  0.41174665  0.41565233  0.59844732  0.63260674]][0m
[37m[1m[2023-07-11 16:00:36,451][233954] Max Reward on eval: 27.920176023689972[0m
[37m[1m[2023-07-11 16:00:36,451][233954] Min Reward on eval: 27.920176023689972[0m
[37m[1m[2023-07-11 16:00:36,452][233954] Mean Reward across all agents: 27.920176023689972[0m
[37m[1m[2023-07-11 16:00:36,452][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:00:41,443][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:00:41,444][233954] Reward + Measures: [[ -9.65655807   0.20670001   0.2811       0.17379999   0.2737
    2.47699285]
 [ 10.95953349   0.26370001   0.5571       0.2712       0.54769999
    2.30933833]
 [ 45.9534518    0.1909       0.2045       0.19590001   0.23449998
    2.8290658 ]
 ...
 [-14.19905476   0.20730002   0.41600004   0.19670001   0.39739999
    2.5183847 ]
 [ 83.83134092   0.18280001   0.18719999   0.14839999   0.2518
    2.01591349]
 [385.00938419   0.11849999   0.9655       0.64989996   0.92390007
    2.50544238]][0m
[37m[1m[2023-07-11 16:00:41,444][233954] Max Reward on eval: 488.41319273952394[0m
[37m[1m[2023-07-11 16:00:41,444][233954] Min Reward on eval: -118.20919961873442[0m
[37m[1m[2023-07-11 16:00:41,444][233954] Mean Reward across all agents: 88.10779427510926[0m
[37m[1m[2023-07-11 16:00:41,445][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:00:41,448][233954] mean_value=-236.7358286434308, max_value=451.31997726960105[0m
[37m[1m[2023-07-11 16:00:41,450][233954] New mean coefficients: [[ 0.76901275  0.5873092   0.8391726  -0.9200435   0.11852223  0.5604227 ]][0m
[37m[1m[2023-07-11 16:00:41,451][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:00:50,376][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 16:00:50,376][233954] FPS: 430345.87[0m
[36m[2023-07-11 16:00:50,378][233954] itr=1168, itrs=2000, Progress: 58.40%[0m
[36m[2023-07-11 16:01:01,944][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 16:01:01,944][233954] FPS: 334786.96[0m
[36m[2023-07-11 16:01:06,183][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:01:06,184][233954] Reward + Measures: [[28.33255227  0.50871497  0.44952196  0.43288198  0.60907     0.72597671]][0m
[37m[1m[2023-07-11 16:01:06,184][233954] Max Reward on eval: 28.33255227423347[0m
[37m[1m[2023-07-11 16:01:06,184][233954] Min Reward on eval: 28.33255227423347[0m
[37m[1m[2023-07-11 16:01:06,184][233954] Mean Reward across all agents: 28.33255227423347[0m
[37m[1m[2023-07-11 16:01:06,185][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:01:11,174][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:01:11,175][233954] Reward + Measures: [[ 50.18350788   0.2316       0.25280002   0.17389999   0.30469999
    1.4734987 ]
 [ 71.53849051   0.39180002   0.27250001   0.37209997   0.2448
    2.4551692 ]
 [ 62.91741054   0.35360003   0.28780001   0.3531       0.23210001
    2.66752553]
 ...
 [ 20.93060107   0.22750001   0.2538       0.23369999   0.24819998
    1.69033015]
 [ 45.6266131    0.40380001   0.34419999   0.41949996   0.26749998
    2.31282806]
 [116.14045857   0.1533       0.22650002   0.16399999   0.21510001
    2.02424502]][0m
[37m[1m[2023-07-11 16:01:11,175][233954] Max Reward on eval: 303.7954006487504[0m
[37m[1m[2023-07-11 16:01:11,175][233954] Min Reward on eval: -66.62124611008912[0m
[37m[1m[2023-07-11 16:01:11,175][233954] Mean Reward across all agents: 73.41406608024275[0m
[37m[1m[2023-07-11 16:01:11,176][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:01:11,178][233954] mean_value=-325.1869281212385, max_value=268.3337825943047[0m
[37m[1m[2023-07-11 16:01:11,180][233954] New mean coefficients: [[ 1.3011034   0.15878448  0.22665125 -0.42604673  0.31046277  0.44943827]][0m
[37m[1m[2023-07-11 16:01:11,181][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:01:20,160][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 16:01:20,160][233954] FPS: 427757.41[0m
[36m[2023-07-11 16:01:20,162][233954] itr=1169, itrs=2000, Progress: 58.45%[0m
[36m[2023-07-11 16:01:31,783][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 16:01:31,784][233954] FPS: 333259.25[0m
[36m[2023-07-11 16:01:36,072][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:01:36,072][233954] Reward + Measures: [[31.70244722  0.47717565  0.422151    0.3967607   0.57111704  0.79393643]][0m
[37m[1m[2023-07-11 16:01:36,072][233954] Max Reward on eval: 31.702447221145462[0m
[37m[1m[2023-07-11 16:01:36,073][233954] Min Reward on eval: 31.702447221145462[0m
[37m[1m[2023-07-11 16:01:36,073][233954] Mean Reward across all agents: 31.702447221145462[0m
[37m[1m[2023-07-11 16:01:36,073][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:01:41,233][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:01:41,233][233954] Reward + Measures: [[  19.81138554    0.43950006    0.54430002    0.42430001    0.54030001
     1.93443871]
 [ -52.23378941    0.1786        0.33790001    0.17029999    0.2735
     2.00384355]
 [  83.97071409    0.32360002    0.15100001    0.3267        0.26729998
     2.71782374]
 ...
 [  72.35832944    0.30720001    0.13410001    0.32160002    0.36359999
     2.85477734]
 [-137.60177181    0.47580004    0.44080001    0.23659997    0.56700003
     2.70090413]
 [  79.17390136    0.3955        0.44560003    0.42700002    0.49259996
     2.79588342]][0m
[37m[1m[2023-07-11 16:01:41,234][233954] Max Reward on eval: 805.352561959764[0m
[37m[1m[2023-07-11 16:01:41,234][233954] Min Reward on eval: -597.8108120104298[0m
[37m[1m[2023-07-11 16:01:41,234][233954] Mean Reward across all agents: 102.70280473451636[0m
[37m[1m[2023-07-11 16:01:41,234][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:01:41,237][233954] mean_value=-199.61734301502602, max_value=680.3313604769627[0m
[37m[1m[2023-07-11 16:01:41,240][233954] New mean coefficients: [[ 1.4764951   0.48596734 -0.0093472  -0.8352066   0.31034687  0.41394886]][0m
[37m[1m[2023-07-11 16:01:41,241][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:01:50,205][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 16:01:50,205][233954] FPS: 428460.78[0m
[36m[2023-07-11 16:01:50,208][233954] itr=1170, itrs=2000, Progress: 58.50%[0m
[37m[1m[2023-07-11 16:05:21,735][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001150[0m
[36m[2023-07-11 16:05:33,993][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 16:05:33,994][233954] FPS: 331109.75[0m
[36m[2023-07-11 16:05:38,175][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:05:38,175][233954] Reward + Measures: [[38.92890004  0.46358129  0.42515633  0.38640165  0.56097066  0.87471688]][0m
[37m[1m[2023-07-11 16:05:38,176][233954] Max Reward on eval: 38.92890004230845[0m
[37m[1m[2023-07-11 16:05:38,176][233954] Min Reward on eval: 38.92890004230845[0m
[37m[1m[2023-07-11 16:05:38,176][233954] Mean Reward across all agents: 38.92890004230845[0m
[37m[1m[2023-07-11 16:05:38,176][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:05:43,235][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:05:43,239][233954] Reward + Measures: [[ 35.42542227   0.0603       0.08990001   0.099        0.14
    2.56668735]
 [ 71.57238643   0.3238       0.61490005   0.32400003   0.60680002
    1.92208517]
 [129.94110688   0.0605       0.226        0.1965       0.26100001
    2.27779531]
 ...
 [ 37.90979669   0.12899999   0.25999999   0.17490001   0.25240001
    2.55532217]
 [ 48.32059192   0.0892       0.2789       0.1822       0.32370001
    2.44783974]
 [124.64800203   0.57380003   0.72600001   0.53440005   0.69139999
    1.64365709]][0m
[37m[1m[2023-07-11 16:05:43,240][233954] Max Reward on eval: 572.3305015564896[0m
[37m[1m[2023-07-11 16:05:43,240][233954] Min Reward on eval: -231.5020637879148[0m
[37m[1m[2023-07-11 16:05:43,240][233954] Mean Reward across all agents: 114.21489860679993[0m
[37m[1m[2023-07-11 16:05:43,240][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:05:43,245][233954] mean_value=-200.25073032254187, max_value=549.4266884849942[0m
[37m[1m[2023-07-11 16:05:43,248][233954] New mean coefficients: [[ 2.1398215   1.3721511  -0.5140173  -0.53579986  1.1431484   0.40782422]][0m
[37m[1m[2023-07-11 16:05:43,249][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:05:52,189][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 16:05:52,189][233954] FPS: 429636.91[0m
[36m[2023-07-11 16:05:52,191][233954] itr=1171, itrs=2000, Progress: 58.55%[0m
[36m[2023-07-11 16:06:03,885][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 16:06:03,885][233954] FPS: 331053.50[0m
[36m[2023-07-11 16:06:08,192][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:06:08,192][233954] Reward + Measures: [[94.85022596  0.45425031  0.51452631  0.45947134  0.63489968  1.04286611]][0m
[37m[1m[2023-07-11 16:06:08,193][233954] Max Reward on eval: 94.8502259602961[0m
[37m[1m[2023-07-11 16:06:08,193][233954] Min Reward on eval: 94.8502259602961[0m
[37m[1m[2023-07-11 16:06:08,193][233954] Mean Reward across all agents: 94.8502259602961[0m
[37m[1m[2023-07-11 16:06:08,193][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:06:13,163][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:06:13,164][233954] Reward + Measures: [[434.40646268   0.0814       0.88949996   0.57680005   0.90970004
    2.65182281]
 [ 42.07134983   0.24240004   0.52790004   0.3495       0.49439999
    2.52946353]
 [ 19.77403467   0.45749998   0.28830001   0.43080002   0.32789999
    2.47062564]
 ...
 [143.1902782    0.273        0.34450004   0.13570002   0.36149999
    3.04326367]
 [120.68984271   0.199        0.4066       0.3141       0.42989999
    2.93819523]
 [126.7562897    0.07840001   0.86320001   0.3344       0.82829994
    2.90208697]][0m
[37m[1m[2023-07-11 16:06:13,164][233954] Max Reward on eval: 817.7021637123544[0m
[37m[1m[2023-07-11 16:06:13,164][233954] Min Reward on eval: -232.8957738856785[0m
[37m[1m[2023-07-11 16:06:13,165][233954] Mean Reward across all agents: 142.4543994850759[0m
[37m[1m[2023-07-11 16:06:13,165][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:06:13,169][233954] mean_value=-157.32976363382565, max_value=646.7061664805468[0m
[37m[1m[2023-07-11 16:06:13,172][233954] New mean coefficients: [[ 2.4665666   0.28576672 -0.4162451  -0.380602    1.6051795   0.23438843]][0m
[37m[1m[2023-07-11 16:06:13,173][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:06:22,116][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 16:06:22,116][233954] FPS: 429466.04[0m
[36m[2023-07-11 16:06:22,118][233954] itr=1172, itrs=2000, Progress: 58.60%[0m
[36m[2023-07-11 16:06:33,878][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 16:06:33,878][233954] FPS: 329305.69[0m
[36m[2023-07-11 16:06:38,213][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:06:38,213][233954] Reward + Measures: [[152.9664171    0.42720932   0.59855032   0.52865899   0.70884764
    1.23415577]][0m
[37m[1m[2023-07-11 16:06:38,213][233954] Max Reward on eval: 152.96641709813588[0m
[37m[1m[2023-07-11 16:06:38,214][233954] Min Reward on eval: 152.96641709813588[0m
[37m[1m[2023-07-11 16:06:38,214][233954] Mean Reward across all agents: 152.96641709813588[0m
[37m[1m[2023-07-11 16:06:38,214][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:06:43,162][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:06:43,163][233954] Reward + Measures: [[ 86.85540408   0.10699999   0.25790003   0.19670002   0.29029998
    1.54172921]
 [165.89627978   0.0794       0.79650003   0.42550001   0.79979992
    1.916412  ]
 [-73.80449855   0.6099       0.2814       0.53780001   0.23080002
    1.88512516]
 ...
 [150.63129204   0.10380001   0.40330002   0.33209997   0.40080005
    2.61201024]
 [ 22.59024968   0.1893       0.23029999   0.21619999   0.30070001
    2.20073962]
 [-11.29477509   0.34870002   0.28239998   0.33060002   0.2904
    2.60198712]][0m
[37m[1m[2023-07-11 16:06:43,163][233954] Max Reward on eval: 496.34407043670944[0m
[37m[1m[2023-07-11 16:06:43,163][233954] Min Reward on eval: -148.7157479978865[0m
[37m[1m[2023-07-11 16:06:43,164][233954] Mean Reward across all agents: 65.33501415343679[0m
[37m[1m[2023-07-11 16:06:43,164][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:06:43,167][233954] mean_value=-329.0366239980877, max_value=402.4724563317876[0m
[37m[1m[2023-07-11 16:06:43,170][233954] New mean coefficients: [[ 0.77678156  1.0681123  -0.33371866 -0.00049821  2.0191383   0.17418829]][0m
[37m[1m[2023-07-11 16:06:43,171][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:06:52,156][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 16:06:52,156][233954] FPS: 427448.95[0m
[36m[2023-07-11 16:06:52,159][233954] itr=1173, itrs=2000, Progress: 58.65%[0m
[36m[2023-07-11 16:07:04,077][233954] train() took 11.82 seconds to complete[0m
[36m[2023-07-11 16:07:04,077][233954] FPS: 324889.61[0m
[36m[2023-07-11 16:07:08,382][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:07:08,383][233954] Reward + Measures: [[199.45234723   0.40544033   0.66613299   0.56039834   0.75851494
    1.34762812]][0m
[37m[1m[2023-07-11 16:07:08,383][233954] Max Reward on eval: 199.4523472344048[0m
[37m[1m[2023-07-11 16:07:08,383][233954] Min Reward on eval: 199.4523472344048[0m
[37m[1m[2023-07-11 16:07:08,383][233954] Mean Reward across all agents: 199.4523472344048[0m
[37m[1m[2023-07-11 16:07:08,384][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:07:13,358][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:07:13,358][233954] Reward + Measures: [[ 52.52749837   0.16859999   0.30220002   0.1381       0.30020002
    2.64389396]
 [ 39.63366413   0.16510001   0.0852       0.1714       0.153
    2.71988177]
 [ 11.93409245   0.54449999   0.41879997   0.43930003   0.32300001
    1.76532018]
 ...
 [193.34314447   0.1754       0.31780002   0.30960003   0.43640003
    2.27224541]
 [117.80159541   0.26359999   0.38340002   0.1811       0.36050001
    2.4078939 ]
 [-33.77116193   0.78100008   0.64110005   0.72360003   0.0227
    1.96317351]][0m
[37m[1m[2023-07-11 16:07:13,358][233954] Max Reward on eval: 454.07398130651563[0m
[37m[1m[2023-07-11 16:07:13,359][233954] Min Reward on eval: -77.66838168692775[0m
[37m[1m[2023-07-11 16:07:13,359][233954] Mean Reward across all agents: 86.16107522897575[0m
[37m[1m[2023-07-11 16:07:13,359][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:07:13,363][233954] mean_value=-198.20428053272414, max_value=710.6286737228104[0m
[37m[1m[2023-07-11 16:07:13,365][233954] New mean coefficients: [[-0.46308482  0.26842964 -0.38147706 -0.15786397  1.44506     0.2360254 ]][0m
[37m[1m[2023-07-11 16:07:13,366][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:07:22,369][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 16:07:22,369][233954] FPS: 426624.67[0m
[36m[2023-07-11 16:07:22,371][233954] itr=1174, itrs=2000, Progress: 58.70%[0m
[36m[2023-07-11 16:07:33,896][233954] train() took 11.43 seconds to complete[0m
[36m[2023-07-11 16:07:33,896][233954] FPS: 335933.67[0m
[36m[2023-07-11 16:07:38,213][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:07:38,213][233954] Reward + Measures: [[186.30209374   0.37339336   0.63024801   0.53371364   0.74646956
    1.44339919]][0m
[37m[1m[2023-07-11 16:07:38,213][233954] Max Reward on eval: 186.30209374140762[0m
[37m[1m[2023-07-11 16:07:38,214][233954] Min Reward on eval: 186.30209374140762[0m
[37m[1m[2023-07-11 16:07:38,214][233954] Mean Reward across all agents: 186.30209374140762[0m
[37m[1m[2023-07-11 16:07:38,214][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:07:43,216][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:07:43,217][233954] Reward + Measures: [[ 35.4176831    0.25760001   0.14839999   0.26530001   0.37490001
    1.49348772]
 [747.73809171   0.07450001   0.96990007   0.67559999   0.96850008
    2.88753343]
 [ 83.19984484   0.16000001   0.58269995   0.23150001   0.54189998
    2.06499457]
 ...
 [144.14975328   0.27840003   0.56560004   0.33750001   0.63420004
    2.09935069]
 [171.02102369   0.13079999   0.61250001   0.45120001   0.65759999
    1.76799238]
 [609.1536972    0.45420003   0.91759998   0.3215       0.91409999
    3.00427794]][0m
[37m[1m[2023-07-11 16:07:43,217][233954] Max Reward on eval: 849.2019195681438[0m
[37m[1m[2023-07-11 16:07:43,217][233954] Min Reward on eval: -147.02097337711604[0m
[37m[1m[2023-07-11 16:07:43,218][233954] Mean Reward across all agents: 280.02593309382667[0m
[37m[1m[2023-07-11 16:07:43,218][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:07:43,223][233954] mean_value=-210.22135356387125, max_value=452.75245271732894[0m
[37m[1m[2023-07-11 16:07:43,226][233954] New mean coefficients: [[ 1.6028565   0.6013841  -0.21834172 -1.6360978   1.5044017  -0.1777193 ]][0m
[37m[1m[2023-07-11 16:07:43,227][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:07:52,317][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 16:07:52,317][233954] FPS: 422519.96[0m
[36m[2023-07-11 16:07:52,319][233954] itr=1175, itrs=2000, Progress: 58.75%[0m
[36m[2023-07-11 16:08:04,123][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 16:08:04,123][233954] FPS: 327948.13[0m
[36m[2023-07-11 16:08:08,387][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:08:08,387][233954] Reward + Measures: [[192.87434208   0.39224467   0.67046994   0.54908431   0.7752744
    1.48384845]][0m
[37m[1m[2023-07-11 16:08:08,388][233954] Max Reward on eval: 192.874342082963[0m
[37m[1m[2023-07-11 16:08:08,388][233954] Min Reward on eval: 192.874342082963[0m
[37m[1m[2023-07-11 16:08:08,388][233954] Mean Reward across all agents: 192.874342082963[0m
[37m[1m[2023-07-11 16:08:08,388][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:08:13,611][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:08:13,612][233954] Reward + Measures: [[270.34674215   0.24290001   0.59910005   0.28830001   0.63849998
    2.55291533]
 [ 60.41942218   0.125        0.1921       0.1813       0.21900001
    1.7579596 ]
 [ 88.1385464    0.21280001   0.25470001   0.2158       0.24510001
    2.32293105]
 ...
 [ 92.25123997   0.19950001   0.25229999   0.20490001   0.27860001
    2.7712152 ]
 [464.48012205   0.292        0.75         0.6329       0.86650002
    2.46862292]
 [ 46.13801376   0.1399       0.30509999   0.1365       0.32410002
    2.23201823]][0m
[37m[1m[2023-07-11 16:08:13,612][233954] Max Reward on eval: 778.8906478747725[0m
[37m[1m[2023-07-11 16:08:13,613][233954] Min Reward on eval: -80.67351556858048[0m
[37m[1m[2023-07-11 16:08:13,613][233954] Mean Reward across all agents: 243.43866289354355[0m
[37m[1m[2023-07-11 16:08:13,613][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:08:13,618][233954] mean_value=-111.44685373596766, max_value=953.3336933455477[0m
[37m[1m[2023-07-11 16:08:13,621][233954] New mean coefficients: [[ 2.3731337   0.8022453  -0.02165264 -1.886838    1.5767249  -0.34735483]][0m
[37m[1m[2023-07-11 16:08:13,622][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:08:22,733][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 16:08:22,733][233954] FPS: 421540.36[0m
[36m[2023-07-11 16:08:22,735][233954] itr=1176, itrs=2000, Progress: 58.80%[0m
[36m[2023-07-11 16:08:34,507][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 16:08:34,507][233954] FPS: 328919.50[0m
[36m[2023-07-11 16:08:38,799][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:08:38,799][233954] Reward + Measures: [[204.43610471   0.33528769   0.65527797   0.50685066   0.76774764
    1.54199088]][0m
[37m[1m[2023-07-11 16:08:38,799][233954] Max Reward on eval: 204.4361047061307[0m
[37m[1m[2023-07-11 16:08:38,800][233954] Min Reward on eval: 204.4361047061307[0m
[37m[1m[2023-07-11 16:08:38,800][233954] Mean Reward across all agents: 204.4361047061307[0m
[37m[1m[2023-07-11 16:08:38,800][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:08:43,805][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:08:43,806][233954] Reward + Measures: [[267.48041726   0.17019999   0.69499999   0.58590001   0.70950001
    2.02699161]
 [166.17803107   0.12080001   0.71600002   0.3256       0.7281
    2.59329462]
 [ 66.80230049   0.2649       0.88789999   0.1171       0.86020005
    2.40695643]
 ...
 [313.89666274   0.1583       0.91520005   0.50889999   0.91320002
    2.38921452]
 [ 79.32590831   0.25540003   0.97540009   0.12539999   0.96950001
    2.67365503]
 [ 68.663905     0.3766       0.54170001   0.37189999   0.55940002
    2.7047739 ]][0m
[37m[1m[2023-07-11 16:08:43,806][233954] Max Reward on eval: 615.4736518910388[0m
[37m[1m[2023-07-11 16:08:43,806][233954] Min Reward on eval: -126.82665084686596[0m
[37m[1m[2023-07-11 16:08:43,806][233954] Mean Reward across all agents: 173.66126131627328[0m
[37m[1m[2023-07-11 16:08:43,807][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:08:43,810][233954] mean_value=-157.22528675982375, max_value=674.6535221580765[0m
[37m[1m[2023-07-11 16:08:43,813][233954] New mean coefficients: [[ 2.8477488   1.3640454  -0.02030197 -1.871136    2.343391    0.00449872]][0m
[37m[1m[2023-07-11 16:08:43,814][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:08:52,897][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 16:08:52,898][233954] FPS: 422813.88[0m
[36m[2023-07-11 16:08:52,900][233954] itr=1177, itrs=2000, Progress: 58.85%[0m
[36m[2023-07-11 16:09:04,642][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 16:09:04,643][233954] FPS: 329742.56[0m
[36m[2023-07-11 16:09:09,058][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:09:09,059][233954] Reward + Measures: [[220.76213099   0.32209164   0.67497504   0.50227201   0.78461099
    1.59427702]][0m
[37m[1m[2023-07-11 16:09:09,059][233954] Max Reward on eval: 220.76213098671806[0m
[37m[1m[2023-07-11 16:09:09,059][233954] Min Reward on eval: 220.76213098671806[0m
[37m[1m[2023-07-11 16:09:09,060][233954] Mean Reward across all agents: 220.76213098671806[0m
[37m[1m[2023-07-11 16:09:09,060][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:09:14,046][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:09:14,047][233954] Reward + Measures: [[123.46377901   0.1945       0.31619999   0.303        0.35730001
    2.03562284]
 [264.85589578   0.23600002   0.85519999   0.5855       0.8531
    1.94347501]
 [213.41965958   0.60040003   0.27770001   0.72539997   0.5862
    2.59162784]
 ...
 [116.0952154    0.46220002   0.2          0.47030002   0.25229999
    2.71754622]
 [122.67489291   0.3436       0.27650002   0.2782       0.42530003
    1.96299207]
 [220.80188036   0.43479997   0.44949999   0.60699999   0.44140002
    2.65114093]][0m
[37m[1m[2023-07-11 16:09:14,047][233954] Max Reward on eval: 519.4718169331551[0m
[37m[1m[2023-07-11 16:09:14,047][233954] Min Reward on eval: -43.72063157593366[0m
[37m[1m[2023-07-11 16:09:14,047][233954] Mean Reward across all agents: 140.30156864327625[0m
[37m[1m[2023-07-11 16:09:14,047][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:09:14,052][233954] mean_value=-260.2028233336766, max_value=725.3849021321116[0m
[37m[1m[2023-07-11 16:09:14,054][233954] New mean coefficients: [[ 2.9738731   2.3364534   0.20322302 -1.2423875   2.975739   -0.09585648]][0m
[37m[1m[2023-07-11 16:09:14,055][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:09:23,099][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 16:09:23,099][233954] FPS: 424678.84[0m
[36m[2023-07-11 16:09:23,101][233954] itr=1178, itrs=2000, Progress: 58.90%[0m
[36m[2023-07-11 16:09:34,935][233954] train() took 11.73 seconds to complete[0m
[36m[2023-07-11 16:09:34,935][233954] FPS: 327237.85[0m
[36m[2023-07-11 16:09:39,147][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:09:39,148][233954] Reward + Measures: [[236.9658935    0.31784701   0.73106235   0.5213207    0.81519431
    1.64838886]][0m
[37m[1m[2023-07-11 16:09:39,148][233954] Max Reward on eval: 236.96589349962747[0m
[37m[1m[2023-07-11 16:09:39,148][233954] Min Reward on eval: 236.96589349962747[0m
[37m[1m[2023-07-11 16:09:39,148][233954] Mean Reward across all agents: 236.96589349962747[0m
[37m[1m[2023-07-11 16:09:39,149][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:09:44,120][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:09:44,120][233954] Reward + Measures: [[355.43885516   0.06         0.65360004   0.37979999   0.66600001
    2.88313746]
 [185.45815181   0.37419999   0.22379999   0.35750002   0.39629999
    3.03457427]
 [ 50.82673264   0.32909998   0.47049999   0.0859       0.47160003
    2.58792663]
 ...
 [151.0225441    0.0993       0.62440008   0.34459999   0.62949997
    2.03389525]
 [310.01666377   0.0955       0.8391       0.45160004   0.82420009
    2.65369964]
 [ 79.60941888   0.34279999   0.18619999   0.2321       0.35210001
    2.66921473]][0m
[37m[1m[2023-07-11 16:09:44,121][233954] Max Reward on eval: 509.89146423889326[0m
[37m[1m[2023-07-11 16:09:44,121][233954] Min Reward on eval: -71.20083140106871[0m
[37m[1m[2023-07-11 16:09:44,121][233954] Mean Reward across all agents: 100.1913890656515[0m
[37m[1m[2023-07-11 16:09:44,121][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:09:44,124][233954] mean_value=-229.55691517956166, max_value=684.5534267812967[0m
[37m[1m[2023-07-11 16:09:44,127][233954] New mean coefficients: [[ 2.654631    2.4352822   0.41176558 -1.5602131   2.167365    0.32406488]][0m
[37m[1m[2023-07-11 16:09:44,128][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:09:53,099][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 16:09:53,100][233954] FPS: 428110.43[0m
[36m[2023-07-11 16:09:53,102][233954] itr=1179, itrs=2000, Progress: 58.95%[0m
[36m[2023-07-11 16:10:04,787][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 16:10:04,787][233954] FPS: 331408.43[0m
[36m[2023-07-11 16:10:09,034][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:10:09,035][233954] Reward + Measures: [[273.61617172   0.29590198   0.72973233   0.53211367   0.82131201
    1.70493829]][0m
[37m[1m[2023-07-11 16:10:09,035][233954] Max Reward on eval: 273.61617172237914[0m
[37m[1m[2023-07-11 16:10:09,035][233954] Min Reward on eval: 273.61617172237914[0m
[37m[1m[2023-07-11 16:10:09,036][233954] Mean Reward across all agents: 273.61617172237914[0m
[37m[1m[2023-07-11 16:10:09,036][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:10:14,029][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:10:14,030][233954] Reward + Measures: [[192.93565508   0.35620001   0.17779998   0.37550002   0.42750001
    2.12799335]
 [132.03544067   0.1331       0.23730002   0.13270001   0.17159998
    2.09244514]
 [182.3293759    0.21180001   0.43590003   0.26449999   0.38260001
    1.89516222]
 ...
 [151.22728132   0.38070002   0.41120002   0.40529999   0.23699999
    2.19729495]
 [ 77.2756683    0.25460002   0.3989       0.2428       0.49350005
    1.99771154]
 [451.17073061   0.0133       0.99069995   0.65399998   0.98810005
    2.71372032]][0m
[37m[1m[2023-07-11 16:10:14,030][233954] Max Reward on eval: 688.0179366810829[0m
[37m[1m[2023-07-11 16:10:14,030][233954] Min Reward on eval: -98.46682725828141[0m
[37m[1m[2023-07-11 16:10:14,030][233954] Mean Reward across all agents: 117.52672262760338[0m
[37m[1m[2023-07-11 16:10:14,031][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:10:14,034][233954] mean_value=-319.6371965939386, max_value=316.5743453974113[0m
[37m[1m[2023-07-11 16:10:14,036][233954] New mean coefficients: [[ 1.2211995   1.5320072   0.10231543 -0.5345601   1.6481295   0.6502146 ]][0m
[37m[1m[2023-07-11 16:10:14,037][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:10:23,061][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 16:10:23,061][233954] FPS: 425635.88[0m
[36m[2023-07-11 16:10:23,063][233954] itr=1180, itrs=2000, Progress: 59.00%[0m
[37m[1m[2023-07-11 16:13:52,164][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001160[0m
[36m[2023-07-11 16:14:04,825][233954] train() took 11.88 seconds to complete[0m
[36m[2023-07-11 16:14:04,825][233954] FPS: 323099.94[0m
[36m[2023-07-11 16:14:09,144][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:14:09,145][233954] Reward + Measures: [[296.35517721   0.29495698   0.76666826   0.53731501   0.84261227
    1.75723922]][0m
[37m[1m[2023-07-11 16:14:09,145][233954] Max Reward on eval: 296.35517721321634[0m
[37m[1m[2023-07-11 16:14:09,145][233954] Min Reward on eval: 296.35517721321634[0m
[37m[1m[2023-07-11 16:14:09,145][233954] Mean Reward across all agents: 296.35517721321634[0m
[37m[1m[2023-07-11 16:14:09,146][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:14:14,131][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:14:14,132][233954] Reward + Measures: [[109.69465513   0.16430001   0.1363       0.2045       0.25420001
    2.30360532]
 [141.09891081   0.2674       0.55919999   0.36990002   0.62490004
    2.03506517]
 [184.24621969   0.1743       0.40619999   0.38069999   0.51380002
    2.59870982]
 ...
 [229.31357048   0.1257       0.43150005   0.3337       0.51990002
    1.59033358]
 [ 57.88005755   0.45989999   0.56370002   0.44910002   0.46380001
    2.33200884]
 [121.45532891   0.14219999   0.22659998   0.26440001   0.38000003
    3.04514623]][0m
[37m[1m[2023-07-11 16:14:14,132][233954] Max Reward on eval: 781.5855026132427[0m
[37m[1m[2023-07-11 16:14:14,132][233954] Min Reward on eval: -263.64992879768835[0m
[37m[1m[2023-07-11 16:14:14,132][233954] Mean Reward across all agents: 140.5250829841072[0m
[37m[1m[2023-07-11 16:14:14,133][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:14:14,138][233954] mean_value=-108.58223076354786, max_value=751.8075743621681[0m
[37m[1m[2023-07-11 16:14:14,141][233954] New mean coefficients: [[ 1.7615399   0.84836465 -0.5791385  -1.147335    0.96965855  0.65908754]][0m
[37m[1m[2023-07-11 16:14:14,142][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:14:23,185][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 16:14:23,185][233954] FPS: 424725.89[0m
[36m[2023-07-11 16:14:23,187][233954] itr=1181, itrs=2000, Progress: 59.05%[0m
[36m[2023-07-11 16:14:34,928][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 16:14:34,929][233954] FPS: 329756.51[0m
[36m[2023-07-11 16:14:39,193][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:14:39,194][233954] Reward + Measures: [[354.323202     0.25643131   0.78653765   0.55491602   0.85901904
    1.87322164]][0m
[37m[1m[2023-07-11 16:14:39,194][233954] Max Reward on eval: 354.32320200257044[0m
[37m[1m[2023-07-11 16:14:39,194][233954] Min Reward on eval: 354.32320200257044[0m
[37m[1m[2023-07-11 16:14:39,194][233954] Mean Reward across all agents: 354.32320200257044[0m
[37m[1m[2023-07-11 16:14:39,195][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:14:44,382][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:14:44,383][233954] Reward + Measures: [[ 28.00075903   0.2106       0.2438       0.1926       0.29049999
    2.24366403]
 [ 84.60806775   0.18279998   0.32449999   0.19760001   0.33659998
    1.88930213]
 [ 47.46458247   0.34         0.40889999   0.14380001   0.47740003
    1.6749413 ]
 ...
 [148.63160609   0.2139       0.3669       0.23839998   0.47159997
    1.83290732]
 [150.89394091   0.14259999   0.64770001   0.28840002   0.62930006
    2.10639691]
 [428.4058609    0.15279999   0.93009996   0.59610003   0.91549999
    2.39277577]][0m
[37m[1m[2023-07-11 16:14:44,383][233954] Max Reward on eval: 548.3524341592565[0m
[37m[1m[2023-07-11 16:14:44,383][233954] Min Reward on eval: -73.224003580492[0m
[37m[1m[2023-07-11 16:14:44,383][233954] Mean Reward across all agents: 191.14318829773032[0m
[37m[1m[2023-07-11 16:14:44,384][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:14:44,389][233954] mean_value=-115.0959018136486, max_value=1048.3524341592565[0m
[37m[1m[2023-07-11 16:14:44,391][233954] New mean coefficients: [[ 0.8950555   0.37282777 -0.34428197 -1.1215931   0.42616397  0.4188231 ]][0m
[37m[1m[2023-07-11 16:14:44,392][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:14:53,409][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 16:14:53,409][233954] FPS: 425961.96[0m
[36m[2023-07-11 16:14:53,411][233954] itr=1182, itrs=2000, Progress: 59.10%[0m
[36m[2023-07-11 16:15:04,974][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 16:15:04,975][233954] FPS: 334839.23[0m
[36m[2023-07-11 16:15:09,208][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:15:09,208][233954] Reward + Measures: [[385.30246173   0.23363267   0.80716437   0.55008698   0.86546165
    1.93693519]][0m
[37m[1m[2023-07-11 16:15:09,208][233954] Max Reward on eval: 385.30246172662584[0m
[37m[1m[2023-07-11 16:15:09,208][233954] Min Reward on eval: 385.30246172662584[0m
[37m[1m[2023-07-11 16:15:09,209][233954] Mean Reward across all agents: 385.30246172662584[0m
[37m[1m[2023-07-11 16:15:09,209][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:15:14,210][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:15:14,211][233954] Reward + Measures: [[113.84490537   0.38069996   0.86289996   0.47139999   0.82050002
    2.0063386 ]
 [137.53504428   0.1498       0.50270003   0.30539998   0.48999998
    1.82037818]
 [ 66.1622295    0.30759999   0.46880004   0.14929999   0.51480001
    1.84005344]
 ...
 [ 66.28054167   0.5711       0.74989998   0.5262       0.7414
    1.60695803]
 [108.25702332   0.59140003   0.92000002   0.30600002   0.8617
    1.77456748]
 [ 67.80897008   0.46370003   0.60260004   0.4533       0.58640003
    1.58172107]][0m
[37m[1m[2023-07-11 16:15:14,211][233954] Max Reward on eval: 547.035549644567[0m
[37m[1m[2023-07-11 16:15:14,211][233954] Min Reward on eval: -116.86463884070982[0m
[37m[1m[2023-07-11 16:15:14,211][233954] Mean Reward across all agents: 159.1338113480163[0m
[37m[1m[2023-07-11 16:15:14,212][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:15:14,217][233954] mean_value=-148.22549749631642, max_value=427.31616772213374[0m
[37m[1m[2023-07-11 16:15:14,219][233954] New mean coefficients: [[ 0.87543434  0.54541177 -0.63882565 -0.15705645  0.8110788   0.98520255]][0m
[37m[1m[2023-07-11 16:15:14,220][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:15:23,202][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 16:15:23,202][233954] FPS: 427631.92[0m
[36m[2023-07-11 16:15:23,204][233954] itr=1183, itrs=2000, Progress: 59.15%[0m
[36m[2023-07-11 16:15:34,951][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 16:15:34,951][233954] FPS: 329594.49[0m
[36m[2023-07-11 16:15:39,214][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:15:39,215][233954] Reward + Measures: [[412.46885111   0.208619     0.82883763   0.55846      0.879179
    2.02623796]][0m
[37m[1m[2023-07-11 16:15:39,215][233954] Max Reward on eval: 412.4688511086772[0m
[37m[1m[2023-07-11 16:15:39,215][233954] Min Reward on eval: 412.4688511086772[0m
[37m[1m[2023-07-11 16:15:39,216][233954] Mean Reward across all agents: 412.4688511086772[0m
[37m[1m[2023-07-11 16:15:39,216][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:15:44,179][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:15:44,180][233954] Reward + Measures: [[511.42465388   0.0152       0.82810003   0.60650009   0.82460004
    2.34885907]
 [374.80933323   0.08270001   0.93710005   0.6354       0.91250002
    2.25537372]
 [  8.30157654   0.4745       0.7148       0.47950003   0.68169999
    2.09278178]
 ...
 [187.15912582   0.1313       0.55750006   0.33970004   0.59310001
    2.40571713]
 [ 12.82192821   0.37720001   0.52950001   0.36430001   0.46830001
    1.49862993]
 [438.547163     0.0204       0.98040003   0.67330003   0.97399998
    2.54247642]][0m
[37m[1m[2023-07-11 16:15:44,180][233954] Max Reward on eval: 664.6837234463543[0m
[37m[1m[2023-07-11 16:15:44,180][233954] Min Reward on eval: -183.6291074943496[0m
[37m[1m[2023-07-11 16:15:44,180][233954] Mean Reward across all agents: 163.79872884756668[0m
[37m[1m[2023-07-11 16:15:44,181][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:15:44,184][233954] mean_value=-218.57470719748648, max_value=743.2089605994745[0m
[37m[1m[2023-07-11 16:15:44,187][233954] New mean coefficients: [[ 0.28782833  0.40099382 -0.6959677   0.7205019   0.9837314   0.99004525]][0m
[37m[1m[2023-07-11 16:15:44,188][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:15:53,172][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 16:15:53,173][233954] FPS: 427482.22[0m
[36m[2023-07-11 16:15:53,175][233954] itr=1184, itrs=2000, Progress: 59.20%[0m
[36m[2023-07-11 16:16:05,059][233954] train() took 11.78 seconds to complete[0m
[36m[2023-07-11 16:16:05,060][233954] FPS: 325840.50[0m
[36m[2023-07-11 16:16:09,390][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:16:09,390][233954] Reward + Measures: [[405.37634947   0.22227335   0.83629507   0.55429101   0.88051766
    2.0246048 ]][0m
[37m[1m[2023-07-11 16:16:09,391][233954] Max Reward on eval: 405.37634946503306[0m
[37m[1m[2023-07-11 16:16:09,391][233954] Min Reward on eval: 405.37634946503306[0m
[37m[1m[2023-07-11 16:16:09,391][233954] Mean Reward across all agents: 405.37634946503306[0m
[37m[1m[2023-07-11 16:16:09,391][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:16:14,424][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:16:14,424][233954] Reward + Measures: [[247.65537551   0.37400001   0.48780003   0.62730002   0.6649
    2.07307243]
 [ 93.44719663   0.09230001   0.33839998   0.18310001   0.4657
    2.29237795]
 [157.47933936   0.1894       0.66370004   0.21660002   0.65240002
    2.13481355]
 ...
 [323.85967272   0.2739       0.79229999   0.31979999   0.8193
    2.05080533]
 [211.96527372   0.26799998   0.68520004   0.24430001   0.70659995
    1.96950877]
 [ 73.38800448   0.2103       0.56840003   0.17020001   0.565
    1.75641537]][0m
[37m[1m[2023-07-11 16:16:14,424][233954] Max Reward on eval: 653.5144500616473[0m
[37m[1m[2023-07-11 16:16:14,425][233954] Min Reward on eval: -1.0262941738590599[0m
[37m[1m[2023-07-11 16:16:14,425][233954] Mean Reward across all agents: 237.64308841697007[0m
[37m[1m[2023-07-11 16:16:14,425][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:16:14,433][233954] mean_value=-6.123566437005466, max_value=663.1216750998028[0m
[37m[1m[2023-07-11 16:16:14,436][233954] New mean coefficients: [[ 0.86702806  0.9165729  -1.0154957   0.22829908  1.0294552   0.75162107]][0m
[37m[1m[2023-07-11 16:16:14,437][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:16:23,449][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 16:16:23,449][233954] FPS: 426189.74[0m
[36m[2023-07-11 16:16:23,451][233954] itr=1185, itrs=2000, Progress: 59.25%[0m
[36m[2023-07-11 16:16:35,247][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 16:16:35,247][233954] FPS: 328200.94[0m
[36m[2023-07-11 16:16:39,501][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:16:39,501][233954] Reward + Measures: [[424.42305716   0.19321699   0.86250603   0.56069636   0.89803529
    2.10441113]][0m
[37m[1m[2023-07-11 16:16:39,502][233954] Max Reward on eval: 424.42305716348983[0m
[37m[1m[2023-07-11 16:16:39,502][233954] Min Reward on eval: 424.42305716348983[0m
[37m[1m[2023-07-11 16:16:39,502][233954] Mean Reward across all agents: 424.42305716348983[0m
[37m[1m[2023-07-11 16:16:39,502][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:16:44,494][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:16:44,494][233954] Reward + Measures: [[193.45940283   0.28760001   0.80599993   0.50980002   0.78640002
    1.93862844]
 [230.73411557   0.20650001   0.509        0.48670003   0.56739998
    2.1809628 ]
 [320.5111815    0.16080001   0.57660002   0.49200001   0.67979997
    2.34455872]
 ...
 [167.83474636   0.24330001   0.50550002   0.50669998   0.59730005
    2.15028048]
 [234.46182883   0.2418       0.52340001   0.51089996   0.59820002
    2.15961432]
 [ 48.37667081   0.15549999   0.13609999   0.17290001   0.1919
    2.08545208]][0m
[37m[1m[2023-07-11 16:16:44,495][233954] Max Reward on eval: 753.4201355222613[0m
[37m[1m[2023-07-11 16:16:44,495][233954] Min Reward on eval: -81.77007036879658[0m
[37m[1m[2023-07-11 16:16:44,495][233954] Mean Reward across all agents: 170.5050267981164[0m
[37m[1m[2023-07-11 16:16:44,495][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:16:44,500][233954] mean_value=-146.06834276342352, max_value=782.0752539574169[0m
[37m[1m[2023-07-11 16:16:44,502][233954] New mean coefficients: [[-0.46527916  0.26969278 -1.1209855   0.58980525  1.2114793   0.7701507 ]][0m
[37m[1m[2023-07-11 16:16:44,503][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:16:53,522][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 16:16:53,522][233954] FPS: 425871.99[0m
[36m[2023-07-11 16:16:53,525][233954] itr=1186, itrs=2000, Progress: 59.30%[0m
[36m[2023-07-11 16:17:05,216][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 16:17:05,216][233954] FPS: 331215.79[0m
[36m[2023-07-11 16:17:09,577][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:17:09,578][233954] Reward + Measures: [[425.37529803   0.19358131   0.85749495   0.57224399   0.89947462
    2.10128593]][0m
[37m[1m[2023-07-11 16:17:09,578][233954] Max Reward on eval: 425.37529802549506[0m
[37m[1m[2023-07-11 16:17:09,578][233954] Min Reward on eval: 425.37529802549506[0m
[37m[1m[2023-07-11 16:17:09,578][233954] Mean Reward across all agents: 425.37529802549506[0m
[37m[1m[2023-07-11 16:17:09,579][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:17:14,556][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:17:14,557][233954] Reward + Measures: [[262.30536049   0.057        0.52439994   0.36279997   0.53039998
    2.16763139]
 [236.70294472   0.23169999   0.8563       0.52420002   0.82740003
    1.98564947]
 [115.40299193   0.26230001   0.35350001   0.0987       0.34599999
    1.46376896]
 ...
 [154.91406394   0.1689       0.49860001   0.22379999   0.49670002
    1.91123617]
 [286.3717533    0.1453       0.53780001   0.4348       0.61540002
    1.88831031]
 [106.17360111   0.11719999   0.43400002   0.19420001   0.40019998
    1.77904129]][0m
[37m[1m[2023-07-11 16:17:14,557][233954] Max Reward on eval: 508.65082168933003[0m
[37m[1m[2023-07-11 16:17:14,558][233954] Min Reward on eval: -130.44768499570782[0m
[37m[1m[2023-07-11 16:17:14,558][233954] Mean Reward across all agents: 211.02672058953894[0m
[37m[1m[2023-07-11 16:17:14,558][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:17:14,563][233954] mean_value=-97.14546656920378, max_value=567.5196901030838[0m
[37m[1m[2023-07-11 16:17:14,566][233954] New mean coefficients: [[-0.6776637  -0.73427176 -1.35594     0.5605164   0.22585535  0.82485473]][0m
[37m[1m[2023-07-11 16:17:14,567][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:17:23,526][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 16:17:23,526][233954] FPS: 428704.16[0m
[36m[2023-07-11 16:17:23,528][233954] itr=1187, itrs=2000, Progress: 59.35%[0m
[36m[2023-07-11 16:17:35,260][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 16:17:35,261][233954] FPS: 329965.63[0m
[36m[2023-07-11 16:17:39,542][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:17:39,542][233954] Reward + Measures: [[423.54163779   0.20856202   0.854725     0.56698436   0.89533037
    2.07051992]][0m
[37m[1m[2023-07-11 16:17:39,543][233954] Max Reward on eval: 423.5416377922082[0m
[37m[1m[2023-07-11 16:17:39,543][233954] Min Reward on eval: 423.5416377922082[0m
[37m[1m[2023-07-11 16:17:39,543][233954] Mean Reward across all agents: 423.5416377922082[0m
[37m[1m[2023-07-11 16:17:39,543][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:17:44,861][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:17:44,862][233954] Reward + Measures: [[284.83289484   0.41409999   0.78719997   0.51000005   0.829
    1.82305551]
 [401.25173522   0.28800002   0.74120003   0.5413       0.81599998
    2.13779998]
 [315.3587029    0.2771       0.55059999   0.55070001   0.70430005
    1.93549728]
 ...
 [395.94784237   0.235        0.89169997   0.6649       0.87980002
    1.89149952]
 [ 90.97948242   0.40549999   0.32280001   0.51550001   0.62799996
    1.57455456]
 [424.47793436   0.1068       0.89270002   0.61660004   0.91280001
    2.11744094]][0m
[37m[1m[2023-07-11 16:17:44,862][233954] Max Reward on eval: 573.5771446045022[0m
[37m[1m[2023-07-11 16:17:44,862][233954] Min Reward on eval: 27.53367715687491[0m
[37m[1m[2023-07-11 16:17:44,862][233954] Mean Reward across all agents: 242.5461217581454[0m
[37m[1m[2023-07-11 16:17:44,862][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:17:44,869][233954] mean_value=1.6328985372804647, max_value=744.8089030773007[0m
[37m[1m[2023-07-11 16:17:44,872][233954] New mean coefficients: [[-0.91820407 -1.0066087  -1.4011368   0.73978597 -0.2545089   0.7562795 ]][0m
[37m[1m[2023-07-11 16:17:44,873][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:17:53,825][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 16:17:53,825][233954] FPS: 429023.07[0m
[36m[2023-07-11 16:17:53,827][233954] itr=1188, itrs=2000, Progress: 59.40%[0m
[36m[2023-07-11 16:18:05,458][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 16:18:05,459][233954] FPS: 332952.67[0m
[36m[2023-07-11 16:18:09,728][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:18:09,728][233954] Reward + Measures: [[411.77500404   0.20401168   0.8239553    0.55462801   0.87431902
    2.04706693]][0m
[37m[1m[2023-07-11 16:18:09,729][233954] Max Reward on eval: 411.7750040377952[0m
[37m[1m[2023-07-11 16:18:09,729][233954] Min Reward on eval: 411.7750040377952[0m
[37m[1m[2023-07-11 16:18:09,729][233954] Mean Reward across all agents: 411.7750040377952[0m
[37m[1m[2023-07-11 16:18:09,729][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:18:14,705][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:18:14,706][233954] Reward + Measures: [[ 88.88811969   0.1778       0.2897       0.14799999   0.23360001
    2.04611373]
 [221.18659424   0.36840001   0.38270003   0.41750002   0.56129998
    2.16576147]
 [632.79201125   0.0133       0.89680004   0.68020004   0.9052
    2.59149265]
 ...
 [ 20.0243415    0.56849998   0.49759999   0.61770004   0.47119999
    2.44322467]
 [129.58171231   0.22719999   0.25229999   0.1928       0.2667
    2.21632886]
 [615.34062194   0.009        0.99279994   0.80109996   0.99729997
    2.25394702]][0m
[37m[1m[2023-07-11 16:18:14,706][233954] Max Reward on eval: 760.7701682975982[0m
[37m[1m[2023-07-11 16:18:14,706][233954] Min Reward on eval: -144.17049513321837[0m
[37m[1m[2023-07-11 16:18:14,706][233954] Mean Reward across all agents: 144.9899170643807[0m
[37m[1m[2023-07-11 16:18:14,707][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:18:14,711][233954] mean_value=-151.93801506553365, max_value=597.5500467382249[0m
[37m[1m[2023-07-11 16:18:14,714][233954] New mean coefficients: [[-0.29106104 -0.6646948  -1.6569492   1.3797402  -0.5877428   1.3236408 ]][0m
[37m[1m[2023-07-11 16:18:14,715][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:18:23,743][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 16:18:23,743][233954] FPS: 425410.98[0m
[36m[2023-07-11 16:18:23,746][233954] itr=1189, itrs=2000, Progress: 59.45%[0m
[36m[2023-07-11 16:18:35,293][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 16:18:35,293][233954] FPS: 335435.16[0m
[36m[2023-07-11 16:18:39,619][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:18:39,620][233954] Reward + Measures: [[408.9036741    0.19242899   0.82041126   0.57275701   0.86971337
    2.0733099 ]][0m
[37m[1m[2023-07-11 16:18:39,620][233954] Max Reward on eval: 408.90367410178453[0m
[37m[1m[2023-07-11 16:18:39,620][233954] Min Reward on eval: 408.90367410178453[0m
[37m[1m[2023-07-11 16:18:39,620][233954] Mean Reward across all agents: 408.90367410178453[0m
[37m[1m[2023-07-11 16:18:39,620][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:18:44,603][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:18:44,604][233954] Reward + Measures: [[449.191594     0.1639       0.82800001   0.57560003   0.83740008
    2.06143546]
 [374.70341816   0.09400001   0.72490001   0.48930001   0.72819996
    2.23995638]
 [380.86250446   0.27379999   0.70860004   0.5499       0.824
    1.99854589]
 ...
 [376.40826511   0.0344       0.62910002   0.48750001   0.69959992
    2.00927901]
 [352.57583191   0.25890002   0.87         0.62790006   0.90210003
    1.85460913]
 [227.91126254   0.27359998   0.7277       0.52459997   0.7712
    1.79665339]][0m
[37m[1m[2023-07-11 16:18:44,604][233954] Max Reward on eval: 627.0685730125755[0m
[37m[1m[2023-07-11 16:18:44,604][233954] Min Reward on eval: 31.845527166430838[0m
[37m[1m[2023-07-11 16:18:44,605][233954] Mean Reward across all agents: 330.27128453857404[0m
[37m[1m[2023-07-11 16:18:44,605][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:18:44,611][233954] mean_value=-23.63825523356976, max_value=816.0668980857386[0m
[37m[1m[2023-07-11 16:18:44,614][233954] New mean coefficients: [[ 0.07969701 -0.8092497  -1.4746459   1.6287715  -0.2449092   1.541341  ]][0m
[37m[1m[2023-07-11 16:18:44,615][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:18:53,640][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 16:18:53,641][233954] FPS: 425530.03[0m
[36m[2023-07-11 16:18:53,643][233954] itr=1190, itrs=2000, Progress: 59.50%[0m
[37m[1m[2023-07-11 16:22:34,145][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001170[0m
[36m[2023-07-11 16:22:47,174][233954] train() took 11.93 seconds to complete[0m
[36m[2023-07-11 16:22:47,174][233954] FPS: 321875.67[0m
[36m[2023-07-11 16:22:51,358][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:22:51,358][233954] Reward + Measures: [[425.28411149   0.16730334   0.81764174   0.58415407   0.86664838
    2.08710289]][0m
[37m[1m[2023-07-11 16:22:51,359][233954] Max Reward on eval: 425.28411148604664[0m
[37m[1m[2023-07-11 16:22:51,359][233954] Min Reward on eval: 425.28411148604664[0m
[37m[1m[2023-07-11 16:22:51,359][233954] Mean Reward across all agents: 425.28411148604664[0m
[37m[1m[2023-07-11 16:22:51,359][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:22:56,326][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:22:56,327][233954] Reward + Measures: [[310.17172717   0.21469998   0.69460005   0.53119993   0.73879999
    1.6607579 ]
 [ 50.69916468   0.3326       0.62659997   0.42130002   0.5693
    1.94043958]
 [146.51656004   0.50139999   0.3132       0.47550002   0.60550004
    2.05415225]
 ...
 [229.30760312   0.31680003   0.46989998   0.48520002   0.65140003
    1.92389524]
 [196.4293835    0.1407       0.57590002   0.38029999   0.67860001
    1.7567215 ]
 [483.14363861   0.0954       0.82520002   0.65350002   0.89750004
    2.43538833]][0m
[37m[1m[2023-07-11 16:22:56,327][233954] Max Reward on eval: 731.5535240285332[0m
[37m[1m[2023-07-11 16:22:56,327][233954] Min Reward on eval: -31.00709055826301[0m
[37m[1m[2023-07-11 16:22:56,327][233954] Mean Reward across all agents: 292.8589754151349[0m
[37m[1m[2023-07-11 16:22:56,328][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:22:56,333][233954] mean_value=-41.20075988311805, max_value=503.73023992846043[0m
[37m[1m[2023-07-11 16:22:56,336][233954] New mean coefficients: [[ 0.5365961  -0.65319127 -1.6132755   1.650247   -0.00180081  1.689385  ]][0m
[37m[1m[2023-07-11 16:22:56,337][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:23:05,326][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 16:23:05,326][233954] FPS: 427262.81[0m
[36m[2023-07-11 16:23:05,328][233954] itr=1191, itrs=2000, Progress: 59.55%[0m
[36m[2023-07-11 16:23:17,083][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 16:23:17,084][233954] FPS: 329441.54[0m
[36m[2023-07-11 16:23:21,350][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:23:21,351][233954] Reward + Measures: [[418.08387278   0.16556534   0.8006587    0.57534665   0.85569638
    2.08628511]][0m
[37m[1m[2023-07-11 16:23:21,351][233954] Max Reward on eval: 418.08387277534354[0m
[37m[1m[2023-07-11 16:23:21,351][233954] Min Reward on eval: 418.08387277534354[0m
[37m[1m[2023-07-11 16:23:21,352][233954] Mean Reward across all agents: 418.08387277534354[0m
[37m[1m[2023-07-11 16:23:21,352][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:23:26,563][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:23:26,564][233954] Reward + Measures: [[427.51960848   0.0449       0.6329       0.46740004   0.71320003
    1.96302032]
 [383.85859519   0.1163       0.72440004   0.4804       0.76770008
    2.08468056]
 [143.79292872   0.2361       0.41640002   0.32940003   0.53310007
    1.49339712]
 ...
 [407.1995611    0.1892       0.89640009   0.61000001   0.89810002
    2.16341376]
 [501.3153764    0.1046       0.80500001   0.58890003   0.83329993
    2.42235231]
 [518.04875375   0.16949999   0.90250009   0.77850002   0.9677
    2.45099068]][0m
[37m[1m[2023-07-11 16:23:26,564][233954] Max Reward on eval: 611.4829015765339[0m
[37m[1m[2023-07-11 16:23:26,564][233954] Min Reward on eval: -94.12431017789058[0m
[37m[1m[2023-07-11 16:23:26,564][233954] Mean Reward across all agents: 326.3856936041087[0m
[37m[1m[2023-07-11 16:23:26,565][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:23:26,570][233954] mean_value=-19.809211315863248, max_value=647.3360008153148[0m
[37m[1m[2023-07-11 16:23:26,573][233954] New mean coefficients: [[ 0.15383774 -0.596564   -1.543575    1.5759141   0.01377897  1.5559497 ]][0m
[37m[1m[2023-07-11 16:23:26,574][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:23:35,609][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 16:23:35,609][233954] FPS: 425092.14[0m
[36m[2023-07-11 16:23:35,612][233954] itr=1192, itrs=2000, Progress: 59.60%[0m
[36m[2023-07-11 16:23:47,223][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 16:23:47,223][233954] FPS: 333484.87[0m
[36m[2023-07-11 16:23:51,439][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:23:51,439][233954] Reward + Measures: [[424.83299005   0.14348434   0.79349333   0.58961165   0.84686601
    2.11648202]][0m
[37m[1m[2023-07-11 16:23:51,439][233954] Max Reward on eval: 424.8329900491811[0m
[37m[1m[2023-07-11 16:23:51,439][233954] Min Reward on eval: 424.8329900491811[0m
[37m[1m[2023-07-11 16:23:51,439][233954] Mean Reward across all agents: 424.8329900491811[0m
[37m[1m[2023-07-11 16:23:51,440][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:23:56,422][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:23:56,423][233954] Reward + Measures: [[519.26840235   0.0145       0.89480001   0.66339999   0.89159995
    2.37859321]
 [220.01767253   0.09100001   0.35900003   0.25959998   0.49980003
    1.61787152]
 [277.76863744   0.21940003   0.69349998   0.43020001   0.6789
    1.80618405]
 ...
 [433.65301456   0.1111       0.56020004   0.4601       0.66339999
    2.16441703]
 [299.208097     0.34580001   0.78669995   0.60440004   0.84350008
    1.81033576]
 [384.51939234   0.18359999   0.70020008   0.53669995   0.79659998
    1.96169662]][0m
[37m[1m[2023-07-11 16:23:56,423][233954] Max Reward on eval: 645.4708647809224[0m
[37m[1m[2023-07-11 16:23:56,423][233954] Min Reward on eval: -7.5258542550029235[0m
[37m[1m[2023-07-11 16:23:56,423][233954] Mean Reward across all agents: 356.80878261227423[0m
[37m[1m[2023-07-11 16:23:56,424][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:23:56,428][233954] mean_value=-32.14808602703304, max_value=665.9491889698674[0m
[37m[1m[2023-07-11 16:23:56,431][233954] New mean coefficients: [[ 0.42951912 -0.53369504 -1.4046327   1.1766387  -0.2830945   0.7120052 ]][0m
[37m[1m[2023-07-11 16:23:56,432][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:24:05,469][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 16:24:05,469][233954] FPS: 425006.20[0m
[36m[2023-07-11 16:24:05,471][233954] itr=1193, itrs=2000, Progress: 59.65%[0m
[36m[2023-07-11 16:24:17,127][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 16:24:17,128][233954] FPS: 332209.16[0m
[36m[2023-07-11 16:24:21,401][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:24:21,402][233954] Reward + Measures: [[441.88234109   0.13821034   0.82313502   0.60726702   0.86784929
    2.17187929]][0m
[37m[1m[2023-07-11 16:24:21,402][233954] Max Reward on eval: 441.88234108652756[0m
[37m[1m[2023-07-11 16:24:21,402][233954] Min Reward on eval: 441.88234108652756[0m
[37m[1m[2023-07-11 16:24:21,403][233954] Mean Reward across all agents: 441.88234108652756[0m
[37m[1m[2023-07-11 16:24:21,403][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:24:26,376][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:24:26,377][233954] Reward + Measures: [[301.63378741   0.1068       0.46520001   0.36179999   0.53719997
    1.6835171 ]
 [239.23215867   0.43209997   0.78520006   0.46680003   0.8811
    1.94363534]
 [301.61688353   0.30609998   0.71859998   0.597        0.73920006
    1.80927432]
 ...
 [433.39563369   0.1074       0.89800006   0.60680002   0.92140001
    2.56049418]
 [361.83510714   0.1163       0.62670004   0.39949998   0.74919999
    2.10338187]
 [339.25923624   0.1936       0.63010001   0.35210001   0.67199999
    1.81669223]][0m
[37m[1m[2023-07-11 16:24:26,377][233954] Max Reward on eval: 629.0746040063444[0m
[37m[1m[2023-07-11 16:24:26,378][233954] Min Reward on eval: -257.2155385279795[0m
[37m[1m[2023-07-11 16:24:26,378][233954] Mean Reward across all agents: 289.0142186386463[0m
[37m[1m[2023-07-11 16:24:26,378][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:24:26,382][233954] mean_value=-80.77428988761343, max_value=409.0248240192009[0m
[37m[1m[2023-07-11 16:24:26,385][233954] New mean coefficients: [[ 0.38017613 -0.20911208 -1.9547007   0.7135339  -0.15232413  0.8894518 ]][0m
[37m[1m[2023-07-11 16:24:26,386][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:24:35,340][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 16:24:35,340][233954] FPS: 428946.61[0m
[36m[2023-07-11 16:24:35,342][233954] itr=1194, itrs=2000, Progress: 59.70%[0m
[36m[2023-07-11 16:24:46,893][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 16:24:46,894][233954] FPS: 335288.36[0m
[36m[2023-07-11 16:24:51,172][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:24:51,173][233954] Reward + Measures: [[423.45160452   0.133803     0.78532171   0.59435862   0.841528
    2.14226079]][0m
[37m[1m[2023-07-11 16:24:51,173][233954] Max Reward on eval: 423.4516045182917[0m
[37m[1m[2023-07-11 16:24:51,173][233954] Min Reward on eval: 423.4516045182917[0m
[37m[1m[2023-07-11 16:24:51,174][233954] Mean Reward across all agents: 423.4516045182917[0m
[37m[1m[2023-07-11 16:24:51,174][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:24:56,201][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:24:56,202][233954] Reward + Measures: [[237.73069143   0.2359       0.68850005   0.5151       0.73169994
    1.7221216 ]
 [212.25190688   0.27210003   0.52880001   0.55240005   0.70649999
    1.83186722]
 [362.2572937    0.43519998   0.90090001   0.41999999   0.92250007
    1.99733257]
 ...
 [181.2403104    0.35680002   0.67810005   0.54710001   0.66900003
    1.53938961]
 [123.47913543   0.29840001   0.3497       0.42410001   0.5729
    1.6946739 ]
 [206.12741636   0.12660001   0.71850008   0.42199999   0.73500007
    1.95418668]][0m
[37m[1m[2023-07-11 16:24:56,202][233954] Max Reward on eval: 579.6528482460417[0m
[37m[1m[2023-07-11 16:24:56,202][233954] Min Reward on eval: -109.60782568193972[0m
[37m[1m[2023-07-11 16:24:56,203][233954] Mean Reward across all agents: 192.21131276415784[0m
[37m[1m[2023-07-11 16:24:56,203][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:24:56,206][233954] mean_value=-186.8016385201194, max_value=342.04611673313525[0m
[37m[1m[2023-07-11 16:24:56,209][233954] New mean coefficients: [[ 0.5412257  -0.49492145 -2.0571678   0.26045233 -0.25581884  0.8459959 ]][0m
[37m[1m[2023-07-11 16:24:56,210][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:25:05,195][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 16:25:05,195][233954] FPS: 427451.72[0m
[36m[2023-07-11 16:25:05,197][233954] itr=1195, itrs=2000, Progress: 59.75%[0m
[36m[2023-07-11 16:25:16,870][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 16:25:16,876][233954] FPS: 331780.52[0m
[36m[2023-07-11 16:25:21,174][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:25:21,174][233954] Reward + Measures: [[434.86028442   0.12906966   0.78468424   0.61058432   0.84122902
    2.14910698]][0m
[37m[1m[2023-07-11 16:25:21,175][233954] Max Reward on eval: 434.86028442086746[0m
[37m[1m[2023-07-11 16:25:21,175][233954] Min Reward on eval: 434.86028442086746[0m
[37m[1m[2023-07-11 16:25:21,175][233954] Mean Reward across all agents: 434.86028442086746[0m
[37m[1m[2023-07-11 16:25:21,176][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:25:26,200][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:25:26,200][233954] Reward + Measures: [[373.29070929   0.1873       0.70430005   0.49070001   0.78000003
    2.07181096]
 [413.07855912   0.0325       0.71349996   0.55559999   0.75240004
    2.11098862]
 [434.69447331   0.1653       0.79019994   0.46779999   0.78610003
    2.07365274]
 ...
 [392.31453727   0.0867       0.66779995   0.53930002   0.73740005
    2.24158359]
 [322.57367039   0.28690001   0.93590003   0.4501       0.90240002
    2.1383779 ]
 [476.34797918   0.10399999   0.72399998   0.63630003   0.84839994
    2.27900124]][0m
[37m[1m[2023-07-11 16:25:26,200][233954] Max Reward on eval: 625.9674453575165[0m
[37m[1m[2023-07-11 16:25:26,201][233954] Min Reward on eval: -40.49321559784003[0m
[37m[1m[2023-07-11 16:25:26,201][233954] Mean Reward across all agents: 388.2036364203332[0m
[37m[1m[2023-07-11 16:25:26,201][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:25:26,205][233954] mean_value=-47.956624905568326, max_value=301.1247475708796[0m
[37m[1m[2023-07-11 16:25:26,208][233954] New mean coefficients: [[ 0.5726073  -1.0001576  -2.0864444  -0.15371153 -0.75724006  0.99833703]][0m
[37m[1m[2023-07-11 16:25:26,209][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:25:35,207][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 16:25:35,207][233954] FPS: 426822.93[0m
[36m[2023-07-11 16:25:35,210][233954] itr=1196, itrs=2000, Progress: 59.80%[0m
[36m[2023-07-11 16:25:46,822][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 16:25:46,822][233954] FPS: 333412.84[0m
[36m[2023-07-11 16:25:51,085][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:25:51,086][233954] Reward + Measures: [[412.5220669    0.12428933   0.75713706   0.58644533   0.81535196
    2.12233543]][0m
[37m[1m[2023-07-11 16:25:51,086][233954] Max Reward on eval: 412.52206690002396[0m
[37m[1m[2023-07-11 16:25:51,086][233954] Min Reward on eval: 412.52206690002396[0m
[37m[1m[2023-07-11 16:25:51,087][233954] Mean Reward across all agents: 412.52206690002396[0m
[37m[1m[2023-07-11 16:25:51,087][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:25:56,111][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:25:56,112][233954] Reward + Measures: [[506.29746963   0.1902       0.89169997   0.60409999   0.91839999
    2.19854212]
 [419.43365863   0.10100001   0.88700008   0.67769998   0.90819997
    2.19406056]
 [393.26330089   0.1069       0.71579999   0.56800002   0.76950002
    2.15421867]
 ...
 [449.14940622   0.1516       0.82800001   0.5316       0.83230001
    2.22909617]
 [358.7324314    0.25320002   0.87169999   0.63090003   0.92889994
    2.2419095 ]
 [472.41058734   0.0213       0.98220009   0.5661       0.98110002
    2.55891728]][0m
[37m[1m[2023-07-11 16:25:56,112][233954] Max Reward on eval: 709.3328323366819[0m
[37m[1m[2023-07-11 16:25:56,112][233954] Min Reward on eval: -175.3476504876744[0m
[37m[1m[2023-07-11 16:25:56,112][233954] Mean Reward across all agents: 361.8512969675293[0m
[37m[1m[2023-07-11 16:25:56,112][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:25:56,116][233954] mean_value=-79.85959597426064, max_value=451.53665577744226[0m
[37m[1m[2023-07-11 16:25:56,119][233954] New mean coefficients: [[ 0.834448   -0.36141717 -2.326404   -0.17022502 -0.5545111   1.2633909 ]][0m
[37m[1m[2023-07-11 16:25:56,120][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:26:05,212][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 16:26:05,212][233954] FPS: 422421.40[0m
[36m[2023-07-11 16:26:05,214][233954] itr=1197, itrs=2000, Progress: 59.85%[0m
[36m[2023-07-11 16:26:16,930][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 16:26:16,930][233954] FPS: 330506.51[0m
[36m[2023-07-11 16:26:21,180][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:26:21,180][233954] Reward + Measures: [[418.20574709   0.11351666   0.73570001   0.58875132   0.80311263
    2.17811608]][0m
[37m[1m[2023-07-11 16:26:21,180][233954] Max Reward on eval: 418.2057470894965[0m
[37m[1m[2023-07-11 16:26:21,181][233954] Min Reward on eval: 418.2057470894965[0m
[37m[1m[2023-07-11 16:26:21,181][233954] Mean Reward across all agents: 418.2057470894965[0m
[37m[1m[2023-07-11 16:26:21,181][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:26:26,476][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:26:26,476][233954] Reward + Measures: [[251.088281     0.1346       0.47569999   0.26760003   0.52710003
    1.88464582]
 [424.90854743   0.0549       0.63210005   0.52940005   0.68410003
    2.1498816 ]
 [315.67158834   0.1214       0.66549999   0.4206       0.71160001
    2.02719188]
 ...
 [104.46150778   0.15200001   0.49909997   0.25559998   0.47010002
    1.76669443]
 [407.80922435   0.0983       0.70340008   0.54080003   0.76719999
    2.16441894]
 [547.97004223   0.17460001   0.79749995   0.69259995   0.89650005
    2.52680016]][0m
[37m[1m[2023-07-11 16:26:26,476][233954] Max Reward on eval: 609.5744619520847[0m
[37m[1m[2023-07-11 16:26:26,477][233954] Min Reward on eval: -16.377795525826514[0m
[37m[1m[2023-07-11 16:26:26,477][233954] Mean Reward across all agents: 320.84540382632616[0m
[37m[1m[2023-07-11 16:26:26,477][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:26:26,482][233954] mean_value=-59.20339816756429, max_value=509.60935893020826[0m
[37m[1m[2023-07-11 16:26:26,485][233954] New mean coefficients: [[ 0.50878775 -1.1658089  -2.0912485  -0.3564739  -0.06691998  0.68233824]][0m
[37m[1m[2023-07-11 16:26:26,486][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:26:35,534][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 16:26:35,534][233954] FPS: 424467.49[0m
[36m[2023-07-11 16:26:35,537][233954] itr=1198, itrs=2000, Progress: 59.90%[0m
[36m[2023-07-11 16:26:47,390][233954] train() took 11.75 seconds to complete[0m
[36m[2023-07-11 16:26:47,390][233954] FPS: 326719.41[0m
[36m[2023-07-11 16:26:51,606][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:26:51,607][233954] Reward + Measures: [[412.46855279   0.11978599   0.69290459   0.56522202   0.77765471
    2.17212129]][0m
[37m[1m[2023-07-11 16:26:51,607][233954] Max Reward on eval: 412.4685527901316[0m
[37m[1m[2023-07-11 16:26:51,607][233954] Min Reward on eval: 412.4685527901316[0m
[37m[1m[2023-07-11 16:26:51,608][233954] Mean Reward across all agents: 412.4685527901316[0m
[37m[1m[2023-07-11 16:26:51,608][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:26:56,502][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:26:56,502][233954] Reward + Measures: [[223.99177915   0.51109999   0.36180001   0.40410003   0.65380001
    2.25350952]
 [411.06062365   0.09640001   0.62980002   0.53299999   0.71990007
    2.51486707]
 [ 98.99050184   0.0839       0.2852       0.24930003   0.419
    2.23584342]
 ...
 [189.14791629   0.46350002   0.41430002   0.14410001   0.56150001
    1.83523333]
 [481.21043981   0.2458       0.62690002   0.57670003   0.76170003
    2.21910477]
 [108.05397102   0.33370003   0.55189997   0.32440001   0.67990005
    2.42528152]][0m
[37m[1m[2023-07-11 16:26:56,502][233954] Max Reward on eval: 623.3915748575121[0m
[37m[1m[2023-07-11 16:26:56,503][233954] Min Reward on eval: -113.81887265443802[0m
[37m[1m[2023-07-11 16:26:56,503][233954] Mean Reward across all agents: 234.7294607509365[0m
[37m[1m[2023-07-11 16:26:56,503][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:26:56,509][233954] mean_value=-46.84925907313396, max_value=729.6536987234489[0m
[37m[1m[2023-07-11 16:26:56,512][233954] New mean coefficients: [[ 0.23902556 -1.8245571  -1.7011619  -0.81740934 -0.37619156  0.5111482 ]][0m
[37m[1m[2023-07-11 16:26:56,513][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:27:05,509][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 16:27:05,509][233954] FPS: 426926.14[0m
[36m[2023-07-11 16:27:05,512][233954] itr=1199, itrs=2000, Progress: 59.95%[0m
[36m[2023-07-11 16:27:17,078][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 16:27:17,078][233954] FPS: 334831.87[0m
[36m[2023-07-11 16:27:21,357][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:27:21,357][233954] Reward + Measures: [[416.10764579   0.09939233   0.65600431   0.54046232   0.7576893
    2.19367218]][0m
[37m[1m[2023-07-11 16:27:21,357][233954] Max Reward on eval: 416.10764579407714[0m
[37m[1m[2023-07-11 16:27:21,358][233954] Min Reward on eval: 416.10764579407714[0m
[37m[1m[2023-07-11 16:27:21,358][233954] Mean Reward across all agents: 416.10764579407714[0m
[37m[1m[2023-07-11 16:27:21,358][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:27:26,311][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:27:26,312][233954] Reward + Measures: [[ 70.3403627    0.2124       0.0849       0.21720003   0.31759998
    1.79777658]
 [318.36534287   0.032        0.62190002   0.43710002   0.6649
    2.06093287]
 [396.11809383   0.0281       0.53460002   0.43120003   0.58700007
    2.10761809]
 ...
 [469.58258303   0.1046       0.6512       0.55809999   0.72220004
    2.086725  ]
 [220.75766529   0.1261       0.47009999   0.32390001   0.52039999
    1.77619994]
 [498.39093831   0.0135       0.8136       0.6372       0.8427
    2.39494634]][0m
[37m[1m[2023-07-11 16:27:26,312][233954] Max Reward on eval: 728.9363784920831[0m
[37m[1m[2023-07-11 16:27:26,312][233954] Min Reward on eval: 70.34036270412616[0m
[37m[1m[2023-07-11 16:27:26,313][233954] Mean Reward across all agents: 366.51849532162294[0m
[37m[1m[2023-07-11 16:27:26,313][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:27:26,317][233954] mean_value=-30.984298443850598, max_value=529.3835003334997[0m
[37m[1m[2023-07-11 16:27:26,320][233954] New mean coefficients: [[ 0.349193   -1.8825818  -1.4878639  -1.1173213  -0.4578054   0.44081116]][0m
[37m[1m[2023-07-11 16:27:26,321][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:27:35,335][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 16:27:35,335][233954] FPS: 426098.34[0m
[36m[2023-07-11 16:27:35,337][233954] itr=1200, itrs=2000, Progress: 60.00%[0m
[37m[1m[2023-07-11 16:31:10,812][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001180[0m
[36m[2023-07-11 16:31:23,133][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 16:31:23,133][233954] FPS: 332159.28[0m
[36m[2023-07-11 16:31:27,345][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:31:27,346][233954] Reward + Measures: [[395.77621481   0.10266066   0.60607696   0.50017929   0.71388298
    2.17113137]][0m
[37m[1m[2023-07-11 16:31:27,346][233954] Max Reward on eval: 395.7762148062189[0m
[37m[1m[2023-07-11 16:31:27,346][233954] Min Reward on eval: 395.7762148062189[0m
[37m[1m[2023-07-11 16:31:27,346][233954] Mean Reward across all agents: 395.7762148062189[0m
[37m[1m[2023-07-11 16:31:27,347][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:31:32,293][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:31:32,294][233954] Reward + Measures: [[412.86297471   0.42869997   0.74650002   0.55379999   0.91360009
    2.20389557]
 [235.19791654   0.19159999   0.34530002   0.36230001   0.5341
    1.79410207]
 [209.88122261   0.24080001   0.34830001   0.37470001   0.55900002
    1.90585923]
 ...
 [188.84718758   0.29050002   0.40570003   0.40600005   0.55520004
    1.96478045]
 [331.37324329   0.1133       0.54220003   0.34910002   0.59460002
    2.0277586 ]
 [245.05427823   0.30200002   0.47310001   0.4508       0.67850006
    2.02083325]][0m
[37m[1m[2023-07-11 16:31:32,294][233954] Max Reward on eval: 566.1437735559302[0m
[37m[1m[2023-07-11 16:31:32,294][233954] Min Reward on eval: 6.636425244715065[0m
[37m[1m[2023-07-11 16:31:32,295][233954] Mean Reward across all agents: 285.93509907773574[0m
[37m[1m[2023-07-11 16:31:32,295][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:31:32,299][233954] mean_value=-40.971946463361476, max_value=573.469488321811[0m
[37m[1m[2023-07-11 16:31:32,302][233954] New mean coefficients: [[ 0.5437337  -1.7224342  -1.6783783  -1.3746905  -0.28062466  0.6601385 ]][0m
[37m[1m[2023-07-11 16:31:32,303][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:31:41,318][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 16:31:41,318][233954] FPS: 426055.49[0m
[36m[2023-07-11 16:31:41,320][233954] itr=1201, itrs=2000, Progress: 60.05%[0m
[36m[2023-07-11 16:31:53,013][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 16:31:53,014][233954] FPS: 331239.65[0m
[36m[2023-07-11 16:31:57,386][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:31:57,386][233954] Reward + Measures: [[378.49257804   0.11351799   0.51237363   0.40137732   0.64810801
    2.09474921]][0m
[37m[1m[2023-07-11 16:31:57,386][233954] Max Reward on eval: 378.4925780426342[0m
[37m[1m[2023-07-11 16:31:57,387][233954] Min Reward on eval: 378.4925780426342[0m
[37m[1m[2023-07-11 16:31:57,387][233954] Mean Reward across all agents: 378.4925780426342[0m
[37m[1m[2023-07-11 16:31:57,387][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:32:02,357][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:32:02,357][233954] Reward + Measures: [[252.20716336   0.19500001   0.59230006   0.52820003   0.65319997
    1.95470738]
 [252.07711519   0.27550003   0.63599998   0.58740002   0.72400004
    2.06496286]
 [236.20172699   0.35789999   0.22000001   0.40109998   0.48699999
    1.79357851]
 ...
 [193.60122104   0.07030001   0.24929996   0.19789998   0.37649998
    1.77216649]
 [156.7966772    0.18010001   0.1148       0.19359998   0.2922
    1.77462101]
 [181.17977165   0.1164       0.26130003   0.2685       0.36559999
    1.81723869]][0m
[37m[1m[2023-07-11 16:32:02,358][233954] Max Reward on eval: 613.4260320409667[0m
[37m[1m[2023-07-11 16:32:02,358][233954] Min Reward on eval: 0.15250946749001743[0m
[37m[1m[2023-07-11 16:32:02,358][233954] Mean Reward across all agents: 288.8107318799402[0m
[37m[1m[2023-07-11 16:32:02,358][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:32:02,364][233954] mean_value=-44.75985380552929, max_value=876.5137012805969[0m
[37m[1m[2023-07-11 16:32:02,367][233954] New mean coefficients: [[ 0.325593   -2.164987   -1.802092   -1.3803918  -0.18150833  0.9637809 ]][0m
[37m[1m[2023-07-11 16:32:02,367][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:32:11,421][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 16:32:11,421][233954] FPS: 424219.63[0m
[36m[2023-07-11 16:32:11,424][233954] itr=1202, itrs=2000, Progress: 60.10%[0m
[36m[2023-07-11 16:32:23,185][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 16:32:23,185][233954] FPS: 329175.60[0m
[36m[2023-07-11 16:32:27,455][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:32:27,455][233954] Reward + Measures: [[268.34500871   0.14483666   0.3279193    0.28577632   0.50605166
    1.95559323]][0m
[37m[1m[2023-07-11 16:32:27,455][233954] Max Reward on eval: 268.3450087128125[0m
[37m[1m[2023-07-11 16:32:27,456][233954] Min Reward on eval: 268.3450087128125[0m
[37m[1m[2023-07-11 16:32:27,456][233954] Mean Reward across all agents: 268.3450087128125[0m
[37m[1m[2023-07-11 16:32:27,456][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:32:32,781][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:32:32,782][233954] Reward + Measures: [[218.37211466   0.27470002   0.2608       0.3581       0.56569999
    1.76683867]
 [235.70138224   0.1779       0.3856       0.3123       0.498
    1.87297475]
 [193.96366179   0.133        0.27579999   0.2376       0.40430003
    1.7048651 ]
 ...
 [275.61580193   0.2244       0.37740001   0.39630002   0.64779997
    2.07335591]
 [388.36565946   0.31279999   0.4594       0.55470002   0.70949996
    2.10287452]
 [316.84001258   0.0586       0.70300001   0.53079998   0.74909991
    2.08325958]][0m
[37m[1m[2023-07-11 16:32:32,782][233954] Max Reward on eval: 491.9511942942801[0m
[37m[1m[2023-07-11 16:32:32,783][233954] Min Reward on eval: -89.35811665444635[0m
[37m[1m[2023-07-11 16:32:32,783][233954] Mean Reward across all agents: 250.4152037029201[0m
[37m[1m[2023-07-11 16:32:32,783][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:32:32,787][233954] mean_value=-48.89237350825801, max_value=748.5550827890053[0m
[37m[1m[2023-07-11 16:32:32,790][233954] New mean coefficients: [[-0.12418821 -2.338772   -1.1652935  -0.60229576  0.34177157  1.1203408 ]][0m
[37m[1m[2023-07-11 16:32:32,791][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:32:41,822][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 16:32:41,823][233954] FPS: 425251.41[0m
[36m[2023-07-11 16:32:41,825][233954] itr=1203, itrs=2000, Progress: 60.15%[0m
[36m[2023-07-11 16:32:53,487][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 16:32:53,488][233954] FPS: 331988.06[0m
[36m[2023-07-11 16:32:57,705][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:32:57,705][233954] Reward + Measures: [[305.98696734   0.13091567   0.36673966   0.32625064   0.52647901
    2.00549674]][0m
[37m[1m[2023-07-11 16:32:57,706][233954] Max Reward on eval: 305.9869673379806[0m
[37m[1m[2023-07-11 16:32:57,706][233954] Min Reward on eval: 305.9869673379806[0m
[37m[1m[2023-07-11 16:32:57,706][233954] Mean Reward across all agents: 305.9869673379806[0m
[37m[1m[2023-07-11 16:32:57,706][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:33:02,611][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:33:02,612][233954] Reward + Measures: [[417.24893476   0.0654       0.75139999   0.54159999   0.77060002
    2.21889639]
 [228.69087363   0.1892       0.21140002   0.2692       0.39879999
    1.84919584]
 [328.88194634   0.1349       0.42570001   0.36849999   0.57369995
    2.01467824]
 ...
 [217.40465343   0.06960001   0.30669999   0.25330001   0.40380001
    1.79423845]
 [315.98904801   0.0902       0.73890001   0.54110003   0.75960004
    2.15166783]
 [458.45882179   0.09370001   0.70420003   0.54790002   0.77800006
    2.31843114]][0m
[37m[1m[2023-07-11 16:33:02,612][233954] Max Reward on eval: 537.2175239591976[0m
[37m[1m[2023-07-11 16:33:02,612][233954] Min Reward on eval: 17.6241613952443[0m
[37m[1m[2023-07-11 16:33:02,613][233954] Mean Reward across all agents: 296.620338590328[0m
[37m[1m[2023-07-11 16:33:02,613][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:33:02,617][233954] mean_value=-61.71292355738419, max_value=642.1356373507011[0m
[37m[1m[2023-07-11 16:33:02,619][233954] New mean coefficients: [[-0.14539781 -2.513368   -1.5024225  -0.7153481   0.10624495  1.1415012 ]][0m
[37m[1m[2023-07-11 16:33:02,620][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:33:11,552][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 16:33:11,553][233954] FPS: 429997.35[0m
[36m[2023-07-11 16:33:11,555][233954] itr=1204, itrs=2000, Progress: 60.20%[0m
[36m[2023-07-11 16:33:23,367][233954] train() took 11.72 seconds to complete[0m
[36m[2023-07-11 16:33:23,367][233954] FPS: 327732.41[0m
[36m[2023-07-11 16:33:27,645][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:33:27,645][233954] Reward + Measures: [[199.36462711   0.13931733   0.23008066   0.217291     0.43261304
    1.88270116]][0m
[37m[1m[2023-07-11 16:33:27,646][233954] Max Reward on eval: 199.3646271080228[0m
[37m[1m[2023-07-11 16:33:27,646][233954] Min Reward on eval: 199.3646271080228[0m
[37m[1m[2023-07-11 16:33:27,646][233954] Mean Reward across all agents: 199.3646271080228[0m
[37m[1m[2023-07-11 16:33:27,646][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:33:32,657][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:33:32,657][233954] Reward + Measures: [[396.98486199   0.26920003   0.43210003   0.57260001   0.68449998
    2.18141341]
 [ 42.68739274   0.26519999   0.1234       0.26190001   0.34630004
    1.59810412]
 [231.1508268    0.1035       0.34800002   0.30790001   0.44870001
    1.87888205]
 ...
 [275.55461218   0.09890001   0.44840002   0.36100003   0.55470002
    2.054919  ]
 [106.59040327   0.1601       0.09249999   0.1603       0.28640002
    1.64967954]
 [ 92.6422573    0.06480001   0.0882       0.085        0.23760001
    1.75678086]][0m
[37m[1m[2023-07-11 16:33:32,658][233954] Max Reward on eval: 571.3113422327675[0m
[37m[1m[2023-07-11 16:33:32,658][233954] Min Reward on eval: 7.300983909750357[0m
[37m[1m[2023-07-11 16:33:32,658][233954] Mean Reward across all agents: 236.59018839113565[0m
[37m[1m[2023-07-11 16:33:32,658][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:33:32,663][233954] mean_value=-65.93828345357991, max_value=812.652234628672[0m
[37m[1m[2023-07-11 16:33:32,666][233954] New mean coefficients: [[-0.15663813 -2.286694   -1.9795754  -0.47511    -0.16655725  1.1022427 ]][0m
[37m[1m[2023-07-11 16:33:32,667][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:33:41,697][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 16:33:41,697][233954] FPS: 425331.99[0m
[36m[2023-07-11 16:33:41,699][233954] itr=1205, itrs=2000, Progress: 60.25%[0m
[36m[2023-07-11 16:33:53,576][233954] train() took 11.78 seconds to complete[0m
[36m[2023-07-11 16:33:53,576][233954] FPS: 325980.73[0m
[36m[2023-07-11 16:33:57,858][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:33:57,859][233954] Reward + Measures: [[178.43229151   0.13258199   0.18422399   0.19207267   0.41067597
    1.89070117]][0m
[37m[1m[2023-07-11 16:33:57,859][233954] Max Reward on eval: 178.4322915132529[0m
[37m[1m[2023-07-11 16:33:57,859][233954] Min Reward on eval: 178.4322915132529[0m
[37m[1m[2023-07-11 16:33:57,860][233954] Mean Reward across all agents: 178.4322915132529[0m
[37m[1m[2023-07-11 16:33:57,860][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:34:02,837][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:34:02,837][233954] Reward + Measures: [[219.02716379   0.26460001   0.22870003   0.32449999   0.49890003
    1.85453224]
 [128.54032424   0.27180001   0.32440004   0.33329999   0.4585
    1.63232255]
 [112.17057313   0.15580001   0.19560002   0.21490002   0.3637
    1.61334157]
 ...
 [ 92.43807122   0.31950003   0.32170001   0.30739999   0.329
    1.79894793]
 [271.64721479   0.2536       0.56910002   0.21379998   0.59670001
    2.38293433]
 [ 87.41210487   0.21340001   0.38830003   0.21399999   0.3184
    1.66193235]][0m
[37m[1m[2023-07-11 16:34:02,837][233954] Max Reward on eval: 589.3461132012774[0m
[37m[1m[2023-07-11 16:34:02,838][233954] Min Reward on eval: -73.4642479534261[0m
[37m[1m[2023-07-11 16:34:02,838][233954] Mean Reward across all agents: 178.60416778966552[0m
[37m[1m[2023-07-11 16:34:02,838][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:34:02,842][233954] mean_value=-261.6564325962607, max_value=613.8989876083859[0m
[37m[1m[2023-07-11 16:34:02,845][233954] New mean coefficients: [[-1.0384694  -2.6498923  -1.8463534  -0.5408516   0.01985142  0.87422717]][0m
[37m[1m[2023-07-11 16:34:02,846][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:34:11,873][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 16:34:11,873][233954] FPS: 425462.43[0m
[36m[2023-07-11 16:34:11,875][233954] itr=1206, itrs=2000, Progress: 60.30%[0m
[36m[2023-07-11 16:34:23,601][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 16:34:23,601][233954] FPS: 330136.87[0m
[36m[2023-07-11 16:34:27,960][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:34:27,961][233954] Reward + Measures: [[138.90227159   0.13691565   0.138577     0.17463534   0.370637
    1.89392507]][0m
[37m[1m[2023-07-11 16:34:27,961][233954] Max Reward on eval: 138.90227159000736[0m
[37m[1m[2023-07-11 16:34:27,961][233954] Min Reward on eval: 138.90227159000736[0m
[37m[1m[2023-07-11 16:34:27,962][233954] Mean Reward across all agents: 138.90227159000736[0m
[37m[1m[2023-07-11 16:34:27,962][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:34:32,912][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:34:32,912][233954] Reward + Measures: [[ 57.68444849   0.0623       0.1346       0.0933       0.21499999
    1.54503858]
 [178.78645183   0.0603       0.2595       0.19490002   0.33399999
    1.612064  ]
 [ 54.86818822   0.1199       0.1349       0.1578       0.26980001
    1.7702862 ]
 ...
 [309.3041123    0.1063       0.60430002   0.4154       0.66289997
    2.28526092]
 [311.03580246   0.0372       0.44309998   0.33580002   0.4887
    1.97459638]
 [ 54.14767892   0.2155       0.2289       0.23239999   0.3193
    1.71041358]][0m
[37m[1m[2023-07-11 16:34:32,913][233954] Max Reward on eval: 572.8468618703075[0m
[37m[1m[2023-07-11 16:34:32,913][233954] Min Reward on eval: 0.6331775216152892[0m
[37m[1m[2023-07-11 16:34:32,913][233954] Mean Reward across all agents: 192.50825807429942[0m
[37m[1m[2023-07-11 16:34:32,913][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:34:32,916][233954] mean_value=-205.71811795103815, max_value=502.5812878189768[0m
[37m[1m[2023-07-11 16:34:32,919][233954] New mean coefficients: [[-0.7637781  -1.8810757  -0.76465666 -0.07182151  0.35844252  1.6479622 ]][0m
[37m[1m[2023-07-11 16:34:32,920][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:34:41,939][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 16:34:41,939][233954] FPS: 425857.48[0m
[36m[2023-07-11 16:34:41,941][233954] itr=1207, itrs=2000, Progress: 60.35%[0m
[36m[2023-07-11 16:34:53,657][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 16:34:53,657][233954] FPS: 330488.55[0m
[36m[2023-07-11 16:34:57,905][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:34:57,906][233954] Reward + Measures: [[145.248953     0.13947698   0.14609466   0.18323201   0.39413166
    1.92636836]][0m
[37m[1m[2023-07-11 16:34:57,906][233954] Max Reward on eval: 145.24895300444925[0m
[37m[1m[2023-07-11 16:34:57,906][233954] Min Reward on eval: 145.24895300444925[0m
[37m[1m[2023-07-11 16:34:57,906][233954] Mean Reward across all agents: 145.24895300444925[0m
[37m[1m[2023-07-11 16:34:57,907][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:35:03,217][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:35:03,218][233954] Reward + Measures: [[358.24070116   0.1288       0.47779998   0.37459999   0.67840004
    2.1205337 ]
 [ 21.9739934    0.0645       0.07720001   0.09600001   0.26859999
    2.07483959]
 [192.54832409   0.13100001   0.22309999   0.2511       0.40980002
    2.05054331]
 ...
 [318.26574934   0.0526       0.47849998   0.3046       0.56770003
    1.9975456 ]
 [292.54153252   0.1086       0.26179999   0.27420002   0.51609999
    2.00588727]
 [175.76836919   0.056        0.2493       0.17760001   0.43090001
    1.95929801]][0m
[37m[1m[2023-07-11 16:35:03,218][233954] Max Reward on eval: 571.5050930930768[0m
[37m[1m[2023-07-11 16:35:03,218][233954] Min Reward on eval: 3.3369056953815743[0m
[37m[1m[2023-07-11 16:35:03,219][233954] Mean Reward across all agents: 193.42607343426764[0m
[37m[1m[2023-07-11 16:35:03,219][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:35:03,223][233954] mean_value=-59.271731095527294, max_value=715.7404267973034[0m
[37m[1m[2023-07-11 16:35:03,226][233954] New mean coefficients: [[-0.5553152  -1.6275041  -1.1543963   0.40773958  0.8340367   1.4879795 ]][0m
[37m[1m[2023-07-11 16:35:03,227][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:35:12,267][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 16:35:12,267][233954] FPS: 424861.19[0m
[36m[2023-07-11 16:35:12,270][233954] itr=1208, itrs=2000, Progress: 60.40%[0m
[36m[2023-07-11 16:35:23,911][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 16:35:23,912][233954] FPS: 332655.81[0m
[36m[2023-07-11 16:35:28,100][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:35:28,100][233954] Reward + Measures: [[175.20932321   0.12438466   0.228045     0.20255531   0.45332736
    1.99172473]][0m
[37m[1m[2023-07-11 16:35:28,101][233954] Max Reward on eval: 175.2093232146008[0m
[37m[1m[2023-07-11 16:35:28,101][233954] Min Reward on eval: 175.2093232146008[0m
[37m[1m[2023-07-11 16:35:28,101][233954] Mean Reward across all agents: 175.2093232146008[0m
[37m[1m[2023-07-11 16:35:28,102][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:35:33,122][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:35:33,128][233954] Reward + Measures: [[146.37061776   0.0557       0.17420003   0.1402       0.37330002
    1.96132112]
 [248.33750744   0.15890001   0.38690004   0.3436       0.53890002
    2.0203433 ]
 [229.99064088   0.44240004   0.2059       0.50569999   0.5668
    1.85512924]
 ...
 [371.17322468   0.16340001   0.55560005   0.4526       0.73540002
    2.16464162]
 [139.37638032   0.0587       0.17400001   0.1182       0.33989999
    1.82217205]
 [250.52543019   0.28440002   0.13150001   0.33630002   0.48800001
    1.86392391]][0m
[37m[1m[2023-07-11 16:35:33,128][233954] Max Reward on eval: 483.2442634720821[0m
[37m[1m[2023-07-11 16:35:33,128][233954] Min Reward on eval: 26.608120437385516[0m
[37m[1m[2023-07-11 16:35:33,129][233954] Mean Reward across all agents: 209.72458998841805[0m
[37m[1m[2023-07-11 16:35:33,129][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:35:33,133][233954] mean_value=-93.92341960042131, max_value=767.9526910901419[0m
[37m[1m[2023-07-11 16:35:33,135][233954] New mean coefficients: [[-0.2987774 -1.1117563 -1.0658021  1.0669664  1.0529788  1.1557697]][0m
[37m[1m[2023-07-11 16:35:33,136][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:35:42,228][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 16:35:42,228][233954] FPS: 422458.24[0m
[36m[2023-07-11 16:35:42,230][233954] itr=1209, itrs=2000, Progress: 60.45%[0m
[36m[2023-07-11 16:35:53,934][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 16:35:53,934][233954] FPS: 330910.30[0m
[36m[2023-07-11 16:35:58,316][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:35:58,316][233954] Reward + Measures: [[363.72963562   0.07557534   0.52477199   0.37335464   0.68090296
    2.20323849]][0m
[37m[1m[2023-07-11 16:35:58,316][233954] Max Reward on eval: 363.72963561728346[0m
[37m[1m[2023-07-11 16:35:58,317][233954] Min Reward on eval: 363.72963561728346[0m
[37m[1m[2023-07-11 16:35:58,317][233954] Mean Reward across all agents: 363.72963561728346[0m
[37m[1m[2023-07-11 16:35:58,317][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:36:03,342][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:36:03,343][233954] Reward + Measures: [[221.17170458   0.0603       0.36569998   0.29070002   0.55120003
    2.05334425]
 [272.11177637   0.1701       0.40490004   0.40149999   0.60100001
    2.10230899]
 [225.81068702   0.19990002   0.47499999   0.4612       0.57780004
    1.97526479]
 ...
 [324.00904654   0.11850001   0.69410008   0.52790004   0.70160002
    2.10161448]
 [450.35861729   0.0253       0.6261       0.41830006   0.7274
    2.24758196]
 [175.75782916   0.1816       0.2868       0.30650002   0.54689997
    2.18820739]][0m
[37m[1m[2023-07-11 16:36:03,343][233954] Max Reward on eval: 577.420189388562[0m
[37m[1m[2023-07-11 16:36:03,344][233954] Min Reward on eval: 15.611863344092853[0m
[37m[1m[2023-07-11 16:36:03,344][233954] Mean Reward across all agents: 243.57690032129912[0m
[37m[1m[2023-07-11 16:36:03,344][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:36:03,347][233954] mean_value=-79.1891429747338, max_value=688.4673317558982[0m
[37m[1m[2023-07-11 16:36:03,350][233954] New mean coefficients: [[ 0.48685235 -0.31233746 -0.15522814  0.81281924  1.7989717   1.6496865 ]][0m
[37m[1m[2023-07-11 16:36:03,351][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:36:12,438][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 16:36:12,438][233954] FPS: 422673.88[0m
[36m[2023-07-11 16:36:12,440][233954] itr=1210, itrs=2000, Progress: 60.50%[0m
[37m[1m[2023-07-11 16:39:47,633][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001190[0m
[36m[2023-07-11 16:39:59,984][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 16:39:59,985][233954] FPS: 331078.63[0m
[36m[2023-07-11 16:40:04,337][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:40:04,338][233954] Reward + Measures: [[395.27436707   0.08215466   0.54900539   0.40459302   0.70727199
    2.24118924]][0m
[37m[1m[2023-07-11 16:40:04,338][233954] Max Reward on eval: 395.2743670728146[0m
[37m[1m[2023-07-11 16:40:04,338][233954] Min Reward on eval: 395.2743670728146[0m
[37m[1m[2023-07-11 16:40:04,338][233954] Mean Reward across all agents: 395.2743670728146[0m
[37m[1m[2023-07-11 16:40:04,339][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:40:09,381][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:40:09,387][233954] Reward + Measures: [[233.27317561   0.046        0.40910003   0.23120001   0.56070006
    2.06292844]
 [361.12769007   0.0457       0.52749997   0.28450003   0.63630003
    2.21115375]
 [445.10891917   0.27150002   0.45550004   0.54930001   0.74539995
    2.22277069]
 ...
 [308.86559881   0.1833       0.35139999   0.34020001   0.51480001
    1.92187464]
 [259.81592393   0.1087       0.37080005   0.30140001   0.57370007
    2.13240552]
 [303.29636741   0.0429       0.57750005   0.36340004   0.64929998
    2.18149877]][0m
[37m[1m[2023-07-11 16:40:09,387][233954] Max Reward on eval: 612.2625026535825[0m
[37m[1m[2023-07-11 16:40:09,388][233954] Min Reward on eval: 58.4747226161242[0m
[37m[1m[2023-07-11 16:40:09,388][233954] Mean Reward across all agents: 276.98151840098416[0m
[37m[1m[2023-07-11 16:40:09,388][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:40:09,391][233954] mean_value=-65.38307558307292, max_value=840.3507302840055[0m
[37m[1m[2023-07-11 16:40:09,394][233954] New mean coefficients: [[ 0.5538659  -0.05593616 -0.35600442  1.1536248   1.875795    1.4505304 ]][0m
[37m[1m[2023-07-11 16:40:09,395][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:40:18,450][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 16:40:18,451][233954] FPS: 424123.61[0m
[36m[2023-07-11 16:40:18,453][233954] itr=1211, itrs=2000, Progress: 60.55%[0m
[36m[2023-07-11 16:40:30,224][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 16:40:30,224][233954] FPS: 328933.33[0m
[36m[2023-07-11 16:40:34,441][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:40:34,442][233954] Reward + Measures: [[363.92220189   0.07908767   0.56312466   0.43287969   0.6932866
    2.20039082]][0m
[37m[1m[2023-07-11 16:40:34,442][233954] Max Reward on eval: 363.9222018874471[0m
[37m[1m[2023-07-11 16:40:34,442][233954] Min Reward on eval: 363.9222018874471[0m
[37m[1m[2023-07-11 16:40:34,442][233954] Mean Reward across all agents: 363.9222018874471[0m
[37m[1m[2023-07-11 16:40:34,443][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:40:39,642][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:40:39,647][233954] Reward + Measures: [[415.96787529   0.23690002   0.59289998   0.64430004   0.76060003
    2.16559243]
 [452.55295158   0.11080001   0.61960006   0.54769999   0.77320004
    2.42855811]
 [330.25025485   0.0339       0.5456       0.40450001   0.6523
    2.30704713]
 ...
 [222.51070246   0.0469       0.40290004   0.21950002   0.58460003
    2.17793918]
 [220.89991257   0.1499       0.44770002   0.3969       0.58219999
    1.87826467]
 [246.53504419   0.0683       0.46689996   0.3527       0.55319995
    1.92140353]][0m
[37m[1m[2023-07-11 16:40:39,648][233954] Max Reward on eval: 682.7629547163612[0m
[37m[1m[2023-07-11 16:40:39,648][233954] Min Reward on eval: 44.34198787678033[0m
[37m[1m[2023-07-11 16:40:39,648][233954] Mean Reward across all agents: 318.0532267638832[0m
[37m[1m[2023-07-11 16:40:39,649][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:40:39,651][233954] mean_value=-77.89479499608946, max_value=676.5678371871157[0m
[37m[1m[2023-07-11 16:40:39,654][233954] New mean coefficients: [[ 0.21320763 -0.23464641 -0.88722634  0.76009023  2.0197082   0.791514  ]][0m
[37m[1m[2023-07-11 16:40:39,655][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:40:48,547][233954] train() took 8.89 seconds to complete[0m
[36m[2023-07-11 16:40:48,547][233954] FPS: 431924.92[0m
[36m[2023-07-11 16:40:48,550][233954] itr=1212, itrs=2000, Progress: 60.60%[0m
[36m[2023-07-11 16:41:00,133][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 16:41:00,134][233954] FPS: 334249.45[0m
[36m[2023-07-11 16:41:04,406][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:41:04,407][233954] Reward + Measures: [[366.76634239   0.07535566   0.59437364   0.45723367   0.72383159
    2.21602893]][0m
[37m[1m[2023-07-11 16:41:04,407][233954] Max Reward on eval: 366.7663423850005[0m
[37m[1m[2023-07-11 16:41:04,407][233954] Min Reward on eval: 366.7663423850005[0m
[37m[1m[2023-07-11 16:41:04,407][233954] Mean Reward across all agents: 366.7663423850005[0m
[37m[1m[2023-07-11 16:41:04,408][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:41:09,344][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:41:09,345][233954] Reward + Measures: [[512.72082043   0.0215       0.74539995   0.45320001   0.82339996
    2.39073133]
 [454.32200719   0.0861       0.51750004   0.39209998   0.64789999
    2.21543932]
 [623.50050736   0.0126       0.99090004   0.72659999   0.99169999
    2.91670108]
 ...
 [243.21086027   0.15339999   0.44910002   0.3177       0.5284
    2.13351202]
 [ 65.88952796   0.0506       0.16500001   0.12560001   0.40840003
    2.0684762 ]
 [  9.50999202   0.27959999   0.35620004   0.2254       0.39760002
    1.9235028 ]][0m
[37m[1m[2023-07-11 16:41:09,345][233954] Max Reward on eval: 664.1139564405196[0m
[37m[1m[2023-07-11 16:41:09,345][233954] Min Reward on eval: -2.1441345816478132[0m
[37m[1m[2023-07-11 16:41:09,346][233954] Mean Reward across all agents: 359.9848634704221[0m
[37m[1m[2023-07-11 16:41:09,346][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:41:09,349][233954] mean_value=-106.26417548517932, max_value=554.1800657174776[0m
[37m[1m[2023-07-11 16:41:09,352][233954] New mean coefficients: [[ 0.23701653 -0.5166195  -0.63047063  1.0164207   2.180212    0.92670196]][0m
[37m[1m[2023-07-11 16:41:09,352][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:41:18,270][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 16:41:18,271][233954] FPS: 430676.10[0m
[36m[2023-07-11 16:41:18,273][233954] itr=1213, itrs=2000, Progress: 60.65%[0m
[36m[2023-07-11 16:41:29,866][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 16:41:29,866][233954] FPS: 334051.18[0m
[36m[2023-07-11 16:41:34,144][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:41:34,144][233954] Reward + Measures: [[365.80886013   0.06973933   0.643857     0.47377229   0.75308031
    2.26469254]][0m
[37m[1m[2023-07-11 16:41:34,145][233954] Max Reward on eval: 365.8088601270693[0m
[37m[1m[2023-07-11 16:41:34,145][233954] Min Reward on eval: 365.8088601270693[0m
[37m[1m[2023-07-11 16:41:34,145][233954] Mean Reward across all agents: 365.8088601270693[0m
[37m[1m[2023-07-11 16:41:34,145][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:41:39,110][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:41:39,111][233954] Reward + Measures: [[218.1146941    0.10470001   0.40780002   0.33320001   0.58170003
    2.01745725]
 [200.02468634   0.0665       0.6910001    0.4316       0.73579997
    2.19555831]
 [494.05650142   0.0326       0.65020001   0.36620003   0.77019995
    2.2914083 ]
 ...
 [227.51786377   0.1446       0.76780003   0.42420003   0.8235001
    2.38855982]
 [280.20128538   0.0349       0.75040001   0.46650001   0.79649997
    2.19907331]
 [283.54452898   0.2016       0.41190001   0.1925       0.59869999
    2.16018939]][0m
[37m[1m[2023-07-11 16:41:39,111][233954] Max Reward on eval: 729.7216529994388[0m
[37m[1m[2023-07-11 16:41:39,111][233954] Min Reward on eval: 83.56448400047375[0m
[37m[1m[2023-07-11 16:41:39,112][233954] Mean Reward across all agents: 314.93515126021526[0m
[37m[1m[2023-07-11 16:41:39,112][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:41:39,115][233954] mean_value=-105.48540092590969, max_value=575.7923423750826[0m
[37m[1m[2023-07-11 16:41:39,118][233954] New mean coefficients: [[ 0.06167148 -1.3275043  -0.64662325  0.6610503   1.8413006  -0.64188904]][0m
[37m[1m[2023-07-11 16:41:39,119][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:41:48,167][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 16:41:48,168][233954] FPS: 424437.30[0m
[36m[2023-07-11 16:41:48,170][233954] itr=1214, itrs=2000, Progress: 60.70%[0m
[36m[2023-07-11 16:41:59,947][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 16:41:59,947][233954] FPS: 328880.20[0m
[36m[2023-07-11 16:42:04,171][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:42:04,171][233954] Reward + Measures: [[390.31840981   0.05028567   0.69123036   0.49620301   0.78405058
    2.3073504 ]][0m
[37m[1m[2023-07-11 16:42:04,172][233954] Max Reward on eval: 390.3184098056589[0m
[37m[1m[2023-07-11 16:42:04,172][233954] Min Reward on eval: 390.3184098056589[0m
[37m[1m[2023-07-11 16:42:04,172][233954] Mean Reward across all agents: 390.3184098056589[0m
[37m[1m[2023-07-11 16:42:04,172][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:42:09,135][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:42:09,136][233954] Reward + Measures: [[176.2937746    0.09940001   0.50779998   0.33989999   0.50270003
    2.3867619 ]
 [165.20135805   0.1432       0.50510001   0.38690004   0.57080001
    2.2290051 ]
 [310.36289382   0.1926       0.54339999   0.46059999   0.61799997
    1.93336093]
 ...
 [246.60722112   0.1024       0.59030002   0.43689999   0.59289998
    2.48618126]
 [137.92595606   0.18230002   0.43779999   0.33849999   0.498
    2.31698251]
 [279.60428239   0.18880001   0.60710001   0.52179998   0.68809998
    2.38670039]][0m
[37m[1m[2023-07-11 16:42:09,136][233954] Max Reward on eval: 564.3805799365276[0m
[37m[1m[2023-07-11 16:42:09,137][233954] Min Reward on eval: -8.613105307403021[0m
[37m[1m[2023-07-11 16:42:09,137][233954] Mean Reward across all agents: 245.82246282597168[0m
[37m[1m[2023-07-11 16:42:09,137][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:42:09,139][233954] mean_value=-147.03143574243106, max_value=858.5084497915686[0m
[37m[1m[2023-07-11 16:42:09,142][233954] New mean coefficients: [[ 0.22128794 -0.3072517  -0.4907037   0.71540403  2.2745905  -0.78784955]][0m
[37m[1m[2023-07-11 16:42:09,143][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:42:18,128][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 16:42:18,128][233954] FPS: 427461.17[0m
[36m[2023-07-11 16:42:18,130][233954] itr=1215, itrs=2000, Progress: 60.75%[0m
[36m[2023-07-11 16:42:29,739][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 16:42:29,739][233954] FPS: 333545.38[0m
[36m[2023-07-11 16:42:34,024][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:42:34,025][233954] Reward + Measures: [[385.25392028   0.043881     0.75520867   0.54530406   0.82707828
    2.28694558]][0m
[37m[1m[2023-07-11 16:42:34,025][233954] Max Reward on eval: 385.2539202787787[0m
[37m[1m[2023-07-11 16:42:34,025][233954] Min Reward on eval: 385.2539202787787[0m
[37m[1m[2023-07-11 16:42:34,026][233954] Mean Reward across all agents: 385.2539202787787[0m
[37m[1m[2023-07-11 16:42:34,026][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:42:39,028][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:42:39,028][233954] Reward + Measures: [[296.97485566   0.0615       0.76990002   0.46810004   0.77329999
    2.20102286]
 [556.57055707   0.0165       0.71060002   0.50889999   0.84370005
    2.38485527]
 [232.00591344   0.08450001   0.54829997   0.36250001   0.62249994
    2.10475397]
 ...
 [148.10481977   0.0974       0.52990001   0.35820001   0.60820001
    2.07900906]
 [199.85363149   0.1031       0.62070006   0.41209999   0.69859999
    2.1438272 ]
 [313.32050132   0.0286       0.56770009   0.37900001   0.71950001
    2.16021132]][0m
[37m[1m[2023-07-11 16:42:39,029][233954] Max Reward on eval: 658.438275099569[0m
[37m[1m[2023-07-11 16:42:39,029][233954] Min Reward on eval: 39.88812152105966[0m
[37m[1m[2023-07-11 16:42:39,029][233954] Mean Reward across all agents: 350.39934441855627[0m
[37m[1m[2023-07-11 16:42:39,029][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:42:39,032][233954] mean_value=-115.9361503285661, max_value=830.8736561240805[0m
[37m[1m[2023-07-11 16:42:39,035][233954] New mean coefficients: [[ 0.77289546 -1.0406661  -0.7574681   0.9792789   2.2301352  -1.7371159 ]][0m
[37m[1m[2023-07-11 16:42:39,036][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:42:48,124][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 16:42:48,124][233954] FPS: 422598.37[0m
[36m[2023-07-11 16:42:48,126][233954] itr=1216, itrs=2000, Progress: 60.80%[0m
[36m[2023-07-11 16:42:59,928][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 16:42:59,928][233954] FPS: 328124.14[0m
[36m[2023-07-11 16:43:04,226][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:43:04,226][233954] Reward + Measures: [[367.607985     0.042713     0.77468961   0.56821662   0.84876466
    2.26734185]][0m
[37m[1m[2023-07-11 16:43:04,226][233954] Max Reward on eval: 367.60798499770044[0m
[37m[1m[2023-07-11 16:43:04,227][233954] Min Reward on eval: 367.60798499770044[0m
[37m[1m[2023-07-11 16:43:04,227][233954] Mean Reward across all agents: 367.60798499770044[0m
[37m[1m[2023-07-11 16:43:04,227][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:43:09,493][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:43:09,494][233954] Reward + Measures: [[-13.28230703   0.58860004   0.79280001   0.55769998   0.74939996
    1.95458567]
 [453.51311301   0.0921       0.75239998   0.52200001   0.81510001
    2.49888158]
 [257.07043359   0.068        0.47350001   0.29450002   0.49610001
    2.20281672]
 ...
 [191.26982863   0.19179998   0.43009996   0.32119998   0.45790002
    1.5163641 ]
 [370.93679333   0.05449999   0.70699996   0.35929999   0.72369999
    2.36898112]
 [ 38.56557621   0.16550002   0.32089999   0.19939999   0.33989999
    1.46732545]][0m
[37m[1m[2023-07-11 16:43:09,494][233954] Max Reward on eval: 880.142234799359[0m
[37m[1m[2023-07-11 16:43:09,495][233954] Min Reward on eval: -118.87689871480688[0m
[37m[1m[2023-07-11 16:43:09,495][233954] Mean Reward across all agents: 370.41825410971455[0m
[37m[1m[2023-07-11 16:43:09,495][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:43:09,500][233954] mean_value=-106.2182708283069, max_value=269.33169704651846[0m
[37m[1m[2023-07-11 16:43:09,502][233954] New mean coefficients: [[ 0.99586153 -0.88731337 -0.7921439   1.7628865   2.3278527  -1.608531  ]][0m
[37m[1m[2023-07-11 16:43:09,503][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:43:18,475][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 16:43:18,475][233954] FPS: 428089.31[0m
[36m[2023-07-11 16:43:18,478][233954] itr=1217, itrs=2000, Progress: 60.85%[0m
[36m[2023-07-11 16:43:30,049][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 16:43:30,049][233954] FPS: 334648.01[0m
[36m[2023-07-11 16:43:34,241][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:43:34,242][233954] Reward + Measures: [[359.77173089   0.04363267   0.80842763   0.60856766   0.87654704
    2.22999334]][0m
[37m[1m[2023-07-11 16:43:34,242][233954] Max Reward on eval: 359.7717308881275[0m
[37m[1m[2023-07-11 16:43:34,242][233954] Min Reward on eval: 359.7717308881275[0m
[37m[1m[2023-07-11 16:43:34,243][233954] Mean Reward across all agents: 359.7717308881275[0m
[37m[1m[2023-07-11 16:43:34,243][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:43:39,206][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:43:39,207][233954] Reward + Measures: [[482.13280059   0.0752       0.74589998   0.57569999   0.85680008
    2.32431102]
 [382.39626743   0.08790001   0.81539994   0.64910001   0.88740009
    2.23975086]
 [373.23158647   0.0883       0.72719997   0.59290004   0.83100003
    2.22969818]
 ...
 [372.72048423   0.2402       0.4594       0.55240005   0.70899999
    2.14033318]
 [339.88657784   0.038        0.50440001   0.34419999   0.56240004
    1.99886596]
 [411.97948838   0.10399999   0.62199998   0.32370004   0.64790004
    2.07247043]][0m
[37m[1m[2023-07-11 16:43:39,207][233954] Max Reward on eval: 661.9794006367563[0m
[37m[1m[2023-07-11 16:43:39,207][233954] Min Reward on eval: 58.09374332065927[0m
[37m[1m[2023-07-11 16:43:39,207][233954] Mean Reward across all agents: 392.92280857601565[0m
[37m[1m[2023-07-11 16:43:39,208][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:43:39,210][233954] mean_value=-102.14730491907726, max_value=824.4852609594352[0m
[37m[1m[2023-07-11 16:43:39,213][233954] New mean coefficients: [[ 0.91252285 -1.284075   -0.5056261   1.6357437   2.306494   -1.4802164 ]][0m
[37m[1m[2023-07-11 16:43:39,214][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:43:48,268][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 16:43:48,268][233954] FPS: 424169.33[0m
[36m[2023-07-11 16:43:48,271][233954] itr=1218, itrs=2000, Progress: 60.90%[0m
[36m[2023-07-11 16:43:59,883][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 16:43:59,883][233954] FPS: 333594.79[0m
[36m[2023-07-11 16:44:04,164][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:44:04,164][233954] Reward + Measures: [[336.67967943   0.038173     0.84005696   0.62680328   0.88932502
    2.17915416]][0m
[37m[1m[2023-07-11 16:44:04,165][233954] Max Reward on eval: 336.6796794271579[0m
[37m[1m[2023-07-11 16:44:04,165][233954] Min Reward on eval: 336.6796794271579[0m
[37m[1m[2023-07-11 16:44:04,165][233954] Mean Reward across all agents: 336.6796794271579[0m
[37m[1m[2023-07-11 16:44:04,166][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:44:09,128][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:44:09,129][233954] Reward + Measures: [[383.0717556    0.0865       0.86070007   0.53549999   0.86949998
    1.95553684]
 [ 82.76998337   0.3026       0.62070006   0.39489999   0.65279996
    1.75975478]
 [474.10482216   0.0896       0.93500006   0.73719996   0.88959998
    2.14393783]
 ...
 [  5.87812986   0.76830006   0.94089997   0.78719997   0.93340009
    2.19501352]
 [179.53483821   0.1876       0.87699997   0.49530002   0.85190004
    1.94874084]
 [ 53.21409178   0.33110002   0.55559999   0.29500005   0.6117
    1.63394392]][0m
[37m[1m[2023-07-11 16:44:09,129][233954] Max Reward on eval: 615.708202342363[0m
[37m[1m[2023-07-11 16:44:09,130][233954] Min Reward on eval: -96.6476042509079[0m
[37m[1m[2023-07-11 16:44:09,130][233954] Mean Reward across all agents: 159.81341021020356[0m
[37m[1m[2023-07-11 16:44:09,130][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:44:09,133][233954] mean_value=-142.69224100899285, max_value=284.828805112657[0m
[37m[1m[2023-07-11 16:44:09,135][233954] New mean coefficients: [[ 1.0192232 -1.376431  -0.8980869  1.0001256  1.7880533 -1.6371036]][0m
[37m[1m[2023-07-11 16:44:09,136][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:44:18,159][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 16:44:18,159][233954] FPS: 425662.89[0m
[36m[2023-07-11 16:44:18,161][233954] itr=1219, itrs=2000, Progress: 60.95%[0m
[36m[2023-07-11 16:44:29,853][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 16:44:29,853][233954] FPS: 331210.78[0m
[36m[2023-07-11 16:44:34,128][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:44:34,128][233954] Reward + Measures: [[334.31470682   0.04109666   0.84612262   0.64353102   0.89276934
    2.14342666]][0m
[37m[1m[2023-07-11 16:44:34,129][233954] Max Reward on eval: 334.3147068226464[0m
[37m[1m[2023-07-11 16:44:34,129][233954] Min Reward on eval: 334.3147068226464[0m
[37m[1m[2023-07-11 16:44:34,129][233954] Mean Reward across all agents: 334.3147068226464[0m
[37m[1m[2023-07-11 16:44:34,129][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:44:39,121][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:44:39,122][233954] Reward + Measures: [[369.7312279    0.0142       0.98680001   0.65079999   0.97060007
    2.25198054]
 [339.95810321   0.0257       0.74159998   0.39949998   0.78709996
    2.39426255]
 [158.76130733   0.10290001   0.38340002   0.19810002   0.4375
    1.75466955]
 ...
 [452.43011333   0.0768       0.83210003   0.60650003   0.87810004
    2.24574161]
 [313.12436771   0.0736       0.72600001   0.43459997   0.77569997
    2.15850163]
 [261.21391435   0.1139       0.71850008   0.56450003   0.74199998
    1.84983003]][0m
[37m[1m[2023-07-11 16:44:39,122][233954] Max Reward on eval: 721.8870582696051[0m
[37m[1m[2023-07-11 16:44:39,122][233954] Min Reward on eval: 93.28952573062853[0m
[37m[1m[2023-07-11 16:44:39,122][233954] Mean Reward across all agents: 356.82051128312423[0m
[37m[1m[2023-07-11 16:44:39,123][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:44:39,125][233954] mean_value=-111.07756034422232, max_value=368.5941016070698[0m
[37m[1m[2023-07-11 16:44:39,128][233954] New mean coefficients: [[ 0.41354853 -1.8877909  -0.9771013   0.9002936   1.7814382  -1.7587179 ]][0m
[37m[1m[2023-07-11 16:44:39,129][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:44:48,112][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 16:44:48,112][233954] FPS: 427569.00[0m
[36m[2023-07-11 16:44:48,114][233954] itr=1220, itrs=2000, Progress: 61.00%[0m
[37m[1m[2023-07-11 16:48:17,636][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001200[0m
[36m[2023-07-11 16:48:29,754][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 16:48:29,755][233954] FPS: 330719.05[0m
[36m[2023-07-11 16:48:33,995][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:48:33,996][233954] Reward + Measures: [[356.67573524   0.03199333   0.85763538   0.67031932   0.90300232
    2.1607182 ]][0m
[37m[1m[2023-07-11 16:48:33,996][233954] Max Reward on eval: 356.6757352415927[0m
[37m[1m[2023-07-11 16:48:33,996][233954] Min Reward on eval: 356.6757352415927[0m
[37m[1m[2023-07-11 16:48:33,997][233954] Mean Reward across all agents: 356.6757352415927[0m
[37m[1m[2023-07-11 16:48:33,997][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:48:39,223][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:48:39,224][233954] Reward + Measures: [[334.65316389   0.0002       0.99760002   0.75800002   0.99750006
    2.31316638]
 [404.44441031   0.0143       0.88980001   0.68129998   0.91820002
    2.41984749]
 [107.73376465   0.1355       0.83479995   0.40260002   0.80159998
    2.59241056]
 ...
 [103.31115748   0.1452       0.77329999   0.37890002   0.75340003
    2.711972  ]
 [111.67382717   0.1524       0.78200001   0.32790002   0.71860003
    2.63691401]
 [648.89475253   0.0719       0.9939       0.71170002   0.98400003
    2.53720951]][0m
[37m[1m[2023-07-11 16:48:39,224][233954] Max Reward on eval: 730.7951240525581[0m
[37m[1m[2023-07-11 16:48:39,225][233954] Min Reward on eval: 17.280205002857837[0m
[37m[1m[2023-07-11 16:48:39,225][233954] Mean Reward across all agents: 304.8776314344401[0m
[37m[1m[2023-07-11 16:48:39,225][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:48:39,227][233954] mean_value=-235.6979302681142, max_value=230.1211235612769[0m
[37m[1m[2023-07-11 16:48:39,229][233954] New mean coefficients: [[ 1.4050338  -0.62099564 -1.0870906   1.3467855   2.0475616  -1.5182544 ]][0m
[37m[1m[2023-07-11 16:48:39,230][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:48:48,266][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 16:48:48,267][233954] FPS: 425036.32[0m
[36m[2023-07-11 16:48:48,269][233954] itr=1221, itrs=2000, Progress: 61.05%[0m
[36m[2023-07-11 16:49:00,044][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 16:49:00,044][233954] FPS: 328796.81[0m
[36m[2023-07-11 16:49:04,327][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:49:04,328][233954] Reward + Measures: [[338.40848202   0.04045467   0.859712     0.68086004   0.90400267
    2.12609363]][0m
[37m[1m[2023-07-11 16:49:04,328][233954] Max Reward on eval: 338.4084820167118[0m
[37m[1m[2023-07-11 16:49:04,328][233954] Min Reward on eval: 338.4084820167118[0m
[37m[1m[2023-07-11 16:49:04,328][233954] Mean Reward across all agents: 338.4084820167118[0m
[37m[1m[2023-07-11 16:49:04,329][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:49:09,241][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:49:09,242][233954] Reward + Measures: [[224.44828441   0.0456       0.52560002   0.37470004   0.56590003
    1.7364949 ]
 [ 94.29997323   0.17580001   0.47839999   0.23819999   0.49419999
    2.20551372]
 [128.05544254   0.09730001   0.30270001   0.24419999   0.35030001
    1.59570777]
 ...
 [350.26142124   0.16690001   0.71890002   0.58880007   0.7823
    2.13298392]
 [510.74873518   0.2431       0.6523       0.71270001   0.85410005
    2.29051828]
 [ 85.81643663   0.24850002   0.37130001   0.23550001   0.41190001
    1.99712408]][0m
[37m[1m[2023-07-11 16:49:09,242][233954] Max Reward on eval: 619.1356506342069[0m
[37m[1m[2023-07-11 16:49:09,242][233954] Min Reward on eval: 11.320067403977736[0m
[37m[1m[2023-07-11 16:49:09,242][233954] Mean Reward across all agents: 290.5986380210121[0m
[37m[1m[2023-07-11 16:49:09,243][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:49:09,245][233954] mean_value=-146.48175473504088, max_value=279.3147791664084[0m
[37m[1m[2023-07-11 16:49:09,247][233954] New mean coefficients: [[ 0.7411244  -0.3293781  -0.77721655  1.4809965   2.2435238  -1.0887399 ]][0m
[37m[1m[2023-07-11 16:49:09,248][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:49:18,165][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 16:49:18,165][233954] FPS: 430700.82[0m
[36m[2023-07-11 16:49:18,168][233954] itr=1222, itrs=2000, Progress: 61.10%[0m
[36m[2023-07-11 16:49:29,757][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 16:49:29,757][233954] FPS: 334271.30[0m
[36m[2023-07-11 16:49:33,980][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:49:33,980][233954] Reward + Measures: [[341.77557947   0.03068533   0.87422067   0.69442743   0.91201711
    2.13472939]][0m
[37m[1m[2023-07-11 16:49:33,980][233954] Max Reward on eval: 341.77557946841387[0m
[37m[1m[2023-07-11 16:49:33,980][233954] Min Reward on eval: 341.77557946841387[0m
[37m[1m[2023-07-11 16:49:33,981][233954] Mean Reward across all agents: 341.77557946841387[0m
[37m[1m[2023-07-11 16:49:33,981][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:49:38,917][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:49:38,917][233954] Reward + Measures: [[354.57890512   0.0004       0.9971       0.76419997   0.99470007
    2.30715299]
 [149.65147897   0.1416       0.30620003   0.20770001   0.45809999
    1.64986157]
 [323.09768297   0.19090001   0.73909998   0.68099999   0.76810002
    1.98500097]
 ...
 [263.5980599    0.1125       0.71270001   0.574        0.72240001
    1.90431237]
 [247.24222933   0.16270001   0.59180003   0.49270001   0.6279
    1.75981712]
 [163.75175779   0.16010001   0.53350002   0.41859999   0.58219999
    1.87296164]][0m
[37m[1m[2023-07-11 16:49:38,918][233954] Max Reward on eval: 656.5933914266527[0m
[37m[1m[2023-07-11 16:49:38,918][233954] Min Reward on eval: 27.398309427371714[0m
[37m[1m[2023-07-11 16:49:38,918][233954] Mean Reward across all agents: 280.7055457475405[0m
[37m[1m[2023-07-11 16:49:38,918][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:49:38,921][233954] mean_value=-133.27242659748256, max_value=768.9162204467924[0m
[37m[1m[2023-07-11 16:49:38,923][233954] New mean coefficients: [[ 0.48423666 -0.34291297 -0.40594855  0.98844635  2.1815815  -0.8425611 ]][0m
[37m[1m[2023-07-11 16:49:38,924][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:49:47,904][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 16:49:47,904][233954] FPS: 427691.60[0m
[36m[2023-07-11 16:49:47,906][233954] itr=1223, itrs=2000, Progress: 61.15%[0m
[36m[2023-07-11 16:49:59,517][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 16:49:59,518][233954] FPS: 333428.67[0m
[36m[2023-07-11 16:50:03,859][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:50:03,859][233954] Reward + Measures: [[352.61143124   0.027386     0.87394565   0.71160269   0.91664362
    2.13651133]][0m
[37m[1m[2023-07-11 16:50:03,859][233954] Max Reward on eval: 352.61143123848234[0m
[37m[1m[2023-07-11 16:50:03,860][233954] Min Reward on eval: 352.61143123848234[0m
[37m[1m[2023-07-11 16:50:03,860][233954] Mean Reward across all agents: 352.61143123848234[0m
[37m[1m[2023-07-11 16:50:03,860][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:50:08,861][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:50:08,867][233954] Reward + Measures: [[458.76816033   0.0226       0.89750004   0.69940001   0.90369999
    2.2422576 ]
 [376.53730964   0.21110001   0.62799996   0.64529997   0.75650007
    2.12799883]
 [356.38168177   0.0212       0.81110001   0.61300004   0.85699999
    2.17983317]
 ...
 [509.72130896   0.0496       0.89870006   0.64709997   0.88280004
    2.24965477]
 [269.66511223   0.021        0.89889997   0.67150003   0.88940001
    2.05526018]
 [382.56242751   0.10630001   0.76070005   0.62989998   0.78780001
    2.01461768]][0m
[37m[1m[2023-07-11 16:50:08,867][233954] Max Reward on eval: 676.3225326537329[0m
[37m[1m[2023-07-11 16:50:08,868][233954] Min Reward on eval: 217.60048060943373[0m
[37m[1m[2023-07-11 16:50:08,868][233954] Mean Reward across all agents: 399.0079170989507[0m
[37m[1m[2023-07-11 16:50:08,868][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:50:08,870][233954] mean_value=-135.2514645409944, max_value=776.3619132557837[0m
[37m[1m[2023-07-11 16:50:08,872][233954] New mean coefficients: [[-0.12295187 -1.0278889   0.46021196  0.68613833  2.7926002  -0.58122253]][0m
[37m[1m[2023-07-11 16:50:08,873][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:50:17,951][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 16:50:17,951][233954] FPS: 423098.32[0m
[36m[2023-07-11 16:50:17,953][233954] itr=1224, itrs=2000, Progress: 61.20%[0m
[36m[2023-07-11 16:50:29,785][233954] train() took 11.73 seconds to complete[0m
[36m[2023-07-11 16:50:29,785][233954] FPS: 327279.20[0m
[36m[2023-07-11 16:50:34,185][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:50:34,186][233954] Reward + Measures: [[356.36897233   0.02772767   0.88810837   0.72292471   0.92384768
    2.14571047]][0m
[37m[1m[2023-07-11 16:50:34,186][233954] Max Reward on eval: 356.3689723312566[0m
[37m[1m[2023-07-11 16:50:34,186][233954] Min Reward on eval: 356.3689723312566[0m
[37m[1m[2023-07-11 16:50:34,186][233954] Mean Reward across all agents: 356.3689723312566[0m
[37m[1m[2023-07-11 16:50:34,187][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:50:39,237][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:50:39,238][233954] Reward + Measures: [[368.97348022   0.19060002   0.73519993   0.73699999   0.81300002
    2.22493553]
 [280.31599425   0.33209997   0.59400004   0.68440002   0.78500003
    2.16616106]
 [271.39408113   0.199        0.68169999   0.6031       0.7748
    2.35272479]
 ...
 [301.84838508   0.164        0.64020002   0.58000004   0.70660001
    2.16718698]
 [273.11214544   0.23439999   0.59689999   0.58410001   0.70839995
    2.29384136]
 [534.8003998    0.0922       0.99190009   0.67270005   0.98390007
    2.349406  ]][0m
[37m[1m[2023-07-11 16:50:39,238][233954] Max Reward on eval: 600.0837688330328[0m
[37m[1m[2023-07-11 16:50:39,238][233954] Min Reward on eval: 18.861070754309186[0m
[37m[1m[2023-07-11 16:50:39,238][233954] Mean Reward across all agents: 286.18309532417817[0m
[37m[1m[2023-07-11 16:50:39,239][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:50:39,241][233954] mean_value=-154.76327730345366, max_value=800.6249395602092[0m
[37m[1m[2023-07-11 16:50:39,243][233954] New mean coefficients: [[ 0.437037   -0.06751919  0.4566956   1.4646709   2.9547176  -1.1258209 ]][0m
[37m[1m[2023-07-11 16:50:39,244][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:50:48,335][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 16:50:48,335][233954] FPS: 422500.66[0m
[36m[2023-07-11 16:50:48,337][233954] itr=1225, itrs=2000, Progress: 61.25%[0m
[36m[2023-07-11 16:51:00,020][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 16:51:00,020][233954] FPS: 331399.20[0m
[36m[2023-07-11 16:51:04,430][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:51:04,431][233954] Reward + Measures: [[344.8353722    0.02406833   0.9011113    0.74319601   0.93028724
    2.13698387]][0m
[37m[1m[2023-07-11 16:51:04,431][233954] Max Reward on eval: 344.8353721974602[0m
[37m[1m[2023-07-11 16:51:04,431][233954] Min Reward on eval: 344.8353721974602[0m
[37m[1m[2023-07-11 16:51:04,431][233954] Mean Reward across all agents: 344.8353721974602[0m
[37m[1m[2023-07-11 16:51:04,431][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:51:09,537][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:51:09,537][233954] Reward + Measures: [[329.78687715   0.12819999   0.77699995   0.58590001   0.78909999
    2.00917816]
 [410.81400871   0.1409       0.96609992   0.7087       0.97979993
    2.22073627]
 [335.26837729   0.0917       0.68290007   0.41910002   0.72470003
    2.04724121]
 ...
 [383.88884403   0.0916       0.71140003   0.51459998   0.77879995
    1.99699879]
 [385.28471372   0.13070001   0.87550002   0.62370002   0.87720007
    2.07692599]
 [336.12193252   0.0283       0.90579998   0.71079999   0.90809995
    2.11426592]][0m
[37m[1m[2023-07-11 16:51:09,537][233954] Max Reward on eval: 637.3199729991844[0m
[37m[1m[2023-07-11 16:51:09,538][233954] Min Reward on eval: 213.41913937172504[0m
[37m[1m[2023-07-11 16:51:09,538][233954] Mean Reward across all agents: 376.84699919151444[0m
[37m[1m[2023-07-11 16:51:09,538][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:51:09,540][233954] mean_value=-150.09108263684595, max_value=819.2147748145669[0m
[37m[1m[2023-07-11 16:51:09,542][233954] New mean coefficients: [[ 0.7053041   0.11125535  0.09902292  2.0600271   3.2732677  -0.15103042]][0m
[37m[1m[2023-07-11 16:51:09,543][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:51:18,649][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 16:51:18,650][233954] FPS: 421755.71[0m
[36m[2023-07-11 16:51:18,652][233954] itr=1226, itrs=2000, Progress: 61.30%[0m
[36m[2023-07-11 16:51:30,378][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 16:51:30,378][233954] FPS: 330227.77[0m
[36m[2023-07-11 16:51:34,637][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:51:34,637][233954] Reward + Measures: [[342.70349924   0.027309     0.90313965   0.75076658   0.93096864
    2.13698435]][0m
[37m[1m[2023-07-11 16:51:34,638][233954] Max Reward on eval: 342.70349924001977[0m
[37m[1m[2023-07-11 16:51:34,638][233954] Min Reward on eval: 342.70349924001977[0m
[37m[1m[2023-07-11 16:51:34,638][233954] Mean Reward across all agents: 342.70349924001977[0m
[37m[1m[2023-07-11 16:51:34,638][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:51:39,832][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:51:39,832][233954] Reward + Measures: [[367.19544983   0.1309       0.85939997   0.66860002   0.85880005
    2.2527256 ]
 [104.99665712   0.3118       0.52430004   0.2987       0.61379999
    1.83066785]
 [367.19767855   0.1134       0.86129999   0.70539999   0.90430003
    2.13229704]
 ...
 [359.19086259   0.39669999   0.7608       0.2906       0.76359999
    2.71741009]
 [402.03191949   0.0753       0.92119998   0.70380002   0.90070003
    2.20805717]
 [169.56604387   0.2931       0.51710004   0.28660002   0.58219999
    1.88180578]][0m
[37m[1m[2023-07-11 16:51:39,833][233954] Max Reward on eval: 696.8607444887981[0m
[37m[1m[2023-07-11 16:51:39,833][233954] Min Reward on eval: 33.219645727798344[0m
[37m[1m[2023-07-11 16:51:39,833][233954] Mean Reward across all agents: 348.7998847278448[0m
[37m[1m[2023-07-11 16:51:39,833][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:51:39,836][233954] mean_value=-136.76420668166622, max_value=205.13484216849497[0m
[37m[1m[2023-07-11 16:51:39,838][233954] New mean coefficients: [[0.86322063 0.8732901  0.7311437  2.2812104  3.4652758  0.5514365 ]][0m
[37m[1m[2023-07-11 16:51:39,839][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:51:48,851][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 16:51:48,851][233954] FPS: 426203.16[0m
[36m[2023-07-11 16:51:48,853][233954] itr=1227, itrs=2000, Progress: 61.35%[0m
[36m[2023-07-11 16:52:00,456][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 16:52:00,456][233954] FPS: 333700.81[0m
[36m[2023-07-11 16:52:04,694][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:52:04,695][233954] Reward + Measures: [[352.94305013   0.032355     0.92900127   0.76650363   0.94813198
    2.13510132]][0m
[37m[1m[2023-07-11 16:52:04,695][233954] Max Reward on eval: 352.94305013411696[0m
[37m[1m[2023-07-11 16:52:04,695][233954] Min Reward on eval: 352.94305013411696[0m
[37m[1m[2023-07-11 16:52:04,695][233954] Mean Reward across all agents: 352.94305013411696[0m
[37m[1m[2023-07-11 16:52:04,696][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:52:09,714][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:52:09,715][233954] Reward + Measures: [[348.11205913   0.12969999   0.89939994   0.63660002   0.90450001
    2.18948412]
 [452.85113335   0.1929       0.86079997   0.60890001   0.89439994
    2.24740362]
 [517.6214237    0.1224       0.98570007   0.76929998   0.96740001
    2.34269762]
 ...
 [409.7145176    0.0559       0.98789996   0.77829999   0.9745
    2.17196441]
 [300.0906086    0.1462       0.85249996   0.61709994   0.81710005
    1.95807898]
 [418.73596429   0.0062       0.90259999   0.7881       0.9278
    2.21806145]][0m
[37m[1m[2023-07-11 16:52:09,715][233954] Max Reward on eval: 638.2498092799448[0m
[37m[1m[2023-07-11 16:52:09,715][233954] Min Reward on eval: 186.4681171411299[0m
[37m[1m[2023-07-11 16:52:09,715][233954] Mean Reward across all agents: 363.92743979138123[0m
[37m[1m[2023-07-11 16:52:09,715][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:52:09,717][233954] mean_value=-172.1877990344702, max_value=147.3492165942106[0m
[37m[1m[2023-07-11 16:52:09,719][233954] New mean coefficients: [[1.0732982  1.8418641  0.71138686 3.5151944  3.0996435  1.9974968 ]][0m
[37m[1m[2023-07-11 16:52:09,720][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:52:18,754][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 16:52:18,754][233954] FPS: 425157.62[0m
[36m[2023-07-11 16:52:18,756][233954] itr=1228, itrs=2000, Progress: 61.40%[0m
[36m[2023-07-11 16:52:30,396][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 16:52:30,396][233954] FPS: 332680.06[0m
[36m[2023-07-11 16:52:34,692][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:52:34,692][233954] Reward + Measures: [[346.4782992    0.036556     0.93123925   0.75959998   0.9512307
    2.12880445]][0m
[37m[1m[2023-07-11 16:52:34,693][233954] Max Reward on eval: 346.47829920381963[0m
[37m[1m[2023-07-11 16:52:34,693][233954] Min Reward on eval: 346.47829920381963[0m
[37m[1m[2023-07-11 16:52:34,693][233954] Mean Reward across all agents: 346.47829920381963[0m
[37m[1m[2023-07-11 16:52:34,693][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:52:39,652][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:52:39,658][233954] Reward + Measures: [[382.65972878   0.14260001   0.7938       0.73940003   0.86449999
    2.08122635]
 [280.19333374   0.0712       0.63350004   0.53909999   0.66390002
    2.06125951]
 [146.40461543   0.13700001   0.45110002   0.29100001   0.4646
    2.13914275]
 ...
 [468.70328806   0.11870001   0.86190003   0.72170001   0.86800003
    2.1801722 ]
 [ 99.44571446   0.2122       0.2167       0.21000002   0.3001
    2.11564708]
 [336.68410681   0.17420001   0.67250001   0.59020007   0.73769999
    2.34327388]][0m
[37m[1m[2023-07-11 16:52:39,658][233954] Max Reward on eval: 624.5087051332463[0m
[37m[1m[2023-07-11 16:52:39,659][233954] Min Reward on eval: -309.0626481065759[0m
[37m[1m[2023-07-11 16:52:39,659][233954] Mean Reward across all agents: 312.3540730971753[0m
[37m[1m[2023-07-11 16:52:39,659][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:52:39,661][233954] mean_value=-173.50574936939458, max_value=167.33462124315048[0m
[37m[1m[2023-07-11 16:52:39,664][233954] New mean coefficients: [[0.895038   0.9573061  0.64514935 3.3542662  3.1717415  2.278842  ]][0m
[37m[1m[2023-07-11 16:52:39,665][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:52:48,687][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 16:52:48,688][233954] FPS: 425673.68[0m
[36m[2023-07-11 16:52:48,690][233954] itr=1229, itrs=2000, Progress: 61.45%[0m
[36m[2023-07-11 16:53:00,348][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 16:53:00,353][233954] FPS: 332174.71[0m
[36m[2023-07-11 16:53:04,728][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:53:04,728][233954] Reward + Measures: [[345.95990609   0.031186     0.9375127    0.77938396   0.95459569
    2.13495207]][0m
[37m[1m[2023-07-11 16:53:04,729][233954] Max Reward on eval: 345.95990609305153[0m
[37m[1m[2023-07-11 16:53:04,729][233954] Min Reward on eval: 345.95990609305153[0m
[37m[1m[2023-07-11 16:53:04,729][233954] Mean Reward across all agents: 345.95990609305153[0m
[37m[1m[2023-07-11 16:53:04,729][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:53:09,781][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:53:09,786][233954] Reward + Measures: [[  1.62655566   0.33119997   0.57690001   0.21230002   0.60030001
    2.1534729 ]
 [367.46101573   0.0075       0.90039998   0.68600005   0.92979997
    2.03354692]
 [403.89399337   0.0004       0.99510002   0.76370001   0.99449998
    2.02968168]
 ...
 [481.63347528   0.022        0.72130007   0.59070003   0.79580009
    2.41092491]
 [263.24196628   0.0221       0.97690004   0.59149998   0.97060007
    2.05793548]
 [  1.30668947   0.19470002   0.5079       0.21879999   0.54660004
    2.25685501]][0m
[37m[1m[2023-07-11 16:53:09,787][233954] Max Reward on eval: 654.6165847464348[0m
[37m[1m[2023-07-11 16:53:09,787][233954] Min Reward on eval: -122.9655677124858[0m
[37m[1m[2023-07-11 16:53:09,787][233954] Mean Reward across all agents: 248.09124992734402[0m
[37m[1m[2023-07-11 16:53:09,788][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:53:09,790][233954] mean_value=-159.89491330040906, max_value=260.85084920178963[0m
[37m[1m[2023-07-11 16:53:09,793][233954] New mean coefficients: [[0.7520121 0.455723  1.0440316 2.8522685 3.2097938 2.3519676]][0m
[37m[1m[2023-07-11 16:53:09,794][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:53:18,896][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 16:53:18,896][233954] FPS: 421958.93[0m
[36m[2023-07-11 16:53:18,899][233954] itr=1230, itrs=2000, Progress: 61.50%[0m
[37m[1m[2023-07-11 16:56:49,436][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001210[0m
[36m[2023-07-11 16:57:01,652][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 16:57:01,652][233954] FPS: 329959.62[0m
[36m[2023-07-11 16:57:05,847][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:57:05,848][233954] Reward + Measures: [[347.52785118   0.02670133   0.93136162   0.77639467   0.9513343
    2.13277411]][0m
[37m[1m[2023-07-11 16:57:05,848][233954] Max Reward on eval: 347.52785117596494[0m
[37m[1m[2023-07-11 16:57:05,848][233954] Min Reward on eval: 347.52785117596494[0m
[37m[1m[2023-07-11 16:57:05,848][233954] Mean Reward across all agents: 347.52785117596494[0m
[37m[1m[2023-07-11 16:57:05,849][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:57:11,028][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:57:11,028][233954] Reward + Measures: [[463.70985221   0.1099       0.88210005   0.70679998   0.91680002
    2.14816737]
 [299.66288558   0.2225       0.70530003   0.44850001   0.77730006
    2.08183479]
 [398.35245896   0.11920001   0.84510005   0.68650001   0.88260001
    2.07552552]
 ...
 [313.08814694   0.17309999   0.79230005   0.5442       0.79180002
    2.02829337]
 [473.86190989   0.0735       0.97450012   0.82950002   0.97060007
    2.20325899]
 [246.70338401   0.1107       0.73719996   0.4217       0.72790003
    2.13384318]][0m
[37m[1m[2023-07-11 16:57:11,028][233954] Max Reward on eval: 568.4826354914345[0m
[37m[1m[2023-07-11 16:57:11,029][233954] Min Reward on eval: 165.74060759717833[0m
[37m[1m[2023-07-11 16:57:11,029][233954] Mean Reward across all agents: 395.5423735548537[0m
[37m[1m[2023-07-11 16:57:11,029][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:57:11,031][233954] mean_value=-140.72182033171717, max_value=67.6995121786884[0m
[37m[1m[2023-07-11 16:57:11,033][233954] New mean coefficients: [[0.45549634 0.2519008  0.7276958  3.0555596  3.7939122  2.9704077 ]][0m
[37m[1m[2023-07-11 16:57:11,034][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:57:19,949][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 16:57:19,949][233954] FPS: 430840.30[0m
[36m[2023-07-11 16:57:19,951][233954] itr=1231, itrs=2000, Progress: 61.55%[0m
[36m[2023-07-11 16:57:31,592][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 16:57:31,593][233954] FPS: 332632.26[0m
[36m[2023-07-11 16:57:35,861][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:57:35,861][233954] Reward + Measures: [[330.25526556   0.030233     0.93890965   0.77717227   0.954979
    2.12895966]][0m
[37m[1m[2023-07-11 16:57:35,861][233954] Max Reward on eval: 330.2552655551057[0m
[37m[1m[2023-07-11 16:57:35,862][233954] Min Reward on eval: 330.2552655551057[0m
[37m[1m[2023-07-11 16:57:35,862][233954] Mean Reward across all agents: 330.2552655551057[0m
[37m[1m[2023-07-11 16:57:35,862][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:57:40,799][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:57:40,800][233954] Reward + Measures: [[462.62014389   0.0007       0.99770004   0.77890003   0.99690002
    2.28989291]
 [644.7981682    0.063        0.99360001   0.72850001   0.98919994
    2.43725109]
 [485.82019232   0.1033       0.89070004   0.61140001   0.90359992
    2.33101153]
 ...
 [445.10319442   0.0187       0.81199998   0.62340003   0.84740001
    2.27529883]
 [353.37823486   0.0572       0.98750001   0.80779999   0.97850001
    2.17492986]
 [506.04183104   0.0123       0.86070007   0.65539998   0.89280003
    2.43233299]][0m
[37m[1m[2023-07-11 16:57:40,800][233954] Max Reward on eval: 679.5813102710993[0m
[37m[1m[2023-07-11 16:57:40,800][233954] Min Reward on eval: 177.08400176065044[0m
[37m[1m[2023-07-11 16:57:40,801][233954] Mean Reward across all agents: 427.5395187807946[0m
[37m[1m[2023-07-11 16:57:40,801][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:57:40,803][233954] mean_value=-145.43529614882564, max_value=261.02249492400273[0m
[37m[1m[2023-07-11 16:57:40,805][233954] New mean coefficients: [[1.1833138  0.82977474 0.86994433 3.4687817  4.7356434  3.055694  ]][0m
[37m[1m[2023-07-11 16:57:40,806][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:57:49,787][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 16:57:49,787][233954] FPS: 427635.97[0m
[36m[2023-07-11 16:57:49,789][233954] itr=1232, itrs=2000, Progress: 61.60%[0m
[36m[2023-07-11 16:58:01,640][233954] train() took 11.75 seconds to complete[0m
[36m[2023-07-11 16:58:01,640][233954] FPS: 326680.78[0m
[36m[2023-07-11 16:58:05,935][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:58:05,935][233954] Reward + Measures: [[334.95994449   0.02793333   0.93854636   0.77536064   0.9555636
    2.14629316]][0m
[37m[1m[2023-07-11 16:58:05,935][233954] Max Reward on eval: 334.959944494098[0m
[37m[1m[2023-07-11 16:58:05,935][233954] Min Reward on eval: 334.959944494098[0m
[37m[1m[2023-07-11 16:58:05,936][233954] Mean Reward across all agents: 334.959944494098[0m
[37m[1m[2023-07-11 16:58:05,936][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:58:10,874][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:58:10,875][233954] Reward + Measures: [[321.59836849   0.2349       0.88819999   0.51929998   0.88300002
    2.10976768]
 [406.15699005   0.07790001   0.99329996   0.78839999   0.98710006
    2.22766566]
 [378.43450165   0.17349999   0.99300003   0.68460006   0.98639995
    2.15471435]
 ...
 [402.7662468    0.27169999   0.96930009   0.58950001   0.95220006
    2.09991908]
 [403.63705824   0.14840001   0.90410006   0.70250005   0.90959996
    2.15253139]
 [457.13139725   0.0771       0.9932       0.73750001   0.98530006
    2.30851722]][0m
[37m[1m[2023-07-11 16:58:10,875][233954] Max Reward on eval: 629.3786430242938[0m
[37m[1m[2023-07-11 16:58:10,875][233954] Min Reward on eval: 124.36602065218612[0m
[37m[1m[2023-07-11 16:58:10,875][233954] Mean Reward across all agents: 357.8521453932778[0m
[37m[1m[2023-07-11 16:58:10,876][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:58:10,877][233954] mean_value=-158.122407494546, max_value=183.62284619530192[0m
[37m[1m[2023-07-11 16:58:10,880][233954] New mean coefficients: [[1.1469871  0.31909394 1.461015   3.6828294  5.4592075  2.360704  ]][0m
[37m[1m[2023-07-11 16:58:10,881][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:58:19,965][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 16:58:19,966][233954] FPS: 422770.19[0m
[36m[2023-07-11 16:58:19,968][233954] itr=1233, itrs=2000, Progress: 61.65%[0m
[36m[2023-07-11 16:58:31,673][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 16:58:31,673][233954] FPS: 330865.57[0m
[36m[2023-07-11 16:58:35,960][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:58:35,965][233954] Reward + Measures: [[329.31767213   0.02759966   0.95111465   0.80252498   0.96343368
    2.16728425]][0m
[37m[1m[2023-07-11 16:58:35,966][233954] Max Reward on eval: 329.3176721268993[0m
[37m[1m[2023-07-11 16:58:35,966][233954] Min Reward on eval: 329.3176721268993[0m
[37m[1m[2023-07-11 16:58:35,966][233954] Mean Reward across all agents: 329.3176721268993[0m
[37m[1m[2023-07-11 16:58:35,966][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:58:40,913][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:58:40,913][233954] Reward + Measures: [[427.11662615   0.0697       0.81029999   0.44070002   0.79809999
    2.33957529]
 [543.34746788   0.11440001   0.97659999   0.44940001   0.95349997
    2.58848071]
 [359.63574922   0.1176       0.47010002   0.37269998   0.64560002
    2.22726822]
 ...
 [635.53076172   0.0011       0.99809998   0.78789997   0.99779999
    2.55109715]
 [375.89778901   0.1382       0.96759999   0.70250005   0.93850005
    2.14764833]
 [422.61910366   0.0168       0.81330007   0.59979999   0.83129996
    2.31405711]][0m
[37m[1m[2023-07-11 16:58:40,914][233954] Max Reward on eval: 706.8704071028158[0m
[37m[1m[2023-07-11 16:58:40,914][233954] Min Reward on eval: 197.3297910951078[0m
[37m[1m[2023-07-11 16:58:40,914][233954] Mean Reward across all agents: 471.91834610946165[0m
[37m[1m[2023-07-11 16:58:40,914][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:58:40,917][233954] mean_value=-103.04910648492424, max_value=280.4501896318486[0m
[37m[1m[2023-07-11 16:58:40,919][233954] New mean coefficients: [[1.6692729 1.0525613 2.0120926 4.7727833 5.8441577 3.5665174]][0m
[37m[1m[2023-07-11 16:58:40,920][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:58:49,868][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 16:58:49,868][233954] FPS: 429233.48[0m
[36m[2023-07-11 16:58:49,871][233954] itr=1234, itrs=2000, Progress: 61.70%[0m
[36m[2023-07-11 16:59:01,703][233954] train() took 11.73 seconds to complete[0m
[36m[2023-07-11 16:59:01,703][233954] FPS: 327241.65[0m
[36m[2023-07-11 16:59:05,990][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:59:05,995][233954] Reward + Measures: [[324.84297418   0.01956467   0.93972832   0.79746664   0.95678061
    2.18736506]][0m
[37m[1m[2023-07-11 16:59:05,996][233954] Max Reward on eval: 324.84297418384966[0m
[37m[1m[2023-07-11 16:59:05,996][233954] Min Reward on eval: 324.84297418384966[0m
[37m[1m[2023-07-11 16:59:05,996][233954] Mean Reward across all agents: 324.84297418384966[0m
[37m[1m[2023-07-11 16:59:05,996][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:59:11,108][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:59:11,114][233954] Reward + Measures: [[475.26203761   0.0296       0.73430002   0.57720006   0.76940006
    2.21508551]
 [382.79240897   0.0767       0.91730005   0.84709996   0.97049999
    2.17591524]
 [680.49445724   0.009        0.99019998   0.70010006   0.98850006
    2.5169549 ]
 ...
 [576.1738155    0.0102       0.90160006   0.70440006   0.91210002
    2.49258995]
 [439.83192012   0.0501       0.89420003   0.64819998   0.9224
    2.30048418]
 [535.00654651   0.0166       0.88990003   0.65200001   0.9012
    2.39399099]][0m
[37m[1m[2023-07-11 16:59:11,114][233954] Max Reward on eval: 685.8321151641779[0m
[37m[1m[2023-07-11 16:59:11,115][233954] Min Reward on eval: 177.16367912273853[0m
[37m[1m[2023-07-11 16:59:11,115][233954] Mean Reward across all agents: 503.5404134403234[0m
[37m[1m[2023-07-11 16:59:11,115][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:59:11,117][233954] mean_value=-95.63001034390912, max_value=1112.6947860851883[0m
[37m[1m[2023-07-11 16:59:11,120][233954] New mean coefficients: [[1.941884  1.1363754 2.0157745 5.1338997 6.1902056 4.8451357]][0m
[37m[1m[2023-07-11 16:59:11,120][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:59:20,187][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 16:59:20,187][233954] FPS: 423612.02[0m
[36m[2023-07-11 16:59:20,189][233954] itr=1235, itrs=2000, Progress: 61.75%[0m
[36m[2023-07-11 16:59:31,994][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 16:59:31,994][233954] FPS: 327979.47[0m
[36m[2023-07-11 16:59:36,263][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:59:36,263][233954] Reward + Measures: [[323.81078059   0.01947967   0.94922531   0.8100493    0.96106899
    2.21334863]][0m
[37m[1m[2023-07-11 16:59:36,264][233954] Max Reward on eval: 323.8107805896084[0m
[37m[1m[2023-07-11 16:59:36,264][233954] Min Reward on eval: 323.8107805896084[0m
[37m[1m[2023-07-11 16:59:36,264][233954] Mean Reward across all agents: 323.8107805896084[0m
[37m[1m[2023-07-11 16:59:36,264][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:59:41,476][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 16:59:41,482][233954] Reward + Measures: [[372.91853809   0.0364       0.54890007   0.39240003   0.62919998
    2.08775997]
 [438.28617667   0.0889       0.89359999   0.55719995   0.89160007
    2.54400063]
 [641.0879898    0.0074       0.991        0.60159999   0.98289996
    2.5887742 ]
 ...
 [382.32957268   0.17800002   0.82310003   0.47139999   0.87060004
    2.48023272]
 [359.22336958   0.2577       0.99420005   0.56390005   0.98690003
    2.55143404]
 [352.56200325   0.0985       0.90320009   0.6692       0.90599996
    2.22017622]][0m
[37m[1m[2023-07-11 16:59:41,482][233954] Max Reward on eval: 764.4326400656021[0m
[37m[1m[2023-07-11 16:59:41,482][233954] Min Reward on eval: 27.32378912726417[0m
[37m[1m[2023-07-11 16:59:41,483][233954] Mean Reward across all agents: 438.8588238385055[0m
[37m[1m[2023-07-11 16:59:41,483][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 16:59:41,486][233954] mean_value=-142.2034036235889, max_value=172.16614999659134[0m
[37m[1m[2023-07-11 16:59:41,488][233954] New mean coefficients: [[1.8570611  0.50364757 1.7623591  5.8259096  6.5022244  4.7480693 ]][0m
[37m[1m[2023-07-11 16:59:41,489][233954] Moving the mean solution point...[0m
[36m[2023-07-11 16:59:50,460][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 16:59:50,461][233954] FPS: 428133.06[0m
[36m[2023-07-11 16:59:50,463][233954] itr=1236, itrs=2000, Progress: 61.80%[0m
[36m[2023-07-11 17:00:02,181][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 17:00:02,182][233954] FPS: 330417.51[0m
[36m[2023-07-11 17:00:06,401][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:00:06,406][233954] Reward + Measures: [[324.69796459   0.02821533   0.96404767   0.81277996   0.969688
    2.23130989]][0m
[37m[1m[2023-07-11 17:00:06,407][233954] Max Reward on eval: 324.69796459107414[0m
[37m[1m[2023-07-11 17:00:06,407][233954] Min Reward on eval: 324.69796459107414[0m
[37m[1m[2023-07-11 17:00:06,407][233954] Mean Reward across all agents: 324.69796459107414[0m
[37m[1m[2023-07-11 17:00:06,407][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:00:11,373][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:00:11,373][233954] Reward + Measures: [[523.36748503   0.08620001   0.98400003   0.80219996   0.96200007
    2.22365761]
 [427.20011521   0.0358       0.93599999   0.75599998   0.9212001
    2.11881709]
 [421.87560654   0.0461       0.96759999   0.78500003   0.94449997
    2.15075278]
 ...
 [445.69886397   0.0968       0.96530002   0.77000004   0.94130003
    2.2015214 ]
 [384.67023277   0.0964       0.89340001   0.70310003   0.88260001
    2.10249686]
 [405.94748472   0.0581       0.89069998   0.72429997   0.86300004
    2.05188632]][0m
[37m[1m[2023-07-11 17:00:11,373][233954] Max Reward on eval: 534.8807220226154[0m
[37m[1m[2023-07-11 17:00:11,374][233954] Min Reward on eval: 122.44286602702923[0m
[37m[1m[2023-07-11 17:00:11,374][233954] Mean Reward across all agents: 390.02387725564586[0m
[37m[1m[2023-07-11 17:00:11,374][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:00:11,376][233954] mean_value=-156.33573770527315, max_value=85.42897301361648[0m
[37m[1m[2023-07-11 17:00:11,378][233954] New mean coefficients: [[1.5927222  0.17619663 1.099743   5.7439623  5.959832   4.317724  ]][0m
[37m[1m[2023-07-11 17:00:11,379][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:00:20,407][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 17:00:20,407][233954] FPS: 425405.79[0m
[36m[2023-07-11 17:00:20,410][233954] itr=1237, itrs=2000, Progress: 61.85%[0m
[36m[2023-07-11 17:00:32,102][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 17:00:32,103][233954] FPS: 331170.95[0m
[36m[2023-07-11 17:00:36,334][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:00:36,334][233954] Reward + Measures: [[313.4823755    0.01814833   0.97739434   0.84232634   0.98067331
    2.28328824]][0m
[37m[1m[2023-07-11 17:00:36,335][233954] Max Reward on eval: 313.4823754978319[0m
[37m[1m[2023-07-11 17:00:36,335][233954] Min Reward on eval: 313.4823754978319[0m
[37m[1m[2023-07-11 17:00:36,335][233954] Mean Reward across all agents: 313.4823754978319[0m
[37m[1m[2023-07-11 17:00:36,335][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:00:41,310][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:00:41,311][233954] Reward + Measures: [[499.69019319   0.032        0.94670004   0.89569998   0.94770002
    2.27356315]
 [383.33274672   0.0225       0.90420002   0.64120001   0.89850008
    2.28677058]
 [184.19814445   0.3364       0.87439996   0.36530003   0.87
    2.50761414]
 ...
 [300.3099668    0.13689999   0.80199999   0.48840004   0.84009999
    2.36910868]
 [494.43646429   0.0052       0.90389997   0.69309998   0.94259995
    2.63466907]
 [338.17069172   0.0407       0.99340004   0.70459998   0.97830003
    2.32274103]][0m
[37m[1m[2023-07-11 17:00:41,311][233954] Max Reward on eval: 585.4465732429177[0m
[37m[1m[2023-07-11 17:00:41,311][233954] Min Reward on eval: -150.8058707811404[0m
[37m[1m[2023-07-11 17:00:41,312][233954] Mean Reward across all agents: 279.22142726072144[0m
[37m[1m[2023-07-11 17:00:41,312][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:00:41,314][233954] mean_value=-170.9241509299566, max_value=267.51551546449[0m
[37m[1m[2023-07-11 17:00:41,317][233954] New mean coefficients: [[ 1.378235   -0.76948595  0.5013721   5.064609    5.5598016   4.231658  ]][0m
[37m[1m[2023-07-11 17:00:41,318][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:00:50,282][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 17:00:50,283][233954] FPS: 428438.51[0m
[36m[2023-07-11 17:00:50,285][233954] itr=1238, itrs=2000, Progress: 61.90%[0m
[36m[2023-07-11 17:01:02,200][233954] train() took 11.82 seconds to complete[0m
[36m[2023-07-11 17:01:02,200][233954] FPS: 324996.62[0m
[36m[2023-07-11 17:01:06,529][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:01:06,535][233954] Reward + Measures: [[307.24695636   0.039547     0.97143734   0.82255328   0.97376633
    2.30660629]][0m
[37m[1m[2023-07-11 17:01:06,535][233954] Max Reward on eval: 307.24695635756706[0m
[37m[1m[2023-07-11 17:01:06,535][233954] Min Reward on eval: 307.24695635756706[0m
[37m[1m[2023-07-11 17:01:06,535][233954] Mean Reward across all agents: 307.24695635756706[0m
[37m[1m[2023-07-11 17:01:06,536][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:01:11,535][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:01:11,541][233954] Reward + Measures: [[366.18335341   0.14850001   0.98969996   0.72539997   0.97170001
    2.37940574]
 [382.00193974   0.21729998   0.98750001   0.67440003   0.9709
    2.37089515]
 [259.19491959   0.0892       0.99610007   0.72430003   0.9799
    2.37590671]
 ...
 [378.30680466   0.1883       0.9601       0.6846       0.95390004
    2.33941197]
 [332.18255996   0.1006       0.94489998   0.74130005   0.9386
    2.28670359]
 [287.27638242   0.0706       0.99610007   0.77069998   0.98680001
    2.28710032]][0m
[37m[1m[2023-07-11 17:01:11,541][233954] Max Reward on eval: 456.00482178861273[0m
[37m[1m[2023-07-11 17:01:11,541][233954] Min Reward on eval: 215.56958436425776[0m
[37m[1m[2023-07-11 17:01:11,542][233954] Mean Reward across all agents: 332.9763229220509[0m
[37m[1m[2023-07-11 17:01:11,542][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:01:11,543][233954] mean_value=-315.18063489337493, max_value=-71.86544462088045[0m
[36m[2023-07-11 17:01:11,548][233954] XNES is restarting with a new solution whose measures are [0.28000003 0.27449998 0.38049999 0.1038     1.21776044] and objective is 341.3786773449741[0m
[36m[2023-07-11 17:01:11,549][233954] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-11 17:01:11,552][233954] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-11 17:01:11,553][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:01:20,604][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 17:01:20,605][233954] FPS: 424314.44[0m
[36m[2023-07-11 17:01:20,607][233954] itr=1239, itrs=2000, Progress: 61.95%[0m
[36m[2023-07-11 17:01:32,357][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 17:01:32,357][233954] FPS: 329517.21[0m
[36m[2023-07-11 17:01:36,600][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:01:36,600][233954] Reward + Measures: [[132.65359458   0.50056231   0.27220666   0.55165601   0.18353766
    0.9176237 ]][0m
[37m[1m[2023-07-11 17:01:36,600][233954] Max Reward on eval: 132.65359457747175[0m
[37m[1m[2023-07-11 17:01:36,601][233954] Min Reward on eval: 132.65359457747175[0m
[37m[1m[2023-07-11 17:01:36,601][233954] Mean Reward across all agents: 132.65359457747175[0m
[37m[1m[2023-07-11 17:01:36,601][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:01:41,611][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:01:41,617][233954] Reward + Measures: [[ 76.14608915   0.54210001   0.3536       0.56440002   0.15119998
    1.15318334]
 [193.57644843   0.47420001   0.33039999   0.43090001   0.17060001
    0.88534451]
 [153.98203632   0.44520006   0.36859998   0.3761       0.31760001
    0.88404447]
 ...
 [212.93650438   0.44760004   0.53469998   0.23370002   0.39159998
    0.85346222]
 [135.26790904   0.42810002   0.3132       0.43250003   0.24419999
    0.9005605 ]
 [223.00609302   0.4068       0.3436       0.36610001   0.25939998
    0.89255697]][0m
[37m[1m[2023-07-11 17:01:41,618][233954] Max Reward on eval: 303.8040556907654[0m
[37m[1m[2023-07-11 17:01:41,619][233954] Min Reward on eval: -73.16011664038524[0m
[37m[1m[2023-07-11 17:01:41,619][233954] Mean Reward across all agents: 125.72925996263936[0m
[37m[1m[2023-07-11 17:01:41,620][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:01:41,633][233954] mean_value=-306.25847338481765, max_value=726.8177818600088[0m
[37m[1m[2023-07-11 17:01:41,637][233954] New mean coefficients: [[ 1.2616006  -0.19628084 -2.384706   -1.185834   -0.9453899  -0.10596418]][0m
[37m[1m[2023-07-11 17:01:41,639][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:01:50,666][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 17:01:50,666][233954] FPS: 425475.54[0m
[36m[2023-07-11 17:01:50,668][233954] itr=1240, itrs=2000, Progress: 62.00%[0m
[37m[1m[2023-07-11 17:05:32,495][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001220[0m
[36m[2023-07-11 17:05:45,095][233954] train() took 11.90 seconds to complete[0m
[36m[2023-07-11 17:05:45,095][233954] FPS: 322675.62[0m
[36m[2023-07-11 17:05:49,349][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:05:49,350][233954] Reward + Measures: [[177.35667356   0.49616697   0.25694934   0.53191733   0.16673467
    0.94154376]][0m
[37m[1m[2023-07-11 17:05:49,350][233954] Max Reward on eval: 177.3566735630183[0m
[37m[1m[2023-07-11 17:05:49,350][233954] Min Reward on eval: 177.3566735630183[0m
[37m[1m[2023-07-11 17:05:49,351][233954] Mean Reward across all agents: 177.3566735630183[0m
[37m[1m[2023-07-11 17:05:49,351][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:05:54,276][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:05:54,277][233954] Reward + Measures: [[ 51.36789555   0.58920002   0.2987       0.5467       0.12730001
    0.92870075]
 [111.29513886   0.53750002   0.2155       0.53830004   0.14330001
    1.13144231]
 [ 50.56112644   0.41580001   0.17820001   0.4434       0.14029999
    1.23812234]
 ...
 [ 74.77838047   0.44870001   0.18160002   0.48529997   0.1391
    1.05857921]
 [  8.66479179   0.57920003   0.34830001   0.53070003   0.20390001
    0.91922826]
 [142.41946694   0.53319997   0.20829999   0.54539996   0.1495
    1.03849661]][0m
[37m[1m[2023-07-11 17:05:54,277][233954] Max Reward on eval: 289.40838146954775[0m
[37m[1m[2023-07-11 17:05:54,277][233954] Min Reward on eval: 6.745577201759443[0m
[37m[1m[2023-07-11 17:05:54,277][233954] Mean Reward across all agents: 135.8769030877043[0m
[37m[1m[2023-07-11 17:05:54,278][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:05:54,284][233954] mean_value=5.943247349090537, max_value=554.1458138713799[0m
[37m[1m[2023-07-11 17:05:54,286][233954] New mean coefficients: [[ 1.727578   -0.56280386 -1.9119389  -1.075259   -0.8104218  -0.6164365 ]][0m
[37m[1m[2023-07-11 17:05:54,287][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:06:03,282][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 17:06:03,282][233954] FPS: 427012.17[0m
[36m[2023-07-11 17:06:03,284][233954] itr=1241, itrs=2000, Progress: 62.05%[0m
[36m[2023-07-11 17:06:15,034][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 17:06:15,034][233954] FPS: 329485.80[0m
[36m[2023-07-11 17:06:19,312][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:06:19,313][233954] Reward + Measures: [[244.86553044   0.42020336   0.24738066   0.46702498   0.16160899
    0.92140728]][0m
[37m[1m[2023-07-11 17:06:19,313][233954] Max Reward on eval: 244.8655304389124[0m
[37m[1m[2023-07-11 17:06:19,313][233954] Min Reward on eval: 244.8655304389124[0m
[37m[1m[2023-07-11 17:06:19,313][233954] Mean Reward across all agents: 244.8655304389124[0m
[37m[1m[2023-07-11 17:06:19,314][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:06:24,537][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:06:24,537][233954] Reward + Measures: [[194.69615558   0.4021       0.23509999   0.4506       0.1602
    0.95227796]
 [118.0497298    0.50660002   0.3635       0.49270001   0.2027
    1.04370892]
 [318.59963612   0.2859       0.27670002   0.40869999   0.22330001
    0.85291445]
 ...
 [356.03825949   0.3585       0.27770001   0.50629997   0.1988
    0.90819472]
 [345.62982036   0.44970003   0.25819999   0.53579998   0.21609998
    0.91616625]
 [183.86297754   0.53190005   0.29170001   0.56489998   0.1877
    0.88039225]][0m
[37m[1m[2023-07-11 17:06:24,538][233954] Max Reward on eval: 463.39072896391156[0m
[37m[1m[2023-07-11 17:06:24,538][233954] Min Reward on eval: 55.896922166645524[0m
[37m[1m[2023-07-11 17:06:24,538][233954] Mean Reward across all agents: 243.63509880583587[0m
[37m[1m[2023-07-11 17:06:24,538][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:06:24,543][233954] mean_value=-296.2055948920409, max_value=586.5434588815385[0m
[37m[1m[2023-07-11 17:06:24,546][233954] New mean coefficients: [[ 0.976197   -1.2461307  -1.6573594  -0.10971075 -0.7791371  -1.5601962 ]][0m
[37m[1m[2023-07-11 17:06:24,547][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:06:33,430][233954] train() took 8.88 seconds to complete[0m
[36m[2023-07-11 17:06:33,430][233954] FPS: 432377.99[0m
[36m[2023-07-11 17:06:33,432][233954] itr=1242, itrs=2000, Progress: 62.10%[0m
[36m[2023-07-11 17:06:45,107][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 17:06:45,107][233954] FPS: 331757.82[0m
[36m[2023-07-11 17:06:49,333][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:06:49,334][233954] Reward + Measures: [[302.59264623   0.30951732   0.21873966   0.41910163   0.18345332
    0.82532293]][0m
[37m[1m[2023-07-11 17:06:49,334][233954] Max Reward on eval: 302.5926462302893[0m
[37m[1m[2023-07-11 17:06:49,334][233954] Min Reward on eval: 302.5926462302893[0m
[37m[1m[2023-07-11 17:06:49,334][233954] Mean Reward across all agents: 302.5926462302893[0m
[37m[1m[2023-07-11 17:06:49,335][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:06:54,263][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:06:54,264][233954] Reward + Measures: [[ 72.77488613   0.49370003   0.28580001   0.58950007   0.2119
    0.80953854]
 [319.25746724   0.40669999   0.22060001   0.47830001   0.17589998
    0.88747352]
 [ 87.38658588   0.36490002   0.55669999   0.45609999   0.41929999
    0.89893931]
 ...
 [301.39669804   0.37859997   0.27329999   0.57920003   0.26390001
    0.79381913]
 [350.94575406   0.4436       0.23600002   0.56639999   0.23469999
    0.94285899]
 [208.48302271   0.36919999   0.2552       0.44749999   0.1981
    0.95356554]][0m
[37m[1m[2023-07-11 17:06:54,264][233954] Max Reward on eval: 399.36309813763944[0m
[37m[1m[2023-07-11 17:06:54,264][233954] Min Reward on eval: 28.248771719820798[0m
[37m[1m[2023-07-11 17:06:54,264][233954] Mean Reward across all agents: 238.0588228209015[0m
[37m[1m[2023-07-11 17:06:54,265][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:06:54,270][233954] mean_value=-255.78114845178996, max_value=540.3197969202128[0m
[37m[1m[2023-07-11 17:06:54,273][233954] New mean coefficients: [[ 0.7662366  -0.05240119 -2.1738994  -0.23908499 -0.25915718 -1.77442   ]][0m
[37m[1m[2023-07-11 17:06:54,274][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:07:03,241][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 17:07:03,241][233954] FPS: 428301.35[0m
[36m[2023-07-11 17:07:03,244][233954] itr=1243, itrs=2000, Progress: 62.15%[0m
[36m[2023-07-11 17:07:14,974][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 17:07:14,975][233954] FPS: 330244.92[0m
[36m[2023-07-11 17:07:19,247][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:07:19,247][233954] Reward + Measures: [[308.25914441   0.28734598   0.18823932   0.44010165   0.20755734
    0.72438598]][0m
[37m[1m[2023-07-11 17:07:19,248][233954] Max Reward on eval: 308.25914440828063[0m
[37m[1m[2023-07-11 17:07:19,248][233954] Min Reward on eval: 308.25914440828063[0m
[37m[1m[2023-07-11 17:07:19,248][233954] Mean Reward across all agents: 308.25914440828063[0m
[37m[1m[2023-07-11 17:07:19,248][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:07:24,148][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:07:24,148][233954] Reward + Measures: [[160.26262095   0.40089998   0.20560001   0.45270005   0.18840002
    0.95456135]
 [249.73112893   0.41610003   0.22679999   0.49690005   0.17910001
    0.86997539]
 [254.03267457   0.40769997   0.20900002   0.54790002   0.21440001
    0.87941849]
 ...
 [154.53150655   0.24749999   0.14850001   0.3116       0.1821
    0.87589854]
 [384.31767465   0.3908       0.223        0.55459994   0.20159999
    0.83655035]
 [ 59.80576607   0.39480004   0.292        0.49730006   0.35680002
    0.7533139 ]][0m
[37m[1m[2023-07-11 17:07:24,149][233954] Max Reward on eval: 416.93392179058867[0m
[37m[1m[2023-07-11 17:07:24,149][233954] Min Reward on eval: 34.93326853085309[0m
[37m[1m[2023-07-11 17:07:24,149][233954] Mean Reward across all agents: 226.73134140747268[0m
[37m[1m[2023-07-11 17:07:24,149][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:07:24,154][233954] mean_value=-251.08838389431733, max_value=578.509783961323[0m
[37m[1m[2023-07-11 17:07:24,157][233954] New mean coefficients: [[ 1.2035949   0.1443316  -1.8950706  -0.20641956 -0.2606081  -1.5298787 ]][0m
[37m[1m[2023-07-11 17:07:24,158][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:07:33,141][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 17:07:33,142][233954] FPS: 427527.58[0m
[36m[2023-07-11 17:07:33,144][233954] itr=1244, itrs=2000, Progress: 62.20%[0m
[36m[2023-07-11 17:07:44,696][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 17:07:44,697][233954] FPS: 335200.80[0m
[36m[2023-07-11 17:07:48,966][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:07:48,966][233954] Reward + Measures: [[384.42001217   0.25731933   0.179223     0.45449764   0.22261433
    0.65481955]][0m
[37m[1m[2023-07-11 17:07:48,967][233954] Max Reward on eval: 384.4200121717217[0m
[37m[1m[2023-07-11 17:07:48,967][233954] Min Reward on eval: 384.4200121717217[0m
[37m[1m[2023-07-11 17:07:48,967][233954] Mean Reward across all agents: 384.4200121717217[0m
[37m[1m[2023-07-11 17:07:48,967][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:07:53,982][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:07:54,042][233954] Reward + Measures: [[336.801728     0.35859999   0.3326       0.52419996   0.22490001
    0.86844206]
 [340.38051132   0.25310001   0.18719999   0.45479998   0.199
    0.88156456]
 [ 93.71040711   0.58740008   0.2476       0.60210001   0.177
    0.79892677]
 ...
 [119.68415949   0.69200009   0.35880002   0.68540001   0.06330001
    0.76173228]
 [232.85444452   0.29660001   0.40869999   0.3175       0.3642
    0.86990088]
 [277.16499089   0.20630001   0.29100001   0.28529999   0.25279999
    0.95011216]][0m
[37m[1m[2023-07-11 17:07:54,042][233954] Max Reward on eval: 530.2668037559837[0m
[37m[1m[2023-07-11 17:07:54,042][233954] Min Reward on eval: -8.286148908548057[0m
[37m[1m[2023-07-11 17:07:54,042][233954] Mean Reward across all agents: 278.67965292022217[0m
[37m[1m[2023-07-11 17:07:54,043][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:07:54,051][233954] mean_value=-107.66847698117766, max_value=823.0342292509973[0m
[37m[1m[2023-07-11 17:07:54,054][233954] New mean coefficients: [[ 1.6582837   1.3218132  -1.9565184   1.8126816  -0.28781673 -1.9237367 ]][0m
[37m[1m[2023-07-11 17:07:54,055][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:08:03,125][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 17:08:03,126][233954] FPS: 423417.65[0m
[36m[2023-07-11 17:08:03,128][233954] itr=1245, itrs=2000, Progress: 62.25%[0m
[36m[2023-07-11 17:08:14,941][233954] train() took 11.72 seconds to complete[0m
[36m[2023-07-11 17:08:14,941][233954] FPS: 327770.90[0m
[36m[2023-07-11 17:08:19,284][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:08:19,284][233954] Reward + Measures: [[451.76128156   0.29584733   0.16901599   0.51916033   0.23719499
    0.61151737]][0m
[37m[1m[2023-07-11 17:08:19,284][233954] Max Reward on eval: 451.7612815562259[0m
[37m[1m[2023-07-11 17:08:19,285][233954] Min Reward on eval: 451.7612815562259[0m
[37m[1m[2023-07-11 17:08:19,285][233954] Mean Reward across all agents: 451.7612815562259[0m
[37m[1m[2023-07-11 17:08:19,285][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:08:24,277][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:08:24,277][233954] Reward + Measures: [[236.07646942   0.39760002   0.26770002   0.51380002   0.31009999
    0.68479264]
 [452.15425871   0.4233       0.19050001   0.62360001   0.24840002
    0.65348834]
 [222.29413322   0.46329999   0.3804       0.59920001   0.26800001
    0.59911567]
 ...
 [284.11839583   0.2784       0.14420001   0.44899997   0.24510001
    0.74002832]
 [448.92369078   0.3335       0.39429998   0.5492       0.2947
    0.74276423]
 [363.26309393   0.4955       0.22359999   0.70670003   0.17809999
    0.58377695]][0m
[37m[1m[2023-07-11 17:08:24,278][233954] Max Reward on eval: 585.6355170985684[0m
[37m[1m[2023-07-11 17:08:24,278][233954] Min Reward on eval: 18.862984163174406[0m
[37m[1m[2023-07-11 17:08:24,278][233954] Mean Reward across all agents: 300.7324335395379[0m
[37m[1m[2023-07-11 17:08:24,278][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:08:24,285][233954] mean_value=99.51170796473231, max_value=788.6071374669546[0m
[37m[1m[2023-07-11 17:08:24,288][233954] New mean coefficients: [[ 0.86066025  1.1031592  -2.7348628   3.3405447  -0.3511883  -1.4039958 ]][0m
[37m[1m[2023-07-11 17:08:24,289][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:08:33,329][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 17:08:33,329][233954] FPS: 424875.78[0m
[36m[2023-07-11 17:08:33,331][233954] itr=1246, itrs=2000, Progress: 62.30%[0m
[36m[2023-07-11 17:08:45,020][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 17:08:45,020][233954] FPS: 331409.11[0m
[36m[2023-07-11 17:08:49,322][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:08:49,323][233954] Reward + Measures: [[486.26361657   0.33540767   0.15792634   0.59283531   0.23265633
    0.58813035]][0m
[37m[1m[2023-07-11 17:08:49,323][233954] Max Reward on eval: 486.26361656841885[0m
[37m[1m[2023-07-11 17:08:49,323][233954] Min Reward on eval: 486.26361656841885[0m
[37m[1m[2023-07-11 17:08:49,323][233954] Mean Reward across all agents: 486.26361656841885[0m
[37m[1m[2023-07-11 17:08:49,324][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:08:54,303][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:08:54,303][233954] Reward + Measures: [[404.33294765   0.38409999   0.2043       0.60400003   0.26980004
    0.64293748]
 [205.86555983   0.38820001   0.15619999   0.60210001   0.18719999
    0.56243736]
 [302.05840663   0.30329999   0.22669999   0.51999998   0.27879998
    0.77277243]
 ...
 [280.01089858   0.49130002   0.16940001   0.62019998   0.2181
    0.61941332]
 [550.69863508   0.33460003   0.20460001   0.64910001   0.24589999
    0.59791553]
 [380.15378757   0.30860001   0.21949999   0.55059999   0.24790001
    0.64246261]][0m
[37m[1m[2023-07-11 17:08:54,304][233954] Max Reward on eval: 593.6142578093335[0m
[37m[1m[2023-07-11 17:08:54,304][233954] Min Reward on eval: 9.921351149771363[0m
[37m[1m[2023-07-11 17:08:54,304][233954] Mean Reward across all agents: 366.4121681454049[0m
[37m[1m[2023-07-11 17:08:54,304][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:08:54,310][233954] mean_value=4.010070167467238, max_value=787.3200009468012[0m
[37m[1m[2023-07-11 17:08:54,313][233954] New mean coefficients: [[ 0.91499037  1.1861123  -2.0605114   3.2661965   0.1253843  -2.4492254 ]][0m
[37m[1m[2023-07-11 17:08:54,314][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:09:03,278][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 17:09:03,278][233954] FPS: 428449.68[0m
[36m[2023-07-11 17:09:03,281][233954] itr=1247, itrs=2000, Progress: 62.35%[0m
[36m[2023-07-11 17:09:14,822][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 17:09:14,822][233954] FPS: 335534.56[0m
[36m[2023-07-11 17:09:19,071][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:09:19,072][233954] Reward + Measures: [[457.59019431   0.41100767   0.151739     0.68076003   0.20496668
    0.53158975]][0m
[37m[1m[2023-07-11 17:09:19,072][233954] Max Reward on eval: 457.59019431138023[0m
[37m[1m[2023-07-11 17:09:19,072][233954] Min Reward on eval: 457.59019431138023[0m
[37m[1m[2023-07-11 17:09:19,072][233954] Mean Reward across all agents: 457.59019431138023[0m
[37m[1m[2023-07-11 17:09:19,073][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:09:24,302][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:09:24,302][233954] Reward + Measures: [[427.12037085   0.41890001   0.1459       0.69329995   0.2287
    0.57090425]
 [269.75080402   0.40030003   0.17999999   0.63320005   0.2441
    0.6758132 ]
 [312.23653224   0.5327       0.18590002   0.74300003   0.18589999
    0.56135768]
 ...
 [331.94726387   0.38420001   0.20370002   0.59050006   0.30289999
    0.72236449]
 [322.39873966   0.36700001   0.23559999   0.4772       0.25830001
    0.64287776]
 [158.2121482    0.51990002   0.2145       0.59650004   0.16540001
    0.72460049]][0m
[37m[1m[2023-07-11 17:09:24,302][233954] Max Reward on eval: 650.5791206434369[0m
[37m[1m[2023-07-11 17:09:24,303][233954] Min Reward on eval: -39.899662427720614[0m
[37m[1m[2023-07-11 17:09:24,303][233954] Mean Reward across all agents: 280.17535027197283[0m
[37m[1m[2023-07-11 17:09:24,303][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:09:24,308][233954] mean_value=26.779403626481606, max_value=868.030357350572[0m
[37m[1m[2023-07-11 17:09:24,311][233954] New mean coefficients: [[ 1.3857689  2.5883608 -1.9431132  3.5840015  1.0489941 -2.4706237]][0m
[37m[1m[2023-07-11 17:09:24,312][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:09:33,350][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 17:09:33,351][233954] FPS: 424940.94[0m
[36m[2023-07-11 17:09:33,353][233954] itr=1248, itrs=2000, Progress: 62.40%[0m
[36m[2023-07-11 17:09:44,906][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 17:09:44,911][233954] FPS: 335208.15[0m
[36m[2023-07-11 17:09:49,164][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:09:49,165][233954] Reward + Measures: [[365.76156582   0.53300637   0.147967     0.77468604   0.18372433
    0.47393465]][0m
[37m[1m[2023-07-11 17:09:49,165][233954] Max Reward on eval: 365.76156581711723[0m
[37m[1m[2023-07-11 17:09:49,165][233954] Min Reward on eval: 365.76156581711723[0m
[37m[1m[2023-07-11 17:09:49,166][233954] Mean Reward across all agents: 365.76156581711723[0m
[37m[1m[2023-07-11 17:09:49,166][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:09:54,122][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:09:54,123][233954] Reward + Measures: [[233.57385916   0.65189999   0.20180002   0.79320002   0.1778
    0.50732917]
 [279.0213804    0.60589999   0.17130001   0.68390006   0.24420002
    0.71890545]
 [187.79644013   0.59440005   0.16949999   0.77600002   0.26659998
    0.64118975]
 ...
 [208.13582802   0.6002       0.13330001   0.74080002   0.29450002
    0.7459293 ]
 [207.41310786   0.5948       0.18920001   0.70240003   0.22140001
    0.73044533]
 [265.35057454   0.56739998   0.13970001   0.76950008   0.31399998
    0.69120395]][0m
[37m[1m[2023-07-11 17:09:54,123][233954] Max Reward on eval: 451.636787423864[0m
[37m[1m[2023-07-11 17:09:54,123][233954] Min Reward on eval: 44.01214439272881[0m
[37m[1m[2023-07-11 17:09:54,124][233954] Mean Reward across all agents: 257.31497209563184[0m
[37m[1m[2023-07-11 17:09:54,124][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:09:54,130][233954] mean_value=169.41708260169491, max_value=793.3216095025675[0m
[37m[1m[2023-07-11 17:09:54,133][233954] New mean coefficients: [[ 0.7158483  1.8238497 -1.6932833  3.5615158  1.8778503 -1.6008162]][0m
[37m[1m[2023-07-11 17:09:54,134][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:10:03,094][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 17:10:03,094][233954] FPS: 428646.14[0m
[36m[2023-07-11 17:10:03,096][233954] itr=1249, itrs=2000, Progress: 62.45%[0m
[36m[2023-07-11 17:10:14,639][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 17:10:14,639][233954] FPS: 335472.05[0m
[36m[2023-07-11 17:10:18,880][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:10:18,880][233954] Reward + Measures: [[272.09547111   0.66792065   0.14947599   0.85901332   0.14121799
    0.43985042]][0m
[37m[1m[2023-07-11 17:10:18,881][233954] Max Reward on eval: 272.0954711068925[0m
[37m[1m[2023-07-11 17:10:18,881][233954] Min Reward on eval: 272.0954711068925[0m
[37m[1m[2023-07-11 17:10:18,881][233954] Mean Reward across all agents: 272.0954711068925[0m
[37m[1m[2023-07-11 17:10:18,881][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:10:23,804][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:10:23,805][233954] Reward + Measures: [[186.41698074   0.61000007   0.19629999   0.86140007   0.19149999
    0.53710818]
 [169.31354813   0.58450001   0.20469999   0.80779999   0.18270002
    0.57862157]
 [293.34014703   0.57579994   0.1533       0.79570001   0.21229999
    0.52495301]
 ...
 [393.64493945   0.4849       0.17580001   0.76120001   0.28459999
    0.46584114]
 [230.63034347   0.66479999   0.17730001   0.82700008   0.1184
    0.52729094]
 [ 14.57255838   0.56309998   0.1556       0.75929993   0.26910001
    0.69341177]][0m
[37m[1m[2023-07-11 17:10:23,805][233954] Max Reward on eval: 532.213829056127[0m
[37m[1m[2023-07-11 17:10:23,805][233954] Min Reward on eval: -75.36892828447745[0m
[37m[1m[2023-07-11 17:10:23,805][233954] Mean Reward across all agents: 164.9928881820133[0m
[37m[1m[2023-07-11 17:10:23,806][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:10:23,812][233954] mean_value=103.87549629569612, max_value=843.6194843592103[0m
[37m[1m[2023-07-11 17:10:23,814][233954] New mean coefficients: [[ 0.1349653   1.9999886  -1.611134    3.2784774   0.97534674 -2.1248791 ]][0m
[37m[1m[2023-07-11 17:10:23,815][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:10:32,736][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 17:10:32,737][233954] FPS: 430524.44[0m
[36m[2023-07-11 17:10:32,739][233954] itr=1250, itrs=2000, Progress: 62.50%[0m
[37m[1m[2023-07-11 17:14:08,753][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001230[0m
[36m[2023-07-11 17:14:20,967][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 17:14:20,968][233954] FPS: 331273.44[0m
[36m[2023-07-11 17:14:25,116][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:14:25,116][233954] Reward + Measures: [[149.54873206   0.7776053    0.16364001   0.92217433   0.114562
    0.41331452]][0m
[37m[1m[2023-07-11 17:14:25,117][233954] Max Reward on eval: 149.54873205660124[0m
[37m[1m[2023-07-11 17:14:25,117][233954] Min Reward on eval: 149.54873205660124[0m
[37m[1m[2023-07-11 17:14:25,117][233954] Mean Reward across all agents: 149.54873205660124[0m
[37m[1m[2023-07-11 17:14:25,117][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:14:30,032][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:14:30,033][233954] Reward + Measures: [[169.89802742   0.71140003   0.16069999   0.87039995   0.1943
    0.4983708 ]
 [ 29.1310967    0.80019999   0.2933       0.87420005   0.093
    0.37173349]
 [145.0684824    0.51719999   0.16950002   0.70180005   0.21490002
    0.65071994]
 ...
 [156.28433598   0.64020002   0.15550001   0.78010005   0.18260001
    0.64744836]
 [ 61.93950067   0.80220002   0.2041       0.92629999   0.14749999
    0.41070792]
 [ 61.9362906    0.63060004   0.31860003   0.76389998   0.25479999
    0.45690933]][0m
[37m[1m[2023-07-11 17:14:30,033][233954] Max Reward on eval: 289.1890144358855[0m
[37m[1m[2023-07-11 17:14:30,033][233954] Min Reward on eval: -50.38379905764013[0m
[37m[1m[2023-07-11 17:14:30,033][233954] Mean Reward across all agents: 115.19434854074944[0m
[37m[1m[2023-07-11 17:14:30,034][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:14:30,041][233954] mean_value=143.7054361282384, max_value=744.9510642454028[0m
[37m[1m[2023-07-11 17:14:30,044][233954] New mean coefficients: [[ 0.2320376   2.0246606  -0.17970347  3.120038    0.8063655  -3.144755  ]][0m
[37m[1m[2023-07-11 17:14:30,045][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:14:39,013][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 17:14:39,014][233954] FPS: 428234.22[0m
[36m[2023-07-11 17:14:39,016][233954] itr=1251, itrs=2000, Progress: 62.55%[0m
[36m[2023-07-11 17:14:50,671][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 17:14:50,672][233954] FPS: 332360.25[0m
[36m[2023-07-11 17:14:54,965][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:14:54,965][233954] Reward + Measures: [[80.64813546  0.86977434  0.20018932  0.95563543  0.07244466  0.36945239]][0m
[37m[1m[2023-07-11 17:14:54,966][233954] Max Reward on eval: 80.64813546192978[0m
[37m[1m[2023-07-11 17:14:54,966][233954] Min Reward on eval: 80.64813546192978[0m
[37m[1m[2023-07-11 17:14:54,966][233954] Mean Reward across all agents: 80.64813546192978[0m
[37m[1m[2023-07-11 17:14:54,966][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:15:00,164][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:15:00,165][233954] Reward + Measures: [[187.06024311   0.63300002   0.3937       0.63910002   0.0315
    0.79263544]
 [ 39.6619637    0.88529998   0.43660003   0.89770001   0.0302
    0.37454087]
 [ 93.88942204   0.56739998   0.18429999   0.72259998   0.24249999
    0.63616878]
 ...
 [ 37.85197423   0.91990006   0.4973       0.87519997   0.0123
    0.40276384]
 [134.22742082   0.80159998   0.2217       0.87280005   0.1346
    0.49206534]
 [  9.84244556   0.63850003   0.21089999   0.76860005   0.2457
    0.61682802]][0m
[37m[1m[2023-07-11 17:15:00,165][233954] Max Reward on eval: 401.4828625250608[0m
[37m[1m[2023-07-11 17:15:00,165][233954] Min Reward on eval: 7.748004050925374[0m
[37m[1m[2023-07-11 17:15:00,165][233954] Mean Reward across all agents: 133.0520543556275[0m
[37m[1m[2023-07-11 17:15:00,166][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:15:00,172][233954] mean_value=44.66921686563528, max_value=767.9956712944434[0m
[37m[1m[2023-07-11 17:15:00,175][233954] New mean coefficients: [[ 0.5074265   1.9983442  -0.78927624  1.8128506  -0.65018594 -4.0245304 ]][0m
[37m[1m[2023-07-11 17:15:00,176][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:15:09,120][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 17:15:09,120][233954] FPS: 429401.64[0m
[36m[2023-07-11 17:15:09,123][233954] itr=1252, itrs=2000, Progress: 62.60%[0m
[36m[2023-07-11 17:15:20,879][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 17:15:20,879][233954] FPS: 329402.95[0m
[36m[2023-07-11 17:15:25,158][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:15:25,158][233954] Reward + Measures: [[41.95933937  0.94564372  0.268446    0.97967428  0.02826767  0.29650849]][0m
[37m[1m[2023-07-11 17:15:25,159][233954] Max Reward on eval: 41.95933936956812[0m
[37m[1m[2023-07-11 17:15:25,159][233954] Min Reward on eval: 41.95933936956812[0m
[37m[1m[2023-07-11 17:15:25,159][233954] Mean Reward across all agents: 41.95933936956812[0m
[37m[1m[2023-07-11 17:15:25,159][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:15:30,145][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:15:30,145][233954] Reward + Measures: [[35.40212629  0.9594      0.31619999  0.98210001  0.0169      0.33965549]
 [38.43890215  0.8531      0.27320001  0.94200003  0.0785      0.41142365]
 [51.07343846  0.92449999  0.4082      0.97430003  0.0097      0.39140815]
 ...
 [62.48057605  0.8860001   0.44430003  0.92819995  0.0153      0.33438236]
 [85.90569594  0.79240006  0.1332      0.92629999  0.13360001  0.46937791]
 [59.51132523  0.92320007  0.29090002  0.95740002  0.0301      0.32413378]][0m
[37m[1m[2023-07-11 17:15:30,145][233954] Max Reward on eval: 235.86632918883114[0m
[37m[1m[2023-07-11 17:15:30,146][233954] Min Reward on eval: -35.442534177517516[0m
[37m[1m[2023-07-11 17:15:30,146][233954] Mean Reward across all agents: 62.68439118622654[0m
[37m[1m[2023-07-11 17:15:30,146][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:15:30,152][233954] mean_value=42.272096155743924, max_value=589.5635995902121[0m
[37m[1m[2023-07-11 17:15:30,155][233954] New mean coefficients: [[ 0.99589086  1.7725817   0.33982646  1.5943601  -0.8484044  -4.2353415 ]][0m
[37m[1m[2023-07-11 17:15:30,156][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:15:39,094][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 17:15:39,094][233954] FPS: 429710.59[0m
[36m[2023-07-11 17:15:39,096][233954] itr=1253, itrs=2000, Progress: 62.65%[0m
[36m[2023-07-11 17:15:50,758][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 17:15:50,759][233954] FPS: 332141.57[0m
[36m[2023-07-11 17:15:55,051][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:15:55,051][233954] Reward + Measures: [[23.13194973  0.97897297  0.34483469  0.99009669  0.00909433  0.24789517]][0m
[37m[1m[2023-07-11 17:15:55,051][233954] Max Reward on eval: 23.13194973260849[0m
[37m[1m[2023-07-11 17:15:55,052][233954] Min Reward on eval: 23.13194973260849[0m
[37m[1m[2023-07-11 17:15:55,052][233954] Mean Reward across all agents: 23.13194973260849[0m
[37m[1m[2023-07-11 17:15:55,052][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:16:00,064][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:16:00,065][233954] Reward + Measures: [[174.31803038   0.37989998   0.49419999   0.57440001   0.4718
    0.46573791]
 [ 31.20814574   0.94300002   0.2613       0.97329998   0.0263
    0.62907457]
 [ 97.60059597   0.86260003   0.36970004   0.94979995   0.0816
    0.47401938]
 ...
 [ 42.37128665   0.9483       0.36500001   0.9429       0.0068
    0.37799892]
 [229.09747219   0.40380001   0.67449999   0.60000002   0.60370004
    0.5045498 ]
 [ 32.19649272   0.94550002   0.22860001   0.96999997   0.072
    1.17019641]][0m
[37m[1m[2023-07-11 17:16:00,065][233954] Max Reward on eval: 336.08570096220353[0m
[37m[1m[2023-07-11 17:16:00,065][233954] Min Reward on eval: -56.58301286962815[0m
[37m[1m[2023-07-11 17:16:00,065][233954] Mean Reward across all agents: 74.03784661985918[0m
[37m[1m[2023-07-11 17:16:00,066][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:16:00,075][233954] mean_value=159.18602717545005, max_value=681.2057025432587[0m
[37m[1m[2023-07-11 17:16:00,078][233954] New mean coefficients: [[ 2.2695584   1.7902403   1.0481869   1.1782663  -0.85028595 -4.8733697 ]][0m
[37m[1m[2023-07-11 17:16:00,079][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:16:09,155][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 17:16:09,156][233954] FPS: 423158.45[0m
[36m[2023-07-11 17:16:09,158][233954] itr=1254, itrs=2000, Progress: 62.70%[0m
[36m[2023-07-11 17:16:20,991][233954] train() took 11.74 seconds to complete[0m
[36m[2023-07-11 17:16:20,991][233954] FPS: 327212.64[0m
[36m[2023-07-11 17:16:25,319][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:16:25,320][233954] Reward + Measures: [[220.45779659   0.67791331   0.75833464   0.46134835   0.516662
    0.69286525]][0m
[37m[1m[2023-07-11 17:16:25,320][233954] Max Reward on eval: 220.4577965948979[0m
[37m[1m[2023-07-11 17:16:25,320][233954] Min Reward on eval: 220.4577965948979[0m
[37m[1m[2023-07-11 17:16:25,320][233954] Mean Reward across all agents: 220.4577965948979[0m
[37m[1m[2023-07-11 17:16:25,320][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:16:30,311][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:16:30,311][233954] Reward + Measures: [[147.04288483   0.67589998   0.66140002   0.50810003   0.40279999
    0.83718663]
 [ 65.15147157   0.69979995   0.7209       0.58490002   0.46339998
    0.67261851]
 [173.93588256   0.72949994   0.74270004   0.48919997   0.50490004
    0.67885274]
 ...
 [ 68.6371207    0.75490004   0.72139996   0.57250005   0.42409998
    0.67428559]
 [116.58882141   0.77749997   0.68980002   0.60580009   0.41809997
    0.6267857 ]
 [ 74.47635411   0.44070002   0.815        0.233        0.64579999
    0.68077523]][0m
[37m[1m[2023-07-11 17:16:30,312][233954] Max Reward on eval: 261.61445808764546[0m
[37m[1m[2023-07-11 17:16:30,312][233954] Min Reward on eval: -111.59241008083336[0m
[37m[1m[2023-07-11 17:16:30,312][233954] Mean Reward across all agents: 114.50956462757308[0m
[37m[1m[2023-07-11 17:16:30,312][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:16:30,323][233954] mean_value=218.8236083585736, max_value=656.0548754860415[0m
[37m[1m[2023-07-11 17:16:30,326][233954] New mean coefficients: [[ 1.8702945   1.6293089   0.95281947  2.555221   -0.9039152  -5.378585  ]][0m
[37m[1m[2023-07-11 17:16:30,327][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:16:39,309][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 17:16:39,310][233954] FPS: 427567.79[0m
[36m[2023-07-11 17:16:39,312][233954] itr=1255, itrs=2000, Progress: 62.75%[0m
[36m[2023-07-11 17:16:51,009][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 17:16:51,010][233954] FPS: 331169.03[0m
[36m[2023-07-11 17:16:55,320][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:16:55,321][233954] Reward + Measures: [[104.43644775   0.83323067   0.82405066   0.61186361   0.54685432
    0.51919794]][0m
[37m[1m[2023-07-11 17:16:55,321][233954] Max Reward on eval: 104.43644775254462[0m
[37m[1m[2023-07-11 17:16:55,321][233954] Min Reward on eval: 104.43644775254462[0m
[37m[1m[2023-07-11 17:16:55,322][233954] Mean Reward across all agents: 104.43644775254462[0m
[37m[1m[2023-07-11 17:16:55,322][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:17:00,275][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:17:00,276][233954] Reward + Measures: [[ 90.8900318    0.93850005   0.94729996   0.84130001   0.7475
    0.57107466]
 [ 23.15958593   0.97269994   0.94560003   0.85740006   0.33940002
    0.70906287]
 [ 92.41595457   0.9084       0.8096       0.78540003   0.0166
    0.56980646]
 ...
 [-35.96162902   0.94629997   0.94580001   0.8563       0.39260003
    0.49484062]
 [ -3.47307368   0.92799997   0.88199997   0.787        0.12060001
    0.54948705]
 [ 67.66047048   0.92360002   0.95410007   0.82460004   0.88199997
    0.66954643]][0m
[37m[1m[2023-07-11 17:17:00,276][233954] Max Reward on eval: 153.54473782838323[0m
[37m[1m[2023-07-11 17:17:00,276][233954] Min Reward on eval: -147.26060391422362[0m
[37m[1m[2023-07-11 17:17:00,276][233954] Mean Reward across all agents: 26.437871685515194[0m
[37m[1m[2023-07-11 17:17:00,277][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:17:00,288][233954] mean_value=343.35409235049656, max_value=635.8364429354667[0m
[37m[1m[2023-07-11 17:17:00,290][233954] New mean coefficients: [[ 2.9610367  3.3645816  0.4306385  2.8276803 -0.7236167 -5.114521 ]][0m
[37m[1m[2023-07-11 17:17:00,291][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:17:09,216][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 17:17:09,216][233954] FPS: 430333.80[0m
[36m[2023-07-11 17:17:09,219][233954] itr=1256, itrs=2000, Progress: 62.80%[0m
[36m[2023-07-11 17:17:20,961][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 17:17:20,961][233954] FPS: 329869.90[0m
[36m[2023-07-11 17:17:25,258][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:17:25,259][233954] Reward + Measures: [[48.09371659  0.71679598  0.83187693  0.81475163  0.54630268  0.63352805]][0m
[37m[1m[2023-07-11 17:17:25,259][233954] Max Reward on eval: 48.09371659270884[0m
[37m[1m[2023-07-11 17:17:25,259][233954] Min Reward on eval: 48.09371659270884[0m
[37m[1m[2023-07-11 17:17:25,259][233954] Mean Reward across all agents: 48.09371659270884[0m
[37m[1m[2023-07-11 17:17:25,260][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:17:30,229][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:17:30,230][233954] Reward + Measures: [[-22.94120969   0.91880006   0.97259998   0.95510006   0.93370003
    0.74836195]
 [ 61.06034493   0.61440003   0.88550007   0.71380001   0.79040003
    0.64472479]
 [  9.63557652   0.80870003   0.84060001   0.86790001   0.78790009
    0.72169036]
 ...
 [-22.14830223   0.81349993   0.76429999   0.89630002   0.55470002
    0.573448  ]
 [-11.4469442    0.42989999   0.86700004   0.36210003   0.86790001
    0.64209098]
 [  6.93793079   0.72389996   0.96869993   0.79250002   0.96689999
    0.57531214]][0m
[37m[1m[2023-07-11 17:17:30,230][233954] Max Reward on eval: 137.94755746880546[0m
[37m[1m[2023-07-11 17:17:30,230][233954] Min Reward on eval: -39.56272231386974[0m
[37m[1m[2023-07-11 17:17:30,231][233954] Mean Reward across all agents: 11.379553832018928[0m
[37m[1m[2023-07-11 17:17:30,231][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:17:30,241][233954] mean_value=212.23862985955148, max_value=544.3708208467997[0m
[37m[1m[2023-07-11 17:17:30,244][233954] New mean coefficients: [[ 2.664312   3.8926392 -1.0860419  2.3252237 -1.0479903 -4.138417 ]][0m
[37m[1m[2023-07-11 17:17:30,245][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:17:39,330][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 17:17:39,330][233954] FPS: 422742.75[0m
[36m[2023-07-11 17:17:39,332][233954] itr=1257, itrs=2000, Progress: 62.85%[0m
[36m[2023-07-11 17:17:51,005][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 17:17:51,005][233954] FPS: 331892.44[0m
[36m[2023-07-11 17:17:55,239][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:17:55,239][233954] Reward + Measures: [[81.60736781  0.95102865  0.93496662  0.98728973  0.00224267  1.15376258]][0m
[37m[1m[2023-07-11 17:17:55,239][233954] Max Reward on eval: 81.60736781166284[0m
[37m[1m[2023-07-11 17:17:55,240][233954] Min Reward on eval: 81.60736781166284[0m
[37m[1m[2023-07-11 17:17:55,240][233954] Mean Reward across all agents: 81.60736781166284[0m
[37m[1m[2023-07-11 17:17:55,240][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:18:00,453][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:18:00,454][233954] Reward + Measures: [[112.7006931    0.80369997   0.49090001   0.90350002   0.0193
    1.13618624]
 [ 70.26345258   0.77169997   0.50060004   0.85500002   0.07120001
    0.9806456 ]
 [ 71.25679063   0.94150001   0.91769999   0.98820001   0.0007
    1.00450194]
 ...
 [104.47306918   0.78970003   0.54720002   0.90749997   0.0163
    1.17974114]
 [ 72.46661065   0.86479998   0.60140002   0.8955       0.0439
    0.88235581]
 [ 91.60251526   0.76540005   0.57989997   0.88970006   0.0192
    1.08865201]][0m
[37m[1m[2023-07-11 17:18:00,454][233954] Max Reward on eval: 167.4612130991649[0m
[37m[1m[2023-07-11 17:18:00,455][233954] Min Reward on eval: 29.314760099444538[0m
[37m[1m[2023-07-11 17:18:00,455][233954] Mean Reward across all agents: 97.67509102564088[0m
[37m[1m[2023-07-11 17:18:00,455][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:18:00,463][233954] mean_value=302.0975071148909, max_value=558.5117039304599[0m
[37m[1m[2023-07-11 17:18:00,466][233954] New mean coefficients: [[ 1.882784   4.6734385 -1.7588655  3.0776563 -0.9168206 -4.4329495]][0m
[37m[1m[2023-07-11 17:18:00,467][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:18:09,531][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 17:18:09,532][233954] FPS: 423716.74[0m
[36m[2023-07-11 17:18:09,534][233954] itr=1258, itrs=2000, Progress: 62.90%[0m
[36m[2023-07-11 17:18:21,239][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 17:18:21,239][233954] FPS: 330982.59[0m
[36m[2023-07-11 17:18:25,481][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:18:25,482][233954] Reward + Measures: [[108.68684288   0.84938502   0.66748333   0.85935694   0.004939
    0.35377872]][0m
[37m[1m[2023-07-11 17:18:25,482][233954] Max Reward on eval: 108.68684287673685[0m
[37m[1m[2023-07-11 17:18:25,482][233954] Min Reward on eval: 108.68684287673685[0m
[37m[1m[2023-07-11 17:18:25,482][233954] Mean Reward across all agents: 108.68684287673685[0m
[37m[1m[2023-07-11 17:18:25,483][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:18:30,452][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:18:30,458][233954] Reward + Measures: [[-39.48465627   0.91379994   0.73710001   0.87939996   0.0021
    0.54902351]
 [ 75.75766229   0.90710002   0.66619998   0.84639996   0.0031
    0.35098436]
 [ 56.67748833   0.87279999   0.51880002   0.77160001   0.0176
    0.45003006]
 ...
 [ 14.08766339   0.86519998   0.49249998   0.75949997   0.0342
    0.36280021]
 [ -1.51886687   0.89860004   0.59799999   0.82130003   0.0107
    0.34680688]
 [ 18.25008147   0.95740002   0.73150003   0.89209998   0.002
    0.38530517]][0m
[37m[1m[2023-07-11 17:18:30,458][233954] Max Reward on eval: 268.33478738535194[0m
[37m[1m[2023-07-11 17:18:30,459][233954] Min Reward on eval: -45.78079189532436[0m
[37m[1m[2023-07-11 17:18:30,459][233954] Mean Reward across all agents: 48.72540004173055[0m
[37m[1m[2023-07-11 17:18:30,459][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:18:30,467][233954] mean_value=175.8103340681491, max_value=605.306902863239[0m
[37m[1m[2023-07-11 17:18:30,470][233954] New mean coefficients: [[ 1.6518673   3.5820632   0.044258    2.4342096  -0.64800024 -5.903842  ]][0m
[37m[1m[2023-07-11 17:18:30,471][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:18:39,447][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 17:18:39,447][233954] FPS: 427882.24[0m
[36m[2023-07-11 17:18:39,449][233954] itr=1259, itrs=2000, Progress: 62.95%[0m
[36m[2023-07-11 17:18:51,206][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 17:18:51,206][233954] FPS: 329413.21[0m
[36m[2023-07-11 17:18:55,462][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:18:55,462][233954] Reward + Measures: [[20.18919869  0.99458033  0.90594465  0.97311991  0.002333    0.23200467]][0m
[37m[1m[2023-07-11 17:18:55,463][233954] Max Reward on eval: 20.189198693753074[0m
[37m[1m[2023-07-11 17:18:55,463][233954] Min Reward on eval: 20.189198693753074[0m
[37m[1m[2023-07-11 17:18:55,463][233954] Mean Reward across all agents: 20.189198693753074[0m
[37m[1m[2023-07-11 17:18:55,463][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:19:00,405][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:19:00,406][233954] Reward + Measures: [[-3.45764048  0.90950006  0.59639996  0.83260006  0.0123      0.39532605]
 [82.74953557  0.70430005  0.49169999  0.67620003  0.2112      0.59978354]
 [38.97670862  0.88339996  0.49899998  0.88710004  0.0306      0.31797051]
 ...
 [46.95087857  0.83390009  0.50410002  0.815       0.048       0.39112279]
 [53.23463823  0.75819999  0.55010003  0.7471      0.11299999  0.49072543]
 [29.82097079  0.95110005  0.72799999  0.8969      0.0032      0.33621427]][0m
[37m[1m[2023-07-11 17:19:00,406][233954] Max Reward on eval: 202.2408237548545[0m
[37m[1m[2023-07-11 17:19:00,406][233954] Min Reward on eval: -106.08943224446848[0m
[37m[1m[2023-07-11 17:19:00,406][233954] Mean Reward across all agents: 21.133670800898862[0m
[37m[1m[2023-07-11 17:19:00,406][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:19:00,412][233954] mean_value=35.48330352594021, max_value=542.8065874549095[0m
[37m[1m[2023-07-11 17:19:00,414][233954] New mean coefficients: [[ 2.125242    2.4069762   0.86683774  1.1653644  -0.11783683 -5.0269256 ]][0m
[37m[1m[2023-07-11 17:19:00,415][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:19:09,480][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 17:19:09,480][233954] FPS: 423702.52[0m
[36m[2023-07-11 17:19:09,482][233954] itr=1260, itrs=2000, Progress: 63.00%[0m
[37m[1m[2023-07-11 17:22:43,562][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001240[0m
[36m[2023-07-11 17:22:55,976][233954] train() took 11.72 seconds to complete[0m
[36m[2023-07-11 17:22:55,977][233954] FPS: 327744.37[0m
[36m[2023-07-11 17:23:00,182][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:23:00,183][233954] Reward + Measures: [[42.10682712  0.97758901  0.42259097  0.97702229  0.06219233  0.43785521]][0m
[37m[1m[2023-07-11 17:23:00,183][233954] Max Reward on eval: 42.10682712128379[0m
[37m[1m[2023-07-11 17:23:00,183][233954] Min Reward on eval: 42.10682712128379[0m
[37m[1m[2023-07-11 17:23:00,184][233954] Mean Reward across all agents: 42.10682712128379[0m
[37m[1m[2023-07-11 17:23:00,184][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:23:05,131][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:23:05,132][233954] Reward + Measures: [[39.96285928  0.86110002  0.95990002  0.21689999  0.93189996  1.09520471]
 [22.50020462  0.91019994  0.64350003  0.91520005  0.48210001  0.38646513]
 [39.3154111   0.97039998  0.5966      0.88670009  0.51790005  1.37009037]
 ...
 [10.55221252  0.93720001  0.62729996  0.79500002  0.48319998  0.33173957]
 [-0.16042146  0.95749998  0.31369999  0.94849998  0.1027      0.36942863]
 [55.41423321  0.92989999  0.64209998  0.89899999  0.55660003  1.47327387]][0m
[37m[1m[2023-07-11 17:23:05,132][233954] Max Reward on eval: 167.67673444338143[0m
[37m[1m[2023-07-11 17:23:05,132][233954] Min Reward on eval: -128.2535596040194[0m
[37m[1m[2023-07-11 17:23:05,132][233954] Mean Reward across all agents: 32.80238557996713[0m
[37m[1m[2023-07-11 17:23:05,133][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:23:05,150][233954] mean_value=276.47402041354263, max_value=581.9937863399275[0m
[37m[1m[2023-07-11 17:23:05,152][233954] New mean coefficients: [[ 3.2994664   2.4005527   0.57314897  1.9410827  -0.43887758 -5.4086957 ]][0m
[37m[1m[2023-07-11 17:23:05,154][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:23:14,071][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 17:23:14,071][233954] FPS: 430698.00[0m
[36m[2023-07-11 17:23:14,073][233954] itr=1261, itrs=2000, Progress: 63.05%[0m
[36m[2023-07-11 17:23:25,636][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 17:23:25,636][233954] FPS: 334928.68[0m
[36m[2023-07-11 17:23:29,868][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:23:29,869][233954] Reward + Measures: [[1.34514106 0.88476068 0.63175797 0.91978174 0.8869133  0.28252071]][0m
[37m[1m[2023-07-11 17:23:29,869][233954] Max Reward on eval: 1.3451410632517995[0m
[37m[1m[2023-07-11 17:23:29,869][233954] Min Reward on eval: 1.3451410632517995[0m
[37m[1m[2023-07-11 17:23:29,869][233954] Mean Reward across all agents: 1.3451410632517995[0m
[37m[1m[2023-07-11 17:23:29,870][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:23:35,145][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:23:35,146][233954] Reward + Measures: [[58.57874081  0.80790007  0.38030002  0.68310004  0.58940005  0.56292194]
 [35.20376048  0.66920006  0.73999995  0.73549998  0.81270009  0.29862174]
 [31.91537402  0.86550009  0.36620003  0.72319996  0.5887      0.45698372]
 ...
 [21.70712209  0.84149998  0.55439997  0.84429997  0.77670002  0.26206836]
 [56.44143894  0.85900003  0.4549      0.61620003  0.71780008  0.44683132]
 [48.79737528  0.85979998  0.57910007  0.82060003  0.79049999  0.25833464]][0m
[37m[1m[2023-07-11 17:23:35,146][233954] Max Reward on eval: 92.15933702057228[0m
[37m[1m[2023-07-11 17:23:35,146][233954] Min Reward on eval: -146.85499455141834[0m
[37m[1m[2023-07-11 17:23:35,146][233954] Mean Reward across all agents: 33.354195642001095[0m
[37m[1m[2023-07-11 17:23:35,147][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:23:35,159][233954] mean_value=336.2762833485433, max_value=592.1593370205723[0m
[37m[1m[2023-07-11 17:23:35,162][233954] New mean coefficients: [[ 3.4490926  2.4849489 -0.541819   2.6586175 -0.47629   -4.939833 ]][0m
[37m[1m[2023-07-11 17:23:35,163][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:23:44,179][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 17:23:44,179][233954] FPS: 426001.01[0m
[36m[2023-07-11 17:23:44,181][233954] itr=1262, itrs=2000, Progress: 63.10%[0m
[36m[2023-07-11 17:23:55,960][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 17:23:55,960][233954] FPS: 328850.09[0m
[36m[2023-07-11 17:24:00,207][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:24:00,207][233954] Reward + Measures: [[28.46364867  0.94571757  0.22759335  0.91214371  0.30186197  0.57454473]][0m
[37m[1m[2023-07-11 17:24:00,207][233954] Max Reward on eval: 28.463648669813818[0m
[37m[1m[2023-07-11 17:24:00,207][233954] Min Reward on eval: 28.463648669813818[0m
[37m[1m[2023-07-11 17:24:00,208][233954] Mean Reward across all agents: 28.463648669813818[0m
[37m[1m[2023-07-11 17:24:00,208][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:24:05,153][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:24:05,154][233954] Reward + Measures: [[55.51721859  0.92479992  0.1908      0.8858      0.32690001  0.63871664]
 [84.83738803  0.80080003  0.15390001  0.77010006  0.28420001  0.73293126]
 [95.13094283  0.47950003  0.2141      0.4118      0.33759999  1.4681443 ]
 ...
 [20.65593845  0.38980001  0.43330002  0.23169999  0.537       1.76024425]
 [24.80025287  0.81550008  0.10039999  0.79949999  0.62470001  1.38445628]
 [ 8.67530202  0.8466      0.4296      0.72579998  0.61390001  0.95877641]][0m
[37m[1m[2023-07-11 17:24:05,154][233954] Max Reward on eval: 360.93896482884884[0m
[37m[1m[2023-07-11 17:24:05,154][233954] Min Reward on eval: -120.85768939193804[0m
[37m[1m[2023-07-11 17:24:05,154][233954] Mean Reward across all agents: 53.52590588924655[0m
[37m[1m[2023-07-11 17:24:05,155][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:24:05,167][233954] mean_value=130.0649584063199, max_value=597.047004805645[0m
[37m[1m[2023-07-11 17:24:05,170][233954] New mean coefficients: [[ 2.5841234   3.132823   -0.33032817  3.363275   -0.16803998 -4.168782  ]][0m
[37m[1m[2023-07-11 17:24:05,171][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:24:14,132][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 17:24:14,133][233954] FPS: 428587.81[0m
[36m[2023-07-11 17:24:14,135][233954] itr=1263, itrs=2000, Progress: 63.15%[0m
[36m[2023-07-11 17:24:25,755][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 17:24:25,755][233954] FPS: 333272.65[0m
[36m[2023-07-11 17:24:29,998][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:24:29,999][233954] Reward + Measures: [[19.31443317  0.8343206   0.73295671  0.88178658  0.78363508  0.51240528]][0m
[37m[1m[2023-07-11 17:24:29,999][233954] Max Reward on eval: 19.3144331729048[0m
[37m[1m[2023-07-11 17:24:29,999][233954] Min Reward on eval: 19.3144331729048[0m
[37m[1m[2023-07-11 17:24:29,999][233954] Mean Reward across all agents: 19.3144331729048[0m
[37m[1m[2023-07-11 17:24:30,000][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:24:34,957][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:24:34,958][233954] Reward + Measures: [[-32.64680791   0.10469999   0.90440005   0.86580002   0.93020004
    1.21495521]
 [-62.20346021   0.2086       0.85470003   0.63060004   0.83729994
    1.40570247]
 [ 13.59020976   0.87019998   0.59140003   0.87449998   0.61219996
    0.30977336]
 ...
 [  9.98789817   0.35489997   0.60020006   0.69160002   0.65210003
    1.13789856]
 [-33.18531512   0.0063       0.99200004   0.96779996   0.98950005
    1.08648646]
 [ 27.3541082    0.75940001   0.6846       0.72960007   0.70769995
    0.5658145 ]][0m
[37m[1m[2023-07-11 17:24:34,958][233954] Max Reward on eval: 73.94689114005305[0m
[37m[1m[2023-07-11 17:24:34,958][233954] Min Reward on eval: -79.40863369815051[0m
[37m[1m[2023-07-11 17:24:34,958][233954] Mean Reward across all agents: -2.9769721278051824[0m
[37m[1m[2023-07-11 17:24:34,959][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:24:34,968][233954] mean_value=45.86933296476844, max_value=523.8127806653455[0m
[37m[1m[2023-07-11 17:24:34,971][233954] New mean coefficients: [[ 2.3430989   2.8936067  -1.1083782   2.589526   -0.66125774 -4.2290387 ]][0m
[37m[1m[2023-07-11 17:24:34,972][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:24:43,938][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 17:24:43,938][233954] FPS: 428369.64[0m
[36m[2023-07-11 17:24:43,940][233954] itr=1264, itrs=2000, Progress: 63.20%[0m
[36m[2023-07-11 17:24:55,488][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 17:24:55,488][233954] FPS: 335487.82[0m
[36m[2023-07-11 17:24:59,707][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:24:59,708][233954] Reward + Measures: [[-15.16291502   0.98355132   0.160091     0.99422169   0.19622999
    1.08516169]][0m
[37m[1m[2023-07-11 17:24:59,708][233954] Max Reward on eval: -15.1629150219536[0m
[37m[1m[2023-07-11 17:24:59,708][233954] Min Reward on eval: -15.1629150219536[0m
[37m[1m[2023-07-11 17:24:59,708][233954] Mean Reward across all agents: -15.1629150219536[0m
[37m[1m[2023-07-11 17:24:59,709][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:25:04,664][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:25:04,664][233954] Reward + Measures: [[-47.00888288   0.97080004   0.92360002   0.99329996   0.0068
    1.38229656]
 [ 14.55890896   0.57650006   0.78310007   0.85669994   0.71049994
    2.08826852]
 [ 22.2845003    0.96779996   0.80940002   0.99210006   0.0218
    1.2780174 ]
 ...
 [ 33.90405672   0.60650003   0.74239999   0.85690004   0.64330006
    1.92901516]
 [-38.49913836   0.98290008   0.1277       0.99509996   0.35209998
    1.00661623]
 [-47.94930363   0.9587       0.1227       0.98299998   0.29360002
    1.3821553 ]][0m
[37m[1m[2023-07-11 17:25:04,665][233954] Max Reward on eval: 126.24423072231002[0m
[37m[1m[2023-07-11 17:25:04,665][233954] Min Reward on eval: -85.44364643543959[0m
[37m[1m[2023-07-11 17:25:04,665][233954] Mean Reward across all agents: 13.32273868092132[0m
[37m[1m[2023-07-11 17:25:04,665][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:25:04,677][233954] mean_value=136.2825838375857, max_value=626.24423072231[0m
[37m[1m[2023-07-11 17:25:04,680][233954] New mean coefficients: [[ 3.0619602  2.54281   -0.8125628  2.5039582 -1.0146728 -4.671307 ]][0m
[37m[1m[2023-07-11 17:25:04,681][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:25:13,635][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 17:25:13,635][233954] FPS: 428954.67[0m
[36m[2023-07-11 17:25:13,637][233954] itr=1265, itrs=2000, Progress: 63.25%[0m
[36m[2023-07-11 17:25:25,193][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 17:25:25,193][233954] FPS: 335182.05[0m
[36m[2023-07-11 17:25:29,405][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:25:29,406][233954] Reward + Measures: [[-34.35222445   0.96437693   0.22897033   0.94811732   0.31523266
    0.74993831]][0m
[37m[1m[2023-07-11 17:25:29,406][233954] Max Reward on eval: -34.35222445300808[0m
[37m[1m[2023-07-11 17:25:29,406][233954] Min Reward on eval: -34.35222445300808[0m
[37m[1m[2023-07-11 17:25:29,407][233954] Mean Reward across all agents: -34.35222445300808[0m
[37m[1m[2023-07-11 17:25:29,407][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:25:34,406][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:25:34,406][233954] Reward + Measures: [[-36.0883431    0.9526       0.3452       0.8998       0.1728
    0.6242677 ]
 [  7.83349885   0.88720006   0.51890004   0.88549995   0.0192
    0.86265832]
 [  8.91839212   0.88370001   0.51350003   0.86040002   0.13860001
    0.7812525 ]
 ...
 [-19.09881877   0.94069999   0.73899996   0.95050001   0.0048
    0.76296121]
 [ 11.49863799   0.96020001   0.74519998   0.98330003   0.0002
    0.71340114]
 [-26.58687244   0.93559998   0.0464       0.86020005   0.59460002
    0.70622569]][0m
[37m[1m[2023-07-11 17:25:34,406][233954] Max Reward on eval: 145.45631598141046[0m
[37m[1m[2023-07-11 17:25:34,407][233954] Min Reward on eval: -81.15421726368368[0m
[37m[1m[2023-07-11 17:25:34,407][233954] Mean Reward across all agents: -10.2908881995656[0m
[37m[1m[2023-07-11 17:25:34,407][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:25:34,414][233954] mean_value=52.05581272978699, max_value=612.6694638121501[0m
[37m[1m[2023-07-11 17:25:34,417][233954] New mean coefficients: [[ 3.2446141   1.6485667  -0.47747886  2.5694458  -1.2455539  -5.635224  ]][0m
[37m[1m[2023-07-11 17:25:34,418][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:25:43,364][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 17:25:43,364][233954] FPS: 429322.17[0m
[36m[2023-07-11 17:25:43,366][233954] itr=1266, itrs=2000, Progress: 63.30%[0m
[36m[2023-07-11 17:25:54,944][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 17:25:54,944][233954] FPS: 334522.42[0m
[36m[2023-07-11 17:25:59,248][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:25:59,248][233954] Reward + Measures: [[23.44439028  0.79224235  0.82010663  0.77032912  0.52074033  0.90583885]][0m
[37m[1m[2023-07-11 17:25:59,248][233954] Max Reward on eval: 23.444390279798128[0m
[37m[1m[2023-07-11 17:25:59,249][233954] Min Reward on eval: 23.444390279798128[0m
[37m[1m[2023-07-11 17:25:59,249][233954] Mean Reward across all agents: 23.444390279798128[0m
[37m[1m[2023-07-11 17:25:59,249][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:26:04,325][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:26:04,326][233954] Reward + Measures: [[-21.09983382   0.77030003   0.5115       0.73019999   0.12990001
    1.02191293]
 [-16.73155117   0.83850002   0.68739998   0.77820003   0.0187
    0.91204292]
 [-14.23090191   0.82240003   0.69800007   0.84739989   0.2244
    1.04501283]
 ...
 [-48.63014126   0.8294       0.56730002   0.787        0.0904
    0.88370639]
 [126.39125847   0.61199999   0.34979999   0.68790001   0.0701
    0.92142057]
 [-14.82345681   0.86380005   0.66099995   0.70349997   0.43129998
    0.68424302]][0m
[37m[1m[2023-07-11 17:26:04,326][233954] Max Reward on eval: 140.0366153595969[0m
[37m[1m[2023-07-11 17:26:04,326][233954] Min Reward on eval: -221.96761898398398[0m
[37m[1m[2023-07-11 17:26:04,327][233954] Mean Reward across all agents: -17.32342947809152[0m
[37m[1m[2023-07-11 17:26:04,327][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:26:04,336][233954] mean_value=-25.352306218773688, max_value=551.3770667437464[0m
[37m[1m[2023-07-11 17:26:04,339][233954] New mean coefficients: [[ 2.6796792   1.8487469  -0.28431275  2.7968628   0.24772108 -4.5160155 ]][0m
[37m[1m[2023-07-11 17:26:04,340][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:26:13,475][233954] train() took 9.13 seconds to complete[0m
[36m[2023-07-11 17:26:13,475][233954] FPS: 420412.10[0m
[36m[2023-07-11 17:26:13,478][233954] itr=1267, itrs=2000, Progress: 63.35%[0m
[36m[2023-07-11 17:26:25,162][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 17:26:25,162][233954] FPS: 331481.33[0m
[36m[2023-07-11 17:26:29,454][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:26:29,454][233954] Reward + Measures: [[8.66376725 0.88808233 0.78799736 0.89824104 0.824287   0.64549673]][0m
[37m[1m[2023-07-11 17:26:29,455][233954] Max Reward on eval: 8.663767246913379[0m
[37m[1m[2023-07-11 17:26:29,455][233954] Min Reward on eval: 8.663767246913379[0m
[37m[1m[2023-07-11 17:26:29,455][233954] Mean Reward across all agents: 8.663767246913379[0m
[37m[1m[2023-07-11 17:26:29,456][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:26:34,780][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:26:34,780][233954] Reward + Measures: [[ -2.04556031   0.80830002   0.38779998   0.91510004   0.0884
    0.69231743]
 [-51.67256786   0.87550002   0.21370001   0.8531       0.2484
    0.5471862 ]
 [-95.77115772   0.60789996   0.15809999   0.80860007   0.2714
    0.59913403]
 ...
 [-18.55681875   0.6724       0.45220003   0.82130003   0.1534
    0.81360239]
 [ -0.99012789   0.88789999   0.49880001   0.92229998   0.5061
    0.62781173]
 [-18.829472     0.4199       0.32539997   0.6978001    0.1954
    1.10082078]][0m
[37m[1m[2023-07-11 17:26:34,781][233954] Max Reward on eval: 105.36810209129472[0m
[37m[1m[2023-07-11 17:26:34,781][233954] Min Reward on eval: -169.00622173519804[0m
[37m[1m[2023-07-11 17:26:34,781][233954] Mean Reward across all agents: -23.879270544011582[0m
[37m[1m[2023-07-11 17:26:34,781][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:26:34,791][233954] mean_value=62.592665975309075, max_value=586.5920975284068[0m
[37m[1m[2023-07-11 17:26:34,793][233954] New mean coefficients: [[ 3.268796    1.6083889  -0.00139377  1.8378661   0.6803443  -4.5176606 ]][0m
[37m[1m[2023-07-11 17:26:34,794][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:26:43,857][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 17:26:43,858][233954] FPS: 423767.95[0m
[36m[2023-07-11 17:26:43,860][233954] itr=1268, itrs=2000, Progress: 63.40%[0m
[36m[2023-07-11 17:26:55,647][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 17:26:55,647][233954] FPS: 328693.71[0m
[36m[2023-07-11 17:26:59,883][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:26:59,883][233954] Reward + Measures: [[-11.00551623   0.97314036   0.95083332   0.96126139   0.95284832
    0.45080325]][0m
[37m[1m[2023-07-11 17:26:59,884][233954] Max Reward on eval: -11.005516232022783[0m
[37m[1m[2023-07-11 17:26:59,884][233954] Min Reward on eval: -11.005516232022783[0m
[37m[1m[2023-07-11 17:26:59,884][233954] Mean Reward across all agents: -11.005516232022783[0m
[37m[1m[2023-07-11 17:26:59,884][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:27:04,823][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:27:04,823][233954] Reward + Measures: [[308.44551229   0.30230001   0.41850001   0.2277       0.1693
    2.13145304]
 [ 11.06195812   0.78010005   0.36820003   0.8549       0.2335
    1.31559491]
 [-18.83743072   0.91450006   0.0255       0.85529995   0.77590007
    0.46017733]
 ...
 [232.52936382   0.60300004   0.63990003   0.73870003   0.0169
    1.61434591]
 [ 19.02887566   0.96499997   0.61760008   0.95220006   0.1718
    0.64813763]
 [ 65.11166622   0.79820007   0.39669999   0.84400004   0.25579998
    1.25762188]][0m
[37m[1m[2023-07-11 17:27:04,823][233954] Max Reward on eval: 404.9731006652117[0m
[37m[1m[2023-07-11 17:27:04,824][233954] Min Reward on eval: -54.26610361891799[0m
[37m[1m[2023-07-11 17:27:04,824][233954] Mean Reward across all agents: 84.52949572704985[0m
[37m[1m[2023-07-11 17:27:04,824][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:27:04,838][233954] mean_value=187.46581948012044, max_value=816.3530988060311[0m
[37m[1m[2023-07-11 17:27:04,841][233954] New mean coefficients: [[ 3.910944   1.3824446  0.2333371  1.944109   0.9835827 -4.228148 ]][0m
[37m[1m[2023-07-11 17:27:04,842][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:27:13,844][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 17:27:13,844][233954] FPS: 426646.28[0m
[36m[2023-07-11 17:27:13,846][233954] itr=1269, itrs=2000, Progress: 63.45%[0m
[36m[2023-07-11 17:27:25,476][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 17:27:25,476][233954] FPS: 333052.92[0m
[36m[2023-07-11 17:27:29,759][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:27:29,760][233954] Reward + Measures: [[13.68220765  0.93766063  0.29148299  0.84251934  0.87955189  0.39824051]][0m
[37m[1m[2023-07-11 17:27:29,760][233954] Max Reward on eval: 13.682207649360523[0m
[37m[1m[2023-07-11 17:27:29,760][233954] Min Reward on eval: 13.682207649360523[0m
[37m[1m[2023-07-11 17:27:29,760][233954] Mean Reward across all agents: 13.682207649360523[0m
[37m[1m[2023-07-11 17:27:29,761][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:27:34,815][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:27:34,816][233954] Reward + Measures: [[140.29414085   0.25650001   0.3048       0.18959999   0.37800002
    1.25488234]
 [103.76791336   0.50630003   0.45269996   0.46230003   0.47569999
    1.10531044]
 [  8.72784962   0.1727       0.2297       0.20290001   0.1865
    2.30946612]
 ...
 [ 53.21468639   0.96050006   0.53540003   0.9637       0.10880001
    1.49725521]
 [ 39.36554768   0.99510002   0.79170007   0.995        0.0051
    1.07596529]
 [ 96.60004431   0.23640001   0.18639998   0.21660002   0.20770001
    1.50845051]][0m
[37m[1m[2023-07-11 17:27:34,816][233954] Max Reward on eval: 209.47274778643623[0m
[37m[1m[2023-07-11 17:27:34,816][233954] Min Reward on eval: -98.75460911169648[0m
[37m[1m[2023-07-11 17:27:34,817][233954] Mean Reward across all agents: 50.442452536549304[0m
[37m[1m[2023-07-11 17:27:34,817][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:27:34,825][233954] mean_value=-191.95897862608922, max_value=561.8982939627255[0m
[37m[1m[2023-07-11 17:27:34,827][233954] New mean coefficients: [[ 2.1852684   2.215489   -0.6565795   3.1090298   0.87858135 -3.7986107 ]][0m
[37m[1m[2023-07-11 17:27:34,828][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:27:43,784][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 17:27:43,785][233954] FPS: 428831.20[0m
[36m[2023-07-11 17:27:43,787][233954] itr=1270, itrs=2000, Progress: 63.50%[0m
[37m[1m[2023-07-11 17:31:23,109][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001250[0m
[36m[2023-07-11 17:31:35,275][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 17:31:35,275][233954] FPS: 328237.27[0m
[36m[2023-07-11 17:31:39,568][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:31:39,568][233954] Reward + Measures: [[-0.10873402  0.98689032  0.60886669  0.99231666  0.01868667  0.57416022]][0m
[37m[1m[2023-07-11 17:31:39,569][233954] Max Reward on eval: -0.10873401511436774[0m
[37m[1m[2023-07-11 17:31:39,569][233954] Min Reward on eval: -0.10873401511436774[0m
[37m[1m[2023-07-11 17:31:39,569][233954] Mean Reward across all agents: -0.10873401511436774[0m
[37m[1m[2023-07-11 17:31:39,569][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:31:44,448][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:31:44,449][233954] Reward + Measures: [[-61.3528733    0.95900005   0.58660001   0.9709       0.2149
    0.65650827]
 [ 10.08422597   0.97049999   0.83149999   0.99060005   0.
    0.53616869]
 [-31.63212785   0.71359998   0.56199998   0.68339998   0.20120001
    0.53642762]
 ...
 [ 12.74230124   0.84020007   0.37470001   0.61339998   0.61210006
    0.74147242]
 [-30.0357299    0.98149997   0.8847       0.97729999   0.0008
    0.36229357]
 [-65.72489846   0.88630003   0.07480001   0.80830002   0.47620001
    1.00794816]][0m
[37m[1m[2023-07-11 17:31:44,449][233954] Max Reward on eval: 98.01824377372395[0m
[37m[1m[2023-07-11 17:31:44,449][233954] Min Reward on eval: -188.14656637196896[0m
[37m[1m[2023-07-11 17:31:44,450][233954] Mean Reward across all agents: -14.673390366570356[0m
[37m[1m[2023-07-11 17:31:44,450][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:31:44,456][233954] mean_value=35.163696180579166, max_value=513.4393908361438[0m
[37m[1m[2023-07-11 17:31:44,459][233954] New mean coefficients: [[ 2.071371   1.7449726 -0.9151094  3.1432426  1.1256424 -3.604286 ]][0m
[37m[1m[2023-07-11 17:31:44,460][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:31:53,409][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 17:31:53,410][233954] FPS: 429141.85[0m
[36m[2023-07-11 17:31:53,412][233954] itr=1271, itrs=2000, Progress: 63.55%[0m
[36m[2023-07-11 17:32:05,223][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 17:32:05,223][233954] FPS: 327906.76[0m
[36m[2023-07-11 17:32:09,462][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:32:09,462][233954] Reward + Measures: [[146.08358632   0.730169     0.77065837   0.76688671   0.53460968
    0.45176309]][0m
[37m[1m[2023-07-11 17:32:09,462][233954] Max Reward on eval: 146.08358631847796[0m
[37m[1m[2023-07-11 17:32:09,463][233954] Min Reward on eval: 146.08358631847796[0m
[37m[1m[2023-07-11 17:32:09,463][233954] Mean Reward across all agents: 146.08358631847796[0m
[37m[1m[2023-07-11 17:32:09,463][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:32:14,336][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:32:14,337][233954] Reward + Measures: [[-139.12259875    0.29139999    0.50319999    0.64849997    0.4914
     2.36141706]
 [ 101.60195733    0.66110003    0.50870001    0.69639999    0.49580002
     0.70064777]
 [  69.78242731    0.78439999    0.58989996    0.81739998    0.39709997
     0.41165429]
 ...
 [ 117.7986231     0.47230002    0.55180001    0.31600001    0.47539997
     1.06316435]
 [ 119.77603531    0.66949999    0.32049999    0.70499998    0.3987
     0.94315618]
 [ 204.17153933    0.56849998    0.33340004    0.64130002    0.36990005
     1.05835021]][0m
[37m[1m[2023-07-11 17:32:14,337][233954] Max Reward on eval: 272.12254332602026[0m
[37m[1m[2023-07-11 17:32:14,337][233954] Min Reward on eval: -193.7621010094881[0m
[37m[1m[2023-07-11 17:32:14,337][233954] Mean Reward across all agents: 72.50225561288363[0m
[37m[1m[2023-07-11 17:32:14,338][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:32:14,354][233954] mean_value=59.89816367043049, max_value=602.0795567652733[0m
[37m[1m[2023-07-11 17:32:14,357][233954] New mean coefficients: [[ 1.7311336   1.2303578  -0.17036796  2.516849    0.97472966 -3.5580668 ]][0m
[37m[1m[2023-07-11 17:32:14,358][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:32:23,298][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 17:32:23,298][233954] FPS: 429620.04[0m
[36m[2023-07-11 17:32:23,300][233954] itr=1272, itrs=2000, Progress: 63.60%[0m
[36m[2023-07-11 17:32:34,860][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 17:32:34,860][233954] FPS: 335160.76[0m
[36m[2023-07-11 17:32:39,087][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:32:39,087][233954] Reward + Measures: [[42.29141482  0.86337531  0.79985368  0.98530304  0.00139367  0.48450169]][0m
[37m[1m[2023-07-11 17:32:39,088][233954] Max Reward on eval: 42.29141481649847[0m
[37m[1m[2023-07-11 17:32:39,088][233954] Min Reward on eval: 42.29141481649847[0m
[37m[1m[2023-07-11 17:32:39,088][233954] Mean Reward across all agents: 42.29141481649847[0m
[37m[1m[2023-07-11 17:32:39,088][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:32:44,024][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:32:44,025][233954] Reward + Measures: [[120.79559993   0.69440001   0.64060003   0.65319997   0.10270001
    0.43567702]
 [ 33.2574746    0.78930008   0.61149997   0.96649998   0.0104
    0.38093004]
 [ 45.42719862   0.81940001   0.77640003   0.89399999   0.0139
    0.4578054 ]
 ...
 [-23.30594911   0.81170005   0.65060008   0.96239996   0.0467
    0.3315632 ]
 [ 14.39316994   0.95410007   0.92339993   0.99419993   0.0001
    0.55393946]
 [ 23.63784205   0.95640004   0.92910004   0.9903       0.0195
    0.43682551]][0m
[37m[1m[2023-07-11 17:32:44,025][233954] Max Reward on eval: 144.41199397901073[0m
[37m[1m[2023-07-11 17:32:44,025][233954] Min Reward on eval: -39.38880892314482[0m
[37m[1m[2023-07-11 17:32:44,025][233954] Mean Reward across all agents: 24.66321279688192[0m
[37m[1m[2023-07-11 17:32:44,025][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:32:44,032][233954] mean_value=94.65433621992348, max_value=598.347974788677[0m
[37m[1m[2023-07-11 17:32:44,034][233954] New mean coefficients: [[ 2.1824145   0.6548077  -0.35061324  2.2798057   1.3959912  -2.45195   ]][0m
[37m[1m[2023-07-11 17:32:44,035][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:32:52,987][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 17:32:52,987][233954] FPS: 429058.87[0m
[36m[2023-07-11 17:32:52,989][233954] itr=1273, itrs=2000, Progress: 63.65%[0m
[36m[2023-07-11 17:33:04,738][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 17:33:04,738][233954] FPS: 329804.47[0m
[36m[2023-07-11 17:33:09,009][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:33:09,009][233954] Reward + Measures: [[-7.24327724  0.90242529  0.13540067  0.83294594  0.74768293  0.48414031]][0m
[37m[1m[2023-07-11 17:33:09,010][233954] Max Reward on eval: -7.243277239418977[0m
[37m[1m[2023-07-11 17:33:09,010][233954] Min Reward on eval: -7.243277239418977[0m
[37m[1m[2023-07-11 17:33:09,010][233954] Mean Reward across all agents: -7.243277239418977[0m
[37m[1m[2023-07-11 17:33:09,010][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:33:14,003][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:33:14,004][233954] Reward + Measures: [[-27.8781691    0.51960003   0.68870002   0.336        0.85089999
    0.90833342]
 [ 54.93555245   0.17990001   0.38949999   0.26269999   0.40720001
    2.02975154]
 [-76.20133147   0.77030003   0.1639       0.58520001   0.53970003
    1.48358274]
 ...
 [ 16.34935283   0.69020003   0.39100003   0.42909995   0.49209997
    0.73795319]
 [172.44487617   0.29630002   0.38390002   0.28369999   0.55800003
    0.95505399]
 [324.31016161   0.27959999   0.47040001   0.26270002   0.43670002
    0.96381283]][0m
[37m[1m[2023-07-11 17:33:14,004][233954] Max Reward on eval: 382.3748111627996[0m
[37m[1m[2023-07-11 17:33:14,004][233954] Min Reward on eval: -522.5457024621777[0m
[37m[1m[2023-07-11 17:33:14,004][233954] Mean Reward across all agents: 10.839510541140182[0m
[37m[1m[2023-07-11 17:33:14,005][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:33:14,015][233954] mean_value=-44.18699518039671, max_value=553.2968747717329[0m
[37m[1m[2023-07-11 17:33:14,018][233954] New mean coefficients: [[ 1.2151477   0.47827563  0.7944002   1.4009471   1.2015982  -2.2429361 ]][0m
[37m[1m[2023-07-11 17:33:14,019][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:33:23,015][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 17:33:23,015][233954] FPS: 426916.27[0m
[36m[2023-07-11 17:33:23,018][233954] itr=1274, itrs=2000, Progress: 63.70%[0m
[36m[2023-07-11 17:33:34,690][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 17:33:34,690][233954] FPS: 332024.94[0m
[36m[2023-07-11 17:33:38,942][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:33:38,943][233954] Reward + Measures: [[22.75947238  0.54573101  0.87728906  0.69136196  0.884642    0.49639636]][0m
[37m[1m[2023-07-11 17:33:38,943][233954] Max Reward on eval: 22.75947238333183[0m
[37m[1m[2023-07-11 17:33:38,943][233954] Min Reward on eval: 22.75947238333183[0m
[37m[1m[2023-07-11 17:33:38,944][233954] Mean Reward across all agents: 22.75947238333183[0m
[37m[1m[2023-07-11 17:33:38,944][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:33:43,886][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:33:43,887][233954] Reward + Measures: [[-48.89930308   0.49770004   0.88360006   0.48250005   0.89069998
    0.92641735]
 [ 21.80278433   0.25100002   0.52069998   0.28749999   0.6196
    1.44864166]
 [162.31223552   0.41580001   0.3066       0.51720005   0.49580002
    0.89069176]
 ...
 [  1.00388462   0.30430001   0.4876       0.38900003   0.56879997
    1.35636008]
 [ 24.72187432   0.1847       0.85729998   0.21959999   0.84580004
    1.17315924]
 [ 20.18536611   0.27590001   0.48410001   0.13079999   0.55360001
    1.06987286]][0m
[37m[1m[2023-07-11 17:33:43,887][233954] Max Reward on eval: 290.2779064079747[0m
[37m[1m[2023-07-11 17:33:43,887][233954] Min Reward on eval: -153.050240712706[0m
[37m[1m[2023-07-11 17:33:43,887][233954] Mean Reward across all agents: 24.65795424891552[0m
[37m[1m[2023-07-11 17:33:43,888][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:33:43,896][233954] mean_value=18.966410861529464, max_value=729.1955680875108[0m
[37m[1m[2023-07-11 17:33:43,899][233954] New mean coefficients: [[ 1.0742565   1.060503    0.42232746  1.7824897   1.2882168  -1.5577685 ]][0m
[37m[1m[2023-07-11 17:33:43,900][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:33:52,876][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 17:33:52,876][233954] FPS: 427879.81[0m
[36m[2023-07-11 17:33:52,878][233954] itr=1275, itrs=2000, Progress: 63.75%[0m
[36m[2023-07-11 17:34:04,489][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 17:34:04,489][233954] FPS: 333734.31[0m
[36m[2023-07-11 17:34:08,823][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:34:08,823][233954] Reward + Measures: [[-18.62287979   0.97478199   0.7950983    0.97478139   0.43107665
    0.43738875]][0m
[37m[1m[2023-07-11 17:34:08,823][233954] Max Reward on eval: -18.62287978953834[0m
[37m[1m[2023-07-11 17:34:08,824][233954] Min Reward on eval: -18.62287978953834[0m
[37m[1m[2023-07-11 17:34:08,824][233954] Mean Reward across all agents: -18.62287978953834[0m
[37m[1m[2023-07-11 17:34:08,824][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:34:13,795][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:34:13,796][233954] Reward + Measures: [[-15.91518578   0.93300003   0.0462       0.89840001   0.56620002
    0.52156681]
 [ 51.28411915   0.85070002   0.0824       0.75150001   0.55459994
    1.07499063]
 [-42.03665953   0.61370003   0.25280002   0.66709995   0.133
    1.10994565]
 ...
 [ 26.24425459   0.89300007   0.51019996   0.90360004   0.20809999
    1.13186967]
 [-26.06484081   0.74000001   0.14619999   0.69620001   0.5104
    0.9588241 ]
 [ 52.88067587   0.49520001   0.12630001   0.40859994   0.2211
    1.39578927]][0m
[37m[1m[2023-07-11 17:34:13,796][233954] Max Reward on eval: 284.32876296387985[0m
[37m[1m[2023-07-11 17:34:13,796][233954] Min Reward on eval: -111.98900698376819[0m
[37m[1m[2023-07-11 17:34:13,796][233954] Mean Reward across all agents: 17.89788206281778[0m
[37m[1m[2023-07-11 17:34:13,797][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:34:13,808][233954] mean_value=-45.543420507147964, max_value=616.6561217295006[0m
[37m[1m[2023-07-11 17:34:13,810][233954] New mean coefficients: [[ 0.811057    0.86957     0.85535294  0.55408704  0.5771054  -1.803147  ]][0m
[37m[1m[2023-07-11 17:34:13,811][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:34:22,804][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 17:34:22,804][233954] FPS: 427092.53[0m
[36m[2023-07-11 17:34:22,807][233954] itr=1276, itrs=2000, Progress: 63.80%[0m
[36m[2023-07-11 17:34:34,339][233954] train() took 11.43 seconds to complete[0m
[36m[2023-07-11 17:34:34,339][233954] FPS: 335933.33[0m
[36m[2023-07-11 17:34:38,638][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:34:38,638][233954] Reward + Measures: [[-27.15178762   0.98137134   0.97606933   0.92216903   0.95560431
    0.4158583 ]][0m
[37m[1m[2023-07-11 17:34:38,639][233954] Max Reward on eval: -27.1517876190159[0m
[37m[1m[2023-07-11 17:34:38,639][233954] Min Reward on eval: -27.1517876190159[0m
[37m[1m[2023-07-11 17:34:38,639][233954] Mean Reward across all agents: -27.1517876190159[0m
[37m[1m[2023-07-11 17:34:38,639][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:34:43,623][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:34:43,624][233954] Reward + Measures: [[-25.45705476   0.3339       0.46310002   0.43109998   0.37520003
    1.57193279]
 [ 31.1706221    0.20639999   0.229        0.23190002   0.2309
    2.5090065 ]
 [-30.75665545   0.48569998   0.63000005   0.5735001    0.42199999
    1.23856068]
 ...
 [-28.07429448   0.7748       0.81879997   0.82779998   0.64490002
    1.11812055]
 [-54.59764003   0.96689999   0.93530005   0.85790008   0.93309993
    0.66289479]
 [-24.99347519   0.96700001   0.96940005   0.91530001   0.9181
    0.68842906]][0m
[37m[1m[2023-07-11 17:34:43,624][233954] Max Reward on eval: 154.50379893891514[0m
[37m[1m[2023-07-11 17:34:43,624][233954] Min Reward on eval: -367.9940292943269[0m
[37m[1m[2023-07-11 17:34:43,624][233954] Mean Reward across all agents: -31.235854043572836[0m
[37m[1m[2023-07-11 17:34:43,625][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:34:43,632][233954] mean_value=-76.3572164677173, max_value=559.8677883289754[0m
[37m[1m[2023-07-11 17:34:43,635][233954] New mean coefficients: [[ 1.1568043  1.147571   1.0537215  1.1483203  0.6513648 -2.1143448]][0m
[37m[1m[2023-07-11 17:34:43,636][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:34:52,612][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 17:34:52,612][233954] FPS: 427882.29[0m
[36m[2023-07-11 17:34:52,614][233954] itr=1277, itrs=2000, Progress: 63.85%[0m
[36m[2023-07-11 17:35:04,458][233954] train() took 11.74 seconds to complete[0m
[36m[2023-07-11 17:35:04,458][233954] FPS: 326995.35[0m
[36m[2023-07-11 17:35:08,670][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:35:08,670][233954] Reward + Measures: [[-20.87124329   0.96734732   0.96188933   0.47397232   0.98076838
    1.20436764]][0m
[37m[1m[2023-07-11 17:35:08,671][233954] Max Reward on eval: -20.87124328744039[0m
[37m[1m[2023-07-11 17:35:08,671][233954] Min Reward on eval: -20.87124328744039[0m
[37m[1m[2023-07-11 17:35:08,671][233954] Mean Reward across all agents: -20.87124328744039[0m
[37m[1m[2023-07-11 17:35:08,671][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:35:13,596][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:35:13,602][233954] Reward + Measures: [[ 30.80901997   0.33410001   0.39140001   0.41529998   0.2561
    1.83784926]
 [-11.9690647    0.57520002   0.6656       0.55160004   0.65009999
    1.24818063]
 [-99.23001909   0.63940001   0.60159999   0.51050007   0.1556
    1.04998958]
 ...
 [ -1.7964738    0.84980005   0.65390003   0.7421       0.31959999
    1.34486222]
 [-38.92005385   0.97520012   0.96870005   0.88070005   0.96490002
    1.19115353]
 [-85.57585929   0.83710003   0.81140006   0.71170008   0.5765
    1.5401808 ]][0m
[37m[1m[2023-07-11 17:35:13,602][233954] Max Reward on eval: 221.56750964380336[0m
[37m[1m[2023-07-11 17:35:13,602][233954] Min Reward on eval: -286.8109941422939[0m
[37m[1m[2023-07-11 17:35:13,602][233954] Mean Reward across all agents: -2.6786115362145324[0m
[37m[1m[2023-07-11 17:35:13,603][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:35:13,611][233954] mean_value=24.70823093701786, max_value=554.2854629581794[0m
[37m[1m[2023-07-11 17:35:13,613][233954] New mean coefficients: [[ 1.2790525   1.8997469   0.03171194  2.24049     1.067281   -0.45748627]][0m
[37m[1m[2023-07-11 17:35:13,614][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:35:22,632][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 17:35:22,637][233954] FPS: 425910.56[0m
[36m[2023-07-11 17:35:22,644][233954] itr=1278, itrs=2000, Progress: 63.90%[0m
[36m[2023-07-11 17:35:34,316][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 17:35:34,317][233954] FPS: 332212.77[0m
[36m[2023-07-11 17:35:38,681][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:35:38,681][233954] Reward + Measures: [[28.75968986  0.9497053   0.7627933   0.87072295  0.73095703  0.9294892 ]][0m
[37m[1m[2023-07-11 17:35:38,681][233954] Max Reward on eval: 28.759689863789166[0m
[37m[1m[2023-07-11 17:35:38,682][233954] Min Reward on eval: 28.759689863789166[0m
[37m[1m[2023-07-11 17:35:38,682][233954] Mean Reward across all agents: 28.759689863789166[0m
[37m[1m[2023-07-11 17:35:38,682][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:35:43,947][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:35:43,947][233954] Reward + Measures: [[ 34.64127641   0.58219999   0.4316       0.4822       0.20250002
    1.64015603]
 [ 64.9829225    0.80360001   0.72730005   0.72680002   0.6196
    0.75749958]
 [-18.48899661   0.9774       0.91390002   0.94379997   0.96359998
    1.11957324]
 ...
 [  2.61026398   0.82550001   0.2958       0.78100002   0.4375
    1.64002347]
 [-23.10451436   0.61560005   0.4822       0.55960006   0.59960002
    1.53712428]
 [-13.81196325   0.98999995   0.95960009   0.98719996   0.98089999
    1.14807653]][0m
[37m[1m[2023-07-11 17:35:43,948][233954] Max Reward on eval: 184.57239628508688[0m
[37m[1m[2023-07-11 17:35:43,948][233954] Min Reward on eval: -119.27310236524791[0m
[37m[1m[2023-07-11 17:35:43,948][233954] Mean Reward across all agents: 9.839636572560469[0m
[37m[1m[2023-07-11 17:35:43,948][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:35:43,957][233954] mean_value=22.99855953414691, max_value=473.8447950704605[0m
[37m[1m[2023-07-11 17:35:43,959][233954] New mean coefficients: [[ 1.8323997   2.2767608   0.033376    2.2359817   0.63925564 -0.5540357 ]][0m
[37m[1m[2023-07-11 17:35:43,960][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:35:52,883][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 17:35:52,888][233954] FPS: 430455.89[0m
[36m[2023-07-11 17:35:52,891][233954] itr=1279, itrs=2000, Progress: 63.95%[0m
[36m[2023-07-11 17:36:04,657][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 17:36:04,658][233954] FPS: 329301.31[0m
[36m[2023-07-11 17:36:08,884][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:36:08,885][233954] Reward + Measures: [[36.58825261  0.91363865  0.32664734  0.9794277   0.32542199  0.90894544]][0m
[37m[1m[2023-07-11 17:36:08,885][233954] Max Reward on eval: 36.58825260791727[0m
[37m[1m[2023-07-11 17:36:08,885][233954] Min Reward on eval: 36.58825260791727[0m
[37m[1m[2023-07-11 17:36:08,886][233954] Mean Reward across all agents: 36.58825260791727[0m
[37m[1m[2023-07-11 17:36:08,886][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:36:13,874][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:36:13,874][233954] Reward + Measures: [[-31.60428689   0.3937       0.27630001   0.40810004   0.17320001
    1.92649782]
 [ 55.5475142    0.77090001   0.34440002   0.92989999   0.4269
    0.83377475]
 [-41.77117546   0.37450001   0.25960001   0.31570002   0.2428
    2.01104975]
 ...
 [-74.69800471   0.32389998   0.20990001   0.29699999   0.15629999
    2.20674682]
 [ 48.72301668   0.53029996   0.45819998   0.65430003   0.33860001
    1.10720944]
 [-20.06454424   0.63269997   0.38589999   0.66420001   0.22239999
    1.10054874]][0m
[37m[1m[2023-07-11 17:36:13,875][233954] Max Reward on eval: 250.77048920500093[0m
[37m[1m[2023-07-11 17:36:13,875][233954] Min Reward on eval: -334.3215026755759[0m
[37m[1m[2023-07-11 17:36:13,875][233954] Mean Reward across all agents: 28.988188801577866[0m
[37m[1m[2023-07-11 17:36:13,875][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:36:13,885][233954] mean_value=-49.55374009975305, max_value=594.2090869024629[0m
[37m[1m[2023-07-11 17:36:13,892][233954] New mean coefficients: [[ 1.4182768   2.0042922   0.10198396  1.4595294   0.46845186 -1.0289935 ]][0m
[37m[1m[2023-07-11 17:36:13,893][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:36:22,927][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 17:36:22,932][233954] FPS: 425169.63[0m
[36m[2023-07-11 17:36:22,935][233954] itr=1280, itrs=2000, Progress: 64.00%[0m
[37m[1m[2023-07-11 17:40:04,075][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001260[0m
[36m[2023-07-11 17:40:16,352][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 17:40:16,352][233954] FPS: 328809.46[0m
[36m[2023-07-11 17:40:20,647][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:40:20,647][233954] Reward + Measures: [[-16.54924716   0.96465904   0.556077     0.92462736   0.13670899
    0.70934373]][0m
[37m[1m[2023-07-11 17:40:20,647][233954] Max Reward on eval: -16.549247159742993[0m
[37m[1m[2023-07-11 17:40:20,648][233954] Min Reward on eval: -16.549247159742993[0m
[37m[1m[2023-07-11 17:40:20,648][233954] Mean Reward across all agents: -16.549247159742993[0m
[37m[1m[2023-07-11 17:40:20,648][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:40:25,946][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:40:25,946][233954] Reward + Measures: [[-19.83854742   0.98369998   0.3118       0.94250005   0.1523
    0.57223099]
 [-18.00786814   0.98390007   0.40159997   0.94550002   0.0509
    0.57417637]
 [ -8.51567489   0.98480004   0.4199       0.94559997   0.05140001
    0.65812272]
 ...
 [ -1.57274258   0.96449995   0.56160003   0.94099998   0.32259998
    0.90267575]
 [  5.06210239   0.98550004   0.55610001   0.94189996   0.0396
    0.64793438]
 [-31.62421855   0.98550004   0.52570003   0.94440001   0.0908
    0.62732196]][0m
[37m[1m[2023-07-11 17:40:25,947][233954] Max Reward on eval: 143.31806706041098[0m
[37m[1m[2023-07-11 17:40:25,947][233954] Min Reward on eval: -93.20869461353868[0m
[37m[1m[2023-07-11 17:40:25,947][233954] Mean Reward across all agents: -7.692861015422829[0m
[37m[1m[2023-07-11 17:40:25,948][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:40:25,953][233954] mean_value=-7.483466765201325, max_value=297.1059403167765[0m
[37m[1m[2023-07-11 17:40:25,956][233954] New mean coefficients: [[ 1.2628502   2.252443   -0.32990146  2.6386533   0.900893   -1.1072499 ]][0m
[37m[1m[2023-07-11 17:40:25,957][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:40:35,019][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 17:40:35,019][233954] FPS: 423842.47[0m
[36m[2023-07-11 17:40:35,021][233954] itr=1281, itrs=2000, Progress: 64.05%[0m
[36m[2023-07-11 17:40:46,742][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 17:40:46,742][233954] FPS: 330624.01[0m
[36m[2023-07-11 17:40:50,979][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:40:50,979][233954] Reward + Measures: [[-4.41463169  0.95074636  0.63822836  0.95914096  0.00091633  0.34039846]][0m
[37m[1m[2023-07-11 17:40:50,979][233954] Max Reward on eval: -4.414631691835566[0m
[37m[1m[2023-07-11 17:40:50,980][233954] Min Reward on eval: -4.414631691835566[0m
[37m[1m[2023-07-11 17:40:50,980][233954] Mean Reward across all agents: -4.414631691835566[0m
[37m[1m[2023-07-11 17:40:50,980][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:40:55,873][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:40:55,874][233954] Reward + Measures: [[143.59814955   0.80470002   0.62700003   0.82310003   0.016
    1.4961561 ]
 [-19.62188663   0.92430001   0.74040002   0.99060005   0.0042
    0.58557624]
 [ 11.97997697   0.88829994   0.6038       0.91030008   0.0039
    0.4436731 ]
 ...
 [ 73.99975298   0.55479997   0.2823       0.61159992   0.2692
    1.12750113]
 [191.55964091   0.76060003   0.53040004   0.91070002   0.0277
    2.11477256]
 [-57.27099088   0.76990002   0.68580002   0.92650002   0.0172
    0.66350502]][0m
[37m[1m[2023-07-11 17:40:55,874][233954] Max Reward on eval: 379.793397888029[0m
[37m[1m[2023-07-11 17:40:55,874][233954] Min Reward on eval: -143.2021293433383[0m
[37m[1m[2023-07-11 17:40:55,875][233954] Mean Reward across all agents: 58.85045063243495[0m
[37m[1m[2023-07-11 17:40:55,875][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:40:55,884][233954] mean_value=99.59881742703008, max_value=657.0229460598156[0m
[37m[1m[2023-07-11 17:40:55,887][233954] New mean coefficients: [[ 2.1378498   2.069096    0.07710552  1.9931382   0.13482612 -2.3219488 ]][0m
[37m[1m[2023-07-11 17:40:55,888][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:41:04,800][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 17:41:04,800][233954] FPS: 430964.33[0m
[36m[2023-07-11 17:41:04,803][233954] itr=1282, itrs=2000, Progress: 64.10%[0m
[36m[2023-07-11 17:41:16,346][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 17:41:16,347][233954] FPS: 335758.53[0m
[36m[2023-07-11 17:41:20,617][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:41:20,617][233954] Reward + Measures: [[0.00437382 0.98586267 0.19133334 0.99226809 0.36322728 0.63006645]][0m
[37m[1m[2023-07-11 17:41:20,617][233954] Max Reward on eval: 0.004373818622087128[0m
[37m[1m[2023-07-11 17:41:20,618][233954] Min Reward on eval: 0.004373818622087128[0m
[37m[1m[2023-07-11 17:41:20,618][233954] Mean Reward across all agents: 0.004373818622087128[0m
[37m[1m[2023-07-11 17:41:20,618][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:41:25,572][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:41:25,573][233954] Reward + Measures: [[  8.58674739   0.24340001   0.94370002   0.07230001   0.93309993
    1.48278892]
 [  8.97005599   0.96829998   0.022        0.93759996   0.42050001
    0.81926233]
 [-92.05384542   0.84420007   0.19340001   0.77560002   0.27340001
    1.02918601]
 ...
 [ 35.61022663   0.7755       0.93350011   0.72509998   0.94690001
    0.98108214]
 [-10.00173246   0.8743       0.26760003   0.94320005   0.11009999
    0.36891308]
 [  1.43457713   0.90740007   0.0921       0.91169995   0.39340001
    0.57346642]][0m
[37m[1m[2023-07-11 17:41:25,573][233954] Max Reward on eval: 245.15724920453505[0m
[37m[1m[2023-07-11 17:41:25,573][233954] Min Reward on eval: -130.05197382252663[0m
[37m[1m[2023-07-11 17:41:25,573][233954] Mean Reward across all agents: 30.577197303077817[0m
[37m[1m[2023-07-11 17:41:25,574][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:41:25,584][233954] mean_value=29.99460742454735, max_value=556.5306152218161[0m
[37m[1m[2023-07-11 17:41:25,586][233954] New mean coefficients: [[ 1.7681929   1.6382433   0.6348717   1.9176182  -0.24168324 -3.241262  ]][0m
[37m[1m[2023-07-11 17:41:25,587][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:41:34,665][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 17:41:34,665][233954] FPS: 423083.77[0m
[36m[2023-07-11 17:41:34,668][233954] itr=1283, itrs=2000, Progress: 64.15%[0m
[36m[2023-07-11 17:41:46,232][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 17:41:46,232][233954] FPS: 335098.10[0m
[36m[2023-07-11 17:41:50,466][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:41:50,467][233954] Reward + Measures: [[-1.66564412  0.95124727  0.51508701  0.89809132  0.08755266  0.27948332]][0m
[37m[1m[2023-07-11 17:41:50,467][233954] Max Reward on eval: -1.6656441242175837[0m
[37m[1m[2023-07-11 17:41:50,467][233954] Min Reward on eval: -1.6656441242175837[0m
[37m[1m[2023-07-11 17:41:50,468][233954] Mean Reward across all agents: -1.6656441242175837[0m
[37m[1m[2023-07-11 17:41:50,468][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:41:55,401][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:41:55,402][233954] Reward + Measures: [[ 39.02059985   0.79010004   0.81660002   0.86919993   0.51770002
    0.4267318 ]
 [ 47.5009007    0.59480006   0.61729997   0.76500005   0.1479
    1.11744297]
 [-19.90429329   0.89559996   0.51309997   0.89839995   0.50309998
    0.45585829]
 ...
 [-30.62515731   0.27090001   0.39140001   0.32969996   0.27579999
    1.46775615]
 [ 31.34618413   0.8143       0.81459999   0.90240002   0.0423
    1.05114281]
 [ 25.31514703   0.30580002   0.47799999   0.38429999   0.37029999
    1.51022315]][0m
[37m[1m[2023-07-11 17:41:55,402][233954] Max Reward on eval: 177.2443065494299[0m
[37m[1m[2023-07-11 17:41:55,402][233954] Min Reward on eval: -241.66210030522197[0m
[37m[1m[2023-07-11 17:41:55,402][233954] Mean Reward across all agents: 2.1371603562926467[0m
[37m[1m[2023-07-11 17:41:55,403][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:41:55,410][233954] mean_value=-165.70498613482235, max_value=553.2623491749167[0m
[37m[1m[2023-07-11 17:41:55,413][233954] New mean coefficients: [[ 1.681441    2.0335908   0.05497009  1.9719919  -0.31320736 -2.403912  ]][0m
[37m[1m[2023-07-11 17:41:55,414][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:42:04,379][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 17:42:04,380][233954] FPS: 428401.21[0m
[36m[2023-07-11 17:42:04,382][233954] itr=1284, itrs=2000, Progress: 64.20%[0m
[36m[2023-07-11 17:42:15,955][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 17:42:15,955][233954] FPS: 334873.04[0m
[36m[2023-07-11 17:42:20,172][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:42:20,173][233954] Reward + Measures: [[-1.98884158  0.96900129  0.05891     0.9684329   0.6721043   0.79503924]][0m
[37m[1m[2023-07-11 17:42:20,173][233954] Max Reward on eval: -1.9888415763161513[0m
[37m[1m[2023-07-11 17:42:20,173][233954] Min Reward on eval: -1.9888415763161513[0m
[37m[1m[2023-07-11 17:42:20,174][233954] Mean Reward across all agents: -1.9888415763161513[0m
[37m[1m[2023-07-11 17:42:20,174][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:42:25,187][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:42:25,187][233954] Reward + Measures: [[-215.582901      0.98180002    0.45950004    0.99379998    0.94739991
     1.44842136]
 [ -96.83377265    0.98379993    0.002         0.99360001    0.81689996
     0.6691255 ]
 [  16.93651147    0.5025        0.63019997    0.3125        0.60059994
     1.61123073]
 ...
 [  28.73188237    0.72490001    0.1061        0.62919998    0.25460002
     2.31789255]
 [  51.06744696    0.95270008    0.0865        0.90170002    0.4677
     1.91839468]
 [-119.44146319    0.81210005    0.114         0.67259997    0.47330004
     1.81114829]][0m
[37m[1m[2023-07-11 17:42:25,188][233954] Max Reward on eval: 329.81561280973256[0m
[37m[1m[2023-07-11 17:42:25,188][233954] Min Reward on eval: -345.44913482572883[0m
[37m[1m[2023-07-11 17:42:25,188][233954] Mean Reward across all agents: 0.8403556259180054[0m
[37m[1m[2023-07-11 17:42:25,188][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:42:25,200][233954] mean_value=17.485923911877446, max_value=728.7463626926765[0m
[37m[1m[2023-07-11 17:42:25,203][233954] New mean coefficients: [[ 1.3307874  2.3312218  1.1193974  2.9021277 -1.0252786 -2.8874624]][0m
[37m[1m[2023-07-11 17:42:25,204][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:42:34,244][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 17:42:34,245][233954] FPS: 424824.24[0m
[36m[2023-07-11 17:42:34,247][233954] itr=1285, itrs=2000, Progress: 64.25%[0m
[36m[2023-07-11 17:42:45,860][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 17:42:45,860][233954] FPS: 333745.57[0m
[36m[2023-07-11 17:42:50,161][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:42:50,162][233954] Reward + Measures: [[2.13351117 0.96463555 0.49836364 0.94486499 0.474024   0.3492347 ]][0m
[37m[1m[2023-07-11 17:42:50,162][233954] Max Reward on eval: 2.133511170207159[0m
[37m[1m[2023-07-11 17:42:50,162][233954] Min Reward on eval: 2.133511170207159[0m
[37m[1m[2023-07-11 17:42:50,163][233954] Mean Reward across all agents: 2.133511170207159[0m
[37m[1m[2023-07-11 17:42:50,163][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:42:55,109][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:42:55,110][233954] Reward + Measures: [[ -63.60887529    0.86750001    0.94309998    0.86660004    0.90970004
     1.62107849]
 [-188.86030501    0.76800007    0.3813        0.47529998    0.26630002
     1.39191401]
 [  -4.6663427     0.99169999    0.53870004    0.99490005    0.1305
     0.62342149]
 ...
 [ 113.80090905    0.74259996    0.66040003    0.67609996    0.4862
     1.96929514]
 [  90.15979306    0.94060004    0.68769997    0.8696        0.27079999
     1.36465442]
 [  63.43091729    0.90380001    0.58770001    0.87770003    0.69
     0.55000067]][0m
[37m[1m[2023-07-11 17:42:55,110][233954] Max Reward on eval: 202.9786816236563[0m
[37m[1m[2023-07-11 17:42:55,110][233954] Min Reward on eval: -228.53292369110858[0m
[37m[1m[2023-07-11 17:42:55,111][233954] Mean Reward across all agents: 3.1672861037524567[0m
[37m[1m[2023-07-11 17:42:55,111][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:42:55,122][233954] mean_value=71.59286974601736, max_value=572.845221988664[0m
[37m[1m[2023-07-11 17:42:55,125][233954] New mean coefficients: [[ 1.1565102  2.0519006  1.1809738  2.6650393 -1.4896848 -3.1534152]][0m
[37m[1m[2023-07-11 17:42:55,126][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:43:04,132][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 17:43:04,132][233954] FPS: 426484.93[0m
[36m[2023-07-11 17:43:04,134][233954] itr=1286, itrs=2000, Progress: 64.30%[0m
[36m[2023-07-11 17:43:15,731][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 17:43:15,732][233954] FPS: 334070.85[0m
[36m[2023-07-11 17:43:19,987][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:43:19,987][233954] Reward + Measures: [[8.64928422 0.97825474 0.43439534 0.9700436  0.36594367 0.27982575]][0m
[37m[1m[2023-07-11 17:43:19,987][233954] Max Reward on eval: 8.649284221974554[0m
[37m[1m[2023-07-11 17:43:19,987][233954] Min Reward on eval: 8.649284221974554[0m
[37m[1m[2023-07-11 17:43:19,988][233954] Mean Reward across all agents: 8.649284221974554[0m
[37m[1m[2023-07-11 17:43:19,988][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:43:25,249][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:43:25,250][233954] Reward + Measures: [[ -7.82278673   0.87820005   0.71430004   0.82609999   0.0093
    1.15681267]
 [ 86.79694982   0.93800002   0.68960005   0.92749995   0.0052
    2.41808724]
 [129.83631849   0.98879999   0.81599998   0.98180002   0.0096
    1.85510862]
 ...
 [-86.54690932   0.91619998   0.67270005   0.91020006   0.0225
    2.79887581]
 [ 54.41350201   0.71649998   0.88840002   0.3353       0.81420004
    0.74603766]
 [ 22.03262152   0.97690004   0.91009998   0.94750005   0.004
    1.22424293]][0m
[37m[1m[2023-07-11 17:43:25,250][233954] Max Reward on eval: 263.7708511262434[0m
[37m[1m[2023-07-11 17:43:25,250][233954] Min Reward on eval: -401.6855013455264[0m
[37m[1m[2023-07-11 17:43:25,251][233954] Mean Reward across all agents: 29.166130437799342[0m
[37m[1m[2023-07-11 17:43:25,251][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:43:25,261][233954] mean_value=77.70796688846183, max_value=700.7294603575929[0m
[37m[1m[2023-07-11 17:43:25,263][233954] New mean coefficients: [[ 0.43310964  1.7172662   1.8170949   2.9003005  -2.0876884  -4.9544506 ]][0m
[37m[1m[2023-07-11 17:43:25,264][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:43:34,193][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 17:43:34,193][233954] FPS: 430176.22[0m
[36m[2023-07-11 17:43:34,195][233954] itr=1287, itrs=2000, Progress: 64.35%[0m
[36m[2023-07-11 17:43:45,983][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 17:43:45,983][233954] FPS: 328748.80[0m
[36m[2023-07-11 17:43:50,227][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:43:50,228][233954] Reward + Measures: [[-10.73275265   0.92658365   0.96936727   0.94876367   0.96122736
    0.79604912]][0m
[37m[1m[2023-07-11 17:43:50,228][233954] Max Reward on eval: -10.732752645868723[0m
[37m[1m[2023-07-11 17:43:50,228][233954] Min Reward on eval: -10.732752645868723[0m
[37m[1m[2023-07-11 17:43:50,229][233954] Mean Reward across all agents: -10.732752645868723[0m
[37m[1m[2023-07-11 17:43:50,229][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:43:55,193][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:43:55,193][233954] Reward + Measures: [[-27.14962102   0.47139999   0.45439997   0.33259997   0.3073
    1.47288299]
 [  5.40352471   0.89740002   0.0144       0.79430002   0.42540002
    0.88344908]
 [  1.6242147    0.3179       0.24480002   0.296        0.2339
    2.03160405]
 ...
 [ -4.66156242   0.56349999   0.39990002   0.56450003   0.23410001
    1.35606849]
 [ 44.59060431   0.95640004   0.31160003   0.97489995   0.0481
    0.59274894]
 [ 64.12577916   0.83920002   0.0602       0.84060001   0.30560002
    1.32730997]][0m
[37m[1m[2023-07-11 17:43:55,194][233954] Max Reward on eval: 180.96902655940502[0m
[37m[1m[2023-07-11 17:43:55,194][233954] Min Reward on eval: -97.87069023903459[0m
[37m[1m[2023-07-11 17:43:55,194][233954] Mean Reward across all agents: 6.901921018808078[0m
[37m[1m[2023-07-11 17:43:55,194][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:43:55,202][233954] mean_value=-136.44078701699183, max_value=576.879598174151[0m
[37m[1m[2023-07-11 17:43:55,204][233954] New mean coefficients: [[ 0.78656316  1.8439832   0.9814826   3.2648222  -1.7633789  -3.0108414 ]][0m
[37m[1m[2023-07-11 17:43:55,205][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:44:04,168][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 17:44:04,169][233954] FPS: 428509.23[0m
[36m[2023-07-11 17:44:04,171][233954] itr=1288, itrs=2000, Progress: 64.40%[0m
[36m[2023-07-11 17:44:15,970][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 17:44:15,970][233954] FPS: 328398.44[0m
[36m[2023-07-11 17:44:20,199][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:44:20,200][233954] Reward + Measures: [[19.49768791  0.95602304  0.98968601  0.08421     0.97181934  0.53375846]][0m
[37m[1m[2023-07-11 17:44:20,200][233954] Max Reward on eval: 19.49768790841456[0m
[37m[1m[2023-07-11 17:44:20,200][233954] Min Reward on eval: 19.49768790841456[0m
[37m[1m[2023-07-11 17:44:20,200][233954] Mean Reward across all agents: 19.49768790841456[0m
[37m[1m[2023-07-11 17:44:20,201][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:44:25,157][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:44:25,158][233954] Reward + Measures: [[-33.49928464   0.33680004   0.29130003   0.31149998   0.44150001
    1.99630415]
 [223.71112251   0.99650002   0.7737       0.99790001   0.0024
    1.94477296]
 [ 14.83038351   0.65399998   0.66730005   0.55320001   0.51220006
    1.80423737]
 ...
 [-36.88254766   0.83829993   0.68120003   0.69440001   0.54339999
    1.19842589]
 [-41.36534315   0.72589999   0.44220001   0.53880006   0.65270001
    0.73957366]
 [116.16760159   0.97460002   0.81870002   0.93669999   0.053
    1.51495159]][0m
[37m[1m[2023-07-11 17:44:25,158][233954] Max Reward on eval: 581.2025070246309[0m
[37m[1m[2023-07-11 17:44:25,158][233954] Min Reward on eval: -79.04517437964678[0m
[37m[1m[2023-07-11 17:44:25,158][233954] Mean Reward across all agents: 51.59033336880986[0m
[37m[1m[2023-07-11 17:44:25,159][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:44:25,168][233954] mean_value=25.943664786618246, max_value=835.0067400553962[0m
[37m[1m[2023-07-11 17:44:25,171][233954] New mean coefficients: [[ 2.2717762  1.2948294  1.46636    2.3952456 -1.4803829 -3.133682 ]][0m
[37m[1m[2023-07-11 17:44:25,172][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:44:34,103][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 17:44:34,103][233954] FPS: 430044.04[0m
[36m[2023-07-11 17:44:34,105][233954] itr=1289, itrs=2000, Progress: 64.45%[0m
[36m[2023-07-11 17:44:45,822][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 17:44:45,823][233954] FPS: 330662.63[0m
[36m[2023-07-11 17:44:50,151][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:44:50,151][233954] Reward + Measures: [[-47.44605472   0.93649834   0.00334267   0.86998898   0.70992935
    0.45257625]][0m
[37m[1m[2023-07-11 17:44:50,151][233954] Max Reward on eval: -47.446054720330324[0m
[37m[1m[2023-07-11 17:44:50,152][233954] Min Reward on eval: -47.446054720330324[0m
[37m[1m[2023-07-11 17:44:50,152][233954] Mean Reward across all agents: -47.446054720330324[0m
[37m[1m[2023-07-11 17:44:50,152][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:44:55,175][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:44:55,181][233954] Reward + Measures: [[  25.03138251    0.90420002    0.0129        0.94560003    0.63219994
     0.57321626]
 [-140.2780956     0.70520002    0.0857        0.66950005    0.5424
     1.05407548]
 [   3.65359038    0.77020007    0.12650001    0.70260006    0.54000002
     1.00093472]
 ...
 [  63.66838016    0.7511        0.28120002    0.68629998    0.51879996
     1.21860015]
 [  13.13362675    0.7137        0.26750001    0.59470004    0.60509998
     0.87527382]
 [  92.66024118    0.74040002    0.15979999    0.72170001    0.46779999
     1.03586662]][0m
[37m[1m[2023-07-11 17:44:55,181][233954] Max Reward on eval: 158.28912881780417[0m
[37m[1m[2023-07-11 17:44:55,181][233954] Min Reward on eval: -145.21812534444035[0m
[37m[1m[2023-07-11 17:44:55,182][233954] Mean Reward across all agents: 21.784819656012694[0m
[37m[1m[2023-07-11 17:44:55,182][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:44:55,192][233954] mean_value=88.34706184114009, max_value=535.5731605552137[0m
[37m[1m[2023-07-11 17:44:55,195][233954] New mean coefficients: [[ 2.6597252  1.6296443  1.4835212  2.077884  -1.7331729 -3.1119716]][0m
[37m[1m[2023-07-11 17:44:55,196][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:45:04,264][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 17:45:04,265][233954] FPS: 423523.10[0m
[36m[2023-07-11 17:45:04,267][233954] itr=1290, itrs=2000, Progress: 64.50%[0m
[37m[1m[2023-07-11 17:48:47,517][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001270[0m
[36m[2023-07-11 17:48:59,868][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 17:48:59,869][233954] FPS: 328853.43[0m
[36m[2023-07-11 17:49:04,073][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:49:04,073][233954] Reward + Measures: [[41.91895617  0.8830573   0.07752733  0.9632923   0.31730098  0.37253398]][0m
[37m[1m[2023-07-11 17:49:04,073][233954] Max Reward on eval: 41.918956168550366[0m
[37m[1m[2023-07-11 17:49:04,074][233954] Min Reward on eval: 41.918956168550366[0m
[37m[1m[2023-07-11 17:49:04,074][233954] Mean Reward across all agents: 41.918956168550366[0m
[37m[1m[2023-07-11 17:49:04,074][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:49:09,322][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:49:09,383][233954] Reward + Measures: [[ 73.88973526   0.60650003   0.49470001   0.32449999   0.58310002
    0.75965875]
 [  1.33228296   0.23190001   0.33389997   0.1692       0.38609999
    2.068362  ]
 [-26.02534367   0.60000002   0.38009998   0.61289996   0.31150001
    1.01748335]
 ...
 [ 71.67530109   0.29820001   0.30560002   0.22120002   0.32170001
    1.72973847]
 [201.8431778    0.46300003   0.33759999   0.62260002   0.40670004
    1.19926262]
 [211.15296934   0.51249999   0.28669998   0.64400005   0.3985
    1.15738201]][0m
[37m[1m[2023-07-11 17:49:09,384][233954] Max Reward on eval: 308.71193696446716[0m
[37m[1m[2023-07-11 17:49:09,384][233954] Min Reward on eval: -126.128934796236[0m
[37m[1m[2023-07-11 17:49:09,384][233954] Mean Reward across all agents: 26.25802086934532[0m
[37m[1m[2023-07-11 17:49:09,384][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:49:09,391][233954] mean_value=-157.6390915183884, max_value=541.3570040867291[0m
[37m[1m[2023-07-11 17:49:09,394][233954] New mean coefficients: [[ 2.0775206   1.8818789   2.0615807   0.51491773 -0.10156798 -2.224049  ]][0m
[37m[1m[2023-07-11 17:49:09,395][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:49:18,465][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 17:49:18,465][233954] FPS: 423469.09[0m
[36m[2023-07-11 17:49:18,467][233954] itr=1291, itrs=2000, Progress: 64.55%[0m
[36m[2023-07-11 17:49:30,253][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 17:49:30,253][233954] FPS: 328732.97[0m
[36m[2023-07-11 17:49:34,580][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:49:34,580][233954] Reward + Measures: [[99.75366466  0.87859637  0.42493895  0.90036196  0.06269533  0.58112448]][0m
[37m[1m[2023-07-11 17:49:34,581][233954] Max Reward on eval: 99.75366465654479[0m
[37m[1m[2023-07-11 17:49:34,581][233954] Min Reward on eval: 99.75366465654479[0m
[37m[1m[2023-07-11 17:49:34,581][233954] Mean Reward across all agents: 99.75366465654479[0m
[37m[1m[2023-07-11 17:49:34,581][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:49:39,555][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:49:39,556][233954] Reward + Measures: [[ 52.40640112   0.59280002   0.31610003   0.6243       0.25420001
    1.23286819]
 [-12.80160651   0.33039999   0.26030001   0.25439999   0.30239996
    1.95135748]
 [ 39.40182298   0.85020012   0.77829999   0.8775       0.0048
    1.03049231]
 ...
 [ 95.15113927   0.72749996   0.35710001   0.77750003   0.20299999
    0.92855865]
 [-87.81380852   0.60790002   0.30500004   0.48389998   0.39120001
    0.9393611 ]
 [ 34.31217979   0.60780001   0.1103       0.68000001   0.38510001
    0.91175491]][0m
[37m[1m[2023-07-11 17:49:39,556][233954] Max Reward on eval: 275.3033140184358[0m
[37m[1m[2023-07-11 17:49:39,556][233954] Min Reward on eval: -191.6544418340549[0m
[37m[1m[2023-07-11 17:49:39,557][233954] Mean Reward across all agents: 19.22294066102025[0m
[37m[1m[2023-07-11 17:49:39,557][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:49:39,564][233954] mean_value=-18.963908776995133, max_value=586.4029674770311[0m
[37m[1m[2023-07-11 17:49:39,567][233954] New mean coefficients: [[ 1.8493392   1.6441319   1.9826043   0.79693234 -0.5080018  -2.0345824 ]][0m
[37m[1m[2023-07-11 17:49:39,568][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:49:48,607][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 17:49:48,607][233954] FPS: 424922.48[0m
[36m[2023-07-11 17:49:48,609][233954] itr=1292, itrs=2000, Progress: 64.60%[0m
[36m[2023-07-11 17:50:00,569][233954] train() took 11.85 seconds to complete[0m
[36m[2023-07-11 17:50:00,569][233954] FPS: 323961.00[0m
[36m[2023-07-11 17:50:04,838][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:50:04,838][233954] Reward + Measures: [[102.8338534    0.93697697   0.58453733   0.91466236   0.00038767
    0.85331446]][0m
[37m[1m[2023-07-11 17:50:04,838][233954] Max Reward on eval: 102.8338534037299[0m
[37m[1m[2023-07-11 17:50:04,839][233954] Min Reward on eval: 102.8338534037299[0m
[37m[1m[2023-07-11 17:50:04,839][233954] Mean Reward across all agents: 102.8338534037299[0m
[37m[1m[2023-07-11 17:50:04,839][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:50:09,773][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:50:09,774][233954] Reward + Measures: [[ 51.76826525   0.43150002   0.57010001   0.28719997   0.6401
    0.87399244]
 [ 42.59696701   0.84330004   0.60100001   0.76340002   0.22239999
    0.64175701]
 [ 71.6916733    0.8596999    0.68799996   0.8488       0.52280003
    1.1790508 ]
 ...
 [-28.00426162   0.88500005   0.34050003   0.81709999   0.1199
    0.61418134]
 [-43.82467804   0.77399999   0.78530002   0.79520005   0.0003
    1.1413424 ]
 [-13.33480007   0.75650001   0.47539997   0.68839997   0.1709
    1.60788906]][0m
[37m[1m[2023-07-11 17:50:09,774][233954] Max Reward on eval: 243.39643287267535[0m
[37m[1m[2023-07-11 17:50:09,774][233954] Min Reward on eval: -231.39785003811122[0m
[37m[1m[2023-07-11 17:50:09,774][233954] Mean Reward across all agents: 42.89447125133895[0m
[37m[1m[2023-07-11 17:50:09,774][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:50:09,783][233954] mean_value=-13.081852699981997, max_value=503.4225805136934[0m
[37m[1m[2023-07-11 17:50:09,786][233954] New mean coefficients: [[ 1.9553779   1.952171    1.2091818   1.5700352   0.06732064 -1.6711814 ]][0m
[37m[1m[2023-07-11 17:50:09,787][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:50:18,785][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 17:50:18,789][233954] FPS: 426838.48[0m
[36m[2023-07-11 17:50:18,791][233954] itr=1293, itrs=2000, Progress: 64.65%[0m
[36m[2023-07-11 17:50:30,458][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 17:50:30,458][233954] FPS: 332121.76[0m
[36m[2023-07-11 17:50:34,715][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:50:34,715][233954] Reward + Measures: [[-36.86065443   0.99161035   0.92388469   0.95146799   0.00063133
    0.81608438]][0m
[37m[1m[2023-07-11 17:50:34,715][233954] Max Reward on eval: -36.8606544324137[0m
[37m[1m[2023-07-11 17:50:34,716][233954] Min Reward on eval: -36.8606544324137[0m
[37m[1m[2023-07-11 17:50:34,716][233954] Mean Reward across all agents: -36.8606544324137[0m
[37m[1m[2023-07-11 17:50:34,716][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:50:39,711][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:50:39,711][233954] Reward + Measures: [[-34.18041859   0.66869998   0.47009999   0.76450002   0.1103
    1.09183776]
 [ 36.16825959   0.70560002   0.69499999   0.76599997   0.071
    0.95757914]
 [  4.63985871   0.79150003   0.47760001   0.66979998   0.18859999
    0.49181557]
 ...
 [ 11.4570854    0.85500002   0.7712       0.88230002   0.2052
    1.12571704]
 [ 49.83136518   0.6936       0.69139999   0.78099996   0.1086
    0.92996198]
 [ 35.24708425   0.88789999   0.85280001   0.87600005   0.1135
    0.85828573]][0m
[37m[1m[2023-07-11 17:50:39,711][233954] Max Reward on eval: 635.8529042965732[0m
[37m[1m[2023-07-11 17:50:39,712][233954] Min Reward on eval: -135.3521713623777[0m
[37m[1m[2023-07-11 17:50:39,712][233954] Mean Reward across all agents: 27.461513095702262[0m
[37m[1m[2023-07-11 17:50:39,712][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:50:39,721][233954] mean_value=53.967178074944265, max_value=894.1858521302231[0m
[37m[1m[2023-07-11 17:50:39,724][233954] New mean coefficients: [[ 1.9197098   1.8766116   1.4316332   1.5501375   1.770433   -0.73466974]][0m
[37m[1m[2023-07-11 17:50:39,725][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:50:48,701][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 17:50:48,702][233954] FPS: 427856.62[0m
[36m[2023-07-11 17:50:48,704][233954] itr=1294, itrs=2000, Progress: 64.70%[0m
[36m[2023-07-11 17:51:00,281][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 17:51:00,281][233954] FPS: 334826.43[0m
[36m[2023-07-11 17:51:04,492][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:51:04,493][233954] Reward + Measures: [[168.005402     0.98469973   0.97058737   0.96704262   0.00342033
    1.65773368]][0m
[37m[1m[2023-07-11 17:51:04,493][233954] Max Reward on eval: 168.00540200220405[0m
[37m[1m[2023-07-11 17:51:04,493][233954] Min Reward on eval: 168.00540200220405[0m
[37m[1m[2023-07-11 17:51:04,494][233954] Mean Reward across all agents: 168.00540200220405[0m
[37m[1m[2023-07-11 17:51:04,494][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:51:09,416][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:51:09,416][233954] Reward + Measures: [[ 45.22509041   0.49840003   0.78209996   0.1842       0.63430005
    2.35551524]
 [ 45.62420132   0.52890003   0.66210002   0.1169       0.69900006
    1.78943574]
 [ 72.26567649   0.28440002   0.51779997   0.15900001   0.49120003
    1.74937081]
 ...
 [355.02611543   0.93340009   0.90039998   0.88569993   0.17910001
    1.9798882 ]
 [ 10.14950912   0.52600002   0.72060007   0.08090001   0.66210002
    2.11160636]
 [ 27.06047073   0.39929998   0.65810001   0.1023       0.53749996
    1.88603616]][0m
[37m[1m[2023-07-11 17:51:09,416][233954] Max Reward on eval: 667.8763217899948[0m
[37m[1m[2023-07-11 17:51:09,417][233954] Min Reward on eval: -249.20711238845252[0m
[37m[1m[2023-07-11 17:51:09,417][233954] Mean Reward across all agents: 81.87776576540064[0m
[37m[1m[2023-07-11 17:51:09,417][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:51:09,428][233954] mean_value=38.64044687131852, max_value=946.0200347966514[0m
[37m[1m[2023-07-11 17:51:09,431][233954] New mean coefficients: [[ 1.210352    0.89198166  1.819435   -0.68113256  2.3035681  -1.3837498 ]][0m
[37m[1m[2023-07-11 17:51:09,432][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:51:18,422][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 17:51:18,427][233954] FPS: 427217.99[0m
[36m[2023-07-11 17:51:18,430][233954] itr=1295, itrs=2000, Progress: 64.75%[0m
[36m[2023-07-11 17:51:30,128][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 17:51:30,128][233954] FPS: 331219.65[0m
[36m[2023-07-11 17:51:34,378][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:51:34,379][233954] Reward + Measures: [[22.80616523  0.98228735  0.98009795  0.98985165  0.002569    2.0859561 ]][0m
[37m[1m[2023-07-11 17:51:34,379][233954] Max Reward on eval: 22.806165229748906[0m
[37m[1m[2023-07-11 17:51:34,379][233954] Min Reward on eval: 22.806165229748906[0m
[37m[1m[2023-07-11 17:51:34,379][233954] Mean Reward across all agents: 22.806165229748906[0m
[37m[1m[2023-07-11 17:51:34,380][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:51:39,421][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:51:39,426][233954] Reward + Measures: [[ 680.8999481     0.98150009    0.9853        0.99139994    0.0019
     3.15951657]
 [ -67.77199666    0.72170007    0.59890002    0.84249991    0.0309
     2.04658008]
 [-141.55123066    0.61789995    0.52800006    0.72509998    0.0725
     2.17378092]
 ...
 [  61.11018493    0.69329995    0.30060002    0.88260001    0.17639999
     1.81419218]
 [-117.52010347    0.52150005    0.65030003    0.33320001    0.3378
     2.67473936]
 [ -21.05050022    0.59930003    0.53420001    0.63340002    0.10450001
     2.24215961]][0m
[37m[1m[2023-07-11 17:51:39,427][233954] Max Reward on eval: 746.9435615524184[0m
[37m[1m[2023-07-11 17:51:39,427][233954] Min Reward on eval: -256.74629596155137[0m
[37m[1m[2023-07-11 17:51:39,427][233954] Mean Reward across all agents: 126.22596980442545[0m
[37m[1m[2023-07-11 17:51:39,427][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:51:39,436][233954] mean_value=104.51270954317805, max_value=1024.9833973146124[0m
[37m[1m[2023-07-11 17:51:39,439][233954] New mean coefficients: [[ 1.5025094   0.73717695  1.894714    0.07493734  2.5963337  -1.4698125 ]][0m
[37m[1m[2023-07-11 17:51:39,440][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:51:48,433][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 17:51:48,433][233954] FPS: 427049.78[0m
[36m[2023-07-11 17:51:48,435][233954] itr=1296, itrs=2000, Progress: 64.80%[0m
[36m[2023-07-11 17:52:00,200][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 17:52:00,200][233954] FPS: 329418.21[0m
[36m[2023-07-11 17:52:04,542][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:52:04,542][233954] Reward + Measures: [[123.47420709   0.94469565   0.93352962   0.92372233   0.03508067
    1.32222915]][0m
[37m[1m[2023-07-11 17:52:04,543][233954] Max Reward on eval: 123.47420708790138[0m
[37m[1m[2023-07-11 17:52:04,543][233954] Min Reward on eval: 123.47420708790138[0m
[37m[1m[2023-07-11 17:52:04,543][233954] Mean Reward across all agents: 123.47420708790138[0m
[37m[1m[2023-07-11 17:52:04,543][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:52:09,840][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:52:09,840][233954] Reward + Measures: [[  96.65912369    0.87060004    0.53669995    0.87130004    0.0401
     2.35131145]
 [ 157.06949806    0.70370001    0.43099999    0.71039999    0.0697
     2.30613351]
 [ 198.14218426    0.90350002    0.72869998    0.77649993    0.0045
     2.95593715]
 ...
 [  55.19078408    0.5783        0.23930001    0.51669997    0.41529998
     1.92150581]
 [ -11.46203845    0.3917        0.22500001    0.41420004    0.20080002
     2.13321376]
 [-260.06258011    0.9795        0.93370003    0.93619996    0.0019
     2.60146427]][0m
[37m[1m[2023-07-11 17:52:09,840][233954] Max Reward on eval: 402.1743287307443[0m
[37m[1m[2023-07-11 17:52:09,841][233954] Min Reward on eval: -368.37663080645723[0m
[37m[1m[2023-07-11 17:52:09,841][233954] Mean Reward across all agents: 70.4446632500638[0m
[37m[1m[2023-07-11 17:52:09,841][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:52:09,850][233954] mean_value=10.990180743257273, max_value=692.3384432584978[0m
[37m[1m[2023-07-11 17:52:09,853][233954] New mean coefficients: [[ 2.368175    1.0344259   1.7500765   0.28718582  3.7801204  -0.93154866]][0m
[37m[1m[2023-07-11 17:52:09,854][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:52:18,857][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 17:52:18,857][233954] FPS: 426576.64[0m
[36m[2023-07-11 17:52:18,859][233954] itr=1297, itrs=2000, Progress: 64.85%[0m
[36m[2023-07-11 17:52:30,425][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 17:52:30,426][233954] FPS: 335047.42[0m
[36m[2023-07-11 17:52:34,730][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:52:34,731][233954] Reward + Measures: [[114.09435713   0.93509233   0.92153329   0.91275829   0.064594
    1.23919785]][0m
[37m[1m[2023-07-11 17:52:34,731][233954] Max Reward on eval: 114.09435712649503[0m
[37m[1m[2023-07-11 17:52:34,731][233954] Min Reward on eval: 114.09435712649503[0m
[37m[1m[2023-07-11 17:52:34,731][233954] Mean Reward across all agents: 114.09435712649503[0m
[37m[1m[2023-07-11 17:52:34,732][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:52:39,674][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:52:39,675][233954] Reward + Measures: [[ 80.6723591    0.54729998   0.41330001   0.51440001   0.30740002
    1.87989318]
 [156.96249484   0.62490004   0.43440005   0.56290001   0.20560001
    2.45171523]
 [467.33868409   0.97690004   0.84969997   0.93400002   0.0085
    2.35568929]
 ...
 [ 67.98694105   0.58030003   0.4729       0.6006       0.45280001
    1.58635724]
 [653.52604294   0.93230003   0.75470001   0.89020008   0.0161
    2.69880462]
 [201.10972278   0.59039992   0.38250002   0.51960003   0.1087
    2.49324751]][0m
[37m[1m[2023-07-11 17:52:39,675][233954] Max Reward on eval: 761.121711723879[0m
[37m[1m[2023-07-11 17:52:39,675][233954] Min Reward on eval: -45.21940888045356[0m
[37m[1m[2023-07-11 17:52:39,676][233954] Mean Reward across all agents: 248.32620021883864[0m
[37m[1m[2023-07-11 17:52:39,676][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:52:39,687][233954] mean_value=144.65007580671494, max_value=871.8662000012092[0m
[37m[1m[2023-07-11 17:52:39,690][233954] New mean coefficients: [[ 2.9481425   1.0360888   0.53443444  0.4485012   4.2150044  -0.8094806 ]][0m
[37m[1m[2023-07-11 17:52:39,691][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:52:48,684][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 17:52:48,684][233954] FPS: 427065.69[0m
[36m[2023-07-11 17:52:48,686][233954] itr=1298, itrs=2000, Progress: 64.90%[0m
[36m[2023-07-11 17:53:00,264][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 17:53:00,265][233954] FPS: 334710.51[0m
[36m[2023-07-11 17:53:04,582][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:53:04,582][233954] Reward + Measures: [[141.51737398   0.94214964   0.94359702   0.95086068   0.00845333
    1.30148304]][0m
[37m[1m[2023-07-11 17:53:04,582][233954] Max Reward on eval: 141.51737397756403[0m
[37m[1m[2023-07-11 17:53:04,582][233954] Min Reward on eval: 141.51737397756403[0m
[37m[1m[2023-07-11 17:53:04,583][233954] Mean Reward across all agents: 141.51737397756403[0m
[37m[1m[2023-07-11 17:53:04,583][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:53:09,597][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:53:09,598][233954] Reward + Measures: [[  13.7197999     0.1525        0.66650003    0.54339999    0.60790002
     2.16420269]
 [ -48.94087867    0.79170001    0.66040003    0.67140001    0.09670001
     1.82624841]
 [  53.14274645    0.36939999    0.31259999    0.3867        0.28839999
     1.78989315]
 ...
 [ 106.71882846    0.38799998    0.33130002    0.42420003    0.21659999
     1.91751826]
 [  32.52405746    0.4677        0.76169997    0.46470004    0.71690005
     1.45232487]
 [-115.02360662    0.86370003    0.72900003    0.64799994    0.0945
     2.05401897]][0m
[37m[1m[2023-07-11 17:53:09,598][233954] Max Reward on eval: 374.7011732990853[0m
[37m[1m[2023-07-11 17:53:09,598][233954] Min Reward on eval: -154.30632402580233[0m
[37m[1m[2023-07-11 17:53:09,598][233954] Mean Reward across all agents: 67.90601813879022[0m
[37m[1m[2023-07-11 17:53:09,599][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:53:09,605][233954] mean_value=-125.86573673231811, max_value=621.0723400391638[0m
[37m[1m[2023-07-11 17:53:09,608][233954] New mean coefficients: [[ 2.0027003   1.1430876   0.68306524  0.39108932  3.2773097  -1.0655603 ]][0m
[37m[1m[2023-07-11 17:53:09,609][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:53:18,618][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 17:53:18,618][233954] FPS: 426318.42[0m
[36m[2023-07-11 17:53:18,620][233954] itr=1299, itrs=2000, Progress: 64.95%[0m
[36m[2023-07-11 17:53:30,444][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 17:53:30,449][233954] FPS: 327800.15[0m
[36m[2023-07-11 17:53:34,695][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:53:34,696][233954] Reward + Measures: [[159.74717194   0.92632127   0.92835796   0.95946628   0.00274767
    1.28635168]][0m
[37m[1m[2023-07-11 17:53:34,696][233954] Max Reward on eval: 159.74717193614933[0m
[37m[1m[2023-07-11 17:53:34,696][233954] Min Reward on eval: 159.74717193614933[0m
[37m[1m[2023-07-11 17:53:34,697][233954] Mean Reward across all agents: 159.74717193614933[0m
[37m[1m[2023-07-11 17:53:34,697][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:53:39,653][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:53:39,654][233954] Reward + Measures: [[113.82864761   0.96200001   0.95720005   0.9795       0.0029
    1.33323991]
 [173.67930129   0.96810001   0.96250004   0.97280008   0.0031
    1.46432674]
 [140.19278716   0.93269998   0.92700005   0.96870005   0.0025
    1.16626704]
 ...
 [150.35863112   0.94859999   0.94249994   0.97560006   0.0029
    1.41792226]
 [121.76178073   0.96449995   0.95990002   0.97670001   0.0024
    1.29457557]
 [111.52571678   0.9677       0.96330005   0.98099995   0.0022
    1.17491663]][0m
[37m[1m[2023-07-11 17:53:39,654][233954] Max Reward on eval: 223.4718017499894[0m
[37m[1m[2023-07-11 17:53:39,654][233954] Min Reward on eval: 48.129381892993116[0m
[37m[1m[2023-07-11 17:53:39,655][233954] Mean Reward across all agents: 118.09507845568456[0m
[37m[1m[2023-07-11 17:53:39,655][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:53:39,658][233954] mean_value=-11.74703219687501, max_value=117.25143962346249[0m
[37m[1m[2023-07-11 17:53:39,661][233954] New mean coefficients: [[ 1.927967   -1.3397784  -0.12247086 -1.108561    3.056467   -0.04654789]][0m
[37m[1m[2023-07-11 17:53:39,662][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:53:48,631][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 17:53:48,631][233954] FPS: 428192.16[0m
[36m[2023-07-11 17:53:48,633][233954] itr=1300, itrs=2000, Progress: 65.00%[0m
[37m[1m[2023-07-11 17:57:32,446][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001280[0m
[36m[2023-07-11 17:57:46,201][233954] train() took 11.81 seconds to complete[0m
[36m[2023-07-11 17:57:46,202][233954] FPS: 325127.99[0m
[36m[2023-07-11 17:57:50,504][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:57:50,504][233954] Reward + Measures: [[131.49482914   0.95378906   0.87726569   0.85433805   0.03424633
    1.31233215]][0m
[37m[1m[2023-07-11 17:57:50,504][233954] Max Reward on eval: 131.49482914013498[0m
[37m[1m[2023-07-11 17:57:50,505][233954] Min Reward on eval: 131.49482914013498[0m
[37m[1m[2023-07-11 17:57:50,505][233954] Mean Reward across all agents: 131.49482914013498[0m
[37m[1m[2023-07-11 17:57:50,505][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:57:55,506][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:57:55,507][233954] Reward + Measures: [[ 45.97674965   0.8427       0.30520001   0.8657999    0.08400001
    1.65294039]
 [ 98.74241109   0.65249997   0.15710001   0.7906       0.15009999
    2.34811592]
 [ 69.03729589   0.77680004   0.1345       0.83269995   0.1313
    2.19517589]
 ...
 [179.95669007   0.81420004   0.2184       0.8648001    0.14
    1.92411673]
 [ 69.84920098   0.67180002   0.15800001   0.77650005   0.1416
    2.06207108]
 [ -0.37450986   0.76189995   0.43779999   0.82010001   0.0383
    1.43906116]][0m
[37m[1m[2023-07-11 17:57:55,507][233954] Max Reward on eval: 439.6893806281034[0m
[37m[1m[2023-07-11 17:57:55,507][233954] Min Reward on eval: -106.83206994789653[0m
[37m[1m[2023-07-11 17:57:55,508][233954] Mean Reward across all agents: 94.29575149946143[0m
[37m[1m[2023-07-11 17:57:55,508][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:57:55,521][233954] mean_value=242.34275078270284, max_value=756.0088844489306[0m
[37m[1m[2023-07-11 17:57:55,524][233954] New mean coefficients: [[ 1.7980777  -1.119257    0.9400743  -0.80258954  5.1574545   0.3428247 ]][0m
[37m[1m[2023-07-11 17:57:55,525][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:58:04,500][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 17:58:04,500][233954] FPS: 427943.69[0m
[36m[2023-07-11 17:58:04,502][233954] itr=1301, itrs=2000, Progress: 65.05%[0m
[36m[2023-07-11 17:58:16,162][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 17:58:16,163][233954] FPS: 332404.00[0m
[36m[2023-07-11 17:58:20,401][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:58:20,402][233954] Reward + Measures: [[103.84830543   0.91295636   0.74599135   0.77894598   0.17199768
    1.15009868]][0m
[37m[1m[2023-07-11 17:58:20,402][233954] Max Reward on eval: 103.84830543448042[0m
[37m[1m[2023-07-11 17:58:20,402][233954] Min Reward on eval: 103.84830543448042[0m
[37m[1m[2023-07-11 17:58:20,403][233954] Mean Reward across all agents: 103.84830543448042[0m
[37m[1m[2023-07-11 17:58:20,403][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:58:25,612][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:58:25,613][233954] Reward + Measures: [[-21.62575258   0.63749999   0.53640002   0.34549999   0.65430003
    1.20267522]
 [-18.19391435   0.94870007   0.23630002   0.97220004   0.25130001
    1.42089367]
 [ 67.99267387   0.78490013   0.65670002   0.65140003   0.35330001
    1.02647746]
 ...
 [ 36.26115688   0.713        0.50840002   0.74879998   0.23899999
    1.20074093]
 [-57.58613439   0.36930001   0.27930003   0.331        0.40480003
    1.76789117]
 [ 94.03110174   0.8071       0.82909995   0.79520005   0.1646
    1.27886605]][0m
[37m[1m[2023-07-11 17:58:25,613][233954] Max Reward on eval: 197.60735180526973[0m
[37m[1m[2023-07-11 17:58:25,613][233954] Min Reward on eval: -103.79545407344122[0m
[37m[1m[2023-07-11 17:58:25,613][233954] Mean Reward across all agents: 42.3876514704047[0m
[37m[1m[2023-07-11 17:58:25,614][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:58:25,621][233954] mean_value=46.836823954782616, max_value=519.7553666666231[0m
[37m[1m[2023-07-11 17:58:25,624][233954] New mean coefficients: [[ 1.193383  -1.1943344  1.4156296 -1.689472   5.1430087  0.6997843]][0m
[37m[1m[2023-07-11 17:58:25,625][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:58:34,535][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 17:58:34,535][233954] FPS: 431068.84[0m
[36m[2023-07-11 17:58:34,537][233954] itr=1302, itrs=2000, Progress: 65.10%[0m
[36m[2023-07-11 17:58:46,081][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 17:58:46,081][233954] FPS: 335723.52[0m
[36m[2023-07-11 17:58:50,294][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:58:50,300][233954] Reward + Measures: [[86.10173566  0.8897      0.69586766  0.74089062  0.27236697  1.04631245]][0m
[37m[1m[2023-07-11 17:58:50,300][233954] Max Reward on eval: 86.10173566118343[0m
[37m[1m[2023-07-11 17:58:50,300][233954] Min Reward on eval: 86.10173566118343[0m
[37m[1m[2023-07-11 17:58:50,300][233954] Mean Reward across all agents: 86.10173566118343[0m
[37m[1m[2023-07-11 17:58:50,301][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:58:55,223][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:58:55,224][233954] Reward + Measures: [[ -10.40758354    0.91750002    0.7119        0.7766        0.0551
     1.75096059]
 [  30.35399708    0.8096        0.61120003    0.83090001    0.48930001
     1.60845339]
 [ -53.52294432    0.76380002    0.62599999    0.78240007    0.3468
     1.6607188 ]
 ...
 [-126.15738928    0.94169998    0.82790005    0.84590006    0.0249
     2.21702409]
 [ 191.00726506    0.62920004    0.55150002    0.4763        0.22669999
     1.52311575]
 [ -24.249009      0.73699999    0.64699996    0.79159999    0.1434
     1.60449255]][0m
[37m[1m[2023-07-11 17:58:55,224][233954] Max Reward on eval: 376.0266056528315[0m
[37m[1m[2023-07-11 17:58:55,224][233954] Min Reward on eval: -355.45637893364767[0m
[37m[1m[2023-07-11 17:58:55,225][233954] Mean Reward across all agents: -5.536723448023635[0m
[37m[1m[2023-07-11 17:58:55,225][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:58:55,231][233954] mean_value=-39.83107560713528, max_value=567.9047697884473[0m
[37m[1m[2023-07-11 17:58:55,234][233954] New mean coefficients: [[ 0.32404298 -1.3061637   0.43505967 -1.7584248   4.4254637   0.6345766 ]][0m
[37m[1m[2023-07-11 17:58:55,235][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:59:04,247][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 17:59:04,247][233954] FPS: 426177.15[0m
[36m[2023-07-11 17:59:04,249][233954] itr=1303, itrs=2000, Progress: 65.15%[0m
[36m[2023-07-11 17:59:15,916][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 17:59:15,916][233954] FPS: 332137.23[0m
[36m[2023-07-11 17:59:20,195][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:59:20,195][233954] Reward + Measures: [[70.24776023  0.81325799  0.66920668  0.628663    0.42251801  0.93482834]][0m
[37m[1m[2023-07-11 17:59:20,196][233954] Max Reward on eval: 70.24776023329892[0m
[37m[1m[2023-07-11 17:59:20,196][233954] Min Reward on eval: 70.24776023329892[0m
[37m[1m[2023-07-11 17:59:20,196][233954] Mean Reward across all agents: 70.24776023329892[0m
[37m[1m[2023-07-11 17:59:20,196][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:59:25,263][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:59:25,264][233954] Reward + Measures: [[-70.06921249   0.77230006   0.72349995   0.7403       0.65310001
    2.37512946]
 [-72.64079425   0.8707       0.875        0.83920002   0.85009998
    2.59972167]
 [105.51059755   0.76879996   0.33290002   0.73169994   0.1935
    0.96544409]
 ...
 [-10.30976894   0.64819998   0.76669997   0.59610003   0.68870002
    2.387532  ]
 [-21.57187949   0.90509999   0.82779998   0.82100004   0.71069998
    2.40656066]
 [-32.93277824   0.92620003   0.78350002   0.82800007   0.47919998
    2.21954226]][0m
[37m[1m[2023-07-11 17:59:25,264][233954] Max Reward on eval: 282.45710179600866[0m
[37m[1m[2023-07-11 17:59:25,264][233954] Min Reward on eval: -114.09307429722976[0m
[37m[1m[2023-07-11 17:59:25,264][233954] Mean Reward across all agents: 6.913995853712393[0m
[37m[1m[2023-07-11 17:59:25,265][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:59:25,272][233954] mean_value=-22.439907855704533, max_value=552.615058682845[0m
[37m[1m[2023-07-11 17:59:25,274][233954] New mean coefficients: [[ 0.49596953 -0.91453165 -1.1426336  -0.6044704   2.252897    0.56535864]][0m
[37m[1m[2023-07-11 17:59:25,275][233954] Moving the mean solution point...[0m
[36m[2023-07-11 17:59:34,145][233954] train() took 8.87 seconds to complete[0m
[36m[2023-07-11 17:59:34,145][233954] FPS: 433029.34[0m
[36m[2023-07-11 17:59:34,147][233954] itr=1304, itrs=2000, Progress: 65.20%[0m
[36m[2023-07-11 17:59:46,562][233954] train() took 12.30 seconds to complete[0m
[36m[2023-07-11 17:59:46,563][233954] FPS: 312115.07[0m
[36m[2023-07-11 17:59:50,279][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:59:50,279][233954] Reward + Measures: [[45.97005298  0.745215    0.6505453   0.57100564  0.49791598  0.88521004]][0m
[37m[1m[2023-07-11 17:59:50,280][233954] Max Reward on eval: 45.970052983979265[0m
[37m[1m[2023-07-11 17:59:50,280][233954] Min Reward on eval: 45.970052983979265[0m
[37m[1m[2023-07-11 17:59:50,280][233954] Mean Reward across all agents: 45.970052983979265[0m
[37m[1m[2023-07-11 17:59:50,280][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:59:55,274][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 17:59:55,274][233954] Reward + Measures: [[ 56.01233333   0.98530006   0.78590006   0.91030008   0.0018
    2.04839492]
 [118.97003743   0.95360005   0.48480001   0.96159995   0.2559
    1.39017117]
 [155.14636605   0.88560003   0.72789997   0.84219998   0.0094
    2.17586303]
 ...
 [162.19741058   0.9113       0.63449997   0.87239999   0.0081
    1.68998814]
 [338.05741024   0.93889999   0.75570005   0.87200004   0.0033
    2.32303524]
 [105.77524639   0.8915       0.59190005   0.71140003   0.0165
    2.67402077]][0m
[37m[1m[2023-07-11 17:59:55,275][233954] Max Reward on eval: 338.05741024361924[0m
[37m[1m[2023-07-11 17:59:55,275][233954] Min Reward on eval: -243.99887707123997[0m
[37m[1m[2023-07-11 17:59:55,275][233954] Mean Reward across all agents: 50.195820177317266[0m
[37m[1m[2023-07-11 17:59:55,275][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 17:59:55,286][233954] mean_value=-4.094011926618683, max_value=631.4891405050876[0m
[37m[1m[2023-07-11 17:59:55,288][233954] New mean coefficients: [[-0.03267318 -1.5980637  -0.68855035 -1.1781805   0.7382289   0.3295461 ]][0m
[37m[1m[2023-07-11 17:59:55,289][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:00:04,204][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 18:00:04,205][233954] FPS: 430806.58[0m
[36m[2023-07-11 18:00:04,207][233954] itr=1305, itrs=2000, Progress: 65.25%[0m
[36m[2023-07-11 18:00:15,756][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 18:00:15,756][233954] FPS: 335580.26[0m
[36m[2023-07-11 18:00:19,933][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:00:19,939][233954] Reward + Measures: [[30.4484926   0.67326701  0.64715731  0.51078498  0.55241531  0.8628239 ]][0m
[37m[1m[2023-07-11 18:00:19,939][233954] Max Reward on eval: 30.44849259733802[0m
[37m[1m[2023-07-11 18:00:19,939][233954] Min Reward on eval: 30.44849259733802[0m
[37m[1m[2023-07-11 18:00:19,939][233954] Mean Reward across all agents: 30.44849259733802[0m
[37m[1m[2023-07-11 18:00:19,940][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:00:24,905][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:00:24,906][233954] Reward + Measures: [[106.998806     0.6724       0.61869997   0.50410002   0.60090005
    0.810022  ]
 [ -0.32930899   0.61610001   0.70769995   0.53739995   0.64390004
    0.87737721]
 [ 42.11447919   0.85860008   0.62120003   0.68580002   0.33870003
    1.00201631]
 ...
 [ 38.0927164    0.71450007   0.65980005   0.76990002   0.62919998
    0.93378514]
 [ 26.13843382   0.72630006   0.73100007   0.72820002   0.69620007
    0.87438279]
 [ 89.39809036   0.80639994   0.66319996   0.6767       0.29620001
    1.05311048]][0m
[37m[1m[2023-07-11 18:00:24,906][233954] Max Reward on eval: 183.17424099947092[0m
[37m[1m[2023-07-11 18:00:24,906][233954] Min Reward on eval: -29.50828287508339[0m
[37m[1m[2023-07-11 18:00:24,907][233954] Mean Reward across all agents: 60.124507645770926[0m
[37m[1m[2023-07-11 18:00:24,907][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:00:24,913][233954] mean_value=12.0014207050469, max_value=511.4739499585331[0m
[37m[1m[2023-07-11 18:00:24,916][233954] New mean coefficients: [[-0.8231358  -0.23410738 -1.0209267   0.04085481 -0.32622504  0.01807991]][0m
[37m[1m[2023-07-11 18:00:24,917][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:00:33,820][233954] train() took 8.90 seconds to complete[0m
[36m[2023-07-11 18:00:33,820][233954] FPS: 431377.56[0m
[36m[2023-07-11 18:00:33,823][233954] itr=1306, itrs=2000, Progress: 65.30%[0m
[36m[2023-07-11 18:00:45,529][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 18:00:45,529][233954] FPS: 331035.03[0m
[36m[2023-07-11 18:00:49,886][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:00:49,887][233954] Reward + Measures: [[2.11365752 0.645118   0.60888934 0.56068498 0.53125167 0.89850616]][0m
[37m[1m[2023-07-11 18:00:49,887][233954] Max Reward on eval: 2.1136575164391203[0m
[37m[1m[2023-07-11 18:00:49,887][233954] Min Reward on eval: 2.1136575164391203[0m
[37m[1m[2023-07-11 18:00:49,888][233954] Mean Reward across all agents: 2.1136575164391203[0m
[37m[1m[2023-07-11 18:00:49,888][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:00:54,800][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:00:54,801][233954] Reward + Measures: [[-22.82295716   0.54460001   0.64320004   0.44320002   0.53619999
    0.8794837 ]
 [ 50.00224312   0.65240002   0.63889998   0.47459999   0.48990002
    0.85642356]
 [-12.87655202   0.58939999   0.64919996   0.49419999   0.53369999
    0.95020813]
 ...
 [109.00864888   0.93850005   0.84779996   0.89019996   0.68190002
    1.29096222]
 [  7.0687533    0.65240002   0.60429996   0.56940001   0.33479998
    0.89008629]
 [ 45.78974271   0.73280001   0.72770005   0.65079999   0.21750002
    1.48303711]][0m
[37m[1m[2023-07-11 18:00:54,801][233954] Max Reward on eval: 167.05049323509448[0m
[37m[1m[2023-07-11 18:00:54,801][233954] Min Reward on eval: -60.241423527454025[0m
[37m[1m[2023-07-11 18:00:54,802][233954] Mean Reward across all agents: 49.24459538064087[0m
[37m[1m[2023-07-11 18:00:54,802][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:00:54,811][233954] mean_value=55.571105593833416, max_value=590.9494509675773[0m
[37m[1m[2023-07-11 18:00:54,813][233954] New mean coefficients: [[-0.4600229   0.91818225 -3.004587    1.8944653  -1.2405005   0.34505373]][0m
[37m[1m[2023-07-11 18:00:54,814][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:01:03,786][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 18:01:03,786][233954] FPS: 428089.80[0m
[36m[2023-07-11 18:01:03,789][233954] itr=1307, itrs=2000, Progress: 65.35%[0m
[36m[2023-07-11 18:01:15,389][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 18:01:15,390][233954] FPS: 334144.57[0m
[36m[2023-07-11 18:01:19,636][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:01:19,641][233954] Reward + Measures: [[-0.22511812  0.66492933  0.56332868  0.59338534  0.47682068  0.90264601]][0m
[37m[1m[2023-07-11 18:01:19,642][233954] Max Reward on eval: -0.22511811527151926[0m
[37m[1m[2023-07-11 18:01:19,642][233954] Min Reward on eval: -0.22511811527151926[0m
[37m[1m[2023-07-11 18:01:19,642][233954] Mean Reward across all agents: -0.22511811527151926[0m
[37m[1m[2023-07-11 18:01:19,642][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:01:24,853][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:01:24,859][233954] Reward + Measures: [[138.71833036   0.76200002   0.60680002   0.67989999   0.16039999
    1.32158458]
 [154.60081164   0.47989997   0.5819       0.2748       0.37350002
    0.98987752]
 [ 84.67791219   0.83960003   0.5686       0.75889999   0.13710001
    1.02752006]
 ...
 [ 20.77074702   0.625        0.57859999   0.52220005   0.35859999
    0.9856208 ]
 [ 83.79908988   0.67370003   0.5571       0.56510001   0.29280001
    1.07805395]
 [ 93.06035711   0.63910002   0.51309997   0.51700002   0.33570001
    1.09232664]][0m
[37m[1m[2023-07-11 18:01:24,859][233954] Max Reward on eval: 210.33834978505038[0m
[37m[1m[2023-07-11 18:01:24,859][233954] Min Reward on eval: -17.87361819925718[0m
[37m[1m[2023-07-11 18:01:24,860][233954] Mean Reward across all agents: 87.16647340963279[0m
[37m[1m[2023-07-11 18:01:24,860][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:01:24,867][233954] mean_value=7.839146244526604, max_value=353.5394714055494[0m
[37m[1m[2023-07-11 18:01:24,870][233954] New mean coefficients: [[-0.32070595  2.218408   -3.2350059   2.140429   -1.065133    0.5540246 ]][0m
[37m[1m[2023-07-11 18:01:24,871][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:01:33,803][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 18:01:33,803][233954] FPS: 429974.72[0m
[36m[2023-07-11 18:01:33,806][233954] itr=1308, itrs=2000, Progress: 65.40%[0m
[36m[2023-07-11 18:01:45,424][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 18:01:45,424][233954] FPS: 333665.95[0m
[36m[2023-07-11 18:01:49,767][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:01:49,773][233954] Reward + Measures: [[4.0776229  0.710186   0.53396034 0.65742534 0.46093971 0.93161118]][0m
[37m[1m[2023-07-11 18:01:49,773][233954] Max Reward on eval: 4.077622901003381[0m
[37m[1m[2023-07-11 18:01:49,773][233954] Min Reward on eval: 4.077622901003381[0m
[37m[1m[2023-07-11 18:01:49,774][233954] Mean Reward across all agents: 4.077622901003381[0m
[37m[1m[2023-07-11 18:01:49,774][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:01:54,788][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:01:54,793][233954] Reward + Measures: [[ 55.2408407    0.87220001   0.60659999   0.81560004   0.4217
    0.9262234 ]
 [162.91711118   0.48839998   0.45389995   0.47930002   0.0296
    2.17372584]
 [ 16.52181662   0.64190006   0.73779994   0.53839999   0.61570001
    1.34493196]
 ...
 [172.04843284   0.71149999   0.5406       0.6742       0.13780001
    1.54516888]
 [ 76.6852472    0.6645       0.64609998   0.69510001   0.51440001
    1.16645563]
 [ 76.54431124   0.78459996   0.51359999   0.76179999   0.3928
    1.31083739]][0m
[37m[1m[2023-07-11 18:01:54,793][233954] Max Reward on eval: 377.54273222156337[0m
[37m[1m[2023-07-11 18:01:54,793][233954] Min Reward on eval: -75.00675083221867[0m
[37m[1m[2023-07-11 18:01:54,794][233954] Mean Reward across all agents: 118.50851848173004[0m
[37m[1m[2023-07-11 18:01:54,794][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:01:54,803][233954] mean_value=69.40853154948644, max_value=840.657031375356[0m
[37m[1m[2023-07-11 18:01:54,806][233954] New mean coefficients: [[-0.5057249  2.3052132 -3.8155305  2.6090813 -3.8591104 -0.4574107]][0m
[37m[1m[2023-07-11 18:01:54,807][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:02:03,887][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 18:02:03,888][233954] FPS: 422976.61[0m
[36m[2023-07-11 18:02:03,890][233954] itr=1309, itrs=2000, Progress: 65.45%[0m
[36m[2023-07-11 18:02:15,696][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 18:02:15,697][233954] FPS: 328157.37[0m
[36m[2023-07-11 18:02:20,015][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:02:20,016][233954] Reward + Measures: [[34.5561858   0.78351206  0.46791732  0.74433494  0.34234866  0.93467528]][0m
[37m[1m[2023-07-11 18:02:20,016][233954] Max Reward on eval: 34.55618579737747[0m
[37m[1m[2023-07-11 18:02:20,016][233954] Min Reward on eval: 34.55618579737747[0m
[37m[1m[2023-07-11 18:02:20,016][233954] Mean Reward across all agents: 34.55618579737747[0m
[37m[1m[2023-07-11 18:02:20,017][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:02:25,016][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:02:25,021][233954] Reward + Measures: [[ 95.94060127   0.5467       0.45740005   0.61230004   0.4271
    1.49852109]
 [ 60.33328603   0.80489999   0.51270002   0.83529997   0.3299
    1.78892481]
 [144.36081315   0.3838       0.41810003   0.3335       0.50769997
    1.78255737]
 ...
 [ 83.81883852   0.36880001   0.4073       0.40050003   0.42109999
    1.77560747]
 [134.8059139    0.94190007   0.71920007   0.9533       0.56669998
    1.53633857]
 [ 39.08539772   0.94410002   0.60540003   0.9321       0.35279998
    1.14340818]][0m
[37m[1m[2023-07-11 18:02:25,022][233954] Max Reward on eval: 230.54987144032492[0m
[37m[1m[2023-07-11 18:02:25,022][233954] Min Reward on eval: -56.935750538285355[0m
[37m[1m[2023-07-11 18:02:25,022][233954] Mean Reward across all agents: 76.54739228677877[0m
[37m[1m[2023-07-11 18:02:25,023][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:02:25,031][233954] mean_value=27.807923008076123, max_value=528.390001965563[0m
[37m[1m[2023-07-11 18:02:25,034][233954] New mean coefficients: [[-1.1958063   1.6881726  -2.642974    1.7718406  -4.281968   -0.67563105]][0m
[37m[1m[2023-07-11 18:02:25,035][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:02:34,035][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 18:02:34,035][233954] FPS: 426759.44[0m
[36m[2023-07-11 18:02:34,037][233954] itr=1310, itrs=2000, Progress: 65.50%[0m
[37m[1m[2023-07-11 18:06:24,269][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001290[0m
[36m[2023-07-11 18:06:38,565][233954] train() took 12.09 seconds to complete[0m
[36m[2023-07-11 18:06:38,566][233954] FPS: 317640.40[0m
[36m[2023-07-11 18:06:42,742][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:06:42,742][233954] Reward + Measures: [[95.49237469  0.96259129  0.52183729  0.93326497  0.04867033  1.14686966]][0m
[37m[1m[2023-07-11 18:06:42,742][233954] Max Reward on eval: 95.49237468879485[0m
[37m[1m[2023-07-11 18:06:42,743][233954] Min Reward on eval: 95.49237468879485[0m
[37m[1m[2023-07-11 18:06:42,743][233954] Mean Reward across all agents: 95.49237468879485[0m
[37m[1m[2023-07-11 18:06:42,743][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:06:47,702][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:06:47,703][233954] Reward + Measures: [[ 11.8895754    0.71150005   0.41069999   0.72479999   0.28210002
    0.9173609 ]
 [ 52.35972515   0.72910005   0.43530002   0.70609999   0.34130001
    0.94547999]
 [ 61.1697462    0.93450004   0.44230005   0.89829999   0.2454
    1.00931668]
 ...
 [102.33157253   0.96880001   0.55479997   0.93349993   0.0612
    1.19400561]
 [ 57.6697607    0.77470005   0.48359999   0.75150007   0.21870001
    1.06151986]
 [ 27.98125265   0.86040002   0.43920001   0.81599998   0.25830004
    0.83100748]][0m
[37m[1m[2023-07-11 18:06:47,703][233954] Max Reward on eval: 243.26527785593643[0m
[37m[1m[2023-07-11 18:06:47,703][233954] Min Reward on eval: 5.348183163814246[0m
[37m[1m[2023-07-11 18:06:47,703][233954] Mean Reward across all agents: 76.41062438454715[0m
[37m[1m[2023-07-11 18:06:47,704][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:06:47,709][233954] mean_value=11.683030422407416, max_value=308.4675288937297[0m
[37m[1m[2023-07-11 18:06:47,712][233954] New mean coefficients: [[-0.92701566  1.4839588  -2.11151     0.63804114 -3.8365815  -0.8960937 ]][0m
[37m[1m[2023-07-11 18:06:47,713][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:06:56,636][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 18:06:56,636][233954] FPS: 430430.17[0m
[36m[2023-07-11 18:06:56,638][233954] itr=1311, itrs=2000, Progress: 65.55%[0m
[36m[2023-07-11 18:07:08,450][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 18:07:08,450][233954] FPS: 327998.32[0m
[36m[2023-07-11 18:07:12,659][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:07:12,660][233954] Reward + Measures: [[104.34312125   0.96795034   0.50572568   0.94341093   0.02948933
    1.19194412]][0m
[37m[1m[2023-07-11 18:07:12,660][233954] Max Reward on eval: 104.34312125008165[0m
[37m[1m[2023-07-11 18:07:12,660][233954] Min Reward on eval: 104.34312125008165[0m
[37m[1m[2023-07-11 18:07:12,661][233954] Mean Reward across all agents: 104.34312125008165[0m
[37m[1m[2023-07-11 18:07:12,661][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:07:17,635][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:07:17,636][233954] Reward + Measures: [[-14.5445547    0.94810003   0.1981       0.949        0.35139999
    1.32406986]
 [ 58.23709751   0.93810004   0.4628       0.95559996   0.35279998
    1.05044067]
 [ 61.52452181   0.949        0.47499999   0.90339994   0.1415
    1.02372456]
 ...
 [-80.520621     0.56920004   0.24340001   0.58160001   0.25
    2.04948616]
 [-57.05591885   0.56990004   0.40820003   0.54230005   0.3646
    1.51985395]
 [ 34.40519185   0.59250003   0.22649999   0.65490001   0.25490001
    1.71780837]][0m
[37m[1m[2023-07-11 18:07:17,636][233954] Max Reward on eval: 260.56733892448244[0m
[37m[1m[2023-07-11 18:07:17,636][233954] Min Reward on eval: -160.35954719334842[0m
[37m[1m[2023-07-11 18:07:17,637][233954] Mean Reward across all agents: 43.76143973348733[0m
[37m[1m[2023-07-11 18:07:17,637][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:07:17,645][233954] mean_value=49.4466465936484, max_value=579.7047962778248[0m
[37m[1m[2023-07-11 18:07:17,648][233954] New mean coefficients: [[-1.1070305   0.86142075 -0.5088222   0.1925562  -4.5044     -1.9511594 ]][0m
[37m[1m[2023-07-11 18:07:17,649][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:07:26,571][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 18:07:26,572][233954] FPS: 430466.74[0m
[36m[2023-07-11 18:07:26,574][233954] itr=1312, itrs=2000, Progress: 65.60%[0m
[36m[2023-07-11 18:07:38,239][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 18:07:38,239][233954] FPS: 332174.46[0m
[36m[2023-07-11 18:07:42,446][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:07:42,447][233954] Reward + Measures: [[103.84748619   0.96969765   0.54664564   0.9439466    0.01772433
    1.18391335]][0m
[37m[1m[2023-07-11 18:07:42,447][233954] Max Reward on eval: 103.84748619329726[0m
[37m[1m[2023-07-11 18:07:42,447][233954] Min Reward on eval: 103.84748619329726[0m
[37m[1m[2023-07-11 18:07:42,447][233954] Mean Reward across all agents: 103.84748619329726[0m
[37m[1m[2023-07-11 18:07:42,447][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:07:47,665][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:07:47,665][233954] Reward + Measures: [[ 63.5258224    0.74980003   0.46560001   0.72609997   0.41950002
    0.80345613]
 [ 67.42979095   0.88790005   0.4086       0.87299997   0.1781
    0.85712957]
 [ 85.7671695    0.96400005   0.49860001   0.95279998   0.0245
    1.18037784]
 ...
 [106.1472311    0.96450007   0.5377       0.95279998   0.0145
    1.16892397]
 [106.86321736   0.96270001   0.49530002   0.95310003   0.0202
    1.54119325]
 [106.70001219   0.95839995   0.58469999   0.95090002   0.0107
    1.19328511]][0m
[37m[1m[2023-07-11 18:07:47,666][233954] Max Reward on eval: 167.61848831139505[0m
[37m[1m[2023-07-11 18:07:47,666][233954] Min Reward on eval: -29.864941557403654[0m
[37m[1m[2023-07-11 18:07:47,666][233954] Mean Reward across all agents: 89.81342323425619[0m
[37m[1m[2023-07-11 18:07:47,666][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:07:47,671][233954] mean_value=-10.144235449565615, max_value=473.5624023159973[0m
[37m[1m[2023-07-11 18:07:47,674][233954] New mean coefficients: [[-0.81372637  0.05396074 -0.714551   -0.00072929 -5.206591   -2.9020464 ]][0m
[37m[1m[2023-07-11 18:07:47,675][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:07:56,608][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 18:07:56,609][233954] FPS: 429903.81[0m
[36m[2023-07-11 18:07:56,611][233954] itr=1313, itrs=2000, Progress: 65.65%[0m
[36m[2023-07-11 18:08:08,252][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 18:08:08,252][233954] FPS: 332970.60[0m
[36m[2023-07-11 18:08:12,505][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:08:12,506][233954] Reward + Measures: [[100.40523437   0.97169435   0.57645595   0.94418967   0.01102133
    1.13275683]][0m
[37m[1m[2023-07-11 18:08:12,506][233954] Max Reward on eval: 100.40523437082139[0m
[37m[1m[2023-07-11 18:08:12,506][233954] Min Reward on eval: 100.40523437082139[0m
[37m[1m[2023-07-11 18:08:12,507][233954] Mean Reward across all agents: 100.40523437082139[0m
[37m[1m[2023-07-11 18:08:12,507][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:08:17,551][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:08:17,552][233954] Reward + Measures: [[135.19001199   0.94309998   0.60939997   0.8950001    0.045
    1.15199459]
 [ 83.31521869   0.91429996   0.70170003   0.82779998   0.1478
    0.98151362]
 [ 75.17531205   0.94960004   0.50050002   0.93310004   0.21890001
    0.93815899]
 ...
 [ 80.00985574   0.82909995   0.71439999   0.76379997   0.13410001
    1.15036654]
 [ 92.72036362   0.88840008   0.26789999   0.82630008   0.18810001
    0.89457893]
 [ 39.86948873   0.84799999   0.63020003   0.78420001   0.1814
    0.95919609]][0m
[37m[1m[2023-07-11 18:08:17,552][233954] Max Reward on eval: 222.11813927227632[0m
[37m[1m[2023-07-11 18:08:17,552][233954] Min Reward on eval: -9.46142467292957[0m
[37m[1m[2023-07-11 18:08:17,552][233954] Mean Reward across all agents: 89.19039973393319[0m
[37m[1m[2023-07-11 18:08:17,553][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:08:17,559][233954] mean_value=24.32722002704118, max_value=362.07567399259875[0m
[37m[1m[2023-07-11 18:08:17,561][233954] New mean coefficients: [[-1.1327915  -1.3775394  -0.61431664 -2.1446419  -7.0570374  -4.1572585 ]][0m
[37m[1m[2023-07-11 18:08:17,562][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:08:26,514][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 18:08:26,514][233954] FPS: 429062.70[0m
[36m[2023-07-11 18:08:26,516][233954] itr=1314, itrs=2000, Progress: 65.70%[0m
[36m[2023-07-11 18:08:38,306][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 18:08:38,307][233954] FPS: 328634.33[0m
[36m[2023-07-11 18:08:42,537][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:08:42,537][233954] Reward + Measures: [[97.48429     0.97163868  0.590303    0.94273806  0.00875267  1.10078681]][0m
[37m[1m[2023-07-11 18:08:42,537][233954] Max Reward on eval: 97.4842899979346[0m
[37m[1m[2023-07-11 18:08:42,538][233954] Min Reward on eval: 97.4842899979346[0m
[37m[1m[2023-07-11 18:08:42,538][233954] Mean Reward across all agents: 97.4842899979346[0m
[37m[1m[2023-07-11 18:08:42,538][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:08:47,504][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:08:47,505][233954] Reward + Measures: [[156.89218813   0.35730001   0.49520001   0.1797       0.44499999
    1.91618919]
 [ 66.23548412   0.42519999   0.42420003   0.2543       0.41850001
    1.55454409]
 [ 56.85389293   0.33590004   0.32140002   0.22440003   0.27649999
    1.7734766 ]
 ...
 [-17.46877669   0.46149999   0.36800003   0.38170001   0.27900001
    1.9274559 ]
 [ 11.74353265   0.59979999   0.57730001   0.60410005   0.53680003
    1.61171186]
 [-49.37442845   0.73379999   0.51340002   0.55070001   0.2059
    1.37474287]][0m
[37m[1m[2023-07-11 18:08:47,505][233954] Max Reward on eval: 269.99807547032833[0m
[37m[1m[2023-07-11 18:08:47,505][233954] Min Reward on eval: -122.20533534393181[0m
[37m[1m[2023-07-11 18:08:47,505][233954] Mean Reward across all agents: 37.21049328496058[0m
[37m[1m[2023-07-11 18:08:47,506][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:08:47,510][233954] mean_value=-180.39749554447383, max_value=403.5861001273195[0m
[37m[1m[2023-07-11 18:08:47,513][233954] New mean coefficients: [[-0.37694257 -1.1435207  -0.69149995 -1.2137325  -5.221641   -3.5552437 ]][0m
[37m[1m[2023-07-11 18:08:47,514][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:08:56,500][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 18:08:56,500][233954] FPS: 427394.71[0m
[36m[2023-07-11 18:08:56,502][233954] itr=1315, itrs=2000, Progress: 65.75%[0m
[36m[2023-07-11 18:09:08,127][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 18:09:08,127][233954] FPS: 333359.91[0m
[36m[2023-07-11 18:09:12,478][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:09:12,479][233954] Reward + Measures: [[107.00524853   0.96669596   0.56538868   0.9443506    0.06187766
    1.05122852]][0m
[37m[1m[2023-07-11 18:09:12,479][233954] Max Reward on eval: 107.00524853219332[0m
[37m[1m[2023-07-11 18:09:12,479][233954] Min Reward on eval: 107.00524853219332[0m
[37m[1m[2023-07-11 18:09:12,480][233954] Mean Reward across all agents: 107.00524853219332[0m
[37m[1m[2023-07-11 18:09:12,480][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:09:17,451][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:09:17,451][233954] Reward + Measures: [[179.54002855   0.87709999   0.78130001   0.77700007   0.01
    1.20350158]
 [ 60.63301323   0.89540005   0.25030002   0.87050003   0.23379998
    0.89750034]
 [ 97.0632997    0.9691       0.43979999   0.94600004   0.0875
    1.18861258]
 ...
 [ 98.13452244   0.93040007   0.54900002   0.88150007   0.0859
    1.02019501]
 [ 95.39852809   0.9709       0.53899997   0.94960004   0.22130001
    0.83699483]
 [ 99.62468529   0.95889997   0.59499997   0.9436       0.3125
    0.98641729]][0m
[37m[1m[2023-07-11 18:09:17,452][233954] Max Reward on eval: 179.54002854647115[0m
[37m[1m[2023-07-11 18:09:17,452][233954] Min Reward on eval: -13.823393531399779[0m
[37m[1m[2023-07-11 18:09:17,452][233954] Mean Reward across all agents: 78.78977349374465[0m
[37m[1m[2023-07-11 18:09:17,452][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:09:17,458][233954] mean_value=19.2358668626309, max_value=338.4100221018222[0m
[37m[1m[2023-07-11 18:09:17,460][233954] New mean coefficients: [[-0.4204492  -0.03606558 -1.6002114  -0.02706647 -5.4492846  -3.4084678 ]][0m
[37m[1m[2023-07-11 18:09:17,461][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:09:26,425][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 18:09:26,426][233954] FPS: 428454.88[0m
[36m[2023-07-11 18:09:26,428][233954] itr=1316, itrs=2000, Progress: 65.80%[0m
[36m[2023-07-11 18:09:38,006][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 18:09:38,006][233954] FPS: 334759.40[0m
[36m[2023-07-11 18:09:42,242][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:09:42,242][233954] Reward + Measures: [[101.44533743   0.95919174   0.53205997   0.94519728   0.03727834
    1.02972269]][0m
[37m[1m[2023-07-11 18:09:42,243][233954] Max Reward on eval: 101.4453374318114[0m
[37m[1m[2023-07-11 18:09:42,243][233954] Min Reward on eval: 101.4453374318114[0m
[37m[1m[2023-07-11 18:09:42,243][233954] Mean Reward across all agents: 101.4453374318114[0m
[37m[1m[2023-07-11 18:09:42,244][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:09:47,190][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:09:47,190][233954] Reward + Measures: [[158.35866831   0.90640002   0.3637       0.9149       0.1549
    1.11477911]
 [128.96724989   0.93339998   0.36089998   0.93380004   0.17839999
    1.22271645]
 [124.8490734    0.94999999   0.41729999   0.93099993   0.0762
    1.05504644]
 ...
 [121.1997242    0.9382       0.477        0.9188       0.056
    0.98269653]
 [138.73137379   0.92670006   0.384        0.93339998   0.11979999
    1.13892162]
 [109.63917925   0.94630003   0.45179996   0.94499999   0.05950001
    1.02429652]][0m
[37m[1m[2023-07-11 18:09:47,191][233954] Max Reward on eval: 170.9285564577207[0m
[37m[1m[2023-07-11 18:09:47,191][233954] Min Reward on eval: 46.647537477873264[0m
[37m[1m[2023-07-11 18:09:47,191][233954] Mean Reward across all agents: 119.39755653527813[0m
[37m[1m[2023-07-11 18:09:47,191][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:09:47,198][233954] mean_value=17.961359699411133, max_value=213.45751617291955[0m
[37m[1m[2023-07-11 18:09:47,200][233954] New mean coefficients: [[-0.31124544  0.5997824  -2.7541456   1.8422885  -8.311613   -3.6137917 ]][0m
[37m[1m[2023-07-11 18:09:47,201][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:09:56,195][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 18:09:56,195][233954] FPS: 427056.37[0m
[36m[2023-07-11 18:09:56,197][233954] itr=1317, itrs=2000, Progress: 65.85%[0m
[36m[2023-07-11 18:10:07,760][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 18:10:07,760][233954] FPS: 335167.81[0m
[36m[2023-07-11 18:10:12,106][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:10:12,107][233954] Reward + Measures: [[91.4046826   0.95449668  0.51455933  0.946711    0.02909533  0.98937696]][0m
[37m[1m[2023-07-11 18:10:12,107][233954] Max Reward on eval: 91.40468259971227[0m
[37m[1m[2023-07-11 18:10:12,107][233954] Min Reward on eval: 91.40468259971227[0m
[37m[1m[2023-07-11 18:10:12,107][233954] Mean Reward across all agents: 91.40468259971227[0m
[37m[1m[2023-07-11 18:10:12,108][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:10:17,118][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:10:17,124][233954] Reward + Measures: [[ 86.22094111   0.92369998   0.4262       0.94300002   0.0657
    1.17005622]
 [ 80.68052246   0.90680009   0.74559999   0.83140004   0.51770002
    0.98022568]
 [-12.54268565   0.6099       0.38660002   0.57709998   0.2703
    1.1116544 ]
 ...
 [ 69.65324771   0.8452       0.36390004   0.8556       0.06
    1.19375992]
 [105.18581487   0.96929997   0.4844       0.94060004   0.0374
    1.05345416]
 [ 85.63401079   0.95200008   0.50270003   0.95459998   0.10570001
    1.05262053]][0m
[37m[1m[2023-07-11 18:10:17,124][233954] Max Reward on eval: 133.93231679741294[0m
[37m[1m[2023-07-11 18:10:17,125][233954] Min Reward on eval: -91.46577451035846[0m
[37m[1m[2023-07-11 18:10:17,125][233954] Mean Reward across all agents: 48.75029245832344[0m
[37m[1m[2023-07-11 18:10:17,125][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:10:17,129][233954] mean_value=-41.30700638586989, max_value=343.73826832919326[0m
[37m[1m[2023-07-11 18:10:17,131][233954] New mean coefficients: [[-0.51476943  0.36218947 -0.8502505   0.89203054 -6.719839   -3.7965248 ]][0m
[37m[1m[2023-07-11 18:10:17,132][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:10:26,166][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 18:10:26,167][233954] FPS: 425134.36[0m
[36m[2023-07-11 18:10:26,169][233954] itr=1318, itrs=2000, Progress: 65.90%[0m
[36m[2023-07-11 18:10:38,212][233954] train() took 11.93 seconds to complete[0m
[36m[2023-07-11 18:10:38,212][233954] FPS: 321804.35[0m
[36m[2023-07-11 18:10:42,514][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:10:42,515][233954] Reward + Measures: [[83.51616147  0.96373338  0.55506766  0.9499203   0.01821467  0.92325252]][0m
[37m[1m[2023-07-11 18:10:42,515][233954] Max Reward on eval: 83.51616146785074[0m
[37m[1m[2023-07-11 18:10:42,515][233954] Min Reward on eval: 83.51616146785074[0m
[37m[1m[2023-07-11 18:10:42,516][233954] Mean Reward across all agents: 83.51616146785074[0m
[37m[1m[2023-07-11 18:10:42,516][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:10:47,820][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:10:47,821][233954] Reward + Measures: [[ 18.83514845   0.73610002   0.50809997   0.64679998   0.47300002
    0.7397216 ]
 [-13.2794186    0.83050007   0.70050007   0.70090002   0.67379999
    0.88133538]
 [105.22026255   0.89320004   0.52969998   0.92080003   0.007
    1.24780917]
 ...
 [ -1.40318429   0.83550006   0.86250001   0.72549999   0.83020002
    1.09214962]
 [126.05942346   0.89530003   0.51500005   0.92309999   0.0092
    1.21604824]
 [ 54.94186447   0.88990003   0.4183       0.87169999   0.44279996
    0.84821224]][0m
[37m[1m[2023-07-11 18:10:47,821][233954] Max Reward on eval: 166.95510387623216[0m
[37m[1m[2023-07-11 18:10:47,821][233954] Min Reward on eval: -74.18970204270445[0m
[37m[1m[2023-07-11 18:10:47,822][233954] Mean Reward across all agents: 54.11665393145747[0m
[37m[1m[2023-07-11 18:10:47,822][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:10:47,826][233954] mean_value=-28.231676529415406, max_value=450.2214500830462[0m
[37m[1m[2023-07-11 18:10:47,829][233954] New mean coefficients: [[ 0.11221421  0.30165976 -1.6337788   1.2568215  -7.7998     -4.2476115 ]][0m
[37m[1m[2023-07-11 18:10:47,830][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:10:56,927][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 18:10:56,928][233954] FPS: 422180.10[0m
[36m[2023-07-11 18:10:56,930][233954] itr=1319, itrs=2000, Progress: 65.95%[0m
[36m[2023-07-11 18:11:08,663][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 18:11:08,663][233954] FPS: 330249.34[0m
[36m[2023-07-11 18:11:13,053][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:11:13,054][233954] Reward + Measures: [[75.7893827   0.9649356   0.54771036  0.95385462  0.01980467  0.85831112]][0m
[37m[1m[2023-07-11 18:11:13,054][233954] Max Reward on eval: 75.78938269756696[0m
[37m[1m[2023-07-11 18:11:13,054][233954] Min Reward on eval: 75.78938269756696[0m
[37m[1m[2023-07-11 18:11:13,054][233954] Mean Reward across all agents: 75.78938269756696[0m
[37m[1m[2023-07-11 18:11:13,055][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:11:18,077][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:11:18,078][233954] Reward + Measures: [[108.62042837   0.65170002   0.52890003   0.72839999   0.0767
    1.36101305]
 [116.75573826   0.94980001   0.72499996   0.86090004   0.0267
    1.07096946]
 [ 95.8883743    0.93610001   0.6814       0.92270005   0.0216
    1.14031088]
 ...
 [195.49165724   0.67079997   0.58290005   0.52710003   0.09420001
    1.96678829]
 [106.93343259   0.96469992   0.78190005   0.92729998   0.0029
    1.13634086]
 [ 95.30205536   0.92070001   0.8804       0.86289996   0.0302
    1.35287762]][0m
[37m[1m[2023-07-11 18:11:18,078][233954] Max Reward on eval: 363.7640219014138[0m
[37m[1m[2023-07-11 18:11:18,078][233954] Min Reward on eval: -89.39334558874835[0m
[37m[1m[2023-07-11 18:11:18,078][233954] Mean Reward across all agents: 93.5720417407046[0m
[37m[1m[2023-07-11 18:11:18,079][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:11:18,083][233954] mean_value=-17.948628942355572, max_value=284.3994585096294[0m
[37m[1m[2023-07-11 18:11:18,086][233954] New mean coefficients: [[-0.12212712  0.43430632 -1.4907597   1.3395815  -7.44478    -4.268763  ]][0m
[37m[1m[2023-07-11 18:11:18,087][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:11:27,199][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 18:11:27,199][233954] FPS: 421504.24[0m
[36m[2023-07-11 18:11:27,201][233954] itr=1320, itrs=2000, Progress: 66.00%[0m
[37m[1m[2023-07-11 18:15:05,832][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001300[0m
[36m[2023-07-11 18:15:17,779][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 18:15:17,779][233954] FPS: 335805.89[0m
[36m[2023-07-11 18:15:21,968][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:15:21,969][233954] Reward + Measures: [[68.79462865  0.970936    0.56226397  0.95742029  0.01347167  0.80308241]][0m
[37m[1m[2023-07-11 18:15:21,969][233954] Max Reward on eval: 68.79462864792907[0m
[37m[1m[2023-07-11 18:15:21,969][233954] Min Reward on eval: 68.79462864792907[0m
[37m[1m[2023-07-11 18:15:21,969][233954] Mean Reward across all agents: 68.79462864792907[0m
[37m[1m[2023-07-11 18:15:21,970][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:15:27,114][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:15:27,120][233954] Reward + Measures: [[ 74.80226041   0.97430003   0.49929997   0.963        0.0176
    1.08442485]
 [-80.36275076   0.76550001   0.4605       0.67020005   0.27360001
    0.89622563]
 [129.21974565   0.88999999   0.60089999   0.86729997   0.0244
    1.26321685]
 ...
 [  2.27222875   0.8556       0.35590002   0.89390004   0.1655
    1.00961494]
 [134.68227769   0.85470003   0.64829999   0.9127       0.0077
    1.36680353]
 [ 74.5401342    0.80459994   0.38659999   0.83569998   0.16010001
    1.33796644]][0m
[37m[1m[2023-07-11 18:15:27,121][233954] Max Reward on eval: 167.1096963804448[0m
[37m[1m[2023-07-11 18:15:27,121][233954] Min Reward on eval: -196.33874795325102[0m
[37m[1m[2023-07-11 18:15:27,121][233954] Mean Reward across all agents: 60.69648091077509[0m
[37m[1m[2023-07-11 18:15:27,122][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:15:27,126][233954] mean_value=-39.344073060598454, max_value=587.5645820426707[0m
[37m[1m[2023-07-11 18:15:27,129][233954] New mean coefficients: [[-1.6581469   0.59718245 -1.0810617   0.55966425 -7.827338   -4.782122  ]][0m
[37m[1m[2023-07-11 18:15:27,130][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:15:36,002][233954] train() took 8.87 seconds to complete[0m
[36m[2023-07-11 18:15:36,003][233954] FPS: 432871.15[0m
[36m[2023-07-11 18:15:36,005][233954] itr=1321, itrs=2000, Progress: 66.05%[0m
[36m[2023-07-11 18:15:47,729][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 18:15:47,730][233954] FPS: 330497.52[0m
[36m[2023-07-11 18:15:51,950][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:15:51,951][233954] Reward + Measures: [[60.40319677  0.97757429  0.57410198  0.96036965  0.011119    0.73746359]][0m
[37m[1m[2023-07-11 18:15:51,951][233954] Max Reward on eval: 60.40319676879347[0m
[37m[1m[2023-07-11 18:15:51,951][233954] Min Reward on eval: 60.40319676879347[0m
[37m[1m[2023-07-11 18:15:51,951][233954] Mean Reward across all agents: 60.40319676879347[0m
[37m[1m[2023-07-11 18:15:51,952][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:15:56,805][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:15:56,805][233954] Reward + Measures: [[ 87.46228933   0.77380002   0.27190003   0.71310002   0.50929999
    0.97820711]
 [-80.13770868   0.84610003   0.56800002   0.80779999   0.0127
    1.21506464]
 [-71.78840305   0.55669999   0.24300002   0.46780005   0.23280001
    0.98341715]
 ...
 [ 21.33489603   0.78039998   0.32730001   0.72690004   0.32210001
    0.87871188]
 [ -2.39237686   0.48779997   0.3053       0.35250002   0.3475
    1.44149101]
 [ 50.55817204   0.73359996   0.13150001   0.70459998   0.32780001
    1.08525074]][0m
[37m[1m[2023-07-11 18:15:56,806][233954] Max Reward on eval: 183.94769693277777[0m
[37m[1m[2023-07-11 18:15:56,806][233954] Min Reward on eval: -139.1670222091023[0m
[37m[1m[2023-07-11 18:15:56,806][233954] Mean Reward across all agents: 32.627106295190316[0m
[37m[1m[2023-07-11 18:15:56,806][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:15:56,813][233954] mean_value=-44.02429048169611, max_value=443.99827518090956[0m
[37m[1m[2023-07-11 18:15:56,815][233954] New mean coefficients: [[-1.901191   0.7292389 -0.767004   0.5896473 -7.429498  -4.6591754]][0m
[37m[1m[2023-07-11 18:15:56,816][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:16:05,790][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 18:16:05,791][233954] FPS: 427975.28[0m
[36m[2023-07-11 18:16:05,793][233954] itr=1322, itrs=2000, Progress: 66.10%[0m
[36m[2023-07-11 18:16:17,353][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 18:16:17,353][233954] FPS: 335255.77[0m
[36m[2023-07-11 18:16:21,567][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:16:21,573][233954] Reward + Measures: [[53.23170081  0.97859472  0.58235133  0.96320534  0.008521    0.68943524]][0m
[37m[1m[2023-07-11 18:16:21,573][233954] Max Reward on eval: 53.231700805106236[0m
[37m[1m[2023-07-11 18:16:21,574][233954] Min Reward on eval: 53.231700805106236[0m
[37m[1m[2023-07-11 18:16:21,574][233954] Mean Reward across all agents: 53.231700805106236[0m
[37m[1m[2023-07-11 18:16:21,574][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:16:26,511][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:16:26,511][233954] Reward + Measures: [[ 22.56614593   0.46190006   0.26890001   0.51139992   0.257
    1.19291425]
 [ 36.82988992   0.6336       0.30230001   0.66119999   0.24170001
    0.8928625 ]
 [-43.9236871    0.42820001   0.5025       0.49390003   0.50209999
    1.5303036 ]
 ...
 [127.06208755   0.89319992   0.42510006   0.91390002   0.0764
    0.82397455]
 [ 40.11647604   0.6024       0.1442       0.63740003   0.23439999
    1.5514617 ]
 [-27.09732783   0.60780001   0.31689999   0.65160006   0.29170001
    0.93409157]][0m
[37m[1m[2023-07-11 18:16:26,511][233954] Max Reward on eval: 379.6316356456373[0m
[37m[1m[2023-07-11 18:16:26,512][233954] Min Reward on eval: -117.07927857134491[0m
[37m[1m[2023-07-11 18:16:26,512][233954] Mean Reward across all agents: 36.53043973023339[0m
[37m[1m[2023-07-11 18:16:26,512][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:16:26,517][233954] mean_value=-403.1064722263469, max_value=458.4909361423789[0m
[37m[1m[2023-07-11 18:16:26,520][233954] New mean coefficients: [[-1.2748824   0.34900016 -0.23744988 -0.9583756  -4.9513607  -4.636125  ]][0m
[37m[1m[2023-07-11 18:16:26,521][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:16:35,532][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 18:16:35,532][233954] FPS: 426229.56[0m
[36m[2023-07-11 18:16:35,534][233954] itr=1323, itrs=2000, Progress: 66.15%[0m
[36m[2023-07-11 18:16:47,109][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 18:16:47,109][233954] FPS: 334869.61[0m
[36m[2023-07-11 18:16:51,399][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:16:51,404][233954] Reward + Measures: [[49.08839738  0.98038763  0.60789698  0.96307504  0.00801067  0.64410049]][0m
[37m[1m[2023-07-11 18:16:51,405][233954] Max Reward on eval: 49.08839737812985[0m
[37m[1m[2023-07-11 18:16:51,405][233954] Min Reward on eval: 49.08839737812985[0m
[37m[1m[2023-07-11 18:16:51,405][233954] Mean Reward across all agents: 49.08839737812985[0m
[37m[1m[2023-07-11 18:16:51,406][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:16:56,459][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:16:56,465][233954] Reward + Measures: [[ 36.23663549   0.39710003   0.41499996   0.45500001   0.3328
    1.26127279]
 [ 21.87519358   0.36740002   0.36670002   0.37059999   0.2191
    1.76302552]
 [ 76.31053612   0.48649999   0.38729998   0.51029998   0.27370003
    1.29706216]
 ...
 [-36.22914822   0.0858       0.62210006   0.48499998   0.5916
    1.5365206 ]
 [ 44.97878237   0.8545       0.48989996   0.82510006   0.1743
    0.68226582]
 [ 64.45129443   0.8075       0.68750006   0.8775       0.64709997
    1.04460227]][0m
[37m[1m[2023-07-11 18:16:56,465][233954] Max Reward on eval: 150.07750514603686[0m
[37m[1m[2023-07-11 18:16:56,466][233954] Min Reward on eval: -70.57230240222998[0m
[37m[1m[2023-07-11 18:16:56,466][233954] Mean Reward across all agents: 21.483265868860627[0m
[37m[1m[2023-07-11 18:16:56,466][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:16:56,471][233954] mean_value=-114.76902634157142, max_value=446.3677749967998[0m
[37m[1m[2023-07-11 18:16:56,474][233954] New mean coefficients: [[-0.58430815  0.92836493 -1.6317531  -0.58716154 -3.9540892  -3.087947  ]][0m
[37m[1m[2023-07-11 18:16:56,475][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:17:05,539][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 18:17:05,539][233954] FPS: 423721.35[0m
[36m[2023-07-11 18:17:05,541][233954] itr=1324, itrs=2000, Progress: 66.20%[0m
[36m[2023-07-11 18:17:17,118][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 18:17:17,119][233954] FPS: 334721.45[0m
[36m[2023-07-11 18:17:21,439][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:17:21,444][233954] Reward + Measures: [[58.13467248  0.93148226  0.22030266  0.92436862  0.09239233  0.62703645]][0m
[37m[1m[2023-07-11 18:17:21,445][233954] Max Reward on eval: 58.13467248156295[0m
[37m[1m[2023-07-11 18:17:21,445][233954] Min Reward on eval: 58.13467248156295[0m
[37m[1m[2023-07-11 18:17:21,445][233954] Mean Reward across all agents: 58.13467248156295[0m
[37m[1m[2023-07-11 18:17:21,446][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:17:26,452][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:17:26,458][233954] Reward + Measures: [[-92.44626152   0.42560002   0.38570002   0.35190001   0.2667
    1.38342345]
 [-11.24604231   0.40079999   0.34910002   0.33489999   0.3272
    1.96954429]
 [  5.64200467   0.7974       0.2395       0.74830002   0.16489999
    1.2942642 ]
 ...
 [ 24.9453588    0.64860004   0.61659998   0.5697       0.1243
    1.47992933]
 [-19.63259715   0.42409998   0.59180003   0.51929998   0.3809
    1.29379737]
 [  5.96287864   0.44239998   0.47550002   0.3263       0.46869999
    0.87757838]][0m
[37m[1m[2023-07-11 18:17:26,458][233954] Max Reward on eval: 152.80379773883033[0m
[37m[1m[2023-07-11 18:17:26,458][233954] Min Reward on eval: -241.026352224499[0m
[37m[1m[2023-07-11 18:17:26,459][233954] Mean Reward across all agents: 15.55274667008378[0m
[37m[1m[2023-07-11 18:17:26,459][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:17:26,463][233954] mean_value=-272.9956741006312, max_value=503.35845686362126[0m
[37m[1m[2023-07-11 18:17:26,466][233954] New mean coefficients: [[-0.37209007  1.6098301  -2.1093967   0.8426527  -3.475632   -2.3223042 ]][0m
[37m[1m[2023-07-11 18:17:26,467][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:17:35,584][233954] train() took 9.12 seconds to complete[0m
[36m[2023-07-11 18:17:35,585][233954] FPS: 421231.01[0m
[36m[2023-07-11 18:17:35,587][233954] itr=1325, itrs=2000, Progress: 66.25%[0m
[36m[2023-07-11 18:17:47,404][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 18:17:47,404][233954] FPS: 327999.51[0m
[36m[2023-07-11 18:17:51,767][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:17:51,772][233954] Reward + Measures: [[46.97386073  0.95537663  0.26076767  0.95371294  0.057622    0.55996859]][0m
[37m[1m[2023-07-11 18:17:51,772][233954] Max Reward on eval: 46.973860734404006[0m
[37m[1m[2023-07-11 18:17:51,773][233954] Min Reward on eval: 46.973860734404006[0m
[37m[1m[2023-07-11 18:17:51,773][233954] Mean Reward across all agents: 46.973860734404006[0m
[37m[1m[2023-07-11 18:17:51,773][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:17:56,801][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:17:56,807][233954] Reward + Measures: [[ 71.7231693    0.76170003   0.1231       0.79320002   0.2379
    1.52375984]
 [-41.66585279   0.75349998   0.26190001   0.79730004   0.35859999
    0.81214267]
 [ 22.59821023   0.94670004   0.0954       0.95190001   0.29730001
    1.44849217]
 ...
 [ -7.7710286    0.50669998   0.30650002   0.51290005   0.3574
    1.03647268]
 [119.55632587   0.55350006   0.60930008   0.64160001   0.2237
    1.32079113]
 [  3.39579487   0.8901       0.12420001   0.87190002   0.26120001
    0.72019917]][0m
[37m[1m[2023-07-11 18:17:56,807][233954] Max Reward on eval: 231.76665209573693[0m
[37m[1m[2023-07-11 18:17:56,807][233954] Min Reward on eval: -88.99650215283036[0m
[37m[1m[2023-07-11 18:17:56,808][233954] Mean Reward across all agents: 48.76335798139634[0m
[37m[1m[2023-07-11 18:17:56,808][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:17:56,815][233954] mean_value=-59.82217069855952, max_value=547.7321001337841[0m
[37m[1m[2023-07-11 18:17:56,817][233954] New mean coefficients: [[-0.05035332  2.0202947  -1.9884154   2.449492   -4.2932444  -2.743029  ]][0m
[37m[1m[2023-07-11 18:17:56,818][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:18:05,886][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 18:18:05,886][233954] FPS: 423562.97[0m
[36m[2023-07-11 18:18:05,888][233954] itr=1326, itrs=2000, Progress: 66.30%[0m
[36m[2023-07-11 18:18:17,686][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 18:18:17,687][233954] FPS: 328450.76[0m
[36m[2023-07-11 18:18:21,969][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:18:21,969][233954] Reward + Measures: [[39.27712453  0.97066998  0.30766031  0.97097427  0.033253    0.49960399]][0m
[37m[1m[2023-07-11 18:18:21,970][233954] Max Reward on eval: 39.277124529254294[0m
[37m[1m[2023-07-11 18:18:21,970][233954] Min Reward on eval: 39.277124529254294[0m
[37m[1m[2023-07-11 18:18:21,970][233954] Mean Reward across all agents: 39.277124529254294[0m
[37m[1m[2023-07-11 18:18:21,970][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:18:27,273][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:18:27,274][233954] Reward + Measures: [[ 30.93903052   0.64289999   0.2608       0.60159999   0.34689999
    1.03052998]
 [-38.28550812   0.52509993   0.29639998   0.5133       0.30070001
    1.4060545 ]
 [  8.78118545   0.76139998   0.28870001   0.69400007   0.34650001
    0.76291418]
 ...
 [  1.99562564   0.6189       0.21010001   0.55409998   0.1912
    1.18375742]
 [ 43.34317109   0.6038       0.29139999   0.51959997   0.3204
    1.78730452]
 [-70.7931044    0.45160004   0.3506       0.45829996   0.38890001
    1.62522888]][0m
[37m[1m[2023-07-11 18:18:27,274][233954] Max Reward on eval: 260.29869080027566[0m
[37m[1m[2023-07-11 18:18:27,274][233954] Min Reward on eval: -145.72935704309492[0m
[37m[1m[2023-07-11 18:18:27,275][233954] Mean Reward across all agents: 25.345616302083272[0m
[37m[1m[2023-07-11 18:18:27,275][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:18:27,279][233954] mean_value=-151.0794068094923, max_value=288.6765085003443[0m
[37m[1m[2023-07-11 18:18:27,282][233954] New mean coefficients: [[-0.10537229  2.0076334  -1.1574256   1.930616   -2.95764    -1.904561  ]][0m
[37m[1m[2023-07-11 18:18:27,283][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:18:36,340][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 18:18:36,340][233954] FPS: 424059.91[0m
[36m[2023-07-11 18:18:36,343][233954] itr=1327, itrs=2000, Progress: 66.35%[0m
[36m[2023-07-11 18:18:48,227][233954] train() took 11.77 seconds to complete[0m
[36m[2023-07-11 18:18:48,228][233954] FPS: 326153.08[0m
[36m[2023-07-11 18:18:52,555][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:18:52,555][233954] Reward + Measures: [[33.3875541   0.976282    0.31053165  0.97611392  0.02617233  0.44017729]][0m
[37m[1m[2023-07-11 18:18:52,556][233954] Max Reward on eval: 33.3875541016822[0m
[37m[1m[2023-07-11 18:18:52,556][233954] Min Reward on eval: 33.3875541016822[0m
[37m[1m[2023-07-11 18:18:52,556][233954] Mean Reward across all agents: 33.3875541016822[0m
[37m[1m[2023-07-11 18:18:52,557][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:18:57,579][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:18:57,584][233954] Reward + Measures: [[ 76.07583122   0.6961       0.59869999   0.7094       0.59840006
    0.97746277]
 [ 97.68597026   0.9346       0.42449999   0.93839997   0.0559
    0.96563929]
 [123.75219011   0.86949998   0.1631       0.85280001   0.26390001
    0.61604804]
 ...
 [ 42.44497873   0.81870002   0.79940003   0.77109998   0.72820002
    0.81907636]
 [ 29.88991509   0.63609999   0.18020001   0.58160001   0.32440001
    0.86817122]
 [ 31.74681631   0.6124       0.67340004   0.64180005   0.62669998
    0.8608551 ]][0m
[37m[1m[2023-07-11 18:18:57,585][233954] Max Reward on eval: 376.92064952570945[0m
[37m[1m[2023-07-11 18:18:57,585][233954] Min Reward on eval: -101.01187379062176[0m
[37m[1m[2023-07-11 18:18:57,585][233954] Mean Reward across all agents: 85.30861217146622[0m
[37m[1m[2023-07-11 18:18:57,585][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:18:57,596][233954] mean_value=1.4336254276549274, max_value=698.8082449703477[0m
[37m[1m[2023-07-11 18:18:57,599][233954] New mean coefficients: [[-0.42613527  2.3090365  -0.34579945 -0.1261853  -2.0835876  -3.1619794 ]][0m
[37m[1m[2023-07-11 18:18:57,600][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:19:06,633][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 18:19:06,633][233954] FPS: 425169.75[0m
[36m[2023-07-11 18:19:06,635][233954] itr=1328, itrs=2000, Progress: 66.40%[0m
[36m[2023-07-11 18:19:18,402][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 18:19:18,403][233954] FPS: 329447.46[0m
[36m[2023-07-11 18:19:22,774][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:19:22,774][233954] Reward + Measures: [[24.01459089  0.98080361  0.36807433  0.97858936  0.01516667  0.36300057]][0m
[37m[1m[2023-07-11 18:19:22,775][233954] Max Reward on eval: 24.01459089362738[0m
[37m[1m[2023-07-11 18:19:22,775][233954] Min Reward on eval: 24.01459089362738[0m
[37m[1m[2023-07-11 18:19:22,775][233954] Mean Reward across all agents: 24.01459089362738[0m
[37m[1m[2023-07-11 18:19:22,775][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:19:27,772][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:19:27,772][233954] Reward + Measures: [[ -42.73578501    0.30340001    0.55940002    0.58289999    0.62799996
     1.96912885]
 [  27.52380245    0.33250001    0.43810001    0.5492        0.63000005
     2.1636641 ]
 [  39.53578455    0.69550002    0.2951        0.70380002    0.40840003
     0.917597  ]
 ...
 [-183.99458021    0.4269        0.19860001    0.57600003    0.22239999
     1.11823928]
 [  49.16872094    0.90570003    0.32859999    0.92060006    0.1344
     0.62840408]
 [  -2.7446493     0.48120004    0.52880001    0.80649996    0.67430001
     1.15018976]][0m
[37m[1m[2023-07-11 18:19:27,773][233954] Max Reward on eval: 378.8610143572325[0m
[37m[1m[2023-07-11 18:19:27,778][233954] Min Reward on eval: -199.85348899858073[0m
[37m[1m[2023-07-11 18:19:27,778][233954] Mean Reward across all agents: -12.21204982879391[0m
[37m[1m[2023-07-11 18:19:27,779][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:19:27,784][233954] mean_value=-166.9859887346065, max_value=487.1836143143475[0m
[37m[1m[2023-07-11 18:19:27,787][233954] New mean coefficients: [[-0.33567297  2.42691     0.05231971 -0.80992514 -0.53801143 -2.7855682 ]][0m
[37m[1m[2023-07-11 18:19:27,788][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:19:36,762][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 18:19:36,762][233954] FPS: 427976.07[0m
[36m[2023-07-11 18:19:36,765][233954] itr=1329, itrs=2000, Progress: 66.45%[0m
[36m[2023-07-11 18:19:48,326][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 18:19:48,326][233954] FPS: 335365.27[0m
[36m[2023-07-11 18:19:52,570][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:19:52,571][233954] Reward + Measures: [[17.78748735  0.98322868  0.47334567  0.97884506  0.01005267  0.3059766 ]][0m
[37m[1m[2023-07-11 18:19:52,571][233954] Max Reward on eval: 17.78748735442217[0m
[37m[1m[2023-07-11 18:19:52,571][233954] Min Reward on eval: 17.78748735442217[0m
[37m[1m[2023-07-11 18:19:52,572][233954] Mean Reward across all agents: 17.78748735442217[0m
[37m[1m[2023-07-11 18:19:52,572][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:19:57,508][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:19:57,514][233954] Reward + Measures: [[-84.02506737   0.60299999   0.30560002   0.537        0.28220001
    1.32601726]
 [ 48.55611707   0.7026       0.41260001   0.61920005   0.12190001
    2.25313306]
 [ 75.21198962   0.51139998   0.2967       0.50209999   0.32240003
    2.39249611]
 ...
 [  5.47619849   0.39089999   0.2059       0.36090001   0.26270002
    2.4812429 ]
 [  0.5395734    0.7723       0.3127       0.73920006   0.39030001
    1.71628916]
 [  6.79215521   0.88849992   0.44679999   0.88409996   0.53750002
    1.2453444 ]][0m
[37m[1m[2023-07-11 18:19:57,514][233954] Max Reward on eval: 182.41534421835095[0m
[37m[1m[2023-07-11 18:19:57,515][233954] Min Reward on eval: -126.63630294948817[0m
[37m[1m[2023-07-11 18:19:57,515][233954] Mean Reward across all agents: 20.46084628655782[0m
[37m[1m[2023-07-11 18:19:57,515][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:19:57,521][233954] mean_value=-53.262728837444996, max_value=484.4105651665479[0m
[37m[1m[2023-07-11 18:19:57,524][233954] New mean coefficients: [[ 0.00018075  2.048969    0.43233368 -1.1196669   0.15051067 -2.1848438 ]][0m
[37m[1m[2023-07-11 18:19:57,525][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:20:06,515][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 18:20:06,515][233954] FPS: 427212.67[0m
[36m[2023-07-11 18:20:06,518][233954] itr=1330, itrs=2000, Progress: 66.50%[0m
[37m[1m[2023-07-11 18:24:00,897][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001310[0m
[36m[2023-07-11 18:24:13,721][233954] train() took 11.99 seconds to complete[0m
[36m[2023-07-11 18:24:13,721][233954] FPS: 320144.65[0m
[36m[2023-07-11 18:24:17,953][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:24:17,954][233954] Reward + Measures: [[12.78037218  0.98508734  0.58421266  0.9809007   0.00887067  0.23833354]][0m
[37m[1m[2023-07-11 18:24:17,954][233954] Max Reward on eval: 12.780372179379125[0m
[37m[1m[2023-07-11 18:24:17,954][233954] Min Reward on eval: 12.780372179379125[0m
[37m[1m[2023-07-11 18:24:17,955][233954] Mean Reward across all agents: 12.780372179379125[0m
[37m[1m[2023-07-11 18:24:17,955][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:24:22,919][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:24:22,920][233954] Reward + Measures: [[  3.81039762   0.82240009   0.37020001   0.75630003   0.38290003
    1.11597919]
 [153.07104045   0.54350007   0.46739998   0.73890001   0.0565
    1.29896736]
 [117.49740357   0.4086       0.40570003   0.4197       0.36590001
    1.46965504]
 ...
 [  4.80315518   0.35490003   0.37439999   0.3256       0.3407
    1.85738981]
 [ 19.69970627   0.78230006   0.50730008   0.74240005   0.10160001
    0.9182319 ]
 [ 38.11944961   0.7511       0.51030004   0.80019999   0.37040001
    1.145913  ]][0m
[37m[1m[2023-07-11 18:24:22,920][233954] Max Reward on eval: 259.5601148535032[0m
[37m[1m[2023-07-11 18:24:22,920][233954] Min Reward on eval: -120.21236858377233[0m
[37m[1m[2023-07-11 18:24:22,920][233954] Mean Reward across all agents: 41.28248488907067[0m
[37m[1m[2023-07-11 18:24:22,921][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:24:22,926][233954] mean_value=-99.96721269126476, max_value=501.60923139013346[0m
[37m[1m[2023-07-11 18:24:22,928][233954] New mean coefficients: [[ 0.5727396   2.1177068  -0.2198123  -0.60287446  2.493822   -0.75946677]][0m
[37m[1m[2023-07-11 18:24:22,929][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:24:31,877][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 18:24:31,877][233954] FPS: 429237.09[0m
[36m[2023-07-11 18:24:31,879][233954] itr=1331, itrs=2000, Progress: 66.55%[0m
[36m[2023-07-11 18:24:43,427][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 18:24:43,428][233954] FPS: 335634.13[0m
[36m[2023-07-11 18:24:47,716][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:24:47,717][233954] Reward + Measures: [[-4.07606857  0.96577698  0.11083767  0.94165361  0.26705065  0.52384406]][0m
[37m[1m[2023-07-11 18:24:47,717][233954] Max Reward on eval: -4.076068571345304[0m
[37m[1m[2023-07-11 18:24:47,717][233954] Min Reward on eval: -4.076068571345304[0m
[37m[1m[2023-07-11 18:24:47,717][233954] Mean Reward across all agents: -4.076068571345304[0m
[37m[1m[2023-07-11 18:24:47,718][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:24:52,689][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:24:52,689][233954] Reward + Measures: [[-45.42262921   0.41459998   0.5485       0.37619999   0.58470005
    0.91700011]
 [ 50.98655141   0.79319996   0.35560003   0.76270002   0.25980002
    0.91618103]
 [114.1274924    0.78510004   0.54730004   0.6771       0.69550002
    1.47491086]
 ...
 [107.84777548   0.85860008   0.5636       0.73200005   0.73809999
    1.32569253]
 [ 77.44973611   0.85289997   0.68230003   0.90830004   0.0192
    0.48824319]
 [-45.03999021   0.74290001   0.45109996   0.72000003   0.40109998
    1.11830997]][0m
[37m[1m[2023-07-11 18:24:52,690][233954] Max Reward on eval: 247.5360517426394[0m
[37m[1m[2023-07-11 18:24:52,690][233954] Min Reward on eval: -189.4639477752615[0m
[37m[1m[2023-07-11 18:24:52,690][233954] Mean Reward across all agents: 18.514905656200252[0m
[37m[1m[2023-07-11 18:24:52,690][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:24:52,696][233954] mean_value=-82.42696754558793, max_value=632.9907579291845[0m
[37m[1m[2023-07-11 18:24:52,699][233954] New mean coefficients: [[ 0.36977455  1.6321603  -1.0561012  -0.72300273  2.5986378   0.26567912]][0m
[37m[1m[2023-07-11 18:24:52,700][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:25:01,682][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 18:25:01,682][233954] FPS: 427567.19[0m
[36m[2023-07-11 18:25:01,685][233954] itr=1332, itrs=2000, Progress: 66.60%[0m
[36m[2023-07-11 18:25:13,278][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 18:25:13,278][233954] FPS: 334395.08[0m
[36m[2023-07-11 18:25:17,514][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:25:17,515][233954] Reward + Measures: [[-27.04406695   0.96401268   0.051211     0.9296546    0.47969466
    0.51810926]][0m
[37m[1m[2023-07-11 18:25:17,515][233954] Max Reward on eval: -27.04406695160559[0m
[37m[1m[2023-07-11 18:25:17,515][233954] Min Reward on eval: -27.04406695160559[0m
[37m[1m[2023-07-11 18:25:17,516][233954] Mean Reward across all agents: -27.04406695160559[0m
[37m[1m[2023-07-11 18:25:17,516][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:25:22,688][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:25:22,689][233954] Reward + Measures: [[  25.9472904     0.89169997    0.07610001    0.84850007    0.49939999
     1.35415745]
 [  -9.88608684    0.75699997    0.1998        0.61809999    0.58960003
     0.68664384]
 [ -41.26061754    0.49710003    0.19559999    0.45240003    0.27669999
     1.1453799 ]
 ...
 [-116.35541094    0.6846        0.28460002    0.59989995    0.1107
     1.26112485]
 [ -27.98384042    0.73719996    0.39420003    0.76910001    0.16140001
     1.53152084]
 [  14.29165578    0.81590003    0.23779999    0.75139999    0.41430002
     0.94137043]][0m
[37m[1m[2023-07-11 18:25:22,689][233954] Max Reward on eval: 222.00416656741874[0m
[37m[1m[2023-07-11 18:25:22,689][233954] Min Reward on eval: -319.78820581054316[0m
[37m[1m[2023-07-11 18:25:22,689][233954] Mean Reward across all agents: -15.290451725489305[0m
[37m[1m[2023-07-11 18:25:22,690][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:25:22,695][233954] mean_value=-182.22289644998452, max_value=554.3193507877179[0m
[37m[1m[2023-07-11 18:25:22,698][233954] New mean coefficients: [[-0.01762059  1.7506095  -0.2369855  -0.12359285  3.2349834  -0.06231198]][0m
[37m[1m[2023-07-11 18:25:22,699][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:25:31,607][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 18:25:31,607][233954] FPS: 431133.72[0m
[36m[2023-07-11 18:25:31,609][233954] itr=1333, itrs=2000, Progress: 66.65%[0m
[36m[2023-07-11 18:25:43,164][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 18:25:43,164][233954] FPS: 335409.94[0m
[36m[2023-07-11 18:25:47,406][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:25:47,412][233954] Reward + Measures: [[-33.51861148   0.97241473   0.036889     0.95139104   0.60474664
    0.48658934]][0m
[37m[1m[2023-07-11 18:25:47,412][233954] Max Reward on eval: -33.51861147560268[0m
[37m[1m[2023-07-11 18:25:47,412][233954] Min Reward on eval: -33.51861147560268[0m
[37m[1m[2023-07-11 18:25:47,413][233954] Mean Reward across all agents: -33.51861147560268[0m
[37m[1m[2023-07-11 18:25:47,413][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:25:52,341][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:25:52,341][233954] Reward + Measures: [[ 29.96694301   0.63870001   0.45499998   0.64460003   0.11000001
    1.28844249]
 [ 22.69711576   0.62940001   0.52640003   0.58120006   0.4948
    1.83105779]
 [ 30.34483141   0.48629999   0.51610005   0.6615001    0.59950006
    1.50720906]
 ...
 [ 45.3631916    0.98719996   0.67320007   0.98190004   0.17080002
    0.51417685]
 [ 74.41887195   0.53630006   0.13340001   0.61050004   0.2811
    1.27527356]
 [-36.71827335   0.84019995   0.37809998   0.86609995   0.25100002
    0.80516016]][0m
[37m[1m[2023-07-11 18:25:52,341][233954] Max Reward on eval: 186.32897262657062[0m
[37m[1m[2023-07-11 18:25:52,342][233954] Min Reward on eval: -145.06509969895706[0m
[37m[1m[2023-07-11 18:25:52,342][233954] Mean Reward across all agents: 15.894840295239167[0m
[37m[1m[2023-07-11 18:25:52,342][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:25:52,349][233954] mean_value=-59.39159927898817, max_value=556.7876071220496[0m
[37m[1m[2023-07-11 18:25:52,352][233954] New mean coefficients: [[ 0.17361957  1.334671   -0.5948069  -1.0395515   3.3830204   0.72732437]][0m
[37m[1m[2023-07-11 18:25:52,353][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:26:01,316][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 18:26:01,317][233954] FPS: 428462.48[0m
[36m[2023-07-11 18:26:01,319][233954] itr=1334, itrs=2000, Progress: 66.70%[0m
[36m[2023-07-11 18:26:13,057][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 18:26:13,057][233954] FPS: 330172.82[0m
[36m[2023-07-11 18:26:17,326][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:26:17,327][233954] Reward + Measures: [[-39.7826645    0.98107266   0.031753     0.9734537    0.7624554
    0.45721656]][0m
[37m[1m[2023-07-11 18:26:17,327][233954] Max Reward on eval: -39.78266449836416[0m
[37m[1m[2023-07-11 18:26:17,327][233954] Min Reward on eval: -39.78266449836416[0m
[37m[1m[2023-07-11 18:26:17,328][233954] Mean Reward across all agents: -39.78266449836416[0m
[37m[1m[2023-07-11 18:26:17,328][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:26:22,376][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:26:22,376][233954] Reward + Measures: [[ 92.11711941   0.8193       0.35179999   0.89890003   0.19569999
    0.95177871]
 [ 89.49445771   0.81080002   0.59499997   0.78669995   0.7123
    1.84536421]
 [ 50.7115493    0.83680004   0.26250002   0.7913       0.62760001
    0.96610701]
 ...
 [ 14.61072457   0.90360004   0.71060002   0.87279999   0.82870001
    1.08006728]
 [-65.04851834   0.69840002   0.2516       0.6676001    0.28260002
    2.18879056]
 [-17.60772028   0.2911       0.23190001   0.48670003   0.0835
    2.24606633]][0m
[37m[1m[2023-07-11 18:26:22,376][233954] Max Reward on eval: 189.37285087574273[0m
[37m[1m[2023-07-11 18:26:22,377][233954] Min Reward on eval: -208.79063030928373[0m
[37m[1m[2023-07-11 18:26:22,377][233954] Mean Reward across all agents: 19.129458647890925[0m
[37m[1m[2023-07-11 18:26:22,377][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:26:22,386][233954] mean_value=-16.51745757384695, max_value=538.7284437064827[0m
[37m[1m[2023-07-11 18:26:22,388][233954] New mean coefficients: [[ 0.41653675  1.2887955  -0.21886492 -2.008535    4.6992373   0.96305114]][0m
[37m[1m[2023-07-11 18:26:22,389][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:26:31,418][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 18:26:31,418][233954] FPS: 425399.56[0m
[36m[2023-07-11 18:26:31,420][233954] itr=1335, itrs=2000, Progress: 66.75%[0m
[36m[2023-07-11 18:26:43,234][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 18:26:43,234][233954] FPS: 328064.13[0m
[36m[2023-07-11 18:26:47,541][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:26:47,542][233954] Reward + Measures: [[-38.12672708   0.98580825   0.028975     0.98046404   0.85350364
    0.43882266]][0m
[37m[1m[2023-07-11 18:26:47,542][233954] Max Reward on eval: -38.12672707846528[0m
[37m[1m[2023-07-11 18:26:47,542][233954] Min Reward on eval: -38.12672707846528[0m
[37m[1m[2023-07-11 18:26:47,542][233954] Mean Reward across all agents: -38.12672707846528[0m
[37m[1m[2023-07-11 18:26:47,543][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:26:52,485][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:26:52,485][233954] Reward + Measures: [[-28.10487568   0.94250005   0.0838       0.85720009   0.3953
    0.96192235]
 [ 72.54974318   0.85100001   0.6767       0.6954       0.4341
    1.19152308]
 [ 16.22111392   0.96849996   0.5363       0.9386       0.0678
    0.5688467 ]
 ...
 [-23.11564252   0.41869998   0.2868       0.4447       0.31720001
    1.66380715]
 [164.54116464   0.57539999   0.42749998   0.66430002   0.46869999
    1.25898099]
 [-19.61753586   0.36160001   0.1833       0.3213       0.24159999
    2.59908009]][0m
[37m[1m[2023-07-11 18:26:52,486][233954] Max Reward on eval: 223.35663603488356[0m
[37m[1m[2023-07-11 18:26:52,486][233954] Min Reward on eval: -142.90219086669384[0m
[37m[1m[2023-07-11 18:26:52,486][233954] Mean Reward across all agents: 23.5658539039223[0m
[37m[1m[2023-07-11 18:26:52,486][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:26:52,495][233954] mean_value=-13.454071299978883, max_value=569.5183811269701[0m
[37m[1m[2023-07-11 18:26:52,498][233954] New mean coefficients: [[ 1.0194994   1.6554202  -0.08279936 -1.8630232   5.7741675   0.18534243]][0m
[37m[1m[2023-07-11 18:26:52,499][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:27:01,489][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 18:27:01,489][233954] FPS: 427211.03[0m
[36m[2023-07-11 18:27:01,492][233954] itr=1336, itrs=2000, Progress: 66.80%[0m
[36m[2023-07-11 18:27:13,111][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 18:27:13,111][233954] FPS: 333623.88[0m
[36m[2023-07-11 18:27:17,356][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:27:17,357][233954] Reward + Measures: [[-37.14715803   0.98674041   0.02968967   0.98271233   0.88872498
    0.42200577]][0m
[37m[1m[2023-07-11 18:27:17,357][233954] Max Reward on eval: -37.14715802657721[0m
[37m[1m[2023-07-11 18:27:17,357][233954] Min Reward on eval: -37.14715802657721[0m
[37m[1m[2023-07-11 18:27:17,357][233954] Mean Reward across all agents: -37.14715802657721[0m
[37m[1m[2023-07-11 18:27:17,358][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:27:22,353][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:27:22,354][233954] Reward + Measures: [[ -7.59373232   0.63819999   0.63500005   0.65160006   0.64389998
    1.3657167 ]
 [-21.57850252   0.87400007   0.16019998   0.84179991   0.47049999
    0.93025303]
 [ -3.26092101   0.32290003   0.43600002   0.3994       0.45889997
    2.00832891]
 ...
 [ 48.2683369    0.89120001   0.50940001   0.85550004   0.48010001
    0.64754361]
 [ 49.13140634   0.4113       0.4154       0.47930002   0.46440002
    1.18939531]
 [ 56.30532268   0.71769994   0.41239998   0.54430002   0.74920005
    1.34422398]][0m
[37m[1m[2023-07-11 18:27:22,354][233954] Max Reward on eval: 239.54770372845232[0m
[37m[1m[2023-07-11 18:27:22,354][233954] Min Reward on eval: -140.53488920005037[0m
[37m[1m[2023-07-11 18:27:22,355][233954] Mean Reward across all agents: 27.888509721712605[0m
[37m[1m[2023-07-11 18:27:22,355][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:27:22,361][233954] mean_value=-81.18479462620778, max_value=583.0170652446337[0m
[37m[1m[2023-07-11 18:27:22,364][233954] New mean coefficients: [[ 1.2017434   2.6422992  -0.35538638  0.68257225  5.4191885  -0.75931835]][0m
[37m[1m[2023-07-11 18:27:22,365][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:27:31,333][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 18:27:31,333][233954] FPS: 428254.40[0m
[36m[2023-07-11 18:27:31,335][233954] itr=1337, itrs=2000, Progress: 66.85%[0m
[36m[2023-07-11 18:27:43,247][233954] train() took 11.81 seconds to complete[0m
[36m[2023-07-11 18:27:43,253][233954] FPS: 325275.98[0m
[36m[2023-07-11 18:27:47,540][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:27:47,541][233954] Reward + Measures: [[-32.60423826   0.98706532   0.02879267   0.98339397   0.8901574
    0.41354319]][0m
[37m[1m[2023-07-11 18:27:47,541][233954] Max Reward on eval: -32.60423826188043[0m
[37m[1m[2023-07-11 18:27:47,541][233954] Min Reward on eval: -32.60423826188043[0m
[37m[1m[2023-07-11 18:27:47,541][233954] Mean Reward across all agents: -32.60423826188043[0m
[37m[1m[2023-07-11 18:27:47,542][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:27:52,563][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:27:52,564][233954] Reward + Measures: [[ 68.9939145    0.64359999   0.1789       0.67330003   0.4059
    0.80221051]
 [-11.94726808   0.87910002   0.88450003   0.81159991   0.85790008
    1.4743731 ]
 [ 67.4437012    0.6027       0.31160003   0.54750007   0.40450001
    1.54905546]
 ...
 [-26.06843164   0.49960002   0.1675       0.53710002   0.25139999
    1.53467274]
 [-57.20644415   0.4673       0.3391       0.55310005   0.34960005
    1.79492128]
 [ 29.54593787   0.93129998   0.46380001   0.8915       0.1662
    1.03763378]][0m
[37m[1m[2023-07-11 18:27:52,564][233954] Max Reward on eval: 232.54391479361803[0m
[37m[1m[2023-07-11 18:27:52,564][233954] Min Reward on eval: -232.31380558330565[0m
[37m[1m[2023-07-11 18:27:52,564][233954] Mean Reward across all agents: 22.552569859363597[0m
[37m[1m[2023-07-11 18:27:52,565][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:27:52,571][233954] mean_value=-78.56660422928387, max_value=513.6338089947286[0m
[37m[1m[2023-07-11 18:27:52,573][233954] New mean coefficients: [[ 0.7925701   2.927702   -0.15146708  1.5713719   5.032678   -0.16927052]][0m
[37m[1m[2023-07-11 18:27:52,574][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:28:01,641][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 18:28:01,642][233954] FPS: 423590.25[0m
[36m[2023-07-11 18:28:01,644][233954] itr=1338, itrs=2000, Progress: 66.90%[0m
[36m[2023-07-11 18:28:13,423][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 18:28:13,423][233954] FPS: 329096.34[0m
[36m[2023-07-11 18:28:17,760][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:28:17,760][233954] Reward + Measures: [[-30.14882577   0.98743564   0.02471867   0.98473966   0.89840901
    0.40496564]][0m
[37m[1m[2023-07-11 18:28:17,761][233954] Max Reward on eval: -30.148825766289907[0m
[37m[1m[2023-07-11 18:28:17,761][233954] Min Reward on eval: -30.148825766289907[0m
[37m[1m[2023-07-11 18:28:17,761][233954] Mean Reward across all agents: -30.148825766289907[0m
[37m[1m[2023-07-11 18:28:17,761][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:28:23,019][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:28:23,019][233954] Reward + Measures: [[-102.24994706    0.26210001    0.29650003    0.2225        0.33250001
     2.30011296]
 [  -8.67031011    0.48740003    0.22760001    0.49990001    0.1468
     2.19669342]
 [ 100.76363189    0.37719998    0.40669999    0.32269999    0.35299999
     2.10235286]
 ...
 [  59.20165575    0.30320001    0.38240001    0.28000003    0.3653
     2.57185411]
 [ -73.28344202    0.34819999    0.40579996    0.36429998    0.46809998
     1.89933169]
 [  25.58910404    0.5345        0.55610001    0.56750005    0.5837
     1.57992256]][0m
[37m[1m[2023-07-11 18:28:23,019][233954] Max Reward on eval: 279.0491375943646[0m
[37m[1m[2023-07-11 18:28:23,020][233954] Min Reward on eval: -174.0757832914591[0m
[37m[1m[2023-07-11 18:28:23,020][233954] Mean Reward across all agents: 17.37777634315354[0m
[37m[1m[2023-07-11 18:28:23,020][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:28:23,025][233954] mean_value=-203.20494737359823, max_value=494.80208452353907[0m
[37m[1m[2023-07-11 18:28:23,027][233954] New mean coefficients: [[ 0.5920805  2.55588   -1.1555417  1.755178   2.5354066  0.3120939]][0m
[37m[1m[2023-07-11 18:28:23,028][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:28:32,096][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 18:28:32,096][233954] FPS: 423572.38[0m
[36m[2023-07-11 18:28:32,099][233954] itr=1339, itrs=2000, Progress: 66.95%[0m
[36m[2023-07-11 18:28:43,775][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 18:28:43,775][233954] FPS: 331958.99[0m
[36m[2023-07-11 18:28:48,093][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:28:48,093][233954] Reward + Measures: [[-32.13405318   0.98883432   0.021882     0.98647869   0.91820538
    0.4022314 ]][0m
[37m[1m[2023-07-11 18:28:48,093][233954] Max Reward on eval: -32.13405318471168[0m
[37m[1m[2023-07-11 18:28:48,094][233954] Min Reward on eval: -32.13405318471168[0m
[37m[1m[2023-07-11 18:28:48,094][233954] Mean Reward across all agents: -32.13405318471168[0m
[37m[1m[2023-07-11 18:28:48,094][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:28:53,089][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:28:53,090][233954] Reward + Measures: [[  -0.7916256     0.73960006    0.79090005    0.77160001    0.71449995
     1.71300507]
 [ -73.45650332    0.30300003    0.35859999    0.28659999    0.28120002
     2.17090154]
 [  77.07454053    0.92500001    0.59039998    0.90550005    0.0314
     1.63782239]
 ...
 [ -25.15106298    0.22830001    0.27659997    0.23800002    0.23699999
     2.37995958]
 [-126.32735097    0.74529999    0.13930002    0.65900004    0.32179999
     1.70728242]
 [  56.68743352    0.26500002    0.19790001    0.2366        0.21180001
     2.31437993]][0m
[37m[1m[2023-07-11 18:28:53,090][233954] Max Reward on eval: 283.0497389363125[0m
[37m[1m[2023-07-11 18:28:53,090][233954] Min Reward on eval: -365.6730317635462[0m
[37m[1m[2023-07-11 18:28:53,090][233954] Mean Reward across all agents: -13.847029307447556[0m
[37m[1m[2023-07-11 18:28:53,091][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:28:53,095][233954] mean_value=-204.6700561614274, max_value=476.5401369383916[0m
[37m[1m[2023-07-11 18:28:53,098][233954] New mean coefficients: [[ 0.24550036  2.1454458  -1.4216877   0.46023917  1.8690519   1.2008773 ]][0m
[37m[1m[2023-07-11 18:28:53,099][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:29:02,111][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 18:29:02,111][233954] FPS: 426176.39[0m
[36m[2023-07-11 18:29:02,113][233954] itr=1340, itrs=2000, Progress: 67.00%[0m
[37m[1m[2023-07-11 18:33:00,029][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001320[0m
[36m[2023-07-11 18:33:13,262][233954] train() took 11.83 seconds to complete[0m
[36m[2023-07-11 18:33:13,262][233954] FPS: 324661.20[0m
[36m[2023-07-11 18:33:17,500][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:33:17,501][233954] Reward + Measures: [[-27.753989     0.98969203   0.00218033   0.99120599   0.92382169
    0.33147347]][0m
[37m[1m[2023-07-11 18:33:17,501][233954] Max Reward on eval: -27.75398900070991[0m
[37m[1m[2023-07-11 18:33:17,501][233954] Min Reward on eval: -27.75398900070991[0m
[37m[1m[2023-07-11 18:33:17,502][233954] Mean Reward across all agents: -27.75398900070991[0m
[37m[1m[2023-07-11 18:33:17,502][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:33:22,396][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:33:22,397][233954] Reward + Measures: [[  17.24507623    0.75439996    0.3757        0.85159999    0.1041
     0.84617966]
 [-158.78526687    0.90950006    0.1763        0.82389992    0.68370003
     0.37676284]
 [ -27.9127508     0.22649999    0.0921        0.32720003    0.19880001
     1.7474159 ]
 ...
 [ -40.62870659    0.73260003    0.23339999    0.82279998    0.11900001
     1.30980623]
 [ -21.13015545    0.52759999    0.59499997    0.48859999    0.40809998
     1.33533478]
 [-125.6639099     0.9601        0.87180007    0.91399997    0.0133
     1.16555643]][0m
[37m[1m[2023-07-11 18:33:22,397][233954] Max Reward on eval: 190.297623164393[0m
[37m[1m[2023-07-11 18:33:22,397][233954] Min Reward on eval: -283.0654716748744[0m
[37m[1m[2023-07-11 18:33:22,397][233954] Mean Reward across all agents: -19.462980547767643[0m
[37m[1m[2023-07-11 18:33:22,398][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:33:22,403][233954] mean_value=-214.39587248793904, max_value=577.8505554222315[0m
[37m[1m[2023-07-11 18:33:22,406][233954] New mean coefficients: [[ 0.7737273   1.7757224  -1.2186226   0.32477713  1.8172867   0.59768885]][0m
[37m[1m[2023-07-11 18:33:22,407][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:33:31,326][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 18:33:31,326][233954] FPS: 430614.34[0m
[36m[2023-07-11 18:33:31,328][233954] itr=1341, itrs=2000, Progress: 67.05%[0m
[36m[2023-07-11 18:33:43,047][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 18:33:43,047][233954] FPS: 330782.21[0m
[36m[2023-07-11 18:33:47,280][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:33:47,280][233954] Reward + Measures: [[-23.45228076   0.99181837   0.001125     0.99383932   0.9277007
    0.32214046]][0m
[37m[1m[2023-07-11 18:33:47,280][233954] Max Reward on eval: -23.452280756680633[0m
[37m[1m[2023-07-11 18:33:47,281][233954] Min Reward on eval: -23.452280756680633[0m
[37m[1m[2023-07-11 18:33:47,281][233954] Mean Reward across all agents: -23.452280756680633[0m
[37m[1m[2023-07-11 18:33:47,281][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:33:52,280][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:33:52,281][233954] Reward + Measures: [[  81.92832948    0.67019999    0.24529998    0.55650002    0.42929998
     0.80913132]
 [  55.00354454    0.24419999    0.26610002    0.34660003    0.36380002
     3.08033013]
 [-162.43302922    0.3098        0.33260003    0.3317        0.25980002
     1.60757065]
 ...
 [  29.76583434    0.62959999    0.32359999    0.65970004    0.20439999
     1.46178079]
 [   4.67873782    0.82800007    0.13690001    0.76050001    0.38940001
     0.48618171]
 [   6.59073597    0.74220002    0.24819998    0.73079997    0.51000005
     1.07669914]][0m
[37m[1m[2023-07-11 18:33:52,281][233954] Max Reward on eval: 341.45356468828396[0m
[37m[1m[2023-07-11 18:33:52,281][233954] Min Reward on eval: -336.58858251199126[0m
[37m[1m[2023-07-11 18:33:52,281][233954] Mean Reward across all agents: 10.462657705406901[0m
[37m[1m[2023-07-11 18:33:52,282][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:33:52,288][233954] mean_value=-226.0248722677892, max_value=640.4816739461758[0m
[37m[1m[2023-07-11 18:33:52,291][233954] New mean coefficients: [[ 1.1434641   1.3504916  -0.57106483 -1.2698191   2.1711583   0.16265327]][0m
[37m[1m[2023-07-11 18:33:52,291][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:34:01,214][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 18:34:01,215][233954] FPS: 430419.35[0m
[36m[2023-07-11 18:34:01,217][233954] itr=1342, itrs=2000, Progress: 67.10%[0m
[36m[2023-07-11 18:34:12,746][233954] train() took 11.42 seconds to complete[0m
[36m[2023-07-11 18:34:12,747][233954] FPS: 336136.58[0m
[36m[2023-07-11 18:34:16,949][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:34:16,954][233954] Reward + Measures: [[-19.55020132   0.99566394   0.000363     0.99169898   0.95047402
    0.24537794]][0m
[37m[1m[2023-07-11 18:34:16,954][233954] Max Reward on eval: -19.550201324740357[0m
[37m[1m[2023-07-11 18:34:16,955][233954] Min Reward on eval: -19.550201324740357[0m
[37m[1m[2023-07-11 18:34:16,955][233954] Mean Reward across all agents: -19.550201324740357[0m
[37m[1m[2023-07-11 18:34:16,955][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:34:22,148][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:34:22,154][233954] Reward + Measures: [[-24.07527791   0.40279999   0.1514       0.29980001   0.31720001
    1.75794625]
 [-17.41425136   0.90200007   0.005        0.79360002   0.7683
    1.75548971]
 [115.82925555   0.48119998   0.18550001   0.36469999   0.29809999
    1.45105553]
 ...
 [-78.43141649   0.4192       0.53710002   0.4034       0.42379999
    1.47784424]
 [ -0.40267937   0.77170002   0.094        0.74089998   0.50059998
    0.51674771]
 [133.67635155   0.62099999   0.08090001   0.63840002   0.51410002
    1.80900228]][0m
[37m[1m[2023-07-11 18:34:22,154][233954] Max Reward on eval: 197.63695574535524[0m
[37m[1m[2023-07-11 18:34:22,154][233954] Min Reward on eval: -234.09688621237873[0m
[37m[1m[2023-07-11 18:34:22,154][233954] Mean Reward across all agents: -1.2468618490184145[0m
[37m[1m[2023-07-11 18:34:22,155][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:34:22,162][233954] mean_value=-121.10982966855649, max_value=636.3257798572071[0m
[37m[1m[2023-07-11 18:34:22,165][233954] New mean coefficients: [[ 1.4278708   2.008202   -0.97313684 -1.0784769   2.6357012  -0.03134345]][0m
[37m[1m[2023-07-11 18:34:22,166][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:34:31,081][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 18:34:31,081][233954] FPS: 430794.72[0m
[36m[2023-07-11 18:34:31,084][233954] itr=1343, itrs=2000, Progress: 67.15%[0m
[36m[2023-07-11 18:34:43,040][233954] train() took 11.84 seconds to complete[0m
[36m[2023-07-11 18:34:43,041][233954] FPS: 324202.10[0m
[36m[2023-07-11 18:34:47,259][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:34:47,260][233954] Reward + Measures: [[3.25259534 0.97764307 0.87779671 0.97436327 0.64440793 0.39948389]][0m
[37m[1m[2023-07-11 18:34:47,260][233954] Max Reward on eval: 3.2525953420836595[0m
[37m[1m[2023-07-11 18:34:47,260][233954] Min Reward on eval: 3.2525953420836595[0m
[37m[1m[2023-07-11 18:34:47,260][233954] Mean Reward across all agents: 3.2525953420836595[0m
[37m[1m[2023-07-11 18:34:47,261][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:34:52,210][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:34:52,210][233954] Reward + Measures: [[ 0.97637037  0.8915      0.1305      0.81520003  0.2626      1.1793288 ]
 [57.39671752  0.6983      0.30470002  0.60519999  0.28819999  1.29589951]
 [-6.09107852  0.7737      0.0622      0.87099999  0.51809996  1.77862763]
 ...
 [-1.93826846  0.95179999  0.8707      0.96240008  0.0046      2.41073585]
 [70.97187823  0.8860001   0.23800002  0.85859996  0.13609999  0.9055354 ]
 [-7.22061312  0.90070003  0.94499999  0.78479999  0.88479996  0.76882738]][0m
[37m[1m[2023-07-11 18:34:52,210][233954] Max Reward on eval: 188.29074386572466[0m
[37m[1m[2023-07-11 18:34:52,211][233954] Min Reward on eval: -217.1155705517158[0m
[37m[1m[2023-07-11 18:34:52,211][233954] Mean Reward across all agents: 6.441251022135654[0m
[37m[1m[2023-07-11 18:34:52,211][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:34:52,218][233954] mean_value=-60.777736674124924, max_value=576.9010248158011[0m
[37m[1m[2023-07-11 18:34:52,220][233954] New mean coefficients: [[ 1.8248737   1.8769165  -1.0692666  -0.96967274  3.3036585   0.33421528]][0m
[37m[1m[2023-07-11 18:34:52,221][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:35:01,155][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 18:35:01,155][233954] FPS: 429915.69[0m
[36m[2023-07-11 18:35:01,158][233954] itr=1344, itrs=2000, Progress: 67.20%[0m
[36m[2023-07-11 18:35:12,820][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 18:35:12,821][233954] FPS: 332411.84[0m
[36m[2023-07-11 18:35:17,130][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:35:17,130][233954] Reward + Measures: [[-14.29275484   0.98210031   0.89699298   0.96820432   0.9161213
    0.48845205]][0m
[37m[1m[2023-07-11 18:35:17,130][233954] Max Reward on eval: -14.29275483875148[0m
[37m[1m[2023-07-11 18:35:17,131][233954] Min Reward on eval: -14.29275483875148[0m
[37m[1m[2023-07-11 18:35:17,131][233954] Mean Reward across all agents: -14.29275483875148[0m
[37m[1m[2023-07-11 18:35:17,131][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:35:22,170][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:35:22,176][233954] Reward + Measures: [[-106.21927595    0.76420003    0.24429999    0.65579998    0.3784
     1.78023946]
 [ -91.6387267     0.66289997    0.56170005    0.34189999    0.59240001
     1.59238553]
 [-114.62637043    0.64460003    0.37180001    0.54179996    0.4779
     2.24846697]
 ...
 [ 133.12010003    0.77770001    0.49779996    0.51699996    0.52240002
     0.91350597]
 [  27.12181442    0.85550004    0.0357        0.93699998    0.63379997
     1.96098638]
 [ -23.54545911    0.8283        0.85320008    0.7105        0.7802
     1.37554812]][0m
[37m[1m[2023-07-11 18:35:22,176][233954] Max Reward on eval: 387.4045906478539[0m
[37m[1m[2023-07-11 18:35:22,176][233954] Min Reward on eval: -380.0382232818753[0m
[37m[1m[2023-07-11 18:35:22,177][233954] Mean Reward across all agents: -1.9965111687715118[0m
[37m[1m[2023-07-11 18:35:22,177][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:35:22,186][233954] mean_value=-24.647330137247845, max_value=667.537386887893[0m
[37m[1m[2023-07-11 18:35:22,189][233954] New mean coefficients: [[ 1.5319903  1.2420547 -1.00583   -2.6406875  3.1375036 -0.2969185]][0m
[37m[1m[2023-07-11 18:35:22,190][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:35:31,206][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 18:35:31,206][233954] FPS: 425981.80[0m
[36m[2023-07-11 18:35:31,208][233954] itr=1345, itrs=2000, Progress: 67.25%[0m
[36m[2023-07-11 18:35:43,355][233954] train() took 12.04 seconds to complete[0m
[36m[2023-07-11 18:35:43,355][233954] FPS: 319056.02[0m
[36m[2023-07-11 18:35:47,565][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:35:47,565][233954] Reward + Measures: [[5.76745776 0.99480265 0.99147964 0.99150729 0.984303   0.77033299]][0m
[37m[1m[2023-07-11 18:35:47,565][233954] Max Reward on eval: 5.767457762161547[0m
[37m[1m[2023-07-11 18:35:47,566][233954] Min Reward on eval: 5.767457762161547[0m
[37m[1m[2023-07-11 18:35:47,566][233954] Mean Reward across all agents: 5.767457762161547[0m
[37m[1m[2023-07-11 18:35:47,566][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:35:52,560][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:35:52,560][233954] Reward + Measures: [[ -8.34403727   0.89920008   0.76550001   0.84750003   0.78630006
    0.83208942]
 [-65.3006508    0.56510001   0.53200001   0.68660003   0.34969997
    0.61624449]
 [-31.07326274   0.8696       0.16670001   0.86730003   0.2493
    0.73877221]
 ...
 [-51.53721602   0.73330003   0.23650001   0.71990007   0.30329999
    0.68380046]
 [ 15.8616612    0.70079994   0.91520005   0.83400005   0.87149996
    1.65977252]
 [-11.60026348   0.99150008   0.99220002   0.9903       0.9842
    1.92213523]][0m
[37m[1m[2023-07-11 18:35:52,561][233954] Max Reward on eval: 251.50384997352958[0m
[37m[1m[2023-07-11 18:35:52,561][233954] Min Reward on eval: -411.1534995914437[0m
[37m[1m[2023-07-11 18:35:52,561][233954] Mean Reward across all agents: -40.70729147676908[0m
[37m[1m[2023-07-11 18:35:52,561][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:35:52,566][233954] mean_value=-150.91907478777395, max_value=523.5953307747841[0m
[37m[1m[2023-07-11 18:35:52,569][233954] New mean coefficients: [[ 1.3735492   1.3226503  -0.83060277 -2.423523    3.8697567  -0.02476874]][0m
[37m[1m[2023-07-11 18:35:52,570][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:36:01,487][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 18:36:01,487][233954] FPS: 430721.36[0m
[36m[2023-07-11 18:36:01,490][233954] itr=1346, itrs=2000, Progress: 67.30%[0m
[36m[2023-07-11 18:36:13,052][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 18:36:13,052][233954] FPS: 335264.46[0m
[36m[2023-07-11 18:36:17,328][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:36:17,329][233954] Reward + Measures: [[-71.13147731   0.99042493   0.56032431   0.98506868   0.83277535
    1.23220325]][0m
[37m[1m[2023-07-11 18:36:17,329][233954] Max Reward on eval: -71.1314773101833[0m
[37m[1m[2023-07-11 18:36:17,329][233954] Min Reward on eval: -71.1314773101833[0m
[37m[1m[2023-07-11 18:36:17,330][233954] Mean Reward across all agents: -71.1314773101833[0m
[37m[1m[2023-07-11 18:36:17,330][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:36:22,361][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:36:22,367][233954] Reward + Measures: [[ 21.19747457   0.89910001   0.53839999   0.87019998   0.4262
    2.06522846]
 [-10.67425282   0.52060002   0.4914       0.32329997   0.52159995
    2.05938554]
 [ 55.24111168   0.54010004   0.50820005   0.52319998   0.48270008
    2.26420045]
 ...
 [-30.40150041   0.97690004   0.96040004   0.92840004   0.92169994
    1.64054203]
 [-21.26022651   0.67470008   0.83210003   0.59869999   0.81900007
    1.49432242]
 [-20.19192141   0.8915       0.93129998   0.67880005   0.91650003
    1.59296358]][0m
[37m[1m[2023-07-11 18:36:22,367][233954] Max Reward on eval: 541.2544479322445[0m
[37m[1m[2023-07-11 18:36:22,367][233954] Min Reward on eval: -217.17465035608038[0m
[37m[1m[2023-07-11 18:36:22,368][233954] Mean Reward across all agents: 32.862287797925156[0m
[37m[1m[2023-07-11 18:36:22,368][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:36:22,376][233954] mean_value=-44.91417029133376, max_value=643.7326089791275[0m
[37m[1m[2023-07-11 18:36:22,378][233954] New mean coefficients: [[ 2.1559083   0.54163027 -1.3077135  -1.8679943   2.7903888   0.25909805]][0m
[37m[1m[2023-07-11 18:36:22,379][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:36:31,321][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 18:36:31,321][233954] FPS: 429519.06[0m
[36m[2023-07-11 18:36:31,323][233954] itr=1347, itrs=2000, Progress: 67.35%[0m
[36m[2023-07-11 18:36:43,053][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 18:36:43,054][233954] FPS: 330374.69[0m
[36m[2023-07-11 18:36:47,307][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:36:47,307][233954] Reward + Measures: [[91.05325017  0.93644661  0.40489233  0.95730466  0.499253    1.69049883]][0m
[37m[1m[2023-07-11 18:36:47,308][233954] Max Reward on eval: 91.05325017282894[0m
[37m[1m[2023-07-11 18:36:47,308][233954] Min Reward on eval: 91.05325017282894[0m
[37m[1m[2023-07-11 18:36:47,308][233954] Mean Reward across all agents: 91.05325017282894[0m
[37m[1m[2023-07-11 18:36:47,308][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:36:52,341][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:36:52,342][233954] Reward + Measures: [[-151.44871616    0.3752        0.48699999    0.53190005    0.69510001
     2.36348844]
 [  14.28038455    0.95809996    0.018         0.96279997    0.86429995
     3.59158826]
 [  50.75423115    0.51229995    0.2947        0.52120006    0.26429999
     2.34888077]
 ...
 [  50.49757445    0.80950004    0.80900002    0.78730005    0.68379998
     3.24996424]
 [   5.18937652    0.98730004    0.38620001    0.94670004    0.47329998
     2.19878745]
 [  34.73724986    0.0473        0.92270005    0.90600008    0.95360005
     2.62833667]][0m
[37m[1m[2023-07-11 18:36:52,342][233954] Max Reward on eval: 737.8928489604965[0m
[37m[1m[2023-07-11 18:36:52,342][233954] Min Reward on eval: -522.4602337001357[0m
[37m[1m[2023-07-11 18:36:52,342][233954] Mean Reward across all agents: 10.870187233460364[0m
[37m[1m[2023-07-11 18:36:52,343][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:36:52,350][233954] mean_value=-89.43964959807995, max_value=707.4842662936077[0m
[37m[1m[2023-07-11 18:36:52,353][233954] New mean coefficients: [[ 2.186796    0.28121325 -0.20563424 -2.2272773   3.380382    0.42976344]][0m
[37m[1m[2023-07-11 18:36:52,354][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:37:01,384][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 18:37:01,385][233954] FPS: 425307.96[0m
[36m[2023-07-11 18:37:01,387][233954] itr=1348, itrs=2000, Progress: 67.40%[0m
[36m[2023-07-11 18:37:13,102][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 18:37:13,102][233954] FPS: 330836.51[0m
[36m[2023-07-11 18:37:17,331][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:37:17,332][233954] Reward + Measures: [[-4.13785241  0.882976    0.94144058  0.17070666  0.89415497  0.89305335]][0m
[37m[1m[2023-07-11 18:37:17,332][233954] Max Reward on eval: -4.1378524090169275[0m
[37m[1m[2023-07-11 18:37:17,332][233954] Min Reward on eval: -4.1378524090169275[0m
[37m[1m[2023-07-11 18:37:17,332][233954] Mean Reward across all agents: -4.1378524090169275[0m
[37m[1m[2023-07-11 18:37:17,333][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:37:22,587][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:37:22,593][233954] Reward + Measures: [[  12.84042609    0.92900002    0.87799996    0.66640002    0.46879998
     2.39921808]
 [   7.29850668    0.94779998    0.97530001    0.0245        0.97670001
     1.57896042]
 [-195.50914911    0.71560001    0.48899999    0.64950001    0.16510001
     2.50451446]
 ...
 [ -17.19409297    0.52490002    0.14230001    0.50590003    0.32320002
     1.80247402]
 [ 130.1850691     0.8754999     0.28580004    0.83640003    0.43390003
     2.20763564]
 [ 203.2402308     0.73890001    0.64219999    0.2041        0.61799997
     3.25292897]][0m
[37m[1m[2023-07-11 18:37:22,593][233954] Max Reward on eval: 594.0317244630307[0m
[37m[1m[2023-07-11 18:37:22,593][233954] Min Reward on eval: -593.6998290874064[0m
[37m[1m[2023-07-11 18:37:22,593][233954] Mean Reward across all agents: -28.086984346852525[0m
[37m[1m[2023-07-11 18:37:22,594][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:37:22,605][233954] mean_value=64.84973366935274, max_value=742.292561521288[0m
[37m[1m[2023-07-11 18:37:22,608][233954] New mean coefficients: [[ 3.2012274   0.5861908  -0.72788346 -1.4773053   3.852355    0.35751444]][0m
[37m[1m[2023-07-11 18:37:22,609][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:37:31,589][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 18:37:31,589][233954] FPS: 427696.58[0m
[36m[2023-07-11 18:37:31,592][233954] itr=1349, itrs=2000, Progress: 67.45%[0m
[36m[2023-07-11 18:37:43,178][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 18:37:43,178][233954] FPS: 334692.38[0m
[36m[2023-07-11 18:37:47,462][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:37:47,462][233954] Reward + Measures: [[36.59245282  0.97008365  0.99113005  0.21836531  0.9659583   1.96826208]][0m
[37m[1m[2023-07-11 18:37:47,463][233954] Max Reward on eval: 36.59245281952301[0m
[37m[1m[2023-07-11 18:37:47,463][233954] Min Reward on eval: 36.59245281952301[0m
[37m[1m[2023-07-11 18:37:47,463][233954] Mean Reward across all agents: 36.59245281952301[0m
[37m[1m[2023-07-11 18:37:47,463][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:37:52,459][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:37:52,460][233954] Reward + Measures: [[ 187.47819902    0.45390001    0.25009999    0.4522        0.38929996
     2.22735095]
 [ -56.31512476    0.26659998    0.26180002    0.22130001    0.16800001
     2.05657768]
 [ 214.8465557     0.98460001    0.86279994    0.96289998    0.3082
     2.25182557]
 ...
 [ -23.33433504    0.31940001    0.15790002    0.29700002    0.1134
     2.42022109]
 [-157.38758964    0.29370001    0.28400001    0.1559        0.20369999
     1.87491667]
 [  -8.05187182    0.16660002    0.16339999    0.16019998    0.1207
     2.56986737]][0m
[37m[1m[2023-07-11 18:37:52,460][233954] Max Reward on eval: 401.7348947697552[0m
[37m[1m[2023-07-11 18:37:52,461][233954] Min Reward on eval: -691.6067657150328[0m
[37m[1m[2023-07-11 18:37:52,461][233954] Mean Reward across all agents: -13.645057866477698[0m
[37m[1m[2023-07-11 18:37:52,461][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:37:52,467][233954] mean_value=-204.9104875302022, max_value=712.0358755407115[0m
[37m[1m[2023-07-11 18:37:52,470][233954] New mean coefficients: [[ 2.1327372   0.384302   -0.9319647  -1.6544328   3.0046294   0.79471344]][0m
[37m[1m[2023-07-11 18:37:52,471][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:38:01,477][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 18:38:01,477][233954] FPS: 426468.68[0m
[36m[2023-07-11 18:38:01,479][233954] itr=1350, itrs=2000, Progress: 67.50%[0m
[37m[1m[2023-07-11 18:42:04,071][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001330[0m
[36m[2023-07-11 18:42:17,584][233954] train() took 12.14 seconds to complete[0m
[36m[2023-07-11 18:42:17,584][233954] FPS: 316314.49[0m
[36m[2023-07-11 18:42:21,822][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:42:21,827][233954] Reward + Measures: [[23.70193014  0.99089867  0.99278498  0.94356394  0.97794467  2.42084599]][0m
[37m[1m[2023-07-11 18:42:21,828][233954] Max Reward on eval: 23.701930139295175[0m
[37m[1m[2023-07-11 18:42:21,828][233954] Min Reward on eval: 23.701930139295175[0m
[37m[1m[2023-07-11 18:42:21,828][233954] Mean Reward across all agents: 23.701930139295175[0m
[37m[1m[2023-07-11 18:42:21,828][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:42:27,027][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:42:27,033][233954] Reward + Measures: [[373.98256302   0.92769998   0.50690001   0.85839999   0.0528
    1.81286967]
 [ -8.24808456   0.4298       0.43109998   0.2739       0.3037
    2.70201921]
 [-14.57623529   0.60760003   0.62230003   0.37419999   0.42410001
    2.62429214]
 ...
 [ 90.95931673   0.86679995   0.87550002   0.0064       0.86720002
    3.11791325]
 [137.17446279   0.62740004   0.31730005   0.58569998   0.3283
    1.62114644]
 [ 17.09835051   0.86559993   0.92049998   0.4262       0.58810008
    2.76556826]][0m
[37m[1m[2023-07-11 18:42:27,033][233954] Max Reward on eval: 547.9949951341375[0m
[37m[1m[2023-07-11 18:42:27,034][233954] Min Reward on eval: -474.04268742736895[0m
[37m[1m[2023-07-11 18:42:27,034][233954] Mean Reward across all agents: 48.729412892941106[0m
[37m[1m[2023-07-11 18:42:27,034][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:42:27,046][233954] mean_value=45.736140240682154, max_value=898.4794071490364[0m
[37m[1m[2023-07-11 18:42:27,048][233954] New mean coefficients: [[ 1.9880556   0.3234479  -1.6943226  -1.1363162   1.6968077   0.93142176]][0m
[37m[1m[2023-07-11 18:42:27,049][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:42:35,927][233954] train() took 8.88 seconds to complete[0m
[36m[2023-07-11 18:42:35,928][233954] FPS: 432608.15[0m
[36m[2023-07-11 18:42:35,930][233954] itr=1351, itrs=2000, Progress: 67.55%[0m
[36m[2023-07-11 18:42:47,778][233954] train() took 11.73 seconds to complete[0m
[36m[2023-07-11 18:42:47,779][233954] FPS: 327244.81[0m
[36m[2023-07-11 18:42:52,022][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:42:52,022][233954] Reward + Measures: [[-52.25689893   0.87629575   0.81854069   0.22348601   0.96803629
    1.9952662 ]][0m
[37m[1m[2023-07-11 18:42:52,022][233954] Max Reward on eval: -52.25689892521962[0m
[37m[1m[2023-07-11 18:42:52,023][233954] Min Reward on eval: -52.25689892521962[0m
[37m[1m[2023-07-11 18:42:52,023][233954] Mean Reward across all agents: -52.25689892521962[0m
[37m[1m[2023-07-11 18:42:52,023][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:42:56,994][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:42:56,995][233954] Reward + Measures: [[  -2.99276032    0.88900006    0.88980001    0.39659998    0.93240005
     1.60963655]
 [  49.83622455    0.95340008    0.9582001     0.52170002    0.96090001
     1.45664692]
 [  20.7725602     0.90950006    0.90799999    0.47779998    0.94999999
     1.75640571]
 ...
 [-134.60742855    0.76640004    0.74649996    0.21529999    0.77210003
     1.84936547]
 [ -68.78401089    0.68380004    0.67689997    0.259         0.71020001
     1.79784453]
 [ -44.58309961    0.75839996    0.7683        0.81709999    0.75000006
     2.0287466 ]][0m
[37m[1m[2023-07-11 18:42:56,995][233954] Max Reward on eval: 200.13959024325013[0m
[37m[1m[2023-07-11 18:42:56,996][233954] Min Reward on eval: -226.11223220098765[0m
[37m[1m[2023-07-11 18:42:56,996][233954] Mean Reward across all agents: -36.33999022128094[0m
[37m[1m[2023-07-11 18:42:56,996][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:42:57,003][233954] mean_value=27.738198001417913, max_value=604.709254263714[0m
[37m[1m[2023-07-11 18:42:57,006][233954] New mean coefficients: [[ 1.5545805   0.72569275 -1.5621129   0.6707381   1.4033237   1.0067234 ]][0m
[37m[1m[2023-07-11 18:42:57,007][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:43:05,967][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 18:43:05,967][233954] FPS: 428671.37[0m
[36m[2023-07-11 18:43:05,969][233954] itr=1352, itrs=2000, Progress: 67.60%[0m
[36m[2023-07-11 18:43:17,508][233954] train() took 11.43 seconds to complete[0m
[36m[2023-07-11 18:43:17,509][233954] FPS: 335935.65[0m
[36m[2023-07-11 18:43:21,769][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:43:21,770][233954] Reward + Measures: [[171.031788     0.94924301   0.98629802   0.42597803   0.93890494
    2.50388646]][0m
[37m[1m[2023-07-11 18:43:21,770][233954] Max Reward on eval: 171.0317879985258[0m
[37m[1m[2023-07-11 18:43:21,770][233954] Min Reward on eval: 171.0317879985258[0m
[37m[1m[2023-07-11 18:43:21,770][233954] Mean Reward across all agents: 171.0317879985258[0m
[37m[1m[2023-07-11 18:43:21,771][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:43:26,714][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:43:26,715][233954] Reward + Measures: [[ -43.12158935    0.58900011    0.65109998    0.42519999    0.66820002
     2.74655962]
 [ 193.51656268    0.63499999    0.66580003    0.3486        0.48419997
     2.12584972]
 [ -51.63379705    0.92440003    0.86009997    0.6354        0.89699996
     2.67692494]
 ...
 [  33.86649751    0.96259993    0.96880001    0.0304        0.93710005
     2.39370036]
 [-148.05949507    0.93959999    0.8962        0.75669998    0.92410004
     2.81356883]
 [ -74.48091938    0.86049998    0.75820005    0.52319998    0.8096
     2.46433353]][0m
[37m[1m[2023-07-11 18:43:26,715][233954] Max Reward on eval: 252.700340274442[0m
[37m[1m[2023-07-11 18:43:26,715][233954] Min Reward on eval: -527.2618681351654[0m
[37m[1m[2023-07-11 18:43:26,715][233954] Mean Reward across all agents: -22.784046501514148[0m
[37m[1m[2023-07-11 18:43:26,716][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:43:26,724][233954] mean_value=-11.319929015070059, max_value=608.4333397448994[0m
[37m[1m[2023-07-11 18:43:26,727][233954] New mean coefficients: [[ 1.2704127   1.0282327  -0.7649845   1.0681703   0.6560241   0.43028826]][0m
[37m[1m[2023-07-11 18:43:26,728][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:43:35,643][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 18:43:35,644][233954] FPS: 430772.25[0m
[36m[2023-07-11 18:43:35,646][233954] itr=1353, itrs=2000, Progress: 67.65%[0m
[36m[2023-07-11 18:43:47,376][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 18:43:47,376][233954] FPS: 330470.75[0m
[36m[2023-07-11 18:43:51,585][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:43:51,586][233954] Reward + Measures: [[-74.4591642    0.99200666   0.99332899   0.97921336   0.98307127
    2.77932525]][0m
[37m[1m[2023-07-11 18:43:51,586][233954] Max Reward on eval: -74.45916420227955[0m
[37m[1m[2023-07-11 18:43:51,586][233954] Min Reward on eval: -74.45916420227955[0m
[37m[1m[2023-07-11 18:43:51,587][233954] Mean Reward across all agents: -74.45916420227955[0m
[37m[1m[2023-07-11 18:43:51,587][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:43:56,538][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:43:56,544][233954] Reward + Measures: [[ 81.0305557    0.87290001   0.87659997   0.63589996   0.66760004
    2.55887508]
 [-51.8554871    0.80500001   0.83479995   0.63450003   0.653
    3.07943749]
 [ 42.0278734    0.99400008   0.98839998   0.98219997   0.96890002
    2.93897915]
 ...
 [-49.67816298   0.98800004   0.98660004   0.96950001   0.96629995
    2.91475844]
 [  7.80337233   0.50780004   0.45310003   0.43730003   0.3892
    1.8588587 ]
 [145.91736195   0.64410001   0.55089998   0.4752       0.44350001
    2.28099895]][0m
[37m[1m[2023-07-11 18:43:56,544][233954] Max Reward on eval: 344.6343296406791[0m
[37m[1m[2023-07-11 18:43:56,544][233954] Min Reward on eval: -713.5643272475339[0m
[37m[1m[2023-07-11 18:43:56,545][233954] Mean Reward across all agents: 3.645162343219854[0m
[37m[1m[2023-07-11 18:43:56,545][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:43:56,551][233954] mean_value=-97.24844581332366, max_value=552.5465664146877[0m
[37m[1m[2023-07-11 18:43:56,554][233954] New mean coefficients: [[ 0.2434839   1.2893908  -0.21356565  1.1299926   0.46131307  0.05220497]][0m
[37m[1m[2023-07-11 18:43:56,555][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:44:05,573][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 18:44:05,573][233954] FPS: 425919.47[0m
[36m[2023-07-11 18:44:05,575][233954] itr=1354, itrs=2000, Progress: 67.70%[0m
[36m[2023-07-11 18:44:17,125][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 18:44:17,126][233954] FPS: 335622.96[0m
[36m[2023-07-11 18:44:21,392][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:44:21,392][233954] Reward + Measures: [[-74.87766011   0.99081671   0.99412435   0.99119967   0.9867534
    3.07175398]][0m
[37m[1m[2023-07-11 18:44:21,392][233954] Max Reward on eval: -74.87766011453009[0m
[37m[1m[2023-07-11 18:44:21,393][233954] Min Reward on eval: -74.87766011453009[0m
[37m[1m[2023-07-11 18:44:21,393][233954] Mean Reward across all agents: -74.87766011453009[0m
[37m[1m[2023-07-11 18:44:21,393][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:44:26,319][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:44:26,320][233954] Reward + Measures: [[ 71.96754626   0.87540001   0.46370003   0.73329997   0.47790003
    2.67878318]
 [-15.01338859   0.27149999   0.21000002   0.25659999   0.27720004
    2.50491452]
 [ 16.2533064    0.61830002   0.57310003   0.57609999   0.5729
    3.43323684]
 ...
 [  2.71074689   0.96869993   0.96530002   0.92249995   0.90609998
    2.87814713]
 [ 49.50698716   0.84370005   0.86669999   0.73839998   0.76239997
    2.77779198]
 [ 18.58444241   0.9429       0.88249999   0.90710002   0.87010002
    2.96542311]][0m
[37m[1m[2023-07-11 18:44:26,320][233954] Max Reward on eval: 534.8717765721027[0m
[37m[1m[2023-07-11 18:44:26,320][233954] Min Reward on eval: -706.4027633699122[0m
[37m[1m[2023-07-11 18:44:26,321][233954] Mean Reward across all agents: -6.7652296509252094[0m
[37m[1m[2023-07-11 18:44:26,321][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:44:26,328][233954] mean_value=-17.158995979590546, max_value=633.1307454138994[0m
[37m[1m[2023-07-11 18:44:26,331][233954] New mean coefficients: [[ 0.38206565  2.1544476  -0.15141195  0.24077451  1.6332593  -0.21607095]][0m
[37m[1m[2023-07-11 18:44:26,332][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:44:35,281][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 18:44:35,281][233954] FPS: 429173.29[0m
[36m[2023-07-11 18:44:35,284][233954] itr=1355, itrs=2000, Progress: 67.75%[0m
[36m[2023-07-11 18:44:46,850][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 18:44:46,850][233954] FPS: 335211.55[0m
[36m[2023-07-11 18:44:51,054][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:44:51,055][233954] Reward + Measures: [[-86.20291263   0.98758698   0.82501334   0.9881897    0.86241126
    3.08038855]][0m
[37m[1m[2023-07-11 18:44:51,055][233954] Max Reward on eval: -86.2029126342221[0m
[37m[1m[2023-07-11 18:44:51,055][233954] Min Reward on eval: -86.2029126342221[0m
[37m[1m[2023-07-11 18:44:51,055][233954] Mean Reward across all agents: -86.2029126342221[0m
[37m[1m[2023-07-11 18:44:51,056][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:44:56,082][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:44:56,083][233954] Reward + Measures: [[ -39.5674593     0.64950001    0.65740001    0.28379998    0.71789998
     2.71669412]
 [ -51.28124905    0.96550006    0.57350004    0.94980001    0.68720001
     2.5526371 ]
 [-290.22172927    0.08220001    0.90280002    0.73089999    0.89050001
     3.67226839]
 ...
 [ -78.21867072    0.99220002    0.99510002    0.99309999    0.98710006
     3.02855992]
 [  37.59824739    0.92950004    0.30020002    0.95340008    0.25999999
     2.90188575]
 [ -34.46000463    0.71160001    0.1969        0.69849998    0.41010004
     2.96022916]][0m
[37m[1m[2023-07-11 18:44:56,083][233954] Max Reward on eval: 743.8713607755956[0m
[37m[1m[2023-07-11 18:44:56,083][233954] Min Reward on eval: -731.8383674629033[0m
[37m[1m[2023-07-11 18:44:56,083][233954] Mean Reward across all agents: -20.812115714924808[0m
[37m[1m[2023-07-11 18:44:56,084][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:44:56,094][233954] mean_value=24.787580118074473, max_value=920.994727601111[0m
[37m[1m[2023-07-11 18:44:56,097][233954] New mean coefficients: [[ 0.3201797   1.4361064   0.24639657  0.09030588  1.8682538  -0.22479938]][0m
[37m[1m[2023-07-11 18:44:56,098][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:45:04,994][233954] train() took 8.89 seconds to complete[0m
[36m[2023-07-11 18:45:04,995][233954] FPS: 431729.74[0m
[36m[2023-07-11 18:45:04,997][233954] itr=1356, itrs=2000, Progress: 67.80%[0m
[36m[2023-07-11 18:45:16,510][233954] train() took 11.40 seconds to complete[0m
[36m[2023-07-11 18:45:16,510][233954] FPS: 336727.22[0m
[36m[2023-07-11 18:45:20,744][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:45:20,745][233954] Reward + Measures: [[-99.69945074   0.99336636   0.83711004   0.99134833   0.89807129
    3.06913638]][0m
[37m[1m[2023-07-11 18:45:20,745][233954] Max Reward on eval: -99.6994507392914[0m
[37m[1m[2023-07-11 18:45:20,745][233954] Min Reward on eval: -99.6994507392914[0m
[37m[1m[2023-07-11 18:45:20,745][233954] Mean Reward across all agents: -99.6994507392914[0m
[37m[1m[2023-07-11 18:45:20,745][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:45:25,942][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:45:25,943][233954] Reward + Measures: [[ 309.62413689    0.52020001    0.49270001    0.36709997    0.4127
     3.00139165]
 [-167.97279666    0.44829997    0.89289999    0.0808        0.82310003
     2.48798966]
 [-144.38887701    0.57679999    0.73570007    0.0444        0.70020002
     2.61453938]
 ...
 [  58.19368757    0.92369998    0.93959999    0.89219999    0.94760001
     2.72432876]
 [ -54.6091376     0.80249995    0.51060003    0.67460001    0.47130004
     2.48619199]
 [-116.22225158    0.98040003    0.97409993    0.9795        0.96749991
     3.56776285]][0m
[37m[1m[2023-07-11 18:45:25,943][233954] Max Reward on eval: 672.1612892134115[0m
[37m[1m[2023-07-11 18:45:25,944][233954] Min Reward on eval: -700.3510970928706[0m
[37m[1m[2023-07-11 18:45:25,944][233954] Mean Reward across all agents: 9.467521136073449[0m
[37m[1m[2023-07-11 18:45:25,944][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:45:25,956][233954] mean_value=49.80665607343794, max_value=699.2440886950897[0m
[37m[1m[2023-07-11 18:45:25,958][233954] New mean coefficients: [[ 1.0898687   1.3298209   0.405487    0.15930167  2.4853985  -0.08390662]][0m
[37m[1m[2023-07-11 18:45:25,959][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:45:34,931][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 18:45:34,932][233954] FPS: 428086.04[0m
[36m[2023-07-11 18:45:34,934][233954] itr=1357, itrs=2000, Progress: 67.85%[0m
[36m[2023-07-11 18:45:46,589][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 18:45:46,589][233954] FPS: 332604.77[0m
[36m[2023-07-11 18:45:50,824][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:45:50,824][233954] Reward + Measures: [[-96.2408401    0.99444735   0.84527701   0.99206096   0.91089875
    3.06013179]][0m
[37m[1m[2023-07-11 18:45:50,824][233954] Max Reward on eval: -96.24084009816633[0m
[37m[1m[2023-07-11 18:45:50,825][233954] Min Reward on eval: -96.24084009816633[0m
[37m[1m[2023-07-11 18:45:50,825][233954] Mean Reward across all agents: -96.24084009816633[0m
[37m[1m[2023-07-11 18:45:50,825][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:45:55,817][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:45:55,818][233954] Reward + Measures: [[-113.20201043    0.93489999    0.15750001    0.92129993    0.33540002
     3.37851119]
 [  40.30722044    0.9077        0.22309999    0.86310005    0.30780002
     3.35722518]
 [  58.49900702    0.65959996    0.26279998    0.5570001     0.1727
     2.18906522]
 ...
 [ 405.08014298    0.99379998    0.94740003    0.99160004    0.0053
     3.62652469]
 [ -12.16872108    0.97119999    0.25840002    0.96289998    0.49519998
     2.796736  ]
 [ -23.02184283    0.86890012    0.82200003    0.49650002    0.87169999
     2.73281741]][0m
[37m[1m[2023-07-11 18:45:55,818][233954] Max Reward on eval: 542.4389629482291[0m
[37m[1m[2023-07-11 18:45:55,818][233954] Min Reward on eval: -370.3711843289435[0m
[37m[1m[2023-07-11 18:45:55,819][233954] Mean Reward across all agents: -12.309552523092547[0m
[37m[1m[2023-07-11 18:45:55,819][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:45:55,828][233954] mean_value=-4.083309676259984, max_value=595.8374763830565[0m
[37m[1m[2023-07-11 18:45:55,831][233954] New mean coefficients: [[ 0.6071335   0.427167    0.8510145  -1.8155041   1.810745   -0.28778136]][0m
[37m[1m[2023-07-11 18:45:55,832][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:46:04,783][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 18:46:04,783][233954] FPS: 429060.16[0m
[36m[2023-07-11 18:46:04,785][233954] itr=1358, itrs=2000, Progress: 67.90%[0m
[36m[2023-07-11 18:46:16,350][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 18:46:16,350][233954] FPS: 335365.22[0m
[36m[2023-07-11 18:46:20,633][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:46:20,634][233954] Reward + Measures: [[-47.42807258   0.99217498   0.99365675   0.9904803    0.98783201
    2.88818812]][0m
[37m[1m[2023-07-11 18:46:20,634][233954] Max Reward on eval: -47.42807258436908[0m
[37m[1m[2023-07-11 18:46:20,634][233954] Min Reward on eval: -47.42807258436908[0m
[37m[1m[2023-07-11 18:46:20,634][233954] Mean Reward across all agents: -47.42807258436908[0m
[37m[1m[2023-07-11 18:46:20,635][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:46:25,611][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:46:25,611][233954] Reward + Measures: [[-112.01102441    0.87610006    0.0122        0.88709992    0.84539998
     3.62212491]
 [  70.71846519    0.9483        0.93900007    0.76670003    0.92830002
     2.83460879]
 [ -28.15114215    0.98839998    0.97310001    0.98450005    0.96480006
     3.02397346]
 ...
 [ -19.02257549    0.6038        0.5011        0.3211        0.57020003
     2.49716663]
 [  20.59011914    0.92980003    0.94459993    0.80820006    0.94329995
     2.6940217 ]
 [ 190.92462923    0.1955        0.63700002    0.57609999    0.67410004
     2.74351096]][0m
[37m[1m[2023-07-11 18:46:25,612][233954] Max Reward on eval: 307.23586844448[0m
[37m[1m[2023-07-11 18:46:25,612][233954] Min Reward on eval: -647.5566635021008[0m
[37m[1m[2023-07-11 18:46:25,612][233954] Mean Reward across all agents: 2.7828231916801114[0m
[37m[1m[2023-07-11 18:46:25,612][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:46:25,620][233954] mean_value=-10.049786540160724, max_value=624.9036818264145[0m
[37m[1m[2023-07-11 18:46:25,623][233954] New mean coefficients: [[ 0.6589541  -0.32155555  0.9117598  -2.5125265   1.0523999  -0.5685075 ]][0m
[37m[1m[2023-07-11 18:46:25,624][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:46:34,603][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 18:46:34,603][233954] FPS: 427774.69[0m
[36m[2023-07-11 18:46:34,605][233954] itr=1359, itrs=2000, Progress: 67.95%[0m
[36m[2023-07-11 18:46:46,292][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 18:46:46,292][233954] FPS: 331695.59[0m
[36m[2023-07-11 18:46:50,539][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:46:50,540][233954] Reward + Measures: [[-43.82025538   0.99208969   0.9936769    0.99034357   0.98749524
    2.87509751]][0m
[37m[1m[2023-07-11 18:46:50,540][233954] Max Reward on eval: -43.82025537643216[0m
[37m[1m[2023-07-11 18:46:50,540][233954] Min Reward on eval: -43.82025537643216[0m
[37m[1m[2023-07-11 18:46:50,540][233954] Mean Reward across all agents: -43.82025537643216[0m
[37m[1m[2023-07-11 18:46:50,541][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:46:55,511][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:46:55,517][233954] Reward + Measures: [[ -15.86749046    0.94869995    0.94539994    0.89849997    0.92080003
     3.04914975]
 [-108.46774043    0.99110001    0.76700002    0.99039996    0.87019998
     3.04864812]
 [-168.37851535    0.9813        0.98229998    0.95949996    0.97349995
     2.92790961]
 ...
 [  20.26707709    0.87150002    0.87799996    0.80669993    0.84709996
     3.07099438]
 [ -77.31460596    0.9842        0.97100002    0.94880003    0.95960009
     2.83498693]
 [ -18.46228798    0.92740005    0.93949997    0.91119999    0.91520005
     3.12325835]][0m
[37m[1m[2023-07-11 18:46:55,517][233954] Max Reward on eval: 160.65785215646028[0m
[37m[1m[2023-07-11 18:46:55,518][233954] Min Reward on eval: -295.62904287469576[0m
[37m[1m[2023-07-11 18:46:55,518][233954] Mean Reward across all agents: -51.72692525587537[0m
[37m[1m[2023-07-11 18:46:55,518][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:46:55,521][233954] mean_value=-202.54130205815528, max_value=376.8802799855146[0m
[37m[1m[2023-07-11 18:46:55,523][233954] New mean coefficients: [[-0.6921956  -0.13340417  1.4491072  -2.7579968   0.89457154 -1.1077962 ]][0m
[37m[1m[2023-07-11 18:46:55,524][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:47:04,504][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 18:47:04,504][233954] FPS: 427732.75[0m
[36m[2023-07-11 18:47:04,506][233954] itr=1360, itrs=2000, Progress: 68.00%[0m
[37m[1m[2023-07-11 18:51:28,243][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001340[0m
[36m[2023-07-11 18:51:42,187][233954] train() took 12.16 seconds to complete[0m
[36m[2023-07-11 18:51:42,188][233954] FPS: 315859.51[0m
[36m[2023-07-11 18:51:46,516][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:51:46,517][233954] Reward + Measures: [[5.23913438 0.87910402 0.73101366 0.78875327 0.84062004 2.6031971 ]][0m
[37m[1m[2023-07-11 18:51:46,517][233954] Max Reward on eval: 5.239134384870878[0m
[37m[1m[2023-07-11 18:51:46,517][233954] Min Reward on eval: 5.239134384870878[0m
[37m[1m[2023-07-11 18:51:46,518][233954] Mean Reward across all agents: 5.239134384870878[0m
[37m[1m[2023-07-11 18:51:46,518][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:51:51,496][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:51:51,497][233954] Reward + Measures: [[ 75.6117549    0.0905       0.43640003   0.26830003   0.44010001
    2.92247438]
 [284.3195987    0.0537       0.7937001    0.52679998   0.88459998
    3.0009315 ]
 [-83.32345862   0.66990006   0.53459996   0.3987       0.51630002
    2.5955236 ]
 ...
 [ 54.19317366   0.0635       0.8865       0.3184       0.8373
    3.27254415]
 [ 71.21620263   0.12850001   0.80200005   0.25670001   0.77100003
    3.06625986]
 [-14.85936581   0.64070004   0.70800006   0.33970001   0.73890001
    2.73076963]][0m
[37m[1m[2023-07-11 18:51:51,497][233954] Max Reward on eval: 716.8276329034009[0m
[37m[1m[2023-07-11 18:51:51,497][233954] Min Reward on eval: -669.4553756547393[0m
[37m[1m[2023-07-11 18:51:51,498][233954] Mean Reward across all agents: 60.06870388973354[0m
[37m[1m[2023-07-11 18:51:51,498][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:51:51,508][233954] mean_value=-189.21617219742166, max_value=620.6622028486803[0m
[37m[1m[2023-07-11 18:51:51,512][233954] New mean coefficients: [[-0.8493855  -0.32588828  1.2210627  -0.99071765  1.204284   -0.7812071 ]][0m
[37m[1m[2023-07-11 18:51:51,513][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:52:00,455][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 18:52:00,455][233954] FPS: 429526.74[0m
[36m[2023-07-11 18:52:00,458][233954] itr=1361, itrs=2000, Progress: 68.05%[0m
[36m[2023-07-11 18:52:12,314][233954] train() took 11.74 seconds to complete[0m
[36m[2023-07-11 18:52:12,314][233954] FPS: 327025.35[0m
[36m[2023-07-11 18:52:16,564][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:52:16,564][233954] Reward + Measures: [[-273.45216536    0.99015623    0.96117765    0.98197329    0.95951593
     3.34145641]][0m
[37m[1m[2023-07-11 18:52:16,565][233954] Max Reward on eval: -273.4521653636017[0m
[37m[1m[2023-07-11 18:52:16,565][233954] Min Reward on eval: -273.4521653636017[0m
[37m[1m[2023-07-11 18:52:16,565][233954] Mean Reward across all agents: -273.4521653636017[0m
[37m[1m[2023-07-11 18:52:16,565][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:52:21,532][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:52:21,533][233954] Reward + Measures: [[-134.84176448    0.67550004    0.16730002    0.65819997    0.4183
     1.91423261]
 [ -26.50109486    0.094         0.10829999    0.112         0.11259999
     2.80022669]
 [ -22.29482936    0.89130002    0.93599999    0.93989992    0.89379996
     3.15993857]
 ...
 [  66.88794877    0.1027        0.80720007    0.5557        0.85790008
     2.76891565]
 [   6.90745311    0.1313        0.23540001    0.23529999    0.26210001
     2.15737987]
 [  17.08717246    0.94960004    0.94929993    0.92320007    0.94169998
     2.76525068]][0m
[37m[1m[2023-07-11 18:52:21,533][233954] Max Reward on eval: 339.4934882940259[0m
[37m[1m[2023-07-11 18:52:21,533][233954] Min Reward on eval: -771.901375759882[0m
[37m[1m[2023-07-11 18:52:21,534][233954] Mean Reward across all agents: -56.056326013341994[0m
[37m[1m[2023-07-11 18:52:21,534][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:52:21,539][233954] mean_value=-159.92737573846446, max_value=704.8693399499124[0m
[37m[1m[2023-07-11 18:52:21,542][233954] New mean coefficients: [[-0.83660644  0.8271971   1.4864256  -0.8796443   0.7287239  -0.88252383]][0m
[37m[1m[2023-07-11 18:52:21,543][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:52:30,513][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 18:52:30,513][233954] FPS: 428162.92[0m
[36m[2023-07-11 18:52:30,516][233954] itr=1362, itrs=2000, Progress: 68.10%[0m
[36m[2023-07-11 18:52:42,314][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 18:52:42,314][233954] FPS: 328581.02[0m
[36m[2023-07-11 18:52:46,510][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:52:46,510][233954] Reward + Measures: [[11.1311748   0.98953867  0.99044561  0.98766959  0.98888832  2.68798542]][0m
[37m[1m[2023-07-11 18:52:46,510][233954] Max Reward on eval: 11.131174802941358[0m
[37m[1m[2023-07-11 18:52:46,511][233954] Min Reward on eval: 11.131174802941358[0m
[37m[1m[2023-07-11 18:52:46,511][233954] Mean Reward across all agents: 11.131174802941358[0m
[37m[1m[2023-07-11 18:52:46,511][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:52:51,409][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:52:51,410][233954] Reward + Measures: [[ 22.65805993   0.991        0.99319994   0.97080004   0.98899996
    2.60920978]
 [-34.04557983   0.9896       0.99150002   0.9896       0.98890001
    2.96067548]
 [ 35.31231338   0.98640007   0.98670006   0.98479998   0.98140001
    2.61205125]
 ...
 [ 20.38067228   0.99220002   0.991        0.98369998   0.98780006
    2.72106409]
 [ 51.3594079    0.94190007   0.88020003   0.92589998   0.88990003
    2.40931225]
 [-12.09958457   0.9472       0.96119994   0.91569996   0.9533
    2.59142995]][0m
[37m[1m[2023-07-11 18:52:51,410][233954] Max Reward on eval: 124.895906455355[0m
[37m[1m[2023-07-11 18:52:51,410][233954] Min Reward on eval: -221.67519950047136[0m
[37m[1m[2023-07-11 18:52:51,411][233954] Mean Reward across all agents: -0.3548463907693908[0m
[37m[1m[2023-07-11 18:52:51,411][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:52:51,413][233954] mean_value=-160.88299699399636, max_value=484.1562215051614[0m
[37m[1m[2023-07-11 18:52:51,416][233954] New mean coefficients: [[-0.66469336  0.8483772   1.3082204  -0.67182577  0.5462673  -1.1626086 ]][0m
[37m[1m[2023-07-11 18:52:51,417][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:53:00,371][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 18:53:00,371][233954] FPS: 428943.51[0m
[36m[2023-07-11 18:53:00,373][233954] itr=1363, itrs=2000, Progress: 68.15%[0m
[36m[2023-07-11 18:53:12,067][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 18:53:12,067][233954] FPS: 331568.70[0m
[36m[2023-07-11 18:53:16,262][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:53:16,262][233954] Reward + Measures: [[-37.9116307    0.972673     0.94294906   0.70765203   0.94574964
    2.06918573]][0m
[37m[1m[2023-07-11 18:53:16,262][233954] Max Reward on eval: -37.911630699574424[0m
[37m[1m[2023-07-11 18:53:16,262][233954] Min Reward on eval: -37.911630699574424[0m
[37m[1m[2023-07-11 18:53:16,263][233954] Mean Reward across all agents: -37.911630699574424[0m
[37m[1m[2023-07-11 18:53:16,263][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:53:21,137][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:53:21,138][233954] Reward + Measures: [[-39.77242334   0.95700008   0.91259998   0.69440001   0.92699999
    2.159971  ]
 [-38.81062816   0.89810002   0.8466       0.4154       0.85220003
    1.92012239]
 [-47.88760358   0.99309999   0.98519993   0.92989999   0.97710001
    2.10257339]
 ...
 [-10.72730364   0.88859999   0.80030006   0.51230001   0.85820001
    2.28617549]
 [-29.47797649   0.87029999   0.71890002   0.6322       0.67500001
    1.92038691]
 [-19.92791594   0.9939       0.99590009   0.98880005   0.98880005
    2.01952004]][0m
[37m[1m[2023-07-11 18:53:21,138][233954] Max Reward on eval: 50.88296925034374[0m
[37m[1m[2023-07-11 18:53:21,138][233954] Min Reward on eval: -109.01966988760978[0m
[37m[1m[2023-07-11 18:53:21,138][233954] Mean Reward across all agents: -32.30862429165754[0m
[37m[1m[2023-07-11 18:53:21,139][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:53:21,145][233954] mean_value=-17.9805818657599, max_value=502.1190188854001[0m
[37m[1m[2023-07-11 18:53:21,148][233954] New mean coefficients: [[-0.95891654  0.10705292  1.2202549  -2.9375458   0.28790432 -1.5871661 ]][0m
[37m[1m[2023-07-11 18:53:21,149][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:53:30,049][233954] train() took 8.90 seconds to complete[0m
[36m[2023-07-11 18:53:30,050][233954] FPS: 431501.69[0m
[36m[2023-07-11 18:53:30,052][233954] itr=1364, itrs=2000, Progress: 68.20%[0m
[36m[2023-07-11 18:53:41,723][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 18:53:41,723][233954] FPS: 332276.44[0m
[36m[2023-07-11 18:53:46,007][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:53:46,008][233954] Reward + Measures: [[-11.17184231   0.94358838   0.88321096   0.35796598   0.92734408
    2.04110503]][0m
[37m[1m[2023-07-11 18:53:46,008][233954] Max Reward on eval: -11.171842314087659[0m
[37m[1m[2023-07-11 18:53:46,008][233954] Min Reward on eval: -11.171842314087659[0m
[37m[1m[2023-07-11 18:53:46,008][233954] Mean Reward across all agents: -11.171842314087659[0m
[37m[1m[2023-07-11 18:53:46,009][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:53:51,006][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:53:51,006][233954] Reward + Measures: [[ 52.05832006   0.99290001   0.995        0.99250001   0.98699999
    3.37446284]
 [125.40075206   0.92409992   0.4533       0.824        0.0152
    2.57558417]
 [-13.07884496   0.69880003   0.58919996   0.71460003   0.0237
    2.56043553]
 ...
 [ -4.59104327   0.91400003   0.85930008   0.75480002   0.89289999
    2.46576476]
 [ 16.67962592   0.9823001    0.74800003   0.96519995   0.
    2.58658981]
 [-69.46229578   0.95110005   0.59009999   0.89600003   0.0037
    2.48929667]][0m
[37m[1m[2023-07-11 18:53:51,007][233954] Max Reward on eval: 772.550323490426[0m
[37m[1m[2023-07-11 18:53:51,007][233954] Min Reward on eval: -407.14524268265814[0m
[37m[1m[2023-07-11 18:53:51,007][233954] Mean Reward across all agents: 15.267712531523253[0m
[37m[1m[2023-07-11 18:53:51,007][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:53:51,014][233954] mean_value=-77.39888098453947, max_value=602.2109545496293[0m
[37m[1m[2023-07-11 18:53:51,016][233954] New mean coefficients: [[-0.9954686   0.14219138  1.4043523  -3.529114   -0.06542391 -1.8903271 ]][0m
[37m[1m[2023-07-11 18:53:51,017][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:54:00,043][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 18:54:00,043][233954] FPS: 425538.97[0m
[36m[2023-07-11 18:54:00,046][233954] itr=1365, itrs=2000, Progress: 68.25%[0m
[36m[2023-07-11 18:54:11,947][233954] train() took 11.79 seconds to complete[0m
[36m[2023-07-11 18:54:11,947][233954] FPS: 325797.50[0m
[36m[2023-07-11 18:54:16,192][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:54:16,193][233954] Reward + Measures: [[14.54127866  0.91385496  0.884215    0.73024774  0.88620561  2.08021259]][0m
[37m[1m[2023-07-11 18:54:16,193][233954] Max Reward on eval: 14.541278664530687[0m
[37m[1m[2023-07-11 18:54:16,193][233954] Min Reward on eval: 14.541278664530687[0m
[37m[1m[2023-07-11 18:54:16,193][233954] Mean Reward across all agents: 14.541278664530687[0m
[37m[1m[2023-07-11 18:54:16,194][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:54:21,204][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:54:21,205][233954] Reward + Measures: [[-63.21356562   0.69770002   0.60580003   0.48699999   0.66060001
    2.12306786]
 [-75.94565026   0.63009995   0.55720001   0.41249999   0.53570002
    2.81372237]
 [-53.61697341   0.76480001   0.59480006   0.34120002   0.80460006
    2.6136322 ]
 ...
 [ -3.75018486   0.96969998   0.91849995   0.81779999   0.90459996
    2.21366978]
 [-53.73854815   0.72559994   0.4289       0.68320006   0.433
    2.94059277]
 [-94.85483406   0.96540004   0.94919997   0.96989995   0.90150005
    2.83689857]][0m
[37m[1m[2023-07-11 18:54:21,205][233954] Max Reward on eval: 121.62943736929446[0m
[37m[1m[2023-07-11 18:54:21,205][233954] Min Reward on eval: -223.8989191113971[0m
[37m[1m[2023-07-11 18:54:21,205][233954] Mean Reward across all agents: -30.56476953384612[0m
[37m[1m[2023-07-11 18:54:21,206][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:54:21,212][233954] mean_value=-35.16881064023269, max_value=525.1955707705886[0m
[37m[1m[2023-07-11 18:54:21,215][233954] New mean coefficients: [[-1.5907831   0.17868268  1.6219151  -3.231989   -0.47161427 -2.074603  ]][0m
[37m[1m[2023-07-11 18:54:21,216][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:54:30,184][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 18:54:30,184][233954] FPS: 428274.54[0m
[36m[2023-07-11 18:54:30,186][233954] itr=1366, itrs=2000, Progress: 68.30%[0m
[36m[2023-07-11 18:54:42,204][233954] train() took 11.91 seconds to complete[0m
[36m[2023-07-11 18:54:42,210][233954] FPS: 322497.62[0m
[36m[2023-07-11 18:54:46,500][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:54:46,501][233954] Reward + Measures: [[-21.47066967   0.82318634   0.7386421    0.37994865   0.83435303
    1.7914269 ]][0m
[37m[1m[2023-07-11 18:54:46,501][233954] Max Reward on eval: -21.470669667242255[0m
[37m[1m[2023-07-11 18:54:46,501][233954] Min Reward on eval: -21.470669667242255[0m
[37m[1m[2023-07-11 18:54:46,501][233954] Mean Reward across all agents: -21.470669667242255[0m
[37m[1m[2023-07-11 18:54:46,502][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:54:51,431][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:54:51,431][233954] Reward + Measures: [[-128.73801135    0.64180005    0.34600002    0.59050006    0.61970007
     1.92352581]
 [  38.94820807    0.52230006    0.46790001    0.229         0.45879999
     2.05574393]
 [  56.30766055    0.60330003    0.65790004    0.37410003    0.64719999
     2.37580681]
 ...
 [  -3.65059718    0.4587        0.3432        0.28739998    0.31420001
     1.93798721]
 [  11.32913718    0.63810003    0.52340001    0.39570001    0.5614
     1.9077183 ]
 [-102.5393052     0.83280003    0.37470001    0.80489999    0.65040004
     2.41242075]][0m
[37m[1m[2023-07-11 18:54:51,431][233954] Max Reward on eval: 455.5608520489186[0m
[37m[1m[2023-07-11 18:54:51,432][233954] Min Reward on eval: -128.73801135234535[0m
[37m[1m[2023-07-11 18:54:51,432][233954] Mean Reward across all agents: 60.75681877782595[0m
[37m[1m[2023-07-11 18:54:51,432][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:54:51,440][233954] mean_value=6.0867684735419125, max_value=575.6069884093479[0m
[37m[1m[2023-07-11 18:54:51,442][233954] New mean coefficients: [[-2.1702986   1.7206097   1.7293824  -2.2511463  -0.60470766 -2.2918642 ]][0m
[37m[1m[2023-07-11 18:54:51,443][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:55:00,429][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 18:55:00,429][233954] FPS: 427427.21[0m
[36m[2023-07-11 18:55:00,431][233954] itr=1367, itrs=2000, Progress: 68.35%[0m
[36m[2023-07-11 18:55:12,171][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 18:55:12,171][233954] FPS: 330308.23[0m
[36m[2023-07-11 18:55:16,394][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:55:16,399][233954] Reward + Measures: [[-24.76742678   0.8483696    0.76138568   0.30912301   0.85372663
    1.75367582]][0m
[37m[1m[2023-07-11 18:55:16,399][233954] Max Reward on eval: -24.767426780878527[0m
[37m[1m[2023-07-11 18:55:16,400][233954] Min Reward on eval: -24.767426780878527[0m
[37m[1m[2023-07-11 18:55:16,400][233954] Mean Reward across all agents: -24.767426780878527[0m
[37m[1m[2023-07-11 18:55:16,400][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:55:21,405][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:55:21,410][233954] Reward + Measures: [[ 30.55990256   0.93220007   0.912        0.50030005   0.93400002
    1.8970083 ]
 [-62.56846333   0.70390004   0.65200007   0.1208       0.76780003
    2.01153827]
 [ -5.71933093   0.88010007   0.80100006   0.4461       0.875
    1.75347257]
 ...
 [ 65.72249056   0.96270001   0.96070004   0.61750001   0.97040004
    1.83747756]
 [-23.75206812   0.89050007   0.8524       0.23309998   0.93730003
    1.92000389]
 [-24.43054722   0.87129992   0.80940002   0.57269996   0.85970002
    1.93348372]][0m
[37m[1m[2023-07-11 18:55:21,410][233954] Max Reward on eval: 98.45012329563906[0m
[37m[1m[2023-07-11 18:55:21,411][233954] Min Reward on eval: -227.07659339671955[0m
[37m[1m[2023-07-11 18:55:21,411][233954] Mean Reward across all agents: -16.338597703273134[0m
[37m[1m[2023-07-11 18:55:21,411][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:55:21,418][233954] mean_value=137.49376236936266, max_value=436.03476069201713[0m
[37m[1m[2023-07-11 18:55:21,421][233954] New mean coefficients: [[-2.192094   2.3081772  1.8521439 -2.5717332 -0.3067852 -2.364249 ]][0m
[37m[1m[2023-07-11 18:55:21,422][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:55:30,380][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 18:55:30,380][233954] FPS: 428726.95[0m
[36m[2023-07-11 18:55:30,383][233954] itr=1368, itrs=2000, Progress: 68.40%[0m
[36m[2023-07-11 18:55:41,956][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 18:55:41,956][233954] FPS: 335115.21[0m
[36m[2023-07-11 18:55:46,227][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:55:46,227][233954] Reward + Measures: [[-17.24015882   0.8872757    0.80972964   0.24274267   0.88892627
    1.71310592]][0m
[37m[1m[2023-07-11 18:55:46,228][233954] Max Reward on eval: -17.24015882049551[0m
[37m[1m[2023-07-11 18:55:46,228][233954] Min Reward on eval: -17.24015882049551[0m
[37m[1m[2023-07-11 18:55:46,228][233954] Mean Reward across all agents: -17.24015882049551[0m
[37m[1m[2023-07-11 18:55:46,229][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:55:51,512][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:55:51,518][233954] Reward + Measures: [[ 12.77859436   0.89420003   0.8215       0.46409997   0.88899994
    1.75617909]
 [ -5.47084103   0.87410003   0.81190008   0.4127       0.88449997
    1.74331784]
 [-32.78513981   0.87370008   0.77950001   0.2009       0.88099998
    1.67091084]
 ...
 [-13.03467493   0.86250001   0.81210005   0.37579998   0.87360001
    1.76381683]
 [ -4.22618404   0.87880003   0.78940004   0.37120005   0.87229997
    1.6988076 ]
 [-14.10513609   0.86739999   0.81290001   0.44399998   0.87290001
    1.69497609]][0m
[37m[1m[2023-07-11 18:55:51,518][233954] Max Reward on eval: 73.29198646415026[0m
[37m[1m[2023-07-11 18:55:51,518][233954] Min Reward on eval: -243.108413701877[0m
[37m[1m[2023-07-11 18:55:51,519][233954] Mean Reward across all agents: -27.8779116022928[0m
[37m[1m[2023-07-11 18:55:51,519][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:55:51,525][233954] mean_value=8.657693375042454, max_value=436.43758569603835[0m
[37m[1m[2023-07-11 18:55:51,527][233954] New mean coefficients: [[-2.8911624   2.8652296   1.3164977  -1.7989322   0.24205008 -2.125743  ]][0m
[37m[1m[2023-07-11 18:55:51,528][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:56:00,496][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 18:56:00,497][233954] FPS: 428268.67[0m
[36m[2023-07-11 18:56:00,499][233954] itr=1369, itrs=2000, Progress: 68.45%[0m
[36m[2023-07-11 18:56:12,218][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 18:56:12,218][233954] FPS: 330835.23[0m
[36m[2023-07-11 18:56:16,470][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:56:16,476][233954] Reward + Measures: [[-208.24474225    0.97292656    0.05705867    0.98963803    0.411816
     2.43243265]][0m
[37m[1m[2023-07-11 18:56:16,476][233954] Max Reward on eval: -208.24474224855476[0m
[37m[1m[2023-07-11 18:56:16,477][233954] Min Reward on eval: -208.24474224855476[0m
[37m[1m[2023-07-11 18:56:16,477][233954] Mean Reward across all agents: -208.24474224855476[0m
[37m[1m[2023-07-11 18:56:16,477][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:56:21,510][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 18:56:21,515][233954] Reward + Measures: [[-136.04615304    0.67430001    0.45199999    0.62370008    0.03950001
     1.72677577]
 [-415.68117145    0.84240001    0.0483        0.76719999    0.41440001
     2.79055572]
 [  15.69509943    0.70570004    0.56370002    0.50569999    0.19319999
     2.06446052]
 ...
 [  -9.5318996     0.96540004    0.87460005    0.92609996    0.72579998
     2.66952634]
 [  76.7854912     0.9533        0.43260002    0.93379992    0.25079998
     2.60053515]
 [  66.71985888    0.93120003    0.45639998    0.92410004    0.30610001
     2.58551097]][0m
[37m[1m[2023-07-11 18:56:21,516][233954] Max Reward on eval: 201.66584251951426[0m
[37m[1m[2023-07-11 18:56:21,516][233954] Min Reward on eval: -646.7741127101705[0m
[37m[1m[2023-07-11 18:56:21,516][233954] Mean Reward across all agents: -78.29151622372814[0m
[37m[1m[2023-07-11 18:56:21,516][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 18:56:21,526][233954] mean_value=39.255035622356345, max_value=620.1506261812896[0m
[37m[1m[2023-07-11 18:56:21,529][233954] New mean coefficients: [[-3.207025    2.369785    2.1270278  -1.6897639  -0.89605606 -2.7428508 ]][0m
[37m[1m[2023-07-11 18:56:21,530][233954] Moving the mean solution point...[0m
[36m[2023-07-11 18:56:30,589][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 18:56:30,589][233954] FPS: 423959.58[0m
[36m[2023-07-11 18:56:30,591][233954] itr=1370, itrs=2000, Progress: 68.50%[0m
[37m[1m[2023-07-11 19:00:16,702][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001350[0m
[36m[2023-07-11 19:00:30,430][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 19:00:30,430][233954] FPS: 327848.58[0m
[36m[2023-07-11 19:00:34,713][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:00:34,714][233954] Reward + Measures: [[50.24187672  0.91827101  0.72860593  0.74467301  0.39069498  1.58045602]][0m
[37m[1m[2023-07-11 19:00:34,714][233954] Max Reward on eval: 50.241876718836025[0m
[37m[1m[2023-07-11 19:00:34,714][233954] Min Reward on eval: 50.241876718836025[0m
[37m[1m[2023-07-11 19:00:34,714][233954] Mean Reward across all agents: 50.241876718836025[0m
[37m[1m[2023-07-11 19:00:34,714][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:00:39,690][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:00:39,690][233954] Reward + Measures: [[-54.2678614    0.67259997   0.40400001   0.59830004   0.38830003
    2.04418588]
 [ 67.20173444   0.47820002   0.36379999   0.44549999   0.26100001
    2.80628848]
 [ 94.66535472   0.95850003   0.86790001   0.87690002   0.54460001
    2.06442618]
 ...
 [ 47.33319045   0.99069995   0.96060002   0.93010008   0.81720001
    1.59069526]
 [-12.19955546   0.7087       0.226        0.70850003   0.2084
    1.68985641]
 [-18.57235474   0.66240001   0.42500001   0.64990008   0.2631
    1.97897041]][0m
[37m[1m[2023-07-11 19:00:39,690][233954] Max Reward on eval: 173.30941598773933[0m
[37m[1m[2023-07-11 19:00:39,691][233954] Min Reward on eval: -274.73178765169575[0m
[37m[1m[2023-07-11 19:00:39,691][233954] Mean Reward across all agents: 15.056556130568698[0m
[37m[1m[2023-07-11 19:00:39,691][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:00:39,697][233954] mean_value=-10.835754896914459, max_value=568.8748640909092[0m
[37m[1m[2023-07-11 19:00:39,706][233954] New mean coefficients: [[-2.8262804   3.1197746   1.6951647   0.12355149 -1.4865665  -2.7527916 ]][0m
[37m[1m[2023-07-11 19:00:39,707][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:00:48,676][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 19:00:48,676][233954] FPS: 428219.37[0m
[36m[2023-07-11 19:00:48,678][233954] itr=1371, itrs=2000, Progress: 68.55%[0m
[36m[2023-07-11 19:01:00,383][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 19:01:00,383][233954] FPS: 331324.82[0m
[36m[2023-07-11 19:01:04,565][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:01:04,565][233954] Reward + Measures: [[2.81771076 0.93573976 0.69856137 0.76292169 0.25280064 1.5571208 ]][0m
[37m[1m[2023-07-11 19:01:04,565][233954] Max Reward on eval: 2.8177107562641517[0m
[37m[1m[2023-07-11 19:01:04,566][233954] Min Reward on eval: 2.8177107562641517[0m
[37m[1m[2023-07-11 19:01:04,566][233954] Mean Reward across all agents: 2.8177107562641517[0m
[37m[1m[2023-07-11 19:01:04,566][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:01:09,481][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:01:09,482][233954] Reward + Measures: [[ 408.4617653     0.96950001    0.88749999    0.80660003    0.0979
     2.05309343]
 [ -39.50136002    0.87819999    0.4305        0.82530004    0.1524
     1.58955002]
 [   5.98768961    0.95539999    0.32480001    0.95640004    0.2739
     2.54370141]
 ...
 [  47.2577092     0.8524        0.30090001    0.87040007    0.25780001
     2.47162247]
 [ -56.53616285    0.91320002    0.82840008    0.76910001    0.38799998
     2.39643431]
 [-104.19527339    0.45269999    0.1856        0.44479999    0.1831
     1.98431456]][0m
[37m[1m[2023-07-11 19:01:09,482][233954] Max Reward on eval: 537.6764374296065[0m
[37m[1m[2023-07-11 19:01:09,482][233954] Min Reward on eval: -285.16182424080324[0m
[37m[1m[2023-07-11 19:01:09,482][233954] Mean Reward across all agents: 13.291964892618408[0m
[37m[1m[2023-07-11 19:01:09,483][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:01:09,492][233954] mean_value=76.30693240840083, max_value=790.6714677537209[0m
[37m[1m[2023-07-11 19:01:09,495][233954] New mean coefficients: [[-3.1795933  2.6675634  1.2971277 -0.2008664 -3.7631774 -2.9371846]][0m
[37m[1m[2023-07-11 19:01:09,496][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:01:18,446][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 19:01:18,446][233954] FPS: 429146.32[0m
[36m[2023-07-11 19:01:18,448][233954] itr=1372, itrs=2000, Progress: 68.60%[0m
[36m[2023-07-11 19:01:29,961][233954] train() took 11.40 seconds to complete[0m
[36m[2023-07-11 19:01:29,962][233954] FPS: 336907.97[0m
[36m[2023-07-11 19:01:34,157][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:01:34,158][233954] Reward + Measures: [[-196.27042238    0.93953872    0.62971896    0.85216928    0.02558833
     1.56793427]][0m
[37m[1m[2023-07-11 19:01:34,158][233954] Max Reward on eval: -196.27042238223902[0m
[37m[1m[2023-07-11 19:01:34,158][233954] Min Reward on eval: -196.27042238223902[0m
[37m[1m[2023-07-11 19:01:34,158][233954] Mean Reward across all agents: -196.27042238223902[0m
[37m[1m[2023-07-11 19:01:34,159][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:01:39,072][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:01:39,073][233954] Reward + Measures: [[ -38.08750201    0.9526        0.69880003    0.85589999    0.0205
     1.56854486]
 [-236.47534709    0.83339995    0.67869997    0.76539999    0.0609
     1.96907163]
 [-156.16225434    0.78330004    0.4693        0.55849999    0.29539999
     1.56821847]
 ...
 [ 180.46426534    0.95510006    0.82180005    0.89659995    0.0024
     1.74336267]
 [ 485.14195253    0.99119997    0.93850005    0.97189999    0.0007
     1.78048706]
 [ 357.68305016    0.98400003    0.88139993    0.96130008    0.0198
     1.71549726]][0m
[37m[1m[2023-07-11 19:01:39,073][233954] Max Reward on eval: 582.7450752486941[0m
[37m[1m[2023-07-11 19:01:39,073][233954] Min Reward on eval: -338.79062083365864[0m
[37m[1m[2023-07-11 19:01:39,073][233954] Mean Reward across all agents: 67.28762563258752[0m
[37m[1m[2023-07-11 19:01:39,074][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:01:39,078][233954] mean_value=-66.36036477621843, max_value=398.292783756979[0m
[37m[1m[2023-07-11 19:01:39,080][233954] New mean coefficients: [[-3.4075823  3.6795578  0.7865586  2.0896857 -3.550836  -2.9335063]][0m
[37m[1m[2023-07-11 19:01:39,082][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:01:47,977][233954] train() took 8.89 seconds to complete[0m
[36m[2023-07-11 19:01:47,978][233954] FPS: 431738.08[0m
[36m[2023-07-11 19:01:47,980][233954] itr=1373, itrs=2000, Progress: 68.65%[0m
[36m[2023-07-11 19:01:59,603][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 19:01:59,604][233954] FPS: 333617.00[0m
[36m[2023-07-11 19:02:03,920][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:02:03,920][233954] Reward + Measures: [[-230.11549247    0.96121329    0.71565825    0.9109143     0.01331767
     1.55491889]][0m
[37m[1m[2023-07-11 19:02:03,921][233954] Max Reward on eval: -230.11549247346616[0m
[37m[1m[2023-07-11 19:02:03,921][233954] Min Reward on eval: -230.11549247346616[0m
[37m[1m[2023-07-11 19:02:03,921][233954] Mean Reward across all agents: -230.11549247346616[0m
[37m[1m[2023-07-11 19:02:03,921][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:02:08,936][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:02:08,936][233954] Reward + Measures: [[ 37.60486342   0.96020001   0.87740004   0.89489996   0.27070001
    1.53784633]
 [  8.88881803   0.97189999   0.80340004   0.90510005   0.0015
    1.54150546]
 [ 71.37827347   0.95930004   0.82600003   0.92110008   0.0009
    1.54805541]
 ...
 [-23.7963305    0.94659996   0.72600001   0.87180007   0.133
    1.54317987]
 [ 61.94861628   0.97600001   0.83549994   0.82910007   0.22669999
    1.52060056]
 [ 26.29998193   0.85659999   0.66890001   0.83170003   0.17390001
    1.53882492]][0m
[37m[1m[2023-07-11 19:02:08,937][233954] Max Reward on eval: 222.0265846016351[0m
[37m[1m[2023-07-11 19:02:08,937][233954] Min Reward on eval: -262.2885827952065[0m
[37m[1m[2023-07-11 19:02:08,937][233954] Mean Reward across all agents: 6.743621771035215[0m
[37m[1m[2023-07-11 19:02:08,938][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:02:08,942][233954] mean_value=-30.209337626706365, max_value=476.7163212938956[0m
[37m[1m[2023-07-11 19:02:08,945][233954] New mean coefficients: [[-4.1963716  4.0053024  2.552796   1.1309099 -3.2919934 -3.7655716]][0m
[37m[1m[2023-07-11 19:02:08,946][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:02:18,028][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 19:02:18,028][233954] FPS: 422905.86[0m
[36m[2023-07-11 19:02:18,030][233954] itr=1374, itrs=2000, Progress: 68.70%[0m
[36m[2023-07-11 19:02:29,770][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 19:02:29,770][233954] FPS: 330372.72[0m
[36m[2023-07-11 19:02:34,137][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:02:34,137][233954] Reward + Measures: [[537.07799698   0.99530631   0.98383301   0.9885096    0.00029
    2.05335474]][0m
[37m[1m[2023-07-11 19:02:34,137][233954] Max Reward on eval: 537.077996979027[0m
[37m[1m[2023-07-11 19:02:34,138][233954] Min Reward on eval: 537.077996979027[0m
[37m[1m[2023-07-11 19:02:34,138][233954] Mean Reward across all agents: 537.077996979027[0m
[37m[1m[2023-07-11 19:02:34,138][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:02:39,156][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:02:39,162][233954] Reward + Measures: [[313.49833012   0.99750006   0.98269999   0.9896       0.
    1.9667145 ]
 [550.14084434   0.99770004   0.98260003   0.98890001   0.
    2.19705391]
 [125.62935664   0.99539995   0.96470004   0.97629994   0.
    1.76375377]
 ...
 [605.35821155   0.99620003   0.98480004   0.99069995   0.0001
    2.1443181 ]
 [643.93667292   0.99610007   0.98140013   0.98939991   0.
    2.26094294]
 [504.33777047   0.99469995   0.97469997   0.97830003   0.
    2.18848395]][0m
[37m[1m[2023-07-11 19:02:39,162][233954] Max Reward on eval: 811.403076171584[0m
[37m[1m[2023-07-11 19:02:39,162][233954] Min Reward on eval: -159.8730388025462[0m
[37m[1m[2023-07-11 19:02:39,163][233954] Mean Reward across all agents: 294.0590559155832[0m
[37m[1m[2023-07-11 19:02:39,163][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:02:39,166][233954] mean_value=-140.84755095693703, max_value=394.8386331992914[0m
[37m[1m[2023-07-11 19:02:39,168][233954] New mean coefficients: [[-2.7233539   3.2203844   2.2814894  -0.26748896 -1.4892069  -4.125027  ]][0m
[37m[1m[2023-07-11 19:02:39,170][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:02:48,177][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 19:02:48,177][233954] FPS: 426385.10[0m
[36m[2023-07-11 19:02:48,180][233954] itr=1375, itrs=2000, Progress: 68.75%[0m
[36m[2023-07-11 19:02:59,960][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 19:02:59,961][233954] FPS: 329259.31[0m
[36m[2023-07-11 19:03:04,271][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:03:04,271][233954] Reward + Measures: [[-35.11442048   0.96071237   0.29341167   0.93609238   0.14578733
    1.63047576]][0m
[37m[1m[2023-07-11 19:03:04,271][233954] Max Reward on eval: -35.11442048377467[0m
[37m[1m[2023-07-11 19:03:04,272][233954] Min Reward on eval: -35.11442048377467[0m
[37m[1m[2023-07-11 19:03:04,272][233954] Mean Reward across all agents: -35.11442048377467[0m
[37m[1m[2023-07-11 19:03:04,272][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:03:09,250][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:03:09,256][233954] Reward + Measures: [[ 70.75954365   0.98760003   0.55100006   0.90630001   0.0108
    1.37319648]
 [ 81.8682518    0.94420004   0.44140002   0.81589997   0.006
    1.49154675]
 [131.52908517   0.94729996   0.66469997   0.82809991   0.0002
    1.49037826]
 ...
 [-35.77367614   0.9485001    0.23709999   0.89390004   0.12719999
    1.64294112]
 [141.44404601   0.94230002   0.50130004   0.81339997   0.0019
    1.47576046]
 [-27.44120608   0.97360003   0.44769999   0.92790002   0.0425
    1.50624955]][0m
[37m[1m[2023-07-11 19:03:09,256][233954] Max Reward on eval: 194.58667183796643[0m
[37m[1m[2023-07-11 19:03:09,256][233954] Min Reward on eval: -87.09991501520854[0m
[37m[1m[2023-07-11 19:03:09,257][233954] Mean Reward across all agents: 30.311345850787898[0m
[37m[1m[2023-07-11 19:03:09,257][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:03:09,261][233954] mean_value=-53.03215153399992, max_value=309.153686533975[0m
[37m[1m[2023-07-11 19:03:09,263][233954] New mean coefficients: [[-4.86375    3.6512349  3.2861736 -2.0934887 -2.0126648 -4.1486425]][0m
[37m[1m[2023-07-11 19:03:09,264][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:03:18,224][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 19:03:18,225][233954] FPS: 428642.19[0m
[36m[2023-07-11 19:03:18,227][233954] itr=1376, itrs=2000, Progress: 68.80%[0m
[36m[2023-07-11 19:03:29,947][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 19:03:29,947][233954] FPS: 330850.16[0m
[36m[2023-07-11 19:03:34,192][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:03:34,192][233954] Reward + Measures: [[-78.18567276   0.89457601   0.46749169   0.75031567   0.01291567
    1.08916497]][0m
[37m[1m[2023-07-11 19:03:34,193][233954] Max Reward on eval: -78.18567275934925[0m
[37m[1m[2023-07-11 19:03:34,193][233954] Min Reward on eval: -78.18567275934925[0m
[37m[1m[2023-07-11 19:03:34,193][233954] Mean Reward across all agents: -78.18567275934925[0m
[37m[1m[2023-07-11 19:03:34,193][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:03:39,240][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:03:39,240][233954] Reward + Measures: [[-32.13770214   0.87460005   0.4542       0.78979999   0.0615
    1.14035797]
 [ -1.27565287   0.85669994   0.43920001   0.80360001   0.0616
    1.17398667]
 [ 11.83770423   0.85790008   0.45500001   0.71760005   0.0185
    1.05398226]
 ...
 [ -3.34261644   0.81029999   0.33810002   0.70450002   0.1177
    1.06091845]
 [-47.47162834   0.87799996   0.46129999   0.81389999   0.05710001
    1.15166867]
 [ 37.40611299   0.79120004   0.37270004   0.65640002   0.0693
    1.09646642]][0m
[37m[1m[2023-07-11 19:03:39,240][233954] Max Reward on eval: 91.97780368551612[0m
[37m[1m[2023-07-11 19:03:39,241][233954] Min Reward on eval: -111.68871164638549[0m
[37m[1m[2023-07-11 19:03:39,241][233954] Mean Reward across all agents: -17.680951981854548[0m
[37m[1m[2023-07-11 19:03:39,241][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:03:39,243][233954] mean_value=-113.79421329798534, max_value=543.2786359534599[0m
[37m[1m[2023-07-11 19:03:39,246][233954] New mean coefficients: [[-5.5699096  3.390029   5.1373844 -4.317596  -1.5705752 -4.53094  ]][0m
[37m[1m[2023-07-11 19:03:39,247][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:03:48,192][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 19:03:48,192][233954] FPS: 429339.80[0m
[36m[2023-07-11 19:03:48,195][233954] itr=1377, itrs=2000, Progress: 68.85%[0m
[36m[2023-07-11 19:03:59,793][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 19:03:59,793][233954] FPS: 334369.09[0m
[36m[2023-07-11 19:04:04,124][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:04:04,125][233954] Reward + Measures: [[-99.48047064   0.91680694   0.50155568   0.77613574   0.01038933
    1.04607356]][0m
[37m[1m[2023-07-11 19:04:04,125][233954] Max Reward on eval: -99.48047063809979[0m
[37m[1m[2023-07-11 19:04:04,125][233954] Min Reward on eval: -99.48047063809979[0m
[37m[1m[2023-07-11 19:04:04,126][233954] Mean Reward across all agents: -99.48047063809979[0m
[37m[1m[2023-07-11 19:04:04,126][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:04:09,132][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:04:09,132][233954] Reward + Measures: [[-34.60728273   0.86280006   0.3962       0.70680004   0.06770001
    0.98674053]
 [  4.13957234   0.91949999   0.40440002   0.86860001   0.07919999
    1.09089541]
 [142.18053294   0.76309997   0.53459996   0.59280002   0.22049999
    1.03442764]
 ...
 [164.26531986   0.78069997   0.4937       0.59689999   0.18169999
    0.9236542 ]
 [  0.0033122    0.90440005   0.46899995   0.72699994   0.0026
    0.98109305]
 [102.25315473   0.77100003   0.52890009   0.61470002   0.24180003
    1.01630116]][0m
[37m[1m[2023-07-11 19:04:09,132][233954] Max Reward on eval: 190.89703558068723[0m
[37m[1m[2023-07-11 19:04:09,133][233954] Min Reward on eval: -119.93999148404691[0m
[37m[1m[2023-07-11 19:04:09,133][233954] Mean Reward across all agents: 30.598159254052067[0m
[37m[1m[2023-07-11 19:04:09,133][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:04:09,137][233954] mean_value=-32.52885485489164, max_value=597.3880043276201[0m
[37m[1m[2023-07-11 19:04:09,140][233954] New mean coefficients: [[-7.1501565  1.8428487  5.7349977 -6.121761  -2.0798204 -4.304764 ]][0m
[37m[1m[2023-07-11 19:04:09,141][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:04:18,156][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 19:04:18,156][233954] FPS: 426028.72[0m
[36m[2023-07-11 19:04:18,159][233954] itr=1378, itrs=2000, Progress: 68.90%[0m
[36m[2023-07-11 19:04:29,850][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 19:04:29,851][233954] FPS: 331581.47[0m
[36m[2023-07-11 19:04:34,204][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:04:34,205][233954] Reward + Measures: [[-118.30842477    0.92613006    0.52066368    0.78682131    0.009111
     1.03058302]][0m
[37m[1m[2023-07-11 19:04:34,205][233954] Max Reward on eval: -118.30842476513148[0m
[37m[1m[2023-07-11 19:04:34,205][233954] Min Reward on eval: -118.30842476513148[0m
[37m[1m[2023-07-11 19:04:34,206][233954] Mean Reward across all agents: -118.30842476513148[0m
[37m[1m[2023-07-11 19:04:34,206][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:04:39,558][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:04:39,559][233954] Reward + Measures: [[65.23611069  0.88889998  0.80360001  0.74119997  0.52860004  1.10285282]
 [16.23686733  0.93520004  0.90269995  0.84720004  0.77749997  1.01337945]
 [43.78612377  0.912       0.8646      0.815       0.63500005  1.10007203]
 ...
 [40.11338531  0.90850002  0.84230006  0.8075      0.63770002  1.09190965]
 [75.23305987  0.93839997  0.89700001  0.80380005  0.4454      1.09508979]
 [46.91368236  0.84209996  0.81230003  0.76840007  0.52170002  1.11535585]][0m
[37m[1m[2023-07-11 19:04:39,559][233954] Max Reward on eval: 127.88323973198422[0m
[37m[1m[2023-07-11 19:04:39,559][233954] Min Reward on eval: -122.69829463674687[0m
[37m[1m[2023-07-11 19:04:39,559][233954] Mean Reward across all agents: 41.12093724423839[0m
[37m[1m[2023-07-11 19:04:39,560][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:04:39,567][233954] mean_value=52.800511307553876, max_value=517.6175746870867[0m
[37m[1m[2023-07-11 19:04:39,570][233954] New mean coefficients: [[-6.1169662  2.2523913  4.9302254 -4.5139465 -1.8335721 -3.889115 ]][0m
[37m[1m[2023-07-11 19:04:39,572][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:04:48,574][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 19:04:48,575][233954] FPS: 426610.22[0m
[36m[2023-07-11 19:04:48,577][233954] itr=1379, itrs=2000, Progress: 68.95%[0m
[36m[2023-07-11 19:05:00,150][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 19:05:00,150][233954] FPS: 335048.03[0m
[36m[2023-07-11 19:05:04,412][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:05:04,413][233954] Reward + Measures: [[-126.05494478    0.93085128    0.5492        0.8039453     0.00781533
     1.00135791]][0m
[37m[1m[2023-07-11 19:05:04,413][233954] Max Reward on eval: -126.05494478013172[0m
[37m[1m[2023-07-11 19:05:04,413][233954] Min Reward on eval: -126.05494478013172[0m
[37m[1m[2023-07-11 19:05:04,414][233954] Mean Reward across all agents: -126.05494478013172[0m
[37m[1m[2023-07-11 19:05:04,414][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:05:09,409][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:05:09,410][233954] Reward + Measures: [[ -4.33864165   0.89899999   0.69120002   0.86040002   0.0398
    1.07627487]
 [-35.44940101   0.92630005   0.7919001    0.90790004   0.0567
    1.11821294]
 [-32.92653223   0.91759998   0.71209997   0.82380003   0.2306
    1.01040864]
 ...
 [ 45.94637512   0.89770001   0.85320008   0.7579       0.62000006
    0.92274141]
 [ 22.53317813   0.93720001   0.84730005   0.8118       0.60260004
    0.93433762]
 [ 24.56111667   0.89680004   0.89919996   0.79320008   0.76500005
    1.07354712]][0m
[37m[1m[2023-07-11 19:05:09,410][233954] Max Reward on eval: 96.73221733253449[0m
[37m[1m[2023-07-11 19:05:09,410][233954] Min Reward on eval: -128.43549058907666[0m
[37m[1m[2023-07-11 19:05:09,410][233954] Mean Reward across all agents: -3.6385316774310823[0m
[37m[1m[2023-07-11 19:05:09,411][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:05:09,413][233954] mean_value=-83.89628076315627, max_value=258.1463759983594[0m
[37m[1m[2023-07-11 19:05:09,468][233954] New mean coefficients: [[-5.155386   3.0435753  5.3718257 -5.1402483  0.9281442 -4.241428 ]][0m
[37m[1m[2023-07-11 19:05:09,469][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:05:18,490][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 19:05:18,490][233954] FPS: 425737.16[0m
[36m[2023-07-11 19:05:18,492][233954] itr=1380, itrs=2000, Progress: 69.00%[0m
[37m[1m[2023-07-11 19:09:10,269][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001360[0m
[36m[2023-07-11 19:09:24,299][233954] train() took 12.25 seconds to complete[0m
[36m[2023-07-11 19:09:24,299][233954] FPS: 313434.92[0m
[36m[2023-07-11 19:09:28,498][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:09:28,499][233954] Reward + Measures: [[-120.71640943    0.94015568    0.58821028    0.821872      0.00468333
     0.95719695]][0m
[37m[1m[2023-07-11 19:09:28,499][233954] Max Reward on eval: -120.71640942754135[0m
[37m[1m[2023-07-11 19:09:28,499][233954] Min Reward on eval: -120.71640942754135[0m
[37m[1m[2023-07-11 19:09:28,499][233954] Mean Reward across all agents: -120.71640942754135[0m
[37m[1m[2023-07-11 19:09:28,500][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:09:33,509][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:09:33,570][233954] Reward + Measures: [[-13.55944929   0.80830002   0.4982       0.63830006   0.21440001
    1.05209506]
 [-34.87755231   0.71350002   0.44650003   0.62659997   0.33760002
    0.98569459]
 [ 59.87280372   0.85570002   0.67370003   0.67480004   0.4506
    1.04408884]
 ...
 [ 61.22206497   0.88059998   0.73970002   0.66219997   0.55790001
    0.91707289]
 [-16.66662181   0.82919997   0.5413       0.65740001   0.2588
    1.02733791]
 [ 48.3879104    0.87560004   0.63819999   0.65090001   0.4237
    0.95741379]][0m
[37m[1m[2023-07-11 19:09:33,571][233954] Max Reward on eval: 117.89521862631663[0m
[37m[1m[2023-07-11 19:09:33,571][233954] Min Reward on eval: -113.80128860305994[0m
[37m[1m[2023-07-11 19:09:33,571][233954] Mean Reward across all agents: 32.049584876951[0m
[37m[1m[2023-07-11 19:09:33,571][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:09:33,579][233954] mean_value=31.479284131577952, max_value=460.24967202578364[0m
[37m[1m[2023-07-11 19:09:33,583][233954] New mean coefficients: [[-4.432559    2.385611    5.1774025  -6.040382    0.93191546 -3.939307  ]][0m
[37m[1m[2023-07-11 19:09:33,584][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:09:42,511][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 19:09:42,511][233954] FPS: 430242.46[0m
[36m[2023-07-11 19:09:42,514][233954] itr=1381, itrs=2000, Progress: 69.05%[0m
[36m[2023-07-11 19:09:54,176][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 19:09:54,176][233954] FPS: 332417.61[0m
[36m[2023-07-11 19:09:58,494][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:09:58,494][233954] Reward + Measures: [[-113.68112204    0.95193464    0.62779331    0.83892202    0.003087
     0.91761601]][0m
[37m[1m[2023-07-11 19:09:58,495][233954] Max Reward on eval: -113.68112204410878[0m
[37m[1m[2023-07-11 19:09:58,495][233954] Min Reward on eval: -113.68112204410878[0m
[37m[1m[2023-07-11 19:09:58,495][233954] Mean Reward across all agents: -113.68112204410878[0m
[37m[1m[2023-07-11 19:09:58,495][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:10:03,521][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:10:03,522][233954] Reward + Measures: [[ -46.35278094    0.79790002    0.68810004    0.62810004    0.55590004
     1.0973444 ]
 [  23.70876792    0.88679999    0.79869998    0.71810001    0.65230006
     1.00297081]
 [  39.20064569    0.93409997    0.89410001    0.82679999    0.55599999
     1.01404583]
 ...
 [-113.91643238    0.96180004    0.63169998    0.86300004    0.0033
     0.95601809]
 [ -99.15398408    0.86019993    0.53200001    0.7245        0.30130002
     1.0942179 ]
 [ -13.59240684    0.8301        0.81570005    0.76300001    0.63149995
     1.15783107]][0m
[37m[1m[2023-07-11 19:10:03,522][233954] Max Reward on eval: 76.9772257721983[0m
[37m[1m[2023-07-11 19:10:03,522][233954] Min Reward on eval: -156.9580946155591[0m
[37m[1m[2023-07-11 19:10:03,523][233954] Mean Reward across all agents: -11.291413431705505[0m
[37m[1m[2023-07-11 19:10:03,523][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:10:03,525][233954] mean_value=-66.51977914426068, max_value=193.9131465876177[0m
[37m[1m[2023-07-11 19:10:03,528][233954] New mean coefficients: [[-5.0222435  2.6990666  5.256025  -5.0179877  2.6758418 -3.4950655]][0m
[37m[1m[2023-07-11 19:10:03,529][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:10:12,504][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 19:10:12,510][233954] FPS: 427911.71[0m
[36m[2023-07-11 19:10:12,512][233954] itr=1382, itrs=2000, Progress: 69.10%[0m
[36m[2023-07-11 19:10:24,248][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 19:10:24,249][233954] FPS: 330387.32[0m
[36m[2023-07-11 19:10:28,562][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:10:28,562][233954] Reward + Measures: [[-112.85408008    0.95355499    0.64701897    0.85019839    0.00331233
     0.88750374]][0m
[37m[1m[2023-07-11 19:10:28,562][233954] Max Reward on eval: -112.85408007864422[0m
[37m[1m[2023-07-11 19:10:28,563][233954] Min Reward on eval: -112.85408007864422[0m
[37m[1m[2023-07-11 19:10:28,563][233954] Mean Reward across all agents: -112.85408007864422[0m
[37m[1m[2023-07-11 19:10:28,563][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:10:33,583][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:10:33,583][233954] Reward + Measures: [[-36.21355428   0.92699999   0.78680003   0.92860001   0.0016
    0.91906244]
 [ 35.70753639   0.85820001   0.79170007   0.72500002   0.62180007
    1.08038032]
 [-17.44833792   0.96600002   0.76069993   0.8646       0.30329999
    0.92454445]
 ...
 [-13.73760292   0.93330002   0.73729998   0.8023001    0.35050005
    0.98554671]
 [ 17.75270498   0.87260002   0.88490003   0.64399999   0.71490002
    1.02929306]
 [-37.60215877   0.9066       0.7489       0.89899999   0.0842
    0.92180502]][0m
[37m[1m[2023-07-11 19:10:33,584][233954] Max Reward on eval: 89.51720046862029[0m
[37m[1m[2023-07-11 19:10:33,584][233954] Min Reward on eval: -113.81271744985133[0m
[37m[1m[2023-07-11 19:10:33,584][233954] Mean Reward across all agents: 4.941429428226208[0m
[37m[1m[2023-07-11 19:10:33,584][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:10:33,587][233954] mean_value=-40.56249643406615, max_value=468.2166220429633[0m
[37m[1m[2023-07-11 19:10:33,590][233954] New mean coefficients: [[-5.686344   1.8077798  6.236517  -5.6344585  2.6589708 -3.8413062]][0m
[37m[1m[2023-07-11 19:10:33,591][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:10:42,644][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 19:10:42,644][233954] FPS: 424238.32[0m
[36m[2023-07-11 19:10:42,647][233954] itr=1383, itrs=2000, Progress: 69.15%[0m
[36m[2023-07-11 19:10:54,238][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 19:10:54,238][233954] FPS: 334618.95[0m
[36m[2023-07-11 19:10:58,467][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:10:58,468][233954] Reward + Measures: [[-111.66720474    0.96260399    0.70543766    0.87772369    0.00214967
     0.86337709]][0m
[37m[1m[2023-07-11 19:10:58,468][233954] Max Reward on eval: -111.66720474235201[0m
[37m[1m[2023-07-11 19:10:58,468][233954] Min Reward on eval: -111.66720474235201[0m
[37m[1m[2023-07-11 19:10:58,468][233954] Mean Reward across all agents: -111.66720474235201[0m
[37m[1m[2023-07-11 19:10:58,469][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:11:03,432][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:11:03,433][233954] Reward + Measures: [[-71.46539194   0.80650008   0.58599997   0.72479999   0.0956
    1.1283474 ]
 [ -4.0200565    0.90070003   0.66079998   0.83339995   0.0531
    0.93161356]
 [-11.64437901   0.89420003   0.634        0.70840001   0.271
    0.90343601]
 ...
 [-31.53771174   0.89469999   0.72110003   0.7105       0.4973
    0.92652273]
 [ -5.40046682   0.85039997   0.61110002   0.6505       0.13759999
    1.02073383]
 [-35.97569876   0.45420003   0.419        0.3563       0.2703
    1.40717411]][0m
[37m[1m[2023-07-11 19:11:03,433][233954] Max Reward on eval: 89.41066992133855[0m
[37m[1m[2023-07-11 19:11:03,433][233954] Min Reward on eval: -156.6636061420664[0m
[37m[1m[2023-07-11 19:11:03,434][233954] Mean Reward across all agents: -39.62359722016011[0m
[37m[1m[2023-07-11 19:11:03,434][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:11:03,436][233954] mean_value=-150.6201250396258, max_value=355.04020804790525[0m
[37m[1m[2023-07-11 19:11:03,438][233954] New mean coefficients: [[-4.6877947  2.0849307  6.4352193 -4.9218984  5.0078373 -3.9726422]][0m
[37m[1m[2023-07-11 19:11:03,439][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:11:12,421][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 19:11:12,427][233954] FPS: 427590.34[0m
[36m[2023-07-11 19:11:12,430][233954] itr=1384, itrs=2000, Progress: 69.20%[0m
[36m[2023-07-11 19:11:24,036][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 19:11:24,036][233954] FPS: 334159.99[0m
[36m[2023-07-11 19:11:28,244][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:11:28,244][233954] Reward + Measures: [[-100.75992583    0.97181296    0.76971823    0.90833497    0.001248
     0.84296823]][0m
[37m[1m[2023-07-11 19:11:28,244][233954] Max Reward on eval: -100.75992582913794[0m
[37m[1m[2023-07-11 19:11:28,244][233954] Min Reward on eval: -100.75992582913794[0m
[37m[1m[2023-07-11 19:11:28,245][233954] Mean Reward across all agents: -100.75992582913794[0m
[37m[1m[2023-07-11 19:11:28,245][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:11:33,171][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:11:33,171][233954] Reward + Measures: [[ 36.26012481   0.77060002   0.54700005   0.70690006   0.0579
    1.05265105]
 [-16.3765049    0.94880009   0.81730002   0.89730006   0.2289
    1.00040209]
 [ 33.27619419   0.97310001   0.88300002   0.89270002   0.34390002
    0.93447542]
 ...
 [ 47.07759481   0.92380011   0.91510004   0.81199998   0.65420002
    0.95193893]
 [-36.9439276    0.85179996   0.76620001   0.77579999   0.2297
    1.07400501]
 [-16.72926491   0.93880004   0.80050004   0.85319996   0.29029998
    0.9596464 ]][0m
[37m[1m[2023-07-11 19:11:33,171][233954] Max Reward on eval: 81.86608508175705[0m
[37m[1m[2023-07-11 19:11:33,172][233954] Min Reward on eval: -113.60318183195777[0m
[37m[1m[2023-07-11 19:11:33,172][233954] Mean Reward across all agents: -12.963733632641354[0m
[37m[1m[2023-07-11 19:11:33,172][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:11:33,175][233954] mean_value=-63.16406719738559, max_value=509.02804385395723[0m
[37m[1m[2023-07-11 19:11:33,178][233954] New mean coefficients: [[-5.1996346  3.3303542  7.087641  -3.9662464  3.8030572 -4.837159 ]][0m
[37m[1m[2023-07-11 19:11:33,178][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:11:42,079][233954] train() took 8.90 seconds to complete[0m
[36m[2023-07-11 19:11:42,080][233954] FPS: 431493.37[0m
[36m[2023-07-11 19:11:42,082][233954] itr=1385, itrs=2000, Progress: 69.25%[0m
[36m[2023-07-11 19:11:53,680][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 19:11:53,680][233954] FPS: 334401.93[0m
[36m[2023-07-11 19:11:58,049][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:11:58,049][233954] Reward + Measures: [[56.43026324  0.98297298  0.75271368  0.97719693  0.10479166  1.22378087]][0m
[37m[1m[2023-07-11 19:11:58,050][233954] Max Reward on eval: 56.43026323771183[0m
[37m[1m[2023-07-11 19:11:58,050][233954] Min Reward on eval: 56.43026323771183[0m
[37m[1m[2023-07-11 19:11:58,050][233954] Mean Reward across all agents: 56.43026323771183[0m
[37m[1m[2023-07-11 19:11:58,050][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:12:03,053][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:12:03,053][233954] Reward + Measures: [[  8.7942834    0.29080001   0.1894       0.2784       0.066
    1.6733551 ]
 [-27.3664487    0.45380002   0.35260001   0.42360002   0.13010001
    2.44994426]
 [  4.81856067   0.27759999   0.23800002   0.32430002   0.1946
    2.48487449]
 ...
 [-16.92375251   0.29749998   0.29260001   0.30240002   0.2393
    2.03503299]
 [-59.8892374    0.3849       0.28620002   0.33810002   0.13250001
    2.0105989 ]
 [ 61.8454459    0.40900001   0.17820001   0.54799998   0.20460001
    2.25651646]][0m
[37m[1m[2023-07-11 19:12:03,053][233954] Max Reward on eval: 342.21186638777147[0m
[37m[1m[2023-07-11 19:12:03,054][233954] Min Reward on eval: -109.27521859444678[0m
[37m[1m[2023-07-11 19:12:03,054][233954] Mean Reward across all agents: 13.12329173256794[0m
[37m[1m[2023-07-11 19:12:03,054][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:12:03,057][233954] mean_value=-220.33761535048612, max_value=620.1542643448338[0m
[37m[1m[2023-07-11 19:12:03,060][233954] New mean coefficients: [[-3.3427591  3.166793   6.5720644 -3.01527    4.1183186 -4.5183   ]][0m
[37m[1m[2023-07-11 19:12:03,061][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:12:11,995][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 19:12:12,001][233954] FPS: 429861.40[0m
[36m[2023-07-11 19:12:12,004][233954] itr=1386, itrs=2000, Progress: 69.30%[0m
[36m[2023-07-11 19:12:23,695][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 19:12:23,695][233954] FPS: 331619.26[0m
[36m[2023-07-11 19:12:27,935][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:12:27,935][233954] Reward + Measures: [[-36.56298798   0.99091959   0.00329567   0.99375391   0.81902069
    1.00921977]][0m
[37m[1m[2023-07-11 19:12:27,936][233954] Max Reward on eval: -36.562987975630975[0m
[37m[1m[2023-07-11 19:12:27,936][233954] Min Reward on eval: -36.562987975630975[0m
[37m[1m[2023-07-11 19:12:27,936][233954] Mean Reward across all agents: -36.562987975630975[0m
[37m[1m[2023-07-11 19:12:27,936][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:12:32,918][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:12:32,919][233954] Reward + Measures: [[-46.32549048   0.37610003   0.27710003   0.34650001   0.24320002
    2.61513114]
 [ -2.48964852   0.48139998   0.32740003   0.48680001   0.27860004
    2.57111835]
 [-51.34449535   0.47919998   0.22309999   0.46030003   0.19420001
    2.09034657]
 ...
 [ 49.54099569   0.33420002   0.2836       0.19240001   0.2383
    2.00689697]
 [-19.03624967   0.42079997   0.24240001   0.3576       0.19039999
    2.20900202]
 [-51.83594756   0.5618       0.29360002   0.58850002   0.17450002
    2.47954535]][0m
[37m[1m[2023-07-11 19:12:32,919][233954] Max Reward on eval: 288.61196067379785[0m
[37m[1m[2023-07-11 19:12:32,919][233954] Min Reward on eval: -538.7759361370001[0m
[37m[1m[2023-07-11 19:12:32,920][233954] Mean Reward across all agents: 39.651228412266114[0m
[37m[1m[2023-07-11 19:12:32,920][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:12:32,926][233954] mean_value=-130.72961832442434, max_value=682.4091116946004[0m
[37m[1m[2023-07-11 19:12:32,929][233954] New mean coefficients: [[-3.6512496  4.243106   6.7595816 -0.4722488  3.9872882 -4.959961 ]][0m
[37m[1m[2023-07-11 19:12:32,930][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:12:41,870][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 19:12:41,870][233954] FPS: 429612.85[0m
[36m[2023-07-11 19:12:41,872][233954] itr=1387, itrs=2000, Progress: 69.35%[0m
[36m[2023-07-11 19:12:53,643][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 19:12:53,643][233954] FPS: 329432.18[0m
[36m[2023-07-11 19:12:57,928][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:12:57,934][233954] Reward + Measures: [[-172.97802066    0.86322403    0.19204634    0.888282      0.60407269
     1.08645225]][0m
[37m[1m[2023-07-11 19:12:57,934][233954] Max Reward on eval: -172.97802065848995[0m
[37m[1m[2023-07-11 19:12:57,935][233954] Min Reward on eval: -172.97802065848995[0m
[37m[1m[2023-07-11 19:12:57,935][233954] Mean Reward across all agents: -172.97802065848995[0m
[37m[1m[2023-07-11 19:12:57,935][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:13:02,886][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:13:02,887][233954] Reward + Measures: [[ -36.6116693     0.61619997    0.1865        0.65570003    0.28010002
     2.0860486 ]
 [ -68.82267448    0.45650002    0.39459997    0.50630009    0.44679999
     2.07083321]
 [  33.72421195    0.30070001    0.40869999    0.32930002    0.42260003
     1.84741998]
 ...
 [-137.96092129    0.61490005    0.1384        0.67110008    0.30890003
     2.25891185]
 [-146.27203931    0.55960006    0.1043        0.60170001    0.34950003
     2.99798369]
 [ -19.71095285    0.25639999    0.42910001    0.25139999    0.45310003
     2.13736844]][0m
[37m[1m[2023-07-11 19:13:02,887][233954] Max Reward on eval: 87.74777358053252[0m
[37m[1m[2023-07-11 19:13:02,887][233954] Min Reward on eval: -616.0657496483997[0m
[37m[1m[2023-07-11 19:13:02,888][233954] Mean Reward across all agents: -100.62090800585244[0m
[37m[1m[2023-07-11 19:13:02,888][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:13:02,891][233954] mean_value=-208.1754226864105, max_value=396.1753091071993[0m
[37m[1m[2023-07-11 19:13:02,894][233954] New mean coefficients: [[-4.698245   5.2573204  5.883046   1.9568517  3.0650501 -4.72597  ]][0m
[37m[1m[2023-07-11 19:13:02,895][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:13:11,968][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 19:13:11,968][233954] FPS: 423326.59[0m
[36m[2023-07-11 19:13:11,970][233954] itr=1388, itrs=2000, Progress: 69.40%[0m
[36m[2023-07-11 19:13:23,744][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 19:13:23,744][233954] FPS: 329281.85[0m
[36m[2023-07-11 19:13:28,062][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:13:28,062][233954] Reward + Measures: [[133.17581582   0.99601001   0.95254034   0.98822397   0.00293833
    1.33599329]][0m
[37m[1m[2023-07-11 19:13:28,062][233954] Max Reward on eval: 133.1758158170509[0m
[37m[1m[2023-07-11 19:13:28,063][233954] Min Reward on eval: 133.1758158170509[0m
[37m[1m[2023-07-11 19:13:28,063][233954] Mean Reward across all agents: 133.1758158170509[0m
[37m[1m[2023-07-11 19:13:28,063][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:13:33,057][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:13:33,057][233954] Reward + Measures: [[210.54863167   0.75390005   0.43870002   0.76610005   0.07130001
    1.62740231]
 [226.5473013    0.57990003   0.32069999   0.51260006   0.1248
    1.81455231]
 [272.08872031   0.65179998   0.36320001   0.55549997   0.15640001
    1.91586292]
 ...
 [187.46623802   0.68720001   0.35479999   0.56639999   0.16509999
    2.00773883]
 [227.74339271   0.54510003   0.35010001   0.47329998   0.16150001
    1.78873718]
 [239.5712557    0.51980001   0.2969       0.44239998   0.1488
    2.42012262]][0m
[37m[1m[2023-07-11 19:13:33,057][233954] Max Reward on eval: 325.31975365509277[0m
[37m[1m[2023-07-11 19:13:33,058][233954] Min Reward on eval: -296.31624222453684[0m
[37m[1m[2023-07-11 19:13:33,058][233954] Mean Reward across all agents: 183.09699844572015[0m
[37m[1m[2023-07-11 19:13:33,058][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:13:33,068][233954] mean_value=120.22033699957721, max_value=670.6416244538501[0m
[37m[1m[2023-07-11 19:13:33,071][233954] New mean coefficients: [[-5.3419623  5.12506    6.3130374  2.2854455  2.772546  -4.497423 ]][0m
[37m[1m[2023-07-11 19:13:33,072][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:13:41,946][233954] train() took 8.87 seconds to complete[0m
[36m[2023-07-11 19:13:41,946][233954] FPS: 432780.56[0m
[36m[2023-07-11 19:13:41,949][233954] itr=1389, itrs=2000, Progress: 69.45%[0m
[36m[2023-07-11 19:13:53,529][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 19:13:53,529][233954] FPS: 334827.72[0m
[36m[2023-07-11 19:13:57,827][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:13:57,827][233954] Reward + Measures: [[1.20601892 0.92401624 0.78837103 0.93183661 0.76156396 1.26230466]][0m
[37m[1m[2023-07-11 19:13:57,827][233954] Max Reward on eval: 1.206018921372635[0m
[37m[1m[2023-07-11 19:13:57,828][233954] Min Reward on eval: 1.206018921372635[0m
[37m[1m[2023-07-11 19:13:57,828][233954] Mean Reward across all agents: 1.206018921372635[0m
[37m[1m[2023-07-11 19:13:57,828][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:14:03,028][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:14:03,029][233954] Reward + Measures: [[ 36.75270351   0.21940003   0.40439996   0.09960001   0.37900001
    2.36380243]
 [ 71.46094056   0.2694       0.44159999   0.0958       0.41440001
    1.97295856]
 [ 31.43510164   0.1512       0.16580001   0.1074       0.1331
    2.39647293]
 ...
 [ 91.59700981   0.21279998   0.26879999   0.1313       0.19250001
    2.18824649]
 [ 37.30769376   0.26400003   0.44830003   0.08230001   0.41170001
    2.29426098]
 [121.97786317   0.1893       0.25580001   0.1221       0.20240001
    2.49826813]][0m
[37m[1m[2023-07-11 19:14:03,029][233954] Max Reward on eval: 295.68295099409295[0m
[37m[1m[2023-07-11 19:14:03,030][233954] Min Reward on eval: -238.58016347731464[0m
[37m[1m[2023-07-11 19:14:03,030][233954] Mean Reward across all agents: 74.42244928902127[0m
[37m[1m[2023-07-11 19:14:03,030][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:14:03,033][233954] mean_value=-356.7831912359567, max_value=295.9339777024202[0m
[37m[1m[2023-07-11 19:14:03,035][233954] New mean coefficients: [[-3.132115   4.6063714  6.145266   1.8840172  2.6065443 -4.4994607]][0m
[37m[1m[2023-07-11 19:14:03,036][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:14:11,932][233954] train() took 8.89 seconds to complete[0m
[36m[2023-07-11 19:14:11,932][233954] FPS: 431755.12[0m
[36m[2023-07-11 19:14:11,934][233954] itr=1390, itrs=2000, Progress: 69.50%[0m
[37m[1m[2023-07-11 19:18:22,673][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001370[0m
[36m[2023-07-11 19:18:37,007][233954] train() took 11.92 seconds to complete[0m
[36m[2023-07-11 19:18:37,008][233954] FPS: 322163.04[0m
[36m[2023-07-11 19:18:41,209][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:18:41,209][233954] Reward + Measures: [[-13.96526957   0.92918992   0.92408127   0.71965599   0.79031867
    1.3030386 ]][0m
[37m[1m[2023-07-11 19:18:41,209][233954] Max Reward on eval: -13.965269570956943[0m
[37m[1m[2023-07-11 19:18:41,210][233954] Min Reward on eval: -13.965269570956943[0m
[37m[1m[2023-07-11 19:18:41,210][233954] Mean Reward across all agents: -13.965269570956943[0m
[37m[1m[2023-07-11 19:18:41,210][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:18:46,182][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:18:46,183][233954] Reward + Measures: [[ 36.05600239   0.9898001    0.99200004   0.97559994   0.98180002
    1.72654176]
 [  0.6185354    0.90059996   0.97579998   0.69840002   0.94430012
    1.62600422]
 [ 55.67855263   0.99270004   0.99529999   0.99069995   0.9896
    1.46347976]
 ...
 [-46.37915875   0.72910005   0.85720009   0.61190003   0.87740004
    1.12676501]
 [-68.73391865   0.58939999   0.70520002   0.38070002   0.62980002
    1.23825788]
 [ 29.64900441   0.82069999   0.91350001   0.53079998   0.86219996
    1.99820292]][0m
[37m[1m[2023-07-11 19:18:46,183][233954] Max Reward on eval: 127.80493637206965[0m
[37m[1m[2023-07-11 19:18:46,183][233954] Min Reward on eval: -215.83833123450168[0m
[37m[1m[2023-07-11 19:18:46,184][233954] Mean Reward across all agents: -1.8126459055385395[0m
[37m[1m[2023-07-11 19:18:46,184][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:18:46,195][233954] mean_value=-45.92077254231275, max_value=476.0923971089162[0m
[37m[1m[2023-07-11 19:18:46,199][233954] New mean coefficients: [[-2.6075428  4.702805   6.415189   1.1676629  3.3679407 -4.8126287]][0m
[37m[1m[2023-07-11 19:18:46,200][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:18:55,120][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 19:18:55,121][233954] FPS: 430553.60[0m
[36m[2023-07-11 19:18:55,123][233954] itr=1391, itrs=2000, Progress: 69.55%[0m
[36m[2023-07-11 19:19:06,757][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 19:19:06,757][233954] FPS: 333304.51[0m
[36m[2023-07-11 19:19:10,948][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:19:10,949][233954] Reward + Measures: [[-294.55422391    0.94375163    0.97159731    0.00695667    0.93698967
     1.47736549]][0m
[37m[1m[2023-07-11 19:19:10,949][233954] Max Reward on eval: -294.5542239079712[0m
[37m[1m[2023-07-11 19:19:10,949][233954] Min Reward on eval: -294.5542239079712[0m
[37m[1m[2023-07-11 19:19:10,949][233954] Mean Reward across all agents: -294.5542239079712[0m
[37m[1m[2023-07-11 19:19:10,950][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:19:15,900][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:19:15,900][233954] Reward + Measures: [[ 112.71884679    0.63430005    0.69040006    0.42030001    0.40839997
     1.16797566]
 [ 310.11461737    0.51010001    0.41759998    0.50030005    0.1631
     1.53902197]
 [-170.40062045    0.82870007    0.93449992    0.0184        0.82380003
     1.30673087]
 ...
 [ 284.61863329    0.49489999    0.43800002    0.40120003    0.1585
     1.63785541]
 [ 270.48289113    0.50529999    0.41620001    0.47129998    0.19600001
     1.59189641]
 [  66.32248259    0.62239999    0.53590006    0.6027        0.0748
     1.56518781]][0m
[37m[1m[2023-07-11 19:19:15,901][233954] Max Reward on eval: 400.28183745536955[0m
[37m[1m[2023-07-11 19:19:15,901][233954] Min Reward on eval: -383.1328735552728[0m
[37m[1m[2023-07-11 19:19:15,901][233954] Mean Reward across all agents: 50.870418697742544[0m
[37m[1m[2023-07-11 19:19:15,901][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:19:15,909][233954] mean_value=-17.660919366372607, max_value=663.0981676075048[0m
[37m[1m[2023-07-11 19:19:15,912][233954] New mean coefficients: [[-2.000043   5.2410803  6.3947754  2.01945    3.192309  -5.2144885]][0m
[37m[1m[2023-07-11 19:19:15,913][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:19:24,938][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 19:19:24,939][233954] FPS: 425557.27[0m
[36m[2023-07-11 19:19:24,941][233954] itr=1392, itrs=2000, Progress: 69.60%[0m
[36m[2023-07-11 19:19:36,651][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 19:19:36,652][233954] FPS: 331218.22[0m
[36m[2023-07-11 19:19:40,857][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:19:40,857][233954] Reward + Measures: [[56.73147965  0.80016136  0.747132    0.5911997   0.44638565  0.88792676]][0m
[37m[1m[2023-07-11 19:19:40,858][233954] Max Reward on eval: 56.731479646778794[0m
[37m[1m[2023-07-11 19:19:40,858][233954] Min Reward on eval: 56.731479646778794[0m
[37m[1m[2023-07-11 19:19:40,858][233954] Mean Reward across all agents: 56.731479646778794[0m
[37m[1m[2023-07-11 19:19:40,858][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:19:45,876][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:19:45,882][233954] Reward + Measures: [[ 6.11787068  0.86800003  0.66259998  0.71360004  0.3046      1.01981795]
 [ 6.55612521  0.88519996  0.75209999  0.82139999  0.48800001  0.85930347]
 [ 3.0965787   0.90640002  0.70690006  0.81619996  0.3348      0.93590039]
 ...
 [21.55460968  0.8179      0.62670004  0.75940001  0.27339998  0.92150658]
 [ 2.52132197  0.80389994  0.71890002  0.59210008  0.58610004  0.81979066]
 [13.35025134  0.90280002  0.76380008  0.79950005  0.47810003  0.96779728]][0m
[37m[1m[2023-07-11 19:19:45,882][233954] Max Reward on eval: 123.64098500199616[0m
[37m[1m[2023-07-11 19:19:45,882][233954] Min Reward on eval: -37.596745002968234[0m
[37m[1m[2023-07-11 19:19:45,883][233954] Mean Reward across all agents: 12.599855923211338[0m
[37m[1m[2023-07-11 19:19:45,883][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:19:45,886][233954] mean_value=-24.131405895323372, max_value=399.32415270018765[0m
[37m[1m[2023-07-11 19:19:45,889][233954] New mean coefficients: [[-0.80755496  4.2245197   5.9202623   0.98162544  2.0924282  -5.01952   ]][0m
[37m[1m[2023-07-11 19:19:45,890][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:19:54,862][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 19:19:54,862][233954] FPS: 428050.91[0m
[36m[2023-07-11 19:19:54,865][233954] itr=1393, itrs=2000, Progress: 69.65%[0m
[36m[2023-07-11 19:20:06,435][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 19:20:06,435][233954] FPS: 335241.48[0m
[36m[2023-07-11 19:20:10,686][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:20:10,687][233954] Reward + Measures: [[16.63498106  0.73280734  0.86948901  0.26902434  0.71360731  0.95954704]][0m
[37m[1m[2023-07-11 19:20:10,687][233954] Max Reward on eval: 16.634981056681944[0m
[37m[1m[2023-07-11 19:20:10,687][233954] Min Reward on eval: 16.634981056681944[0m
[37m[1m[2023-07-11 19:20:10,688][233954] Mean Reward across all agents: 16.634981056681944[0m
[37m[1m[2023-07-11 19:20:10,688][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:20:15,672][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:20:15,673][233954] Reward + Measures: [[-22.97882094   0.92070001   0.70969999   0.78330004   0.08840001
    0.87090933]
 [ 15.18776322   0.9436       0.68879998   0.81820011   0.0034
    0.88903373]
 [ 15.0785618    0.71259999   0.89099997   0.1952       0.72049999
    0.79100001]
 ...
 [  7.528426     0.92620003   0.70690006   0.81259996   0.0564
    0.96836185]
 [ -2.47762632   0.72830003   0.76080006   0.40740004   0.56760001
    0.80385631]
 [  0.09249618   0.78520006   0.79610008   0.49090004   0.49039999
    0.84787637]][0m
[37m[1m[2023-07-11 19:20:15,673][233954] Max Reward on eval: 110.32787320963106[0m
[37m[1m[2023-07-11 19:20:15,673][233954] Min Reward on eval: -40.48548676011851[0m
[37m[1m[2023-07-11 19:20:15,674][233954] Mean Reward across all agents: 9.779940416013927[0m
[37m[1m[2023-07-11 19:20:15,674][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:20:15,679][233954] mean_value=-4.6785073297495305, max_value=528.8969776292331[0m
[37m[1m[2023-07-11 19:20:15,681][233954] New mean coefficients: [[ 0.35868335  3.80778     5.53634    -1.0062746   2.9456582  -5.252822  ]][0m
[37m[1m[2023-07-11 19:20:15,682][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:20:24,647][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 19:20:24,647][233954] FPS: 428424.95[0m
[36m[2023-07-11 19:20:24,650][233954] itr=1394, itrs=2000, Progress: 69.70%[0m
[36m[2023-07-11 19:20:36,339][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 19:20:36,339][233954] FPS: 331764.92[0m
[36m[2023-07-11 19:20:40,608][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:20:40,608][233954] Reward + Measures: [[13.54250883  0.76165301  0.92321759  0.20495333  0.8027553   0.95229578]][0m
[37m[1m[2023-07-11 19:20:40,609][233954] Max Reward on eval: 13.542508833586202[0m
[37m[1m[2023-07-11 19:20:40,609][233954] Min Reward on eval: 13.542508833586202[0m
[37m[1m[2023-07-11 19:20:40,609][233954] Mean Reward across all agents: 13.542508833586202[0m
[37m[1m[2023-07-11 19:20:40,609][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:20:45,674][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:20:45,681][233954] Reward + Measures: [[34.91735556  0.8222      0.9325      0.44949999  0.81230003  0.86644709]
 [ 3.84734492  0.97730011  0.99230003  0.0027      0.99250001  1.24961364]
 [28.223171    0.88249999  0.89860004  0.63350004  0.70520002  0.8706705 ]
 ...
 [ 5.41620404  0.95539999  0.98800004  0.007       0.98730004  1.15999973]
 [ 7.41526208  0.87690002  0.90609998  0.60500002  0.74599999  0.80828267]
 [23.38589313  0.88339996  0.98220009  0.48520002  0.91440004  1.01493931]][0m
[37m[1m[2023-07-11 19:20:45,681][233954] Max Reward on eval: 96.37109635611996[0m
[37m[1m[2023-07-11 19:20:45,682][233954] Min Reward on eval: -14.365668493555859[0m
[37m[1m[2023-07-11 19:20:45,682][233954] Mean Reward across all agents: 15.729400182058411[0m
[37m[1m[2023-07-11 19:20:45,683][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:20:45,693][233954] mean_value=72.70709712316595, max_value=401.8681161344957[0m
[37m[1m[2023-07-11 19:20:45,697][233954] New mean coefficients: [[-0.5964628   4.2765284   5.2428255   0.15437198  1.2228613  -5.4607825 ]][0m
[37m[1m[2023-07-11 19:20:45,699][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:20:54,790][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 19:20:54,790][233954] FPS: 422464.37[0m
[36m[2023-07-11 19:20:54,792][233954] itr=1395, itrs=2000, Progress: 69.75%[0m
[36m[2023-07-11 19:21:06,675][233954] train() took 11.77 seconds to complete[0m
[36m[2023-07-11 19:21:06,675][233954] FPS: 326306.73[0m
[36m[2023-07-11 19:21:10,958][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:21:10,958][233954] Reward + Measures: [[-142.10909913    0.88676369    0.96056265    0.01476       0.89747071
     0.99077797]][0m
[37m[1m[2023-07-11 19:21:10,958][233954] Max Reward on eval: -142.1090991263392[0m
[37m[1m[2023-07-11 19:21:10,958][233954] Min Reward on eval: -142.1090991263392[0m
[37m[1m[2023-07-11 19:21:10,959][233954] Mean Reward across all agents: -142.1090991263392[0m
[37m[1m[2023-07-11 19:21:10,959][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:21:15,909][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:21:15,914][233954] Reward + Measures: [[163.71363451   0.5596       0.80760002   0.0202       0.72920001
    1.74531305]
 [119.00647068   0.63120002   0.9370001    0.0017       0.83030003
    1.36206281]
 [ 84.94372464   0.72410005   0.9677       0.0001       0.85059994
    1.41592729]
 ...
 [423.78264381   0.97730011   0.92480004   0.95839995   0.0013
    2.45882845]
 [246.03391266   0.43710002   0.64289999   0.0737       0.53640002
    1.79090333]
 [ 45.88145798   0.78820002   0.96509999   0.0004       0.84780008
    1.08254075]][0m
[37m[1m[2023-07-11 19:21:15,915][233954] Max Reward on eval: 612.3269767319783[0m
[37m[1m[2023-07-11 19:21:15,915][233954] Min Reward on eval: -290.27303694570435[0m
[37m[1m[2023-07-11 19:21:15,915][233954] Mean Reward across all agents: 92.61690248384873[0m
[37m[1m[2023-07-11 19:21:15,915][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:21:15,921][233954] mean_value=-13.338118974555218, max_value=636.399092573505[0m
[37m[1m[2023-07-11 19:21:15,924][233954] New mean coefficients: [[-0.38250923  3.7430806   5.165404   -0.5008365   1.0381929  -5.614092  ]][0m
[37m[1m[2023-07-11 19:21:15,925][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:21:24,858][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 19:21:24,858][233954] FPS: 429948.83[0m
[36m[2023-07-11 19:21:24,860][233954] itr=1396, itrs=2000, Progress: 69.80%[0m
[36m[2023-07-11 19:21:36,495][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 19:21:36,495][233954] FPS: 333298.11[0m
[36m[2023-07-11 19:21:40,706][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:21:40,706][233954] Reward + Measures: [[-341.41240425    0.96960527    0.99569333    0.001956      0.98722631
     0.98528528]][0m
[37m[1m[2023-07-11 19:21:40,706][233954] Max Reward on eval: -341.412404245941[0m
[37m[1m[2023-07-11 19:21:40,706][233954] Min Reward on eval: -341.412404245941[0m
[37m[1m[2023-07-11 19:21:40,707][233954] Mean Reward across all agents: -341.412404245941[0m
[37m[1m[2023-07-11 19:21:40,707][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:21:45,748][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:21:45,754][233954] Reward + Measures: [[ -26.53155492    0.73950005    0.93810004    0.0771        0.89000005
     0.88781327]
 [ -58.16644       0.78850001    0.93450004    0.0087        0.90109998
     0.91042894]
 [-104.14221477    0.60359997    0.95270008    0.004         0.90760005
     0.91213578]
 ...
 [ -49.73701118    0.90240002    0.95760006    0.0081        0.92799997
     0.8667559 ]
 [ -93.59786702    0.78430003    0.95649999    0.0028        0.95230001
     0.98238844]
 [  96.38216115    0.67480004    0.9066        0.31030002    0.76310003
     0.7765643 ]][0m
[37m[1m[2023-07-11 19:21:45,754][233954] Max Reward on eval: 126.83236122142989[0m
[37m[1m[2023-07-11 19:21:45,755][233954] Min Reward on eval: -350.4643840927165[0m
[37m[1m[2023-07-11 19:21:45,755][233954] Mean Reward across all agents: -91.63141799029974[0m
[37m[1m[2023-07-11 19:21:45,755][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:21:45,758][233954] mean_value=-145.64012187718183, max_value=499.0183020281605[0m
[37m[1m[2023-07-11 19:21:45,761][233954] New mean coefficients: [[-2.135807    5.4283314   6.0033617   2.054163    0.04216021 -7.0218153 ]][0m
[37m[1m[2023-07-11 19:21:45,762][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:21:54,745][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 19:21:54,745][233954] FPS: 427533.52[0m
[36m[2023-07-11 19:21:54,748][233954] itr=1397, itrs=2000, Progress: 69.85%[0m
[36m[2023-07-11 19:22:06,437][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 19:22:06,437][233954] FPS: 331727.36[0m
[36m[2023-07-11 19:22:10,683][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:22:10,683][233954] Reward + Measures: [[-50.3577524    0.78329265   0.95946693   0.80349106   0.92977571
    1.15006936]][0m
[37m[1m[2023-07-11 19:22:10,684][233954] Max Reward on eval: -50.35775240322804[0m
[37m[1m[2023-07-11 19:22:10,684][233954] Min Reward on eval: -50.35775240322804[0m
[37m[1m[2023-07-11 19:22:10,684][233954] Mean Reward across all agents: -50.35775240322804[0m
[37m[1m[2023-07-11 19:22:10,684][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:22:15,621][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:22:15,622][233954] Reward + Measures: [[190.49886508   0.37480003   0.48569998   0.2455       0.32440001
    1.88886666]
 [258.67775153   0.71439999   0.86049998   0.81910002   0.15820001
    1.77351475]
 [ 79.56837113   0.56879997   0.70430005   0.0958       0.69300002
    1.28629386]
 ...
 [ 59.09713944   0.56519997   0.88150007   0.28389999   0.73629999
    1.1246742 ]
 [163.87752482   0.36410001   0.50739998   0.2339       0.33530003
    1.65355957]
 [-48.84965925   0.70600003   0.96560001   0.67869997   0.91180003
    1.11291754]][0m
[37m[1m[2023-07-11 19:22:15,622][233954] Max Reward on eval: 539.7893180938438[0m
[37m[1m[2023-07-11 19:22:15,622][233954] Min Reward on eval: -150.8322849274613[0m
[37m[1m[2023-07-11 19:22:15,623][233954] Mean Reward across all agents: 85.77123141576199[0m
[37m[1m[2023-07-11 19:22:15,623][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:22:15,631][233954] mean_value=-52.38697011564046, max_value=724.9895296337083[0m
[37m[1m[2023-07-11 19:22:15,634][233954] New mean coefficients: [[-2.0291278  5.449659   5.4679165  4.1491337 -0.5970285 -6.901321 ]][0m
[37m[1m[2023-07-11 19:22:15,635][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:22:24,547][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 19:22:24,547][233954] FPS: 430941.49[0m
[36m[2023-07-11 19:22:24,550][233954] itr=1398, itrs=2000, Progress: 69.90%[0m
[36m[2023-07-11 19:22:36,338][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 19:22:36,338][233954] FPS: 328956.49[0m
[36m[2023-07-11 19:22:40,620][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:22:40,620][233954] Reward + Measures: [[45.34463171  0.68132734  0.93569863  0.5454433   0.63569361  1.25624025]][0m
[37m[1m[2023-07-11 19:22:40,620][233954] Max Reward on eval: 45.344631713358844[0m
[37m[1m[2023-07-11 19:22:40,621][233954] Min Reward on eval: 45.344631713358844[0m
[37m[1m[2023-07-11 19:22:40,621][233954] Mean Reward across all agents: 45.344631713358844[0m
[37m[1m[2023-07-11 19:22:40,621][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:22:45,587][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:22:45,587][233954] Reward + Measures: [[ 49.36032159   0.62540001   0.92189997   0.59380001   0.61659998
    1.27509797]
 [ 48.33465159   0.40200001   0.57600003   0.39809999   0.3691
    1.36192155]
 [ 17.45674519   0.34990001   0.50239998   0.3646       0.35790002
    1.50621831]
 ...
 [157.72899815   0.63500005   0.82480001   0.56309998   0.59230006
    1.13536131]
 [-28.89437962   0.50019997   0.79479998   0.33059999   0.67270005
    1.36959839]
 [106.66811037   0.93110001   0.97920001   0.69480002   0.90640002
    1.1576966 ]][0m
[37m[1m[2023-07-11 19:22:45,587][233954] Max Reward on eval: 179.9030408654362[0m
[37m[1m[2023-07-11 19:22:45,588][233954] Min Reward on eval: -95.42808939246461[0m
[37m[1m[2023-07-11 19:22:45,588][233954] Mean Reward across all agents: 19.06937187670553[0m
[37m[1m[2023-07-11 19:22:45,588][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:22:45,594][233954] mean_value=-87.37850302396399, max_value=610.2249321309879[0m
[37m[1m[2023-07-11 19:22:45,597][233954] New mean coefficients: [[-0.9961442   4.092022    5.4114523   1.0726478   0.37626666 -6.329421  ]][0m
[37m[1m[2023-07-11 19:22:45,598][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:22:54,549][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 19:22:54,549][233954] FPS: 429087.38[0m
[36m[2023-07-11 19:22:54,551][233954] itr=1399, itrs=2000, Progress: 69.95%[0m
[36m[2023-07-11 19:23:06,255][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 19:23:06,256][233954] FPS: 331241.08[0m
[36m[2023-07-11 19:23:10,523][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:23:10,523][233954] Reward + Measures: [[-7.3640774   0.99053496  0.80583566  0.98325127  0.041341    0.92900395]][0m
[37m[1m[2023-07-11 19:23:10,524][233954] Max Reward on eval: -7.364077403160052[0m
[37m[1m[2023-07-11 19:23:10,524][233954] Min Reward on eval: -7.364077403160052[0m
[37m[1m[2023-07-11 19:23:10,524][233954] Mean Reward across all agents: -7.364077403160052[0m
[37m[1m[2023-07-11 19:23:10,524][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:23:15,802][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:23:15,808][233954] Reward + Measures: [[ -9.54455429   0.77109998   0.6189       0.72150004   0.32969996
    0.91594523]
 [ 70.80767368   0.29530001   0.65670007   0.0553       0.48610002
    1.41573644]
 [ 42.34797918   0.4262       0.70300001   0.07540001   0.51990002
    1.03424954]
 ...
 [-23.00967281   0.78670007   0.6868       0.67179996   0.37830004
    0.89489508]
 [ 34.08180565   0.78400004   0.6983       0.71289998   0.50419998
    0.83899993]
 [ 52.68348937   0.43220001   0.71260005   0.0796       0.54809999
    1.0985775 ]][0m
[37m[1m[2023-07-11 19:23:15,808][233954] Max Reward on eval: 569.6164932198357[0m
[37m[1m[2023-07-11 19:23:15,809][233954] Min Reward on eval: -119.52146951793694[0m
[37m[1m[2023-07-11 19:23:15,809][233954] Mean Reward across all agents: 124.94996311675445[0m
[37m[1m[2023-07-11 19:23:15,809][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:23:15,814][233954] mean_value=-58.42346019577485, max_value=563.5407475095235[0m
[37m[1m[2023-07-11 19:23:15,816][233954] New mean coefficients: [[-0.78046185  3.9733233   5.3465457   1.0150082   0.45787376 -6.28652   ]][0m
[37m[1m[2023-07-11 19:23:15,817][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:23:24,794][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 19:23:24,794][233954] FPS: 427849.21[0m
[36m[2023-07-11 19:23:24,796][233954] itr=1400, itrs=2000, Progress: 70.00%[0m
[37m[1m[2023-07-11 19:27:12,928][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001380[0m
[36m[2023-07-11 19:27:26,370][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 19:27:26,371][233954] FPS: 332970.73[0m
[36m[2023-07-11 19:27:30,608][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:27:30,608][233954] Reward + Measures: [[21.62145376  0.88697332  0.8801983   0.83756804  0.71876293  0.98293698]][0m
[37m[1m[2023-07-11 19:27:30,608][233954] Max Reward on eval: 21.62145376226954[0m
[37m[1m[2023-07-11 19:27:30,609][233954] Min Reward on eval: 21.62145376226954[0m
[37m[1m[2023-07-11 19:27:30,609][233954] Mean Reward across all agents: 21.62145376226954[0m
[37m[1m[2023-07-11 19:27:30,609][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:27:35,727][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:27:35,733][233954] Reward + Measures: [[  9.241457     0.58570004   0.6972       0.19630001   0.69099993
    0.91995287]
 [-40.87287085   0.20180002   0.71260005   0.36849999   0.61570001
    1.44678104]
 [-15.33190735   0.35449997   0.72439998   0.4515       0.62160003
    1.48253095]
 ...
 [ 37.64589895   0.93000001   0.74220002   0.89799994   0.31460002
    0.92384738]
 [ 70.14079264   0.57450002   0.49950001   0.56910002   0.54940003
    1.47328484]
 [ 51.76967789   0.79570001   0.829        0.74419999   0.76920003
    1.59434688]][0m
[37m[1m[2023-07-11 19:27:35,733][233954] Max Reward on eval: 294.85205222070215[0m
[37m[1m[2023-07-11 19:27:35,734][233954] Min Reward on eval: -76.53504109461792[0m
[37m[1m[2023-07-11 19:27:35,734][233954] Mean Reward across all agents: 64.83920277801643[0m
[37m[1m[2023-07-11 19:27:35,734][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:27:35,761][233954] mean_value=-135.45089198472579, max_value=261.6519078712584[0m
[37m[1m[2023-07-11 19:27:35,769][233954] New mean coefficients: [[-1.4500974  4.6985207  5.188058   2.0604696 -1.2279155 -6.285898 ]][0m
[37m[1m[2023-07-11 19:27:35,771][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:27:44,708][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 19:27:44,708][233954] FPS: 429775.98[0m
[36m[2023-07-11 19:27:44,711][233954] itr=1401, itrs=2000, Progress: 70.05%[0m
[36m[2023-07-11 19:27:56,313][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 19:27:56,313][233954] FPS: 334270.82[0m
[36m[2023-07-11 19:28:00,545][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:28:00,551][233954] Reward + Measures: [[134.24028755   0.99457699   0.97808266   0.9918226    0.00181067
    0.9969064 ]][0m
[37m[1m[2023-07-11 19:28:00,551][233954] Max Reward on eval: 134.24028754515854[0m
[37m[1m[2023-07-11 19:28:00,552][233954] Min Reward on eval: 134.24028754515854[0m
[37m[1m[2023-07-11 19:28:00,552][233954] Mean Reward across all agents: 134.24028754515854[0m
[37m[1m[2023-07-11 19:28:00,552][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:28:05,521][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:28:05,521][233954] Reward + Measures: [[-69.83069967   0.76940006   0.68980002   0.68939996   0.2784
    0.78639066]
 [-49.71876328   0.74870002   0.66659999   0.72460002   0.13880001
    0.76204389]
 [-33.41405613   0.81669998   0.62360001   0.78970003   0.15589999
    0.79494727]
 ...
 [ 17.46908109   0.89180005   0.5783       0.83540004   0.44779998
    0.77226162]
 [ -3.9098095    0.88399994   0.61940002   0.87010002   0.0519
    0.79869598]
 [ 32.69316852   0.93290007   0.64230001   0.91280001   0.149
    0.79380006]][0m
[37m[1m[2023-07-11 19:28:05,521][233954] Max Reward on eval: 231.24484824301908[0m
[37m[1m[2023-07-11 19:28:05,522][233954] Min Reward on eval: -105.90506317219698[0m
[37m[1m[2023-07-11 19:28:05,522][233954] Mean Reward across all agents: -1.179336884770411[0m
[37m[1m[2023-07-11 19:28:05,522][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:28:05,525][233954] mean_value=-77.77505233483474, max_value=300.18122120447543[0m
[37m[1m[2023-07-11 19:28:05,527][233954] New mean coefficients: [[-1.9200594  4.7736435  5.6335416  2.8821392 -0.7077895 -6.4220524]][0m
[37m[1m[2023-07-11 19:28:05,528][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:28:14,463][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 19:28:14,464][233954] FPS: 429827.89[0m
[36m[2023-07-11 19:28:14,466][233954] itr=1402, itrs=2000, Progress: 70.10%[0m
[36m[2023-07-11 19:28:26,193][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 19:28:26,193][233954] FPS: 330704.36[0m
[36m[2023-07-11 19:28:30,522][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:28:30,528][233954] Reward + Measures: [[-17.17059801   0.76614767   0.80281931   0.22913867   0.73277068
    0.82639617]][0m
[37m[1m[2023-07-11 19:28:30,528][233954] Max Reward on eval: -17.17059800695403[0m
[37m[1m[2023-07-11 19:28:30,529][233954] Min Reward on eval: -17.17059800695403[0m
[37m[1m[2023-07-11 19:28:30,529][233954] Mean Reward across all agents: -17.17059800695403[0m
[37m[1m[2023-07-11 19:28:30,529][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:28:35,490][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:28:35,491][233954] Reward + Measures: [[ 449.72363848    0.97199994    0.98780006    0.            0.97189999
     3.00051117]
 [  40.15568528    0.68559998    0.63060004    0.78210002    0.37960002
     1.51930666]
 [  -1.76930248    0.56020004    0.90399998    0.50809997    0.9188
     1.3392663 ]
 ...
 [  80.93092513    0.54519999    0.09890001    0.60280001    0.2309
     2.27116704]
 [ -50.95055925    0.75209999    0.0787        0.78360003    0.28830001
     2.40065742]
 [-516.50664804    0.94939995    0.97200006    0.0077        0.96480006
     2.0895288 ]][0m
[37m[1m[2023-07-11 19:28:35,491][233954] Max Reward on eval: 456.3046970359981[0m
[37m[1m[2023-07-11 19:28:35,491][233954] Min Reward on eval: -673.8313465387561[0m
[37m[1m[2023-07-11 19:28:35,491][233954] Mean Reward across all agents: -18.797796262047456[0m
[37m[1m[2023-07-11 19:28:35,492][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:28:35,500][233954] mean_value=7.384547852672161, max_value=588.1277097389486[0m
[37m[1m[2023-07-11 19:28:35,503][233954] New mean coefficients: [[-1.5091704  4.7968493  5.7593994  3.426077  -0.243054  -6.662745 ]][0m
[37m[1m[2023-07-11 19:28:35,504][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:28:44,521][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 19:28:44,521][233954] FPS: 425943.16[0m
[36m[2023-07-11 19:28:44,523][233954] itr=1403, itrs=2000, Progress: 70.15%[0m
[36m[2023-07-11 19:28:56,154][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 19:28:56,154][233954] FPS: 333519.43[0m
[36m[2023-07-11 19:29:00,396][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:29:00,396][233954] Reward + Measures: [[32.46729533  0.97224331  0.578767    0.95644939  0.43974763  0.95429116]][0m
[37m[1m[2023-07-11 19:29:00,396][233954] Max Reward on eval: 32.467295331694615[0m
[37m[1m[2023-07-11 19:29:00,397][233954] Min Reward on eval: 32.467295331694615[0m
[37m[1m[2023-07-11 19:29:00,397][233954] Mean Reward across all agents: 32.467295331694615[0m
[37m[1m[2023-07-11 19:29:00,397][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:29:05,341][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:29:05,341][233954] Reward + Measures: [[ 27.96378383   0.45890003   0.57270002   0.42440006   0.60910004
    1.24699211]
 [ 93.22436457   0.45860001   0.43270001   0.38190004   0.3213
    1.62891161]
 [ 32.69572363   0.59320003   0.4111       0.49040005   0.2624
    1.25744712]
 ...
 [-13.07840294   0.72539997   0.27360001   0.74049997   0.17310002
    1.09089971]
 [ 24.17138472   0.62699997   0.52430004   0.48529997   0.50500005
    1.36302507]
 [101.36899044   0.45269999   0.34870002   0.35960001   0.41700003
    1.29683626]][0m
[37m[1m[2023-07-11 19:29:05,342][233954] Max Reward on eval: 211.8091850452125[0m
[37m[1m[2023-07-11 19:29:05,342][233954] Min Reward on eval: -151.32740973455367[0m
[37m[1m[2023-07-11 19:29:05,342][233954] Mean Reward across all agents: 53.17652769547745[0m
[37m[1m[2023-07-11 19:29:05,342][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:29:05,346][233954] mean_value=-152.15345728731458, max_value=336.882136822634[0m
[37m[1m[2023-07-11 19:29:05,348][233954] New mean coefficients: [[-0.64109254  3.398844    5.577028    2.5991516  -0.16812763 -6.442152  ]][0m
[37m[1m[2023-07-11 19:29:05,349][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:29:14,383][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 19:29:14,383][233954] FPS: 425144.82[0m
[36m[2023-07-11 19:29:14,386][233954] itr=1404, itrs=2000, Progress: 70.20%[0m
[36m[2023-07-11 19:29:25,973][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 19:29:25,973][233954] FPS: 334742.85[0m
[36m[2023-07-11 19:29:30,273][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:29:30,274][233954] Reward + Measures: [[-8.11721792  0.94413167  0.75556201  0.95103329  0.003435    0.99435937]][0m
[37m[1m[2023-07-11 19:29:30,274][233954] Max Reward on eval: -8.11721791679452[0m
[37m[1m[2023-07-11 19:29:30,274][233954] Min Reward on eval: -8.11721791679452[0m
[37m[1m[2023-07-11 19:29:30,274][233954] Mean Reward across all agents: -8.11721791679452[0m
[37m[1m[2023-07-11 19:29:30,275][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:29:35,228][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:29:35,228][233954] Reward + Measures: [[-18.73701376   0.92740005   0.90960008   0.97419995   0.0114
    1.13467145]
 [-27.18077574   0.94019997   0.89660007   0.97550005   0.0098
    1.13741124]
 [-27.56092146   0.86359996   0.64440006   0.87959999   0.103
    1.07417047]
 ...
 [-20.80283071   0.86580002   0.88120002   0.7001       0.52950001
    1.09732616]
 [-17.58968656   0.92409992   0.91280001   0.97139996   0.018
    1.14100063]
 [-17.92298659   0.93440002   0.90330011   0.9780001    0.0031
    1.11496925]][0m
[37m[1m[2023-07-11 19:29:35,229][233954] Max Reward on eval: 52.892280344711615[0m
[37m[1m[2023-07-11 19:29:35,229][233954] Min Reward on eval: -78.15010116323829[0m
[37m[1m[2023-07-11 19:29:35,229][233954] Mean Reward across all agents: -3.310750070973659[0m
[37m[1m[2023-07-11 19:29:35,229][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:29:35,231][233954] mean_value=-146.77139403180962, max_value=398.4733611361329[0m
[37m[1m[2023-07-11 19:29:35,234][233954] New mean coefficients: [[-0.6177161  2.8336082  5.929405   3.4141693  1.333044  -5.72224  ]][0m
[37m[1m[2023-07-11 19:29:35,235][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:29:44,200][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 19:29:44,200][233954] FPS: 428413.70[0m
[36m[2023-07-11 19:29:44,202][233954] itr=1405, itrs=2000, Progress: 70.25%[0m
[36m[2023-07-11 19:29:55,973][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 19:29:55,974][233954] FPS: 329492.50[0m
[36m[2023-07-11 19:30:00,277][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:30:00,278][233954] Reward + Measures: [[22.65928269  0.94569027  0.94108731  0.93633527  0.89311463  0.93864924]][0m
[37m[1m[2023-07-11 19:30:00,278][233954] Max Reward on eval: 22.659282691240474[0m
[37m[1m[2023-07-11 19:30:00,278][233954] Min Reward on eval: 22.659282691240474[0m
[37m[1m[2023-07-11 19:30:00,278][233954] Mean Reward across all agents: 22.659282691240474[0m
[37m[1m[2023-07-11 19:30:00,279][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:30:05,258][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:30:05,259][233954] Reward + Measures: [[ 40.29994655   0.98040003   0.95649999   0.9472       0.92070001
    0.90814537]
 [-23.37760769   0.95489997   0.76109999   0.9052       0.3987
    0.90606213]
 [ 22.17579473   0.99469995   0.98859996   0.98240006   0.97290003
    0.98033923]
 ...
 [ 44.9782567    0.90950006   0.79469997   0.87099999   0.66970003
    0.88703108]
 [-40.18349451   0.86610001   0.64380002   0.7931       0.31150001
    0.91717333]
 [-54.42407657   0.95069999   0.70199996   0.87330002   0.21110001
    0.88206398]][0m
[37m[1m[2023-07-11 19:30:05,259][233954] Max Reward on eval: 115.41494533508084[0m
[37m[1m[2023-07-11 19:30:05,259][233954] Min Reward on eval: -56.50760910771787[0m
[37m[1m[2023-07-11 19:30:05,259][233954] Mean Reward across all agents: 20.175883687289485[0m
[37m[1m[2023-07-11 19:30:05,260][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:30:05,262][233954] mean_value=-31.398850514221557, max_value=90.76045464262647[0m
[37m[1m[2023-07-11 19:30:05,265][233954] New mean coefficients: [[-0.34882396  2.161344    6.299168    0.52936625  1.360547   -5.6021996 ]][0m
[37m[1m[2023-07-11 19:30:05,266][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:30:14,269][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 19:30:14,269][233954] FPS: 426585.64[0m
[36m[2023-07-11 19:30:14,271][233954] itr=1406, itrs=2000, Progress: 70.30%[0m
[36m[2023-07-11 19:30:26,071][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 19:30:26,071][233954] FPS: 328610.94[0m
[36m[2023-07-11 19:30:30,339][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:30:30,339][233954] Reward + Measures: [[21.44202638  0.97065794  0.98789394  0.965132    0.97143596  0.96018243]][0m
[37m[1m[2023-07-11 19:30:30,339][233954] Max Reward on eval: 21.442026383760357[0m
[37m[1m[2023-07-11 19:30:30,340][233954] Min Reward on eval: 21.442026383760357[0m
[37m[1m[2023-07-11 19:30:30,340][233954] Mean Reward across all agents: 21.442026383760357[0m
[37m[1m[2023-07-11 19:30:30,340][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:30:35,596][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:30:35,596][233954] Reward + Measures: [[47.15536667  0.87379998  0.76990002  0.8682      0.42899999  0.94074249]
 [18.54471128  0.96260005  0.96680003  0.97170001  0.95310003  0.88929558]
 [14.62784106  0.98210001  0.98540002  0.97459996  0.98349994  1.00948322]
 ...
 [12.02571941  0.97369999  0.98740005  0.97690004  0.98500007  0.99537671]
 [21.57270824  0.96509999  0.98290008  0.80610001  0.96700001  1.10823822]
 [29.78093289  0.76800007  0.87950003  0.38560003  0.86040002  0.98456031]][0m
[37m[1m[2023-07-11 19:30:35,596][233954] Max Reward on eval: 97.82512662761147[0m
[37m[1m[2023-07-11 19:30:35,597][233954] Min Reward on eval: -29.62122680586763[0m
[37m[1m[2023-07-11 19:30:35,597][233954] Mean Reward across all agents: 23.492838970045973[0m
[37m[1m[2023-07-11 19:30:35,597][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:30:35,601][233954] mean_value=-40.360492661080954, max_value=368.60193763771684[0m
[37m[1m[2023-07-11 19:30:35,603][233954] New mean coefficients: [[-0.75711226  1.9346124   6.366267   -0.18786067 -0.6935502  -6.1593323 ]][0m
[37m[1m[2023-07-11 19:30:35,604][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:30:44,575][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 19:30:44,575][233954] FPS: 428147.87[0m
[36m[2023-07-11 19:30:44,577][233954] itr=1407, itrs=2000, Progress: 70.35%[0m
[36m[2023-07-11 19:30:56,339][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 19:30:56,339][233954] FPS: 329695.93[0m
[36m[2023-07-11 19:31:00,631][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:31:00,631][233954] Reward + Measures: [[24.86270114  0.98081827  0.99489027  0.00426367  0.9946841   0.84967828]][0m
[37m[1m[2023-07-11 19:31:00,632][233954] Max Reward on eval: 24.862701135173992[0m
[37m[1m[2023-07-11 19:31:00,632][233954] Min Reward on eval: 24.862701135173992[0m
[37m[1m[2023-07-11 19:31:00,632][233954] Mean Reward across all agents: 24.862701135173992[0m
[37m[1m[2023-07-11 19:31:00,632][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:31:05,604][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:31:05,604][233954] Reward + Measures: [[-52.78446816   0.31529999   0.80140001   0.35429999   0.77749997
    1.80544555]
 [ 20.04450332   0.47440001   0.97080004   0.27379999   0.93529999
    1.52345622]
 [ 16.77372016   0.97570002   0.1471       0.90570003   0.5291
    1.43254018]
 ...
 [ -7.28571573   0.6044001    0.38369998   0.5406       0.29500002
    1.9584856 ]
 [131.57068186   0.31460002   0.53979999   0.3265       0.4993
    1.86838758]
 [ 12.23918318   0.88789999   0.88899994   0.77930003   0.81350005
    1.17802465]][0m
[37m[1m[2023-07-11 19:31:05,605][233954] Max Reward on eval: 220.41132830390706[0m
[37m[1m[2023-07-11 19:31:05,605][233954] Min Reward on eval: -190.61080549876206[0m
[37m[1m[2023-07-11 19:31:05,605][233954] Mean Reward across all agents: 2.3653473609962874[0m
[37m[1m[2023-07-11 19:31:05,606][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:31:05,610][233954] mean_value=-158.8057140942163, max_value=394.5432974119583[0m
[37m[1m[2023-07-11 19:31:05,613][233954] New mean coefficients: [[-1.4625541   1.3064679   6.100412   -0.91838515 -1.6721357  -5.932322  ]][0m
[37m[1m[2023-07-11 19:31:05,614][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:31:14,657][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 19:31:14,657][233954] FPS: 424717.65[0m
[36m[2023-07-11 19:31:14,660][233954] itr=1408, itrs=2000, Progress: 70.40%[0m
[36m[2023-07-11 19:31:26,465][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 19:31:26,465][233954] FPS: 328547.28[0m
[36m[2023-07-11 19:31:30,785][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:31:30,790][233954] Reward + Measures: [[4.69955325 0.60180533 0.96134967 0.14954233 0.88172233 0.94602573]][0m
[37m[1m[2023-07-11 19:31:30,791][233954] Max Reward on eval: 4.699553250496862[0m
[37m[1m[2023-07-11 19:31:30,791][233954] Min Reward on eval: 4.699553250496862[0m
[37m[1m[2023-07-11 19:31:30,791][233954] Mean Reward across all agents: 4.699553250496862[0m
[37m[1m[2023-07-11 19:31:30,792][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:31:35,819][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:31:35,825][233954] Reward + Measures: [[ 189.80815495    0.59030002    0.61680001    0.51520002    0.26559997
     0.7855286 ]
 [ 100.08923588    0.62110001    0.79820007    0.4429        0.45700002
     0.78510153]
 [  22.39235205    0.79689997    0.73660004    0.74509996    0.0761
     0.89587307]
 ...
 [ -40.60028921    0.6261        0.91309994    0.30749997    0.81280005
     0.82356626]
 [  89.80752629    0.59580004    0.84310001    0.41080004    0.61510003
     0.78617203]
 [-132.6993539     0.67259997    0.74690002    0.35440001    0.50470001
     0.94446021]][0m
[37m[1m[2023-07-11 19:31:35,825][233954] Max Reward on eval: 224.06868742629885[0m
[37m[1m[2023-07-11 19:31:35,825][233954] Min Reward on eval: -171.61587762119015[0m
[37m[1m[2023-07-11 19:31:35,826][233954] Mean Reward across all agents: 13.780228979138625[0m
[37m[1m[2023-07-11 19:31:35,826][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:31:35,831][233954] mean_value=-36.58284627125495, max_value=449.48943177093946[0m
[37m[1m[2023-07-11 19:31:35,834][233954] New mean coefficients: [[-1.9312426   1.5697192   5.99185     0.28818655 -3.180286   -6.167408  ]][0m
[37m[1m[2023-07-11 19:31:35,835][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:31:44,821][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 19:31:44,822][233954] FPS: 427380.41[0m
[36m[2023-07-11 19:31:44,824][233954] itr=1409, itrs=2000, Progress: 70.45%[0m
[36m[2023-07-11 19:31:56,445][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 19:31:56,445][233954] FPS: 333792.66[0m
[36m[2023-07-11 19:32:00,696][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:32:00,701][233954] Reward + Measures: [[34.72131486  0.9117347   0.77494597  0.86677068  0.04528334  0.68924135]][0m
[37m[1m[2023-07-11 19:32:00,701][233954] Max Reward on eval: 34.72131486013086[0m
[37m[1m[2023-07-11 19:32:00,702][233954] Min Reward on eval: 34.72131486013086[0m
[37m[1m[2023-07-11 19:32:00,702][233954] Mean Reward across all agents: 34.72131486013086[0m
[37m[1m[2023-07-11 19:32:00,702][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:32:05,656][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:32:05,657][233954] Reward + Measures: [[ 65.86305807   0.4628       0.34540001   0.35980001   0.36520001
    1.54311264]
 [116.38119769   0.29659998   0.43540001   0.3263       0.29580003
    1.38309336]
 [ 17.00620567   0.88459998   0.77080005   0.75360006   0.41869998
    0.72930574]
 ...
 [143.30410852   0.32320002   0.32679999   0.28869998   0.21799998
    1.93401241]
 [ 75.64620185   0.63099998   0.67620003   0.29580003   0.45520002
    1.43469417]
 [-20.23323664   0.34730002   0.63620007   0.47779998   0.48430005
    1.117836  ]][0m
[37m[1m[2023-07-11 19:32:05,657][233954] Max Reward on eval: 252.78521724678575[0m
[37m[1m[2023-07-11 19:32:05,657][233954] Min Reward on eval: -144.81315566301345[0m
[37m[1m[2023-07-11 19:32:05,657][233954] Mean Reward across all agents: 36.63955818476825[0m
[37m[1m[2023-07-11 19:32:05,658][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:32:05,662][233954] mean_value=-482.5777903627123, max_value=419.0437890859571[0m
[37m[1m[2023-07-11 19:32:05,665][233954] New mean coefficients: [[ 0.56435525  0.93365884  5.7240825  -0.6493365  -1.7683222  -6.24401   ]][0m
[37m[1m[2023-07-11 19:32:05,666][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:32:14,598][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 19:32:14,598][233954] FPS: 429993.52[0m
[36m[2023-07-11 19:32:14,600][233954] itr=1410, itrs=2000, Progress: 70.50%[0m
[37m[1m[2023-07-11 19:36:09,830][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001390[0m
[36m[2023-07-11 19:36:22,999][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 19:36:22,999][233954] FPS: 328715.52[0m
[36m[2023-07-11 19:36:27,302][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:36:27,303][233954] Reward + Measures: [[94.51955515  0.83991289  0.80573827  0.80765462  0.172382    0.89514118]][0m
[37m[1m[2023-07-11 19:36:27,303][233954] Max Reward on eval: 94.51955514701687[0m
[37m[1m[2023-07-11 19:36:27,303][233954] Min Reward on eval: 94.51955514701687[0m
[37m[1m[2023-07-11 19:36:27,303][233954] Mean Reward across all agents: 94.51955514701687[0m
[37m[1m[2023-07-11 19:36:27,304][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:36:32,293][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:36:32,294][233954] Reward + Measures: [[159.31605295   0.4824       0.49770004   0.3337       0.21870001
    1.40857148]
 [109.51945157   0.63959998   0.58770007   0.56260008   0.1357
    1.74520957]
 [-88.0638766    0.63889998   0.52330005   0.52999997   0.25639999
    1.28134143]
 ...
 [211.93853209   0.44049999   0.38949999   0.3179       0.3892
    1.69812679]
 [149.27293745   0.29800001   0.36609998   0.23140001   0.18340002
    1.97094846]
 [ 17.90657067   0.4535       0.54260004   0.27810001   0.36740002
    1.5615294 ]][0m
[37m[1m[2023-07-11 19:36:32,294][233954] Max Reward on eval: 288.4995708430186[0m
[37m[1m[2023-07-11 19:36:32,294][233954] Min Reward on eval: -148.67067745290697[0m
[37m[1m[2023-07-11 19:36:32,294][233954] Mean Reward across all agents: 43.42504247639202[0m
[37m[1m[2023-07-11 19:36:32,294][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:36:32,305][233954] mean_value=-137.05269319536174, max_value=438.0250457713065[0m
[37m[1m[2023-07-11 19:36:32,309][233954] New mean coefficients: [[ 1.5209363  0.4069301  5.9844666 -2.829694  -1.4892769 -6.3597374]][0m
[37m[1m[2023-07-11 19:36:32,310][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:36:41,342][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 19:36:41,342][233954] FPS: 425230.05[0m
[36m[2023-07-11 19:36:41,344][233954] itr=1411, itrs=2000, Progress: 70.55%[0m
[36m[2023-07-11 19:36:53,401][233954] train() took 11.94 seconds to complete[0m
[36m[2023-07-11 19:36:53,401][233954] FPS: 321614.13[0m
[36m[2023-07-11 19:36:57,690][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:36:57,695][233954] Reward + Measures: [[90.38783823  0.81908798  0.79714769  0.75042832  0.27798167  0.78889501]][0m
[37m[1m[2023-07-11 19:36:57,696][233954] Max Reward on eval: 90.38783823172669[0m
[37m[1m[2023-07-11 19:36:57,696][233954] Min Reward on eval: 90.38783823172669[0m
[37m[1m[2023-07-11 19:36:57,696][233954] Mean Reward across all agents: 90.38783823172669[0m
[37m[1m[2023-07-11 19:36:57,697][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:37:02,695][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:37:02,701][233954] Reward + Measures: [[112.82032825   0.82569999   0.704        0.70609999   0.13610001
    0.94998455]
 [165.23588657   0.95749998   0.86900008   0.85200006   0.012
    1.03008187]
 [  9.403265     0.93219995   0.91949999   0.94760001   0.74529999
    1.25426924]
 ...
 [123.98001863   0.94260007   0.8222       0.85089999   0.044
    0.96616507]
 [126.4618502    0.93180001   0.8063001    0.81170005   0.0418
    0.98720759]
 [108.9712763    0.95749998   0.86300004   0.82490009   0.0269
    1.07984769]][0m
[37m[1m[2023-07-11 19:37:02,701][233954] Max Reward on eval: 382.8564376946539[0m
[37m[1m[2023-07-11 19:37:02,702][233954] Min Reward on eval: -62.676176115684214[0m
[37m[1m[2023-07-11 19:37:02,702][233954] Mean Reward across all agents: 138.55507093216806[0m
[37m[1m[2023-07-11 19:37:02,702][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:37:02,710][233954] mean_value=65.71417431338179, max_value=759.9425401730463[0m
[37m[1m[2023-07-11 19:37:02,713][233954] New mean coefficients: [[ 3.8953507 -1.2285217  5.5162644 -4.849021  -0.5765017 -6.2042327]][0m
[37m[1m[2023-07-11 19:37:02,714][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:37:11,778][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 19:37:11,778][233954] FPS: 423708.73[0m
[36m[2023-07-11 19:37:11,780][233954] itr=1412, itrs=2000, Progress: 70.60%[0m
[36m[2023-07-11 19:37:23,502][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 19:37:23,502][233954] FPS: 330806.88[0m
[36m[2023-07-11 19:37:27,706][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:37:27,712][233954] Reward + Measures: [[90.93786387  0.76552033  0.77652764  0.66168964  0.39533934  0.74532163]][0m
[37m[1m[2023-07-11 19:37:27,713][233954] Max Reward on eval: 90.93786386784467[0m
[37m[1m[2023-07-11 19:37:27,713][233954] Min Reward on eval: 90.93786386784467[0m
[37m[1m[2023-07-11 19:37:27,714][233954] Mean Reward across all agents: 90.93786386784467[0m
[37m[1m[2023-07-11 19:37:27,714][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:37:32,648][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:37:32,649][233954] Reward + Measures: [[-29.96865448   0.5359       0.67479998   0.49169999   0.52600002
    0.89964932]
 [ 77.81311104   0.44039997   0.5212       0.3768       0.45760003
    0.95726532]
 [ -7.21463854   0.69580001   0.73790008   0.66839999   0.43210003
    0.80423278]
 ...
 [-43.25684834   0.53259999   0.61289996   0.44250003   0.4605
    0.80167705]
 [-48.87966213   0.49849996   0.63850003   0.44010001   0.51980007
    0.80949205]
 [ 24.97310695   0.43150002   0.60500002   0.47420001   0.4271
    0.84990513]][0m
[37m[1m[2023-07-11 19:37:32,649][233954] Max Reward on eval: 173.0407619726844[0m
[37m[1m[2023-07-11 19:37:32,649][233954] Min Reward on eval: -95.61431828688364[0m
[37m[1m[2023-07-11 19:37:32,649][233954] Mean Reward across all agents: 37.33255855578285[0m
[37m[1m[2023-07-11 19:37:32,650][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:37:32,654][233954] mean_value=-98.02390997024936, max_value=301.1760597850422[0m
[37m[1m[2023-07-11 19:37:32,656][233954] New mean coefficients: [[ 0.8819668  -0.11297596  5.5002527  -3.583134   -3.221304   -6.687688  ]][0m
[37m[1m[2023-07-11 19:37:32,657][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:37:41,618][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 19:37:41,618][233954] FPS: 428613.77[0m
[36m[2023-07-11 19:37:41,620][233954] itr=1413, itrs=2000, Progress: 70.65%[0m
[36m[2023-07-11 19:37:53,375][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 19:37:53,375][233954] FPS: 329860.17[0m
[36m[2023-07-11 19:37:57,616][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:37:57,622][233954] Reward + Measures: [[102.49215063   0.76788229   0.80368      0.63580132   0.43301368
    0.70762336]][0m
[37m[1m[2023-07-11 19:37:57,622][233954] Max Reward on eval: 102.49215062989656[0m
[37m[1m[2023-07-11 19:37:57,622][233954] Min Reward on eval: 102.49215062989656[0m
[37m[1m[2023-07-11 19:37:57,623][233954] Mean Reward across all agents: 102.49215062989656[0m
[37m[1m[2023-07-11 19:37:57,623][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:38:02,647][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:38:02,652][233954] Reward + Measures: [[28.77350646  0.82790005  0.78979999  0.81170005  0.5133      0.9324457 ]
 [39.62848686  0.93620008  0.94570011  0.92670006  0.89660007  1.12526798]
 [11.52921157  0.95130008  0.92629999  0.86849993  0.76540005  1.02221155]
 ...
 [14.63594016  0.90410006  0.95139998  0.83210003  0.88020003  1.27679265]
 [-2.50963544  0.43990001  0.52469999  0.34310001  0.49969998  1.37282312]
 [69.3406873   0.37280002  0.45950004  0.25579998  0.4481      1.64689791]][0m
[37m[1m[2023-07-11 19:38:02,653][233954] Max Reward on eval: 138.13627387252637[0m
[37m[1m[2023-07-11 19:38:02,653][233954] Min Reward on eval: -66.54771026233212[0m
[37m[1m[2023-07-11 19:38:02,653][233954] Mean Reward across all agents: 22.818074722719338[0m
[37m[1m[2023-07-11 19:38:02,654][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:38:02,657][233954] mean_value=-105.35543174844824, max_value=416.28585134879677[0m
[37m[1m[2023-07-11 19:38:02,659][233954] New mean coefficients: [[-0.01753533  0.8261014   5.4376993  -2.0967045  -2.578939   -6.2104917 ]][0m
[37m[1m[2023-07-11 19:38:02,660][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:38:11,625][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 19:38:11,625][233954] FPS: 428417.30[0m
[36m[2023-07-11 19:38:11,628][233954] itr=1414, itrs=2000, Progress: 70.70%[0m
[36m[2023-07-11 19:38:23,380][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 19:38:23,381][233954] FPS: 330041.51[0m
[36m[2023-07-11 19:38:27,713][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:38:27,713][233954] Reward + Measures: [[91.60819836  0.93177801  0.89417398  0.88099664  0.00397333  0.90618026]][0m
[37m[1m[2023-07-11 19:38:27,713][233954] Max Reward on eval: 91.60819835864855[0m
[37m[1m[2023-07-11 19:38:27,714][233954] Min Reward on eval: 91.60819835864855[0m
[37m[1m[2023-07-11 19:38:27,714][233954] Mean Reward across all agents: 91.60819835864855[0m
[37m[1m[2023-07-11 19:38:27,714][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:38:32,709][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:38:32,715][233954] Reward + Measures: [[-47.73717133   0.83359998   0.60570002   0.75469995   0.12980001
    1.28328025]
 [ 69.9709834    0.87700003   0.81440002   0.75840002   0.3682
    1.06910169]
 [-56.27770903   0.75780004   0.73229998   0.58149999   0.31240001
    1.51057446]
 ...
 [102.66463997   0.792        0.73019999   0.74730003   0.0631
    0.93669969]
 [107.73945809   0.90080005   0.83840007   0.82849997   0.0212
    1.00316226]
 [159.21656892   0.82190001   0.796        0.7766       0.0975
    1.43826854]][0m
[37m[1m[2023-07-11 19:38:32,715][233954] Max Reward on eval: 260.10554648288525[0m
[37m[1m[2023-07-11 19:38:32,716][233954] Min Reward on eval: -137.484862348577[0m
[37m[1m[2023-07-11 19:38:32,716][233954] Mean Reward across all agents: 71.14986533736061[0m
[37m[1m[2023-07-11 19:38:32,716][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:38:32,720][233954] mean_value=-59.67250775164726, max_value=377.7652849966295[0m
[37m[1m[2023-07-11 19:38:32,723][233954] New mean coefficients: [[-0.21013844  1.3469856   5.521665    0.02858996 -2.2092412  -6.3848825 ]][0m
[37m[1m[2023-07-11 19:38:32,724][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:38:41,747][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 19:38:41,747][233954] FPS: 425664.56[0m
[36m[2023-07-11 19:38:41,749][233954] itr=1415, itrs=2000, Progress: 70.75%[0m
[36m[2023-07-11 19:38:53,462][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 19:38:53,462][233954] FPS: 331044.45[0m
[36m[2023-07-11 19:38:57,699][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:38:57,699][233954] Reward + Measures: [[126.09639191   0.97021061   0.93228692   0.92655027   0.00575
    1.13299167]][0m
[37m[1m[2023-07-11 19:38:57,700][233954] Max Reward on eval: 126.09639190678969[0m
[37m[1m[2023-07-11 19:38:57,700][233954] Min Reward on eval: 126.09639190678969[0m
[37m[1m[2023-07-11 19:38:57,700][233954] Mean Reward across all agents: 126.09639190678969[0m
[37m[1m[2023-07-11 19:38:57,700][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:39:02,650][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:39:02,651][233954] Reward + Measures: [[ 22.94595276   0.92519999   0.91170007   0.87290001   0.82489997
    1.59102118]
 [ 59.31755304   0.92150003   0.72670001   0.83529997   0.0893
    1.11781049]
 [  8.32215729   0.89519995   0.86230004   0.82959998   0.74439996
    1.43768132]
 ...
 [311.81822205   0.98619998   0.96469992   0.97189999   0.0027
    1.34140754]
 [ 99.50711779   0.71719998   0.69239998   0.61450005   0.40559998
    1.14436138]
 [ -1.51201937   0.51289999   0.51990002   0.34119999   0.31490001
    1.32001007]][0m
[37m[1m[2023-07-11 19:39:02,651][233954] Max Reward on eval: 440.60448838816956[0m
[37m[1m[2023-07-11 19:39:02,651][233954] Min Reward on eval: -42.09181484176079[0m
[37m[1m[2023-07-11 19:39:02,652][233954] Mean Reward across all agents: 101.75563395521667[0m
[37m[1m[2023-07-11 19:39:02,652][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:39:02,656][233954] mean_value=-99.36611114663077, max_value=698.2750854602084[0m
[37m[1m[2023-07-11 19:39:02,659][233954] New mean coefficients: [[-0.46600688  0.8222748   6.2194333  -1.7902378  -2.8139842  -6.9567404 ]][0m
[37m[1m[2023-07-11 19:39:02,660][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:39:11,609][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 19:39:11,609][233954] FPS: 429155.26[0m
[36m[2023-07-11 19:39:11,612][233954] itr=1416, itrs=2000, Progress: 70.80%[0m
[36m[2023-07-11 19:39:23,536][233954] train() took 11.81 seconds to complete[0m
[36m[2023-07-11 19:39:23,536][233954] FPS: 325233.94[0m
[36m[2023-07-11 19:39:27,926][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:39:27,926][233954] Reward + Measures: [[119.05222194   0.83346564   0.79061961   0.74109298   0.042646
    0.9312036 ]][0m
[37m[1m[2023-07-11 19:39:27,926][233954] Max Reward on eval: 119.05222194185619[0m
[37m[1m[2023-07-11 19:39:27,927][233954] Min Reward on eval: 119.05222194185619[0m
[37m[1m[2023-07-11 19:39:27,927][233954] Mean Reward across all agents: 119.05222194185619[0m
[37m[1m[2023-07-11 19:39:27,927][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:39:32,970][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:39:32,970][233954] Reward + Measures: [[101.95553239   0.43580005   0.65219998   0.317        0.30410001
    1.11716974]
 [127.05616858   0.8391       0.77430004   0.75080007   0.0341
    0.95087731]
 [126.87896345   0.74439996   0.76500005   0.69410002   0.0715
    0.91616648]
 ...
 [ 58.23121749   0.6469       0.58460003   0.52820003   0.2441
    0.99889773]
 [139.46512985   0.8551001    0.79990005   0.76620001   0.0138
    1.08808601]
 [141.85803034   0.80150002   0.77760005   0.75170004   0.10290001
    0.96937239]][0m
[37m[1m[2023-07-11 19:39:32,971][233954] Max Reward on eval: 194.88308332881425[0m
[37m[1m[2023-07-11 19:39:32,971][233954] Min Reward on eval: -42.12021018862724[0m
[37m[1m[2023-07-11 19:39:32,971][233954] Mean Reward across all agents: 101.90184777978806[0m
[37m[1m[2023-07-11 19:39:32,971][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:39:32,978][233954] mean_value=59.9229846803048, max_value=613.2755961514428[0m
[37m[1m[2023-07-11 19:39:32,981][233954] New mean coefficients: [[ 0.7281904   0.07311481  6.9240375  -2.0599017  -1.661547   -6.850562  ]][0m
[37m[1m[2023-07-11 19:39:32,982][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:39:42,106][233954] train() took 9.12 seconds to complete[0m
[36m[2023-07-11 19:39:42,107][233954] FPS: 420917.33[0m
[36m[2023-07-11 19:39:42,109][233954] itr=1417, itrs=2000, Progress: 70.85%[0m
[36m[2023-07-11 19:39:53,841][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 19:39:53,842][233954] FPS: 330514.06[0m
[36m[2023-07-11 19:39:58,077][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:39:58,077][233954] Reward + Measures: [[107.03398606   0.90948063   0.89208901   0.84988701   0.02231467
    0.90095294]][0m
[37m[1m[2023-07-11 19:39:58,077][233954] Max Reward on eval: 107.03398605840347[0m
[37m[1m[2023-07-11 19:39:58,077][233954] Min Reward on eval: 107.03398605840347[0m
[37m[1m[2023-07-11 19:39:58,078][233954] Mean Reward across all agents: 107.03398605840347[0m
[37m[1m[2023-07-11 19:39:58,078][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:40:03,053][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:40:03,053][233954] Reward + Measures: [[ 86.30918119   0.68650001   0.73609996   0.56779999   0.18919998
    0.91021883]
 [ 87.99932061   0.66280001   0.80550003   0.45240003   0.44520003
    0.85297173]
 [ 65.73916503   0.84790003   0.69260001   0.71940005   0.10950001
    1.25149858]
 ...
 [ 43.62238695   0.6469       0.70170003   0.54469997   0.2757
    0.90584511]
 [108.1345212    0.68000001   0.75720006   0.56980002   0.27550003
    1.00316942]
 [ 94.118011     0.72220004   0.88700002   0.41929999   0.5686
    0.94756824]][0m
[37m[1m[2023-07-11 19:40:03,054][233954] Max Reward on eval: 173.72498892713338[0m
[37m[1m[2023-07-11 19:40:03,054][233954] Min Reward on eval: -128.4498670122586[0m
[37m[1m[2023-07-11 19:40:03,054][233954] Mean Reward across all agents: 83.78937601134712[0m
[37m[1m[2023-07-11 19:40:03,054][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:40:03,061][233954] mean_value=46.84205418082562, max_value=559.8589857275749[0m
[37m[1m[2023-07-11 19:40:03,064][233954] New mean coefficients: [[ 2.1689577  -0.05395295  7.1800423  -2.1581547  -0.78701425 -6.7040477 ]][0m
[37m[1m[2023-07-11 19:40:03,065][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:40:12,004][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 19:40:12,004][233954] FPS: 429630.36[0m
[36m[2023-07-11 19:40:12,007][233954] itr=1418, itrs=2000, Progress: 70.90%[0m
[36m[2023-07-11 19:40:23,592][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 19:40:23,592][233954] FPS: 334877.07[0m
[36m[2023-07-11 19:40:27,880][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:40:27,881][233954] Reward + Measures: [[75.9364392   0.94516373  0.93593097  0.90589803  0.016786    0.7189216 ]][0m
[37m[1m[2023-07-11 19:40:27,881][233954] Max Reward on eval: 75.93643919555267[0m
[37m[1m[2023-07-11 19:40:27,881][233954] Min Reward on eval: 75.93643919555267[0m
[37m[1m[2023-07-11 19:40:27,881][233954] Mean Reward across all agents: 75.93643919555267[0m
[37m[1m[2023-07-11 19:40:27,882][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:40:32,940][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:40:32,946][233954] Reward + Measures: [[ 54.06272623   0.68720001   0.84639996   0.23049998   0.66180003
    1.04444122]
 [ 73.94935945   0.67530006   0.79960001   0.51589996   0.4612
    0.95555115]
 [ 43.49537531   0.5352       0.85319996   0.0883       0.76159996
    1.16514719]
 ...
 [102.7810247    0.67550004   0.77990001   0.52130002   0.28280002
    0.84720767]
 [ 64.59687656   0.67010003   0.83890003   0.45759997   0.60560006
    0.99377835]
 [ 67.29095338   0.62470001   0.80509996   0.3427       0.48810002
    0.91451705]][0m
[37m[1m[2023-07-11 19:40:32,946][233954] Max Reward on eval: 131.77965067271143[0m
[37m[1m[2023-07-11 19:40:32,946][233954] Min Reward on eval: -179.8390121121891[0m
[37m[1m[2023-07-11 19:40:32,946][233954] Mean Reward across all agents: 51.16141214693683[0m
[37m[1m[2023-07-11 19:40:32,947][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:40:32,951][233954] mean_value=-10.745672636875934, max_value=576.6439707372338[0m
[37m[1m[2023-07-11 19:40:32,954][233954] New mean coefficients: [[ 1.3042036  0.2093225  7.289769  -1.8147633 -2.8341708 -7.09055  ]][0m
[37m[1m[2023-07-11 19:40:32,955][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:40:42,030][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 19:40:42,031][233954] FPS: 423190.98[0m
[36m[2023-07-11 19:40:42,033][233954] itr=1419, itrs=2000, Progress: 70.95%[0m
[36m[2023-07-11 19:40:53,831][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 19:40:53,831][233954] FPS: 328683.57[0m
[36m[2023-07-11 19:40:58,125][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:40:58,125][233954] Reward + Measures: [[49.41153857  0.96048731  0.95569301  0.93232     0.013064    0.5565868 ]][0m
[37m[1m[2023-07-11 19:40:58,125][233954] Max Reward on eval: 49.411538566502244[0m
[37m[1m[2023-07-11 19:40:58,126][233954] Min Reward on eval: 49.411538566502244[0m
[37m[1m[2023-07-11 19:40:58,126][233954] Mean Reward across all agents: 49.411538566502244[0m
[37m[1m[2023-07-11 19:40:58,126][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:41:03,426][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:41:03,432][233954] Reward + Measures: [[ 39.53687539   0.9188       0.92029995   0.87540001   0.0604
    1.43024826]
 [ 68.88237904   0.94999999   0.88459998   0.91380006   0.0219
    1.15488434]
 [ 84.73130348   0.89519995   0.80389994   0.79899997   0.0074
    1.15514731]
 ...
 [ 86.13045596   0.70179999   0.79829997   0.42670003   0.40559998
    0.73113555]
 [129.69058897   0.91370004   0.8714       0.86749995   0.0348
    1.2006731 ]
 [123.49337576   0.9059       0.83029997   0.81159991   0.0242
    1.09556508]][0m
[37m[1m[2023-07-11 19:41:03,433][233954] Max Reward on eval: 181.86657333085313[0m
[37m[1m[2023-07-11 19:41:03,433][233954] Min Reward on eval: -92.21979716958012[0m
[37m[1m[2023-07-11 19:41:03,433][233954] Mean Reward across all agents: 95.45457384023916[0m
[37m[1m[2023-07-11 19:41:03,434][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:41:03,437][233954] mean_value=-40.61819622816844, max_value=616.7620158155216[0m
[37m[1m[2023-07-11 19:41:03,440][233954] New mean coefficients: [[ 2.2121112 -0.576313   7.200481  -2.4849832 -1.0202997 -6.183029 ]][0m
[37m[1m[2023-07-11 19:41:03,441][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:41:12,416][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 19:41:12,416][233954] FPS: 427934.17[0m
[36m[2023-07-11 19:41:12,419][233954] itr=1420, itrs=2000, Progress: 71.00%[0m
[37m[1m[2023-07-11 19:44:59,510][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001400[0m
[36m[2023-07-11 19:45:13,675][233954] train() took 12.25 seconds to complete[0m
[36m[2023-07-11 19:45:13,675][233954] FPS: 313433.44[0m
[36m[2023-07-11 19:45:17,912][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:45:17,912][233954] Reward + Measures: [[43.71403843  0.96573466  0.96235138  0.93943429  0.011319    0.52338767]][0m
[37m[1m[2023-07-11 19:45:17,912][233954] Max Reward on eval: 43.714038425236524[0m
[37m[1m[2023-07-11 19:45:17,913][233954] Min Reward on eval: 43.714038425236524[0m
[37m[1m[2023-07-11 19:45:17,913][233954] Mean Reward across all agents: 43.714038425236524[0m
[37m[1m[2023-07-11 19:45:17,913][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:45:22,881][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:45:22,882][233954] Reward + Measures: [[ 83.52063397   0.76989996   0.88709992   0.50099999   0.76389998
    1.48931968]
 [ 39.73859314   0.33750001   0.4316       0.26980004   0.252
    1.55607736]
 [-38.02592517   0.13259999   0.1214       0.1171       0.1099
    2.32588792]
 ...
 [117.82618405   0.727        0.71040004   0.54689997   0.1496
    1.30901706]
 [ 44.74614441   0.51840001   0.64699996   0.49720001   0.16880001
    1.22541511]
 [162.41696631   0.67520005   0.71890002   0.43540001   0.2899
    1.23489177]][0m
[37m[1m[2023-07-11 19:45:22,882][233954] Max Reward on eval: 223.67606157264672[0m
[37m[1m[2023-07-11 19:45:22,882][233954] Min Reward on eval: -121.54851340025198[0m
[37m[1m[2023-07-11 19:45:22,882][233954] Mean Reward across all agents: 51.54508381553606[0m
[37m[1m[2023-07-11 19:45:22,883][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:45:22,888][233954] mean_value=-423.25926402104386, max_value=675.1683887775405[0m
[37m[1m[2023-07-11 19:45:22,898][233954] New mean coefficients: [[ 2.0681605   0.03851104  7.2753124  -0.6197125  -0.7333586  -6.022946  ]][0m
[37m[1m[2023-07-11 19:45:22,899][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:45:31,939][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 19:45:31,940][233954] FPS: 424835.38[0m
[36m[2023-07-11 19:45:31,942][233954] itr=1421, itrs=2000, Progress: 71.05%[0m
[36m[2023-07-11 19:45:43,547][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 19:45:43,547][233954] FPS: 334114.12[0m
[36m[2023-07-11 19:45:47,751][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:45:47,751][233954] Reward + Measures: [[38.53406273  0.96789962  0.96509463  0.94260895  0.01104067  0.46871987]][0m
[37m[1m[2023-07-11 19:45:47,751][233954] Max Reward on eval: 38.53406272755819[0m
[37m[1m[2023-07-11 19:45:47,752][233954] Min Reward on eval: 38.53406272755819[0m
[37m[1m[2023-07-11 19:45:47,752][233954] Mean Reward across all agents: 38.53406272755819[0m
[37m[1m[2023-07-11 19:45:47,752][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:45:52,703][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:45:52,704][233954] Reward + Measures: [[128.98744858   0.31239998   0.47469997   0.17050001   0.2414
    1.67449439]
 [109.92127559   0.32900003   0.52319998   0.19140001   0.2762
    1.6732434 ]
 [190.13714983   0.61830008   0.72230005   0.50819999   0.46799999
    1.06617999]
 ...
 [ -9.44270054   0.31210002   0.66400003   0.1006       0.50459999
    1.48437548]
 [ 51.66668503   0.56650001   0.4251       0.4955       0.2185
    1.34531486]
 [-78.36752486   0.3876       0.57630008   0.2402       0.53439999
    1.23593581]][0m
[37m[1m[2023-07-11 19:45:52,704][233954] Max Reward on eval: 243.31779292738065[0m
[37m[1m[2023-07-11 19:45:52,704][233954] Min Reward on eval: -171.23820875715464[0m
[37m[1m[2023-07-11 19:45:52,705][233954] Mean Reward across all agents: 37.33773269462[0m
[37m[1m[2023-07-11 19:45:52,705][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:45:52,709][233954] mean_value=-193.78565969896812, max_value=558.5666984260292[0m
[37m[1m[2023-07-11 19:45:52,712][233954] New mean coefficients: [[ 2.4204865  -0.41741693  7.5727673  -1.6154461   0.62923884 -5.90213   ]][0m
[37m[1m[2023-07-11 19:45:52,713][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:46:01,658][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 19:46:01,658][233954] FPS: 429339.23[0m
[36m[2023-07-11 19:46:01,661][233954] itr=1422, itrs=2000, Progress: 71.10%[0m
[36m[2023-07-11 19:46:13,222][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 19:46:13,222][233954] FPS: 335410.20[0m
[36m[2023-07-11 19:46:17,478][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:46:17,479][233954] Reward + Measures: [[34.46184783  0.96991062  0.9673903   0.94643098  0.010099    0.39331248]][0m
[37m[1m[2023-07-11 19:46:17,479][233954] Max Reward on eval: 34.46184783396116[0m
[37m[1m[2023-07-11 19:46:17,479][233954] Min Reward on eval: 34.46184783396116[0m
[37m[1m[2023-07-11 19:46:17,480][233954] Mean Reward across all agents: 34.46184783396116[0m
[37m[1m[2023-07-11 19:46:17,480][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:46:22,431][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:46:22,432][233954] Reward + Measures: [[  -2.12463044    0.57849997    0.83540004    0.20820001    0.54980004
     0.91205072]
 [  59.66644481    0.79369998    0.93470001    0.1759        0.74900001
     0.99252146]
 [  73.46905707    0.86930001    0.92799997    0.39839998    0.56620002
     0.90054178]
 ...
 [-103.44956375    0.72609997    0.88230002    0.15210001    0.76770002
     1.45829856]
 [  54.37985398    0.79870003    0.95609999    0.0857        0.8222
     0.87630194]
 [-440.86888547    0.79459995    0.86070007    0.0378        0.83319998
     2.2929318 ]][0m
[37m[1m[2023-07-11 19:46:22,432][233954] Max Reward on eval: 218.1211700626649[0m
[37m[1m[2023-07-11 19:46:22,432][233954] Min Reward on eval: -620.4523887579329[0m
[37m[1m[2023-07-11 19:46:22,433][233954] Mean Reward across all agents: -73.86990958817138[0m
[37m[1m[2023-07-11 19:46:22,433][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:46:22,437][233954] mean_value=-122.90458559055328, max_value=573.4690570674836[0m
[37m[1m[2023-07-11 19:46:22,440][233954] New mean coefficients: [[ 0.11117363  0.2744177   6.985234    0.05518937 -1.350507   -5.8989563 ]][0m
[37m[1m[2023-07-11 19:46:22,441][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:46:31,358][233954] train() took 8.92 seconds to complete[0m
[36m[2023-07-11 19:46:31,358][233954] FPS: 430707.18[0m
[36m[2023-07-11 19:46:31,360][233954] itr=1423, itrs=2000, Progress: 71.15%[0m
[36m[2023-07-11 19:46:42,979][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 19:46:42,979][233954] FPS: 333743.63[0m
[36m[2023-07-11 19:46:47,260][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:46:47,261][233954] Reward + Measures: [[35.32674894  0.841811    0.93895733  0.63975132  0.28314334  0.66824484]][0m
[37m[1m[2023-07-11 19:46:47,261][233954] Max Reward on eval: 35.32674894412393[0m
[37m[1m[2023-07-11 19:46:47,261][233954] Min Reward on eval: 35.32674894412393[0m
[37m[1m[2023-07-11 19:46:47,262][233954] Mean Reward across all agents: 35.32674894412393[0m
[37m[1m[2023-07-11 19:46:47,262][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:46:52,259][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:46:52,320][233954] Reward + Measures: [[ 59.70287757   0.65450001   0.92789996   0.14060001   0.69480002
    0.92887747]
 [ 68.07953273   0.7978       0.83359998   0.54629999   0.39359999
    0.84693831]
 [121.7164721    0.5054       0.85100001   0.2299       0.56180006
    0.89765495]
 ...
 [153.03705548   0.53869998   0.74809998   0.39669999   0.3351
    0.79676354]
 [ 36.01747825   0.56070006   0.9217       0.16309999   0.74750006
    0.8797124 ]
 [ 90.80226037   0.6929       0.87550002   0.28639999   0.53929996
    0.88577026]][0m
[37m[1m[2023-07-11 19:46:52,320][233954] Max Reward on eval: 235.37063598032108[0m
[37m[1m[2023-07-11 19:46:52,320][233954] Min Reward on eval: -170.37360954582692[0m
[37m[1m[2023-07-11 19:46:52,321][233954] Mean Reward across all agents: 85.59532929279779[0m
[37m[1m[2023-07-11 19:46:52,321][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:46:52,332][233954] mean_value=8.528712167189187, max_value=655.8557003457355[0m
[37m[1m[2023-07-11 19:46:52,335][233954] New mean coefficients: [[ 0.25537592  0.932609    6.1741757   1.2626845  -0.6966122  -5.289355  ]][0m
[37m[1m[2023-07-11 19:46:52,336][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:47:01,356][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 19:47:01,356][233954] FPS: 425820.11[0m
[36m[2023-07-11 19:47:01,358][233954] itr=1424, itrs=2000, Progress: 71.20%[0m
[36m[2023-07-11 19:47:13,155][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 19:47:13,156][233954] FPS: 328748.79[0m
[36m[2023-07-11 19:47:17,397][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:47:17,398][233954] Reward + Measures: [[38.46413929  0.88505596  0.75863832  0.68337303  0.04495966  1.12630594]][0m
[37m[1m[2023-07-11 19:47:17,398][233954] Max Reward on eval: 38.46413928991573[0m
[37m[1m[2023-07-11 19:47:17,398][233954] Min Reward on eval: 38.46413928991573[0m
[37m[1m[2023-07-11 19:47:17,399][233954] Mean Reward across all agents: 38.46413928991573[0m
[37m[1m[2023-07-11 19:47:17,399][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:47:22,381][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:47:22,381][233954] Reward + Measures: [[-62.95109173   0.24269998   0.34660003   0.13         0.26659998
    2.20237017]
 [-52.83170401   0.39910004   0.57570004   0.102        0.5395
    1.44768798]
 [100.56787728   0.90630001   0.85610002   0.82730001   0.0354
    1.30816877]
 ...
 [ 69.87726285   0.28590003   0.3089       0.24080001   0.2278
    1.60509706]
 [119.63929624   0.6038       0.53189999   0.50690001   0.28260002
    1.57550144]
 [152.0023761    0.9181       0.83140004   0.76980001   0.0121
    1.25772798]][0m
[37m[1m[2023-07-11 19:47:22,382][233954] Max Reward on eval: 235.2161502751056[0m
[37m[1m[2023-07-11 19:47:22,382][233954] Min Reward on eval: -487.4055271108053[0m
[37m[1m[2023-07-11 19:47:22,382][233954] Mean Reward across all agents: 47.45817828582908[0m
[37m[1m[2023-07-11 19:47:22,382][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:47:22,387][233954] mean_value=-276.7210363624904, max_value=564.1781960026193[0m
[37m[1m[2023-07-11 19:47:22,389][233954] New mean coefficients: [[ 0.33358055  0.88932514  6.1831975   1.0560046  -1.1132185  -5.4249234 ]][0m
[37m[1m[2023-07-11 19:47:22,390][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:47:31,505][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 19:47:31,506][233954] FPS: 421357.61[0m
[36m[2023-07-11 19:47:31,508][233954] itr=1425, itrs=2000, Progress: 71.25%[0m
[36m[2023-07-11 19:47:43,327][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 19:47:43,327][233954] FPS: 328075.40[0m
[36m[2023-07-11 19:47:47,669][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:47:47,670][233954] Reward + Measures: [[31.25105944  0.91505271  0.81187361  0.74262595  0.03268733  1.08851111]][0m
[37m[1m[2023-07-11 19:47:47,670][233954] Max Reward on eval: 31.25105944485712[0m
[37m[1m[2023-07-11 19:47:47,670][233954] Min Reward on eval: 31.25105944485712[0m
[37m[1m[2023-07-11 19:47:47,670][233954] Mean Reward across all agents: 31.25105944485712[0m
[37m[1m[2023-07-11 19:47:47,671][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:47:52,631][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:47:52,632][233954] Reward + Measures: [[ 146.62922716    0.94910002    0.9004001     0.90070003    0.002
     0.96410412]
 [ -49.46421476    0.32300001    0.27740002    0.20460001    0.2244
     1.18023527]
 [-140.69536479    0.48210001    0.39379999    0.31310001    0.21089999
     1.30104148]
 ...
 [ 152.97146703    0.75940001    0.60830003    0.61880004    0.0583
     1.29448402]
 [ 215.04209924    0.92059994    0.86490005    0.85610008    0.004
     1.08918738]
 [  25.67127868    0.22239999    0.15820001    0.13990001    0.13340001
     1.90557754]][0m
[37m[1m[2023-07-11 19:47:52,632][233954] Max Reward on eval: 238.15223216572775[0m
[37m[1m[2023-07-11 19:47:52,632][233954] Min Reward on eval: -229.2956500152126[0m
[37m[1m[2023-07-11 19:47:52,633][233954] Mean Reward across all agents: 79.64761126674847[0m
[37m[1m[2023-07-11 19:47:52,633][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:47:52,637][233954] mean_value=-403.25300767418094, max_value=512.9773074310273[0m
[37m[1m[2023-07-11 19:47:52,639][233954] New mean coefficients: [[ 0.15730426  1.3043197   6.61533     1.1210439   0.4410162  -5.2917213 ]][0m
[37m[1m[2023-07-11 19:47:52,640][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:48:01,664][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 19:48:01,664][233954] FPS: 425625.66[0m
[36m[2023-07-11 19:48:01,666][233954] itr=1426, itrs=2000, Progress: 71.30%[0m
[36m[2023-07-11 19:48:13,273][233954] train() took 11.49 seconds to complete[0m
[36m[2023-07-11 19:48:13,273][233954] FPS: 334137.14[0m
[36m[2023-07-11 19:48:17,463][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:48:17,463][233954] Reward + Measures: [[141.05587725   0.85289967   0.63348264   0.71617001   0.062314
    0.79729766]][0m
[37m[1m[2023-07-11 19:48:17,463][233954] Max Reward on eval: 141.05587724796442[0m
[37m[1m[2023-07-11 19:48:17,464][233954] Min Reward on eval: 141.05587724796442[0m
[37m[1m[2023-07-11 19:48:17,464][233954] Mean Reward across all agents: 141.05587724796442[0m
[37m[1m[2023-07-11 19:48:17,464][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:48:22,403][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:48:22,404][233954] Reward + Measures: [[174.31325436   0.80149996   0.63219994   0.6354       0.102
    1.69960237]
 [ 83.78426505   0.54080003   0.55260003   0.4797       0.33610001
    1.36752737]
 [ 44.62574902   0.48650002   0.60170001   0.34310001   0.37279999
    1.16653883]
 ...
 [152.04806803   0.43309999   0.81470007   0.30830002   0.70170003
    1.33876503]
 [103.21510565   0.60810006   0.67710006   0.43930003   0.52430004
    0.97494406]
 [ 57.67469442   0.60070002   0.56819999   0.53280002   0.29300001
    0.95416069]][0m
[37m[1m[2023-07-11 19:48:22,404][233954] Max Reward on eval: 322.8837318609876[0m
[37m[1m[2023-07-11 19:48:22,405][233954] Min Reward on eval: -151.6442685371265[0m
[37m[1m[2023-07-11 19:48:22,405][233954] Mean Reward across all agents: 79.8671509910859[0m
[37m[1m[2023-07-11 19:48:22,405][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:48:22,410][233954] mean_value=-137.23618368953956, max_value=627.2862713908764[0m
[37m[1m[2023-07-11 19:48:22,413][233954] New mean coefficients: [[ 0.18781866  1.9727551   6.9713473   1.2301774   1.8169315  -5.3301363 ]][0m
[37m[1m[2023-07-11 19:48:22,414][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:48:31,360][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 19:48:31,360][233954] FPS: 429309.68[0m
[36m[2023-07-11 19:48:31,363][233954] itr=1427, itrs=2000, Progress: 71.35%[0m
[36m[2023-07-11 19:48:43,325][233954] train() took 11.85 seconds to complete[0m
[36m[2023-07-11 19:48:43,326][233954] FPS: 324127.07[0m
[36m[2023-07-11 19:48:47,570][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:48:47,570][233954] Reward + Measures: [[130.58392099   0.88975227   0.74047869   0.76925439   0.03875433
    0.77240193]][0m
[37m[1m[2023-07-11 19:48:47,571][233954] Max Reward on eval: 130.58392098530953[0m
[37m[1m[2023-07-11 19:48:47,571][233954] Min Reward on eval: 130.58392098530953[0m
[37m[1m[2023-07-11 19:48:47,571][233954] Mean Reward across all agents: 130.58392098530953[0m
[37m[1m[2023-07-11 19:48:47,571][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:48:52,615][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:48:52,616][233954] Reward + Measures: [[ 77.39469313   0.77819997   0.49950001   0.6681       0.1103
    2.06165624]
 [101.99994416   0.81510001   0.73360002   0.68970007   0.1847
    1.02016091]
 [-20.64567704   0.46040002   0.34100002   0.40790001   0.1506
    1.88368607]
 ...
 [126.65942405   0.73520005   0.43080002   0.5941       0.08110001
    1.02653706]
 [131.73561725   0.68460006   0.51120001   0.56280005   0.14040001
    1.03148842]
 [-45.67378103   0.41689998   0.60079998   0.2931       0.42530003
    1.30283535]][0m
[37m[1m[2023-07-11 19:48:52,616][233954] Max Reward on eval: 284.9157676753588[0m
[37m[1m[2023-07-11 19:48:52,616][233954] Min Reward on eval: -127.0254320829641[0m
[37m[1m[2023-07-11 19:48:52,617][233954] Mean Reward across all agents: 81.23658163830696[0m
[37m[1m[2023-07-11 19:48:52,617][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:48:52,621][233954] mean_value=-161.5605559988502, max_value=511.9584188999856[0m
[37m[1m[2023-07-11 19:48:52,624][233954] New mean coefficients: [[ 0.2162213  1.8596141  6.8461666  0.9733559  1.5345674 -5.475848 ]][0m
[37m[1m[2023-07-11 19:48:52,625][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:49:01,699][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 19:49:01,699][233954] FPS: 423270.70[0m
[36m[2023-07-11 19:49:01,701][233954] itr=1428, itrs=2000, Progress: 71.40%[0m
[36m[2023-07-11 19:49:13,497][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 19:49:13,497][233954] FPS: 328859.89[0m
[36m[2023-07-11 19:49:17,754][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:49:17,755][233954] Reward + Measures: [[107.53874085   0.91664201   0.80637199   0.81835026   0.03247233
    0.72748601]][0m
[37m[1m[2023-07-11 19:49:17,755][233954] Max Reward on eval: 107.5387408480965[0m
[37m[1m[2023-07-11 19:49:17,755][233954] Min Reward on eval: 107.5387408480965[0m
[37m[1m[2023-07-11 19:49:17,755][233954] Mean Reward across all agents: 107.5387408480965[0m
[37m[1m[2023-07-11 19:49:17,756][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:49:22,745][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:49:22,745][233954] Reward + Measures: [[ 99.40338194   0.89379996   0.85789996   0.80290002   0.07669999
    0.83762914]
 [112.01590108   0.90020001   0.78400004   0.7723       0.0294
    1.04896939]
 [ 37.8144794    0.54009998   0.52190006   0.38200003   0.39820004
    1.07845485]
 ...
 [ 96.31711804   0.82569999   0.75870007   0.69620001   0.13280001
    0.79160005]
 [109.38877346   0.88840002   0.77159995   0.80330002   0.0062
    0.99606848]
 [ 96.21873143   0.57080001   0.52990001   0.43650004   0.28550002
    1.21428132]][0m
[37m[1m[2023-07-11 19:49:22,746][233954] Max Reward on eval: 219.32649229448288[0m
[37m[1m[2023-07-11 19:49:22,746][233954] Min Reward on eval: -58.85839256811887[0m
[37m[1m[2023-07-11 19:49:22,746][233954] Mean Reward across all agents: 86.88938575193947[0m
[37m[1m[2023-07-11 19:49:22,746][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:49:22,750][233954] mean_value=-59.59978279467454, max_value=488.5648026866998[0m
[37m[1m[2023-07-11 19:49:22,753][233954] New mean coefficients: [[ 0.8282478  2.1316621  6.3346987  1.8736823  3.4947176 -4.9572854]][0m
[37m[1m[2023-07-11 19:49:22,754][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:49:31,720][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 19:49:31,720][233954] FPS: 428351.89[0m
[36m[2023-07-11 19:49:31,723][233954] itr=1429, itrs=2000, Progress: 71.45%[0m
[36m[2023-07-11 19:49:43,448][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 19:49:43,448][233954] FPS: 330785.86[0m
[36m[2023-07-11 19:49:47,691][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:49:47,692][233954] Reward + Measures: [[87.94450182  0.93963563  0.86735743  0.8661657   0.02473533  0.70181209]][0m
[37m[1m[2023-07-11 19:49:47,692][233954] Max Reward on eval: 87.94450181828364[0m
[37m[1m[2023-07-11 19:49:47,692][233954] Min Reward on eval: 87.94450181828364[0m
[37m[1m[2023-07-11 19:49:47,692][233954] Mean Reward across all agents: 87.94450181828364[0m
[37m[1m[2023-07-11 19:49:47,693][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:49:52,932][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:49:52,933][233954] Reward + Measures: [[21.9560597   0.90669996  0.87589997  0.79500002  0.11440001  0.61672604]
 [47.10836969  0.73789996  0.58389997  0.66520005  0.0424      1.13056231]
 [69.11492634  0.89390004  0.79710001  0.81120008  0.0191      1.00550616]
 ...
 [93.18282605  0.89359999  0.7942      0.78549999  0.0302      0.83137172]
 [33.9272387   0.89909995  0.84680003  0.8028      0.0761      0.69042885]
 [ 5.47394955  0.2933      0.27780002  0.2843      0.1602      2.19991851]][0m
[37m[1m[2023-07-11 19:49:52,933][233954] Max Reward on eval: 191.50740815695607[0m
[37m[1m[2023-07-11 19:49:52,933][233954] Min Reward on eval: -55.396291443286465[0m
[37m[1m[2023-07-11 19:49:52,934][233954] Mean Reward across all agents: 63.133482688612666[0m
[37m[1m[2023-07-11 19:49:52,934][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:49:52,937][233954] mean_value=-140.33049797350702, max_value=430.7216827237345[0m
[37m[1m[2023-07-11 19:49:52,939][233954] New mean coefficients: [[ 1.3437662   1.7584007   6.1276016   0.82506275  3.3914897  -4.838269  ]][0m
[37m[1m[2023-07-11 19:49:52,940][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:50:01,851][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 19:50:01,851][233954] FPS: 431014.43[0m
[36m[2023-07-11 19:50:01,853][233954] itr=1430, itrs=2000, Progress: 71.50%[0m
[37m[1m[2023-07-11 19:53:54,715][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001410[0m
[36m[2023-07-11 19:54:08,087][233954] train() took 12.07 seconds to complete[0m
[36m[2023-07-11 19:54:08,087][233954] FPS: 318188.97[0m
[36m[2023-07-11 19:54:12,316][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:54:12,317][233954] Reward + Measures: [[67.09746582  0.95345801  0.90238035  0.89909637  0.02260333  0.65514308]][0m
[37m[1m[2023-07-11 19:54:12,317][233954] Max Reward on eval: 67.09746581994443[0m
[37m[1m[2023-07-11 19:54:12,317][233954] Min Reward on eval: 67.09746581994443[0m
[37m[1m[2023-07-11 19:54:12,317][233954] Mean Reward across all agents: 67.09746581994443[0m
[37m[1m[2023-07-11 19:54:12,318][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:54:17,239][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:54:17,240][233954] Reward + Measures: [[-10.58376417   0.45469999   0.29749998   0.37120005   0.31469998
    2.30032849]
 [-21.06268548   0.58219999   0.39429998   0.52520001   0.32880002
    1.62679923]
 [ 50.14371785   0.44479999   0.30360001   0.40629998   0.28070003
    2.43092155]
 ...
 [ 25.56026304   0.45539999   0.33449998   0.41430002   0.32410002
    2.27659035]
 [  6.41396123   0.4474       0.47530004   0.39900002   0.44080001
    1.66899049]
 [  5.85100405   0.79320002   0.75800002   0.71150005   0.1181
    1.08270776]][0m
[37m[1m[2023-07-11 19:54:17,240][233954] Max Reward on eval: 127.01635982654989[0m
[37m[1m[2023-07-11 19:54:17,240][233954] Min Reward on eval: -240.96450230497868[0m
[37m[1m[2023-07-11 19:54:17,240][233954] Mean Reward across all agents: -3.306636174632335[0m
[37m[1m[2023-07-11 19:54:17,241][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:54:17,243][233954] mean_value=-261.57041531721984, max_value=292.6437023037077[0m
[37m[1m[2023-07-11 19:54:17,252][233954] New mean coefficients: [[ 0.5567128   1.2394273   6.0572987   0.07336146  1.1240311  -5.2910323 ]][0m
[37m[1m[2023-07-11 19:54:17,253][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:54:26,099][233954] train() took 8.84 seconds to complete[0m
[36m[2023-07-11 19:54:26,099][233954] FPS: 434195.58[0m
[36m[2023-07-11 19:54:26,102][233954] itr=1431, itrs=2000, Progress: 71.55%[0m
[36m[2023-07-11 19:54:37,961][233954] train() took 11.75 seconds to complete[0m
[36m[2023-07-11 19:54:37,962][233954] FPS: 326936.05[0m
[36m[2023-07-11 19:54:42,201][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:54:42,202][233954] Reward + Measures: [[52.0019183   0.96156073  0.92600602  0.92033964  0.024471    0.59320402]][0m
[37m[1m[2023-07-11 19:54:42,202][233954] Max Reward on eval: 52.00191829879349[0m
[37m[1m[2023-07-11 19:54:42,202][233954] Min Reward on eval: 52.00191829879349[0m
[37m[1m[2023-07-11 19:54:42,202][233954] Mean Reward across all agents: 52.00191829879349[0m
[37m[1m[2023-07-11 19:54:42,203][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:54:47,441][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:54:47,442][233954] Reward + Measures: [[  55.76617887    0.97640002    0.92150003    0.93269998    0.0151
     0.62783617]
 [ 151.92094337    0.96190006    0.93339998    0.9181        0.0436
     0.89614409]
 [ 168.14614015    0.56400001    0.68010002    0.38949999    0.4066
     1.42340457]
 ...
 [  13.07113273    0.49589998    0.65289998    0.39110002    0.4402
     0.98958206]
 [-102.65347908    0.68949997    0.51669997    0.72250003    0.1751
     0.898049  ]
 [  -4.29328254    0.2491        0.36250001    0.25830004    0.32440001
     1.47947836]][0m
[37m[1m[2023-07-11 19:54:47,442][233954] Max Reward on eval: 256.7072601064574[0m
[37m[1m[2023-07-11 19:54:47,442][233954] Min Reward on eval: -139.3512299428694[0m
[37m[1m[2023-07-11 19:54:47,443][233954] Mean Reward across all agents: 51.17845563413993[0m
[37m[1m[2023-07-11 19:54:47,443][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:54:47,447][233954] mean_value=-418.9581440699241, max_value=497.61450496897385[0m
[37m[1m[2023-07-11 19:54:47,450][233954] New mean coefficients: [[-1.8302908   2.1063373   6.544316    1.3313711   0.48874086 -5.438146  ]][0m
[37m[1m[2023-07-11 19:54:47,451][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:54:56,345][233954] train() took 8.89 seconds to complete[0m
[36m[2023-07-11 19:54:56,345][233954] FPS: 431804.94[0m
[36m[2023-07-11 19:54:56,348][233954] itr=1432, itrs=2000, Progress: 71.60%[0m
[36m[2023-07-11 19:55:08,101][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 19:55:08,101][233954] FPS: 330013.57[0m
[36m[2023-07-11 19:55:12,408][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:55:12,409][233954] Reward + Measures: [[50.86146301  0.96717173  0.93576592  0.93021792  0.02039933  0.61226547]][0m
[37m[1m[2023-07-11 19:55:12,409][233954] Max Reward on eval: 50.86146300625149[0m
[37m[1m[2023-07-11 19:55:12,409][233954] Min Reward on eval: 50.86146300625149[0m
[37m[1m[2023-07-11 19:55:12,410][233954] Mean Reward across all agents: 50.86146300625149[0m
[37m[1m[2023-07-11 19:55:12,410][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:55:17,375][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:55:17,376][233954] Reward + Measures: [[ 98.09172583   0.91479999   0.81339997   0.83120006   0.0173
    0.83048469]
 [195.52288294   0.94130003   0.85890007   0.8865       0.0043
    1.28829002]
 [159.18470202   0.95590001   0.89309996   0.88740009   0.0334
    1.21689475]
 ...
 [ 41.91895585   0.63700002   0.89239997   0.64770001   0.52829999
    1.22966301]
 [-14.13086946   0.51010007   0.33630002   0.54500002   0.16870001
    1.35831988]
 [ 95.3561456    0.86790001   0.7464       0.82439995   0.13710001
    1.58212662]][0m
[37m[1m[2023-07-11 19:55:17,376][233954] Max Reward on eval: 288.86551379286686[0m
[37m[1m[2023-07-11 19:55:17,376][233954] Min Reward on eval: -44.422889298343215[0m
[37m[1m[2023-07-11 19:55:17,376][233954] Mean Reward across all agents: 74.33935855025501[0m
[37m[1m[2023-07-11 19:55:17,377][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:55:17,381][233954] mean_value=-164.6334505461671, max_value=729.2812778769667[0m
[37m[1m[2023-07-11 19:55:17,383][233954] New mean coefficients: [[-3.286276    2.4479268   6.4549017   0.60826766 -2.2440817  -5.4933586 ]][0m
[37m[1m[2023-07-11 19:55:17,384][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:55:26,381][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 19:55:26,382][233954] FPS: 426879.57[0m
[36m[2023-07-11 19:55:26,384][233954] itr=1433, itrs=2000, Progress: 71.65%[0m
[36m[2023-07-11 19:55:37,938][233954] train() took 11.44 seconds to complete[0m
[36m[2023-07-11 19:55:37,939][233954] FPS: 335755.06[0m
[36m[2023-07-11 19:55:42,154][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:55:42,155][233954] Reward + Measures: [[51.57062395  0.96993995  0.93962973  0.93493408  0.01702667  0.61159039]][0m
[37m[1m[2023-07-11 19:55:42,155][233954] Max Reward on eval: 51.57062395463358[0m
[37m[1m[2023-07-11 19:55:42,155][233954] Min Reward on eval: 51.57062395463358[0m
[37m[1m[2023-07-11 19:55:42,156][233954] Mean Reward across all agents: 51.57062395463358[0m
[37m[1m[2023-07-11 19:55:42,156][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:55:47,110][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:55:47,111][233954] Reward + Measures: [[ 87.46424503   0.74000001   0.60760003   0.55990005   0.0711
    1.08120215]
 [ 34.66271694   0.66870004   0.52810001   0.52749997   0.28140002
    1.16113532]
 [278.22481824   0.93650001   0.90249997   0.89490002   0.0789
    1.39918995]
 ...
 [ -0.1708723    0.3969       0.49300003   0.30240002   0.4384
    1.43844008]
 [232.99679446   0.98430008   0.97040004   0.95389998   0.0103
    1.36385977]
 [ 78.91204192   0.49569997   0.57930005   0.42180005   0.29239997
    1.12274337]][0m
[37m[1m[2023-07-11 19:55:47,111][233954] Max Reward on eval: 561.1571769984905[0m
[37m[1m[2023-07-11 19:55:47,111][233954] Min Reward on eval: -226.52499680668114[0m
[37m[1m[2023-07-11 19:55:47,112][233954] Mean Reward across all agents: 61.763617344458616[0m
[37m[1m[2023-07-11 19:55:47,112][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:55:47,115][233954] mean_value=-125.37567742647599, max_value=385.4886730228576[0m
[37m[1m[2023-07-11 19:55:47,118][233954] New mean coefficients: [[-2.6271417   2.3468726   6.1329455   0.06183529 -1.1653697  -5.1980367 ]][0m
[37m[1m[2023-07-11 19:55:47,119][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:55:56,085][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 19:55:56,086][233954] FPS: 428334.26[0m
[36m[2023-07-11 19:55:56,088][233954] itr=1434, itrs=2000, Progress: 71.70%[0m
[36m[2023-07-11 19:56:07,753][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 19:56:07,753][233954] FPS: 332454.68[0m
[36m[2023-07-11 19:56:11,981][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:56:11,982][233954] Reward + Measures: [[40.73161558  0.97283763  0.94801694  0.94245338  0.018306    0.53956443]][0m
[37m[1m[2023-07-11 19:56:11,982][233954] Max Reward on eval: 40.7316155766585[0m
[37m[1m[2023-07-11 19:56:11,982][233954] Min Reward on eval: 40.7316155766585[0m
[37m[1m[2023-07-11 19:56:11,982][233954] Mean Reward across all agents: 40.7316155766585[0m
[37m[1m[2023-07-11 19:56:11,983][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:56:16,946][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:56:16,947][233954] Reward + Measures: [[ 84.30588701   0.78270006   0.75319999   0.579        0.22149999
    1.13259816]
 [163.38658286   0.42490003   0.42179999   0.31720001   0.34900004
    1.56349409]
 [146.23310806   0.49529997   0.53600001   0.25479999   0.48070002
    1.58444464]
 ...
 [173.2273102    0.4436       0.40770003   0.37450001   0.29260001
    1.37280226]
 [202.93893139   0.37729999   0.44380003   0.1807       0.46399999
    1.31762767]
 [ 71.90065101   0.4217       0.61540002   0.40580001   0.40079999
    1.07481289]][0m
[37m[1m[2023-07-11 19:56:16,947][233954] Max Reward on eval: 406.93103732038287[0m
[37m[1m[2023-07-11 19:56:16,947][233954] Min Reward on eval: -239.144996620249[0m
[37m[1m[2023-07-11 19:56:16,947][233954] Mean Reward across all agents: 62.05881049366072[0m
[37m[1m[2023-07-11 19:56:16,948][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:56:16,952][233954] mean_value=-173.20162312370886, max_value=264.6344847849948[0m
[37m[1m[2023-07-11 19:56:16,954][233954] New mean coefficients: [[-2.029934   1.4821975  5.938843  -0.6316254 -1.097401  -4.7893224]][0m
[37m[1m[2023-07-11 19:56:16,955][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:56:25,856][233954] train() took 8.90 seconds to complete[0m
[36m[2023-07-11 19:56:25,856][233954] FPS: 431506.48[0m
[36m[2023-07-11 19:56:25,858][233954] itr=1435, itrs=2000, Progress: 71.75%[0m
[36m[2023-07-11 19:56:37,825][233954] train() took 11.85 seconds to complete[0m
[36m[2023-07-11 19:56:37,825][233954] FPS: 324032.51[0m
[36m[2023-07-11 19:56:42,032][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:56:42,032][233954] Reward + Measures: [[34.7264522   0.97636634  0.95440167  0.95192462  0.01434533  0.48423356]][0m
[37m[1m[2023-07-11 19:56:42,033][233954] Max Reward on eval: 34.726452204836164[0m
[37m[1m[2023-07-11 19:56:42,033][233954] Min Reward on eval: 34.726452204836164[0m
[37m[1m[2023-07-11 19:56:42,033][233954] Mean Reward across all agents: 34.726452204836164[0m
[37m[1m[2023-07-11 19:56:42,033][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:56:47,017][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:56:47,018][233954] Reward + Measures: [[ 49.66747049   0.87589997   0.81630003   0.82959998   0.0209
    1.3522011 ]
 [144.13977578   0.56109995   0.56220001   0.65379995   0.18270002
    1.62352526]
 [110.12395693   0.64210004   0.67210001   0.67589998   0.22449999
    1.52331519]
 ...
 [106.94746103   0.74290007   0.62630004   0.86869997   0.0082
    1.51414752]
 [ 58.6124206    0.33329999   0.36480001   0.30210003   0.18640001
    1.86539042]
 [230.75005639   0.62779999   0.72070003   0.61430001   0.29890001
    1.73437905]][0m
[37m[1m[2023-07-11 19:56:47,018][233954] Max Reward on eval: 595.0926818887004[0m
[37m[1m[2023-07-11 19:56:47,018][233954] Min Reward on eval: -129.17252927334047[0m
[37m[1m[2023-07-11 19:56:47,019][233954] Mean Reward across all agents: 117.39801919477135[0m
[37m[1m[2023-07-11 19:56:47,019][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:56:47,026][233954] mean_value=-12.039625998874483, max_value=513.5397237012967[0m
[37m[1m[2023-07-11 19:56:47,028][233954] New mean coefficients: [[-2.7493634  1.423728   6.758571  -1.8242793 -2.378046  -5.541948 ]][0m
[37m[1m[2023-07-11 19:56:47,029][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:56:55,972][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 19:56:55,972][233954] FPS: 429604.69[0m
[36m[2023-07-11 19:56:55,975][233954] itr=1436, itrs=2000, Progress: 71.80%[0m
[36m[2023-07-11 19:57:07,971][233954] train() took 11.88 seconds to complete[0m
[36m[2023-07-11 19:57:07,971][233954] FPS: 323301.95[0m
[36m[2023-07-11 19:57:12,272][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:57:12,273][233954] Reward + Measures: [[38.46807015  0.97959131  0.95827198  0.95681328  0.01108233  0.50878757]][0m
[37m[1m[2023-07-11 19:57:12,273][233954] Max Reward on eval: 38.46807014700976[0m
[37m[1m[2023-07-11 19:57:12,273][233954] Min Reward on eval: 38.46807014700976[0m
[37m[1m[2023-07-11 19:57:12,273][233954] Mean Reward across all agents: 38.46807014700976[0m
[37m[1m[2023-07-11 19:57:12,274][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:57:17,220][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:57:17,220][233954] Reward + Measures: [[-43.07155119   0.37990001   0.28929999   0.34150001   0.29120001
    1.91375577]
 [-19.56673268   0.49130002   0.4632       0.3917       0.4007
    1.24552059]
 [-36.84849284   0.58429998   0.49460003   0.5212       0.36460003
    1.43600976]
 ...
 [ 69.54304596   0.65710002   0.75290006   0.55070001   0.54280001
    1.59696686]
 [  8.6300566    0.70390004   0.72550005   0.56360006   0.30310002
    1.00862086]
 [ 36.95145806   0.9465       0.91870004   0.87360001   0.03490001
    0.80039519]][0m
[37m[1m[2023-07-11 19:57:17,220][233954] Max Reward on eval: 243.1038589924574[0m
[37m[1m[2023-07-11 19:57:17,221][233954] Min Reward on eval: -81.59997653197497[0m
[37m[1m[2023-07-11 19:57:17,221][233954] Mean Reward across all agents: 46.24978144110607[0m
[37m[1m[2023-07-11 19:57:17,221][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:57:17,224][233954] mean_value=-225.89653727754194, max_value=368.9249751237678[0m
[37m[1m[2023-07-11 19:57:17,227][233954] New mean coefficients: [[-1.3639293  1.9261296  6.7515936 -0.7263758  0.6118181 -4.9146256]][0m
[37m[1m[2023-07-11 19:57:17,228][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:57:26,236][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 19:57:26,237][233954] FPS: 426322.04[0m
[36m[2023-07-11 19:57:26,239][233954] itr=1437, itrs=2000, Progress: 71.85%[0m
[36m[2023-07-11 19:57:37,876][233954] train() took 11.52 seconds to complete[0m
[36m[2023-07-11 19:57:37,877][233954] FPS: 333438.83[0m
[36m[2023-07-11 19:57:42,145][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:57:42,146][233954] Reward + Measures: [[36.62455403  0.98053801  0.96019799  0.95736635  0.01122533  0.46233204]][0m
[37m[1m[2023-07-11 19:57:42,146][233954] Max Reward on eval: 36.62455403337917[0m
[37m[1m[2023-07-11 19:57:42,146][233954] Min Reward on eval: 36.62455403337917[0m
[37m[1m[2023-07-11 19:57:42,146][233954] Mean Reward across all agents: 36.62455403337917[0m
[37m[1m[2023-07-11 19:57:42,147][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:57:47,434][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:57:47,435][233954] Reward + Measures: [[138.84000586   0.44660005   0.39320001   0.36590001   0.35139999
    1.54774737]
 [-82.28316796   0.39490005   0.31200001   0.33380002   0.1681
    1.92625356]
 [118.23016593   0.48249999   0.54070002   0.23120001   0.60720009
    1.05089414]
 ...
 [ 70.59317703   0.49950004   0.49990001   0.3493       0.4298
    1.12930739]
 [ 77.09601505   0.80629998   0.80030006   0.653        0.36760002
    0.74102491]
 [-76.32089284   0.67939997   0.61320001   0.48930001   0.29850003
    1.42590821]][0m
[37m[1m[2023-07-11 19:57:47,435][233954] Max Reward on eval: 253.69693371403264[0m
[37m[1m[2023-07-11 19:57:47,435][233954] Min Reward on eval: -133.11210579965262[0m
[37m[1m[2023-07-11 19:57:47,436][233954] Mean Reward across all agents: 48.970358869129605[0m
[37m[1m[2023-07-11 19:57:47,436][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:57:47,440][233954] mean_value=-170.69544836731367, max_value=400.4004529314137[0m
[37m[1m[2023-07-11 19:57:47,442][233954] New mean coefficients: [[-1.4952159   1.9253247   6.792102   -1.167237    0.25207102 -4.974831  ]][0m
[37m[1m[2023-07-11 19:57:47,443][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:57:56,419][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 19:57:56,419][233954] FPS: 427915.99[0m
[36m[2023-07-11 19:57:56,421][233954] itr=1438, itrs=2000, Progress: 71.90%[0m
[36m[2023-07-11 19:58:08,041][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 19:58:08,042][233954] FPS: 333737.26[0m
[36m[2023-07-11 19:58:12,275][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:58:12,276][233954] Reward + Measures: [[32.08088011  0.98282731  0.96367007  0.96204865  0.00812667  0.42526746]][0m
[37m[1m[2023-07-11 19:58:12,276][233954] Max Reward on eval: 32.080880109044585[0m
[37m[1m[2023-07-11 19:58:12,276][233954] Min Reward on eval: 32.080880109044585[0m
[37m[1m[2023-07-11 19:58:12,276][233954] Mean Reward across all agents: 32.080880109044585[0m
[37m[1m[2023-07-11 19:58:12,276][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:58:17,306][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:58:17,312][233954] Reward + Measures: [[ 68.44034649   0.67690003   0.65700001   0.61600006   0.55139995
    1.50499678]
 [-10.72607866   0.1092       0.13590001   0.0948       0.1013
    2.34092498]
 [ 60.99388518   0.384        0.3804       0.28090003   0.19700001
    1.50703084]
 ...
 [ 26.34308851   0.88300002   0.76179999   0.79689997   0.19490001
    0.59769171]
 [ 35.56668437   0.1506       0.17         0.1301       0.10580001
    1.8651123 ]
 [-39.31702427   0.50999999   0.50690001   0.42389998   0.29660001
    1.17943466]][0m
[37m[1m[2023-07-11 19:58:17,312][233954] Max Reward on eval: 326.9318771173712[0m
[37m[1m[2023-07-11 19:58:17,313][233954] Min Reward on eval: -128.0175595334731[0m
[37m[1m[2023-07-11 19:58:17,313][233954] Mean Reward across all agents: 88.88810128769295[0m
[37m[1m[2023-07-11 19:58:17,313][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:58:17,319][233954] mean_value=-219.9055904979747, max_value=341.7034826788911[0m
[37m[1m[2023-07-11 19:58:17,322][233954] New mean coefficients: [[-1.042174   2.011238   6.9920063 -1.518364   0.9406972 -5.0873013]][0m
[37m[1m[2023-07-11 19:58:17,323][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:58:26,348][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 19:58:26,349][233954] FPS: 425544.86[0m
[36m[2023-07-11 19:58:26,351][233954] itr=1439, itrs=2000, Progress: 71.95%[0m
[36m[2023-07-11 19:58:38,070][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 19:58:38,070][233954] FPS: 330868.35[0m
[36m[2023-07-11 19:58:42,429][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:58:42,429][233954] Reward + Measures: [[29.61712765  0.98369431  0.96725428  0.96398306  0.007178    0.38306978]][0m
[37m[1m[2023-07-11 19:58:42,430][233954] Max Reward on eval: 29.617127650491877[0m
[37m[1m[2023-07-11 19:58:42,430][233954] Min Reward on eval: 29.617127650491877[0m
[37m[1m[2023-07-11 19:58:42,430][233954] Mean Reward across all agents: 29.617127650491877[0m
[37m[1m[2023-07-11 19:58:42,430][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:58:47,455][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 19:58:47,455][233954] Reward + Measures: [[ 71.63886857   0.89560002   0.67820001   0.78509998   0.0402
    1.12431991]
 [  1.35038558   0.89000005   0.90980005   0.80489999   0.1071
    0.7313785 ]
 [-61.19003486   0.3175       0.50400001   0.24130002   0.45840001
    1.54451311]
 ...
 [142.44217832   0.46559998   0.63009995   0.35260001   0.39850003
    1.2129184 ]
 [-27.29094923   0.38909999   0.77680004   0.2762       0.66150004
    1.04723454]
 [138.86054795   0.47819996   0.73100007   0.0928       0.62540001
    0.87133139]][0m
[37m[1m[2023-07-11 19:58:47,456][233954] Max Reward on eval: 157.77870465405286[0m
[37m[1m[2023-07-11 19:58:47,456][233954] Min Reward on eval: -226.0857296318747[0m
[37m[1m[2023-07-11 19:58:47,456][233954] Mean Reward across all agents: 28.696506644351775[0m
[37m[1m[2023-07-11 19:58:47,456][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 19:58:47,460][233954] mean_value=-131.15193869928234, max_value=320.00496484906546[0m
[37m[1m[2023-07-11 19:58:47,463][233954] New mean coefficients: [[-0.19636393  1.9537251   6.6759987  -1.1041794   1.9019604  -4.8608956 ]][0m
[37m[1m[2023-07-11 19:58:47,463][233954] Moving the mean solution point...[0m
[36m[2023-07-11 19:58:56,506][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 19:58:56,507][233954] FPS: 424722.27[0m
[36m[2023-07-11 19:58:56,509][233954] itr=1440, itrs=2000, Progress: 72.00%[0m
[37m[1m[2023-07-11 20:02:52,992][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001420[0m
[36m[2023-07-11 20:03:06,061][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 20:03:06,061][233954] FPS: 329974.04[0m
[36m[2023-07-11 20:03:10,280][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:03:10,281][233954] Reward + Measures: [[26.27995486  0.98320359  0.96591437  0.96361732  0.00845567  0.32838953]][0m
[37m[1m[2023-07-11 20:03:10,281][233954] Max Reward on eval: 26.279954862339103[0m
[37m[1m[2023-07-11 20:03:10,281][233954] Min Reward on eval: 26.279954862339103[0m
[37m[1m[2023-07-11 20:03:10,281][233954] Mean Reward across all agents: 26.279954862339103[0m
[37m[1m[2023-07-11 20:03:10,282][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:03:15,222][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:03:15,223][233954] Reward + Measures: [[   5.11061177    0.88030005    0.6936        0.83649999    0.13770001
     0.54280823]
 [  -6.41977056    0.6103        0.75309998    0.31380001    0.61709994
     0.72688067]
 [ 162.13500026    0.42220002    0.57010001    0.2771        0.39430004
     1.25081837]
 ...
 [  32.48107341    0.61709994    0.77990001    0.28450003    0.56739998
     0.93345779]
 [  -5.61677376    0.54000002    0.69540006    0.44080001    0.58230001
     0.80324787]
 [-143.10057068    0.5097        0.76540005    0.41339999    0.67270005
     0.74476826]][0m
[37m[1m[2023-07-11 20:03:15,223][233954] Max Reward on eval: 242.19923477824779[0m
[37m[1m[2023-07-11 20:03:15,224][233954] Min Reward on eval: -172.11908216690645[0m
[37m[1m[2023-07-11 20:03:15,224][233954] Mean Reward across all agents: 14.342692044473985[0m
[37m[1m[2023-07-11 20:03:15,224][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:03:15,234][233954] mean_value=-88.12394403192424, max_value=418.56523931218993[0m
[37m[1m[2023-07-11 20:03:15,238][233954] New mean coefficients: [[-0.3074806   0.5726383   6.0279684  -2.1498067   0.12982535 -5.3403544 ]][0m
[37m[1m[2023-07-11 20:03:15,239][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:03:24,245][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 20:03:24,245][233954] FPS: 426457.55[0m
[36m[2023-07-11 20:03:24,247][233954] itr=1441, itrs=2000, Progress: 72.05%[0m
[36m[2023-07-11 20:03:35,900][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 20:03:35,905][233954] FPS: 332761.58[0m
[36m[2023-07-11 20:03:40,152][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:03:40,152][233954] Reward + Measures: [[24.2811057   0.98588294  0.97095132  0.96783465  0.005849    0.31084523]][0m
[37m[1m[2023-07-11 20:03:40,152][233954] Max Reward on eval: 24.281105699946536[0m
[37m[1m[2023-07-11 20:03:40,153][233954] Min Reward on eval: 24.281105699946536[0m
[37m[1m[2023-07-11 20:03:40,153][233954] Mean Reward across all agents: 24.281105699946536[0m
[37m[1m[2023-07-11 20:03:40,153][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:03:45,307][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:03:45,308][233954] Reward + Measures: [[-22.80496389   0.56010002   0.36630002   0.45360002   0.2678
    1.65150344]
 [  0.07186244   0.7001       0.49699998   0.58090001   0.2115
    0.96939343]
 [-31.68898564   0.63770002   0.79270005   0.48530003   0.491
    0.67695677]
 ...
 [-55.05887865   0.67620003   0.69020003   0.8193       0.0135
    0.65252608]
 [-23.51414125   0.75100005   0.60939997   0.64749998   0.08270001
    0.85781366]
 [128.39608003   0.34489998   0.76770002   0.40359998   0.61899996
    1.24600792]][0m
[37m[1m[2023-07-11 20:03:45,308][233954] Max Reward on eval: 246.57608792878455[0m
[37m[1m[2023-07-11 20:03:45,309][233954] Min Reward on eval: -163.0725536653772[0m
[37m[1m[2023-07-11 20:03:45,309][233954] Mean Reward across all agents: 20.97615899853542[0m
[37m[1m[2023-07-11 20:03:45,309][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:03:45,312][233954] mean_value=-163.75379284577406, max_value=354.1615358078359[0m
[37m[1m[2023-07-11 20:03:45,315][233954] New mean coefficients: [[ 0.94128036  0.06110823  5.913728   -1.6282773   0.59162647 -4.5567594 ]][0m
[37m[1m[2023-07-11 20:03:45,316][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:03:54,313][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 20:03:54,313][233954] FPS: 426888.48[0m
[36m[2023-07-11 20:03:54,315][233954] itr=1442, itrs=2000, Progress: 72.10%[0m
[36m[2023-07-11 20:04:06,211][233954] train() took 11.78 seconds to complete[0m
[36m[2023-07-11 20:04:06,212][233954] FPS: 325962.73[0m
[36m[2023-07-11 20:04:10,409][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:04:10,409][233954] Reward + Measures: [[24.04060574  0.98518801  0.96778369  0.96626568  0.00714767  0.2761189 ]][0m
[37m[1m[2023-07-11 20:04:10,409][233954] Max Reward on eval: 24.040605740245336[0m
[37m[1m[2023-07-11 20:04:10,410][233954] Min Reward on eval: 24.040605740245336[0m
[37m[1m[2023-07-11 20:04:10,410][233954] Mean Reward across all agents: 24.040605740245336[0m
[37m[1m[2023-07-11 20:04:10,410][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:04:15,408][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:04:15,408][233954] Reward + Measures: [[  2.64713938   0.33850002   0.33650002   0.26109999   0.23980001
    1.3414228 ]
 [  9.54931309   0.68879998   0.72050011   0.58780003   0.1754
    1.56519699]
 [-29.04928935   0.57590002   0.59450001   0.49020004   0.51239997
    0.9394207 ]
 ...
 [-14.67813487   0.66040003   0.3964       0.70010006   0.36789998
    1.22437751]
 [ -9.47097681   0.66710007   0.5341       0.57420009   0.55419999
    0.9725219 ]
 [ 84.37839084   0.75269997   0.73699993   0.64630002   0.15450001
    1.67929113]][0m
[37m[1m[2023-07-11 20:04:15,408][233954] Max Reward on eval: 207.47131918165832[0m
[37m[1m[2023-07-11 20:04:15,409][233954] Min Reward on eval: -225.49842024142853[0m
[37m[1m[2023-07-11 20:04:15,409][233954] Mean Reward across all agents: 24.608968363621184[0m
[37m[1m[2023-07-11 20:04:15,409][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:04:15,412][233954] mean_value=-274.19162106820255, max_value=485.38542982441436[0m
[37m[1m[2023-07-11 20:04:15,415][233954] New mean coefficients: [[ 1.0174278   0.24273238  5.927148   -0.56992507  1.0066704  -4.6123567 ]][0m
[37m[1m[2023-07-11 20:04:15,416][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:04:24,471][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 20:04:24,471][233954] FPS: 424158.99[0m
[36m[2023-07-11 20:04:24,473][233954] itr=1443, itrs=2000, Progress: 72.15%[0m
[36m[2023-07-11 20:04:36,182][233954] train() took 11.59 seconds to complete[0m
[36m[2023-07-11 20:04:36,183][233954] FPS: 331226.68[0m
[36m[2023-07-11 20:04:40,471][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:04:40,471][233954] Reward + Measures: [[29.87976602  0.98833567  0.97604066  0.96917397  0.00309467  0.28783581]][0m
[37m[1m[2023-07-11 20:04:40,471][233954] Max Reward on eval: 29.879766024961967[0m
[37m[1m[2023-07-11 20:04:40,471][233954] Min Reward on eval: 29.879766024961967[0m
[37m[1m[2023-07-11 20:04:40,472][233954] Mean Reward across all agents: 29.879766024961967[0m
[37m[1m[2023-07-11 20:04:40,472][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:04:45,446][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:04:45,447][233954] Reward + Measures: [[ 38.45155813   0.74080002   0.44799995   0.63720006   0.0566
    1.4889549 ]
 [ 19.17440784   0.71070004   0.81070006   0.69090003   0.63460004
    0.81307358]
 [171.4705386    0.40420005   0.44860002   0.41599998   0.37530002
    1.26803946]
 ...
 [ 44.64954637   0.59990001   0.76160002   0.63140005   0.61049998
    1.24038124]
 [ 57.68406928   0.75189996   0.68370003   0.75669998   0.18669999
    0.77357244]
 [  5.0198286    0.85079998   0.86300004   0.7177       0.1944
    0.79564995]][0m
[37m[1m[2023-07-11 20:04:45,447][233954] Max Reward on eval: 291.21260356064886[0m
[37m[1m[2023-07-11 20:04:45,447][233954] Min Reward on eval: -173.1640784829855[0m
[37m[1m[2023-07-11 20:04:45,447][233954] Mean Reward across all agents: 52.852161778491244[0m
[37m[1m[2023-07-11 20:04:45,447][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:04:45,454][233954] mean_value=-73.00807243341903, max_value=418.78686521465534[0m
[37m[1m[2023-07-11 20:04:45,456][233954] New mean coefficients: [[ 1.3966444   0.25847834  5.7411046   0.5921861   1.6486835  -3.959635  ]][0m
[37m[1m[2023-07-11 20:04:45,457][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:04:54,411][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 20:04:54,411][233954] FPS: 428969.94[0m
[36m[2023-07-11 20:04:54,413][233954] itr=1444, itrs=2000, Progress: 72.20%[0m
[36m[2023-07-11 20:05:06,149][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 20:05:06,149][233954] FPS: 330518.06[0m
[36m[2023-07-11 20:05:10,440][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:05:10,440][233954] Reward + Measures: [[136.03169772   0.96226537   0.94980437   0.9292236    0.010468
    0.78240478]][0m
[37m[1m[2023-07-11 20:05:10,440][233954] Max Reward on eval: 136.0316977242464[0m
[37m[1m[2023-07-11 20:05:10,441][233954] Min Reward on eval: 136.0316977242464[0m
[37m[1m[2023-07-11 20:05:10,441][233954] Mean Reward across all agents: 136.0316977242464[0m
[37m[1m[2023-07-11 20:05:10,441][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:05:15,409][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:05:15,410][233954] Reward + Measures: [[-20.20366061   0.30500004   0.32619998   0.2572       0.2106
    2.16757274]
 [103.31843948   0.93549997   0.93589991   0.89090008   0.0611
    0.88814336]
 [148.84908199   0.96990007   0.89410001   0.79100001   0.0031
    1.26584244]
 ...
 [ 18.39593492   0.77720004   0.6257       0.72170001   0.0107
    1.18252838]
 [ 34.61579849   0.60270005   0.55199999   0.54440004   0.59420007
    1.53632891]
 [141.39257714   0.91449994   0.8276       0.80500001   0.0183
    0.86419755]][0m
[37m[1m[2023-07-11 20:05:15,410][233954] Max Reward on eval: 571.2924919273704[0m
[37m[1m[2023-07-11 20:05:15,410][233954] Min Reward on eval: -268.55336761660874[0m
[37m[1m[2023-07-11 20:05:15,410][233954] Mean Reward across all agents: 37.09004571061836[0m
[37m[1m[2023-07-11 20:05:15,411][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:05:15,416][233954] mean_value=-330.4396490162633, max_value=679.3097769701387[0m
[37m[1m[2023-07-11 20:05:15,418][233954] New mean coefficients: [[ 1.1227832   0.18089795  5.6761336   0.7857789  -0.32123065 -4.254838  ]][0m
[37m[1m[2023-07-11 20:05:15,419][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:05:24,390][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 20:05:24,390][233954] FPS: 428154.31[0m
[36m[2023-07-11 20:05:24,392][233954] itr=1445, itrs=2000, Progress: 72.25%[0m
[36m[2023-07-11 20:05:36,046][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 20:05:36,046][233954] FPS: 332765.40[0m
[36m[2023-07-11 20:05:40,250][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:05:40,250][233954] Reward + Measures: [[12.48058603  0.9765594   0.94340461  0.89707339  0.34906766  0.81674689]][0m
[37m[1m[2023-07-11 20:05:40,250][233954] Max Reward on eval: 12.480586034783142[0m
[37m[1m[2023-07-11 20:05:40,251][233954] Min Reward on eval: 12.480586034783142[0m
[37m[1m[2023-07-11 20:05:40,251][233954] Mean Reward across all agents: 12.480586034783142[0m
[37m[1m[2023-07-11 20:05:40,251][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:05:45,248][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:05:45,249][233954] Reward + Measures: [[  81.69836629    0.81230003    0.76130003    0.73570001    0.0202
     1.03367829]
 [ -77.20321938    0.72430003    0.71399993    0.59779996    0.21259999
     1.61462617]
 [  54.03671934    0.94349998    0.92160004    0.87989998    0.0361
     0.66404271]
 ...
 [  10.57303696    0.741         0.68080002    0.64870006    0.1355
     2.09794021]
 [ -73.71421624    0.86959994    0.79079998    0.84300005    0.0515
     1.66633093]
 [-208.90487853    0.77210003    0.76960003    0.82539999    0.0826
     2.01360035]][0m
[37m[1m[2023-07-11 20:05:45,249][233954] Max Reward on eval: 292.043260575016[0m
[37m[1m[2023-07-11 20:05:45,250][233954] Min Reward on eval: -305.7294445203617[0m
[37m[1m[2023-07-11 20:05:45,250][233954] Mean Reward across all agents: 23.792680537515107[0m
[37m[1m[2023-07-11 20:05:45,250][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:05:45,254][233954] mean_value=-149.39713443530812, max_value=671.7171571622766[0m
[37m[1m[2023-07-11 20:05:45,256][233954] New mean coefficients: [[ 1.575006   -0.29863086  5.697684   -0.12899351 -0.66887087 -3.996963  ]][0m
[37m[1m[2023-07-11 20:05:45,257][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:05:54,375][233954] train() took 9.12 seconds to complete[0m
[36m[2023-07-11 20:05:54,375][233954] FPS: 421250.00[0m
[36m[2023-07-11 20:05:54,377][233954] itr=1446, itrs=2000, Progress: 72.30%[0m
[36m[2023-07-11 20:06:06,191][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 20:06:06,191][233954] FPS: 328280.24[0m
[36m[2023-07-11 20:06:10,498][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:06:10,498][233954] Reward + Measures: [[44.71555871  0.96029294  0.94294131  0.95197427  0.79062361  1.09232008]][0m
[37m[1m[2023-07-11 20:06:10,498][233954] Max Reward on eval: 44.71555871007268[0m
[37m[1m[2023-07-11 20:06:10,499][233954] Min Reward on eval: 44.71555871007268[0m
[37m[1m[2023-07-11 20:06:10,499][233954] Mean Reward across all agents: 44.71555871007268[0m
[37m[1m[2023-07-11 20:06:10,499][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:06:15,517][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:06:15,517][233954] Reward + Measures: [[-10.80812677   0.6688       0.46760002   0.72760004   0.19710001
    1.90705764]
 [  0.12471109   0.96520007   0.63550001   0.90270007   0.0083
    1.07296443]
 [ 24.78924861   0.8391       0.84400004   0.72399998   0.465
    1.15146101]
 ...
 [ -5.5557368    0.9443       0.7299       0.83000004   0.32440001
    1.16422522]
 [-14.54074361   0.88370001   0.75349998   0.94249994   0.0041
    1.73052967]
 [125.50550237   0.60970002   0.2836       0.66430002   0.44409999
    2.00467181]][0m
[37m[1m[2023-07-11 20:06:15,518][233954] Max Reward on eval: 406.6217727707699[0m
[37m[1m[2023-07-11 20:06:15,518][233954] Min Reward on eval: -299.58535766927525[0m
[37m[1m[2023-07-11 20:06:15,518][233954] Mean Reward across all agents: 38.397640499352725[0m
[37m[1m[2023-07-11 20:06:15,518][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:06:15,525][233954] mean_value=-76.9295111055771, max_value=582.1812787020579[0m
[37m[1m[2023-07-11 20:06:15,528][233954] New mean coefficients: [[ 2.4600816  -0.14206348  5.692601   -0.5921817   0.25203735 -3.74844   ]][0m
[37m[1m[2023-07-11 20:06:15,529][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:06:24,437][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 20:06:24,438][233954] FPS: 431117.96[0m
[36m[2023-07-11 20:06:24,440][233954] itr=1447, itrs=2000, Progress: 72.35%[0m
[36m[2023-07-11 20:06:36,038][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 20:06:36,038][233954] FPS: 334365.62[0m
[36m[2023-07-11 20:06:40,401][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:06:40,401][233954] Reward + Measures: [[9.03022569 0.99062866 0.99459195 0.95596969 0.99052894 1.26974726]][0m
[37m[1m[2023-07-11 20:06:40,401][233954] Max Reward on eval: 9.030225689277984[0m
[37m[1m[2023-07-11 20:06:40,402][233954] Min Reward on eval: 9.030225689277984[0m
[37m[1m[2023-07-11 20:06:40,402][233954] Mean Reward across all agents: 9.030225689277984[0m
[37m[1m[2023-07-11 20:06:40,402][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:06:45,692][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:06:45,693][233954] Reward + Measures: [[ 32.51041685   0.18730001   0.1847       0.1521       0.12119999
    2.21838069]
 [ 41.45603913   0.27270001   0.31230003   0.25780001   0.24519999
    1.82874358]
 [ 47.14893952   0.23140001   0.2445       0.22259998   0.1899
    1.93451524]
 ...
 [-16.36560158   0.1877       0.18050002   0.15440001   0.15260001
    1.8488735 ]
 [ 64.226195     0.23980001   0.2516       0.1742       0.2146
    1.54407084]
 [-12.38398856   0.207        0.2131       0.18910001   0.17569999
    1.98050368]][0m
[37m[1m[2023-07-11 20:06:45,693][233954] Max Reward on eval: 187.4978475678654[0m
[37m[1m[2023-07-11 20:06:45,693][233954] Min Reward on eval: -294.8268795622047[0m
[37m[1m[2023-07-11 20:06:45,694][233954] Mean Reward across all agents: -0.7810644704876663[0m
[37m[1m[2023-07-11 20:06:45,694][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:06:45,697][233954] mean_value=-1231.2608709963451, max_value=468.38654650441606[0m
[37m[1m[2023-07-11 20:06:45,700][233954] New mean coefficients: [[ 2.3686357   0.08154732  3.4732246  -0.23356646 -0.90907174 -2.8118677 ]][0m
[37m[1m[2023-07-11 20:06:45,701][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:06:54,738][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 20:06:54,739][233954] FPS: 424973.84[0m
[36m[2023-07-11 20:06:54,741][233954] itr=1448, itrs=2000, Progress: 72.40%[0m
[36m[2023-07-11 20:07:06,545][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 20:07:06,545][233954] FPS: 328645.15[0m
[36m[2023-07-11 20:07:10,775][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:07:10,775][233954] Reward + Measures: [[-6.99292874  0.97165328  0.66928232  0.95280975  0.49765038  1.47609532]][0m
[37m[1m[2023-07-11 20:07:10,775][233954] Max Reward on eval: -6.992928736837193[0m
[37m[1m[2023-07-11 20:07:10,776][233954] Min Reward on eval: -6.992928736837193[0m
[37m[1m[2023-07-11 20:07:10,776][233954] Mean Reward across all agents: -6.992928736837193[0m
[37m[1m[2023-07-11 20:07:10,776][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:07:15,734][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:07:15,734][233954] Reward + Measures: [[153.55113791   0.75480002   0.5151       0.74660003   0.3285
    1.84960806]
 [ 31.13250614   0.59780002   0.52080005   0.46760002   0.1061
    1.86301649]
 [ 49.63415344   0.55759996   0.47990003   0.46479997   0.0698
    2.09523749]
 ...
 [-68.09635328   0.97369999   0.95839995   0.96170008   0.0037
    2.07732177]
 [ 53.67696958   0.94499999   0.3436       0.91300005   0.24510001
    1.95441246]
 [126.47300585   0.4973       0.34819999   0.43190002   0.1594
    2.05950546]][0m
[37m[1m[2023-07-11 20:07:15,734][233954] Max Reward on eval: 613.1284084452316[0m
[37m[1m[2023-07-11 20:07:15,735][233954] Min Reward on eval: -534.1377893029712[0m
[37m[1m[2023-07-11 20:07:15,735][233954] Mean Reward across all agents: 58.40106935139452[0m
[37m[1m[2023-07-11 20:07:15,735][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:07:15,739][233954] mean_value=-327.84414340859325, max_value=655.7918831163458[0m
[37m[1m[2023-07-11 20:07:15,742][233954] New mean coefficients: [[ 2.2447703   0.4648502   3.5008836  -0.29272422 -0.45863223 -2.8382268 ]][0m
[37m[1m[2023-07-11 20:07:15,743][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:07:24,771][233954] train() took 9.03 seconds to complete[0m
[36m[2023-07-11 20:07:24,771][233954] FPS: 425426.28[0m
[36m[2023-07-11 20:07:24,774][233954] itr=1449, itrs=2000, Progress: 72.45%[0m
[36m[2023-07-11 20:07:36,414][233954] train() took 11.53 seconds to complete[0m
[36m[2023-07-11 20:07:36,414][233954] FPS: 333153.49[0m
[36m[2023-07-11 20:07:40,689][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:07:40,690][233954] Reward + Measures: [[58.80395117  0.94613832  0.83565235  0.9405883   0.05711     1.57325685]][0m
[37m[1m[2023-07-11 20:07:40,690][233954] Max Reward on eval: 58.80395116621074[0m
[37m[1m[2023-07-11 20:07:40,690][233954] Min Reward on eval: 58.80395116621074[0m
[37m[1m[2023-07-11 20:07:40,691][233954] Mean Reward across all agents: 58.80395116621074[0m
[37m[1m[2023-07-11 20:07:40,691][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:07:45,711][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:07:45,711][233954] Reward + Measures: [[ 35.38195942   0.98070002   0.65869999   0.87730008   0.0137
    1.6384964 ]
 [ 61.40759991   0.88370001   0.5636       0.81560004   0.01
    2.10348082]
 [ 39.46065019   0.88309997   0.1117       0.88619995   0.1551
    2.78972793]
 ...
 [-54.99735583   0.52780002   0.34720001   0.55970001   0.2431
    1.77633798]
 [ 21.36370898   0.93359995   0.59110004   0.84680003   0.0111
    1.57116067]
 [-25.62872529   0.99500006   0.0196       0.98890001   0.49499997
    2.23619628]][0m
[37m[1m[2023-07-11 20:07:45,712][233954] Max Reward on eval: 406.1353702545166[0m
[37m[1m[2023-07-11 20:07:45,712][233954] Min Reward on eval: -197.93516347967088[0m
[37m[1m[2023-07-11 20:07:45,712][233954] Mean Reward across all agents: 31.80796280786053[0m
[37m[1m[2023-07-11 20:07:45,712][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:07:45,720][233954] mean_value=-2.5986914820198943, max_value=658.8185195764352[0m
[37m[1m[2023-07-11 20:07:45,722][233954] New mean coefficients: [[ 2.829352   -0.5143577   2.835832   -0.02920517 -0.32488033 -2.5970798 ]][0m
[37m[1m[2023-07-11 20:07:45,723][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:07:54,780][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 20:07:54,780][233954] FPS: 424075.34[0m
[36m[2023-07-11 20:07:54,782][233954] itr=1450, itrs=2000, Progress: 72.50%[0m
[37m[1m[2023-07-11 20:12:02,832][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001430[0m
[36m[2023-07-11 20:12:16,344][233954] train() took 12.08 seconds to complete[0m
[36m[2023-07-11 20:12:16,344][233954] FPS: 317788.82[0m
[36m[2023-07-11 20:12:20,552][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:12:20,553][233954] Reward + Measures: [[23.71035393  0.94845295  0.9369843   0.92438233  0.21683565  1.4899919 ]][0m
[37m[1m[2023-07-11 20:12:20,553][233954] Max Reward on eval: 23.71035393368115[0m
[37m[1m[2023-07-11 20:12:20,553][233954] Min Reward on eval: 23.71035393368115[0m
[37m[1m[2023-07-11 20:12:20,553][233954] Mean Reward across all agents: 23.71035393368115[0m
[37m[1m[2023-07-11 20:12:20,554][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:12:25,463][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:12:25,464][233954] Reward + Measures: [[194.95104572   0.61220002   0.62770003   0.5808       0.0731
    1.96714652]
 [-68.59254978   0.49829999   0.34290001   0.42519999   0.21369998
    2.35074234]
 [-21.21238763   0.30579999   0.31440002   0.23450001   0.16440001
    2.12890816]
 ...
 [ 22.16756069   0.75309998   0.67500001   0.6814       0.12850001
    1.42188907]
 [-95.97324135   0.70609999   0.60020006   0.64090002   0.1224
    1.82164419]
 [-84.55249646   0.50579995   0.3915       0.4515       0.18249999
    1.42776763]][0m
[37m[1m[2023-07-11 20:12:25,464][233954] Max Reward on eval: 634.6413688614499[0m
[37m[1m[2023-07-11 20:12:25,465][233954] Min Reward on eval: -162.7664680663962[0m
[37m[1m[2023-07-11 20:12:25,465][233954] Mean Reward across all agents: 41.1883547459191[0m
[37m[1m[2023-07-11 20:12:25,465][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:12:25,469][233954] mean_value=-221.2300170363363, max_value=806.9209394589998[0m
[37m[1m[2023-07-11 20:12:25,479][233954] New mean coefficients: [[ 2.2800817   0.17843217  3.414043   -0.23249814  0.9177811  -2.639353  ]][0m
[37m[1m[2023-07-11 20:12:25,480][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:12:34,460][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 20:12:34,460][233954] FPS: 427694.06[0m
[36m[2023-07-11 20:12:34,463][233954] itr=1451, itrs=2000, Progress: 72.55%[0m
[36m[2023-07-11 20:12:46,208][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 20:12:46,208][233954] FPS: 330110.75[0m
[36m[2023-07-11 20:12:50,467][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:12:50,467][233954] Reward + Measures: [[-13.38083794   0.95071238   0.93654168   0.94919765   0.01339767
    0.67611557]][0m
[37m[1m[2023-07-11 20:12:50,468][233954] Max Reward on eval: -13.380837938614423[0m
[37m[1m[2023-07-11 20:12:50,468][233954] Min Reward on eval: -13.380837938614423[0m
[37m[1m[2023-07-11 20:12:50,468][233954] Mean Reward across all agents: -13.380837938614423[0m
[37m[1m[2023-07-11 20:12:50,468][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:12:55,686][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:12:55,687][233954] Reward + Measures: [[124.60820723   0.92620003   0.92210001   0.91399997   0.0027
    0.81614029]
 [-34.65943297   0.74739999   0.47249994   0.66339999   0.14209999
    1.34284401]
 [ 85.9549761    0.84970009   0.7446       0.81300002   0.0487
    1.37373185]
 ...
 [ 90.39547586   0.86329997   0.77240002   0.83260006   0.0682
    1.58544624]
 [ 38.34993123   0.90369999   0.78590006   0.778        0.0306
    1.12745452]
 [ -4.92525203   0.51030004   0.55060005   0.398        0.34640002
    1.23678863]][0m
[37m[1m[2023-07-11 20:12:55,687][233954] Max Reward on eval: 166.11614896943792[0m
[37m[1m[2023-07-11 20:12:55,687][233954] Min Reward on eval: -76.14041102770716[0m
[37m[1m[2023-07-11 20:12:55,687][233954] Mean Reward across all agents: 50.32220345119757[0m
[37m[1m[2023-07-11 20:12:55,688][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:12:55,692][233954] mean_value=-95.33977526981627, max_value=273.64986762395563[0m
[37m[1m[2023-07-11 20:12:55,694][233954] New mean coefficients: [[ 2.6781635  -0.34795493  3.9525867  -1.3883219   1.2299435  -2.8710594 ]][0m
[37m[1m[2023-07-11 20:12:55,695][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:13:04,630][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 20:13:04,631][233954] FPS: 429845.02[0m
[36m[2023-07-11 20:13:04,633][233954] itr=1452, itrs=2000, Progress: 72.60%[0m
[36m[2023-07-11 20:13:16,234][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 20:13:16,234][233954] FPS: 334377.02[0m
[36m[2023-07-11 20:13:20,460][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:13:20,461][233954] Reward + Measures: [[-1.02393977  0.9750787   0.97190136  0.9785763   0.00811567  0.83404571]][0m
[37m[1m[2023-07-11 20:13:20,461][233954] Max Reward on eval: -1.0239397670411854[0m
[37m[1m[2023-07-11 20:13:20,461][233954] Min Reward on eval: -1.0239397670411854[0m
[37m[1m[2023-07-11 20:13:20,462][233954] Mean Reward across all agents: -1.0239397670411854[0m
[37m[1m[2023-07-11 20:13:20,462][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:13:25,456][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:13:25,457][233954] Reward + Measures: [[-36.31899063   0.79930001   0.77560002   0.7622       0.1839
    0.79245043]
 [-28.7315269    0.76630002   0.78470004   0.55970001   0.47129998
    0.71770662]
 [-28.2875553    0.62930006   0.75029999   0.56800002   0.3924
    0.81938517]
 ...
 [-29.23773303   0.6437       0.80500001   0.4966       0.50019997
    0.80010837]
 [ -7.04815096   0.83740008   0.85680008   0.79350001   0.15750001
    0.81927311]
 [-67.87677502   0.51860005   0.74559999   0.28959998   0.51140004
    0.7179873 ]][0m
[37m[1m[2023-07-11 20:13:25,457][233954] Max Reward on eval: 116.83594067338854[0m
[37m[1m[2023-07-11 20:13:25,458][233954] Min Reward on eval: -188.70971105005592[0m
[37m[1m[2023-07-11 20:13:25,458][233954] Mean Reward across all agents: -17.808380069101432[0m
[37m[1m[2023-07-11 20:13:25,458][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:13:25,460][233954] mean_value=-144.3668259976873, max_value=209.0506807061136[0m
[37m[1m[2023-07-11 20:13:25,462][233954] New mean coefficients: [[ 2.4795454 -0.4560141  3.5092623 -1.6706796  1.4111634 -3.22569  ]][0m
[37m[1m[2023-07-11 20:13:25,463][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:13:34,466][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 20:13:34,467][233954] FPS: 426605.46[0m
[36m[2023-07-11 20:13:34,469][233954] itr=1453, itrs=2000, Progress: 72.65%[0m
[36m[2023-07-11 20:13:46,204][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 20:13:46,204][233954] FPS: 330543.30[0m
[36m[2023-07-11 20:13:50,508][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:13:50,508][233954] Reward + Measures: [[30.62792548  0.958992    0.97081435  0.95130295  0.73699635  0.75316077]][0m
[37m[1m[2023-07-11 20:13:50,509][233954] Max Reward on eval: 30.627925475080584[0m
[37m[1m[2023-07-11 20:13:50,509][233954] Min Reward on eval: 30.627925475080584[0m
[37m[1m[2023-07-11 20:13:50,509][233954] Mean Reward across all agents: 30.627925475080584[0m
[37m[1m[2023-07-11 20:13:50,509][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:13:55,550][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:13:55,550][233954] Reward + Measures: [[ 79.49527641   0.87729996   0.92200005   0.88450003   0.79469997
    0.79250181]
 [103.47578408   0.58540004   0.69609994   0.5643       0.55129999
    1.51695037]
 [ 16.24724633   0.2746       0.3522       0.27860001   0.35909998
    1.81396759]
 ...
 [ 71.39499757   0.90630001   0.93790001   0.90389997   0.84940004
    1.04147875]
 [ 30.8447013    0.65630001   0.75600004   0.59560007   0.55989999
    1.30714881]
 [ 46.03408085   0.94090003   0.95740002   0.92439997   0.89560002
    0.87735003]][0m
[37m[1m[2023-07-11 20:13:55,551][233954] Max Reward on eval: 156.8990362490993[0m
[37m[1m[2023-07-11 20:13:55,551][233954] Min Reward on eval: -29.085464389808475[0m
[37m[1m[2023-07-11 20:13:55,551][233954] Mean Reward across all agents: 59.44258510529003[0m
[37m[1m[2023-07-11 20:13:55,551][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:13:55,556][233954] mean_value=-145.9512919020653, max_value=317.67403782789546[0m
[37m[1m[2023-07-11 20:13:55,559][233954] New mean coefficients: [[ 2.021452   -0.05369067  3.9691873  -1.9964507   0.8515842  -3.4588342 ]][0m
[37m[1m[2023-07-11 20:13:55,560][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:14:04,674][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 20:14:04,675][233954] FPS: 421366.08[0m
[36m[2023-07-11 20:14:04,677][233954] itr=1454, itrs=2000, Progress: 72.70%[0m
[36m[2023-07-11 20:14:16,507][233954] train() took 11.71 seconds to complete[0m
[36m[2023-07-11 20:14:16,507][233954] FPS: 327887.01[0m
[36m[2023-07-11 20:14:20,847][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:14:20,847][233954] Reward + Measures: [[42.05121641  0.93483102  0.90880466  0.8990553   0.00930067  0.75251698]][0m
[37m[1m[2023-07-11 20:14:20,848][233954] Max Reward on eval: 42.05121640640063[0m
[37m[1m[2023-07-11 20:14:20,848][233954] Min Reward on eval: 42.05121640640063[0m
[37m[1m[2023-07-11 20:14:20,848][233954] Mean Reward across all agents: 42.05121640640063[0m
[37m[1m[2023-07-11 20:14:20,848][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:14:25,881][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:14:25,882][233954] Reward + Measures: [[-42.60202115   0.67119998   0.71670002   0.48470002   0.3502
    0.99002618]
 [ 77.36176445   0.84939998   0.77759999   0.76220006   0.0516
    0.87237597]
 [ -9.25852696   0.53680003   0.53969997   0.39659998   0.21760002
    1.30496085]
 ...
 [ 13.37424537   0.87659997   0.86320001   0.85269994   0.33450001
    0.82902968]
 [ -1.76593324   0.88710004   0.88079995   0.82200003   0.36970001
    0.85742152]
 [ 24.16151478   0.89849997   0.84609997   0.83050007   0.0065
    0.79628575]][0m
[37m[1m[2023-07-11 20:14:25,882][233954] Max Reward on eval: 115.72182989511639[0m
[37m[1m[2023-07-11 20:14:25,882][233954] Min Reward on eval: -50.662422922672704[0m
[37m[1m[2023-07-11 20:14:25,883][233954] Mean Reward across all agents: 19.75462957229174[0m
[37m[1m[2023-07-11 20:14:25,883][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:14:25,885][233954] mean_value=-134.9885507007858, max_value=472.69750088065865[0m
[37m[1m[2023-07-11 20:14:25,888][233954] New mean coefficients: [[ 2.487774   -0.40486673  3.7571728  -1.4597749   0.8216731  -3.2575927 ]][0m
[37m[1m[2023-07-11 20:14:25,889][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:14:34,846][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 20:14:34,847][233954] FPS: 428764.96[0m
[36m[2023-07-11 20:14:34,849][233954] itr=1455, itrs=2000, Progress: 72.75%[0m
[36m[2023-07-11 20:14:46,591][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 20:14:46,592][233954] FPS: 330285.55[0m
[36m[2023-07-11 20:14:50,898][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:14:50,898][233954] Reward + Measures: [[35.25225452  0.94788671  0.92718196  0.92477894  0.00956767  0.6294992 ]][0m
[37m[1m[2023-07-11 20:14:50,899][233954] Max Reward on eval: 35.252254519297026[0m
[37m[1m[2023-07-11 20:14:50,899][233954] Min Reward on eval: 35.252254519297026[0m
[37m[1m[2023-07-11 20:14:50,899][233954] Mean Reward across all agents: 35.252254519297026[0m
[37m[1m[2023-07-11 20:14:50,899][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:14:55,933][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:14:55,933][233954] Reward + Measures: [[-32.98439781   0.71880001   0.74509996   0.48660001   0.3743
    0.78457135]
 [  2.85649287   0.87939996   0.83210003   0.80159998   0.0947
    0.61683351]
 [  4.06564076   0.69169998   0.7177       0.4104       0.43239999
    0.8078934 ]
 ...
 [ 17.46850598   0.92370003   0.91670001   0.77070004   0.25140002
    0.83423322]
 [ -5.62352951   0.63600004   0.58350003   0.45000002   0.30420002
    0.83884043]
 [-15.06657099   0.75139999   0.72840005   0.48249999   0.52000004
    0.8044098 ]][0m
[37m[1m[2023-07-11 20:14:55,933][233954] Max Reward on eval: 92.25320575721562[0m
[37m[1m[2023-07-11 20:14:55,934][233954] Min Reward on eval: -63.623869160469624[0m
[37m[1m[2023-07-11 20:14:55,934][233954] Mean Reward across all agents: -4.403890167668889[0m
[37m[1m[2023-07-11 20:14:55,934][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:14:55,936][233954] mean_value=-111.15853345442903, max_value=386.1650340051763[0m
[37m[1m[2023-07-11 20:14:55,939][233954] New mean coefficients: [[ 3.4502318 -1.2962976  3.4883196 -1.8280106  1.4738033 -2.4236712]][0m
[37m[1m[2023-07-11 20:14:55,939][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:15:04,981][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 20:15:04,981][233954] FPS: 424790.66[0m
[36m[2023-07-11 20:15:04,983][233954] itr=1456, itrs=2000, Progress: 72.80%[0m
[36m[2023-07-11 20:15:16,744][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 20:15:16,744][233954] FPS: 329773.21[0m
[36m[2023-07-11 20:15:20,961][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:15:20,961][233954] Reward + Measures: [[37.57063686  0.95666403  0.95081234  0.94422036  0.008621    0.57266217]][0m
[37m[1m[2023-07-11 20:15:20,962][233954] Max Reward on eval: 37.5706368560168[0m
[37m[1m[2023-07-11 20:15:20,962][233954] Min Reward on eval: 37.5706368560168[0m
[37m[1m[2023-07-11 20:15:20,962][233954] Mean Reward across all agents: 37.5706368560168[0m
[37m[1m[2023-07-11 20:15:20,962][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:15:25,901][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:15:25,902][233954] Reward + Measures: [[-95.66165938   0.38459998   0.56420004   0.27989998   0.53859997
    1.5283432 ]
 [175.93337393   0.3066       0.40440002   0.1682       0.32819998
    1.49275553]
 [-50.03695828   0.33469999   0.33930001   0.24340001   0.37729999
    1.6336292 ]
 ...
 [ 32.66031019   0.56170005   0.63720006   0.35279998   0.41780001
    0.97372246]
 [-34.70713874   0.43120003   0.45930001   0.27380002   0.41190004
    1.15555632]
 [ 84.63428829   0.32269999   0.43730003   0.24959998   0.20650001
    1.44250715]][0m
[37m[1m[2023-07-11 20:15:25,902][233954] Max Reward on eval: 224.28119757045062[0m
[37m[1m[2023-07-11 20:15:25,902][233954] Min Reward on eval: -203.80718516949565[0m
[37m[1m[2023-07-11 20:15:25,903][233954] Mean Reward across all agents: -22.855338890016817[0m
[37m[1m[2023-07-11 20:15:25,903][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:15:25,905][233954] mean_value=-325.7095534419987, max_value=211.79739881588418[0m
[37m[1m[2023-07-11 20:15:25,908][233954] New mean coefficients: [[ 2.3340383  -0.411425    3.2001302   0.19893312  1.1995604  -1.9564574 ]][0m
[37m[1m[2023-07-11 20:15:25,908][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:15:34,844][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 20:15:34,844][233954] FPS: 429814.78[0m
[36m[2023-07-11 20:15:34,847][233954] itr=1457, itrs=2000, Progress: 72.85%[0m
[36m[2023-07-11 20:15:46,570][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 20:15:46,570][233954] FPS: 330960.50[0m
[36m[2023-07-11 20:15:50,886][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:15:50,887][233954] Reward + Measures: [[12.43350368  0.952784    0.9445197   0.94790268  0.01121267  0.5981943 ]][0m
[37m[1m[2023-07-11 20:15:50,887][233954] Max Reward on eval: 12.433503683017818[0m
[37m[1m[2023-07-11 20:15:50,887][233954] Min Reward on eval: 12.433503683017818[0m
[37m[1m[2023-07-11 20:15:50,888][233954] Mean Reward across all agents: 12.433503683017818[0m
[37m[1m[2023-07-11 20:15:50,888][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:15:56,118][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:15:56,118][233954] Reward + Measures: [[126.53326323   0.27560002   0.50049996   0.1586       0.54229999
    1.57834554]
 [ 12.10300759   0.45250002   0.4707       0.34549999   0.25600001
    1.79345155]
 [ 82.31565248   0.40099999   0.54650003   0.15000001   0.5485
    1.21390176]
 ...
 [-36.54094006   0.36859998   0.56270003   0.1811       0.56560004
    1.38557827]
 [ 14.41160385   0.95459998   0.77040005   0.94639999   0.0066
    0.85981101]
 [-10.28581308   0.25210002   0.45159999   0.23280001   0.46809998
    1.66225207]][0m
[37m[1m[2023-07-11 20:15:56,118][233954] Max Reward on eval: 167.04481774072627[0m
[37m[1m[2023-07-11 20:15:56,119][233954] Min Reward on eval: -256.55712552238253[0m
[37m[1m[2023-07-11 20:15:56,119][233954] Mean Reward across all agents: 0.7688975344293479[0m
[37m[1m[2023-07-11 20:15:56,119][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:15:56,122][233954] mean_value=-414.18581033550186, max_value=309.2683989926958[0m
[37m[1m[2023-07-11 20:15:56,124][233954] New mean coefficients: [[ 1.6852719   0.2476356   3.7776551   0.8514426   0.63861257 -2.0963666 ]][0m
[37m[1m[2023-07-11 20:15:56,125][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:16:05,103][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 20:16:05,103][233954] FPS: 427815.90[0m
[36m[2023-07-11 20:16:05,105][233954] itr=1458, itrs=2000, Progress: 72.90%[0m
[36m[2023-07-11 20:16:16,914][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 20:16:16,914][233954] FPS: 328367.30[0m
[36m[2023-07-11 20:16:21,236][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:16:21,236][233954] Reward + Measures: [[42.78670383  0.95854372  0.95376527  0.95195931  0.008119    0.63067853]][0m
[37m[1m[2023-07-11 20:16:21,236][233954] Max Reward on eval: 42.78670382694053[0m
[37m[1m[2023-07-11 20:16:21,236][233954] Min Reward on eval: 42.78670382694053[0m
[37m[1m[2023-07-11 20:16:21,237][233954] Mean Reward across all agents: 42.78670382694053[0m
[37m[1m[2023-07-11 20:16:21,237][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:16:26,280][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:16:26,281][233954] Reward + Measures: [[-35.47129154   0.88730001   0.81189996   0.70319998   0.2782
    0.83986628]
 [  3.78560786   0.78690004   0.71740001   0.72729999   0.11839999
    1.44737613]
 [ 41.02562066   0.72640002   0.80109996   0.62700003   0.62980002
    0.99808735]
 ...
 [-50.59959243   0.77580005   0.75649995   0.45720002   0.48740003
    1.15600109]
 [ 22.94044388   0.87220001   0.81830007   0.83360004   0.06940001
    1.53642642]
 [ 12.1778076    0.49499997   0.57330006   0.27210003   0.44520003
    1.00935066]][0m
[37m[1m[2023-07-11 20:16:26,281][233954] Max Reward on eval: 255.01784705976024[0m
[37m[1m[2023-07-11 20:16:26,281][233954] Min Reward on eval: -134.68935109190642[0m
[37m[1m[2023-07-11 20:16:26,282][233954] Mean Reward across all agents: 24.006004359612074[0m
[37m[1m[2023-07-11 20:16:26,282][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:16:26,285][233954] mean_value=-301.7911821127005, max_value=527.188363489979[0m
[37m[1m[2023-07-11 20:16:26,287][233954] New mean coefficients: [[ 1.4433389   0.26201782  3.881709    0.32424855  0.90850985 -1.8011293 ]][0m
[37m[1m[2023-07-11 20:16:26,288][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:16:35,420][233954] train() took 9.13 seconds to complete[0m
[36m[2023-07-11 20:16:35,420][233954] FPS: 420581.80[0m
[36m[2023-07-11 20:16:35,423][233954] itr=1459, itrs=2000, Progress: 72.95%[0m
[36m[2023-07-11 20:16:47,192][233954] train() took 11.65 seconds to complete[0m
[36m[2023-07-11 20:16:47,192][233954] FPS: 329609.13[0m
[36m[2023-07-11 20:16:51,513][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:16:51,518][233954] Reward + Measures: [[64.26080384  0.96265829  0.96200031  0.95998663  0.007736    0.61045527]][0m
[37m[1m[2023-07-11 20:16:51,518][233954] Max Reward on eval: 64.26080383526383[0m
[37m[1m[2023-07-11 20:16:51,518][233954] Min Reward on eval: 64.26080383526383[0m
[37m[1m[2023-07-11 20:16:51,519][233954] Mean Reward across all agents: 64.26080383526383[0m
[37m[1m[2023-07-11 20:16:51,519][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:16:56,502][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:16:56,502][233954] Reward + Measures: [[  41.81231516    0.59580004    0.66140002    0.4341        0.33310002
     1.13127482]
 [ 104.87397805    0.67620003    0.80170006    0.57429999    0.63410002
     1.62285745]
 [  61.83600435    0.59200001    0.68670005    0.55400002    0.52500004
     1.76822782]
 ...
 [  46.55114465    0.80039996    0.86790001    0.78200001    0.71660006
     0.92925519]
 [-105.4969566     0.41510001    0.50640005    0.37580001    0.38960001
     1.40894639]
 [ 142.70599081    0.97190011    0.95640004    0.95839995    0.0133
     1.06795108]][0m
[37m[1m[2023-07-11 20:16:56,503][233954] Max Reward on eval: 368.81955145765096[0m
[37m[1m[2023-07-11 20:16:56,503][233954] Min Reward on eval: -178.447533632908[0m
[37m[1m[2023-07-11 20:16:56,503][233954] Mean Reward across all agents: 64.23610011804966[0m
[37m[1m[2023-07-11 20:16:56,503][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:16:56,509][233954] mean_value=-82.88438735885325, max_value=746.7270370656158[0m
[37m[1m[2023-07-11 20:16:56,512][233954] New mean coefficients: [[ 2.105779    0.6664987   4.309952    0.85294014  3.3794546  -1.2921475 ]][0m
[37m[1m[2023-07-11 20:16:56,513][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:17:05,572][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 20:17:05,572][233954] FPS: 424076.55[0m
[36m[2023-07-11 20:17:05,575][233954] itr=1460, itrs=2000, Progress: 73.00%[0m
[37m[1m[2023-07-11 20:21:03,075][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001440[0m
[36m[2023-07-11 20:21:17,965][233954] train() took 12.56 seconds to complete[0m
[36m[2023-07-11 20:21:17,965][233954] FPS: 305640.21[0m
[36m[2023-07-11 20:21:22,181][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:21:22,182][233954] Reward + Measures: [[52.62264978  0.97281593  0.95933658  0.9656533   0.003824    0.66510499]][0m
[37m[1m[2023-07-11 20:21:22,182][233954] Max Reward on eval: 52.622649778943604[0m
[37m[1m[2023-07-11 20:21:22,182][233954] Min Reward on eval: 52.622649778943604[0m
[37m[1m[2023-07-11 20:21:22,182][233954] Mean Reward across all agents: 52.622649778943604[0m
[37m[1m[2023-07-11 20:21:22,183][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:21:27,420][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:21:27,421][233954] Reward + Measures: [[162.54367251   0.59210008   0.75190002   0.17059998   0.6778
    1.70293927]
 [-33.65247401   0.3545       0.37300003   0.35769996   0.36739999
    1.60937464]
 [ 49.92909767   0.39260003   0.4585       0.2467       0.46680003
    1.5725472 ]
 ...
 [ 83.96144178   0.3258       0.40780002   0.17410001   0.38249999
    2.03443027]
 [-50.21490191   0.115        0.114        0.10160001   0.1164
    2.35508537]
 [ 77.78105922   0.55380005   0.5395       0.46709999   0.092
    1.66608703]][0m
[37m[1m[2023-07-11 20:21:27,421][233954] Max Reward on eval: 278.1520991738886[0m
[37m[1m[2023-07-11 20:21:27,421][233954] Min Reward on eval: -217.30051040314137[0m
[37m[1m[2023-07-11 20:21:27,422][233954] Mean Reward across all agents: -10.935257517140078[0m
[37m[1m[2023-07-11 20:21:27,422][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:21:27,433][233954] mean_value=-405.3264667893572, max_value=551.252915202938[0m
[37m[1m[2023-07-11 20:21:27,446][233954] New mean coefficients: [[ 1.2120101   0.96749103  4.182436    1.3437055   1.1874533  -1.1586474 ]][0m
[37m[1m[2023-07-11 20:21:27,447][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:21:36,391][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 20:21:36,391][233954] FPS: 429462.50[0m
[36m[2023-07-11 20:21:36,393][233954] itr=1461, itrs=2000, Progress: 73.05%[0m
[36m[2023-07-11 20:21:47,933][233954] train() took 11.42 seconds to complete[0m
[36m[2023-07-11 20:21:47,933][233954] FPS: 336110.67[0m
[36m[2023-07-11 20:21:52,198][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:21:52,199][233954] Reward + Measures: [[40.29849144  0.9768517   0.96306598  0.97261596  0.00366467  0.63815784]][0m
[37m[1m[2023-07-11 20:21:52,199][233954] Max Reward on eval: 40.298491440835086[0m
[37m[1m[2023-07-11 20:21:52,199][233954] Min Reward on eval: 40.298491440835086[0m
[37m[1m[2023-07-11 20:21:52,199][233954] Mean Reward across all agents: 40.298491440835086[0m
[37m[1m[2023-07-11 20:21:52,200][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:21:57,202][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:21:57,208][233954] Reward + Measures: [[161.87992395   0.91619998   0.92900002   0.60910004   0.71250004
    1.03580058]
 [ -7.49353608   0.97570002   0.95600003   0.97390002   0.0077
    0.85631657]
 [ 38.3971201    0.95790005   0.81910002   0.88040012   0.0034
    0.66186178]
 ...
 [ 10.0202669    0.97390002   0.88670009   0.93899995   0.0039
    0.64762849]
 [129.03710661   0.48859999   0.84420007   0.0528       0.67149997
    1.89240515]
 [ 24.73850322   0.94950002   0.88900006   0.91800004   0.11620001
    0.72672522]][0m
[37m[1m[2023-07-11 20:21:57,209][233954] Max Reward on eval: 376.0546035859734[0m
[37m[1m[2023-07-11 20:21:57,210][233954] Min Reward on eval: -136.04292427141337[0m
[37m[1m[2023-07-11 20:21:57,210][233954] Mean Reward across all agents: 83.95478573091587[0m
[37m[1m[2023-07-11 20:21:57,211][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:21:57,223][233954] mean_value=-26.23544201519915, max_value=625.2039435529211[0m
[37m[1m[2023-07-11 20:21:57,227][233954] New mean coefficients: [[ 1.1270621  1.2368687  4.3001842  2.2464476  2.7116835 -1.2365829]][0m
[37m[1m[2023-07-11 20:21:57,229][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:22:06,121][233954] train() took 8.89 seconds to complete[0m
[36m[2023-07-11 20:22:06,121][233954] FPS: 431932.09[0m
[36m[2023-07-11 20:22:06,124][233954] itr=1462, itrs=2000, Progress: 73.10%[0m
[36m[2023-07-11 20:22:17,658][233954] train() took 11.42 seconds to complete[0m
[36m[2023-07-11 20:22:17,658][233954] FPS: 336397.78[0m
[36m[2023-07-11 20:22:21,853][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:22:21,853][233954] Reward + Measures: [[33.4685206   0.98021793  0.96519399  0.97791159  0.003832    0.67237693]][0m
[37m[1m[2023-07-11 20:22:21,854][233954] Max Reward on eval: 33.46852060257762[0m
[37m[1m[2023-07-11 20:22:21,854][233954] Min Reward on eval: 33.46852060257762[0m
[37m[1m[2023-07-11 20:22:21,854][233954] Mean Reward across all agents: 33.46852060257762[0m
[37m[1m[2023-07-11 20:22:21,854][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:22:26,749][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:22:26,749][233954] Reward + Measures: [[175.89537802   0.78500003   0.74680001   0.75010008   0.25459999
    1.40562379]
 [152.22555878   0.83980006   0.81419992   0.82369995   0.0109
    2.07972574]
 [ -5.60716449   0.47319999   0.45339999   0.35960001   0.29590002
    2.17433095]
 ...
 [457.61135291   0.88129997   0.87670004   0.86590004   0.1901
    2.10005546]
 [-69.49072297   0.33880001   0.2861       0.2411       0.21180001
    2.24155068]
 [ 84.97849847   0.67390007   0.6426       0.53320003   0.33590001
    1.52695096]][0m
[37m[1m[2023-07-11 20:22:26,749][233954] Max Reward on eval: 562.7290840081871[0m
[37m[1m[2023-07-11 20:22:26,750][233954] Min Reward on eval: -114.13519549979829[0m
[37m[1m[2023-07-11 20:22:26,750][233954] Mean Reward across all agents: 79.8955534678813[0m
[37m[1m[2023-07-11 20:22:26,750][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:22:26,754][233954] mean_value=-228.59827378684815, max_value=734.6872718893625[0m
[37m[1m[2023-07-11 20:22:26,756][233954] New mean coefficients: [[ 1.5902517  1.4815476  4.3018484  2.594159   1.7080371 -1.4627173]][0m
[37m[1m[2023-07-11 20:22:26,757][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:22:35,670][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 20:22:35,670][233954] FPS: 430905.70[0m
[36m[2023-07-11 20:22:35,673][233954] itr=1463, itrs=2000, Progress: 73.15%[0m
[36m[2023-07-11 20:22:47,476][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 20:22:47,476][233954] FPS: 328513.34[0m
[36m[2023-07-11 20:22:51,702][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:22:51,707][233954] Reward + Measures: [[37.69419045  0.98461932  0.96056265  0.98154229  0.003158    0.77014542]][0m
[37m[1m[2023-07-11 20:22:51,708][233954] Max Reward on eval: 37.694190454118505[0m
[37m[1m[2023-07-11 20:22:51,708][233954] Min Reward on eval: 37.694190454118505[0m
[37m[1m[2023-07-11 20:22:51,708][233954] Mean Reward across all agents: 37.694190454118505[0m
[37m[1m[2023-07-11 20:22:51,708][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:22:56,646][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:22:56,647][233954] Reward + Measures: [[ -9.61881714   0.71950001   0.74040002   0.57199997   0.2122
    0.85220605]
 [ 58.13931591   0.3321       0.35670003   0.29860002   0.15280001
    1.60558605]
 [-33.93508395   0.2103       0.26040003   0.19310001   0.2026
    1.92102742]
 ...
 [ 34.12666464   0.97749996   0.94849998   0.92189997   0.0356
    0.65213263]
 [544.20898819   0.93790001   0.95850003   0.91539997   0.73210001
    1.67790449]
 [  5.47693305   0.98220009   0.88630003   0.96509999   0.0035
    0.85927647]][0m
[37m[1m[2023-07-11 20:22:56,647][233954] Max Reward on eval: 790.3570213215484[0m
[37m[1m[2023-07-11 20:22:56,647][233954] Min Reward on eval: -398.227468484384[0m
[37m[1m[2023-07-11 20:22:56,647][233954] Mean Reward across all agents: 95.87692662800174[0m
[37m[1m[2023-07-11 20:22:56,647][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:22:56,651][233954] mean_value=-317.8621223309082, max_value=702.949887743518[0m
[37m[1m[2023-07-11 20:22:56,654][233954] New mean coefficients: [[ 1.9181228   0.6055994   4.1408987   2.1029963   0.14861941 -1.7425619 ]][0m
[37m[1m[2023-07-11 20:22:56,655][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:23:05,634][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 20:23:05,634][233954] FPS: 427758.05[0m
[36m[2023-07-11 20:23:05,636][233954] itr=1464, itrs=2000, Progress: 73.20%[0m
[36m[2023-07-11 20:23:17,216][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 20:23:17,217][233954] FPS: 335056.79[0m
[36m[2023-07-11 20:23:21,452][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:23:21,452][233954] Reward + Measures: [[-6.42791641  0.95861357  0.90275294  0.96891302  0.45961332  0.74834669]][0m
[37m[1m[2023-07-11 20:23:21,452][233954] Max Reward on eval: -6.427916409652585[0m
[37m[1m[2023-07-11 20:23:21,453][233954] Min Reward on eval: -6.427916409652585[0m
[37m[1m[2023-07-11 20:23:21,453][233954] Mean Reward across all agents: -6.427916409652585[0m
[37m[1m[2023-07-11 20:23:21,453][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:23:26,435][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:23:26,436][233954] Reward + Measures: [[57.93516871  0.97970003  0.92799997  0.97150004  0.0066      0.80596679]
 [49.03559329  0.79479998  0.81199998  0.65620005  0.53360003  0.80831635]
 [27.84888517  0.97670001  0.89880002  0.95530003  0.0336      0.64089173]
 ...
 [51.54538943  0.85410005  0.84079999  0.79460001  0.44530001  1.19061255]
 [63.3471496   0.94159997  0.84219998  0.93370003  0.0173      0.86694545]
 [43.66620948  0.97540009  0.91679996  0.96820003  0.0125      0.80212051]][0m
[37m[1m[2023-07-11 20:23:26,436][233954] Max Reward on eval: 185.30867099659517[0m
[37m[1m[2023-07-11 20:23:26,436][233954] Min Reward on eval: -14.601259977929294[0m
[37m[1m[2023-07-11 20:23:26,436][233954] Mean Reward across all agents: 60.44026077635303[0m
[37m[1m[2023-07-11 20:23:26,437][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:23:26,439][233954] mean_value=-78.51650121874061, max_value=328.8119181232017[0m
[37m[1m[2023-07-11 20:23:26,442][233954] New mean coefficients: [[ 2.344184    0.5863922   4.5723896   1.7697105  -0.44341093 -2.0572076 ]][0m
[37m[1m[2023-07-11 20:23:26,443][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:23:35,353][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 20:23:35,353][233954] FPS: 431065.02[0m
[36m[2023-07-11 20:23:35,355][233954] itr=1465, itrs=2000, Progress: 73.25%[0m
[36m[2023-07-11 20:23:47,135][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 20:23:47,135][233954] FPS: 329321.45[0m
[36m[2023-07-11 20:23:51,469][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:23:51,469][233954] Reward + Measures: [[81.23213305  0.77752161  0.85169703  0.48602033  0.58501434  0.88475633]][0m
[37m[1m[2023-07-11 20:23:51,470][233954] Max Reward on eval: 81.23213305277773[0m
[37m[1m[2023-07-11 20:23:51,470][233954] Min Reward on eval: 81.23213305277773[0m
[37m[1m[2023-07-11 20:23:51,470][233954] Mean Reward across all agents: 81.23213305277773[0m
[37m[1m[2023-07-11 20:23:51,470][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:23:56,539][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:23:56,545][233954] Reward + Measures: [[  -5.22104477    0.65450001    0.82419997    0.0402        0.76169997
     0.89825338]
 [  48.49920575    0.65410006    0.76450008    0.3337        0.58990002
     0.9411025 ]
 [-173.27752684    0.59470004    0.68140006    0.11860001    0.59260005
     1.34246957]
 ...
 [-106.30321633    0.77679998    0.84530002    0.1603        0.77229995
     0.91133338]
 [ -45.1558225     0.53310007    0.6419        0.0785        0.58809996
     1.43101525]
 [ -45.96662176    0.39439997    0.36719999    0.18370001    0.26809999
     1.52872849]][0m
[37m[1m[2023-07-11 20:23:56,545][233954] Max Reward on eval: 145.47366523426027[0m
[37m[1m[2023-07-11 20:23:56,545][233954] Min Reward on eval: -227.4423866267316[0m
[37m[1m[2023-07-11 20:23:56,546][233954] Mean Reward across all agents: -39.81411572999977[0m
[37m[1m[2023-07-11 20:23:56,546][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:23:56,549][233954] mean_value=-143.898455417981, max_value=432.97479339838026[0m
[37m[1m[2023-07-11 20:23:56,552][233954] New mean coefficients: [[ 1.7096149  0.4263806  4.740964   1.4989679 -0.9621184 -2.2743614]][0m
[37m[1m[2023-07-11 20:23:56,553][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:24:05,613][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 20:24:05,613][233954] FPS: 423895.99[0m
[36m[2023-07-11 20:24:05,616][233954] itr=1466, itrs=2000, Progress: 73.30%[0m
[36m[2023-07-11 20:24:17,244][233954] train() took 11.51 seconds to complete[0m
[36m[2023-07-11 20:24:17,245][233954] FPS: 333561.30[0m
[36m[2023-07-11 20:24:21,459][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:24:21,460][233954] Reward + Measures: [[20.04570248  0.89470607  0.98733437  0.13040599  0.88374966  0.6512745 ]][0m
[37m[1m[2023-07-11 20:24:21,460][233954] Max Reward on eval: 20.045702480030634[0m
[37m[1m[2023-07-11 20:24:21,460][233954] Min Reward on eval: 20.045702480030634[0m
[37m[1m[2023-07-11 20:24:21,460][233954] Mean Reward across all agents: 20.045702480030634[0m
[37m[1m[2023-07-11 20:24:21,461][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:24:26,774][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:24:26,780][233954] Reward + Measures: [[  43.32248115    0.83540004    0.90369999    0.0395        0.8544001
     1.52195776]
 [-111.55708596    0.84300005    0.95320004    0.05419999    0.86219996
     1.0925734 ]
 [  66.31957152    0.58669996    0.88630003    0.31220001    0.5201
     1.04267538]
 ...
 [  34.81861998    0.82889998    0.93079996    0.0172        0.88570005
     1.80477321]
 [ -13.82119016    0.50119996    0.3682        0.4763        0.2277
     1.48066306]
 [  45.16606557    0.72850001    0.81120008    0.0977        0.71110004
     1.29621172]][0m
[37m[1m[2023-07-11 20:24:26,780][233954] Max Reward on eval: 161.5256013866514[0m
[37m[1m[2023-07-11 20:24:26,781][233954] Min Reward on eval: -212.1225337904878[0m
[37m[1m[2023-07-11 20:24:26,781][233954] Mean Reward across all agents: 20.14214029866896[0m
[37m[1m[2023-07-11 20:24:26,781][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:24:26,786][233954] mean_value=-27.712504218486167, max_value=534.9738488139957[0m
[37m[1m[2023-07-11 20:24:26,789][233954] New mean coefficients: [[ 1.5873495  1.0399722  4.3128166  1.7368155 -0.4951162 -1.5190957]][0m
[37m[1m[2023-07-11 20:24:26,790][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:24:35,760][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 20:24:35,760][233954] FPS: 428189.53[0m
[36m[2023-07-11 20:24:35,762][233954] itr=1467, itrs=2000, Progress: 73.35%[0m
[36m[2023-07-11 20:24:47,454][233954] train() took 11.58 seconds to complete[0m
[36m[2023-07-11 20:24:47,454][233954] FPS: 331700.90[0m
[36m[2023-07-11 20:24:51,824][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:24:51,824][233954] Reward + Measures: [[7.37814804 0.84013098 0.92213899 0.65113765 0.6578303  1.36123407]][0m
[37m[1m[2023-07-11 20:24:51,825][233954] Max Reward on eval: 7.378148039289284[0m
[37m[1m[2023-07-11 20:24:51,825][233954] Min Reward on eval: 7.378148039289284[0m
[37m[1m[2023-07-11 20:24:51,825][233954] Mean Reward across all agents: 7.378148039289284[0m
[37m[1m[2023-07-11 20:24:51,825][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:24:56,826][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:24:56,832][233954] Reward + Measures: [[-113.94498179    0.66769999    0.89279997    0.0136        0.6972
     1.45537508]
 [  13.22335317    0.7155        0.86380005    0.33949998    0.49450001
     1.56518877]
 [ -68.74776427    0.84130001    0.72450006    0.71630001    0.35869998
     1.14638841]
 ...
 [ -96.15807566    0.33269998    0.44119999    0.25400001    0.35010001
     2.01735091]
 [  56.87802491    0.34670004    0.37870002    0.257         0.25479999
     1.87503815]
 [   4.58711251    0.51850003    0.54330009    0.31619999    0.39820001
     1.26806748]][0m
[37m[1m[2023-07-11 20:24:56,832][233954] Max Reward on eval: 187.3679621130228[0m
[37m[1m[2023-07-11 20:24:56,832][233954] Min Reward on eval: -280.5432682130486[0m
[37m[1m[2023-07-11 20:24:56,832][233954] Mean Reward across all agents: -11.274392435532688[0m
[37m[1m[2023-07-11 20:24:56,833][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:24:56,836][233954] mean_value=-209.5702583212803, max_value=454.7197805372253[0m
[37m[1m[2023-07-11 20:24:56,838][233954] New mean coefficients: [[ 1.1740025   0.83270705  4.1977916  -0.04952538 -0.94233835 -2.0596366 ]][0m
[37m[1m[2023-07-11 20:24:56,839][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:25:05,886][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 20:25:05,887][233954] FPS: 424527.06[0m
[36m[2023-07-11 20:25:05,889][233954] itr=1468, itrs=2000, Progress: 73.40%[0m
[36m[2023-07-11 20:25:17,619][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 20:25:17,619][233954] FPS: 330610.54[0m
[36m[2023-07-11 20:25:21,955][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:25:21,956][233954] Reward + Measures: [[-25.16780385   0.92334193   0.95014805   0.80285698   0.72160971
    1.44928217]][0m
[37m[1m[2023-07-11 20:25:21,956][233954] Max Reward on eval: -25.16780384618335[0m
[37m[1m[2023-07-11 20:25:21,956][233954] Min Reward on eval: -25.16780384618335[0m
[37m[1m[2023-07-11 20:25:21,956][233954] Mean Reward across all agents: -25.16780384618335[0m
[37m[1m[2023-07-11 20:25:21,956][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:25:26,976][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:25:26,977][233954] Reward + Measures: [[ -49.54736139    0.63399994    0.78330004    0.35640001    0.64030004
     1.20868039]
 [ 203.7089203     0.33870003    0.4364        0.19759999    0.38340002
     1.40192366]
 [  61.12783417    0.92900002    0.96660006    0.42300001    0.89320004
     2.30156493]
 ...
 [-264.53742983    0.83630002    0.94980001    0.            0.92430001
     1.45522392]
 [ 182.65365405    0.41490003    0.68029994    0.31799999    0.37199998
     1.34289932]
 [ 109.9710622     0.53969997    0.78459996    0.39660004    0.47010002
     1.24429893]][0m
[37m[1m[2023-07-11 20:25:26,977][233954] Max Reward on eval: 331.0200576769188[0m
[37m[1m[2023-07-11 20:25:26,978][233954] Min Reward on eval: -380.56425668001174[0m
[37m[1m[2023-07-11 20:25:26,978][233954] Mean Reward across all agents: 63.31111498566466[0m
[37m[1m[2023-07-11 20:25:26,978][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:25:26,986][233954] mean_value=-62.311549437624414, max_value=589.4033233945817[0m
[37m[1m[2023-07-11 20:25:26,989][233954] New mean coefficients: [[ 1.4509044   1.047165    3.5693157   1.5500448  -0.51789296 -1.661322  ]][0m
[37m[1m[2023-07-11 20:25:26,990][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:25:36,050][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 20:25:36,051][233954] FPS: 423891.92[0m
[36m[2023-07-11 20:25:36,053][233954] itr=1469, itrs=2000, Progress: 73.45%[0m
[36m[2023-07-11 20:25:47,717][233954] train() took 11.55 seconds to complete[0m
[36m[2023-07-11 20:25:47,717][233954] FPS: 332493.52[0m
[36m[2023-07-11 20:25:51,980][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:25:51,981][233954] Reward + Measures: [[-81.10286481   0.84321404   0.95439392   0.27905968   0.70309162
    1.47302473]][0m
[37m[1m[2023-07-11 20:25:51,981][233954] Max Reward on eval: -81.10286480732022[0m
[37m[1m[2023-07-11 20:25:51,981][233954] Min Reward on eval: -81.10286480732022[0m
[37m[1m[2023-07-11 20:25:51,981][233954] Mean Reward across all agents: -81.10286480732022[0m
[37m[1m[2023-07-11 20:25:51,981][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:25:56,949][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:25:56,950][233954] Reward + Measures: [[-89.53435466   0.4276       0.74870008   0.1566       0.58740002
    1.15227497]
 [-44.13746722   0.34250003   0.63999999   0.26820001   0.53399998
    1.22551882]
 [ 38.39378053   0.50029999   0.48519999   0.36180001   0.43360004
    1.59699595]
 ...
 [-10.32251877   0.84040004   0.87390006   0.40159997   0.85729998
    1.62227857]
 [ 28.01278359   0.542        0.64540005   0.23120001   0.57560003
    1.50686073]
 [203.1630883    0.80660003   0.59989995   0.71179998   0.0978
    1.69504392]][0m
[37m[1m[2023-07-11 20:25:56,950][233954] Max Reward on eval: 502.61611437676476[0m
[37m[1m[2023-07-11 20:25:56,950][233954] Min Reward on eval: -235.17933656014503[0m
[37m[1m[2023-07-11 20:25:56,951][233954] Mean Reward across all agents: 55.87570625012175[0m
[37m[1m[2023-07-11 20:25:56,951][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:25:56,957][233954] mean_value=-45.74063738450029, max_value=566.4270091592647[0m
[37m[1m[2023-07-11 20:25:56,960][233954] New mean coefficients: [[ 1.0569791  1.4171283  3.754385   1.4242692 -1.5618802 -1.9312975]][0m
[37m[1m[2023-07-11 20:25:56,961][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:26:05,942][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 20:26:05,942][233954] FPS: 427651.24[0m
[36m[2023-07-11 20:26:05,944][233954] itr=1470, itrs=2000, Progress: 73.50%[0m
[37m[1m[2023-07-11 20:29:56,282][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001450[0m
[36m[2023-07-11 20:30:10,327][233954] train() took 12.05 seconds to complete[0m
[36m[2023-07-11 20:30:10,327][233954] FPS: 318583.35[0m
[36m[2023-07-11 20:30:14,645][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:30:14,646][233954] Reward + Measures: [[58.694075    0.91938233  0.92232341  0.68450391  0.71513498  1.19863307]][0m
[37m[1m[2023-07-11 20:30:14,646][233954] Max Reward on eval: 58.69407499783959[0m
[37m[1m[2023-07-11 20:30:14,646][233954] Min Reward on eval: 58.69407499783959[0m
[37m[1m[2023-07-11 20:30:14,646][233954] Mean Reward across all agents: 58.69407499783959[0m
[37m[1m[2023-07-11 20:30:14,647][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:30:19,915][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:30:19,916][233954] Reward + Measures: [[144.01409053   0.87260002   0.89830011   0.38979998   0.50060004
    1.08032346]
 [-27.00662434   0.34590003   0.87050003   0.27180001   0.49840003
    1.18153703]
 [-43.62855233   0.5607       0.90109998   0.24319999   0.4729
    1.0546838 ]
 ...
 [ 30.56610823   0.31189999   0.80669993   0.30700001   0.52640003
    1.16889775]
 [176.65501261   0.78850001   0.79940003   0.54640001   0.23260002
    0.9917165 ]
 [ 15.71207183   0.36750001   0.87729996   0.24330001   0.55330002
    1.17613959]][0m
[37m[1m[2023-07-11 20:30:19,916][233954] Max Reward on eval: 270.76364326705516[0m
[37m[1m[2023-07-11 20:30:19,916][233954] Min Reward on eval: -145.68613100829535[0m
[37m[1m[2023-07-11 20:30:19,916][233954] Mean Reward across all agents: 56.39093069947926[0m
[37m[1m[2023-07-11 20:30:19,917][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:30:19,941][233954] mean_value=69.84591788551316, max_value=644.0140905267588[0m
[37m[1m[2023-07-11 20:30:19,947][233954] New mean coefficients: [[ 0.59545594  1.220263    4.279438    0.64651084 -2.6829247  -2.6735399 ]][0m
[37m[1m[2023-07-11 20:30:19,948][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:30:28,942][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 20:30:28,942][233954] FPS: 427051.47[0m
[36m[2023-07-11 20:30:28,944][233954] itr=1471, itrs=2000, Progress: 73.55%[0m
[36m[2023-07-11 20:30:40,684][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 20:30:40,684][233954] FPS: 330370.16[0m
[36m[2023-07-11 20:30:44,916][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:30:44,916][233954] Reward + Measures: [[-12.05806245   0.71792871   0.84399933   0.714275     0.64561033
    0.98605841]][0m
[37m[1m[2023-07-11 20:30:44,916][233954] Max Reward on eval: -12.058062449932379[0m
[37m[1m[2023-07-11 20:30:44,917][233954] Min Reward on eval: -12.058062449932379[0m
[37m[1m[2023-07-11 20:30:44,917][233954] Mean Reward across all agents: -12.058062449932379[0m
[37m[1m[2023-07-11 20:30:44,917][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:30:49,925][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:30:49,926][233954] Reward + Measures: [[176.46045185   0.88479996   0.35339999   0.83460009   0.1103
    1.54245913]
 [ 99.54333955   0.70329994   0.77289999   0.48049998   0.49770004
    1.01055837]
 [-23.01772316   0.84979993   0.71070004   0.83579999   0.5165
    1.56341648]
 ...
 [115.99154949   0.87530005   0.3951       0.8132       0.18120001
    0.99122459]
 [ 10.66471384   0.8976       0.89920008   0.86160004   0.77790004
    1.58555675]
 [131.09259891   0.86940002   0.93510002   0.73809999   0.48130003
    0.96647567]][0m
[37m[1m[2023-07-11 20:30:49,926][233954] Max Reward on eval: 232.99148558881134[0m
[37m[1m[2023-07-11 20:30:49,926][233954] Min Reward on eval: -92.0948906167876[0m
[37m[1m[2023-07-11 20:30:49,927][233954] Mean Reward across all agents: 54.215565911578466[0m
[37m[1m[2023-07-11 20:30:49,927][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:30:49,933][233954] mean_value=-18.647534705006727, max_value=410.48022734614557[0m
[37m[1m[2023-07-11 20:30:49,935][233954] New mean coefficients: [[ 0.5247033   1.2784123   4.6009483   0.23818257 -1.945218   -2.3629162 ]][0m
[37m[1m[2023-07-11 20:30:49,936][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:30:58,953][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 20:30:58,953][233954] FPS: 425951.74[0m
[36m[2023-07-11 20:30:58,956][233954] itr=1472, itrs=2000, Progress: 73.60%[0m
[36m[2023-07-11 20:31:10,691][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 20:31:10,691][233954] FPS: 330575.57[0m
[36m[2023-07-11 20:31:14,904][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:31:14,904][233954] Reward + Measures: [[60.40561036  0.67827564  0.69480103  0.71991503  0.19998401  0.75759667]][0m
[37m[1m[2023-07-11 20:31:14,904][233954] Max Reward on eval: 60.40561035989735[0m
[37m[1m[2023-07-11 20:31:14,905][233954] Min Reward on eval: 60.40561035989735[0m
[37m[1m[2023-07-11 20:31:14,905][233954] Mean Reward across all agents: 60.40561035989735[0m
[37m[1m[2023-07-11 20:31:14,905][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:31:19,926][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:31:19,926][233954] Reward + Measures: [[152.63557244   0.59939998   0.80669993   0.63309997   0.46020004
    0.88692343]
 [ 79.62240074   0.5697       0.73860008   0.52829999   0.37530002
    0.79232877]
 [ 94.73470069   0.78479999   0.76860011   0.73499995   0.23450001
    0.79184562]
 ...
 [167.37410068   0.7877       0.7949       0.78179997   0.40369996
    0.98132384]
 [ 87.08237362   0.64109993   0.78360003   0.69169998   0.46440002
    0.88629687]
 [ 87.53356131   0.63550001   0.69649994   0.73460001   0.32390004
    0.88444245]][0m
[37m[1m[2023-07-11 20:31:19,926][233954] Max Reward on eval: 245.47700692004292[0m
[37m[1m[2023-07-11 20:31:19,927][233954] Min Reward on eval: -126.89500763379037[0m
[37m[1m[2023-07-11 20:31:19,927][233954] Mean Reward across all agents: 103.44560703383046[0m
[37m[1m[2023-07-11 20:31:19,927][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:31:19,934][233954] mean_value=19.349007680489503, max_value=543.3227671641205[0m
[37m[1m[2023-07-11 20:31:19,936][233954] New mean coefficients: [[ 0.12433362  1.2497125   5.17504     1.028735   -2.0109992  -3.03307   ]][0m
[37m[1m[2023-07-11 20:31:19,937][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:31:28,912][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 20:31:28,913][233954] FPS: 427938.46[0m
[36m[2023-07-11 20:31:28,915][233954] itr=1473, itrs=2000, Progress: 73.65%[0m
[36m[2023-07-11 20:31:40,917][233954] train() took 11.89 seconds to complete[0m
[36m[2023-07-11 20:31:40,917][233954] FPS: 323042.40[0m
[36m[2023-07-11 20:31:45,253][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:31:45,254][233954] Reward + Measures: [[49.68426115  0.71614599  0.71869057  0.77587366  0.16204166  0.73716682]][0m
[37m[1m[2023-07-11 20:31:45,254][233954] Max Reward on eval: 49.68426114514858[0m
[37m[1m[2023-07-11 20:31:45,254][233954] Min Reward on eval: 49.68426114514858[0m
[37m[1m[2023-07-11 20:31:45,254][233954] Mean Reward across all agents: 49.68426114514858[0m
[37m[1m[2023-07-11 20:31:45,255][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:31:50,255][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:31:50,256][233954] Reward + Measures: [[ 76.36969482   0.62519997   0.61469996   0.51519996   0.33269998
    1.02295625]
 [ 88.05861567   0.73019999   0.78910005   0.55740005   0.31370002
    0.79322129]
 [155.58497575   0.78659999   0.8495       0.7184       0.40970001
    0.85939735]
 ...
 [ 83.23653075   0.70679998   0.66160005   0.68480003   0.26460001
    0.72254866]
 [142.98780249   0.63639998   0.73149997   0.65180004   0.35729998
    0.79612273]
 [ 64.86105087   0.75299996   0.71419996   0.75119996   0.1383
    0.7866559 ]][0m
[37m[1m[2023-07-11 20:31:50,256][233954] Max Reward on eval: 169.65451241759584[0m
[37m[1m[2023-07-11 20:31:50,257][233954] Min Reward on eval: -4.219955510110594[0m
[37m[1m[2023-07-11 20:31:50,257][233954] Mean Reward across all agents: 77.68473354413075[0m
[37m[1m[2023-07-11 20:31:50,257][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:31:50,260][233954] mean_value=-73.90137257549578, max_value=238.37927981636724[0m
[37m[1m[2023-07-11 20:31:50,263][233954] New mean coefficients: [[ 0.6851393   0.57036346  5.774302   -0.24074924 -2.2156177  -3.210901  ]][0m
[37m[1m[2023-07-11 20:31:50,263][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:31:59,328][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 20:31:59,329][233954] FPS: 423691.45[0m
[36m[2023-07-11 20:31:59,331][233954] itr=1474, itrs=2000, Progress: 73.70%[0m
[36m[2023-07-11 20:32:11,111][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 20:32:11,112][233954] FPS: 329300.41[0m
[36m[2023-07-11 20:32:15,394][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:32:15,395][233954] Reward + Measures: [[35.85965378  0.77289534  0.74962938  0.84644634  0.09543034  0.72430128]][0m
[37m[1m[2023-07-11 20:32:15,395][233954] Max Reward on eval: 35.85965377963334[0m
[37m[1m[2023-07-11 20:32:15,395][233954] Min Reward on eval: 35.85965377963334[0m
[37m[1m[2023-07-11 20:32:15,396][233954] Mean Reward across all agents: 35.85965377963334[0m
[37m[1m[2023-07-11 20:32:15,396][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:32:20,417][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:32:20,417][233954] Reward + Measures: [[137.87508203   0.84450006   0.74539995   0.75160003   0.0836
    0.82690442]
 [-56.06932475   0.38960001   0.5722       0.12810002   0.52590001
    1.40281808]
 [ 27.79219499   0.60109997   0.70329994   0.44009995   0.57030004
    0.96828187]
 ...
 [ 89.07418218   0.7669       0.63690007   0.6875       0.0254
    1.13718212]
 [ 21.21135912   0.43800002   0.61320001   0.24430001   0.45220003
    1.05311739]
 [-51.81065503   0.6505       0.6419       0.47750002   0.24600001
    1.06272733]][0m
[37m[1m[2023-07-11 20:32:20,418][233954] Max Reward on eval: 253.28980253532063[0m
[37m[1m[2023-07-11 20:32:20,418][233954] Min Reward on eval: -144.7695169646293[0m
[37m[1m[2023-07-11 20:32:20,418][233954] Mean Reward across all agents: 20.03855331039047[0m
[37m[1m[2023-07-11 20:32:20,418][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:32:20,421][233954] mean_value=-179.63924129121895, max_value=433.57758739780616[0m
[37m[1m[2023-07-11 20:32:20,423][233954] New mean coefficients: [[ 0.7830219   0.8061429   6.0859585   0.24762785 -1.0941887  -2.6772861 ]][0m
[37m[1m[2023-07-11 20:32:20,424][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:32:29,366][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 20:32:29,366][233954] FPS: 429536.64[0m
[36m[2023-07-11 20:32:29,368][233954] itr=1475, itrs=2000, Progress: 73.75%[0m
[36m[2023-07-11 20:32:40,938][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 20:32:40,938][233954] FPS: 335284.80[0m
[36m[2023-07-11 20:32:45,196][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:32:45,197][233954] Reward + Measures: [[33.91335209  0.82313704  0.79422069  0.89569336  0.073683    0.71051431]][0m
[37m[1m[2023-07-11 20:32:45,197][233954] Max Reward on eval: 33.913352093142535[0m
[37m[1m[2023-07-11 20:32:45,197][233954] Min Reward on eval: 33.913352093142535[0m
[37m[1m[2023-07-11 20:32:45,198][233954] Mean Reward across all agents: 33.913352093142535[0m
[37m[1m[2023-07-11 20:32:45,198][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:32:50,171][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:32:50,171][233954] Reward + Measures: [[121.88513284   0.90720004   0.82579994   0.83789998   0.14709999
    0.77503836]
 [139.84947204   0.94929999   0.82700008   0.85700005   0.0329
    0.90760195]
 [ 31.46017644   0.81339997   0.66390002   0.86949998   0.0219
    0.65640366]
 ...
 [ 53.01362469   0.78130001   0.70760006   0.87839997   0.0175
    0.69529468]
 [ 26.07380277   0.75830001   0.54790002   0.70740002   0.16500001
    0.85672563]
 [ 53.34134651   0.72589999   0.65889996   0.87290001   0.0468
    0.67838222]][0m
[37m[1m[2023-07-11 20:32:50,171][233954] Max Reward on eval: 225.40604926359373[0m
[37m[1m[2023-07-11 20:32:50,172][233954] Min Reward on eval: -96.71380116771907[0m
[37m[1m[2023-07-11 20:32:50,172][233954] Mean Reward across all agents: 68.74961147416016[0m
[37m[1m[2023-07-11 20:32:50,172][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:32:50,176][233954] mean_value=-36.219009148834566, max_value=469.2940154905897[0m
[37m[1m[2023-07-11 20:32:50,178][233954] New mean coefficients: [[-0.21013755  1.3365815   7.099288   -0.38644898 -0.950168   -3.2124536 ]][0m
[37m[1m[2023-07-11 20:32:50,179][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:32:59,146][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 20:32:59,146][233954] FPS: 428351.52[0m
[36m[2023-07-11 20:32:59,148][233954] itr=1476, itrs=2000, Progress: 73.80%[0m
[36m[2023-07-11 20:33:10,748][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 20:33:10,749][233954] FPS: 334471.93[0m
[36m[2023-07-11 20:33:15,019][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:33:15,020][233954] Reward + Measures: [[36.26178368  0.86999869  0.83023405  0.92652023  0.05767234  0.6858893 ]][0m
[37m[1m[2023-07-11 20:33:15,020][233954] Max Reward on eval: 36.26178368140019[0m
[37m[1m[2023-07-11 20:33:15,020][233954] Min Reward on eval: 36.26178368140019[0m
[37m[1m[2023-07-11 20:33:15,020][233954] Mean Reward across all agents: 36.26178368140019[0m
[37m[1m[2023-07-11 20:33:15,021][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:33:20,306][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:33:20,306][233954] Reward + Measures: [[ 42.91700876   0.4896       0.73640007   0.52779996   0.56649995
    0.79142308]
 [142.84341432   0.80319995   0.7155       0.85820007   0.0301
    0.77080375]
 [103.46075438   0.50260001   0.74230003   0.50800008   0.52940005
    0.78655189]
 ...
 [ 86.21254536   0.82240003   0.80699998   0.89429998   0.10250001
    0.66277808]
 [108.5521693    0.74250001   0.67299998   0.77620006   0.23099999
    0.94037861]
 [221.6588545    0.88500005   0.86630005   0.86309999   0.47680002
    1.21340847]][0m
[37m[1m[2023-07-11 20:33:20,306][233954] Max Reward on eval: 247.37004662901163[0m
[37m[1m[2023-07-11 20:33:20,307][233954] Min Reward on eval: -117.22337576509453[0m
[37m[1m[2023-07-11 20:33:20,307][233954] Mean Reward across all agents: 110.75652369756669[0m
[37m[1m[2023-07-11 20:33:20,307][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:33:20,313][233954] mean_value=-12.557881309253313, max_value=418.62889699939984[0m
[37m[1m[2023-07-11 20:33:20,316][233954] New mean coefficients: [[-0.46582273  1.4451389   7.5920286  -0.18245867 -0.65637994 -2.7558289 ]][0m
[37m[1m[2023-07-11 20:33:20,317][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:33:29,366][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 20:33:29,366][233954] FPS: 424431.97[0m
[36m[2023-07-11 20:33:29,368][233954] itr=1477, itrs=2000, Progress: 73.85%[0m
[36m[2023-07-11 20:33:41,332][233954] train() took 11.84 seconds to complete[0m
[36m[2023-07-11 20:33:41,333][233954] FPS: 324223.48[0m
[36m[2023-07-11 20:33:45,653][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:33:45,654][233954] Reward + Measures: [[39.25115995  0.96115965  0.93197805  0.98160237  0.01299567  0.69248492]][0m
[37m[1m[2023-07-11 20:33:45,654][233954] Max Reward on eval: 39.25115995251061[0m
[37m[1m[2023-07-11 20:33:45,654][233954] Min Reward on eval: 39.25115995251061[0m
[37m[1m[2023-07-11 20:33:45,655][233954] Mean Reward across all agents: 39.25115995251061[0m
[37m[1m[2023-07-11 20:33:45,655][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:33:50,599][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:33:50,600][233954] Reward + Measures: [[-47.48678259   0.76249999   0.39720002   0.74430001   0.24240001
    0.92700261]
 [ 66.59865619   0.88319999   0.69379997   0.96730006   0.0147
    0.83406079]
 [ 53.16178132   0.83100003   0.86809999   0.7938       0.53389996
    0.98630506]
 ...
 [ 54.76843537   0.84549999   0.81660002   0.89700001   0.0558
    0.77874261]
 [-10.74551337   0.65990001   0.5557       0.65329999   0.24059999
    0.87475759]
 [ 46.51488783   0.76459998   0.86140007   0.72839993   0.61939996
    0.8721537 ]][0m
[37m[1m[2023-07-11 20:33:50,600][233954] Max Reward on eval: 200.0553054620512[0m
[37m[1m[2023-07-11 20:33:50,600][233954] Min Reward on eval: -87.96974885111558[0m
[37m[1m[2023-07-11 20:33:50,600][233954] Mean Reward across all agents: 60.750203082232375[0m
[37m[1m[2023-07-11 20:33:50,600][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:33:50,604][233954] mean_value=-93.2669783461547, max_value=474.4306310370038[0m
[37m[1m[2023-07-11 20:33:50,607][233954] New mean coefficients: [[ 0.12468019  0.80288947  7.828318   -0.5360913   0.1607784  -2.8704011 ]][0m
[37m[1m[2023-07-11 20:33:50,608][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:33:59,614][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 20:33:59,615][233954] FPS: 426423.20[0m
[36m[2023-07-11 20:33:59,617][233954] itr=1478, itrs=2000, Progress: 73.90%[0m
[36m[2023-07-11 20:34:11,408][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 20:34:11,409][233954] FPS: 329029.02[0m
[36m[2023-07-11 20:34:15,789][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:34:15,790][233954] Reward + Measures: [[30.69407645  0.98330534  0.97221166  0.99079531  0.00596     0.6638189 ]][0m
[37m[1m[2023-07-11 20:34:15,790][233954] Max Reward on eval: 30.694076445050257[0m
[37m[1m[2023-07-11 20:34:15,790][233954] Min Reward on eval: 30.694076445050257[0m
[37m[1m[2023-07-11 20:34:15,790][233954] Mean Reward across all agents: 30.694076445050257[0m
[37m[1m[2023-07-11 20:34:15,791][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:34:20,814][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:34:20,815][233954] Reward + Measures: [[  -1.91410672    0.55920005    0.81520003    0.62980002    0.74330008
     1.5596565 ]
 [  35.7855822     0.43990001    0.7123        0.31529999    0.62709999
     1.390221  ]
 [-100.00308768    0.70829999    0.22939999    0.70710003    0.3344
     1.8922857 ]
 ...
 [  48.76760014    0.55369997    0.76210004    0.55550003    0.63510001
     1.2818439 ]
 [  21.43291478    0.72030002    0.40419999    0.74610001    0.0542
     1.38912833]
 [ 195.86927034    0.64740002    0.71880001    0.2536        0.48130003
     0.78400689]][0m
[37m[1m[2023-07-11 20:34:20,815][233954] Max Reward on eval: 212.44910431251628[0m
[37m[1m[2023-07-11 20:34:20,815][233954] Min Reward on eval: -466.4949674475938[0m
[37m[1m[2023-07-11 20:34:20,815][233954] Mean Reward across all agents: 25.33492727192865[0m
[37m[1m[2023-07-11 20:34:20,815][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:34:20,819][233954] mean_value=-133.34417831844164, max_value=445.3544448194215[0m
[37m[1m[2023-07-11 20:34:20,823][233954] New mean coefficients: [[ 0.59069526  0.5113548   7.8712134  -0.45491102 -0.3753397  -2.5428374 ]][0m
[37m[1m[2023-07-11 20:34:20,824][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:34:29,904][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 20:34:29,904][233954] FPS: 422975.79[0m
[36m[2023-07-11 20:34:29,906][233954] itr=1479, itrs=2000, Progress: 73.95%[0m
[36m[2023-07-11 20:34:41,646][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 20:34:41,646][233954] FPS: 330358.92[0m
[36m[2023-07-11 20:34:45,909][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:34:45,910][233954] Reward + Measures: [[27.81076261  0.958179    0.98079866  0.9701364   0.051878    0.66519725]][0m
[37m[1m[2023-07-11 20:34:45,910][233954] Max Reward on eval: 27.81076261248734[0m
[37m[1m[2023-07-11 20:34:45,910][233954] Min Reward on eval: 27.81076261248734[0m
[37m[1m[2023-07-11 20:34:45,911][233954] Mean Reward across all agents: 27.81076261248734[0m
[37m[1m[2023-07-11 20:34:45,911][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:34:50,873][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:34:50,874][233954] Reward + Measures: [[84.24495314  0.67899996  0.69980001  0.79220003  0.0468      1.62986243]
 [21.92328033  0.97259998  0.79550004  0.98110002  0.0158      0.73899764]
 [25.03635266  0.66260004  0.82069999  0.77149999  0.49830005  0.7907837 ]
 ...
 [38.75567055  0.74700004  0.76050007  0.88220006  0.0589      0.92043322]
 [40.38599317  0.55480003  0.4691      0.5873      0.3739      0.95556968]
 [47.37590355  0.83129996  0.7978      0.90740007  0.0309      1.10968435]][0m
[37m[1m[2023-07-11 20:34:50,874][233954] Max Reward on eval: 350.90442371293904[0m
[37m[1m[2023-07-11 20:34:50,875][233954] Min Reward on eval: -155.96260930905117[0m
[37m[1m[2023-07-11 20:34:50,875][233954] Mean Reward across all agents: 55.1415165202217[0m
[37m[1m[2023-07-11 20:34:50,875][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:34:50,880][233954] mean_value=-111.66118829991882, max_value=850.904423712939[0m
[37m[1m[2023-07-11 20:34:50,882][233954] New mean coefficients: [[ 0.81003964 -0.07100999  7.47804     0.13261339 -0.76127    -2.3111427 ]][0m
[37m[1m[2023-07-11 20:34:50,883][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:34:59,875][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 20:34:59,875][233954] FPS: 427151.77[0m
[36m[2023-07-11 20:34:59,877][233954] itr=1480, itrs=2000, Progress: 74.00%[0m
[37m[1m[2023-07-11 20:38:57,522][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001460[0m
[36m[2023-07-11 20:39:12,370][233954] train() took 12.62 seconds to complete[0m
[36m[2023-07-11 20:39:12,371][233954] FPS: 304360.58[0m
[36m[2023-07-11 20:39:16,619][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:39:16,623][233954] Reward + Measures: [[25.70513229  0.98977929  0.98975766  0.99189097  0.01030167  0.63496786]][0m
[37m[1m[2023-07-11 20:39:16,623][233954] Max Reward on eval: 25.70513228705695[0m
[37m[1m[2023-07-11 20:39:16,624][233954] Min Reward on eval: 25.70513228705695[0m
[37m[1m[2023-07-11 20:39:16,625][233954] Mean Reward across all agents: 25.70513228705695[0m
[37m[1m[2023-07-11 20:39:16,625][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:39:21,591][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:39:21,591][233954] Reward + Measures: [[ 93.45511246   0.9091       0.86420006   0.97140008   0.0061
    1.17416549]
 [ 35.09884299   0.75509995   0.70069999   0.75099999   0.1184
    2.02891064]
 [ 84.94876475   0.82139999   0.7985       0.93330002   0.0017
    1.65491903]
 ...
 [  9.28635776   0.95009995   0.71280003   0.86240005   0.0061
    0.81412679]
 [ 92.68531893   0.87000006   0.85280001   0.95520002   0.0014
    1.99234903]
 [104.62464422   0.7209       0.65389997   0.72080004   0.0327
    1.45076656]][0m
[37m[1m[2023-07-11 20:39:21,592][233954] Max Reward on eval: 198.4393806276843[0m
[37m[1m[2023-07-11 20:39:21,592][233954] Min Reward on eval: -84.25398042351007[0m
[37m[1m[2023-07-11 20:39:21,592][233954] Mean Reward across all agents: 63.50251349777291[0m
[37m[1m[2023-07-11 20:39:21,592][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:39:21,607][233954] mean_value=-126.60251184042743, max_value=342.3627661292689[0m
[37m[1m[2023-07-11 20:39:21,617][233954] New mean coefficients: [[ 1.2169513  -0.59069353  7.06034    -0.08520737 -0.80512846 -2.037115  ]][0m
[37m[1m[2023-07-11 20:39:21,618][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:39:30,629][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 20:39:30,629][233954] FPS: 426260.81[0m
[36m[2023-07-11 20:39:30,632][233954] itr=1481, itrs=2000, Progress: 74.05%[0m
[36m[2023-07-11 20:39:42,223][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 20:39:42,223][233954] FPS: 334726.17[0m
[36m[2023-07-11 20:39:46,388][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:39:46,388][233954] Reward + Measures: [[35.05445782  0.98708934  0.99115098  0.99268228  0.046277    0.64791209]][0m
[37m[1m[2023-07-11 20:39:46,388][233954] Max Reward on eval: 35.05445781958792[0m
[37m[1m[2023-07-11 20:39:46,388][233954] Min Reward on eval: 35.05445781958792[0m
[37m[1m[2023-07-11 20:39:46,389][233954] Mean Reward across all agents: 35.05445781958792[0m
[37m[1m[2023-07-11 20:39:46,389][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:39:51,316][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:39:51,317][233954] Reward + Measures: [[13.60155444  0.97339994  0.96400005  0.97749996  0.81800002  0.74840635]
 [13.25625181  0.92360002  0.8538      0.8738001   0.29159999  0.69837737]
 [ 4.82629543  0.97830003  0.99140006  0.98790008  0.84650004  0.73113698]
 ...
 [47.82120159  0.89040005  0.70069999  0.79070002  0.0147      1.03451753]
 [ 1.34902797  0.90859997  0.96760005  0.92320007  0.86300009  0.78606343]
 [ 8.38492529  0.97670001  0.97920001  0.98130006  0.89150012  0.75014919]][0m
[37m[1m[2023-07-11 20:39:51,317][233954] Max Reward on eval: 279.1878834156785[0m
[37m[1m[2023-07-11 20:39:51,317][233954] Min Reward on eval: -167.32675098474138[0m
[37m[1m[2023-07-11 20:39:51,318][233954] Mean Reward across all agents: 28.088510242085842[0m
[37m[1m[2023-07-11 20:39:51,318][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:39:51,321][233954] mean_value=-78.64013880037547, max_value=569.937795643229[0m
[37m[1m[2023-07-11 20:39:51,324][233954] New mean coefficients: [[ 1.0650084  -0.7747909   6.8956685  -0.60390556 -0.9982339  -2.1574552 ]][0m
[37m[1m[2023-07-11 20:39:51,325][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:40:00,235][233954] train() took 8.91 seconds to complete[0m
[36m[2023-07-11 20:40:00,235][233954] FPS: 431052.33[0m
[36m[2023-07-11 20:40:00,238][233954] itr=1482, itrs=2000, Progress: 74.10%[0m
[36m[2023-07-11 20:40:11,971][233954] train() took 11.61 seconds to complete[0m
[36m[2023-07-11 20:40:11,972][233954] FPS: 330673.53[0m
[36m[2023-07-11 20:40:16,171][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:40:16,171][233954] Reward + Measures: [[24.39124883  0.99204725  0.97868866  0.99126267  0.01618667  0.63597953]][0m
[37m[1m[2023-07-11 20:40:16,171][233954] Max Reward on eval: 24.39124882752205[0m
[37m[1m[2023-07-11 20:40:16,172][233954] Min Reward on eval: 24.39124882752205[0m
[37m[1m[2023-07-11 20:40:16,172][233954] Mean Reward across all agents: 24.39124882752205[0m
[37m[1m[2023-07-11 20:40:16,172][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:40:21,169][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:40:21,170][233954] Reward + Measures: [[42.08579874  0.76820004  0.75170004  0.72199994  0.10420001  0.86014396]
 [75.82090772  0.80769998  0.51879996  0.67610002  0.1251      0.74456924]
 [19.1729752   0.9339      0.72240001  0.94099998  0.41550002  0.76504701]
 ...
 [-3.10096806  0.64930004  0.65880001  0.70220006  0.123       1.02424514]
 [ 3.24639492  0.89860004  0.88490003  0.90539998  0.56690001  1.77517593]
 [71.46237701  0.46049997  0.52500004  0.40129995  0.26690003  1.09411466]][0m
[37m[1m[2023-07-11 20:40:21,170][233954] Max Reward on eval: 204.03638837006875[0m
[37m[1m[2023-07-11 20:40:21,170][233954] Min Reward on eval: -169.40037059916648[0m
[37m[1m[2023-07-11 20:40:21,171][233954] Mean Reward across all agents: 34.091785895097246[0m
[37m[1m[2023-07-11 20:40:21,171][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:40:21,174][233954] mean_value=-209.04553940966161, max_value=323.26804458245385[0m
[37m[1m[2023-07-11 20:40:21,176][233954] New mean coefficients: [[ 0.6266063  -0.33116078  7.3390927  -0.46489167 -1.1286998  -2.499677  ]][0m
[37m[1m[2023-07-11 20:40:21,177][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:40:30,255][233954] train() took 9.08 seconds to complete[0m
[36m[2023-07-11 20:40:30,260][233954] FPS: 423090.09[0m
[36m[2023-07-11 20:40:30,263][233954] itr=1483, itrs=2000, Progress: 74.15%[0m
[36m[2023-07-11 20:40:42,080][233954] train() took 11.70 seconds to complete[0m
[36m[2023-07-11 20:40:42,080][233954] FPS: 328196.81[0m
[36m[2023-07-11 20:40:46,334][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:40:46,334][233954] Reward + Measures: [[38.55464182  0.98740935  0.98774838  0.98879898  0.043356    0.66265655]][0m
[37m[1m[2023-07-11 20:40:46,334][233954] Max Reward on eval: 38.55464182068551[0m
[37m[1m[2023-07-11 20:40:46,335][233954] Min Reward on eval: 38.55464182068551[0m
[37m[1m[2023-07-11 20:40:46,335][233954] Mean Reward across all agents: 38.55464182068551[0m
[37m[1m[2023-07-11 20:40:46,335][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:40:51,274][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:40:51,283][233954] Reward + Measures: [[ 19.49442064   0.97480005   0.98990005   0.95559996   0.47080001
    0.76585478]
 [ 27.97702766   0.98579997   0.9806       0.97659999   0.0295
    0.73913199]
 [-11.10495711   0.76070005   0.95419997   0.57479995   0.89510006
    0.8051489 ]
 ...
 [ 50.51639328   0.53930008   0.8858       0.47870001   0.62099999
    0.69616973]
 [ 79.07181027   0.52920002   0.70730001   0.40790001   0.41879997
    0.84035426]
 [  7.69483015   0.98060006   0.99080002   0.9835       0.96490002
    0.88994294]][0m
[37m[1m[2023-07-11 20:40:51,284][233954] Max Reward on eval: 219.92527485748286[0m
[37m[1m[2023-07-11 20:40:51,284][233954] Min Reward on eval: -119.29106163326651[0m
[37m[1m[2023-07-11 20:40:51,285][233954] Mean Reward across all agents: 56.30652085648971[0m
[37m[1m[2023-07-11 20:40:51,285][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:40:51,295][233954] mean_value=-39.06883048510128, max_value=432.3671829958[0m
[37m[1m[2023-07-11 20:40:51,299][233954] New mean coefficients: [[ 0.74893963 -0.2429061   7.427063   -0.466805   -1.0005283  -2.8604665 ]][0m
[37m[1m[2023-07-11 20:40:51,301][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:41:00,278][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 20:41:00,278][233954] FPS: 427820.84[0m
[36m[2023-07-11 20:41:00,281][233954] itr=1484, itrs=2000, Progress: 74.20%[0m
[36m[2023-07-11 20:41:11,901][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 20:41:11,902][233954] FPS: 333797.38[0m
[36m[2023-07-11 20:41:16,174][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:41:16,174][233954] Reward + Measures: [[3.03956465 0.58842397 0.93883365 0.071286   0.82405275 0.73829311]][0m
[37m[1m[2023-07-11 20:41:16,174][233954] Max Reward on eval: 3.039564646889087[0m
[37m[1m[2023-07-11 20:41:16,175][233954] Min Reward on eval: 3.039564646889087[0m
[37m[1m[2023-07-11 20:41:16,175][233954] Mean Reward across all agents: 3.039564646889087[0m
[37m[1m[2023-07-11 20:41:16,175][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:41:21,166][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:41:21,166][233954] Reward + Measures: [[ 57.86090448   0.85330009   0.93580002   0.09730001   0.90039998
    0.83714545]
 [ 25.91707671   0.92060006   0.91940004   0.9077999    0.0408
    0.84885502]
 [ 67.87268007   0.49090001   0.55870003   0.38910002   0.28449997
    1.33976233]
 ...
 [-20.07051884   0.82369995   0.93360007   0.0025       0.8889001
    0.81978112]
 [ 28.75157975   0.64740008   0.713        0.53500003   0.27620003
    1.09142911]
 [ 61.71136119   0.90530008   0.94370002   0.83549994   0.21209998
    0.73373824]][0m
[37m[1m[2023-07-11 20:41:21,167][233954] Max Reward on eval: 194.92419720294419[0m
[37m[1m[2023-07-11 20:41:21,167][233954] Min Reward on eval: -76.56023309570737[0m
[37m[1m[2023-07-11 20:41:21,167][233954] Mean Reward across all agents: 67.88472912119464[0m
[37m[1m[2023-07-11 20:41:21,167][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:41:21,174][233954] mean_value=-38.32127992255728, max_value=435.3014944801952[0m
[37m[1m[2023-07-11 20:41:21,182][233954] New mean coefficients: [[ 0.63260305 -0.75563776  7.3431425  -0.5151333  -0.93143994 -3.0485704 ]][0m
[37m[1m[2023-07-11 20:41:21,183][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:41:30,166][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 20:41:30,166][233954] FPS: 427559.59[0m
[36m[2023-07-11 20:41:30,168][233954] itr=1485, itrs=2000, Progress: 74.25%[0m
[36m[2023-07-11 20:41:41,742][233954] train() took 11.45 seconds to complete[0m
[36m[2023-07-11 20:41:41,742][233954] FPS: 335261.94[0m
[36m[2023-07-11 20:41:46,007][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:41:46,008][233954] Reward + Measures: [[-8.12258787  0.41379631  0.96906835  0.08254033  0.85798103  0.68480963]][0m
[37m[1m[2023-07-11 20:41:46,008][233954] Max Reward on eval: -8.12258786977779[0m
[37m[1m[2023-07-11 20:41:46,008][233954] Min Reward on eval: -8.12258786977779[0m
[37m[1m[2023-07-11 20:41:46,008][233954] Mean Reward across all agents: -8.12258786977779[0m
[37m[1m[2023-07-11 20:41:46,009][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:41:51,037][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:41:51,043][233954] Reward + Measures: [[55.55870509  0.35879999  0.78570002  0.3062      0.59320003  1.01310158]
 [43.9300118   0.26370001  0.90100002  0.16700001  0.74220002  0.88643259]
 [65.6537378   0.36680004  0.84109992  0.10940001  0.60750002  0.97993785]
 ...
 [53.09896415  0.3048      0.87050003  0.14820002  0.72559994  0.85662061]
 [39.31003821  0.27800003  0.85150003  0.109       0.7281      1.007218  ]
 [-9.13968636  0.47080001  0.90090001  0.1182      0.72329998  0.68401337]][0m
[37m[1m[2023-07-11 20:41:51,043][233954] Max Reward on eval: 293.21453570043667[0m
[37m[1m[2023-07-11 20:41:51,044][233954] Min Reward on eval: -134.75614261534065[0m
[37m[1m[2023-07-11 20:41:51,044][233954] Mean Reward across all agents: 12.857708577907506[0m
[37m[1m[2023-07-11 20:41:51,044][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:41:51,050][233954] mean_value=-36.75120142276729, max_value=530.9753837560804[0m
[37m[1m[2023-07-11 20:41:51,053][233954] New mean coefficients: [[ 0.82372046 -1.3454261   7.2383084  -1.6438153  -0.8149417  -3.3654985 ]][0m
[37m[1m[2023-07-11 20:41:51,054][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:42:00,183][233954] train() took 9.13 seconds to complete[0m
[36m[2023-07-11 20:42:00,184][233954] FPS: 420676.68[0m
[36m[2023-07-11 20:42:00,186][233954] itr=1486, itrs=2000, Progress: 74.30%[0m
[36m[2023-07-11 20:42:11,948][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 20:42:11,948][233954] FPS: 329891.19[0m
[36m[2023-07-11 20:42:16,494][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:42:16,494][233954] Reward + Measures: [[77.6599452   0.29643968  0.87285161  0.50557834  0.59025699  0.70212346]][0m
[37m[1m[2023-07-11 20:42:16,494][233954] Max Reward on eval: 77.65994520249083[0m
[37m[1m[2023-07-11 20:42:16,495][233954] Min Reward on eval: 77.65994520249083[0m
[37m[1m[2023-07-11 20:42:16,495][233954] Mean Reward across all agents: 77.65994520249083[0m
[37m[1m[2023-07-11 20:42:16,495][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:42:21,442][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:42:21,442][233954] Reward + Measures: [[ 88.76533073   0.07579999   0.80699998   0.53190005   0.58170003
    0.92296821]
 [ 75.24993726   0.46989998   0.78809994   0.64930004   0.51810002
    0.87146133]
 [121.60993829   0.72860003   0.86940002   0.54229999   0.64820004
    0.83586258]
 ...
 [ 53.47053649   0.63349998   0.78210002   0.81210005   0.50570005
    0.8098737 ]
 [ 83.78847425   0.28340003   0.78590006   0.3917       0.57580006
    0.81039333]
 [ 16.31916985   0.20280002   0.91830009   0.6103       0.58740002
    0.76622814]][0m
[37m[1m[2023-07-11 20:42:21,443][233954] Max Reward on eval: 212.15185642712748[0m
[37m[1m[2023-07-11 20:42:21,443][233954] Min Reward on eval: -131.01898475731724[0m
[37m[1m[2023-07-11 20:42:21,443][233954] Mean Reward across all agents: 81.0604767700162[0m
[37m[1m[2023-07-11 20:42:21,443][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:42:21,455][233954] mean_value=123.88807343334624, max_value=630.3576316760852[0m
[37m[1m[2023-07-11 20:42:21,458][233954] New mean coefficients: [[ 0.7334663 -1.2942655  6.2621846 -0.9789323 -0.8193443 -2.924424 ]][0m
[37m[1m[2023-07-11 20:42:21,459][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:42:30,433][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 20:42:30,433][233954] FPS: 427999.56[0m
[36m[2023-07-11 20:42:30,435][233954] itr=1487, itrs=2000, Progress: 74.35%[0m
[36m[2023-07-11 20:42:42,152][233954] train() took 11.60 seconds to complete[0m
[36m[2023-07-11 20:42:42,157][233954] FPS: 331034.42[0m
[36m[2023-07-11 20:42:46,466][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:42:46,467][233954] Reward + Measures: [[80.52805835  0.25969967  0.89478791  0.51826799  0.59533     0.68533486]][0m
[37m[1m[2023-07-11 20:42:46,467][233954] Max Reward on eval: 80.52805834860122[0m
[37m[1m[2023-07-11 20:42:46,467][233954] Min Reward on eval: 80.52805834860122[0m
[37m[1m[2023-07-11 20:42:46,467][233954] Mean Reward across all agents: 80.52805834860122[0m
[37m[1m[2023-07-11 20:42:46,468][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:42:51,496][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:42:51,496][233954] Reward + Measures: [[ 37.04773181   0.32979998   0.84650004   0.44260001   0.58340001
    0.76313204]
 [114.2240343    0.4659       0.82770008   0.36269999   0.60350001
    0.81382722]
 [115.26781561   0.39949998   0.79760003   0.30450001   0.64219999
    0.83454764]
 ...
 [118.87398725   0.53140002   0.81440002   0.31590003   0.69750005
    0.83663696]
 [103.92994354   0.61770004   0.76029998   0.3933       0.61120003
    0.88898939]
 [199.90724271   0.75770003   0.53700006   0.62290001   0.31239998
    1.45131111]][0m
[37m[1m[2023-07-11 20:42:51,496][233954] Max Reward on eval: 199.9072427133098[0m
[37m[1m[2023-07-11 20:42:51,497][233954] Min Reward on eval: -35.22201422601938[0m
[37m[1m[2023-07-11 20:42:51,497][233954] Mean Reward across all agents: 81.52343916577244[0m
[37m[1m[2023-07-11 20:42:51,497][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:42:51,505][233954] mean_value=121.15471373890148, max_value=587.8363762304186[0m
[37m[1m[2023-07-11 20:42:51,508][233954] New mean coefficients: [[-0.6912319   0.18662727  6.3854117  -0.4408716  -1.203455   -2.8856332 ]][0m
[37m[1m[2023-07-11 20:42:51,509][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:43:00,606][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 20:43:00,606][233954] FPS: 422183.91[0m
[36m[2023-07-11 20:43:00,609][233954] itr=1488, itrs=2000, Progress: 74.40%[0m
[36m[2023-07-11 20:43:12,366][233954] train() took 11.64 seconds to complete[0m
[36m[2023-07-11 20:43:12,367][233954] FPS: 329945.70[0m
[36m[2023-07-11 20:43:16,623][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:43:16,624][233954] Reward + Measures: [[77.31304471  0.25310934  0.90488434  0.47942898  0.57798499  0.66926855]][0m
[37m[1m[2023-07-11 20:43:16,624][233954] Max Reward on eval: 77.31304471192416[0m
[37m[1m[2023-07-11 20:43:16,624][233954] Min Reward on eval: 77.31304471192416[0m
[37m[1m[2023-07-11 20:43:16,624][233954] Mean Reward across all agents: 77.31304471192416[0m
[37m[1m[2023-07-11 20:43:16,625][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:43:21,584][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:43:21,584][233954] Reward + Measures: [[ 130.54733371    0.1744        0.88500005    0.45460001    0.60549998
     0.66427082]
 [ 136.64103505    0.60500002    0.90270007    0.38400003    0.69950002
     0.68198591]
 [ -59.27073607    0.45380002    0.71439999    0.27480003    0.60430002
     0.86697483]
 ...
 [-102.90039875    0.59880006    0.76340002    0.32510003    0.62530005
     0.81626838]
 [  11.61823843    0.0829        0.91720009    0.44840002    0.80500001
     0.86346674]
 [ 164.1474123     0.34279999    0.87250006    0.40459999    0.59039998
     0.67367822]][0m
[37m[1m[2023-07-11 20:43:21,585][233954] Max Reward on eval: 177.51839824952185[0m
[37m[1m[2023-07-11 20:43:21,585][233954] Min Reward on eval: -112.96970130577684[0m
[37m[1m[2023-07-11 20:43:21,585][233954] Mean Reward across all agents: 40.463733403856914[0m
[37m[1m[2023-07-11 20:43:21,585][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:43:21,592][233954] mean_value=-11.4075141901766, max_value=426.29409424759604[0m
[37m[1m[2023-07-11 20:43:21,595][233954] New mean coefficients: [[ 0.04829717 -0.2144331   6.7301188  -0.10529459 -0.12923694 -3.2028332 ]][0m
[37m[1m[2023-07-11 20:43:21,596][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:43:30,550][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 20:43:30,556][233954] FPS: 428925.82[0m
[36m[2023-07-11 20:43:30,558][233954] itr=1489, itrs=2000, Progress: 74.45%[0m
[36m[2023-07-11 20:43:42,290][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 20:43:42,291][233954] FPS: 330592.47[0m
[36m[2023-07-11 20:43:46,596][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:43:46,596][233954] Reward + Measures: [[47.54862302  0.12461001  0.91576463  0.52045435  0.55093265  0.64118975]][0m
[37m[1m[2023-07-11 20:43:46,597][233954] Max Reward on eval: 47.548623015163[0m
[37m[1m[2023-07-11 20:43:46,597][233954] Min Reward on eval: 47.548623015163[0m
[37m[1m[2023-07-11 20:43:46,597][233954] Mean Reward across all agents: 47.548623015163[0m
[37m[1m[2023-07-11 20:43:46,597][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:43:51,631][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:43:51,683][233954] Reward + Measures: [[-27.69642686   0.13020001   0.86739999   0.34059998   0.68970007
    0.95991534]
 [ 68.83924028   0.1005       0.82380003   0.55500001   0.51990002
    0.69713026]
 [ 47.76453856   0.25299999   0.87470001   0.23669998   0.72909999
    0.82547134]
 ...
 [-53.32362296   0.1128       0.85859996   0.39420006   0.71810001
    0.76680154]
 [109.46655129   0.24419999   0.76670003   0.54460001   0.40560004
    0.76181871]
 [-12.87648156   0.41990003   0.67640007   0.48540002   0.38180003
    0.80338699]][0m
[37m[1m[2023-07-11 20:43:51,683][233954] Max Reward on eval: 162.00275177564473[0m
[37m[1m[2023-07-11 20:43:51,684][233954] Min Reward on eval: -93.93330859299749[0m
[37m[1m[2023-07-11 20:43:51,684][233954] Mean Reward across all agents: 33.030077889419395[0m
[37m[1m[2023-07-11 20:43:51,684][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:43:51,690][233954] mean_value=-4.8590172349551475, max_value=600.1836571843363[0m
[37m[1m[2023-07-11 20:43:51,693][233954] New mean coefficients: [[ 0.42221892 -0.4506102   6.1236987   0.7678276  -0.04463559 -3.2151024 ]][0m
[37m[1m[2023-07-11 20:43:51,694][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:44:00,749][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 20:44:00,749][233954] FPS: 424140.78[0m
[36m[2023-07-11 20:44:00,752][233954] itr=1490, itrs=2000, Progress: 74.50%[0m
[37m[1m[2023-07-11 20:47:52,734][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001470[0m
[36m[2023-07-11 20:48:06,070][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 20:48:06,070][233954] FPS: 330537.50[0m
[36m[2023-07-11 20:48:10,257][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:48:10,257][233954] Reward + Measures: [[45.52943783  0.82430208  0.86603802  0.62601733  0.44342235  0.73082876]][0m
[37m[1m[2023-07-11 20:48:10,257][233954] Max Reward on eval: 45.52943782670932[0m
[37m[1m[2023-07-11 20:48:10,258][233954] Min Reward on eval: 45.52943782670932[0m
[37m[1m[2023-07-11 20:48:10,258][233954] Mean Reward across all agents: 45.52943782670932[0m
[37m[1m[2023-07-11 20:48:10,258][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:48:15,226][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:48:15,226][233954] Reward + Measures: [[ 60.89323764   0.76139992   0.58170003   0.73369998   0.1428
    0.85160732]
 [-31.41124357   0.76550001   0.5244       0.67219996   0.20680001
    0.70165968]
 [ 23.27950671   0.44119999   0.412        0.45269999   0.26710001
    1.31168604]
 ...
 [-42.0291237    0.76600009   0.57300007   0.6573       0.222
    0.69909286]
 [  0.15482067   0.61490005   0.57609999   0.59090006   0.34250003
    0.71067011]
 [ 14.83651307   0.7159       0.88990003   0.47569999   0.63099998
    0.76985544]][0m
[37m[1m[2023-07-11 20:48:15,227][233954] Max Reward on eval: 207.88483905130997[0m
[37m[1m[2023-07-11 20:48:15,227][233954] Min Reward on eval: -138.15528109627775[0m
[37m[1m[2023-07-11 20:48:15,227][233954] Mean Reward across all agents: 27.012441099911086[0m
[37m[1m[2023-07-11 20:48:15,227][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:48:15,230][233954] mean_value=-183.02157611260017, max_value=205.61387243120163[0m
[37m[1m[2023-07-11 20:48:15,244][233954] New mean coefficients: [[ 0.43156308 -0.507686    6.2944207   0.3893338  -0.11262045 -3.1260693 ]][0m
[37m[1m[2023-07-11 20:48:15,245][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:48:24,195][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 20:48:24,196][233954] FPS: 429118.60[0m
[36m[2023-07-11 20:48:24,198][233954] itr=1491, itrs=2000, Progress: 74.55%[0m
[36m[2023-07-11 20:48:35,818][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 20:48:35,818][233954] FPS: 333784.96[0m
[36m[2023-07-11 20:48:40,019][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:48:40,019][233954] Reward + Measures: [[47.27642906  0.84431934  0.89394629  0.65947133  0.46870166  0.72285968]][0m
[37m[1m[2023-07-11 20:48:40,019][233954] Max Reward on eval: 47.27642906036077[0m
[37m[1m[2023-07-11 20:48:40,020][233954] Min Reward on eval: 47.27642906036077[0m
[37m[1m[2023-07-11 20:48:40,020][233954] Mean Reward across all agents: 47.27642906036077[0m
[37m[1m[2023-07-11 20:48:40,020][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:48:44,948][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:48:44,948][233954] Reward + Measures: [[-56.186086     0.55680001   0.5226       0.43529996   0.22989999
    1.0977602 ]
 [ 43.41129999   0.41640002   0.75270003   0.20060001   0.51679999
    1.40400124]
 [181.3113652    0.42090002   0.67839998   0.37939999   0.46410003
    0.97488308]
 ...
 [ 63.08226895   0.84280008   0.95879996   0.59000009   0.76850003
    0.71091682]
 [ 10.74262964   0.77060002   0.73400003   0.59170002   0.13170002
    0.76042533]
 [203.61633394   0.50200003   0.65560001   0.4698       0.20060001
    0.94856614]][0m
[37m[1m[2023-07-11 20:48:44,949][233954] Max Reward on eval: 335.57367322817447[0m
[37m[1m[2023-07-11 20:48:44,949][233954] Min Reward on eval: -324.71126746423545[0m
[37m[1m[2023-07-11 20:48:44,949][233954] Mean Reward across all agents: 36.18238816302722[0m
[37m[1m[2023-07-11 20:48:44,949][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:48:44,954][233954] mean_value=-213.23901934699236, max_value=546.3763178598415[0m
[37m[1m[2023-07-11 20:48:44,957][233954] New mean coefficients: [[ 1.1914513  -1.2428454   5.3996353   0.52857906 -0.6775007  -3.2958496 ]][0m
[37m[1m[2023-07-11 20:48:44,958][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:48:53,909][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 20:48:53,910][233954] FPS: 429065.78[0m
[36m[2023-07-11 20:48:53,912][233954] itr=1492, itrs=2000, Progress: 74.60%[0m
[36m[2023-07-11 20:49:05,537][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 20:49:05,537][233954] FPS: 333824.77[0m
[36m[2023-07-11 20:49:09,782][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:49:09,782][233954] Reward + Measures: [[56.80704695  0.86410433  0.91553092  0.69746828  0.51291764  0.71785527]][0m
[37m[1m[2023-07-11 20:49:09,782][233954] Max Reward on eval: 56.807046950207564[0m
[37m[1m[2023-07-11 20:49:09,783][233954] Min Reward on eval: 56.807046950207564[0m
[37m[1m[2023-07-11 20:49:09,783][233954] Mean Reward across all agents: 56.807046950207564[0m
[37m[1m[2023-07-11 20:49:09,783][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:49:14,747][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:49:14,748][233954] Reward + Measures: [[ 24.04635072   0.27079999   0.54619998   0.24829999   0.45089999
    1.21423709]
 [-99.24061157   0.37490004   0.71880001   0.45899996   0.51029998
    0.72299689]
 [-47.55031549   0.47019997   0.65630001   0.43109998   0.45720002
    0.81873387]
 ...
 [-19.85203893   0.49920002   0.51809996   0.32329997   0.40009999
    1.14940536]
 [-57.42464921   0.80540007   0.69559997   0.68709999   0.21340001
    0.60011446]
 [105.06168892   0.86899996   0.94150001   0.6469       0.74879998
    0.84153938]][0m
[37m[1m[2023-07-11 20:49:14,748][233954] Max Reward on eval: 224.43271827474237[0m
[37m[1m[2023-07-11 20:49:14,748][233954] Min Reward on eval: -183.92036011600868[0m
[37m[1m[2023-07-11 20:49:14,748][233954] Mean Reward across all agents: 24.58547128111401[0m
[37m[1m[2023-07-11 20:49:14,749][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:49:14,753][233954] mean_value=-231.70054352290205, max_value=234.91919793134016[0m
[37m[1m[2023-07-11 20:49:14,756][233954] New mean coefficients: [[ 1.2476438  -0.7697673   5.451496    1.1071744   0.37040067 -3.0384715 ]][0m
[37m[1m[2023-07-11 20:49:14,757][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:49:23,694][233954] train() took 8.94 seconds to complete[0m
[36m[2023-07-11 20:49:23,694][233954] FPS: 429761.36[0m
[36m[2023-07-11 20:49:23,696][233954] itr=1493, itrs=2000, Progress: 74.65%[0m
[36m[2023-07-11 20:49:35,590][233954] train() took 11.78 seconds to complete[0m
[36m[2023-07-11 20:49:35,591][233954] FPS: 326011.77[0m
[36m[2023-07-11 20:49:39,880][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:49:39,880][233954] Reward + Measures: [[43.24342895  0.86387026  0.92572767  0.70510036  0.54030031  0.71148372]][0m
[37m[1m[2023-07-11 20:49:39,881][233954] Max Reward on eval: 43.243428950735066[0m
[37m[1m[2023-07-11 20:49:39,881][233954] Min Reward on eval: 43.243428950735066[0m
[37m[1m[2023-07-11 20:49:39,881][233954] Mean Reward across all agents: 43.243428950735066[0m
[37m[1m[2023-07-11 20:49:39,881][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:49:44,874][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:49:44,874][233954] Reward + Measures: [[ 57.73482509   0.48619995   0.66069996   0.16440001   0.50709999
    1.04563642]
 [ 60.45125701   0.79030001   0.89219999   0.54330003   0.57540005
    0.74831057]
 [ 80.13422013   0.49839997   0.70730001   0.2554       0.56819999
    0.95716679]
 ...
 [ 27.35553693   0.57260007   0.59899998   0.61440003   0.36530003
    1.30444801]
 [-11.06667464   0.90999997   0.82870001   0.92469996   0.0197
    0.89017719]
 [ 50.79903365   0.82499999   0.9224       0.308        0.67730004
    0.86437076]][0m
[37m[1m[2023-07-11 20:49:44,875][233954] Max Reward on eval: 162.41373881255277[0m
[37m[1m[2023-07-11 20:49:44,875][233954] Min Reward on eval: -27.546891371300443[0m
[37m[1m[2023-07-11 20:49:44,875][233954] Mean Reward across all agents: 42.96835124346914[0m
[37m[1m[2023-07-11 20:49:44,875][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:49:44,879][233954] mean_value=-71.38287642420165, max_value=459.2466203431311[0m
[37m[1m[2023-07-11 20:49:44,882][233954] New mean coefficients: [[ 0.94080967 -0.71506506  5.9618874   0.08140171 -0.13565004 -3.651713  ]][0m
[37m[1m[2023-07-11 20:49:44,883][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:49:53,953][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 20:49:53,954][233954] FPS: 423419.61[0m
[36m[2023-07-11 20:49:53,956][233954] itr=1494, itrs=2000, Progress: 74.70%[0m
[36m[2023-07-11 20:50:05,702][233954] train() took 11.63 seconds to complete[0m
[36m[2023-07-11 20:50:05,702][233954] FPS: 330173.63[0m
[36m[2023-07-11 20:50:09,966][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:50:09,966][233954] Reward + Measures: [[9.88733955 0.954166   0.9488247  0.94068635 0.29763466 0.72856778]][0m
[37m[1m[2023-07-11 20:50:09,966][233954] Max Reward on eval: 9.887339548714488[0m
[37m[1m[2023-07-11 20:50:09,967][233954] Min Reward on eval: 9.887339548714488[0m
[37m[1m[2023-07-11 20:50:09,967][233954] Mean Reward across all agents: 9.887339548714488[0m
[37m[1m[2023-07-11 20:50:09,967][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:50:14,925][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:50:14,926][233954] Reward + Measures: [[70.73551453  0.72850007  0.69979995  0.64120007  0.26200002  0.9781146 ]
 [30.44868184  0.90679997  0.77170002  0.86059994  0.2208      0.57890475]
 [41.91339404  0.92309999  0.875       0.89670002  0.21760002  0.71865612]
 ...
 [62.29127025  0.68600005  0.7859      0.66230005  0.67390007  0.69802964]
 [42.52298329  0.79249996  0.70910001  0.73699999  0.66110003  0.65455198]
 [53.56011388  0.84209996  0.76880002  0.7926001   0.1674      0.83122128]][0m
[37m[1m[2023-07-11 20:50:14,926][233954] Max Reward on eval: 147.23131750398315[0m
[37m[1m[2023-07-11 20:50:14,926][233954] Min Reward on eval: -181.19076918861828[0m
[37m[1m[2023-07-11 20:50:14,927][233954] Mean Reward across all agents: 26.289305504245014[0m
[37m[1m[2023-07-11 20:50:14,927][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:50:14,930][233954] mean_value=-75.91843719655581, max_value=368.34348233020745[0m
[37m[1m[2023-07-11 20:50:14,933][233954] New mean coefficients: [[ 0.7018955 -0.8628035  6.3286943 -0.3611392  0.2375102 -4.0031013]][0m
[37m[1m[2023-07-11 20:50:14,934][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:50:23,900][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 20:50:23,900][233954] FPS: 428348.36[0m
[36m[2023-07-11 20:50:23,903][233954] itr=1495, itrs=2000, Progress: 74.75%[0m
[36m[2023-07-11 20:50:35,693][233954] train() took 11.67 seconds to complete[0m
[36m[2023-07-11 20:50:35,694][233954] FPS: 329045.67[0m
[36m[2023-07-11 20:50:39,911][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:50:39,911][233954] Reward + Measures: [[56.46189653  0.70235634  0.97460431  0.38713631  0.82717204  0.75398207]][0m
[37m[1m[2023-07-11 20:50:39,911][233954] Max Reward on eval: 56.46189653380062[0m
[37m[1m[2023-07-11 20:50:39,912][233954] Min Reward on eval: 56.46189653380062[0m
[37m[1m[2023-07-11 20:50:39,912][233954] Mean Reward across all agents: 56.46189653380062[0m
[37m[1m[2023-07-11 20:50:39,912][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:50:44,870][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:50:44,870][233954] Reward + Measures: [[ 135.0552628     0.67510003    0.91340011    0.0645        0.6997
     0.8195284 ]
 [-101.3878574     0.57779998    0.73790002    0.38060004    0.43059999
     0.61972612]
 [  42.5199201     0.8229        0.80479997    0.59250003    0.23729999
     1.00083923]
 ...
 [  70.3803587     0.79530001    0.90469998    0.59320003    0.62519997
     0.76954269]
 [ 124.03585337    0.72860003    0.86689997    0.2692        0.55250001
     0.79596925]
 [   2.26485157    0.87319994    0.63440001    0.82340002    0.1019
     0.74793875]][0m
[37m[1m[2023-07-11 20:50:44,871][233954] Max Reward on eval: 184.79217670802026[0m
[37m[1m[2023-07-11 20:50:44,871][233954] Min Reward on eval: -226.35279273800552[0m
[37m[1m[2023-07-11 20:50:44,871][233954] Mean Reward across all agents: 45.558527819375044[0m
[37m[1m[2023-07-11 20:50:44,871][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:50:44,877][233954] mean_value=-44.206568682107246, max_value=491.13215616249806[0m
[37m[1m[2023-07-11 20:50:44,880][233954] New mean coefficients: [[ 0.61453235 -0.5501827   6.707148   -0.38032886  0.97054374 -4.081983  ]][0m
[37m[1m[2023-07-11 20:50:44,881][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:50:53,891][233954] train() took 9.01 seconds to complete[0m
[36m[2023-07-11 20:50:53,891][233954] FPS: 426255.51[0m
[36m[2023-07-11 20:50:53,894][233954] itr=1496, itrs=2000, Progress: 74.80%[0m
[36m[2023-07-11 20:51:05,676][233954] train() took 11.66 seconds to complete[0m
[36m[2023-07-11 20:51:05,676][233954] FPS: 329250.62[0m
[36m[2023-07-11 20:51:09,993][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:51:09,994][233954] Reward + Measures: [[-83.58802455   0.67119801   0.92494172   0.22279333   0.64539999
    0.82496583]][0m
[37m[1m[2023-07-11 20:51:09,994][233954] Max Reward on eval: -83.58802455483446[0m
[37m[1m[2023-07-11 20:51:09,994][233954] Min Reward on eval: -83.58802455483446[0m
[37m[1m[2023-07-11 20:51:09,995][233954] Mean Reward across all agents: -83.58802455483446[0m
[37m[1m[2023-07-11 20:51:09,995][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:51:15,292][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:51:15,293][233954] Reward + Measures: [[-47.13400457   0.78220004   0.97259998   0.0081       0.79400009
    0.85792553]
 [-77.62758115   0.84139997   0.8818       0.46070001   0.50159997
    0.89703888]
 [149.77055021   0.36310002   0.60110003   0.19990002   0.54759997
    1.05066025]
 ...
 [ 61.28216209   0.32300001   0.5528       0.21750002   0.43070003
    0.96606934]
 [113.64697168   0.36250001   0.75360006   0.0871       0.60970002
    0.97814465]
 [ 58.58906466   0.47230005   0.69980001   0.40739998   0.54089999
    0.92265624]][0m
[37m[1m[2023-07-11 20:51:15,293][233954] Max Reward on eval: 175.5261058807373[0m
[37m[1m[2023-07-11 20:51:15,293][233954] Min Reward on eval: -255.31575108077377[0m
[37m[1m[2023-07-11 20:51:15,294][233954] Mean Reward across all agents: 23.19682427135141[0m
[37m[1m[2023-07-11 20:51:15,294][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:51:15,298][233954] mean_value=-195.1909771195765, max_value=326.72549857293[0m
[37m[1m[2023-07-11 20:51:15,300][233954] New mean coefficients: [[ 1.0498868  -1.2628846   5.815533   -0.28627375  1.0414845  -3.7280025 ]][0m
[37m[1m[2023-07-11 20:51:15,301][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:51:24,339][233954] train() took 9.04 seconds to complete[0m
[36m[2023-07-11 20:51:24,340][233954] FPS: 424953.98[0m
[36m[2023-07-11 20:51:24,342][233954] itr=1497, itrs=2000, Progress: 74.85%[0m
[36m[2023-07-11 20:51:36,410][233954] train() took 11.95 seconds to complete[0m
[36m[2023-07-11 20:51:36,410][233954] FPS: 321409.16[0m
[36m[2023-07-11 20:51:40,778][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:51:40,778][233954] Reward + Measures: [[22.16133052  0.73159635  0.83249766  0.47992301  0.55144203  0.52833676]][0m
[37m[1m[2023-07-11 20:51:40,778][233954] Max Reward on eval: 22.161330518503217[0m
[37m[1m[2023-07-11 20:51:40,779][233954] Min Reward on eval: 22.161330518503217[0m
[37m[1m[2023-07-11 20:51:40,779][233954] Mean Reward across all agents: 22.161330518503217[0m
[37m[1m[2023-07-11 20:51:40,779][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:51:45,879][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:51:45,880][233954] Reward + Measures: [[189.2186346    0.59640002   0.70979995   0.46499997   0.50159997
    0.91663229]
 [ 67.24095594   0.6347       0.81450003   0.26280001   0.62619996
    0.67325544]
 [ 60.00959246   0.83899993   0.83750004   0.73399997   0.14960001
    0.53579146]
 ...
 [ 84.91116866   0.31529999   0.87519997   0.12800001   0.67790002
    0.72142005]
 [ 48.0165895    0.53470004   0.81689996   0.40529999   0.54590005
    0.64174551]
 [ 46.02659912   0.61000007   0.63299996   0.44350004   0.396
    0.75763512]][0m
[37m[1m[2023-07-11 20:51:45,880][233954] Max Reward on eval: 315.501981656719[0m
[37m[1m[2023-07-11 20:51:45,880][233954] Min Reward on eval: -130.24705686131492[0m
[37m[1m[2023-07-11 20:51:45,881][233954] Mean Reward across all agents: 62.97823826202252[0m
[37m[1m[2023-07-11 20:51:45,881][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:51:45,886][233954] mean_value=-208.96042352881835, max_value=533.5186340875421[0m
[37m[1m[2023-07-11 20:51:45,889][233954] New mean coefficients: [[ 1.263256   -0.95101976  5.8125606  -0.9548317   1.3555322  -3.7650712 ]][0m
[37m[1m[2023-07-11 20:51:45,890][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:51:54,960][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 20:51:54,960][233954] FPS: 423488.23[0m
[36m[2023-07-11 20:51:54,962][233954] itr=1498, itrs=2000, Progress: 74.90%[0m
[36m[2023-07-11 20:52:06,764][233954] train() took 11.69 seconds to complete[0m
[36m[2023-07-11 20:52:06,764][233954] FPS: 328620.04[0m
[36m[2023-07-11 20:52:11,011][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:52:11,012][233954] Reward + Measures: [[40.27596725  0.69263101  0.94730693  0.33602867  0.66219032  0.49795905]][0m
[37m[1m[2023-07-11 20:52:11,012][233954] Max Reward on eval: 40.275967248530954[0m
[37m[1m[2023-07-11 20:52:11,012][233954] Min Reward on eval: 40.275967248530954[0m
[37m[1m[2023-07-11 20:52:11,012][233954] Mean Reward across all agents: 40.275967248530954[0m
[37m[1m[2023-07-11 20:52:11,013][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:52:16,037][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:52:16,038][233954] Reward + Measures: [[ 44.6673179    0.39949998   0.69539994   0.1611       0.55980003
    0.9310686 ]
 [-28.69621235   0.52790004   0.92340004   0.1091       0.75670004
    0.66373789]
 [ 43.66429978   0.5995       0.8021       0.0305       0.65230006
    0.88416195]
 ...
 [-16.24498319   0.58910006   0.82860005   0.1015       0.61180001
    0.71399468]
 [-27.31979056   0.42589998   0.69150001   0.1416       0.49180004
    1.022385  ]
 [107.41128498   0.58230001   0.68370003   0.29850003   0.6983
    0.71831471]][0m
[37m[1m[2023-07-11 20:52:16,038][233954] Max Reward on eval: 225.5267410177621[0m
[37m[1m[2023-07-11 20:52:16,038][233954] Min Reward on eval: -186.367959986208[0m
[37m[1m[2023-07-11 20:52:16,038][233954] Mean Reward across all agents: 10.714484834029385[0m
[37m[1m[2023-07-11 20:52:16,039][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:52:16,042][233954] mean_value=-195.7781817858874, max_value=205.91212365433447[0m
[37m[1m[2023-07-11 20:52:16,044][233954] New mean coefficients: [[ 1.1164688  -0.4748609   5.222839    0.20148396  0.27361333 -4.154423  ]][0m
[37m[1m[2023-07-11 20:52:16,045][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:52:25,008][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 20:52:25,008][233954] FPS: 428538.89[0m
[36m[2023-07-11 20:52:25,010][233954] itr=1499, itrs=2000, Progress: 74.95%[0m
[36m[2023-07-11 20:52:36,813][233954] train() took 11.68 seconds to complete[0m
[36m[2023-07-11 20:52:36,813][233954] FPS: 328687.15[0m
[36m[2023-07-11 20:52:41,065][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:52:41,066][233954] Reward + Measures: [[66.0318977   0.27966368  0.9296577   0.221559    0.77999234  0.6317057 ]][0m
[37m[1m[2023-07-11 20:52:41,066][233954] Max Reward on eval: 66.03189770260838[0m
[37m[1m[2023-07-11 20:52:41,066][233954] Min Reward on eval: 66.03189770260838[0m
[37m[1m[2023-07-11 20:52:41,066][233954] Mean Reward across all agents: 66.03189770260838[0m
[37m[1m[2023-07-11 20:52:41,067][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:52:46,083][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:52:46,084][233954] Reward + Measures: [[ 80.85782556   0.38910002   0.91670001   0.13520001   0.73559999
    0.64388108]
 [186.48319436   0.0654       0.84450006   0.3179       0.84630007
    0.82052487]
 [ 16.29551728   0.72930002   0.56269997   0.66259998   0.17120002
    0.69833612]
 ...
 [ 83.6237488    0.0755       0.88079995   0.33879995   0.86540002
    0.70280451]
 [ 50.31582877   0.70819998   0.55470002   0.68760008   0.19270001
    0.74529165]
 [114.28003096   0.15699999   0.75950003   0.31170002   0.77969998
    1.06537187]][0m
[37m[1m[2023-07-11 20:52:46,084][233954] Max Reward on eval: 193.60764976628124[0m
[37m[1m[2023-07-11 20:52:46,084][233954] Min Reward on eval: -39.83185385204852[0m
[37m[1m[2023-07-11 20:52:46,084][233954] Mean Reward across all agents: 57.47074567941626[0m
[37m[1m[2023-07-11 20:52:46,085][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:52:46,091][233954] mean_value=18.170984375033985, max_value=597.1691589495513[0m
[37m[1m[2023-07-11 20:52:46,094][233954] New mean coefficients: [[ 0.8460238  -0.07854673  5.499765   -0.9927231   0.14548597 -3.7143185 ]][0m
[37m[1m[2023-07-11 20:52:46,095][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:52:55,060][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 20:52:55,060][233954] FPS: 428388.78[0m
[36m[2023-07-11 20:52:55,062][233954] itr=1500, itrs=2000, Progress: 75.00%[0m
[37m[1m[2023-07-11 20:56:54,324][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001480[0m
[36m[2023-07-11 20:57:06,832][233954] train() took 11.79 seconds to complete[0m
[36m[2023-07-11 20:57:06,832][233954] FPS: 325666.31[0m
[36m[2023-07-11 20:57:11,093][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:57:11,095][233954] Reward + Measures: [[58.49541259  0.15378633  0.89398265  0.327602    0.8979609   0.66242582]][0m
[37m[1m[2023-07-11 20:57:11,095][233954] Max Reward on eval: 58.4954125861648[0m
[37m[1m[2023-07-11 20:57:11,096][233954] Min Reward on eval: 58.4954125861648[0m
[37m[1m[2023-07-11 20:57:11,096][233954] Mean Reward across all agents: 58.4954125861648[0m
[37m[1m[2023-07-11 20:57:11,096][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:57:16,099][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:57:16,100][233954] Reward + Measures: [[26.2718074   0.28210002  0.9174      0.2753      0.9012      0.57282114]
 [ 0.53448787  0.6225      0.81449997  0.55330002  0.53999996  0.37847617]
 [22.08620923  0.2516      0.95619994  0.1513      0.85710001  0.5657531 ]
 ...
 [12.28707635  0.47300002  0.83990002  0.30840001  0.74480003  0.42629811]
 [16.07172712  0.31880003  0.9278      0.1523      0.83490002  0.50771207]
 [17.45160682  0.34919998  0.84380001  0.25840002  0.72530001  0.46026579]][0m
[37m[1m[2023-07-11 20:57:16,100][233954] Max Reward on eval: 93.01680282130837[0m
[37m[1m[2023-07-11 20:57:16,100][233954] Min Reward on eval: -144.20394765618258[0m
[37m[1m[2023-07-11 20:57:16,100][233954] Mean Reward across all agents: 13.603676274825101[0m
[37m[1m[2023-07-11 20:57:16,101][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:57:16,113][233954] mean_value=94.82379548532995, max_value=488.86973532946547[0m
[37m[1m[2023-07-11 20:57:16,132][233954] New mean coefficients: [[ 0.5085156   0.5837593   5.3634033  -0.69093925  0.12041025 -2.6832867 ]][0m
[37m[1m[2023-07-11 20:57:16,134][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:57:25,113][233954] train() took 8.98 seconds to complete[0m
[36m[2023-07-11 20:57:25,113][233954] FPS: 427729.98[0m
[36m[2023-07-11 20:57:25,116][233954] itr=1501, itrs=2000, Progress: 75.05%[0m
[36m[2023-07-11 20:57:36,810][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 20:57:36,811][233954] FPS: 331779.77[0m
[36m[2023-07-11 20:57:41,130][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:57:41,131][233954] Reward + Measures: [[56.01133344  0.41717163  0.56348467  0.42441133  0.67383534  0.72640061]][0m
[37m[1m[2023-07-11 20:57:41,131][233954] Max Reward on eval: 56.0113334418671[0m
[37m[1m[2023-07-11 20:57:41,131][233954] Min Reward on eval: 56.0113334418671[0m
[37m[1m[2023-07-11 20:57:41,132][233954] Mean Reward across all agents: 56.0113334418671[0m
[37m[1m[2023-07-11 20:57:41,132][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:57:46,177][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:57:46,178][233954] Reward + Measures: [[-40.91855431   0.699        0.5643       0.61970001   0.48499998
    0.74285334]
 [ 23.28167189   0.7202       0.68470001   0.66360003   0.44490001
    0.82409614]
 [ 56.09368141   0.22909999   0.7143001    0.50910002   0.79360002
    0.65755594]
 ...
 [ -2.59899636   0.48499998   0.6038       0.48330003   0.63950008
    0.74268389]
 [-43.47522954   0.72460002   0.66399997   0.63129997   0.44300005
    0.8320446 ]
 [ 27.54355882   0.51899999   0.72770005   0.60780001   0.67880005
    0.66708219]][0m
[37m[1m[2023-07-11 20:57:46,178][233954] Max Reward on eval: 125.12651421637274[0m
[37m[1m[2023-07-11 20:57:46,179][233954] Min Reward on eval: -209.6415252527222[0m
[37m[1m[2023-07-11 20:57:46,179][233954] Mean Reward across all agents: -18.016091576367387[0m
[37m[1m[2023-07-11 20:57:46,179][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:57:46,182][233954] mean_value=-92.64727077410544, max_value=607.2963164940477[0m
[37m[1m[2023-07-11 20:57:46,185][233954] New mean coefficients: [[ 0.7891109  -0.2734381   3.8684392   0.5703066   0.05831141 -2.5351217 ]][0m
[37m[1m[2023-07-11 20:57:46,186][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:57:55,302][233954] train() took 9.11 seconds to complete[0m
[36m[2023-07-11 20:57:55,303][233954] FPS: 421298.99[0m
[36m[2023-07-11 20:57:55,305][233954] itr=1502, itrs=2000, Progress: 75.10%[0m
[36m[2023-07-11 20:58:06,886][233954] train() took 11.46 seconds to complete[0m
[36m[2023-07-11 20:58:06,887][233954] FPS: 335058.95[0m
[36m[2023-07-11 20:58:11,100][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:58:11,101][233954] Reward + Measures: [[37.98720806  0.35363269  0.65093565  0.43451366  0.72568262  0.70724779]][0m
[37m[1m[2023-07-11 20:58:11,101][233954] Max Reward on eval: 37.987208056337785[0m
[37m[1m[2023-07-11 20:58:11,101][233954] Min Reward on eval: 37.987208056337785[0m
[37m[1m[2023-07-11 20:58:11,101][233954] Mean Reward across all agents: 37.987208056337785[0m
[37m[1m[2023-07-11 20:58:11,101][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:58:16,155][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:58:16,156][233954] Reward + Measures: [[  47.76316643    0.58170003    0.6534        0.39330003    0.68559998
     0.62162131]
 [ -12.77468639    0.44689998    0.76639998    0.37170002    0.59440005
     0.86592644]
 [-123.651639      0.4601        0.74540007    0.3107        0.6652
     0.7889263 ]
 ...
 [  -1.72541929    0.4131        0.82120001    0.0936        0.6221
     0.96993732]
 [  26.75550034    0.3637        0.67320001    0.27620003    0.6347
     0.83206695]
 [  25.70960151    0.2859        0.68470007    0.3091        0.76609999
     0.67776573]][0m
[37m[1m[2023-07-11 20:58:16,156][233954] Max Reward on eval: 143.62529086424038[0m
[37m[1m[2023-07-11 20:58:16,156][233954] Min Reward on eval: -246.64274597652258[0m
[37m[1m[2023-07-11 20:58:16,156][233954] Mean Reward across all agents: -21.111674987368886[0m
[37m[1m[2023-07-11 20:58:16,157][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:58:16,160][233954] mean_value=-118.6381531665892, max_value=305.232408139741[0m
[37m[1m[2023-07-11 20:58:16,163][233954] New mean coefficients: [[-0.05274916  0.22077331  3.9310126   1.586971    0.12756824 -2.416198  ]][0m
[37m[1m[2023-07-11 20:58:16,164][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:58:25,240][233954] train() took 9.07 seconds to complete[0m
[36m[2023-07-11 20:58:25,241][233954] FPS: 423145.30[0m
[36m[2023-07-11 20:58:25,243][233954] itr=1503, itrs=2000, Progress: 75.15%[0m
[36m[2023-07-11 20:58:37,124][233954] train() took 11.76 seconds to complete[0m
[36m[2023-07-11 20:58:37,124][233954] FPS: 326451.77[0m
[36m[2023-07-11 20:58:41,394][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:58:41,394][233954] Reward + Measures: [[-41.52320046   0.83795702   0.93602866   0.93395734   0.98488373
    1.1266377 ]][0m
[37m[1m[2023-07-11 20:58:41,394][233954] Max Reward on eval: -41.52320045813429[0m
[37m[1m[2023-07-11 20:58:41,394][233954] Min Reward on eval: -41.52320045813429[0m
[37m[1m[2023-07-11 20:58:41,395][233954] Mean Reward across all agents: -41.52320045813429[0m
[37m[1m[2023-07-11 20:58:41,395][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:58:46,347][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:58:46,348][233954] Reward + Measures: [[102.54365064   0.20869999   0.39920002   0.24529998   0.3671
    1.11681712]
 [ -8.8135993    0.32550001   0.8057       0.54680002   0.86830008
    0.81378955]
 [ -5.02683238   0.6225       0.87719995   0.66539997   0.91300005
    0.86427611]
 ...
 [  8.14425545   0.3572       0.78909999   0.28290001   0.64539999
    0.95679677]
 [ 85.35757589   0.1804       0.5122       0.26220003   0.54220003
    1.18022346]
 [-72.10838652   0.1962       0.92669994   0.87290001   0.96829998
    1.10150993]][0m
[37m[1m[2023-07-11 20:58:46,348][233954] Max Reward on eval: 204.88180925883353[0m
[37m[1m[2023-07-11 20:58:46,349][233954] Min Reward on eval: -403.6045475013554[0m
[37m[1m[2023-07-11 20:58:46,349][233954] Mean Reward across all agents: 4.955976435869273[0m
[37m[1m[2023-07-11 20:58:46,349][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:58:46,354][233954] mean_value=-245.3645396882037, max_value=648.5732424820796[0m
[37m[1m[2023-07-11 20:58:46,357][233954] New mean coefficients: [[ 0.7809303  -1.398613    3.1471288   1.5545417   0.09741035 -2.5499625 ]][0m
[37m[1m[2023-07-11 20:58:46,358][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:58:55,420][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 20:58:55,420][233954] FPS: 423804.70[0m
[36m[2023-07-11 20:58:55,423][233954] itr=1504, itrs=2000, Progress: 75.20%[0m
[36m[2023-07-11 20:59:07,023][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 20:59:07,023][233954] FPS: 334436.28[0m
[36m[2023-07-11 20:59:11,338][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:59:11,338][233954] Reward + Measures: [[-16.75158143   0.7978397    0.90795261   0.67879003   0.89176601
    0.62543565]][0m
[37m[1m[2023-07-11 20:59:11,338][233954] Max Reward on eval: -16.75158142872272[0m
[37m[1m[2023-07-11 20:59:11,339][233954] Min Reward on eval: -16.75158142872272[0m
[37m[1m[2023-07-11 20:59:11,339][233954] Mean Reward across all agents: -16.75158142872272[0m
[37m[1m[2023-07-11 20:59:11,339][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:59:16,389][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:59:16,390][233954] Reward + Measures: [[ 14.0958741    0.88190001   0.92980003   0.8427       0.93059999
    1.1715678 ]
 [  0.23322773   0.79170001   0.88230002   0.65549994   0.83319998
    0.63299739]
 [-39.43452228   0.77640003   0.87029999   0.6286       0.88900006
    0.8146826 ]
 ...
 [-13.73066493   0.6936       0.85659999   0.58579999   0.87010002
    0.97786421]
 [ 29.34286725   0.92690003   0.94779998   0.9084       0.95760006
    1.45059717]
 [-18.06750397   0.78050005   0.90960008   0.63099998   0.89960003
    0.70064849]][0m
[37m[1m[2023-07-11 20:59:16,390][233954] Max Reward on eval: 69.72294580056332[0m
[37m[1m[2023-07-11 20:59:16,390][233954] Min Reward on eval: -71.64450802411884[0m
[37m[1m[2023-07-11 20:59:16,390][233954] Mean Reward across all agents: -3.750034984256466[0m
[37m[1m[2023-07-11 20:59:16,390][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:59:16,393][233954] mean_value=-73.48695161878777, max_value=122.85025551355024[0m
[37m[1m[2023-07-11 20:59:16,395][233954] New mean coefficients: [[ 0.90177596 -1.7668567   3.4797766   0.953973    0.87772906 -2.2297983 ]][0m
[37m[1m[2023-07-11 20:59:16,396][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:59:25,445][233954] train() took 9.05 seconds to complete[0m
[36m[2023-07-11 20:59:25,446][233954] FPS: 424438.54[0m
[36m[2023-07-11 20:59:25,448][233954] itr=1505, itrs=2000, Progress: 75.25%[0m
[36m[2023-07-11 20:59:37,193][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 20:59:37,194][233954] FPS: 330357.11[0m
[36m[2023-07-11 20:59:41,526][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:59:41,527][233954] Reward + Measures: [[-16.72178561   0.85680532   0.94067633   0.76596564   0.91624868
    0.53699106]][0m
[37m[1m[2023-07-11 20:59:41,527][233954] Max Reward on eval: -16.721785613200808[0m
[37m[1m[2023-07-11 20:59:41,527][233954] Min Reward on eval: -16.721785613200808[0m
[37m[1m[2023-07-11 20:59:41,527][233954] Mean Reward across all agents: -16.721785613200808[0m
[37m[1m[2023-07-11 20:59:41,528][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:59:46,487][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 20:59:46,488][233954] Reward + Measures: [[  9.55125614   0.79500002   0.88810009   0.55409998   0.88290006
    1.06396568]
 [ -7.67530355   0.71530002   0.89720005   0.52810001   0.88050002
    0.5810129 ]
 [ -5.8411762    0.7026       0.884        0.54229999   0.88170004
    0.95061129]
 ...
 [-10.1513686    0.73780006   0.90620005   0.40489998   0.87860006
    1.04817128]
 [-23.54842832   0.84260005   0.84009999   0.66180003   0.82840008
    0.98007864]
 [  6.70374264   0.71600002   0.87580007   0.51639998   0.86310005
    0.94312233]][0m
[37m[1m[2023-07-11 20:59:46,488][233954] Max Reward on eval: 92.9061362718232[0m
[37m[1m[2023-07-11 20:59:46,488][233954] Min Reward on eval: -73.38004804570228[0m
[37m[1m[2023-07-11 20:59:46,489][233954] Mean Reward across all agents: 3.450725144902082[0m
[37m[1m[2023-07-11 20:59:46,489][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 20:59:46,492][233954] mean_value=-35.3928728852464, max_value=182.00772358670042[0m
[37m[1m[2023-07-11 20:59:46,495][233954] New mean coefficients: [[ 0.684368  -2.3945696  4.227543   1.1581126  1.5383945 -2.4963052]][0m
[37m[1m[2023-07-11 20:59:46,496][233954] Moving the mean solution point...[0m
[36m[2023-07-11 20:59:55,588][233954] train() took 9.09 seconds to complete[0m
[36m[2023-07-11 20:59:55,588][233954] FPS: 422415.35[0m
[36m[2023-07-11 20:59:55,590][233954] itr=1506, itrs=2000, Progress: 75.30%[0m
[36m[2023-07-11 21:00:07,501][233954] train() took 11.80 seconds to complete[0m
[36m[2023-07-11 21:00:07,501][233954] FPS: 325556.58[0m
[36m[2023-07-11 21:00:11,707][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:00:11,708][233954] Reward + Measures: [[-14.62417489   0.89978629   0.96343929   0.82327962   0.93945295
    0.49514905]][0m
[37m[1m[2023-07-11 21:00:11,708][233954] Max Reward on eval: -14.624174892993256[0m
[37m[1m[2023-07-11 21:00:11,708][233954] Min Reward on eval: -14.624174892993256[0m
[37m[1m[2023-07-11 21:00:11,709][233954] Mean Reward across all agents: -14.624174892993256[0m
[37m[1m[2023-07-11 21:00:11,709][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:00:16,654][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:00:16,655][233954] Reward + Measures: [[-12.65080228   0.89790004   0.96450007   0.81650013   0.91820002
    0.46247101]
 [ 10.36800461   0.63630003   0.80239993   0.52150005   0.8028
    0.66333324]
 [-15.25133848   0.86899996   0.95890009   0.74809998   0.94220001
    0.54714102]
 ...
 [-16.41310026   0.91140002   0.96969998   0.83459997   0.94390005
    0.48161608]
 [  7.35445      0.70980006   0.87410003   0.54910004   0.83890003
    0.56278515]
 [  4.175432     0.69889998   0.87120003   0.56690007   0.84639996
    0.58164889]][0m
[37m[1m[2023-07-11 21:00:16,655][233954] Max Reward on eval: 37.53104854025878[0m
[37m[1m[2023-07-11 21:00:16,655][233954] Min Reward on eval: -40.637794265942645[0m
[37m[1m[2023-07-11 21:00:16,655][233954] Mean Reward across all agents: -11.177168708791001[0m
[37m[1m[2023-07-11 21:00:16,656][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:00:16,658][233954] mean_value=-71.59400815080063, max_value=175.62282819042042[0m
[37m[1m[2023-07-11 21:00:16,660][233954] New mean coefficients: [[ 0.9561811  -2.8020918   4.088791   -0.02376795  1.6113628  -3.4795818 ]][0m
[37m[1m[2023-07-11 21:00:16,661][233954] Moving the mean solution point...[0m
[36m[2023-07-11 21:00:25,609][233954] train() took 8.95 seconds to complete[0m
[36m[2023-07-11 21:00:25,609][233954] FPS: 429234.27[0m
[36m[2023-07-11 21:00:25,611][233954] itr=1507, itrs=2000, Progress: 75.35%[0m
[36m[2023-07-11 21:00:37,348][233954] train() took 11.62 seconds to complete[0m
[36m[2023-07-11 21:00:37,348][233954] FPS: 330476.68[0m
[36m[2023-07-11 21:00:41,698][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:00:41,698][233954] Reward + Measures: [[15.01025633  0.35321534  0.84333235  0.60624331  0.90473837  0.84967095]][0m
[37m[1m[2023-07-11 21:00:41,698][233954] Max Reward on eval: 15.010256330426646[0m
[37m[1m[2023-07-11 21:00:41,698][233954] Min Reward on eval: 15.010256330426646[0m
[37m[1m[2023-07-11 21:00:41,699][233954] Mean Reward across all agents: 15.010256330426646[0m
[37m[1m[2023-07-11 21:00:41,699][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:00:46,714][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:00:46,714][233954] Reward + Measures: [[ -7.94159869   0.58719999   0.86149997   0.51880002   0.83789998
    0.7164886 ]
 [ 29.39650629   0.63120002   0.88569993   0.6534       0.88529998
    0.82810158]
 [ 69.13625308   0.38570002   0.69350004   0.34590003   0.64160007
    1.60696638]
 ...
 [ 44.80847049   0.24330001   0.71359998   0.35279998   0.75299996
    1.13662899]
 [116.45256354   0.32030001   0.64469999   0.24629998   0.57660002
    1.92107809]
 [ 48.04855419   0.39639997   0.70079994   0.45789996   0.70099998
    1.10192645]][0m
[37m[1m[2023-07-11 21:00:46,715][233954] Max Reward on eval: 137.79311322011054[0m
[37m[1m[2023-07-11 21:00:46,715][233954] Min Reward on eval: -63.419252035114916[0m
[37m[1m[2023-07-11 21:00:46,715][233954] Mean Reward across all agents: 29.8926840206105[0m
[37m[1m[2023-07-11 21:00:46,715][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:00:46,718][233954] mean_value=-87.88330708074164, max_value=323.91426031071865[0m
[37m[1m[2023-07-11 21:00:46,721][233954] New mean coefficients: [[ 0.18267566 -2.847139    4.3142586  -0.44246557  2.0178454  -3.4474711 ]][0m
[37m[1m[2023-07-11 21:00:46,722][233954] Moving the mean solution point...[0m
[36m[2023-07-11 21:00:55,787][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 21:00:55,788][233954] FPS: 423662.22[0m
[36m[2023-07-11 21:00:55,790][233954] itr=1508, itrs=2000, Progress: 75.40%[0m
[36m[2023-07-11 21:01:07,402][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 21:01:07,402][233954] FPS: 334051.83[0m
[36m[2023-07-11 21:01:11,600][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:01:11,600][233954] Reward + Measures: [[-12.01584419   0.11243901   0.78392035   0.40390432   0.84096628
    1.01843226]][0m
[37m[1m[2023-07-11 21:01:11,600][233954] Max Reward on eval: -12.015844188808245[0m
[37m[1m[2023-07-11 21:01:11,601][233954] Min Reward on eval: -12.015844188808245[0m
[37m[1m[2023-07-11 21:01:11,601][233954] Mean Reward across all agents: -12.015844188808245[0m
[37m[1m[2023-07-11 21:01:11,601][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:01:16,541][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:01:16,542][233954] Reward + Measures: [[ -8.76264833   0.0584       0.72970003   0.40529999   0.7392
    1.17339301]
 [142.82324805   0.1973       0.5478       0.34040001   0.45860001
    1.40092647]
 [-43.43069196   0.076        0.7398001    0.39289999   0.7719
    1.1056757 ]
 ...
 [125.28997609   0.1142       0.60289997   0.40760002   0.54510003
    1.23446786]
 [ 35.31034479   0.111        0.66090006   0.37380001   0.6663
    1.30136406]
 [ 39.18020297   0.09059999   0.66619998   0.43510005   0.62600005
    1.19269025]][0m
[37m[1m[2023-07-11 21:01:16,542][233954] Max Reward on eval: 247.884248176869[0m
[37m[1m[2023-07-11 21:01:16,542][233954] Min Reward on eval: -111.45187187334523[0m
[37m[1m[2023-07-11 21:01:16,542][233954] Mean Reward across all agents: 55.724746045274244[0m
[37m[1m[2023-07-11 21:01:16,543][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:01:16,545][233954] mean_value=-103.32975755865121, max_value=333.7057176292731[0m
[37m[1m[2023-07-11 21:01:16,548][233954] New mean coefficients: [[-0.09305912 -3.1768029   3.9849749  -0.23666056  2.7095327  -3.4913497 ]][0m
[37m[1m[2023-07-11 21:01:16,549][233954] Moving the mean solution point...[0m
[36m[2023-07-11 21:01:25,525][233954] train() took 8.97 seconds to complete[0m
[36m[2023-07-11 21:01:25,525][233954] FPS: 427879.22[0m
[36m[2023-07-11 21:01:25,527][233954] itr=1509, itrs=2000, Progress: 75.45%[0m
[36m[2023-07-11 21:01:37,198][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 21:01:37,199][233954] FPS: 332305.94[0m
[36m[2023-07-11 21:01:41,419][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:01:41,419][233954] Reward + Measures: [[-18.3036241    0.08913235   0.81824774   0.41883463   0.8743763
    1.00047231]][0m
[37m[1m[2023-07-11 21:01:41,419][233954] Max Reward on eval: -18.303624096862045[0m
[37m[1m[2023-07-11 21:01:41,420][233954] Min Reward on eval: -18.303624096862045[0m
[37m[1m[2023-07-11 21:01:41,420][233954] Mean Reward across all agents: -18.303624096862045[0m
[37m[1m[2023-07-11 21:01:41,420][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:01:46,706][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:01:46,706][233954] Reward + Measures: [[  2.6119321    0.081        0.81440002   0.42360002   0.84440005
    1.33551812]
 [  6.5378566    0.1295       0.84189999   0.41029999   0.83379996
    1.03335083]
 [-34.69431596   0.2879       0.80859995   0.2182       0.73890001
    1.09363461]
 ...
 [  4.99101659   0.12400001   0.80690002   0.3405       0.78900003
    1.03908348]
 [  1.11578023   0.2299       0.76999998   0.28710002   0.77950001
    0.95461589]
 [  6.81260797   0.28760001   0.69         0.19589999   0.60390002
    1.08746231]][0m
[37m[1m[2023-07-11 21:01:46,707][233954] Max Reward on eval: 184.26123381778598[0m
[37m[1m[2023-07-11 21:01:46,707][233954] Min Reward on eval: -99.0634093211498[0m
[37m[1m[2023-07-11 21:01:46,707][233954] Mean Reward across all agents: -2.949059587440764[0m
[37m[1m[2023-07-11 21:01:46,707][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:01:46,710][233954] mean_value=-91.96751282833475, max_value=614.3695991231361[0m
[37m[1m[2023-07-11 21:01:46,713][233954] New mean coefficients: [[-0.60094947 -3.0444434   3.3579006  -0.18115714  2.469534   -3.3937163 ]][0m
[37m[1m[2023-07-11 21:01:46,714][233954] Moving the mean solution point...[0m
[36m[2023-07-11 21:01:55,777][233954] train() took 9.06 seconds to complete[0m
[36m[2023-07-11 21:01:55,777][233954] FPS: 423767.32[0m
[36m[2023-07-11 21:01:55,779][233954] itr=1510, itrs=2000, Progress: 75.50%[0m
[37m[1m[2023-07-11 21:06:30,024][233954] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001490[0m
[36m[2023-07-11 21:06:44,721][233954] train() took 11.87 seconds to complete[0m
[36m[2023-07-11 21:06:44,722][233954] FPS: 323395.37[0m
[36m[2023-07-11 21:06:49,767][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:06:49,839][233954] Reward + Measures: [[-13.51150927   0.084359     0.85200894   0.41194099   0.89270431
    0.93110138]][0m
[37m[1m[2023-07-11 21:06:49,840][233954] Max Reward on eval: -13.511509268832567[0m
[37m[1m[2023-07-11 21:06:49,841][233954] Min Reward on eval: -13.511509268832567[0m
[37m[1m[2023-07-11 21:06:49,842][233954] Mean Reward across all agents: -13.511509268832567[0m
[37m[1m[2023-07-11 21:06:49,843][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:06:55,393][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:06:55,394][233954] Reward + Measures: [[-24.07556731   0.1286       0.69480002   0.32340002   0.67410004
    1.30842876]
 [-51.64329335   0.12260001   0.68170005   0.41799998   0.59569997
    1.59006071]
 [  5.42832089   0.1508       0.68310004   0.3008       0.6455
    1.46123528]
 ...
 [180.77348806   0.19960001   0.55910003   0.22910002   0.51739997
    1.91175044]
 [ 60.66933957   0.0393       0.68009996   0.58410001   0.8373
    1.54603434]
 [-62.06450939   0.0695       0.75470001   0.39930001   0.73159999
    1.2732352 ]][0m
[37m[1m[2023-07-11 21:06:55,394][233954] Max Reward on eval: 189.6512451333925[0m
[37m[1m[2023-07-11 21:06:55,394][233954] Min Reward on eval: -119.40081841535866[0m
[37m[1m[2023-07-11 21:06:55,395][233954] Mean Reward across all agents: 4.878064225048952[0m
[37m[1m[2023-07-11 21:06:55,395][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:06:55,414][233954] mean_value=-128.54405620640446, max_value=381.64587034813457[0m
[37m[1m[2023-07-11 21:06:55,434][233954] New mean coefficients: [[ 0.0382219  -2.9212594   1.6472051   0.54681265  2.2459826  -3.1305127 ]][0m
[37m[1m[2023-07-11 21:06:55,435][233954] Moving the mean solution point...[0m
[36m[2023-07-11 21:07:04,394][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 21:07:04,394][233954] FPS: 428726.65[0m
[36m[2023-07-11 21:07:04,396][233954] itr=1511, itrs=2000, Progress: 75.55%[0m
[36m[2023-07-11 21:07:16,370][233954] train() took 11.86 seconds to complete[0m
[36m[2023-07-11 21:07:16,370][233954] FPS: 323858.11[0m
[36m[2023-07-11 21:07:20,614][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:07:20,615][233954] Reward + Measures: [[-26.97320767   0.03781033   0.8902477    0.46226701   0.92380261
    0.878187  ]][0m
[37m[1m[2023-07-11 21:07:20,615][233954] Max Reward on eval: -26.97320766832006[0m
[37m[1m[2023-07-11 21:07:20,615][233954] Min Reward on eval: -26.97320766832006[0m
[37m[1m[2023-07-11 21:07:20,615][233954] Mean Reward across all agents: -26.97320766832006[0m
[37m[1m[2023-07-11 21:07:20,616][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:07:25,622][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:07:25,628][233954] Reward + Measures: [[-20.92018278   0.0648       0.81210005   0.46040002   0.88129997
    0.98847151]
 [ 20.14923439   0.07049999   0.81449997   0.38110003   0.82980007
    1.39158344]
 [281.87067413   0.002        0.88959998   0.78520006   0.98339999
    2.19466496]
 ...
 [ -6.1613507    0.10110001   0.66129994   0.39990005   0.75729996
    1.2981329 ]
 [614.15219117   0.0027       0.97339994   0.94449997   0.99060005
    2.65725589]
 [ 16.63269747   0.07129999   0.68440002   0.4172       0.81049997
    1.4781872 ]][0m
[37m[1m[2023-07-11 21:07:25,628][233954] Max Reward on eval: 680.1957282972289[0m
[37m[1m[2023-07-11 21:07:25,629][233954] Min Reward on eval: -79.42545226290822[0m
[37m[1m[2023-07-11 21:07:25,629][233954] Mean Reward across all agents: 127.9089512431318[0m
[37m[1m[2023-07-11 21:07:25,629][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:07:25,632][233954] mean_value=-161.0721955830663, max_value=577.2178464067634[0m
[37m[1m[2023-07-11 21:07:25,635][233954] New mean coefficients: [[-0.5161785 -2.5956502  0.9767999  0.3673749  2.674425  -3.0802934]][0m
[37m[1m[2023-07-11 21:07:25,636][233954] Moving the mean solution point...[0m
[36m[2023-07-11 21:07:34,630][233954] train() took 8.99 seconds to complete[0m
[36m[2023-07-11 21:07:34,631][233954] FPS: 427015.38[0m
[36m[2023-07-11 21:07:34,633][233954] itr=1512, itrs=2000, Progress: 75.60%[0m
[36m[2023-07-11 21:07:46,522][233954] train() took 11.77 seconds to complete[0m
[36m[2023-07-11 21:07:46,522][233954] FPS: 326301.97[0m
[36m[2023-07-11 21:07:50,821][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:07:50,822][233954] Reward + Measures: [[-50.5759855    0.013615     0.93440902   0.54470199   0.94871265
    0.80128378]][0m
[37m[1m[2023-07-11 21:07:50,822][233954] Max Reward on eval: -50.57598550350058[0m
[37m[1m[2023-07-11 21:07:50,822][233954] Min Reward on eval: -50.57598550350058[0m
[37m[1m[2023-07-11 21:07:50,822][233954] Mean Reward across all agents: -50.57598550350058[0m
[37m[1m[2023-07-11 21:07:50,823][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:07:55,737][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:07:55,738][233954] Reward + Measures: [[-29.63571339   0.0258       0.8434       0.58289999   0.89850008
    1.22936165]
 [-33.63830488   0.0191       0.82730007   0.59600002   0.88000005
    1.00058126]
 [  8.7817993    0.016        0.77539998   0.61970001   0.83579999
    1.1455456 ]
 ...
 [ 25.93773544   0.24080001   0.5887       0.5855       0.63180006
    1.05760038]
 [  1.51849332   0.0158       0.78369999   0.59740001   0.81949997
    1.09006941]
 [-26.69721974   0.12100001   0.88910002   0.42560002   0.9429
    1.23082304]][0m
[37m[1m[2023-07-11 21:07:55,738][233954] Max Reward on eval: 86.10575129017234[0m
[37m[1m[2023-07-11 21:07:55,739][233954] Min Reward on eval: -104.36518080644309[0m
[37m[1m[2023-07-11 21:07:55,739][233954] Mean Reward across all agents: -15.196268257350118[0m
[37m[1m[2023-07-11 21:07:55,739][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:07:55,743][233954] mean_value=14.984552273553048, max_value=551.3856907110661[0m
[37m[1m[2023-07-11 21:07:55,746][233954] New mean coefficients: [[-1.090848   -1.9266241   1.6671789   0.68024945  3.4429662  -3.3995    ]][0m
[37m[1m[2023-07-11 21:07:55,747][233954] Moving the mean solution point...[0m
[36m[2023-07-11 21:08:04,768][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 21:08:04,768][233954] FPS: 425746.76[0m
[36m[2023-07-11 21:08:04,770][233954] itr=1513, itrs=2000, Progress: 75.65%[0m
[36m[2023-07-11 21:08:16,459][233954] train() took 11.57 seconds to complete[0m
[36m[2023-07-11 21:08:16,459][233954] FPS: 331985.99[0m
[36m[2023-07-11 21:08:20,730][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:08:20,730][233954] Reward + Measures: [[-74.69182317   0.00926667   0.95336765   0.61991835   0.9568693
    0.74815226]][0m
[37m[1m[2023-07-11 21:08:20,731][233954] Max Reward on eval: -74.69182316844375[0m
[37m[1m[2023-07-11 21:08:20,731][233954] Min Reward on eval: -74.69182316844375[0m
[37m[1m[2023-07-11 21:08:20,731][233954] Mean Reward across all agents: -74.69182316844375[0m
[37m[1m[2023-07-11 21:08:20,731][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:08:25,707][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:08:25,708][233954] Reward + Measures: [[163.30328989   0.0129       0.97090006   0.68979996   0.96100008
    1.63751411]
 [  6.4736318    0.0455       0.94659996   0.54510003   0.78850001
    1.20769274]
 [ 33.50269725   0.0426       0.94890004   0.55330002   0.80310005
    1.4424628 ]
 ...
 [-36.19882818   0.0264       0.92580003   0.51190001   0.9601
    0.9205882 ]
 [ 19.389662     0.03049999   0.90109998   0.61090004   0.91250002
    1.2059778 ]
 [ 21.07064861   0.0984       0.82609999   0.54870003   0.82190001
    1.26335144]][0m
[37m[1m[2023-07-11 21:08:25,708][233954] Max Reward on eval: 169.74290561713278[0m
[37m[1m[2023-07-11 21:08:25,708][233954] Min Reward on eval: -59.93838238157332[0m
[37m[1m[2023-07-11 21:08:25,708][233954] Mean Reward across all agents: 20.017114898899877[0m
[37m[1m[2023-07-11 21:08:25,709][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:08:25,714][233954] mean_value=-1.224260800147804, max_value=633.7283038999885[0m
[37m[1m[2023-07-11 21:08:25,716][233954] New mean coefficients: [[-1.6995087  -2.2739577   2.172811    0.40754327  3.5753436  -3.6680064 ]][0m
[37m[1m[2023-07-11 21:08:25,717][233954] Moving the mean solution point...[0m
[36m[2023-07-11 21:08:34,681][233954] train() took 8.96 seconds to complete[0m
[36m[2023-07-11 21:08:34,681][233954] FPS: 428488.10[0m
[36m[2023-07-11 21:08:34,683][233954] itr=1514, itrs=2000, Progress: 75.70%[0m
[36m[2023-07-11 21:08:46,598][233954] train() took 11.80 seconds to complete[0m
[36m[2023-07-11 21:08:46,599][233954] FPS: 325481.10[0m
[36m[2023-07-11 21:08:50,844][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:08:50,844][233954] Reward + Measures: [[-63.34527091   0.008271     0.96265334   0.64060336   0.9627077
    0.64934742]][0m
[37m[1m[2023-07-11 21:08:50,844][233954] Max Reward on eval: -63.34527091083016[0m
[37m[1m[2023-07-11 21:08:50,845][233954] Min Reward on eval: -63.34527091083016[0m
[37m[1m[2023-07-11 21:08:50,845][233954] Mean Reward across all agents: -63.34527091083016[0m
[37m[1m[2023-07-11 21:08:50,845][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:08:55,799][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:08:55,800][233954] Reward + Measures: [[-96.17978857   0.0112       0.9066       0.55929995   0.94859999
    0.8012749 ]
 [-12.66643184   0.1134       0.63989997   0.33160001   0.84960002
    1.35805166]
 [-39.03197446   0.06460001   0.72420007   0.38909999   0.87260002
    1.10812628]
 ...
 [-38.67812476   0.0971       0.63740009   0.38420004   0.84149998
    1.24628055]
 [  0.96608913   0.1454       0.59400004   0.331        0.79700005
    1.38014209]
 [ 52.11223638   0.0103       0.81380004   0.5654       0.93599999
    1.56345391]][0m
[37m[1m[2023-07-11 21:08:55,800][233954] Max Reward on eval: 113.86517932298594[0m
[37m[1m[2023-07-11 21:08:55,800][233954] Min Reward on eval: -113.48125881636516[0m
[37m[1m[2023-07-11 21:08:55,800][233954] Mean Reward across all agents: -30.170954803213462[0m
[37m[1m[2023-07-11 21:08:55,801][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:08:55,805][233954] mean_value=42.46087934107723, max_value=415.990243545128[0m
[37m[1m[2023-07-11 21:08:55,808][233954] New mean coefficients: [[-2.336491   -1.2797439   3.9614987   0.34270316  3.9829354  -4.262918  ]][0m
[37m[1m[2023-07-11 21:08:55,809][233954] Moving the mean solution point...[0m
[36m[2023-07-11 21:09:04,738][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 21:09:04,744][233954] FPS: 430128.12[0m
[36m[2023-07-11 21:09:04,746][233954] itr=1515, itrs=2000, Progress: 75.75%[0m
[36m[2023-07-11 21:09:16,363][233954] train() took 11.50 seconds to complete[0m
[36m[2023-07-11 21:09:16,364][233954] FPS: 333959.78[0m
[36m[2023-07-11 21:09:20,597][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:09:20,597][233954] Reward + Measures: [[-66.50755119   0.008218     0.96556967   0.67447901   0.96465361
    0.6109671 ]][0m
[37m[1m[2023-07-11 21:09:20,597][233954] Max Reward on eval: -66.50755118609318[0m
[37m[1m[2023-07-11 21:09:20,598][233954] Min Reward on eval: -66.50755118609318[0m
[37m[1m[2023-07-11 21:09:20,598][233954] Mean Reward across all agents: -66.50755118609318[0m
[37m[1m[2023-07-11 21:09:20,598][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:09:25,551][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:09:25,551][233954] Reward + Measures: [[ 92.50671818   0.64449996   0.91949999   0.19580002   0.91460001
    0.98329431]
 [ -2.89021468   0.0285       0.75980008   0.62070006   0.81660002
    1.04096019]
 [ 12.91106496   0.0026       0.86190003   0.77019995   0.92670006
    1.09100521]
 ...
 [-37.26934028   0.85729998   0.85830003   0.84359998   0.85299999
    1.68441391]
 [ 35.44713345   0.1373       0.64530003   0.39410004   0.69080001
    1.24064958]
 [ 49.94750761   0.20350002   0.79080003   0.33080003   0.78369999
    0.89247721]][0m
[37m[1m[2023-07-11 21:09:25,552][233954] Max Reward on eval: 402.2586555734277[0m
[37m[1m[2023-07-11 21:09:25,552][233954] Min Reward on eval: -129.1873483632691[0m
[37m[1m[2023-07-11 21:09:25,552][233954] Mean Reward across all agents: 26.893762903219272[0m
[37m[1m[2023-07-11 21:09:25,552][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:09:25,558][233954] mean_value=-12.734036687098591, max_value=345.51136307693434[0m
[37m[1m[2023-07-11 21:09:25,560][233954] New mean coefficients: [[-2.6118102  -0.7342821   4.356262    0.17524591  4.3782516  -4.264984  ]][0m
[37m[1m[2023-07-11 21:09:25,561][233954] Moving the mean solution point...[0m
[36m[2023-07-11 21:09:34,498][233954] train() took 8.93 seconds to complete[0m
[36m[2023-07-11 21:09:34,498][233954] FPS: 429782.10[0m
[36m[2023-07-11 21:09:34,500][233954] itr=1516, itrs=2000, Progress: 75.80%[0m
[36m[2023-07-11 21:09:46,090][233954] train() took 11.47 seconds to complete[0m
[36m[2023-07-11 21:09:46,090][233954] FPS: 334738.95[0m
[36m[2023-07-11 21:09:50,373][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:09:50,374][233954] Reward + Measures: [[-64.1105375    0.00781933   0.96999395   0.69792432   0.96820939
    0.58308673]][0m
[37m[1m[2023-07-11 21:09:50,374][233954] Max Reward on eval: -64.11053750085512[0m
[37m[1m[2023-07-11 21:09:50,374][233954] Min Reward on eval: -64.11053750085512[0m
[37m[1m[2023-07-11 21:09:50,374][233954] Mean Reward across all agents: -64.11053750085512[0m
[37m[1m[2023-07-11 21:09:50,375][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:09:55,694][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:09:55,695][233954] Reward + Measures: [[120.04672623   0.005        0.9698       0.9309001    0.98780006
    2.25151992]
 [203.93783855   0.0052       0.96040004   0.86910003   0.99500006
    1.97275102]
 [ 48.70194332   0.36310002   0.74180001   0.21869998   0.69500005
    1.51605797]
 ...
 [201.73552894   0.0024       0.97180003   0.91119999   0.98839998
    2.06162429]
 [119.11007404   0.0033       0.96029997   0.88520002   0.99329996
    2.07520771]
 [179.79288101   0.0039       0.96140003   0.8853001    0.99360001
    1.94747579]][0m
[37m[1m[2023-07-11 21:09:55,695][233954] Max Reward on eval: 260.09251403063536[0m
[37m[1m[2023-07-11 21:09:55,696][233954] Min Reward on eval: -98.96658323588781[0m
[37m[1m[2023-07-11 21:09:55,696][233954] Mean Reward across all agents: 112.79780795749822[0m
[37m[1m[2023-07-11 21:09:55,696][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:09:55,698][233954] mean_value=-242.00511626241277, max_value=104.4845703177925[0m
[37m[1m[2023-07-11 21:09:55,701][233954] New mean coefficients: [[-1.6493816  -0.16929775  5.3515677   0.35186738  4.212431   -4.057131  ]][0m
[37m[1m[2023-07-11 21:09:55,702][233954] Moving the mean solution point...[0m
[36m[2023-07-11 21:10:04,725][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 21:10:04,725][233954] FPS: 425643.49[0m
[36m[2023-07-11 21:10:04,728][233954] itr=1517, itrs=2000, Progress: 75.85%[0m
[36m[2023-07-11 21:10:16,331][233954] train() took 11.48 seconds to complete[0m
[36m[2023-07-11 21:10:16,336][233954] FPS: 334354.16[0m
[36m[2023-07-11 21:10:20,555][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:10:20,556][233954] Reward + Measures: [[-61.08744089   0.007435     0.97338933   0.7209959    0.97188294
    0.54240149]][0m
[37m[1m[2023-07-11 21:10:20,556][233954] Max Reward on eval: -61.08744088615091[0m
[37m[1m[2023-07-11 21:10:20,556][233954] Min Reward on eval: -61.08744088615091[0m
[37m[1m[2023-07-11 21:10:20,557][233954] Mean Reward across all agents: -61.08744088615091[0m
[37m[1m[2023-07-11 21:10:20,557][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:10:25,594][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:10:25,595][233954] Reward + Measures: [[-51.31438705   0.0167       0.88780004   0.61479998   0.88689995
    0.90496063]
 [-70.81287169   0.0331       0.81650001   0.62650007   0.7608
    0.55769372]
 [-53.58042355   0.2102       0.75229996   0.68100005   0.74039996
    0.88680947]
 ...
 [ 27.12927429   0.47189999   0.8653       0.62639999   0.91029996
    1.09142673]
 [ 21.84321408   0.08419999   0.81199998   0.52329999   0.83709997
    0.8832823 ]
 [  8.07689485   0.38500002   0.66470003   0.64050001   0.67169994
    0.78865737]][0m
[37m[1m[2023-07-11 21:10:25,595][233954] Max Reward on eval: 128.4071006867569[0m
[37m[1m[2023-07-11 21:10:25,596][233954] Min Reward on eval: -109.0112586054951[0m
[37m[1m[2023-07-11 21:10:25,596][233954] Mean Reward across all agents: 3.857659963202891[0m
[37m[1m[2023-07-11 21:10:25,596][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:10:25,602][233954] mean_value=-7.252266767670444, max_value=460.2437555728294[0m
[37m[1m[2023-07-11 21:10:25,605][233954] New mean coefficients: [[-1.9163688   0.29346514  5.2894483   0.6386813   4.3242025  -3.8489413 ]][0m
[37m[1m[2023-07-11 21:10:25,606][233954] Moving the mean solution point...[0m
[36m[2023-07-11 21:10:34,630][233954] train() took 9.02 seconds to complete[0m
[36m[2023-07-11 21:10:34,630][233954] FPS: 425609.16[0m
[36m[2023-07-11 21:10:34,632][233954] itr=1518, itrs=2000, Progress: 75.90%[0m
[36m[2023-07-11 21:10:46,292][233954] train() took 11.54 seconds to complete[0m
[36m[2023-07-11 21:10:46,292][233954] FPS: 332848.18[0m
[36m[2023-07-11 21:10:50,575][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:10:50,576][233954] Reward + Measures: [[-51.3424212    0.006394     0.97802269   0.74696994   0.97388965
    0.51049584]][0m
[37m[1m[2023-07-11 21:10:50,576][233954] Max Reward on eval: -51.34242119920204[0m
[37m[1m[2023-07-11 21:10:50,576][233954] Min Reward on eval: -51.34242119920204[0m
[37m[1m[2023-07-11 21:10:50,576][233954] Mean Reward across all agents: -51.34242119920204[0m
[37m[1m[2023-07-11 21:10:50,577][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:10:55,589][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:10:55,590][233954] Reward + Measures: [[128.32818318   0.002        0.95419997   0.95520002   0.9891001
    2.04938388]
 [132.95023299   0.0019       0.93379992   0.94999999   0.99169999
    1.68351865]
 [101.22319888   0.002        0.88360006   0.90430003   0.97920001
    1.71429253]
 ...
 [108.57343579   0.0022       0.89160007   0.90390009   0.97699994
    1.74824941]
 [113.56371642   0.0027       0.93110001   0.94360012   0.98519993
    1.99646497]
 [118.26221373   0.002        0.92210001   0.94049996   0.99259996
    1.5171119 ]][0m
[37m[1m[2023-07-11 21:10:55,590][233954] Max Reward on eval: 223.3938922975911[0m
[37m[1m[2023-07-11 21:10:55,590][233954] Min Reward on eval: -98.82244588145986[0m
[37m[1m[2023-07-11 21:10:55,590][233954] Mean Reward across all agents: 89.92237281832973[0m
[37m[1m[2023-07-11 21:10:55,590][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:10:55,594][233954] mean_value=-73.2347394896386, max_value=362.6156843304837[0m
[37m[1m[2023-07-11 21:10:55,596][233954] New mean coefficients: [[-1.0937929  -0.09280357  5.7738757   0.52186203  3.893515   -3.4555902 ]][0m
[37m[1m[2023-07-11 21:10:55,597][233954] Moving the mean solution point...[0m
[36m[2023-07-11 21:11:04,697][233954] train() took 9.10 seconds to complete[0m
[36m[2023-07-11 21:11:04,697][233954] FPS: 422054.02[0m
[36m[2023-07-11 21:11:04,700][233954] itr=1519, itrs=2000, Progress: 75.95%[0m
[36m[2023-07-11 21:11:16,385][233954] train() took 11.56 seconds to complete[0m
[36m[2023-07-11 21:11:16,385][233954] FPS: 332120.04[0m
[36m[2023-07-11 21:11:20,684][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:11:20,685][233954] Reward + Measures: [[-40.65911255   0.00652333   0.97985399   0.78543293   0.97599268
    0.50216722]][0m
[37m[1m[2023-07-11 21:11:20,685][233954] Max Reward on eval: -40.65911254730504[0m
[37m[1m[2023-07-11 21:11:20,685][233954] Min Reward on eval: -40.65911254730504[0m
[37m[1m[2023-07-11 21:11:20,685][233954] Mean Reward across all agents: -40.65911254730504[0m
[37m[1m[2023-07-11 21:11:20,686][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:11:25,704][233954] Finished Evaluation Step[0m
[37m[1m[2023-07-11 21:11:25,704][233954] Reward + Measures: [[ -1.81654358   0.19840001   0.72579998   0.68129998   0.73800004
    0.87815279]
 [ 17.66739082   0.206        0.80740005   0.69160002   0.85280001
    1.03777039]
 [-29.7662649    0.0134       0.94150001   0.68980002   0.95749998
    0.76241332]
 ...
 [ 22.54492715   0.23650001   0.72760004   0.66510004   0.79120004
    0.93669242]
 [-31.4364281    0.14430001   0.82569999   0.61540002   0.85519999
    0.7935999 ]
 [-34.43100664   0.0073       0.97250003   0.73960006   0.96890002
    0.57764369]][0m
[37m[1m[2023-07-11 21:11:25,705][233954] Max Reward on eval: 224.09491971954702[0m
[37m[1m[2023-07-11 21:11:25,705][233954] Min Reward on eval: -58.24832108201226[0m
[37m[1m[2023-07-11 21:11:25,705][233954] Mean Reward across all agents: 25.25800681949314[0m
[37m[1m[2023-07-11 21:11:25,705][233954] Average Trajectory Length: 1000.0[0m
[36m[2023-07-11 21:11:25,711][233954] mean_value=6.49111752071755, max_value=450.5315891088209[0m
[37m[1m[2023-07-11 21:11:25,713][233954] New mean coefficients: [[-1.619378    0.12181026  5.636663    0.7545599   4.1647186  -3.128737  ]][0m
[37m[1m[2023-07-11 21:11:25,714][233954] Moving the mean solution point...[0m
[36m[2023-07-11 21:11:34,711][233954] train() took 9.00 seconds to complete[0m
[36m[2023-07-11 21:11:34,711][233954] FPS: 426901.26[0m
[36m[2023-07-11 21:11:34,714][233954] itr=1520, itrs=2000, Progress: 76.00%[0m
./runners/local/train_ppga_ant.sh: line 42: 233954 Killed                  python -u -m algorithm.train_ppga --env_name=$ENV_NAME --rollout_length=128 --use_wandb=True --seed=$SEED --wandb_group=qdrl --num_dims=5 --num_minibatches=8 --update_epochs=4 --normalize_obs=True --normalize_returns=True --wandb_run_name=$RUN_NAME --popsize=300 --env_batch_size=3000 --learning_rate=0.001 --vf_coef=2 --max_grad_norm=1 --torch_deterministic=False --total_iterations=2000 --dqd_algorithm=cma_maega --calc_gradient_iters=10 --move_mean_iters=10 --archive_lr=0.1 --restart_rule=no_improvement --sigma0=3.0 --threshold_min=-500 --grid_size=$GRID_SIZE --take_archive_snapshots=True --use_cvt_archive=True --cvt_cells=10000 --cvt_samples=100000 --cvt_use_kd_tree=True --is_energy_measures=True --expdir=./experiments/paper_ppga_"$ENV_NAME"
