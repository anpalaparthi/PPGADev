bounds:
[(0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.0, 4.0)]
using cvt archive
no kmeans
[36m[2023-07-16 20:12:31,483][256877] Environment ant, action_dim=8, obs_dim=87
[36m[2023-07-16 20:12:35,057][256877] Created Scheduler for cma_maega with an archive learning rate of 0.1, and add mode batch, using solution dim 28816 and archive dims [10, 10, 10, 10, 10]. Min threshold is -500.0. Restart rule is no_improvement
[36m[2023-07-16 20:12:53,099][256877] train() took 13.97 seconds to complete
[36m[2023-07-16 20:12:53,099][256877] FPS: 274830.36
[36m[2023-07-16 20:12:57,263][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:12:57,263][256877] Reward + Measures: [[-1.94723382  0.18991865  0.19024701  0.18939799  0.18927269  2.06405091]]
[37m[1m[2023-07-16 20:12:57,264][256877] Max Reward on eval: -1.947233822206724
[37m[1m[2023-07-16 20:12:57,264][256877] Min Reward on eval: -1.947233822206724
[37m[1m[2023-07-16 20:12:57,264][256877] Mean Reward across all agents: -1.947233822206724
[37m[1m[2023-07-16 20:12:57,264][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:13:03,006][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:13:03,007][256877] Reward + Measures: [[ 1.53353002  0.2192      0.20179999  0.19100001  0.204       2.24912   ]
[37m[1m [62.9534572   0.1681      0.17990001  0.17380001  0.1895      2.18909192]
[37m[1m [ 6.76713161  0.1478      0.14049999  0.15460001  0.1277      2.16653228]
[37m[1m ...
[37m[1m [21.3839151   0.17460001  0.1742      0.1635      0.16760001  2.11309409]
[37m[1m [ 6.24140085  0.17690001  0.19700001  0.16500001  0.18120001  2.15638614]
[37m[1m [91.59819071  0.16970001  0.16320001  0.1717      0.15870002  2.19301724]]
[37m[1m[2023-07-16 20:13:03,007][256877] Max Reward on eval: 146.0861939182505
[37m[1m[2023-07-16 20:13:03,007][256877] Min Reward on eval: -73.51128799654543
[37m[1m[2023-07-16 20:13:03,008][256877] Mean Reward across all agents: 8.947176582055333
[37m[1m[2023-07-16 20:13:03,008][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:13:03,029][256877] mean_value=494.50364636289936, max_value=646.0861939182505
[37m[1m[2023-07-16 20:13:03,062][256877] New mean coefficients: [[ 2.5845835  -1.2481898  -0.50177246 -2.0785587  -1.3737314  -0.27382666]]
[37m[1m[2023-07-16 20:13:03,063][256877] Moving the mean solution point...
[36m[2023-07-16 20:13:11,971][256877] train() took 8.91 seconds to complete
[36m[2023-07-16 20:13:11,971][256877] FPS: 431154.10
[36m[2023-07-16 20:13:11,973][256877] itr=0, itrs=2000, Progress: 0.00%
[36m[2023-07-16 20:13:23,641][256877] train() took 11.65 seconds to complete
[36m[2023-07-16 20:13:23,641][256877] FPS: 329524.57
[36m[2023-07-16 20:13:27,830][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:13:27,831][256877] Reward + Measures: [[146.28257388   0.16612001   0.18326598   0.16545632   0.17398466
[37m[1m    1.99541855]]
[37m[1m[2023-07-16 20:13:27,831][256877] Max Reward on eval: 146.28257387668256
[37m[1m[2023-07-16 20:13:27,831][256877] Min Reward on eval: 146.28257387668256
[37m[1m[2023-07-16 20:13:27,831][256877] Mean Reward across all agents: 146.28257387668256
[37m[1m[2023-07-16 20:13:27,832][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:13:32,834][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:13:32,834][256877] Reward + Measures: [[ 20.09628007   0.10880001   0.123        0.0966       0.1142
[37m[1m    2.16154528]
[37m[1m [160.89413167   0.10640001   0.14869998   0.1176       0.15089999
[37m[1m    2.3690815 ]
[37m[1m [ 87.15730519   0.1362       0.1181       0.12850001   0.1268
[37m[1m    2.35097671]
[37m[1m ...
[37m[1m [159.9160606    0.1164       0.1331       0.1244       0.123
[37m[1m    2.18830347]
[37m[1m [ 68.83460481   0.18610001   0.1743       0.12460001   0.16790001
[37m[1m    2.30614996]
[37m[1m [157.77246334   0.1046       0.11960001   0.1045       0.1336
[37m[1m    2.2465601 ]]
[37m[1m[2023-07-16 20:13:32,835][256877] Max Reward on eval: 293.59309912249444
[37m[1m[2023-07-16 20:13:32,835][256877] Min Reward on eval: -37.31656750421971
[37m[1m[2023-07-16 20:13:32,835][256877] Mean Reward across all agents: 104.66019119751978
[37m[1m[2023-07-16 20:13:32,835][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:13:32,844][256877] mean_value=90.57964133220612, max_value=428.05480259231706
[37m[1m[2023-07-16 20:13:32,855][256877] New mean coefficients: [[ 4.8602457  -1.5151217   0.48511595 -1.1903756  -0.5406405   0.28746742]]
[37m[1m[2023-07-16 20:13:32,856][256877] Moving the mean solution point...
[36m[2023-07-16 20:13:41,860][256877] train() took 9.00 seconds to complete
[36m[2023-07-16 20:13:41,860][256877] FPS: 426532.60
[36m[2023-07-16 20:13:41,863][256877] itr=1, itrs=2000, Progress: 0.05%
[36m[2023-07-16 20:13:53,519][256877] train() took 11.63 seconds to complete
[36m[2023-07-16 20:13:53,519][256877] FPS: 330165.02
[36m[2023-07-16 20:13:57,737][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:13:57,737][256877] Reward + Measures: [[257.36620808   0.14015132   0.16121501   0.14198966   0.14952399
[37m[1m    1.98683596]]
[37m[1m[2023-07-16 20:13:57,737][256877] Max Reward on eval: 257.3662080839234
[37m[1m[2023-07-16 20:13:57,738][256877] Min Reward on eval: 257.3662080839234
[37m[1m[2023-07-16 20:13:57,738][256877] Mean Reward across all agents: 257.3662080839234
[37m[1m[2023-07-16 20:13:57,738][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:14:02,843][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:14:02,843][256877] Reward + Measures: [[241.37362244   0.14829999   0.1635       0.126        0.14930001
[37m[1m    2.03162074]
[37m[1m [112.03802038   0.1089       0.1464       0.1305       0.1168
[37m[1m    2.37120676]
[37m[1m [242.3077401    0.15320002   0.17790002   0.14560001   0.14890002
[37m[1m    2.26657152]
[37m[1m ...
[37m[1m [268.44123884   0.17630002   0.15570001   0.15470001   0.1408
[37m[1m    2.11614323]
[37m[1m [ 76.33999858   0.08670001   0.1111       0.1141       0.1105
[37m[1m    2.29610205]
[37m[1m [244.03660774   0.1525       0.18359999   0.13399999   0.17279999
[37m[1m    2.08395362]]
[37m[1m[2023-07-16 20:14:02,844][256877] Max Reward on eval: 425.4348931569606
[37m[1m[2023-07-16 20:14:02,844][256877] Min Reward on eval: 50.08907634210773
[37m[1m[2023-07-16 20:14:02,844][256877] Mean Reward across all agents: 193.16403302434338
[37m[1m[2023-07-16 20:14:02,844][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:14:02,850][256877] mean_value=72.52545155594356, max_value=306.55163924687355
[37m[1m[2023-07-16 20:14:02,853][256877] New mean coefficients: [[ 4.2157755  -0.68102455  0.39435023 -1.1055682  -0.5524513   1.183048  ]]
[37m[1m[2023-07-16 20:14:02,854][256877] Moving the mean solution point...
[36m[2023-07-16 20:14:11,780][256877] train() took 8.92 seconds to complete
[36m[2023-07-16 20:14:11,780][256877] FPS: 430280.56
[36m[2023-07-16 20:14:11,783][256877] itr=2, itrs=2000, Progress: 0.10%
[36m[2023-07-16 20:14:23,414][256877] train() took 11.61 seconds to complete
[36m[2023-07-16 20:14:23,415][256877] FPS: 330779.63
[36m[2023-07-16 20:14:27,654][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:14:27,654][256877] Reward + Measures: [[331.66518701   0.12562034   0.14409767   0.128115     0.13278833
[37m[1m    2.03087378]]
[37m[1m[2023-07-16 20:14:27,655][256877] Max Reward on eval: 331.6651870140205
[37m[1m[2023-07-16 20:14:27,655][256877] Min Reward on eval: 331.6651870140205
[37m[1m[2023-07-16 20:14:27,655][256877] Mean Reward across all agents: 331.6651870140205
[37m[1m[2023-07-16 20:14:27,655][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:14:32,653][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:14:32,653][256877] Reward + Measures: [[225.61603734   0.11700001   0.1292       0.1411       0.1184
[37m[1m    2.16040015]
[37m[1m [344.7394695    0.13030002   0.17099999   0.12119999   0.1506
[37m[1m    2.25706363]
[37m[1m [203.65251706   0.11809999   0.13500001   0.10500001   0.1027
[37m[1m    2.28982282]
[37m[1m ...
[37m[1m [167.82601938   0.13170001   0.1488       0.1188       0.1327
[37m[1m    2.12996531]
[37m[1m [287.43701601   0.10820001   0.13420001   0.1211       0.12280001
[37m[1m    2.15902257]
[37m[1m [283.11222242   0.13000001   0.17040001   0.1374       0.16400002
[37m[1m    2.15289927]]
[37m[1m[2023-07-16 20:14:32,653][256877] Max Reward on eval: 510.2788722541183
[37m[1m[2023-07-16 20:14:32,654][256877] Min Reward on eval: 73.11491028554738
[37m[1m[2023-07-16 20:14:32,654][256877] Mean Reward across all agents: 256.1403369826705
[37m[1m[2023-07-16 20:14:32,654][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:14:32,659][256877] mean_value=39.66177566220137, max_value=286.6103144946678
[37m[1m[2023-07-16 20:14:32,662][256877] New mean coefficients: [[ 4.309668   -0.31841034  1.1911088  -0.9459603   0.96726817  1.2336694 ]]
[37m[1m[2023-07-16 20:14:32,663][256877] Moving the mean solution point...
[36m[2023-07-16 20:14:41,689][256877] train() took 9.02 seconds to complete
[36m[2023-07-16 20:14:41,689][256877] FPS: 425503.87
[36m[2023-07-16 20:14:41,691][256877] itr=3, itrs=2000, Progress: 0.15%
[36m[2023-07-16 20:14:53,403][256877] train() took 11.69 seconds to complete
[36m[2023-07-16 20:14:53,404][256877] FPS: 328591.69
[36m[2023-07-16 20:14:57,731][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:14:57,732][256877] Reward + Measures: [[463.99361123   0.12836033   0.14871733   0.13130367   0.13466866
[37m[1m    2.0874579 ]]
[37m[1m[2023-07-16 20:14:57,732][256877] Max Reward on eval: 463.99361122913797
[37m[1m[2023-07-16 20:14:57,732][256877] Min Reward on eval: 463.99361122913797
[37m[1m[2023-07-16 20:14:57,733][256877] Mean Reward across all agents: 463.99361122913797
[37m[1m[2023-07-16 20:14:57,733][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:15:02,723][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:15:02,723][256877] Reward + Measures: [[400.59003475   0.1348       0.142        0.13590001   0.13180001
[37m[1m    2.11810374]
[37m[1m [531.07683868   0.15750001   0.17910001   0.16400002   0.1689
[37m[1m    2.24478889]
[37m[1m [679.53528117   0.17910001   0.19720002   0.18970001   0.16500001
[37m[1m    2.31562686]
[37m[1m ...
[37m[1m [373.75231073   0.11360001   0.1364       0.1224       0.1276
[37m[1m    2.09339023]
[37m[1m [629.88231085   0.14940001   0.16180001   0.1705       0.1495
[37m[1m    2.25602698]
[37m[1m [426.05334662   0.1512       0.19679999   0.14400001   0.1806
[37m[1m    2.19641542]]
[37m[1m[2023-07-16 20:15:02,723][256877] Max Reward on eval: 679.5352811710443
[37m[1m[2023-07-16 20:15:02,724][256877] Min Reward on eval: 132.35383140407504
[37m[1m[2023-07-16 20:15:02,724][256877] Mean Reward across all agents: 388.56627324734836
[37m[1m[2023-07-16 20:15:02,724][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:15:02,729][256877] mean_value=87.90641098968675, max_value=397.4877160059461
[37m[1m[2023-07-16 20:15:02,732][256877] New mean coefficients: [[ 4.5647345   0.32890832  0.9051759  -0.36680496  0.14771569  1.1028833 ]]
[37m[1m[2023-07-16 20:15:02,733][256877] Moving the mean solution point...
[36m[2023-07-16 20:15:11,673][256877] train() took 8.94 seconds to complete
[36m[2023-07-16 20:15:11,673][256877] FPS: 429606.79
[36m[2023-07-16 20:15:11,675][256877] itr=4, itrs=2000, Progress: 0.20%
[36m[2023-07-16 20:15:23,197][256877] train() took 11.50 seconds to complete
[36m[2023-07-16 20:15:23,198][256877] FPS: 333937.16
[36m[2023-07-16 20:15:27,391][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:15:27,392][256877] Reward + Measures: [[593.64745503   0.131193     0.15417466   0.13564099   0.13477834
[37m[1m    2.13341928]]
[37m[1m[2023-07-16 20:15:27,392][256877] Max Reward on eval: 593.6474550327189
[37m[1m[2023-07-16 20:15:27,392][256877] Min Reward on eval: 593.6474550327189
[37m[1m[2023-07-16 20:15:27,392][256877] Mean Reward across all agents: 593.6474550327189
[37m[1m[2023-07-16 20:15:27,393][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:15:32,364][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:15:32,364][256877] Reward + Measures: [[504.83439351   0.14540002   0.18390001   0.13869999   0.15000001
[37m[1m    2.12406421]
[37m[1m [639.31160644   0.1468       0.17690001   0.15890001   0.1435
[37m[1m    2.17896152]
[37m[1m [309.07753942   0.11870001   0.12730001   0.1156       0.10270001
[37m[1m    2.28113437]
[37m[1m ...
[37m[1m [412.61713708   0.13859999   0.1533       0.1398       0.1283
[37m[1m    2.19398308]
[37m[1m [463.5704517    0.13680001   0.15899999   0.13849999   0.14570001
[37m[1m    2.28781271]
[37m[1m [290.57053349   0.1258       0.1384       0.1213       0.1209
[37m[1m    2.36684966]]
[37m[1m[2023-07-16 20:15:32,364][256877] Max Reward on eval: 953.5695495512336
[37m[1m[2023-07-16 20:15:32,365][256877] Min Reward on eval: 163.3557433994487
[37m[1m[2023-07-16 20:15:32,365][256877] Mean Reward across all agents: 494.9533789887555
[37m[1m[2023-07-16 20:15:32,365][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:15:32,369][256877] mean_value=73.70888899882371, max_value=541.2867092182016
[37m[1m[2023-07-16 20:15:32,372][256877] New mean coefficients: [[4.2339277  0.44468626 0.5937251  0.16638285 0.19721861 1.8679891 ]]
[37m[1m[2023-07-16 20:15:32,373][256877] Moving the mean solution point...
[36m[2023-07-16 20:15:41,404][256877] train() took 9.03 seconds to complete
[36m[2023-07-16 20:15:41,404][256877] FPS: 425272.14
[36m[2023-07-16 20:15:41,406][256877] itr=5, itrs=2000, Progress: 0.25%
[36m[2023-07-16 20:15:53,075][256877] train() took 11.65 seconds to complete
[36m[2023-07-16 20:15:53,075][256877] FPS: 329712.54
[36m[2023-07-16 20:15:57,409][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:15:57,409][256877] Reward + Measures: [[727.47119822   0.13285534   0.16181633   0.13961799   0.14045601
[37m[1m    2.2139976 ]]
[37m[1m[2023-07-16 20:15:57,409][256877] Max Reward on eval: 727.4711982171534
[37m[1m[2023-07-16 20:15:57,410][256877] Min Reward on eval: 727.4711982171534
[37m[1m[2023-07-16 20:15:57,410][256877] Mean Reward across all agents: 727.4711982171534
[37m[1m[2023-07-16 20:15:57,410][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:16:02,605][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:16:02,611][256877] Reward + Measures: [[676.271019     0.15760002   0.18370001   0.1451       0.15699999
[37m[1m    2.33553219]
[37m[1m [593.02703666   0.1374       0.16250001   0.14060001   0.1477
[37m[1m    2.22032523]
[37m[1m [762.28363036   0.13240001   0.18350001   0.14500001   0.1592
[37m[1m    2.27139711]
[37m[1m ...
[37m[1m [395.89942217   0.12090001   0.13210002   0.1025       0.10740001
[37m[1m    2.29663682]
[37m[1m [332.26337946   0.1061       0.14000002   0.12249999   0.1319
[37m[1m    2.17204452]
[37m[1m [792.43900872   0.1408       0.17840001   0.15700001   0.16129999
[37m[1m    2.32522082]]
[37m[1m[2023-07-16 20:16:02,611][256877] Max Reward on eval: 1044.3642883536406
[37m[1m[2023-07-16 20:16:02,612][256877] Min Reward on eval: 228.6571376234293
[37m[1m[2023-07-16 20:16:02,612][256877] Mean Reward across all agents: 622.2826112068838
[37m[1m[2023-07-16 20:16:02,612][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:16:02,616][256877] mean_value=51.51590674880421, max_value=472.36080138183786
[37m[1m[2023-07-16 20:16:02,619][256877] New mean coefficients: [[3.2162385  0.7111155  0.45768294 0.47690386 0.08339544 2.2755704 ]]
[37m[1m[2023-07-16 20:16:02,620][256877] Moving the mean solution point...
[36m[2023-07-16 20:16:11,619][256877] train() took 9.00 seconds to complete
[36m[2023-07-16 20:16:11,619][256877] FPS: 426776.58
[36m[2023-07-16 20:16:11,622][256877] itr=6, itrs=2000, Progress: 0.30%
[36m[2023-07-16 20:16:23,243][256877] train() took 11.60 seconds to complete
[36m[2023-07-16 20:16:23,243][256877] FPS: 331098.05
[36m[2023-07-16 20:16:27,489][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:16:27,489][256877] Reward + Measures: [[845.29908564   0.13438566   0.16634065   0.14278734   0.14097333
[37m[1m    2.30358791]]
[37m[1m[2023-07-16 20:16:27,490][256877] Max Reward on eval: 845.2990856439116
[37m[1m[2023-07-16 20:16:27,490][256877] Min Reward on eval: 845.2990856439116
[37m[1m[2023-07-16 20:16:27,490][256877] Mean Reward across all agents: 845.2990856439116
[37m[1m[2023-07-16 20:16:27,490][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:16:32,534][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:16:32,535][256877] Reward + Measures: [[718.78944586   0.1243       0.16759999   0.14160001   0.1503
[37m[1m    2.33982134]
[37m[1m [727.65216256   0.1506       0.185        0.1494       0.16170001
[37m[1m    2.32170844]
[37m[1m [751.90217786   0.14829999   0.17120001   0.15100001   0.1657
[37m[1m    2.31868911]
[37m[1m ...
[37m[1m [706.28590632   0.12319999   0.1569       0.14260001   0.1252
[37m[1m    2.31563044]
[37m[1m [808.45193293   0.14040001   0.17489998   0.15329999   0.15050001
[37m[1m    2.33709979]
[37m[1m [444.79886916   0.14030001   0.13780001   0.1513       0.13759999
[37m[1m    2.36009479]]
[37m[1m[2023-07-16 20:16:32,535][256877] Max Reward on eval: 1170.9082870692946
[37m[1m[2023-07-16 20:16:32,535][256877] Min Reward on eval: 326.0957870595157
[37m[1m[2023-07-16 20:16:32,536][256877] Mean Reward across all agents: 715.8405317757465
[37m[1m[2023-07-16 20:16:32,536][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:16:32,539][256877] mean_value=-4.3243989337476645, max_value=450.7433563598004
[37m[1m[2023-07-16 20:16:32,542][256877] New mean coefficients: [[2.52206    1.2071002  1.053858   0.13984501 0.4527685  1.6228722 ]]
[37m[1m[2023-07-16 20:16:32,543][256877] Moving the mean solution point...
[36m[2023-07-16 20:16:41,545][256877] train() took 9.00 seconds to complete
[36m[2023-07-16 20:16:41,546][256877] FPS: 426645.58
[36m[2023-07-16 20:16:41,548][256877] itr=7, itrs=2000, Progress: 0.35%
[36m[2023-07-16 20:16:53,099][256877] train() took 11.53 seconds to complete
[36m[2023-07-16 20:16:53,099][256877] FPS: 333106.78
[36m[2023-07-16 20:16:57,397][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:16:57,397][256877] Reward + Measures: [[1001.10342354    0.14091599    0.17523633    0.15179634    0.14565399
[37m[1m     2.37494397]]
[37m[1m[2023-07-16 20:16:57,398][256877] Max Reward on eval: 1001.1034235390142
[37m[1m[2023-07-16 20:16:57,398][256877] Min Reward on eval: 1001.1034235390142
[37m[1m[2023-07-16 20:16:57,398][256877] Mean Reward across all agents: 1001.1034235390142
[37m[1m[2023-07-16 20:16:57,398][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:17:02,442][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:17:02,443][256877] Reward + Measures: [[ 920.68293759    0.14189999    0.168         0.1559        0.14070001
[37m[1m     2.37327647]
[37m[1m [ 869.81667703    0.1221        0.1531        0.1411        0.13600001
[37m[1m     2.38991141]
[37m[1m [ 301.40951927    0.0887        0.0987        0.08530001    0.08560001
[37m[1m     2.48577857]
[37m[1m ...
[37m[1m [ 865.35922049    0.14300001    0.17910002    0.16250001    0.1454
[37m[1m     2.41637683]
[37m[1m [1187.01244351    0.1619        0.26100001    0.18770002    0.2016
[37m[1m     2.39283991]
[37m[1m [ 573.17636873    0.14850001    0.16719998    0.13450001    0.16670001
[37m[1m     2.53332138]]
[37m[1m[2023-07-16 20:17:02,443][256877] Max Reward on eval: 1303.363159180805
[37m[1m[2023-07-16 20:17:02,443][256877] Min Reward on eval: 301.4095192691311
[37m[1m[2023-07-16 20:17:02,443][256877] Mean Reward across all agents: 855.1595903904185
[37m[1m[2023-07-16 20:17:02,444][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:17:02,448][256877] mean_value=-17.075025857733284, max_value=431.1285429326532
[37m[1m[2023-07-16 20:17:02,451][256877] New mean coefficients: [[1.8958328  1.3319274  1.4368069  0.71393454 0.25364858 0.8016335 ]]
[37m[1m[2023-07-16 20:17:02,452][256877] Moving the mean solution point...
[36m[2023-07-16 20:17:11,428][256877] train() took 8.97 seconds to complete
[36m[2023-07-16 20:17:11,429][256877] FPS: 427861.35
[36m[2023-07-16 20:17:11,431][256877] itr=8, itrs=2000, Progress: 0.40%
[36m[2023-07-16 20:17:23,100][256877] train() took 11.64 seconds to complete
[36m[2023-07-16 20:17:23,100][256877] FPS: 329793.88
[36m[2023-07-16 20:17:27,472][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:17:27,472][256877] Reward + Measures: [[1137.80884909    0.14368667    0.18294199    0.15992233    0.14723065
[37m[1m     2.42103481]]
[37m[1m[2023-07-16 20:17:27,473][256877] Max Reward on eval: 1137.8088490860735
[37m[1m[2023-07-16 20:17:27,473][256877] Min Reward on eval: 1137.8088490860735
[37m[1m[2023-07-16 20:17:27,473][256877] Mean Reward across all agents: 1137.8088490860735
[37m[1m[2023-07-16 20:17:27,473][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:17:32,648][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:17:32,649][256877] Reward + Measures: [[ 648.8099251     0.13020001    0.1476        0.1148        0.11900001
[37m[1m     2.49614978]
[37m[1m [ 817.56400208    0.13520001    0.1671        0.15040001    0.14400001
[37m[1m     2.484761  ]
[37m[1m [ 960.01704404    0.15530001    0.20999999    0.15620001    0.1534
[37m[1m     2.41194868]
[37m[1m ...
[37m[1m [1107.03913498    0.16559999    0.19980001    0.1948        0.1736
[37m[1m     2.29416728]
[37m[1m [ 778.2745971     0.1551        0.1736        0.17290001    0.14749999
[37m[1m     2.41383648]
[37m[1m [ 826.67300599    0.1365        0.175         0.139         0.13950001
[37m[1m     2.32379031]]
[37m[1m[2023-07-16 20:17:32,649][256877] Max Reward on eval: 1502.4036598205566
[37m[1m[2023-07-16 20:17:32,649][256877] Min Reward on eval: 399.9020213507116
[37m[1m[2023-07-16 20:17:32,649][256877] Mean Reward across all agents: 967.2287862034166
[37m[1m[2023-07-16 20:17:32,650][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:17:32,653][256877] mean_value=-58.82109312817817, max_value=476.3537804889618
[37m[1m[2023-07-16 20:17:32,655][256877] New mean coefficients: [[1.3047972 0.938684  1.303218  0.8743455 0.6758562 0.8769821]]
[37m[1m[2023-07-16 20:17:32,656][256877] Moving the mean solution point...
[36m[2023-07-16 20:17:41,752][256877] train() took 9.09 seconds to complete
[36m[2023-07-16 20:17:41,752][256877] FPS: 422269.76
[36m[2023-07-16 20:17:41,754][256877] itr=9, itrs=2000, Progress: 0.45%
[36m[2023-07-16 20:17:53,619][256877] train() took 11.84 seconds to complete
[36m[2023-07-16 20:17:53,619][256877] FPS: 324351.37
[36m[2023-07-16 20:17:57,923][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:17:57,924][256877] Reward + Measures: [[1299.38336065    0.15277767    0.19470966    0.16834334    0.15423366
[37m[1m     2.46414733]]
[37m[1m[2023-07-16 20:17:57,924][256877] Max Reward on eval: 1299.3833606524988
[37m[1m[2023-07-16 20:17:57,924][256877] Min Reward on eval: 1299.3833606524988
[37m[1m[2023-07-16 20:17:57,925][256877] Mean Reward across all agents: 1299.3833606524988
[37m[1m[2023-07-16 20:17:57,925][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:18:02,940][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:18:02,941][256877] Reward + Measures: [[1044.51195141    0.15290001    0.17410001    0.15320002    0.15340002
[37m[1m     2.48017502]
[37m[1m [1184.04514122    0.1675        0.22000001    0.18319999    0.17080002
[37m[1m     2.5188756 ]
[37m[1m [1011.01410725    0.14140001    0.19250001    0.1637        0.15000001
[37m[1m     2.39254069]
[37m[1m ...
[37m[1m [1349.89666751    0.182         0.2119        0.1899        0.1779
[37m[1m     2.42350173]
[37m[1m [1157.14004712    0.14689998    0.2009        0.17029999    0.1471
[37m[1m     2.47924781]
[37m[1m [1045.81583599    0.15180001    0.1956        0.16779999    0.15800002
[37m[1m     2.43989158]]
[37m[1m[2023-07-16 20:18:02,941][256877] Max Reward on eval: 1588.9286346245558
[37m[1m[2023-07-16 20:18:02,941][256877] Min Reward on eval: 474.0509705118835
[37m[1m[2023-07-16 20:18:02,942][256877] Mean Reward across all agents: 1122.6139971505936
[37m[1m[2023-07-16 20:18:02,942][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:18:02,945][256877] mean_value=-59.301012769920796, max_value=407.0136247040414
[37m[1m[2023-07-16 20:18:02,948][256877] New mean coefficients: [[1.4600269  0.1174134  1.1373023  1.2640792  0.88508976 0.98181415]]
[37m[1m[2023-07-16 20:18:02,949][256877] Moving the mean solution point...
[36m[2023-07-16 20:18:11,888][256877] train() took 8.94 seconds to complete
[36m[2023-07-16 20:18:11,889][256877] FPS: 429644.81
[36m[2023-07-16 20:18:11,891][256877] itr=10, itrs=2000, Progress: 0.50%
[36m[2023-07-16 20:20:46,535][256877] train() took 11.75 seconds to complete
[36m[2023-07-16 20:20:46,651][256877] FPS: 326892.94
[36m[2023-07-16 20:20:50,875][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:20:50,881][256877] Reward + Measures: [[1429.56751843    0.15071332    0.19983831    0.17295967    0.15711766
[37m[1m     2.51165009]]
[37m[1m[2023-07-16 20:20:50,881][256877] Max Reward on eval: 1429.5675184298696
[37m[1m[2023-07-16 20:20:50,882][256877] Min Reward on eval: 1429.5675184298696
[37m[1m[2023-07-16 20:20:50,882][256877] Mean Reward across all agents: 1429.5675184298696
[37m[1m[2023-07-16 20:20:50,882][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:20:55,841][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:20:55,842][256877] Reward + Measures: [[1562.29291917    0.1786        0.2335        0.18970001    0.189
[37m[1m     2.5346868 ]
[37m[1m [ 876.07517245    0.1244        0.16059999    0.1402        0.13069999
[37m[1m     2.50212598]
[37m[1m [1063.50240903    0.16049999    0.198         0.15750001    0.1609
[37m[1m     2.45971394]
[37m[1m ...
[37m[1m [1181.95561593    0.16580001    0.21870001    0.18090001    0.1741
[37m[1m     2.49609423]
[37m[1m [1183.34802054    0.13360001    0.1752        0.15549999    0.1354
[37m[1m     2.53932524]
[37m[1m [1504.93121344    0.16730002    0.2368        0.18980001    0.1652
[37m[1m     2.45532799]]
[37m[1m[2023-07-16 20:20:55,842][256877] Max Reward on eval: 1887.2615661419927
[37m[1m[2023-07-16 20:20:55,842][256877] Min Reward on eval: 538.9809351161123
[37m[1m[2023-07-16 20:20:55,843][256877] Mean Reward across all agents: 1236.0200620087332
[37m[1m[2023-07-16 20:20:55,843][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:20:55,846][256877] mean_value=-96.99864189016623, max_value=1796.7936346227675
[37m[1m[2023-07-16 20:20:55,848][256877] New mean coefficients: [[0.82378036 0.3273783  1.5894848  0.8448788  0.5225855  0.5448371 ]]
[37m[1m[2023-07-16 20:20:55,849][256877] Moving the mean solution point...
[36m[2023-07-16 20:21:04,556][256877] train() took 8.71 seconds to complete
[36m[2023-07-16 20:21:04,557][256877] FPS: 441091.19
[36m[2023-07-16 20:21:04,559][256877] itr=11, itrs=2000, Progress: 0.55%
[36m[2023-07-16 20:21:16,377][256877] train() took 11.80 seconds to complete
[36m[2023-07-16 20:21:16,382][256877] FPS: 325533.28
[36m[2023-07-16 20:21:20,674][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:21:20,674][256877] Reward + Measures: [[1596.48006893    0.15416667    0.21388066    0.18451001    0.16237067
[37m[1m     2.55319667]]
[37m[1m[2023-07-16 20:21:20,674][256877] Max Reward on eval: 1596.4800689341444
[37m[1m[2023-07-16 20:21:20,674][256877] Min Reward on eval: 1596.4800689341444
[37m[1m[2023-07-16 20:21:20,675][256877] Mean Reward across all agents: 1596.4800689341444
[37m[1m[2023-07-16 20:21:20,675][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:21:25,839][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:21:25,840][256877] Reward + Measures: [[1327.99502565    0.16749999    0.2218        0.20250002    0.1743
[37m[1m     2.35531616]
[37m[1m [1323.18548968    0.1566        0.1847        0.1674        0.1584
[37m[1m     2.43932033]
[37m[1m [1658.10217285    0.1705        0.25319999    0.22250001    0.20910001
[37m[1m     2.50998998]
[37m[1m ...
[37m[1m [1659.51060483    0.17060001    0.23440002    0.20539999    0.17279999
[37m[1m     2.48566031]
[37m[1m [1826.06600953    0.19420001    0.26480001    0.23270002    0.20460001
[37m[1m     2.54687262]
[37m[1m [1175.31188587    0.15809999    0.22150002    0.1938        0.18279999
[37m[1m     2.45732927]]
[37m[1m[2023-07-16 20:21:25,840][256877] Max Reward on eval: 1864.2398681238294
[37m[1m[2023-07-16 20:21:25,840][256877] Min Reward on eval: 697.758735686168
[37m[1m[2023-07-16 20:21:25,840][256877] Mean Reward across all agents: 1375.7755310331647
[37m[1m[2023-07-16 20:21:25,841][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:21:25,843][256877] mean_value=-42.3321297902215, max_value=2123.6565608191268
[37m[1m[2023-07-16 20:21:25,846][256877] New mean coefficients: [[0.27260786 0.94945025 1.293243   1.0561824  0.9271772  1.027072  ]]
[37m[1m[2023-07-16 20:21:25,847][256877] Moving the mean solution point...
[36m[2023-07-16 20:21:34,895][256877] train() took 9.05 seconds to complete
[36m[2023-07-16 20:21:34,895][256877] FPS: 424489.97
[36m[2023-07-16 20:21:34,897][256877] itr=12, itrs=2000, Progress: 0.60%
[36m[2023-07-16 20:21:46,525][256877] train() took 11.60 seconds to complete
[36m[2023-07-16 20:21:46,526][256877] FPS: 330909.23
[36m[2023-07-16 20:21:50,826][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:21:50,826][256877] Reward + Measures: [[1747.41955722    0.16869734    0.23721966    0.20147701    0.17301001
[37m[1m     2.63945556]]
[37m[1m[2023-07-16 20:21:50,826][256877] Max Reward on eval: 1747.4195572243582
[37m[1m[2023-07-16 20:21:50,827][256877] Min Reward on eval: 1747.4195572243582
[37m[1m[2023-07-16 20:21:50,827][256877] Mean Reward across all agents: 1747.4195572243582
[37m[1m[2023-07-16 20:21:50,827][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:21:55,846][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:21:55,847][256877] Reward + Measures: [[1985.20214078    0.17130001    0.2545        0.207         0.1797
[37m[1m     2.65749383]
[37m[1m [1640.7680912     0.17900001    0.2669        0.2141        0.1909
[37m[1m     2.59627485]
[37m[1m [1569.93916891    0.1652        0.2494        0.2142        0.1904
[37m[1m     2.59581542]
[37m[1m ...
[37m[1m [1444.96058652    0.16670001    0.21510001    0.1846        0.15970001
[37m[1m     2.62197757]
[37m[1m [1579.57633968    0.16470002    0.2304        0.18230002    0.1626
[37m[1m     2.60942316]
[37m[1m [1156.6634807     0.14479999    0.193         0.16580002    0.1468
[37m[1m     2.6202724 ]]
[37m[1m[2023-07-16 20:21:55,847][256877] Max Reward on eval: 2027.7310638271272
[37m[1m[2023-07-16 20:21:55,847][256877] Min Reward on eval: 963.6293029783294
[37m[1m[2023-07-16 20:21:55,848][256877] Mean Reward across all agents: 1549.3590898665675
[37m[1m[2023-07-16 20:21:55,848][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:21:55,852][256877] mean_value=648.3698920978987, max_value=2221.0728872022687
[37m[1m[2023-07-16 20:21:55,855][256877] New mean coefficients: [[-0.7751879   1.5217547   0.88349223  0.7742163   1.3597434   1.4681171 ]]
[37m[1m[2023-07-16 20:21:55,856][256877] Moving the mean solution point...
[36m[2023-07-16 20:22:04,792][256877] train() took 8.93 seconds to complete
[36m[2023-07-16 20:22:04,792][256877] FPS: 429822.48
[36m[2023-07-16 20:22:04,794][256877] itr=13, itrs=2000, Progress: 0.65%
[36m[2023-07-16 20:22:16,396][256877] train() took 11.58 seconds to complete
[36m[2023-07-16 20:22:16,397][256877] FPS: 331592.88
[36m[2023-07-16 20:22:20,642][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:22:20,643][256877] Reward + Measures: [[1712.6260836     0.18736234    0.27130833    0.220965      0.19413501
[37m[1m     2.71876812]]
[37m[1m[2023-07-16 20:22:20,643][256877] Max Reward on eval: 1712.6260836003198
[37m[1m[2023-07-16 20:22:20,643][256877] Min Reward on eval: 1712.6260836003198
[37m[1m[2023-07-16 20:22:20,643][256877] Mean Reward across all agents: 1712.6260836003198
[37m[1m[2023-07-16 20:22:20,644][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:22:25,618][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:22:25,619][256877] Reward + Measures: [[1396.06406216    0.19990002    0.27239999    0.19889998    0.2043
[37m[1m     2.6861279 ]
[37m[1m [1577.27088168    0.19960001    0.2879        0.21859999    0.21400002
[37m[1m     2.71864963]
[37m[1m [1625.87953191    0.20700002    0.29650003    0.23779999    0.2093
[37m[1m     2.72708869]
[37m[1m ...
[37m[1m [1507.12571716    0.17730001    0.26289997    0.22189999    0.17550002
[37m[1m     2.72779393]
[37m[1m [1555.7811508     0.20639999    0.30340001    0.2254        0.20509999
[37m[1m     2.64501119]
[37m[1m [1337.42680363    0.16040002    0.23450001    0.18960001    0.1768
[37m[1m     2.74582911]]
[37m[1m[2023-07-16 20:22:25,619][256877] Max Reward on eval: 1896.8010711431502
[37m[1m[2023-07-16 20:22:25,620][256877] Min Reward on eval: 984.2134151533246
[37m[1m[2023-07-16 20:22:25,620][256877] Mean Reward across all agents: 1553.9609087651154
[37m[1m[2023-07-16 20:22:25,620][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:22:25,624][256877] mean_value=1.1611298478538568, max_value=368.26445222972484
[37m[1m[2023-07-16 20:22:25,627][256877] New mean coefficients: [[-0.00907499  1.3147306   0.5310888   0.7263297   1.0416687   1.3751327 ]]
[37m[1m[2023-07-16 20:22:25,627][256877] Moving the mean solution point...
[36m[2023-07-16 20:22:34,620][256877] train() took 8.99 seconds to complete
[36m[2023-07-16 20:22:34,620][256877] FPS: 427100.58
[36m[2023-07-16 20:22:34,622][256877] itr=14, itrs=2000, Progress: 0.70%
[36m[2023-07-16 20:22:46,249][256877] train() took 11.61 seconds to complete
[36m[2023-07-16 20:22:46,250][256877] FPS: 330886.73
[36m[2023-07-16 20:22:50,562][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:22:50,562][256877] Reward + Measures: [[1666.71337146    0.19750266    0.28080267    0.22566134    0.20475635
[37m[1m     2.84131575]]
[37m[1m[2023-07-16 20:22:50,563][256877] Max Reward on eval: 1666.713371463215
[37m[1m[2023-07-16 20:22:50,563][256877] Min Reward on eval: 1666.713371463215
[37m[1m[2023-07-16 20:22:50,563][256877] Mean Reward across all agents: 1666.713371463215
[37m[1m[2023-07-16 20:22:50,563][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:22:55,615][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:22:55,616][256877] Reward + Measures: [[1525.68058017    0.19530001    0.2737        0.22410002    0.21090002
[37m[1m     2.76659536]
[37m[1m [1605.1400337     0.1948        0.27090001    0.2194        0.2043
[37m[1m     2.84191585]
[37m[1m [1677.02353286    0.1954        0.28340003    0.23029999    0.19890001
[37m[1m     2.86476779]
[37m[1m ...
[37m[1m [1715.2548371     0.21270001    0.2947        0.23360001    0.21259999
[37m[1m     2.86001945]
[37m[1m [1749.94224551    0.20470002    0.29439998    0.2404        0.21830001
[37m[1m     2.8479917 ]
[37m[1m [1481.1320877     0.20280002    0.30410001    0.2247        0.20640002
[37m[1m     2.71653032]]
[37m[1m[2023-07-16 20:22:55,616][256877] Max Reward on eval: 1814.5794983092696
[37m[1m[2023-07-16 20:22:55,616][256877] Min Reward on eval: 1063.796131168306
[37m[1m[2023-07-16 20:22:55,617][256877] Mean Reward across all agents: 1556.2952870483564
[37m[1m[2023-07-16 20:22:55,617][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:22:55,620][256877] mean_value=-94.83256098385657, max_value=1548.9019921326628
[37m[1m[2023-07-16 20:22:55,623][256877] New mean coefficients: [[0.8471879 0.8206413 0.46124   1.0372593 0.6103419 1.2334282]]
[37m[1m[2023-07-16 20:22:55,624][256877] Moving the mean solution point...
[36m[2023-07-16 20:23:04,603][256877] train() took 8.98 seconds to complete
[36m[2023-07-16 20:23:04,603][256877] FPS: 427723.90
[36m[2023-07-16 20:23:04,606][256877] itr=15, itrs=2000, Progress: 0.75%
[36m[2023-07-16 20:23:16,248][256877] train() took 11.62 seconds to complete
[36m[2023-07-16 20:23:16,249][256877] FPS: 330539.40
[36m[2023-07-16 20:23:20,577][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:23:20,578][256877] Reward + Measures: [[1763.84092813    0.19178866    0.26652268    0.22177368    0.19797066
[37m[1m     2.90754008]]
[37m[1m[2023-07-16 20:23:20,578][256877] Max Reward on eval: 1763.8409281260954
[37m[1m[2023-07-16 20:23:20,578][256877] Min Reward on eval: 1763.8409281260954
[37m[1m[2023-07-16 20:23:20,578][256877] Mean Reward across all agents: 1763.8409281260954
[37m[1m[2023-07-16 20:23:20,579][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:23:25,612][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:23:25,613][256877] Reward + Measures: [[1902.19694516    0.20630001    0.27210003    0.23810001    0.2067
[37m[1m     2.88165092]
[37m[1m [1622.71369938    0.1858        0.25250003    0.207         0.1811
[37m[1m     2.86077762]
[37m[1m [1815.83881373    0.1991        0.26719999    0.2297        0.1873
[37m[1m     2.86595654]
[37m[1m ...
[37m[1m [1823.81831361    0.2086        0.28690001    0.23300003    0.21089999
[37m[1m     2.87858272]
[37m[1m [1854.21377566    0.1954        0.2967        0.2332        0.19130002
[37m[1m     2.85525227]
[37m[1m [1632.51793098    0.193         0.26340002    0.21089999    0.19600001
[37m[1m     2.84730053]]
[37m[1m[2023-07-16 20:23:25,613][256877] Max Reward on eval: 2000.5705565929413
[37m[1m[2023-07-16 20:23:25,613][256877] Min Reward on eval: 1157.8874325655402
[37m[1m[2023-07-16 20:23:25,614][256877] Mean Reward across all agents: 1647.2587185426746
[37m[1m[2023-07-16 20:23:25,614][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:23:25,617][256877] mean_value=-67.32496036893627, max_value=285.9868776813305
[37m[1m[2023-07-16 20:23:25,619][256877] New mean coefficients: [[1.0489178  0.7736424  0.48273525 0.81268716 0.40410542 1.292442  ]]
[37m[1m[2023-07-16 20:23:25,620][256877] Moving the mean solution point...
[36m[2023-07-16 20:23:34,624][256877] train() took 9.00 seconds to complete
[36m[2023-07-16 20:23:34,624][256877] FPS: 426569.36
[36m[2023-07-16 20:23:34,627][256877] itr=16, itrs=2000, Progress: 0.80%
[36m[2023-07-16 20:23:46,201][256877] train() took 11.55 seconds to complete
[36m[2023-07-16 20:23:46,201][256877] FPS: 332461.78
[36m[2023-07-16 20:23:50,556][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:23:50,556][256877] Reward + Measures: [[1851.26266825    0.18664366    0.25295332    0.21369767    0.19120732
[37m[1m     2.95450115]]
[37m[1m[2023-07-16 20:23:50,556][256877] Max Reward on eval: 1851.262668245115
[37m[1m[2023-07-16 20:23:50,557][256877] Min Reward on eval: 1851.262668245115
[37m[1m[2023-07-16 20:23:50,557][256877] Mean Reward across all agents: 1851.262668245115
[37m[1m[2023-07-16 20:23:50,557][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:23:55,720][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:23:55,721][256877] Reward + Measures: [[1890.38152313    0.1859        0.2705        0.21669999    0.2102
[37m[1m     2.92300534]
[37m[1m [1655.73318099    0.1781        0.23650001    0.20549999    0.18710001
[37m[1m     2.93724799]
[37m[1m [1558.72005851    0.17119999    0.22840002    0.18710001    0.16229999
[37m[1m     2.94625545]
[37m[1m ...
[37m[1m [1999.90238953    0.19509999    0.27220002    0.22430001    0.19050001
[37m[1m     2.94051218]
[37m[1m [1612.13124087    0.21350001    0.26050001    0.21949999    0.21729998
[37m[1m     2.88398552]
[37m[1m [1607.78792386    0.18170001    0.2368        0.19750002    0.18380001
[37m[1m     2.90181327]]
[37m[1m[2023-07-16 20:23:55,721][256877] Max Reward on eval: 2129.8779754810034
[37m[1m[2023-07-16 20:23:55,721][256877] Min Reward on eval: 1105.1323318310083
[37m[1m[2023-07-16 20:23:55,721][256877] Mean Reward across all agents: 1724.4691445596725
[37m[1m[2023-07-16 20:23:55,722][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:23:55,725][256877] mean_value=-83.67875609815981, max_value=321.73007482317075
[37m[1m[2023-07-16 20:23:55,728][256877] New mean coefficients: [[0.9635079  0.87163454 0.69314784 0.32945606 0.2887771  1.2274038 ]]
[37m[1m[2023-07-16 20:23:55,729][256877] Moving the mean solution point...
[36m[2023-07-16 20:24:04,718][256877] train() took 8.99 seconds to complete
[36m[2023-07-16 20:24:04,718][256877] FPS: 427264.05
[36m[2023-07-16 20:24:04,720][256877] itr=17, itrs=2000, Progress: 0.85%
[36m[2023-07-16 20:24:16,545][256877] train() took 11.80 seconds to complete
[36m[2023-07-16 20:24:16,545][256877] FPS: 325335.88
[36m[2023-07-16 20:24:20,798][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:24:20,799][256877] Reward + Measures: [[1928.42897       0.18265167    0.24599868    0.20897067    0.18469
[37m[1m     2.99348307]]
[37m[1m[2023-07-16 20:24:20,799][256877] Max Reward on eval: 1928.4289700019478
[37m[1m[2023-07-16 20:24:20,799][256877] Min Reward on eval: 1928.4289700019478
[37m[1m[2023-07-16 20:24:20,800][256877] Mean Reward across all agents: 1928.4289700019478
[37m[1m[2023-07-16 20:24:20,800][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:24:25,802][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:24:25,802][256877] Reward + Measures: [[1749.9822807     0.1772        0.23790002    0.1886        0.1692
[37m[1m     2.99381137]
[37m[1m [1861.0652161     0.1768        0.22850001    0.19850001    0.16900001
[37m[1m     3.01226759]
[37m[1m [1677.12759395    0.17380002    0.2211        0.19490001    0.17739999
[37m[1m     2.93375182]
[37m[1m ...
[37m[1m [1662.92165754    0.17569999    0.23020001    0.1965        0.1665
[37m[1m     2.94872093]
[37m[1m [1822.48842624    0.198         0.2545        0.2088        0.1866
[37m[1m     2.95320129]
[37m[1m [1483.85244365    0.15009999    0.1981        0.1688        0.1601
[37m[1m     3.00360417]]
[37m[1m[2023-07-16 20:24:25,803][256877] Max Reward on eval: 2141.6164092853664
[37m[1m[2023-07-16 20:24:25,803][256877] Min Reward on eval: 1032.252952588722
[37m[1m[2023-07-16 20:24:25,803][256877] Mean Reward across all agents: 1803.854536264233
[37m[1m[2023-07-16 20:24:25,803][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:24:25,806][256877] mean_value=-88.66109676457546, max_value=2088.324064260721
[37m[1m[2023-07-16 20:24:25,809][256877] New mean coefficients: [[ 1.1181896   0.55647707  0.45048144 -0.20481274  0.37086     0.96574354]]
[37m[1m[2023-07-16 20:24:25,810][256877] Moving the mean solution point...
[36m[2023-07-16 20:24:34,814][256877] train() took 9.00 seconds to complete
[36m[2023-07-16 20:24:34,814][256877] FPS: 426557.89
[36m[2023-07-16 20:24:34,816][256877] itr=18, itrs=2000, Progress: 0.90%
[36m[2023-07-16 20:24:46,373][256877] train() took 11.54 seconds to complete
[36m[2023-07-16 20:24:46,373][256877] FPS: 332887.96
[36m[2023-07-16 20:24:50,690][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:24:50,690][256877] Reward + Measures: [[1965.39991295    0.17855167    0.23748934    0.20257902    0.17838232
[37m[1m     3.02170515]]
[37m[1m[2023-07-16 20:24:50,690][256877] Max Reward on eval: 1965.3999129541398
[37m[1m[2023-07-16 20:24:50,691][256877] Min Reward on eval: 1965.3999129541398
[37m[1m[2023-07-16 20:24:50,691][256877] Mean Reward across all agents: 1965.3999129541398
[37m[1m[2023-07-16 20:24:50,691][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:24:55,728][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:24:55,728][256877] Reward + Measures: [[1933.16688157    0.18859999    0.24240001    0.23370002    0.17760001
[37m[1m     2.97642207]
[37m[1m [2062.22581488    0.19219999    0.2563        0.2124        0.1911
[37m[1m     2.97589254]
[37m[1m [2096.08442688    0.19149999    0.25740001    0.2225        0.20379999
[37m[1m     2.96518302]
[37m[1m ...
[37m[1m [1956.97742465    0.17          0.23710001    0.2059        0.16970001
[37m[1m     2.99412084]
[37m[1m [1668.72657772    0.17819999    0.21879999    0.19100001    0.16429999
[37m[1m     3.00945663]
[37m[1m [1858.09290316    0.1753        0.2296        0.19840001    0.18610001
[37m[1m     2.98654294]]
[37m[1m[2023-07-16 20:24:55,728][256877] Max Reward on eval: 2257.2185974381864
[37m[1m[2023-07-16 20:24:55,729][256877] Min Reward on eval: 1213.893928565085
[37m[1m[2023-07-16 20:24:55,729][256877] Mean Reward across all agents: 1855.0681400092033
[37m[1m[2023-07-16 20:24:55,729][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:24:55,732][256877] mean_value=-29.43986635876931, max_value=2406.1790529293567
[37m[1m[2023-07-16 20:24:55,734][256877] New mean coefficients: [[ 1.1107597   0.48945704 -0.04198217  0.36395666  0.15726003  1.1576087 ]]
[37m[1m[2023-07-16 20:24:55,735][256877] Moving the mean solution point...
[36m[2023-07-16 20:25:04,768][256877] train() took 9.03 seconds to complete
[36m[2023-07-16 20:25:04,769][256877] FPS: 425182.52
[36m[2023-07-16 20:25:04,771][256877] itr=19, itrs=2000, Progress: 0.95%
[36m[2023-07-16 20:25:16,377][256877] train() took 11.58 seconds to complete
[36m[2023-07-16 20:25:16,377][256877] FPS: 331532.93
[36m[2023-07-16 20:25:20,696][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:25:20,696][256877] Reward + Measures: [[1996.28627818    0.17309299    0.22994566    0.19746       0.17351633
[37m[1m     3.03824997]]
[37m[1m[2023-07-16 20:25:20,697][256877] Max Reward on eval: 1996.2862781770068
[37m[1m[2023-07-16 20:25:20,697][256877] Min Reward on eval: 1996.2862781770068
[37m[1m[2023-07-16 20:25:20,697][256877] Mean Reward across all agents: 1996.2862781770068
[37m[1m[2023-07-16 20:25:20,697][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:25:25,751][256877] Finished Evaluation Step
[37m[1m[2023-07-16 20:25:25,751][256877] Reward + Measures: [[1902.27920916    0.1664        0.2172        0.1865        0.15969999
[37m[1m     3.00325346]
[37m[1m [2232.48812862    0.20019999    0.26610002    0.2185        0.19430001
[37m[1m     2.94545484]
[37m[1m [1992.68960477    0.18390001    0.24160002    0.19759999    0.1831
[37m[1m     3.03908706]
[37m[1m ...
[37m[1m [2066.3073349     0.18980001    0.25499997    0.21409999    0.1921
[37m[1m     3.00275803]
[37m[1m [1944.3461151     0.1927        0.23480001    0.19170001    0.17709999
[37m[1m     3.01331687]
[37m[1m [1819.53717799    0.16679999    0.22650002    0.19500001    0.16890001
[37m[1m     3.00823092]]
[37m[1m[2023-07-16 20:25:25,751][256877] Max Reward on eval: 2356.0679321855305
[37m[1m[2023-07-16 20:25:25,752][256877] Min Reward on eval: 1218.8602294921875
[37m[1m[2023-07-16 20:25:25,752][256877] Mean Reward across all agents: 1887.7003203184795
[37m[1m[2023-07-16 20:25:25,752][256877] Average Trajectory Length: 1000.0
[36m[2023-07-16 20:25:25,755][256877] mean_value=-53.10886239778039, max_value=907.3385094572116
[37m[1m[2023-07-16 20:25:25,758][256877] New mean coefficients: [[ 1.4320847  -0.01995268  0.5138953   0.5082853   0.36718154  1.4295069 ]]
[37m[1m[2023-07-16 20:25:25,758][256877] Moving the mean solution point...
[36m[2023-07-16 20:25:34,897][256877] train() took 9.14 seconds to complete
[36m[2023-07-16 20:25:34,897][256877] FPS: 420270.45
[36m[2023-07-16 20:25:34,900][256877] itr=20, itrs=2000, Progress: 1.00%