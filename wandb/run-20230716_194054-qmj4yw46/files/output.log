
[36m[2023-07-16 19:40:58,429][255637] Environment ant, action_dim=8, obs_dim=87
bounds:
[(0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.0, 4.0)]
using cvt archive
no kmeans
[36m[2023-07-16 19:41:03,283][255637] Created Scheduler for cma_maega with an archive learning rate of 0.1, and add mode batch, using solution dim 28816 and archive dims [10, 10, 10, 10, 10]. Min threshold is -500.0. Restart rule is no_improvement
[36m[2023-07-16 19:41:20,985][255637] train() took 13.68 seconds to complete
[36m[2023-07-16 19:41:20,985][255637] FPS: 280713.34
[36m[2023-07-16 19:41:25,181][255637] Finished Evaluation Step
[37m[1m[2023-07-16 19:41:25,181][255637] Reward + Measures: [[-0.19191747  0.18961932  0.18959534  0.18938598  0.18953234  2.0640378 ]]
[37m[1m[2023-07-16 19:41:25,182][255637] Max Reward on eval: -0.1919174743133141
[37m[1m[2023-07-16 19:41:25,182][255637] Min Reward on eval: -0.1919174743133141
[37m[1m[2023-07-16 19:41:25,182][255637] Mean Reward across all agents: -0.1919174743133141
[37m[1m[2023-07-16 19:41:25,182][255637] Average Trajectory Length: 1000.0
[36m[2023-07-16 19:41:31,036][255637] Finished Evaluation Step
[37m[1m[2023-07-16 19:41:31,037][255637] Reward + Measures: [[-25.85473225   0.19270001   0.1534       0.1592       0.1513
[37m[1m    2.23517489]
[37m[1m [  6.82254288   0.1481       0.15710001   0.16860001   0.1718
[37m[1m    2.19762874]
[37m[1m [  7.82613186   0.12260001   0.1293       0.1279       0.1153
[37m[1m    2.15343928]
[37m[1m ...
[37m[1m [ 22.43109957   0.16779999   0.17569999   0.1656       0.1619
[37m[1m    2.12639308]
[37m[1m [ 35.33276738   0.16419999   0.15710001   0.15100001   0.1657
[37m[1m    2.14637733]
[37m[1m [ 94.1449533    0.17399999   0.16770001   0.17900001   0.1593
[37m[1m    2.20177579]]
[37m[1m[2023-07-16 19:41:31,042][255637] Max Reward on eval: 139.1271899227053
[37m[1m[2023-07-16 19:41:31,042][255637] Min Reward on eval: -89.13732756804674
[37m[1m[2023-07-16 19:41:31,043][255637] Mean Reward across all agents: 13.027569826152703
[37m[1m[2023-07-16 19:41:31,043][255637] Average Trajectory Length: 1000.0
[36m[2023-07-16 19:41:31,067][255637] mean_value=498.5331354329077, max_value=639.1271899227053
[37m[1m[2023-07-16 19:41:31,108][255637] New mean coefficients: [[ 3.236452  -0.7415366 -0.7430911 -1.8843267 -1.9431126 -0.637302 ]]
[37m[1m[2023-07-16 19:41:31,109][255637] Moving the mean solution point...
Traceback (most recent call last):
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/icaros/Documents/PPGADev/algorithm/train_ppga.py", line 599, in <module>
    train_ppga(cfg, vec_env)
  File "/home/icaros/Documents/PPGADev/algorithm/train_ppga.py", line 467, in train_ppga
    ppo.train(vec_env=vec_env,
  File "/home/icaros/Documents/PPGADev/RL/ppo.py", line 344, in train
    self.next_obs, reward, dones, infos = vec_env.step(action)
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/site-packages/brax/envs/to_torch.py", line 61, in step
    reward = self.reward(reward)
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/site-packages/brax/envs/to_torch.py", line 45, in reward
    return torch.jax_to_torch(reward, device=self.device)
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/functools.py", line 888, in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/site-packages/brax/io/torch.py", line 83, in _devicearray_to_tensor
    dpack = jax_dlpack.to_dlpack(value.astype("float32"))
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/site-packages/jax/_src/dlpack.py", line 51, in to_dlpack
    return xla_client._xla.buffer_to_dlpack_managed_tensor(
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/icaros/Documents/PPGADev/algorithm/train_ppga.py", line 599, in <module>
    train_ppga(cfg, vec_env)
  File "/home/icaros/Documents/PPGADev/algorithm/train_ppga.py", line 467, in train_ppga
    ppo.train(vec_env=vec_env,
  File "/home/icaros/Documents/PPGADev/RL/ppo.py", line 344, in train
    self.next_obs, reward, dones, infos = vec_env.step(action)
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/site-packages/brax/envs/to_torch.py", line 61, in step
    reward = self.reward(reward)
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/site-packages/brax/envs/to_torch.py", line 45, in reward
    return torch.jax_to_torch(reward, device=self.device)
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/functools.py", line 888, in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/site-packages/brax/io/torch.py", line 83, in _devicearray_to_tensor
    dpack = jax_dlpack.to_dlpack(value.astype("float32"))
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/site-packages/jax/_src/dlpack.py", line 51, in to_dlpack
    return xla_client._xla.buffer_to_dlpack_managed_tensor(
KeyboardInterrupt