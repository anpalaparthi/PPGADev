{"charts/actor_avg_logstd": -2.1279640197753906, "charts/average_rew_magnitude": 7.199673175811768, "losses/move_mean_agent=False/value_loss": 0.0882125198841095, "losses/value_loss": 0.0882125198841095, "losses/policy_loss": -0.0004391816328279674, "losses/entropy": -0.7090520858764648, "losses/old_approx_kl": 0.023711733520030975, "losses/approx_kl": 0.02195305936038494, "losses/clipfrac": 0.31366340816020966, "losses/explained_variance": 0.2822948098182678, "train/value_loss": 0.0882125198841095, "train/policy_loss": -0.0004391816328279674, "train/value": 0.857232928276062, "train/adv_mean": -1.801020860671997, "train/adv_std": 63.566287994384766, "train/adv_max": 417.4569091796875, "train/adv_min": -683.3949584960938, "train/act_min": -1.7216796875, "train/act_max": 2.07421875, "train/ratio_min": 0.23546209931373596, "train/ratio_max": 3.363816261291504, "Env step": 176640000, "global_step": 176640000, "Update": 460, "FPS": 383721.7812999301, "perf/_fps": 383721.7812999301, "_timestamp": 1694563652.959967, "_runtime": 468.36003589630127, "_step": 1379, "Average Episodic Reward": 7184.457742919922, "reward/reward": 7184.457742919922, "reward/reward_min": 1360.7401123046875, "reward/reward_max": 7417.5546875, "len/len_max": 1000.0, "len/len_min": 207.0, "len/len": 990.48, "train/obs_running_std": 0.5993473529815674, "train/obs_running_mean": 0.08952200412750244, "_wandb": {"runtime": 468}}