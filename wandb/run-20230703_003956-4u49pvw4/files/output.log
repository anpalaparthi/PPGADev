
[36m[2023-07-03 00:40:00,088][187823] Environment ant, action_dim=8, obs_dim=87
using cvt archive
no kmeans
[36m[2023-07-03 00:40:04,426][187823] Created Scheduler for cma_maega with an archive learning rate of 0.1, and add mode batch, using solution dim 28816 and archive dims [7, 7, 7, 7, 7]. Min threshold is -500.0. Restart rule is no_improvement
[36m[2023-07-03 00:40:22,231][187823] train() took 13.84 seconds to complete
[36m[2023-07-03 00:40:22,231][187823] FPS: 277404.30
[36m[2023-07-03 00:40:26,411][187823] Finished Evaluation Step
[37m[1m[2023-07-03 00:40:26,412][187823] Reward + Measures: [[-1.95920535  0.188317    0.18903531  0.18851066  0.18863599  2.06405997]]
[37m[1m[2023-07-03 00:40:26,412][187823] Max Reward on eval: -1.9592053454166296
[37m[1m[2023-07-03 00:40:26,413][187823] Min Reward on eval: -1.9592053454166296
[37m[1m[2023-07-03 00:40:26,413][187823] Mean Reward across all agents: -1.9592053454166296
[37m[1m[2023-07-03 00:40:26,413][187823] Average Trajectory Length: 1000.0
[36m[2023-07-03 00:40:32,016][187823] Finished Evaluation Step
[37m[1m[2023-07-03 00:40:32,016][187823] Reward + Measures: [[-31.34517593   0.15040001   0.14359999   0.13450001   0.13340001
[37m[1m    2.22050691]
[37m[1m [ 54.82826955   0.12740001   0.13770001   0.14030001   0.1478
[37m[1m    2.18729949]
[37m[1m [-10.88372541   0.1744       0.16329999   0.1881       0.1508
[37m[1m    2.16867566]
[37m[1m ...
[37m[1m [ -5.29044623   0.1689       0.17020001   0.1649       0.16540001
[37m[1m    2.11232185]
[37m[1m [-44.00442845   0.1913       0.20490001   0.18740001   0.19670001
[37m[1m    2.1414578 ]
[37m[1m [  9.97661555   0.14580001   0.14389999   0.15190001   0.15030001
[37m[1m    2.20246983]]
[37m[1m[2023-07-03 00:40:32,017][187823] Max Reward on eval: 117.46220595333725
[37m[1m[2023-07-03 00:40:32,017][187823] Min Reward on eval: -104.52886994108557
[37m[1m[2023-07-03 00:40:32,017][187823] Mean Reward across all agents: 5.890751768442622
[37m[1m[2023-07-03 00:40:32,018][187823] Average Trajectory Length: 1000.0
[36m[2023-07-03 00:40:32,038][187823] mean_value=463.05924342814853, max_value=569.7880878306926
[37m[1m[2023-07-03 00:40:32,071][187823] New mean coefficients: [[ 1.7576138  -0.4943005   0.57426035 -0.52869916 -0.98086095 -0.86893135]]
[37m[1m[2023-07-03 00:40:32,072][187823] Moving the mean solution point...
[36m[2023-07-03 00:40:40,844][187823] train() took 8.77 seconds to complete
[36m[2023-07-03 00:40:40,845][187823] FPS: 437839.92
[36m[2023-07-03 00:40:40,847][187823] itr=0, itrs=2000, Progress: 0.00%
[36m[2023-07-03 00:40:52,310][187823] train() took 11.45 seconds to complete
[36m[2023-07-03 00:40:52,310][187823] FPS: 335456.37
Traceback (most recent call last):
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/icaros/Documents/PPGADev/algorithm/train_ppga.py", line 582, in <module>
    train_ppga(cfg, vec_env)
  File "/home/icaros/Documents/PPGADev/algorithm/train_ppga.py", line 398, in train_ppga
    objs, measures, jacobian, metadata = ppo.train(vec_env=vec_env,
  File "/home/icaros/Documents/PPGADev/RL/ppo.py", line 509, in train
    f, m, metadata = self.evaluate(self.vec_inference,
  File "/home/icaros/Documents/PPGADev/RL/ppo.py", line 545, in evaluate
    obs, rew, next_dones, infos = vec_env.step(acts)
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/site-packages/brax/envs/to_torch.py", line 59, in step
    obs, reward, done, info = super().step(action)
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/site-packages/gym/core.py", line 280, in step
    return self.env.step(action)
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/site-packages/gym/wrappers/order_enforcing.py", line 13, in step
    observation, reward, done, info = self.env.step(action)
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/site-packages/brax/envs/wrappers.py", line 323, in step
    self._state, obs, reward, done, info = self._step(self._state, action)
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/site-packages/flax/struct.py", line 119, in clz_from_iterable
    def clz_from_iterable(meta, data):
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/icaros/Documents/PPGADev/algorithm/train_ppga.py", line 582, in <module>
    train_ppga(cfg, vec_env)
  File "/home/icaros/Documents/PPGADev/algorithm/train_ppga.py", line 398, in train_ppga
    objs, measures, jacobian, metadata = ppo.train(vec_env=vec_env,
  File "/home/icaros/Documents/PPGADev/RL/ppo.py", line 509, in train
    f, m, metadata = self.evaluate(self.vec_inference,
  File "/home/icaros/Documents/PPGADev/RL/ppo.py", line 545, in evaluate
    obs, rew, next_dones, infos = vec_env.step(acts)
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/site-packages/brax/envs/to_torch.py", line 59, in step
    obs, reward, done, info = super().step(action)
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/site-packages/gym/core.py", line 280, in step
    return self.env.step(action)
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/site-packages/gym/wrappers/order_enforcing.py", line 13, in step
    observation, reward, done, info = self.env.step(action)
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/site-packages/brax/envs/wrappers.py", line 323, in step
    self._state, obs, reward, done, info = self._step(self._state, action)
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/site-packages/flax/struct.py", line 119, in clz_from_iterable
    def clz_from_iterable(meta, data):
KeyboardInterrupt