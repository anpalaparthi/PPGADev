
[36m[2023-06-25 01:15:33,910][129146] Environment ant, action_dim=8, obs_dim=87
using cvt archive
no kmeans
[36m[2023-06-25 01:15:58,446][129146] Created Scheduler for cma_maega with an archive learning rate of 0.1, and add mode batch, using solution dim 28816 and archive dims [10, 10, 10, 10]. Min threshold is -500.0. Restart rule is no_improvement
[36m[2023-06-25 01:16:18,264][129146] train() took 15.82 seconds to complete
[36m[2023-06-25 01:16:18,264][129146] FPS: 242768.23
[36m[2023-06-25 01:16:22,867][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:16:22,868][129146] Reward + Measures: [[-589.56620482    0.25645345    0.25657788    0.25587821    0.25630942]]
[37m[1m[2023-06-25 01:16:22,868][129146] Max Reward on eval: -589.5662048237124
[37m[1m[2023-06-25 01:16:22,868][129146] Min Reward on eval: -589.5662048237124
[37m[1m[2023-06-25 01:16:22,869][129146] Mean Reward across all agents: -589.5662048237124
[37m[1m[2023-06-25 01:16:22,869][129146] Average Trajectory Length: 554.359
[36m[2023-06-25 01:16:28,664][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:16:28,665][129146] Reward + Measures: [[-932.38187547    0.29278666    0.23957391    0.26401657    0.26295611]
[37m[1m [-669.85654597    0.24115741    0.1845604     0.22632091    0.2281651 ]
[37m[1m [-441.78991366    0.19223468    0.24638148    0.1987222     0.19957183]
[37m[1m ...
[37m[1m [-597.21119315    0.30688223    0.23827758    0.28650433    0.23376475]
[37m[1m [-659.70092038    0.25579262    0.26315928    0.22062416    0.25736505]
[37m[1m [-671.15024294    0.25786       0.24721561    0.2338935     0.23314686]]
[37m[1m[2023-06-25 01:16:28,665][129146] Max Reward on eval: -167.4748681314173
[37m[1m[2023-06-25 01:16:28,665][129146] Min Reward on eval: -1384.0675957415485
[37m[1m[2023-06-25 01:16:28,666][129146] Mean Reward across all agents: -600.5248580009459
[37m[1m[2023-06-25 01:16:28,666][129146] Average Trajectory Length: 501.50066666666663
[36m[2023-06-25 01:16:29,028][129146] mean_value=-100.52485800094584, max_value=332.52513186858266
[37m[1m[2023-06-25 01:16:29,063][129146] New mean coefficients: [[ 0.41363478 -1.705337   -1.6176689  -3.1981122  -1.8664252 ]]
[37m[1m[2023-06-25 01:16:29,065][129146] Moving the mean solution point...
[36m[2023-06-25 01:16:38,611][129146] train() took 9.54 seconds to complete
[36m[2023-06-25 01:16:38,611][129146] FPS: 402330.32
[36m[2023-06-25 01:16:38,613][129146] itr=0, itrs=2000, Progress: 0.00%
[36m[2023-06-25 01:16:49,877][129146] train() took 11.25 seconds to complete
[36m[2023-06-25 01:16:49,877][129146] FPS: 341343.37
[36m[2023-06-25 01:16:54,465][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:16:54,465][129146] Reward + Measures: [[-239.64324766    0.23106974    0.26487088    0.19770637    0.25742072]]
[37m[1m[2023-06-25 01:16:54,465][129146] Max Reward on eval: -239.64324766125304
[37m[1m[2023-06-25 01:16:54,466][129146] Min Reward on eval: -239.64324766125304
[37m[1m[2023-06-25 01:16:54,466][129146] Mean Reward across all agents: -239.64324766125304
[37m[1m[2023-06-25 01:16:54,466][129146] Average Trajectory Length: 245.01066666666665
[36m[2023-06-25 01:16:59,870][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:16:59,871][129146] Reward + Measures: [[-248.62965473    0.24298358    0.31608173    0.08580072    0.25179762]
[37m[1m [-228.72332866    0.28403142    0.20860134    0.22382675    0.24344328]
[37m[1m [-425.32692852    0.15849261    0.28184363    0.14373186    0.21888004]
[37m[1m ...
[37m[1m [-191.70755576    0.17813441    0.22836553    0.29709396    0.19205447]
[37m[1m [-452.65720514    0.31712767    0.26259419    0.19676602    0.19735792]
[37m[1m [-123.71533935    0.21281293    0.29908618    0.13760073    0.24391694]]
[37m[1m[2023-06-25 01:16:59,871][129146] Max Reward on eval: -60.95412204293534
[37m[1m[2023-06-25 01:16:59,871][129146] Min Reward on eval: -654.7423117175
[37m[1m[2023-06-25 01:16:59,871][129146] Mean Reward across all agents: -240.23038550302053
[37m[1m[2023-06-25 01:16:59,872][129146] Average Trajectory Length: 206.20466666666667
[36m[2023-06-25 01:16:59,887][129146] mean_value=238.20370535281629, max_value=426.9814154668711
[37m[1m[2023-06-25 01:16:59,890][129146] New mean coefficients: [[ 1.0701262 -2.2912958 -2.1002493 -3.6149428 -2.5849338]]
[37m[1m[2023-06-25 01:16:59,891][129146] Moving the mean solution point...
[36m[2023-06-25 01:17:09,494][129146] train() took 9.60 seconds to complete
[36m[2023-06-25 01:17:09,495][129146] FPS: 399936.55
[36m[2023-06-25 01:17:09,497][129146] itr=1, itrs=2000, Progress: 0.05%
[36m[2023-06-25 01:17:21,103][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 01:17:21,103][129146] FPS: 331233.46
[36m[2023-06-25 01:17:25,892][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:17:25,893][129146] Reward + Measures: [[-94.43596612   0.21310587   0.27883697   0.16964798   0.25685853]]
[37m[1m[2023-06-25 01:17:25,893][129146] Max Reward on eval: -94.43596612133253
[37m[1m[2023-06-25 01:17:25,893][129146] Min Reward on eval: -94.43596612133253
[37m[1m[2023-06-25 01:17:25,894][129146] Mean Reward across all agents: -94.43596612133253
[37m[1m[2023-06-25 01:17:25,894][129146] Average Trajectory Length: 105.14966666666666
[36m[2023-06-25 01:17:31,364][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:17:31,365][129146] Reward + Measures: [[ -77.55409886    0.27401102    0.29395393    0.07817046    0.28314817]
[37m[1m [ -97.91262306    0.22829123    0.22080465    0.18213241    0.31366616]
[37m[1m [-144.20485657    0.185248      0.25299481    0.14877239    0.24728751]
[37m[1m ...
[37m[1m [-349.53301452    0.18971346    0.25197086    0.12875447    0.1686874 ]
[37m[1m [-215.62850799    0.22027388    0.24248815    0.20930901    0.21185099]
[37m[1m [ -37.17254774    0.23061343    0.32171473    0.13890643    0.22959445]]
[37m[1m[2023-06-25 01:17:31,365][129146] Max Reward on eval: -13.930708567984402
[37m[1m[2023-06-25 01:17:31,366][129146] Min Reward on eval: -379.0355100866873
[37m[1m[2023-06-25 01:17:31,366][129146] Mean Reward across all agents: -112.5049248019938
[37m[1m[2023-06-25 01:17:31,366][129146] Average Trajectory Length: 110.86
[36m[2023-06-25 01:17:31,376][129146] mean_value=194.08958439331445, max_value=453.13382088113576
[37m[1m[2023-06-25 01:17:31,379][129146] New mean coefficients: [[ 1.0448221 -1.6259258 -2.0318277 -3.8228605 -3.01191  ]]
[37m[1m[2023-06-25 01:17:31,381][129146] Moving the mean solution point...
[36m[2023-06-25 01:17:41,077][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 01:17:41,077][129146] FPS: 396093.55
[36m[2023-06-25 01:17:41,079][129146] itr=2, itrs=2000, Progress: 0.10%
[36m[2023-06-25 01:17:52,587][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 01:17:52,587][129146] FPS: 334066.33
[36m[2023-06-25 01:17:57,372][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:17:57,372][129146] Reward + Measures: [[-44.5091767    0.2199945    0.30495888   0.12785327   0.25856239]]
[37m[1m[2023-06-25 01:17:57,373][129146] Max Reward on eval: -44.50917670192486
[37m[1m[2023-06-25 01:17:57,373][129146] Min Reward on eval: -44.50917670192486
[37m[1m[2023-06-25 01:17:57,373][129146] Mean Reward across all agents: -44.50917670192486
[37m[1m[2023-06-25 01:17:57,374][129146] Average Trajectory Length: 52.449666666666666
[36m[2023-06-25 01:18:02,694][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:18:02,695][129146] Reward + Measures: [[ -16.93919232    0.18924826    0.29351717    0.22398777    0.19752708]
[37m[1m [ -38.56813117    0.24600816    0.29980236    0.09932365    0.27795786]
[37m[1m [  -6.1846093     0.14751016    0.3055982     0.21353097    0.25620964]
[37m[1m ...
[37m[1m [ -53.5239264     0.27656513    0.26260027    0.20520072    0.23446229]
[37m[1m [-147.20465336    0.18289065    0.24033165    0.12521671    0.27331844]
[37m[1m [ -55.65251387    0.26766989    0.4023889     0.14944141    0.22514811]]
[37m[1m[2023-06-25 01:18:02,695][129146] Max Reward on eval: -6.184609299642034
[37m[1m[2023-06-25 01:18:02,695][129146] Min Reward on eval: -248.84226516286725
[37m[1m[2023-06-25 01:18:02,696][129146] Mean Reward across all agents: -67.29836419065398
[37m[1m[2023-06-25 01:18:02,696][129146] Average Trajectory Length: 68.68466666666666
[36m[2023-06-25 01:18:02,708][129146] mean_value=148.5484714236848, max_value=471.64989752471445
[37m[1m[2023-06-25 01:18:02,711][129146] New mean coefficients: [[-0.4095546  -0.14061034 -1.8115205  -4.6183395  -2.824336  ]]
[37m[1m[2023-06-25 01:18:02,712][129146] Moving the mean solution point...
[36m[2023-06-25 01:18:12,402][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 01:18:12,402][129146] FPS: 396349.28
[36m[2023-06-25 01:18:12,404][129146] itr=3, itrs=2000, Progress: 0.15%
[36m[2023-06-25 01:18:23,988][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 01:18:23,988][129146] FPS: 331896.06
[36m[2023-06-25 01:18:28,754][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:18:28,754][129146] Reward + Measures: [[-26.46955336   0.37302595   0.32171991   0.03465511   0.25727719]]
[37m[1m[2023-06-25 01:18:28,755][129146] Max Reward on eval: -26.469553362644724
[37m[1m[2023-06-25 01:18:28,755][129146] Min Reward on eval: -26.469553362644724
[37m[1m[2023-06-25 01:18:28,755][129146] Mean Reward across all agents: -26.469553362644724
[37m[1m[2023-06-25 01:18:28,755][129146] Average Trajectory Length: 17.971
[36m[2023-06-25 01:18:34,181][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:18:34,182][129146] Reward + Measures: [[-31.71074093   0.30827799   0.23654461   0.13440971   0.2508218 ]
[37m[1m [-56.78641052   0.38523635   0.24051526   0.05674688   0.23903993]
[37m[1m [-38.31365078   0.31807542   0.34186509   0.0296131    0.23102184]
[37m[1m ...
[37m[1m [-28.50450211   0.3834129    0.3217862    0.08317795   0.29339036]
[37m[1m [-46.87028962   0.27271539   0.2782062    0.11719386   0.22457512]
[37m[1m [-55.82579033   0.27132827   0.26934746   0.10833762   0.2324883 ]]
[37m[1m[2023-06-25 01:18:34,182][129146] Max Reward on eval: -11.667755157127976
[37m[1m[2023-06-25 01:18:34,182][129146] Min Reward on eval: -356.8861227379181
[37m[1m[2023-06-25 01:18:34,182][129146] Mean Reward across all agents: -54.544714241885394
[37m[1m[2023-06-25 01:18:34,183][129146] Average Trajectory Length: 42.67466666666667
[36m[2023-06-25 01:18:34,190][129146] mean_value=163.95922569957258, max_value=482.7148766733706
[37m[1m[2023-06-25 01:18:34,193][129146] New mean coefficients: [[-1.1496636   0.02800114 -1.4039159  -3.8009548  -1.6484144 ]]
[37m[1m[2023-06-25 01:18:34,195][129146] Moving the mean solution point...
[36m[2023-06-25 01:18:43,767][129146] train() took 9.57 seconds to complete
[36m[2023-06-25 01:18:43,768][129146] FPS: 401206.35
[36m[2023-06-25 01:18:43,770][129146] itr=4, itrs=2000, Progress: 0.20%
[36m[2023-06-25 01:18:55,078][129146] train() took 11.30 seconds to complete
[36m[2023-06-25 01:18:55,078][129146] FPS: 339950.11
[36m[2023-06-25 01:18:59,789][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:18:59,789][129146] Reward + Measures: [[-51.30735801   0.37012142   0.24177641   0.03515191   0.25047868]]
[37m[1m[2023-06-25 01:18:59,790][129146] Max Reward on eval: -51.3073580079509
[37m[1m[2023-06-25 01:18:59,790][129146] Min Reward on eval: -51.3073580079509
[37m[1m[2023-06-25 01:18:59,790][129146] Mean Reward across all agents: -51.3073580079509
[37m[1m[2023-06-25 01:18:59,791][129146] Average Trajectory Length: 23.573
[36m[2023-06-25 01:19:05,303][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:19:05,304][129146] Reward + Measures: [[-79.07742944   0.38263646   0.19491513   0.02388602   0.2837019 ]
[37m[1m [-71.78430147   0.31974396   0.2135953    0.06098143   0.26607984]
[37m[1m [-76.34481231   0.32677746   0.27004454   0.06494386   0.19394965]
[37m[1m ...
[37m[1m [-51.26046546   0.41681018   0.22816899   0.01213235   0.25055894]
[37m[1m [-38.58024308   0.34170198   0.30208966   0.09132669   0.25403666]
[37m[1m [-60.01416107   0.3921462    0.20297936   0.03625357   0.23621526]]
[37m[1m[2023-06-25 01:19:05,304][129146] Max Reward on eval: -14.488381600938737
[37m[1m[2023-06-25 01:19:05,304][129146] Min Reward on eval: -188.20884963390418
[37m[1m[2023-06-25 01:19:05,304][129146] Mean Reward across all agents: -53.55241218536834
[37m[1m[2023-06-25 01:19:05,305][129146] Average Trajectory Length: 29.884333333333334
[36m[2023-06-25 01:19:05,311][129146] mean_value=101.35155314958307, max_value=479.4418417602777
[37m[1m[2023-06-25 01:19:05,314][129146] New mean coefficients: [[-1.4918689   0.41998166 -2.1560907  -4.249759   -1.0500921 ]]
[37m[1m[2023-06-25 01:19:05,315][129146] Moving the mean solution point...
[36m[2023-06-25 01:19:15,088][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 01:19:15,089][129146] FPS: 392970.09
[36m[2023-06-25 01:19:15,091][129146] itr=5, itrs=2000, Progress: 0.25%
[36m[2023-06-25 01:19:26,675][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 01:19:26,675][129146] FPS: 331859.52
[36m[2023-06-25 01:19:31,524][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:19:31,524][129146] Reward + Measures: [[-94.14756242   0.2955291    0.22142407   0.06450677   0.23567976]]
[37m[1m[2023-06-25 01:19:31,524][129146] Max Reward on eval: -94.147562417453
[37m[1m[2023-06-25 01:19:31,524][129146] Min Reward on eval: -94.147562417453
[37m[1m[2023-06-25 01:19:31,525][129146] Mean Reward across all agents: -94.147562417453
[37m[1m[2023-06-25 01:19:31,525][129146] Average Trajectory Length: 35.18233333333333
[36m[2023-06-25 01:19:36,946][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:19:36,947][129146] Reward + Measures: [[ -41.55812041    0.27667212    0.24158958    0.01987281    0.26308736]
[37m[1m [-158.69829555    0.28952724    0.19786248    0.03112523    0.2643784 ]
[37m[1m [ -93.64621683    0.29521543    0.21909676    0.04151144    0.26614246]
[37m[1m ...
[37m[1m [ -96.58522514    0.26296458    0.22280906    0.07807777    0.23614852]
[37m[1m [-101.68942353    0.2908901     0.20782195    0.11109366    0.22515967]
[37m[1m [-122.33342251    0.30041173    0.23877142    0.09262965    0.2180132 ]]
[37m[1m[2023-06-25 01:19:36,947][129146] Max Reward on eval: -28.968127056211234
[37m[1m[2023-06-25 01:19:36,947][129146] Min Reward on eval: -254.541005397783
[37m[1m[2023-06-25 01:19:36,948][129146] Mean Reward across all agents: -86.7314817533902
[37m[1m[2023-06-25 01:19:36,948][129146] Average Trajectory Length: 38.979
[36m[2023-06-25 01:19:36,951][129146] mean_value=26.13210071358536, max_value=428.72019464061594
[37m[1m[2023-06-25 01:19:36,954][129146] New mean coefficients: [[-1.1519737   0.09063357 -2.7859414  -3.7318327  -1.2272828 ]]
[37m[1m[2023-06-25 01:19:36,955][129146] Moving the mean solution point...
[36m[2023-06-25 01:19:46,706][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 01:19:46,706][129146] FPS: 393855.14
[36m[2023-06-25 01:19:46,709][129146] itr=6, itrs=2000, Progress: 0.30%
[36m[2023-06-25 01:19:58,290][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 01:19:58,290][129146] FPS: 331974.88
[36m[2023-06-25 01:20:03,198][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:20:03,199][129146] Reward + Measures: [[-114.14493779    0.28360954    0.20714331    0.05599814    0.23411173]]
[37m[1m[2023-06-25 01:20:03,199][129146] Max Reward on eval: -114.14493778767503
[37m[1m[2023-06-25 01:20:03,199][129146] Min Reward on eval: -114.14493778767503
[37m[1m[2023-06-25 01:20:03,200][129146] Mean Reward across all agents: -114.14493778767503
[37m[1m[2023-06-25 01:20:03,200][129146] Average Trajectory Length: 39.603
[36m[2023-06-25 01:20:08,786][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:20:08,792][129146] Reward + Measures: [[-176.63283568    0.30474219    0.18431698    0.03483137    0.20702401]
[37m[1m [-109.47839314    0.33030489    0.19827734    0.05057564    0.20443653]
[37m[1m [ -68.28233605    0.3150858     0.19642173    0.10514325    0.22861965]
[37m[1m ...
[37m[1m [-118.47188358    0.26886719    0.20312582    0.0449931     0.25066137]
[37m[1m [ -78.24418216    0.32679316    0.2339526     0.09333828    0.20006524]
[37m[1m [ -95.48348753    0.26727843    0.220385      0.06697407    0.23882528]]
[37m[1m[2023-06-25 01:20:08,792][129146] Max Reward on eval: -55.1536177739501
[37m[1m[2023-06-25 01:20:08,793][129146] Min Reward on eval: -323.23736761170437
[37m[1m[2023-06-25 01:20:08,793][129146] Mean Reward across all agents: -107.27242449295244
[37m[1m[2023-06-25 01:20:08,793][129146] Average Trajectory Length: 45.702999999999996
[36m[2023-06-25 01:20:08,796][129146] mean_value=-16.486172840020362, max_value=436.67435081601144
[37m[1m[2023-06-25 01:20:08,799][129146] New mean coefficients: [[-0.11290944 -0.02550915 -3.1472929  -3.2280476  -0.9176382 ]]
[37m[1m[2023-06-25 01:20:08,800][129146] Moving the mean solution point...
[36m[2023-06-25 01:20:18,676][129146] train() took 9.87 seconds to complete
[36m[2023-06-25 01:20:18,677][129146] FPS: 388883.52
[36m[2023-06-25 01:20:18,679][129146] itr=7, itrs=2000, Progress: 0.35%
[36m[2023-06-25 01:20:30,203][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 01:20:30,203][129146] FPS: 333638.35
[36m[2023-06-25 01:20:35,047][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:20:35,048][129146] Reward + Measures: [[-104.59968646    0.31367475    0.17892769    0.03580558    0.22536649]]
[37m[1m[2023-06-25 01:20:35,048][129146] Max Reward on eval: -104.59968645811117
[37m[1m[2023-06-25 01:20:35,048][129146] Min Reward on eval: -104.59968645811117
[37m[1m[2023-06-25 01:20:35,048][129146] Mean Reward across all agents: -104.59968645811117
[37m[1m[2023-06-25 01:20:35,048][129146] Average Trajectory Length: 36.21333333333333
[36m[2023-06-25 01:20:40,546][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:20:40,547][129146] Reward + Measures: [[-116.04757872    0.30848932    0.15263563    0.05286951    0.25539169]
[37m[1m [ -54.04210384    0.40049562    0.22308636    0.04337662    0.29063243]
[37m[1m [ -96.43841497    0.34537384    0.18035109    0.02395605    0.22256775]
[37m[1m ...
[37m[1m [-111.1067886     0.29485586    0.19163907    0.06011218    0.20467043]
[37m[1m [ -96.3863275     0.31552717    0.19101384    0.03453955    0.25914177]
[37m[1m [ -85.33769679    0.3480618     0.22594856    0.05960757    0.26258659]]
[37m[1m[2023-06-25 01:20:40,547][129146] Max Reward on eval: -33.846940249204636
[37m[1m[2023-06-25 01:20:40,547][129146] Min Reward on eval: -332.7401992168103
[37m[1m[2023-06-25 01:20:40,547][129146] Mean Reward across all agents: -90.60140427720776
[37m[1m[2023-06-25 01:20:40,548][129146] Average Trajectory Length: 36.57966666666667
[36m[2023-06-25 01:20:40,551][129146] mean_value=-1.0365544397571496, max_value=221.33425284004517
[37m[1m[2023-06-25 01:20:40,553][129146] New mean coefficients: [[ 1.1943504 -0.4619684 -3.080403  -2.6453245 -1.130263 ]]
[37m[1m[2023-06-25 01:20:40,554][129146] Moving the mean solution point...
[36m[2023-06-25 01:20:50,250][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 01:20:50,250][129146] FPS: 396117.68
[36m[2023-06-25 01:20:50,252][129146] itr=8, itrs=2000, Progress: 0.40%
[36m[2023-06-25 01:21:01,937][129146] train() took 11.67 seconds to complete
[36m[2023-06-25 01:21:01,937][129146] FPS: 329036.07
[36m[2023-06-25 01:21:06,739][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:21:06,739][129146] Reward + Measures: [[-55.55407346   0.33963588   0.18516296   0.0113416    0.23321332]]
[37m[1m[2023-06-25 01:21:06,740][129146] Max Reward on eval: -55.55407345630539
[37m[1m[2023-06-25 01:21:06,740][129146] Min Reward on eval: -55.55407345630539
[37m[1m[2023-06-25 01:21:06,740][129146] Mean Reward across all agents: -55.55407345630539
[37m[1m[2023-06-25 01:21:06,740][129146] Average Trajectory Length: 18.302
[36m[2023-06-25 01:21:12,412][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:21:12,412][129146] Reward + Measures: [[-36.51919831   0.30136526   0.23387118   0.01406619   0.26835996]
[37m[1m [-23.60013382   0.31944445   0.23333333   0.           0.29444444]
[37m[1m [-33.11659781   0.28464055   0.22603874   0.00714286   0.26672503]
[37m[1m ...
[37m[1m [-73.58897055   0.33373305   0.17480229   0.01886076   0.19621514]
[37m[1m [-58.78736354   0.33297238   0.16211677   0.03022727   0.3030214 ]
[37m[1m [-32.67755789   0.3818011    0.21212831   0.           0.27767777]]
[37m[1m[2023-06-25 01:21:12,413][129146] Max Reward on eval: -19.227796763181686
[37m[1m[2023-06-25 01:21:12,413][129146] Min Reward on eval: -212.83582516501193
[37m[1m[2023-06-25 01:21:12,413][129146] Mean Reward across all agents: -51.20672909890362
[37m[1m[2023-06-25 01:21:12,414][129146] Average Trajectory Length: 20.904999999999998
[36m[2023-06-25 01:21:12,418][129146] mean_value=16.787557703089522, max_value=163.50446947676036
[37m[1m[2023-06-25 01:21:12,421][129146] New mean coefficients: [[ 2.2894797  -0.36832452 -3.197295   -2.8165567  -1.508307  ]]
[37m[1m[2023-06-25 01:21:12,422][129146] Moving the mean solution point...
[36m[2023-06-25 01:21:22,210][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 01:21:22,210][129146] FPS: 392382.89
[36m[2023-06-25 01:21:22,212][129146] itr=9, itrs=2000, Progress: 0.45%
[36m[2023-06-25 01:21:33,729][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 01:21:33,730][129146] FPS: 333792.48
[36m[2023-06-25 01:21:38,463][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:21:38,463][129146] Reward + Measures: [[-25.98690456   0.34844291   0.22929434   0.00095946   0.29114631]]
[37m[1m[2023-06-25 01:21:38,463][129146] Max Reward on eval: -25.986904558312535
[37m[1m[2023-06-25 01:21:38,464][129146] Min Reward on eval: -25.986904558312535
[37m[1m[2023-06-25 01:21:38,464][129146] Mean Reward across all agents: -25.986904558312535
[37m[1m[2023-06-25 01:21:38,464][129146] Average Trajectory Length: 9.821333333333333
[36m[2023-06-25 01:21:44,000][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:21:44,001][129146] Reward + Measures: [[-26.85618135   0.39730158   0.20285714   0.           0.30746031]
[37m[1m [-29.31566085   0.44591269   0.19400795   0.           0.28101191]
[37m[1m [-20.40125939   0.44333336   0.24111108   0.           0.32583332]
[37m[1m ...
[37m[1m [-26.18565816   0.410384     0.21218136   0.           0.24653594]
[37m[1m [-22.22954645   0.34999999   0.23333335   0.           0.29166669]
[37m[1m [-44.51911169   0.43566266   0.19061162   0.01384615   0.29986486]]
[37m[1m[2023-06-25 01:21:44,001][129146] Max Reward on eval: -18.42804812192917
[37m[1m[2023-06-25 01:21:44,001][129146] Min Reward on eval: -170.7267191309016
[37m[1m[2023-06-25 01:21:44,002][129146] Mean Reward across all agents: -28.66736189355385
[37m[1m[2023-06-25 01:21:44,002][129146] Average Trajectory Length: 12.198666666666666
[36m[2023-06-25 01:21:44,007][129146] mean_value=17.135355026846174, max_value=470.92856178879737
[37m[1m[2023-06-25 01:21:44,010][129146] New mean coefficients: [[ 2.0338693   0.07166255 -3.5317812  -3.8198678  -1.3560932 ]]
[37m[1m[2023-06-25 01:21:44,011][129146] Moving the mean solution point...
[36m[2023-06-25 01:21:53,890][129146] train() took 9.88 seconds to complete
[36m[2023-06-25 01:21:53,890][129146] FPS: 388777.34
[36m[2023-06-25 01:21:53,892][129146] itr=10, itrs=2000, Progress: 0.50%
[36m[2023-06-25 01:22:07,661][129146] train() took 11.69 seconds to complete
[36m[2023-06-25 01:22:07,667][129146] FPS: 328536.11
[36m[2023-06-25 01:22:08,097][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:22:08,097][129146] Reward + Measures: [[-18.97007232   0.37851566   0.24564193   0.00021739   0.31555989]]
[37m[1m[2023-06-25 01:22:08,097][129146] Max Reward on eval: -18.970072324037552
[37m[1m[2023-06-25 01:22:08,098][129146] Min Reward on eval: -18.970072324037552
[37m[1m[2023-06-25 01:22:08,098][129146] Mean Reward across all agents: -18.970072324037552
[37m[1m[2023-06-25 01:22:08,098][129146] Average Trajectory Length: 8.243333333333332
[36m[2023-06-25 01:22:13,529][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:22:13,529][129146] Reward + Measures: [[-20.42704509   0.38944444   0.24000001   0.           0.28472224]
[37m[1m [-19.09483064   0.39583334   0.23888889   0.           0.32361111]
[37m[1m [-24.68019825   0.36416671   0.21777777   0.           0.27805558]
[37m[1m ...
[37m[1m [-20.87774936   0.37027776   0.22694445   0.           0.30430555]
[37m[1m [-20.3382039    0.36250001   0.24166666   0.           0.33750001]
[37m[1m [-18.14809081   0.34999999   0.23333335   0.           0.30416667]]
[37m[1m[2023-06-25 01:22:13,529][129146] Max Reward on eval: -15.266164809465408
[37m[1m[2023-06-25 01:22:13,530][129146] Min Reward on eval: -119.75762619678862
[37m[1m[2023-06-25 01:22:13,530][129146] Mean Reward across all agents: -24.013046937134195
[37m[1m[2023-06-25 01:22:13,530][129146] Average Trajectory Length: 10.825333333333333
[36m[2023-06-25 01:22:13,535][129146] mean_value=4.9921749754563525, max_value=473.76154094338415
[37m[1m[2023-06-25 01:22:13,538][129146] New mean coefficients: [[ 2.873366   0.9222357 -2.7787652 -2.9149055 -1.739003 ]]
[37m[1m[2023-06-25 01:22:13,539][129146] Moving the mean solution point...
[36m[2023-06-25 01:22:23,338][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 01:22:23,338][129146] FPS: 391952.48
[36m[2023-06-25 01:22:23,340][129146] itr=11, itrs=2000, Progress: 0.55%
[36m[2023-06-25 01:22:34,736][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 01:22:34,736][129146] FPS: 337417.98
[36m[2023-06-25 01:22:35,165][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:22:35,165][129146] Reward + Measures: [[-13.58227654   0.38980708   0.24475451   0.00043907   0.28594062]]
[37m[1m[2023-06-25 01:22:35,165][129146] Max Reward on eval: -13.582276535943771
[37m[1m[2023-06-25 01:22:35,165][129146] Min Reward on eval: -13.582276535943771
[37m[1m[2023-06-25 01:22:35,165][129146] Mean Reward across all agents: -13.582276535943771
[37m[1m[2023-06-25 01:22:35,166][129146] Average Trajectory Length: 8.282
[36m[2023-06-25 01:22:40,734][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:22:40,734][129146] Reward + Measures: [[-15.83214286   0.39583334   0.24722223   0.           0.33472225]
[37m[1m [-20.72053379   0.39430556   0.25111112   0.00625      0.2163889 ]
[37m[1m [-14.37033011   0.37222221   0.24166666   0.           0.30277777]
[37m[1m ...
[37m[1m [-15.12191724   0.43333337   0.24722223   0.           0.25972223]
[37m[1m [-64.63553036   0.3835496    0.21425128   0.0066       0.26227352]
[37m[1m [-18.36888214   0.41624999   0.21916667   0.           0.32500002]]
[37m[1m[2023-06-25 01:22:40,735][129146] Max Reward on eval: -11.112141409027391
[37m[1m[2023-06-25 01:22:40,735][129146] Min Reward on eval: -85.88326830500155
[37m[1m[2023-06-25 01:22:40,735][129146] Mean Reward across all agents: -16.631303266007045
[37m[1m[2023-06-25 01:22:40,735][129146] Average Trajectory Length: 10.692
[36m[2023-06-25 01:22:40,742][129146] mean_value=8.848857554496568, max_value=396.1085489415601
[37m[1m[2023-06-25 01:22:40,745][129146] New mean coefficients: [[ 2.9651196  1.6910262 -1.9113892 -2.169433  -2.5889456]]
[37m[1m[2023-06-25 01:22:40,746][129146] Moving the mean solution point...
[36m[2023-06-25 01:22:50,501][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 01:22:50,501][129146] FPS: 393711.23
[36m[2023-06-25 01:22:50,503][129146] itr=12, itrs=2000, Progress: 0.60%
[36m[2023-06-25 01:23:01,984][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 01:23:01,985][129146] FPS: 334819.31
[36m[2023-06-25 01:23:02,277][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:23:02,278][129146] Reward + Measures: [[-9.83674576  0.39695191  0.24327734  0.00038626  0.25096402]]
[37m[1m[2023-06-25 01:23:02,278][129146] Max Reward on eval: -9.836745757188105
[37m[1m[2023-06-25 01:23:02,278][129146] Min Reward on eval: -9.836745757188105
[37m[1m[2023-06-25 01:23:02,278][129146] Mean Reward across all agents: -9.836745757188105
[37m[1m[2023-06-25 01:23:02,279][129146] Average Trajectory Length: 8.364666666666666
[36m[2023-06-25 01:23:07,658][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:23:07,664][129146] Reward + Measures: [[-11.70356199   0.38416669   0.23944445   0.           0.26055557]
[37m[1m [ -9.55753329   0.35583332   0.23722222   0.           0.24833333]
[37m[1m [ -9.08329725   0.33472225   0.23055556   0.           0.25277779]
[37m[1m ...
[37m[1m [ -9.65551674   0.34583339   0.23055556   0.           0.25416669]
[37m[1m [-10.02291429   0.35694447   0.23055553   0.           0.25416666]
[37m[1m [-10.83966861   0.36250004   0.24166666   0.           0.24166666]]
[37m[1m[2023-06-25 01:23:07,664][129146] Max Reward on eval: -7.265015251934528
[37m[1m[2023-06-25 01:23:07,665][129146] Min Reward on eval: -95.74204367541824
[37m[1m[2023-06-25 01:23:07,665][129146] Mean Reward across all agents: -14.360713635868432
[37m[1m[2023-06-25 01:23:07,665][129146] Average Trajectory Length: 11.020666666666667
[36m[2023-06-25 01:23:07,670][129146] mean_value=2.7250486471326676, max_value=55.13950780634539
[37m[1m[2023-06-25 01:23:07,673][129146] New mean coefficients: [[ 4.0641217   0.45609617 -0.98776466 -1.835111   -2.5155983 ]]
[37m[1m[2023-06-25 01:23:07,674][129146] Moving the mean solution point...
[36m[2023-06-25 01:23:17,247][129146] train() took 9.57 seconds to complete
[36m[2023-06-25 01:23:17,248][129146] FPS: 401186.57
[36m[2023-06-25 01:23:17,250][129146] itr=13, itrs=2000, Progress: 0.65%
[36m[2023-06-25 01:23:28,880][129146] train() took 11.62 seconds to complete
[36m[2023-06-25 01:23:28,880][129146] FPS: 330576.67
[36m[2023-06-25 01:23:29,251][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:23:29,252][129146] Reward + Measures: [[-6.98433977  0.38471293  0.23796894  0.00054152  0.23873352]]
[37m[1m[2023-06-25 01:23:29,252][129146] Max Reward on eval: -6.984339766914665
[37m[1m[2023-06-25 01:23:29,252][129146] Min Reward on eval: -6.984339766914665
[37m[1m[2023-06-25 01:23:29,252][129146] Mean Reward across all agents: -6.984339766914665
[37m[1m[2023-06-25 01:23:29,252][129146] Average Trajectory Length: 8.565333333333333
[36m[2023-06-25 01:23:34,687][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:23:34,687][129146] Reward + Measures: [[ -7.50930238   0.37916669   0.24444444   0.           0.24444444]
[37m[1m [-10.04543941   0.31782827   0.23949495   0.           0.24151516]
[37m[1m [-13.31579725   0.33194444   0.22944446   0.           0.23833333]
[37m[1m ...
[37m[1m [ -9.37822671   0.3391667    0.2261111    0.           0.29305559]
[37m[1m [-15.02446571   0.41323033   0.22516833   0.00645161   0.21103303]
[37m[1m [-11.20565082   0.37888891   0.22805558   0.           0.19916667]]
[37m[1m[2023-06-25 01:23:34,688][129146] Max Reward on eval: -5.883562184125185
[37m[1m[2023-06-25 01:23:34,688][129146] Min Reward on eval: -110.57654734044335
[37m[1m[2023-06-25 01:23:34,688][129146] Mean Reward across all agents: -13.957943018562617
[37m[1m[2023-06-25 01:23:34,689][129146] Average Trajectory Length: 14.991333333333333
[36m[2023-06-25 01:23:34,694][129146] mean_value=6.8063247748873605, max_value=418.50213805636156
[37m[1m[2023-06-25 01:23:34,697][129146] New mean coefficients: [[ 4.934033   0.9576169 -1.0110427 -2.752696  -3.0042362]]
[37m[1m[2023-06-25 01:23:34,698][129146] Moving the mean solution point...
[36m[2023-06-25 01:23:44,319][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 01:23:44,320][129146] FPS: 399160.20
[36m[2023-06-25 01:23:44,322][129146] itr=14, itrs=2000, Progress: 0.70%
[36m[2023-06-25 01:23:55,873][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 01:23:55,873][129146] FPS: 332799.69
[36m[2023-06-25 01:23:56,266][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:23:56,266][129146] Reward + Measures: [[-5.09153318  0.36952701  0.23190771  0.00043585  0.23172486]]
[37m[1m[2023-06-25 01:23:56,266][129146] Max Reward on eval: -5.091533183321977
[37m[1m[2023-06-25 01:23:56,267][129146] Min Reward on eval: -5.091533183321977
[37m[1m[2023-06-25 01:23:56,267][129146] Mean Reward across all agents: -5.091533183321977
[37m[1m[2023-06-25 01:23:56,267][129146] Average Trajectory Length: 8.781666666666666
[36m[2023-06-25 01:24:01,817][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:24:01,823][129146] Reward + Measures: [[-13.72328577   0.39694449   0.24166667   0.           0.27000004]
[37m[1m [ -8.006112     0.3103922    0.22065361   0.           0.2095425 ]
[37m[1m [-12.54498623   0.34753585   0.23446237   0.00322581   0.21224013]
[37m[1m ...
[37m[1m [-14.3145949    0.30309471   0.20191646   0.01074661   0.21757689]
[37m[1m [ -9.76428267   0.36932752   0.24383043   0.           0.22219299]
[37m[1m [-11.25581989   0.39437136   0.21915205   0.01052632   0.24020468]]
[37m[1m[2023-06-25 01:24:01,823][129146] Max Reward on eval: -4.849048108235001
[37m[1m[2023-06-25 01:24:01,823][129146] Min Reward on eval: -138.92148755512318
[37m[1m[2023-06-25 01:24:01,824][129146] Mean Reward across all agents: -16.633166310054587
[37m[1m[2023-06-25 01:24:01,824][129146] Average Trajectory Length: 20.116
[36m[2023-06-25 01:24:01,828][129146] mean_value=-2.106933505264998, max_value=445.2631584383047
[37m[1m[2023-06-25 01:24:01,830][129146] New mean coefficients: [[ 5.023846    0.66344535 -0.29288465 -2.3835988  -2.0268753 ]]
[37m[1m[2023-06-25 01:24:01,831][129146] Moving the mean solution point...
[36m[2023-06-25 01:24:11,531][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 01:24:11,532][129146] FPS: 395937.90
[36m[2023-06-25 01:24:11,534][129146] itr=15, itrs=2000, Progress: 0.75%
[36m[2023-06-25 01:24:23,071][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 01:24:23,071][129146] FPS: 333291.55
[36m[2023-06-25 01:24:23,589][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:24:23,589][129146] Reward + Measures: [[-3.73926063  0.3696053   0.22928849  0.00094418  0.22830057]]
[37m[1m[2023-06-25 01:24:23,590][129146] Max Reward on eval: -3.739260625425726
[37m[1m[2023-06-25 01:24:23,590][129146] Min Reward on eval: -3.739260625425726
[37m[1m[2023-06-25 01:24:23,590][129146] Mean Reward across all agents: -3.739260625425726
[37m[1m[2023-06-25 01:24:23,590][129146] Average Trajectory Length: 8.898666666666667
[36m[2023-06-25 01:24:24,929][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:24:24,930][129146] Reward + Measures: [[ -6.09825065   0.40277776   0.23611112   0.           0.23611112]
[37m[1m [-15.0606807    0.38138038   0.24012156   0.01012658   0.23174906]
[37m[1m [ -5.92821562   0.39722225   0.23333333   0.           0.23333333]
[37m[1m ...
[37m[1m [ -5.10249741   0.36666664   0.23055556   0.           0.23055556]
[37m[1m [ -7.96724782   0.31358978   0.21700855   0.00769231   0.21700855]
[37m[1m [ -5.37129948   0.34166667   0.22777779   0.           0.22777779]]
[37m[1m[2023-06-25 01:24:24,930][129146] Max Reward on eval: -3.0583045735955237
[37m[1m[2023-06-25 01:24:24,930][129146] Min Reward on eval: -35.81039722468704
[37m[1m[2023-06-25 01:24:24,931][129146] Mean Reward across all agents: -7.874695974414517
[37m[1m[2023-06-25 01:24:24,931][129146] Average Trajectory Length: 9.646666666666667
[36m[2023-06-25 01:24:24,934][129146] mean_value=-1.1480260900724293, max_value=18.802360466720465
[37m[1m[2023-06-25 01:24:24,937][129146] New mean coefficients: [[ 4.039705    0.22433329  0.2327677  -1.1136916  -1.4573134 ]]
[37m[1m[2023-06-25 01:24:24,938][129146] Moving the mean solution point...
[36m[2023-06-25 01:24:34,628][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 01:24:34,629][129146] FPS: 396341.06
[36m[2023-06-25 01:24:34,631][129146] itr=16, itrs=2000, Progress: 0.80%
[36m[2023-06-25 01:24:46,304][129146] train() took 11.66 seconds to complete
[36m[2023-06-25 01:24:46,304][129146] FPS: 329318.37
[36m[2023-06-25 01:24:51,182][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:24:51,183][129146] Reward + Measures: [[-4.94794366  0.33175105  0.22298917  0.00200055  0.22317426]]
[37m[1m[2023-06-25 01:24:51,183][129146] Max Reward on eval: -4.9479436573938385
[37m[1m[2023-06-25 01:24:51,183][129146] Min Reward on eval: -4.9479436573938385
[37m[1m[2023-06-25 01:24:51,184][129146] Mean Reward across all agents: -4.9479436573938385
[37m[1m[2023-06-25 01:24:51,184][129146] Average Trajectory Length: 9.857666666666667
[36m[2023-06-25 01:24:56,972][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:24:56,972][129146] Reward + Measures: [[ -5.08061625   0.30361113   0.22555557   0.0125       0.22555557]
[37m[1m [ -4.94417996   0.34250003   0.21761905   0.           0.21761905]
[37m[1m [ -7.26475138   0.30154628   0.23739421   0.           0.17617992]
[37m[1m ...
[37m[1m [-16.60991453   0.3132605    0.19972679   0.01766849   0.21361569]
[37m[1m [-13.73254402   0.28884062   0.20326641   0.01468254   0.22126496]
[37m[1m [ -5.43874986   0.31400329   0.21287583   0.00588235   0.22398694]]
[37m[1m[2023-06-25 01:24:56,972][129146] Max Reward on eval: -1.3417061443731655
[37m[1m[2023-06-25 01:24:56,973][129146] Min Reward on eval: -53.321161567792295
[37m[1m[2023-06-25 01:24:56,973][129146] Mean Reward across all agents: -8.519659328197227
[37m[1m[2023-06-25 01:24:56,973][129146] Average Trajectory Length: 12.866
[36m[2023-06-25 01:24:56,978][129146] mean_value=26.537994760676636, max_value=437.3121202968571
[37m[1m[2023-06-25 01:24:56,981][129146] New mean coefficients: [[ 4.9355326   0.18412435  0.05614552 -0.28583324 -1.3944595 ]]
[37m[1m[2023-06-25 01:24:56,982][129146] Moving the mean solution point...
[36m[2023-06-25 01:25:06,915][129146] train() took 9.93 seconds to complete
[36m[2023-06-25 01:25:06,915][129146] FPS: 386665.35
[36m[2023-06-25 01:25:06,918][129146] itr=17, itrs=2000, Progress: 0.85%
[36m[2023-06-25 01:25:18,448][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 01:25:18,448][129146] FPS: 333434.83
[36m[2023-06-25 01:25:23,254][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:25:23,254][129146] Reward + Measures: [[-3.28254466  0.33233765  0.22527802  0.00281238  0.22501473]]
[37m[1m[2023-06-25 01:25:23,254][129146] Max Reward on eval: -3.2825446554395796
[37m[1m[2023-06-25 01:25:23,255][129146] Min Reward on eval: -3.2825446554395796
[37m[1m[2023-06-25 01:25:23,255][129146] Mean Reward across all agents: -3.2825446554395796
[37m[1m[2023-06-25 01:25:23,255][129146] Average Trajectory Length: 9.493
[36m[2023-06-25 01:25:28,678][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:25:28,678][129146] Reward + Measures: [[ -8.2643001    0.32222223   0.22222224   0.01111111   0.22777776]
[37m[1m [ -7.44642521   0.30260164   0.20775068   0.           0.2795122 ]
[37m[1m [-11.8045787    0.31292376   0.21709041   0.00508475   0.20353107]
[37m[1m ...
[37m[1m [ -3.4450691    0.3201389    0.22083333   0.           0.22083333]
[37m[1m [ -4.40855205   0.34305558   0.23611112   0.0125       0.23611112]
[37m[1m [ -4.45737692   0.35361108   0.23666666   0.           0.23666666]]
[37m[1m[2023-06-25 01:25:28,678][129146] Max Reward on eval: 1.4513687987287995
[37m[1m[2023-06-25 01:25:28,679][129146] Min Reward on eval: -179.78579685810254
[37m[1m[2023-06-25 01:25:28,679][129146] Mean Reward across all agents: -20.864946283571843
[37m[1m[2023-06-25 01:25:28,679][129146] Average Trajectory Length: 24.63533333333333
[36m[2023-06-25 01:25:28,682][129146] mean_value=-4.527496906395825, max_value=484.4912349849939
[37m[1m[2023-06-25 01:25:28,685][129146] New mean coefficients: [[ 4.740352    0.10344032 -0.17903289 -0.5303819  -2.1624405 ]]
[37m[1m[2023-06-25 01:25:28,686][129146] Moving the mean solution point...
[36m[2023-06-25 01:25:38,405][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 01:25:38,405][129146] FPS: 395172.90
[36m[2023-06-25 01:25:38,407][129146] itr=18, itrs=2000, Progress: 0.90%
[36m[2023-06-25 01:25:49,974][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 01:25:49,975][129146] FPS: 332361.73
[36m[2023-06-25 01:25:54,723][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:25:54,724][129146] Reward + Measures: [[-3.98730643  0.32871813  0.23940991  0.00288377  0.20390321]]
[37m[1m[2023-06-25 01:25:54,724][129146] Max Reward on eval: -3.9873064327009295
[37m[1m[2023-06-25 01:25:54,724][129146] Min Reward on eval: -3.9873064327009295
[37m[1m[2023-06-25 01:25:54,725][129146] Mean Reward across all agents: -3.9873064327009295
[37m[1m[2023-06-25 01:25:54,725][129146] Average Trajectory Length: 13.182666666666666
[36m[2023-06-25 01:26:00,215][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:26:00,216][129146] Reward + Measures: [[ -3.53211435   0.34999999   0.23333333   0.           0.23333333]
[37m[1m [-91.39450231   0.21878648   0.18498707   0.08011735   0.19178651]
[37m[1m [-17.55181122   0.34082675   0.25154236   0.01030928   0.15921031]
[37m[1m ...
[37m[1m [-10.27592696   0.22636478   0.25814778   0.04193372   0.19674811]
[37m[1m [-17.74280518   0.33767906   0.20370542   0.00789474   0.1935325 ]
[37m[1m [ -3.42223034   0.30000001   0.26527783   0.           0.23055556]]
[37m[1m[2023-06-25 01:26:00,216][129146] Max Reward on eval: 18.168855993030594
[37m[1m[2023-06-25 01:26:00,216][129146] Min Reward on eval: -91.39450230874645
[37m[1m[2023-06-25 01:26:00,217][129146] Mean Reward across all agents: -9.269855666911807
[37m[1m[2023-06-25 01:26:00,217][129146] Average Trajectory Length: 19.356666666666666
[36m[2023-06-25 01:26:00,220][129146] mean_value=13.850735850691986, max_value=489.67361925570293
[37m[1m[2023-06-25 01:26:00,223][129146] New mean coefficients: [[ 4.485315    0.6009593   0.5376222   0.26699513 -1.9818178 ]]
[37m[1m[2023-06-25 01:26:00,224][129146] Moving the mean solution point...
[36m[2023-06-25 01:26:10,058][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 01:26:10,058][129146] FPS: 390561.56
[36m[2023-06-25 01:26:10,060][129146] itr=19, itrs=2000, Progress: 0.95%
[36m[2023-06-25 01:26:21,704][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 01:26:21,704][129146] FPS: 330153.67
[36m[2023-06-25 01:26:22,222][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:26:22,222][129146] Reward + Measures: [[-1.99888348  0.34024581  0.23736767  0.00222445  0.22796375]]
[37m[1m[2023-06-25 01:26:22,223][129146] Max Reward on eval: -1.9988834754290292
[37m[1m[2023-06-25 01:26:22,223][129146] Min Reward on eval: -1.9988834754290292
[37m[1m[2023-06-25 01:26:22,223][129146] Mean Reward across all agents: -1.9988834754290292
[37m[1m[2023-06-25 01:26:22,223][129146] Average Trajectory Length: 8.929666666666666
[36m[2023-06-25 01:26:27,812][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:26:27,813][129146] Reward + Measures: [[ -2.028761     0.35527778   0.24875002   0.           0.2102778 ]
[37m[1m [-26.75730841   0.30273336   0.29141113   0.0044       0.21115556]
[37m[1m [-17.91151946   0.41436711   0.30955201   0.01636364   0.1824753 ]
[37m[1m ...
[37m[1m [-70.09318945   0.32402048   0.25093755   0.01159231   0.18999359]
[37m[1m [-28.47621393   0.29724577   0.25370157   0.02851951   0.24105015]
[37m[1m [-14.36148613   0.29109231   0.27907318   0.03780195   0.190038  ]]
[37m[1m[2023-06-25 01:26:27,813][129146] Max Reward on eval: -0.9283500667661428
[37m[1m[2023-06-25 01:26:27,813][129146] Min Reward on eval: -211.77526406643446
[37m[1m[2023-06-25 01:26:27,814][129146] Mean Reward across all agents: -22.605597032075387
[37m[1m[2023-06-25 01:26:27,814][129146] Average Trajectory Length: 25.61133333333333
[36m[2023-06-25 01:26:27,816][129146] mean_value=-11.679596911412155, max_value=486.97180402446537
[37m[1m[2023-06-25 01:26:27,819][129146] New mean coefficients: [[ 2.76018     0.3735571   0.05803177 -1.2590175  -1.2071126 ]]
[37m[1m[2023-06-25 01:26:27,820][129146] Moving the mean solution point...
[36m[2023-06-25 01:26:37,653][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 01:26:37,654][129146] FPS: 390557.89
[36m[2023-06-25 01:26:37,656][129146] itr=20, itrs=2000, Progress: 1.00%
[36m[2023-06-25 01:26:51,598][129146] train() took 11.82 seconds to complete
[36m[2023-06-25 01:26:51,599][129146] FPS: 324892.88
[36m[2023-06-25 01:26:52,025][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:26:52,025][129146] Reward + Measures: [[-1.45272241  0.35367984  0.22335656  0.00085307  0.19955143]]
[37m[1m[2023-06-25 01:26:52,025][129146] Max Reward on eval: -1.4527224056889925
[37m[1m[2023-06-25 01:26:52,026][129146] Min Reward on eval: -1.4527224056889925
[37m[1m[2023-06-25 01:26:52,026][129146] Mean Reward across all agents: -1.4527224056889925
[37m[1m[2023-06-25 01:26:52,026][129146] Average Trajectory Length: 10.802999999999999
[36m[2023-06-25 01:26:57,531][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:26:57,531][129146] Reward + Measures: [[  -8.50589388    0.25296131    0.20639865    0.05622682    0.22517434]
[37m[1m [  -8.14947102    0.33615386    0.23324788    0.01153846    0.21153846]
[37m[1m [-541.97967253    0.2392447     0.2109081     0.15425661    0.20291916]
[37m[1m ...
[37m[1m [  -4.18372006    0.36325127    0.2515589     0.01489362    0.16185091]
[37m[1m [ -84.54389672    0.26845971    0.1847695     0.08142576    0.19103323]
[37m[1m [ -47.68302081    0.25671992    0.23922002    0.07273845    0.24295233]]
[37m[1m[2023-06-25 01:26:57,532][129146] Max Reward on eval: 42.14563615907682
[37m[1m[2023-06-25 01:26:57,532][129146] Min Reward on eval: -671.2817468789988
[37m[1m[2023-06-25 01:26:57,532][129146] Mean Reward across all agents: -76.10455883994528
[37m[1m[2023-06-25 01:26:57,532][129146] Average Trajectory Length: 199.79066666666665
[36m[2023-06-25 01:26:57,536][129146] mean_value=3.318593589160076, max_value=499.263800798977
[37m[1m[2023-06-25 01:26:57,539][129146] New mean coefficients: [[ 3.0958228   1.3061595  -0.06273057 -1.237601   -1.4029118 ]]
[37m[1m[2023-06-25 01:26:57,540][129146] Moving the mean solution point...
[36m[2023-06-25 01:27:07,161][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 01:27:07,161][129146] FPS: 399198.14
[36m[2023-06-25 01:27:07,163][129146] itr=21, itrs=2000, Progress: 1.05%
[36m[2023-06-25 01:27:19,058][129146] train() took 11.88 seconds to complete
[36m[2023-06-25 01:27:19,058][129146] FPS: 323227.77
[36m[2023-06-25 01:27:23,872][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:27:23,873][129146] Reward + Measures: [[0.51040739 0.36993384 0.23573884 0.00101658 0.19475031]]
[37m[1m[2023-06-25 01:27:23,873][129146] Max Reward on eval: 0.5104073913254445
[37m[1m[2023-06-25 01:27:23,873][129146] Min Reward on eval: 0.5104073913254445
[37m[1m[2023-06-25 01:27:23,874][129146] Mean Reward across all agents: 0.5104073913254445
[37m[1m[2023-06-25 01:27:23,874][129146] Average Trajectory Length: 11.700666666666667
[36m[2023-06-25 01:27:29,435][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:27:29,435][129146] Reward + Measures: [[-546.77008921    0.2647157     0.17479907    0.25179943    0.14974734]
[37m[1m [ -14.61552656    0.29951799    0.25281149    0.06709519    0.18825433]
[37m[1m [  -2.21516755    0.42679402    0.19645752    0.            0.1336199 ]
[37m[1m ...
[37m[1m [ -46.70046578    0.2228481     0.2475739     0.18065858    0.2214587 ]
[37m[1m [ -67.4689434     0.24346612    0.26574364    0.15882294    0.17166735]
[37m[1m [-361.03664508    0.24647455    0.20529249    0.2162559     0.20330623]]
[37m[1m[2023-06-25 01:27:29,436][129146] Max Reward on eval: 18.911799158109353
[37m[1m[2023-06-25 01:27:29,436][129146] Min Reward on eval: -730.5707707169
[37m[1m[2023-06-25 01:27:29,436][129146] Mean Reward across all agents: -113.57718638818818
[37m[1m[2023-06-25 01:27:29,436][129146] Average Trajectory Length: 97.405
[36m[2023-06-25 01:27:29,441][129146] mean_value=38.795856204393395, max_value=449.02181723995136
[37m[1m[2023-06-25 01:27:29,444][129146] New mean coefficients: [[ 3.729498    1.1320792  -0.15093938 -1.1584435  -1.8243706 ]]
[37m[1m[2023-06-25 01:27:29,445][129146] Moving the mean solution point...
[36m[2023-06-25 01:27:39,182][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 01:27:39,187][129146] FPS: 394445.00
[36m[2023-06-25 01:27:39,190][129146] itr=22, itrs=2000, Progress: 1.10%
[36m[2023-06-25 01:27:50,699][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 01:27:50,699][129146] FPS: 334007.90
[36m[2023-06-25 01:27:55,330][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:27:55,330][129146] Reward + Measures: [[4.42109221 0.35129428 0.24306372 0.00839111 0.15811449]]
[37m[1m[2023-06-25 01:27:55,331][129146] Max Reward on eval: 4.4210922066272955
[37m[1m[2023-06-25 01:27:55,331][129146] Min Reward on eval: 4.4210922066272955
[37m[1m[2023-06-25 01:27:55,331][129146] Mean Reward across all agents: 4.4210922066272955
[37m[1m[2023-06-25 01:27:55,331][129146] Average Trajectory Length: 24.174
[36m[2023-06-25 01:28:00,853][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:28:00,854][129146] Reward + Measures: [[ -10.95880862    0.32048571    0.25487921    0.03596639    0.13631444]
[37m[1m [ -66.59001654    0.30263254    0.21571591    0.13323212    0.19801843]
[37m[1m [   4.07091989    0.29498443    0.24910855    0.015625      0.17027387]
[37m[1m ...
[37m[1m [ -49.44512515    0.37236467    0.22725451    0.09431536    0.18334401]
[37m[1m [ -28.70098628    0.34154058    0.21764207    0.08897742    0.16894455]
[37m[1m [-186.15487121    0.29520747    0.22863272    0.14603949    0.20971023]]
[37m[1m[2023-06-25 01:28:00,854][129146] Max Reward on eval: 68.24290231599007
[37m[1m[2023-06-25 01:28:00,855][129146] Min Reward on eval: -364.6667968526075
[37m[1m[2023-06-25 01:28:00,855][129146] Mean Reward across all agents: -55.96645990453299
[37m[1m[2023-06-25 01:28:00,855][129146] Average Trajectory Length: 70.43233333333333
[36m[2023-06-25 01:28:00,858][129146] mean_value=27.383556779628112, max_value=429.7178549393267
[37m[1m[2023-06-25 01:28:00,861][129146] New mean coefficients: [[ 2.9419708  2.5305843 -0.6905618 -1.8218515 -2.177033 ]]
[37m[1m[2023-06-25 01:28:00,862][129146] Moving the mean solution point...
[36m[2023-06-25 01:28:10,504][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 01:28:10,504][129146] FPS: 398330.16
[36m[2023-06-25 01:28:10,506][129146] itr=23, itrs=2000, Progress: 1.15%
[36m[2023-06-25 01:28:21,933][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 01:28:21,933][129146] FPS: 336427.52
[36m[2023-06-25 01:28:26,727][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:28:26,727][129146] Reward + Measures: [[13.36741257  0.31552166  0.24124493  0.01819357  0.16559814]]
[37m[1m[2023-06-25 01:28:26,727][129146] Max Reward on eval: 13.367412566530897
[37m[1m[2023-06-25 01:28:26,728][129146] Min Reward on eval: 13.367412566530897
[37m[1m[2023-06-25 01:28:26,728][129146] Mean Reward across all agents: 13.367412566530897
[37m[1m[2023-06-25 01:28:26,728][129146] Average Trajectory Length: 33.135999999999996
[36m[2023-06-25 01:28:32,196][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:28:32,197][129146] Reward + Measures: [[   2.33933958    0.39895427    0.24019609    0.            0.19176471]
[37m[1m [  10.92527715    0.25486651    0.23495093    0.07073765    0.17326282]
[37m[1m [ -20.63160702    0.1346242     0.35854286    0.10046431    0.2443924 ]
[37m[1m ...
[37m[1m [ -21.61236057    0.2448626     0.23866844    0.11855832    0.21120679]
[37m[1m [  -1.39988547    0.26230174    0.17795761    0.07787175    0.21830361]
[37m[1m [-110.33886199    0.15708812    0.17359649    0.15415952    0.25599018]]
[37m[1m[2023-06-25 01:28:32,197][129146] Max Reward on eval: 45.942736504334604
[37m[1m[2023-06-25 01:28:32,197][129146] Min Reward on eval: -392.15323527173604
[37m[1m[2023-06-25 01:28:32,198][129146] Mean Reward across all agents: -47.752433463680724
[37m[1m[2023-06-25 01:28:32,198][129146] Average Trajectory Length: 116.07433333333333
[36m[2023-06-25 01:28:32,203][129146] mean_value=25.164306324157895, max_value=508.62533276932083
[37m[1m[2023-06-25 01:28:32,206][129146] New mean coefficients: [[ 3.3060539  2.600879  -0.8700042 -2.2158492 -2.0498817]]
[37m[1m[2023-06-25 01:28:32,207][129146] Moving the mean solution point...
[36m[2023-06-25 01:28:41,934][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 01:28:41,934][129146] FPS: 394837.16
[36m[2023-06-25 01:28:41,936][129146] itr=24, itrs=2000, Progress: 1.20%
[36m[2023-06-25 01:28:53,359][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 01:28:53,359][129146] FPS: 336538.98
[36m[2023-06-25 01:28:58,131][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:28:58,132][129146] Reward + Measures: [[25.86619859  0.28946662  0.23835249  0.04134528  0.16952148]]
[37m[1m[2023-06-25 01:28:58,132][129146] Max Reward on eval: 25.86619858569039
[37m[1m[2023-06-25 01:28:58,132][129146] Min Reward on eval: 25.86619858569039
[37m[1m[2023-06-25 01:28:58,132][129146] Mean Reward across all agents: 25.86619858569039
[37m[1m[2023-06-25 01:28:58,133][129146] Average Trajectory Length: 51.503
[36m[2023-06-25 01:29:03,533][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:29:03,533][129146] Reward + Measures: [[ -22.70336335    0.28628737    0.21696983    0.06063894    0.21253406]
[37m[1m [-388.5478972     0.14283624    0.24662058    0.15474628    0.20909004]
[37m[1m [ -21.01639002    0.25218281    0.18987051    0.09673018    0.19973145]
[37m[1m ...
[37m[1m [ -43.54122962    0.3213636     0.23133616    0.05685008    0.19536005]
[37m[1m [-114.89795704    0.26110134    0.26962364    0.07276787    0.23850361]
[37m[1m [-257.97829834    0.18244191    0.21813671    0.11120104    0.17400286]]
[37m[1m[2023-06-25 01:29:03,534][129146] Max Reward on eval: 39.13908731713891
[37m[1m[2023-06-25 01:29:03,534][129146] Min Reward on eval: -866.3328123548184
[37m[1m[2023-06-25 01:29:03,534][129146] Mean Reward across all agents: -75.1896816235529
[37m[1m[2023-06-25 01:29:03,534][129146] Average Trajectory Length: 103.26133333333333
[36m[2023-06-25 01:29:03,537][129146] mean_value=-52.23666828216461, max_value=359.39925648325686
[37m[1m[2023-06-25 01:29:03,539][129146] New mean coefficients: [[ 2.6115346   2.0248852  -0.49203408 -1.3751076  -1.6339804 ]]
[37m[1m[2023-06-25 01:29:03,540][129146] Moving the mean solution point...
[36m[2023-06-25 01:29:13,199][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 01:29:13,200][129146] FPS: 397617.36
[36m[2023-06-25 01:29:13,202][129146] itr=25, itrs=2000, Progress: 1.25%
[36m[2023-06-25 01:29:24,627][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 01:29:24,627][129146] FPS: 336473.53
[36m[2023-06-25 01:29:29,401][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:29:29,402][129146] Reward + Measures: [[39.11200046  0.27416834  0.24607979  0.05695351  0.16097754]]
[37m[1m[2023-06-25 01:29:29,402][129146] Max Reward on eval: 39.112000462309794
[37m[1m[2023-06-25 01:29:29,402][129146] Min Reward on eval: 39.112000462309794
[37m[1m[2023-06-25 01:29:29,402][129146] Mean Reward across all agents: 39.112000462309794
[37m[1m[2023-06-25 01:29:29,403][129146] Average Trajectory Length: 68.42533333333333
[36m[2023-06-25 01:29:34,929][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:29:34,930][129146] Reward + Measures: [[ -14.05755188    0.34670398    0.21964116    0.04752228    0.17225151]
[37m[1m [ -28.59575091    0.27300939    0.24758795    0.06043595    0.26194182]
[37m[1m [ -37.81686848    0.34570742    0.2384183     0.01922004    0.18743072]
[37m[1m ...
[37m[1m [ -23.82960042    0.41112199    0.24673405    0.02566372    0.19927362]
[37m[1m [ -46.60978977    0.33603022    0.24908064    0.02959463    0.1533452 ]
[37m[1m [-180.10482953    0.36412337    0.30754918    0.20616595    0.31258735]]
[37m[1m[2023-06-25 01:29:34,930][129146] Max Reward on eval: 56.67988943783566
[37m[1m[2023-06-25 01:29:34,930][129146] Min Reward on eval: -659.642683346261
[37m[1m[2023-06-25 01:29:34,930][129146] Mean Reward across all agents: -24.241287241474577
[37m[1m[2023-06-25 01:29:34,931][129146] Average Trajectory Length: 75.801
[36m[2023-06-25 01:29:34,934][129146] mean_value=-3.807553102464711, max_value=457.22330720499156
[37m[1m[2023-06-25 01:29:34,937][129146] New mean coefficients: [[ 2.366664    1.2952102   0.3484     -0.38878137 -1.3349528 ]]
[37m[1m[2023-06-25 01:29:34,938][129146] Moving the mean solution point...
[36m[2023-06-25 01:29:44,779][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 01:29:44,779][129146] FPS: 390285.05
[36m[2023-06-25 01:29:44,782][129146] itr=26, itrs=2000, Progress: 1.30%
[36m[2023-06-25 01:29:56,440][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 01:29:56,440][129146] FPS: 329760.81
[36m[2023-06-25 01:30:01,345][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:30:01,345][129146] Reward + Measures: [[51.33421     0.26316154  0.24857438  0.06400893  0.15820314]]
[37m[1m[2023-06-25 01:30:01,346][129146] Max Reward on eval: 51.33420999873379
[37m[1m[2023-06-25 01:30:01,346][129146] Min Reward on eval: 51.33420999873379
[37m[1m[2023-06-25 01:30:01,346][129146] Mean Reward across all agents: 51.33420999873379
[37m[1m[2023-06-25 01:30:01,346][129146] Average Trajectory Length: 80.58833333333332
[36m[2023-06-25 01:30:06,877][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:30:06,882][129146] Reward + Measures: [[-74.98006021   0.30390856   0.22388302   0.11684337   0.26798129]
[37m[1m [ -5.5762202    0.31620222   0.23694308   0.02142121   0.15434147]
[37m[1m [-56.42982187   0.26972699   0.25745508   0.08593772   0.16458431]
[37m[1m ...
[37m[1m [ 28.90060583   0.27903271   0.29577199   0.07283896   0.17892613]
[37m[1m [-52.18964326   0.26478735   0.22861867   0.0844513    0.26954687]
[37m[1m [-11.21385835   0.27394673   0.22648096   0.10210736   0.24168618]]
[37m[1m[2023-06-25 01:30:06,883][129146] Max Reward on eval: 104.62999163508648
[37m[1m[2023-06-25 01:30:06,883][129146] Min Reward on eval: -282.22073002692775
[37m[1m[2023-06-25 01:30:06,883][129146] Mean Reward across all agents: -30.487325927467463
[37m[1m[2023-06-25 01:30:06,884][129146] Average Trajectory Length: 84.62666666666667
[36m[2023-06-25 01:30:06,887][129146] mean_value=-19.982122911991965, max_value=346.3621984122263
[37m[1m[2023-06-25 01:30:06,889][129146] New mean coefficients: [[ 2.4277856   1.7345563   0.7653535   1.1727386  -0.78459185]]
[37m[1m[2023-06-25 01:30:06,890][129146] Moving the mean solution point...
[36m[2023-06-25 01:30:16,669][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 01:30:16,669][129146] FPS: 392765.39
[36m[2023-06-25 01:30:16,671][129146] itr=27, itrs=2000, Progress: 1.35%
[36m[2023-06-25 01:30:28,312][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 01:30:28,312][129146] FPS: 330271.89
[36m[2023-06-25 01:30:33,123][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:30:33,124][129146] Reward + Measures: [[50.65190261  0.26747575  0.22384733  0.05301613  0.18497452]]
[37m[1m[2023-06-25 01:30:33,124][129146] Max Reward on eval: 50.651902607369685
[37m[1m[2023-06-25 01:30:33,124][129146] Min Reward on eval: 50.651902607369685
[37m[1m[2023-06-25 01:30:33,125][129146] Mean Reward across all agents: 50.651902607369685
[37m[1m[2023-06-25 01:30:33,125][129146] Average Trajectory Length: 112.71233333333333
[36m[2023-06-25 01:30:38,673][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:30:38,678][129146] Reward + Measures: [[11.15670109  0.26645312  0.23325601  0.0227993   0.19573307]
[37m[1m [ 1.52505681  0.3008517   0.2363019   0.06510634  0.20017961]
[37m[1m [-2.02662521  0.36734113  0.24102774  0.01219512  0.15910593]
[37m[1m ...
[37m[1m [ 3.95817926  0.31555334  0.24243975  0.01840659  0.17611852]
[37m[1m [25.06357937  0.21803418  0.2412989   0.15569043  0.18473856]
[37m[1m [25.75765823  0.26535138  0.22022076  0.06796046  0.13647163]]
[37m[1m[2023-06-25 01:30:38,679][129146] Max Reward on eval: 208.25395634101878
[37m[1m[2023-06-25 01:30:38,679][129146] Min Reward on eval: -51.71371090027969
[37m[1m[2023-06-25 01:30:38,679][129146] Mean Reward across all agents: 32.845116895581896
[37m[1m[2023-06-25 01:30:38,679][129146] Average Trajectory Length: 130.60733333333332
[36m[2023-06-25 01:30:38,684][129146] mean_value=18.034901283949495, max_value=356.43138342797454
[37m[1m[2023-06-25 01:30:38,686][129146] New mean coefficients: [[ 2.6103582   1.3045744   1.4369621   1.8780684  -0.27810025]]
[37m[1m[2023-06-25 01:30:38,687][129146] Moving the mean solution point...
[36m[2023-06-25 01:30:48,735][129146] train() took 10.05 seconds to complete
[36m[2023-06-25 01:30:48,736][129146] FPS: 382236.38
[36m[2023-06-25 01:30:48,738][129146] itr=28, itrs=2000, Progress: 1.40%
[36m[2023-06-25 01:31:00,449][129146] train() took 11.70 seconds to complete
[36m[2023-06-25 01:31:00,449][129146] FPS: 328305.95
[36m[2023-06-25 01:31:05,314][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:31:05,315][129146] Reward + Measures: [[68.43102222  0.25275475  0.21902743  0.06501585  0.18052803]]
[37m[1m[2023-06-25 01:31:05,315][129146] Max Reward on eval: 68.43102222171268
[37m[1m[2023-06-25 01:31:05,315][129146] Min Reward on eval: 68.43102222171268
[37m[1m[2023-06-25 01:31:05,315][129146] Mean Reward across all agents: 68.43102222171268
[37m[1m[2023-06-25 01:31:05,315][129146] Average Trajectory Length: 147.43266666666665
[36m[2023-06-25 01:31:10,853][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:31:10,854][129146] Reward + Measures: [[38.30658493  0.16790894  0.17176144  0.10345314  0.16456909]
[37m[1m [16.80692902  0.34386507  0.27196053  0.04791347  0.15478531]
[37m[1m [30.57367234  0.28288504  0.23273225  0.03802242  0.22205082]
[37m[1m ...
[37m[1m [ 6.99655693  0.27229911  0.24146119  0.1218515   0.23391064]
[37m[1m [22.59929279  0.30403656  0.22832556  0.05851557  0.22852431]
[37m[1m [36.44762717  0.22487862  0.20514159  0.09642824  0.24993749]]
[37m[1m[2023-06-25 01:31:10,854][129146] Max Reward on eval: 188.75011287909584
[37m[1m[2023-06-25 01:31:10,854][129146] Min Reward on eval: -32.87536853197962
[37m[1m[2023-06-25 01:31:10,855][129146] Mean Reward across all agents: 49.90253257373113
[37m[1m[2023-06-25 01:31:10,855][129146] Average Trajectory Length: 186.64499999999998
[36m[2023-06-25 01:31:10,859][129146] mean_value=21.27209428063269, max_value=623.039325001568
[37m[1m[2023-06-25 01:31:10,862][129146] New mean coefficients: [[ 3.2011316   1.6205751   1.2129663   2.0066023  -0.48574844]]
[37m[1m[2023-06-25 01:31:10,863][129146] Moving the mean solution point...
[36m[2023-06-25 01:31:20,517][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 01:31:20,517][129146] FPS: 397817.62
[36m[2023-06-25 01:31:20,519][129146] itr=29, itrs=2000, Progress: 1.45%
[36m[2023-06-25 01:31:31,934][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 01:31:31,934][129146] FPS: 336775.58
[36m[2023-06-25 01:31:36,739][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:31:36,739][129146] Reward + Measures: [[82.32056597  0.24763891  0.21748465  0.06456959  0.1757441 ]]
[37m[1m[2023-06-25 01:31:36,739][129146] Max Reward on eval: 82.32056597433903
[37m[1m[2023-06-25 01:31:36,739][129146] Min Reward on eval: 82.32056597433903
[37m[1m[2023-06-25 01:31:36,740][129146] Mean Reward across all agents: 82.32056597433903
[37m[1m[2023-06-25 01:31:36,740][129146] Average Trajectory Length: 158.625
[36m[2023-06-25 01:31:42,339][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:31:42,340][129146] Reward + Measures: [[  2.62197756   0.28364035   0.20012012   0.13702963   0.23287061]
[37m[1m [202.96997831   0.17903598   0.14185025   0.10860127   0.15408228]
[37m[1m [ 27.16287385   0.23901252   0.24714246   0.09787648   0.23971467]
[37m[1m ...
[37m[1m [  3.20524343   0.29809976   0.2597973    0.07458031   0.19658908]
[37m[1m [131.09515506   0.25923491   0.18894379   0.06457617   0.17798904]
[37m[1m [171.58538302   0.19943206   0.16353093   0.08491714   0.14193809]]
[37m[1m[2023-06-25 01:31:42,340][129146] Max Reward on eval: 245.83990881700302
[37m[1m[2023-06-25 01:31:42,340][129146] Min Reward on eval: -83.07938552903943
[37m[1m[2023-06-25 01:31:42,340][129146] Mean Reward across all agents: 56.31134559803919
[37m[1m[2023-06-25 01:31:42,341][129146] Average Trajectory Length: 188.492
[36m[2023-06-25 01:31:42,344][129146] mean_value=0.6869687300662524, max_value=574.4707808827641
[37m[1m[2023-06-25 01:31:42,347][129146] New mean coefficients: [[ 3.0751593   2.0827172   0.66683954  1.362297   -0.3970114 ]]
[37m[1m[2023-06-25 01:31:42,348][129146] Moving the mean solution point...
[36m[2023-06-25 01:31:52,074][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 01:31:52,074][129146] FPS: 394900.66
[36m[2023-06-25 01:31:52,076][129146] itr=30, itrs=2000, Progress: 1.50%
[37m[1m[2023-06-25 01:31:54,188][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000010
[36m[2023-06-25 01:32:05,773][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 01:32:05,773][129146] FPS: 334455.70
[36m[2023-06-25 01:32:10,548][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:32:10,549][129146] Reward + Measures: [[94.54717265  0.24414316  0.21555123  0.07028082  0.17498384]]
[37m[1m[2023-06-25 01:32:10,549][129146] Max Reward on eval: 94.54717264861466
[37m[1m[2023-06-25 01:32:10,549][129146] Min Reward on eval: 94.54717264861466
[37m[1m[2023-06-25 01:32:10,550][129146] Mean Reward across all agents: 94.54717264861466
[37m[1m[2023-06-25 01:32:10,550][129146] Average Trajectory Length: 173.37066666666666
[36m[2023-06-25 01:32:15,890][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:32:15,891][129146] Reward + Measures: [[108.09655997   0.23678601   0.24018244   0.08395498   0.16442053]
[37m[1m [ 77.25197886   0.26340726   0.18555968   0.07986081   0.18978064]
[37m[1m [ 71.25246212   0.22655781   0.202317     0.06157569   0.18149462]
[37m[1m ...
[37m[1m [-20.58276856   0.30634657   0.28433368   0.01918498   0.11997298]
[37m[1m [ 15.45961336   0.321814     0.24412599   0.02126931   0.11100085]
[37m[1m [-26.63940521   0.27296102   0.21807325   0.05743388   0.25036559]]
[37m[1m[2023-06-25 01:32:15,891][129146] Max Reward on eval: 298.61989055264274
[37m[1m[2023-06-25 01:32:15,891][129146] Min Reward on eval: -52.92547178789973
[37m[1m[2023-06-25 01:32:15,892][129146] Mean Reward across all agents: 54.9346487867754
[37m[1m[2023-06-25 01:32:15,892][129146] Average Trajectory Length: 190.46133333333333
[36m[2023-06-25 01:32:15,895][129146] mean_value=-24.911489091922736, max_value=272.27438321099
[37m[1m[2023-06-25 01:32:15,897][129146] New mean coefficients: [[ 2.989898    1.2792528   0.9045908   0.6803292  -0.44445196]]
[37m[1m[2023-06-25 01:32:15,898][129146] Moving the mean solution point...
[36m[2023-06-25 01:32:25,518][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 01:32:25,518][129146] FPS: 399255.63
[36m[2023-06-25 01:32:25,520][129146] itr=31, itrs=2000, Progress: 1.55%
[36m[2023-06-25 01:32:37,173][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 01:32:37,173][129146] FPS: 329895.23
[36m[2023-06-25 01:32:41,933][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:32:41,934][129146] Reward + Measures: [[102.61819577   0.24193044   0.21824796   0.0736656    0.16774197]]
[37m[1m[2023-06-25 01:32:41,934][129146] Max Reward on eval: 102.618195765052
[37m[1m[2023-06-25 01:32:41,934][129146] Min Reward on eval: 102.618195765052
[37m[1m[2023-06-25 01:32:41,935][129146] Mean Reward across all agents: 102.618195765052
[37m[1m[2023-06-25 01:32:41,935][129146] Average Trajectory Length: 181.77966666666666
[36m[2023-06-25 01:32:47,413][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:32:47,413][129146] Reward + Measures: [[ 55.01813749   0.24353848   0.25724113   0.14026497   0.19475184]
[37m[1m [162.79404106   0.1720144    0.16777524   0.10303583   0.15508841]
[37m[1m [114.43487188   0.24338453   0.17105857   0.10755187   0.22154705]
[37m[1m ...
[37m[1m [  1.38459247   0.36317202   0.24385782   0.04342487   0.19190191]
[37m[1m [  8.07564762   0.33304092   0.27741134   0.00483871   0.18779956]
[37m[1m [  5.50903009   0.31661895   0.32065299   0.02537594   0.1610495 ]]
[37m[1m[2023-06-25 01:32:47,414][129146] Max Reward on eval: 277.22854261168976
[37m[1m[2023-06-25 01:32:47,414][129146] Min Reward on eval: -17.610693279700353
[37m[1m[2023-06-25 01:32:47,414][129146] Mean Reward across all agents: 64.53525144105325
[37m[1m[2023-06-25 01:32:47,414][129146] Average Trajectory Length: 164.478
[36m[2023-06-25 01:32:47,417][129146] mean_value=-11.136022452545784, max_value=435.3926190323706
[37m[1m[2023-06-25 01:32:47,420][129146] New mean coefficients: [[ 3.5023875   0.60273725  0.7838153   0.9942904  -0.3676309 ]]
[37m[1m[2023-06-25 01:32:47,421][129146] Moving the mean solution point...
[36m[2023-06-25 01:32:57,227][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 01:32:57,228][129146] FPS: 391638.56
[36m[2023-06-25 01:32:57,230][129146] itr=32, itrs=2000, Progress: 1.60%
[36m[2023-06-25 01:33:08,700][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 01:33:08,700][129146] FPS: 335218.06
[36m[2023-06-25 01:33:13,497][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:33:13,497][129146] Reward + Measures: [[113.58407889   0.24079825   0.2176863    0.07297734   0.16491921]]
[37m[1m[2023-06-25 01:33:13,497][129146] Max Reward on eval: 113.58407888695302
[37m[1m[2023-06-25 01:33:13,498][129146] Min Reward on eval: 113.58407888695302
[37m[1m[2023-06-25 01:33:13,498][129146] Mean Reward across all agents: 113.58407888695302
[37m[1m[2023-06-25 01:33:13,498][129146] Average Trajectory Length: 191.70366666666666
[36m[2023-06-25 01:33:18,975][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:33:18,975][129146] Reward + Measures: [[-369.05376777    0.24303131    0.17582671    0.18554257    0.23508303]
[37m[1m [  28.69828494    0.2829411     0.18344145    0.11807241    0.18047187]
[37m[1m [  72.7590901     0.24548624    0.20282762    0.0897039     0.19305825]
[37m[1m ...
[37m[1m [  98.3158495     0.24664794    0.18836571    0.07271834    0.14989172]
[37m[1m [ -60.20762076    0.30848667    0.26194403    0.159546      0.22198872]
[37m[1m [  31.3087798     0.31714302    0.22469759    0.06387217    0.18522398]]
[37m[1m[2023-06-25 01:33:18,975][129146] Max Reward on eval: 353.0367430073617
[37m[1m[2023-06-25 01:33:18,976][129146] Min Reward on eval: -1095.877027640352
[37m[1m[2023-06-25 01:33:18,976][129146] Mean Reward across all agents: -7.923334784581822
[37m[1m[2023-06-25 01:33:18,976][129146] Average Trajectory Length: 219.94833333333332
[36m[2023-06-25 01:33:18,979][129146] mean_value=-67.7832452003277, max_value=853.0367430073617
[37m[1m[2023-06-25 01:33:18,982][129146] New mean coefficients: [[3.3979082  0.43475354 1.2288961  1.0445844  0.20722806]]
[37m[1m[2023-06-25 01:33:18,983][129146] Moving the mean solution point...
[36m[2023-06-25 01:33:28,711][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 01:33:28,711][129146] FPS: 394793.49
[36m[2023-06-25 01:33:28,713][129146] itr=33, itrs=2000, Progress: 1.65%
[36m[2023-06-25 01:33:40,192][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 01:33:40,193][129146] FPS: 334883.85
[36m[2023-06-25 01:33:45,041][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:33:45,042][129146] Reward + Measures: [[127.59552593   0.23519063   0.21450423   0.07868415   0.16553336]]
[37m[1m[2023-06-25 01:33:45,042][129146] Max Reward on eval: 127.59552592744816
[37m[1m[2023-06-25 01:33:45,042][129146] Min Reward on eval: 127.59552592744816
[37m[1m[2023-06-25 01:33:45,043][129146] Mean Reward across all agents: 127.59552592744816
[37m[1m[2023-06-25 01:33:45,043][129146] Average Trajectory Length: 210.25733333333332
[36m[2023-06-25 01:33:50,529][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:33:50,530][129146] Reward + Measures: [[ 81.33717039   0.25330564   0.2314128    0.0650836    0.14764343]
[37m[1m [ -6.91958199   0.33159396   0.2354406    0.04113765   0.14774115]
[37m[1m [ 69.18845984   0.2394599    0.25806463   0.07484114   0.21498668]
[37m[1m ...
[37m[1m [126.63564102   0.24909902   0.22980788   0.04022184   0.1246552 ]
[37m[1m [123.80598401   0.26460651   0.17976868   0.10219617   0.15959306]
[37m[1m [122.38980131   0.19529381   0.17407855   0.11970394   0.19461672]]
[37m[1m[2023-06-25 01:33:50,530][129146] Max Reward on eval: 260.82670032503955
[37m[1m[2023-06-25 01:33:50,530][129146] Min Reward on eval: -12.88540478150826
[37m[1m[2023-06-25 01:33:50,530][129146] Mean Reward across all agents: 70.43701977870117
[37m[1m[2023-06-25 01:33:50,530][129146] Average Trajectory Length: 173.75766666666667
[36m[2023-06-25 01:33:50,533][129146] mean_value=-30.943352698274467, max_value=500.38032034568255
[37m[1m[2023-06-25 01:33:50,536][129146] New mean coefficients: [[3.0491066  0.71704865 1.1171926  0.35217685 0.52152115]]
[37m[1m[2023-06-25 01:33:50,537][129146] Moving the mean solution point...
[36m[2023-06-25 01:34:00,366][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 01:34:00,366][129146] FPS: 390746.91
[36m[2023-06-25 01:34:00,368][129146] itr=34, itrs=2000, Progress: 1.70%
[36m[2023-06-25 01:34:11,951][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 01:34:11,952][129146] FPS: 331880.14
[36m[2023-06-25 01:34:16,773][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:34:16,773][129146] Reward + Measures: [[132.6241523    0.23474126   0.21579933   0.07396092   0.16706148]]
[37m[1m[2023-06-25 01:34:16,773][129146] Max Reward on eval: 132.6241523022632
[37m[1m[2023-06-25 01:34:16,774][129146] Min Reward on eval: 132.6241523022632
[37m[1m[2023-06-25 01:34:16,774][129146] Mean Reward across all agents: 132.6241523022632
[37m[1m[2023-06-25 01:34:16,774][129146] Average Trajectory Length: 210.01133333333334
[36m[2023-06-25 01:34:22,415][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:34:22,415][129146] Reward + Measures: [[ 22.618167     0.3052935    0.23679081   0.07892738   0.18837591]
[37m[1m [151.63610293   0.24484585   0.21932183   0.05223867   0.16103172]
[37m[1m [ 49.64294942   0.2254498    0.20086856   0.03794267   0.15528463]
[37m[1m ...
[37m[1m [ 52.03394398   0.28929001   0.23466976   0.11594056   0.19249004]
[37m[1m [147.3830551    0.23942783   0.2298502    0.06935054   0.16269079]
[37m[1m [ 65.09444528   0.2859554    0.23229182   0.08470309   0.17872171]]
[37m[1m[2023-06-25 01:34:22,415][129146] Max Reward on eval: 307.28628165562407
[37m[1m[2023-06-25 01:34:22,416][129146] Min Reward on eval: -108.91865163727198
[37m[1m[2023-06-25 01:34:22,416][129146] Mean Reward across all agents: 70.32643695822244
[37m[1m[2023-06-25 01:34:22,416][129146] Average Trajectory Length: 192.62566666666666
[36m[2023-06-25 01:34:22,419][129146] mean_value=-41.5580356450067, max_value=359.4559644058421
[37m[1m[2023-06-25 01:34:22,421][129146] New mean coefficients: [[ 3.108997    0.6301846   0.9943955  -0.00156298  0.46487305]]
[37m[1m[2023-06-25 01:34:22,422][129146] Moving the mean solution point...
[36m[2023-06-25 01:34:32,309][129146] train() took 9.89 seconds to complete
[36m[2023-06-25 01:34:32,309][129146] FPS: 388453.21
[36m[2023-06-25 01:34:32,312][129146] itr=35, itrs=2000, Progress: 1.75%
[36m[2023-06-25 01:34:44,015][129146] train() took 11.69 seconds to complete
[36m[2023-06-25 01:34:44,015][129146] FPS: 328480.87
[36m[2023-06-25 01:34:48,800][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:34:48,801][129146] Reward + Measures: [[145.58769987   0.23432043   0.21981107   0.07610407   0.16021496]]
[37m[1m[2023-06-25 01:34:48,801][129146] Max Reward on eval: 145.58769986663376
[37m[1m[2023-06-25 01:34:48,801][129146] Min Reward on eval: 145.58769986663376
[37m[1m[2023-06-25 01:34:48,801][129146] Mean Reward across all agents: 145.58769986663376
[37m[1m[2023-06-25 01:34:48,802][129146] Average Trajectory Length: 225.28466666666665
[36m[2023-06-25 01:34:54,397][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:34:54,398][129146] Reward + Measures: [[ 16.34760429   0.29599378   0.22772287   0.08043838   0.20708369]
[37m[1m [117.07988643   0.25196791   0.21747832   0.05351896   0.1930449 ]
[37m[1m [105.19268594   0.24712197   0.21540451   0.04685855   0.12525821]
[37m[1m ...
[37m[1m [ 21.90710873   0.28275573   0.23104349   0.07830592   0.19156902]
[37m[1m [ 50.57547202   0.25897416   0.2147202    0.06063355   0.19454204]
[37m[1m [ 99.40827143   0.20885117   0.13691561   0.07407673   0.13491538]]
[37m[1m[2023-06-25 01:34:54,398][129146] Max Reward on eval: 302.9710329112735
[37m[1m[2023-06-25 01:34:54,398][129146] Min Reward on eval: -76.69151277695201
[37m[1m[2023-06-25 01:34:54,398][129146] Mean Reward across all agents: 63.377768734607564
[37m[1m[2023-06-25 01:34:54,399][129146] Average Trajectory Length: 200.35033333333334
[36m[2023-06-25 01:34:54,401][129146] mean_value=-59.40473688874844, max_value=703.2096348843304
[37m[1m[2023-06-25 01:34:54,404][129146] New mean coefficients: [[ 2.7014036  1.0464324  0.5810122 -0.7140634  0.4309299]]
[37m[1m[2023-06-25 01:34:54,404][129146] Moving the mean solution point...
[36m[2023-06-25 01:35:04,258][129146] train() took 9.85 seconds to complete
[36m[2023-06-25 01:35:04,258][129146] FPS: 389782.71
[36m[2023-06-25 01:35:04,260][129146] itr=36, itrs=2000, Progress: 1.80%
[36m[2023-06-25 01:35:15,845][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 01:35:15,846][129146] FPS: 331850.87
[36m[2023-06-25 01:35:20,606][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:35:20,607][129146] Reward + Measures: [[161.11044288   0.22903515   0.21749718   0.07725919   0.15587614]]
[37m[1m[2023-06-25 01:35:20,607][129146] Max Reward on eval: 161.11044287797142
[37m[1m[2023-06-25 01:35:20,607][129146] Min Reward on eval: 161.11044287797142
[37m[1m[2023-06-25 01:35:20,608][129146] Mean Reward across all agents: 161.11044287797142
[37m[1m[2023-06-25 01:35:20,608][129146] Average Trajectory Length: 243.52733333333333
[36m[2023-06-25 01:35:26,107][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:35:26,107][129146] Reward + Measures: [[-14.16681334   0.45138893   0.3416667    0.           0.24444444]
[37m[1m [ 69.31917654   0.26131567   0.22474425   0.04423466   0.18028219]
[37m[1m [-14.99459836   0.39583334   0.35833335   0.           0.23888889]
[37m[1m ...
[37m[1m [ 54.15550829   0.24942689   0.26510558   0.15082029   0.15949206]
[37m[1m [-15.63561606   0.45416671   0.29444444   0.           0.24444444]
[37m[1m [ 69.29969408   0.24151538   0.22122419   0.03509805   0.19748946]]
[37m[1m[2023-06-25 01:35:26,108][129146] Max Reward on eval: 337.9869422533433
[37m[1m[2023-06-25 01:35:26,108][129146] Min Reward on eval: -1023.0120943346992
[37m[1m[2023-06-25 01:35:26,108][129146] Mean Reward across all agents: -14.753258024372338
[37m[1m[2023-06-25 01:35:26,108][129146] Average Trajectory Length: 208.25266666666667
[36m[2023-06-25 01:35:26,111][129146] mean_value=-71.91490363255471, max_value=468.0740268996544
[37m[1m[2023-06-25 01:35:26,114][129146] New mean coefficients: [[ 2.2509553   1.8179241   0.6170599  -1.8182473   0.32685488]]
[37m[1m[2023-06-25 01:35:26,115][129146] Moving the mean solution point...
[36m[2023-06-25 01:35:36,026][129146] train() took 9.91 seconds to complete
[36m[2023-06-25 01:35:36,027][129146] FPS: 387505.67
[36m[2023-06-25 01:35:36,029][129146] itr=37, itrs=2000, Progress: 1.85%
[36m[2023-06-25 01:35:47,692][129146] train() took 11.65 seconds to complete
[36m[2023-06-25 01:35:47,693][129146] FPS: 329593.75
[36m[2023-06-25 01:35:52,401][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:35:52,401][129146] Reward + Measures: [[173.2449046    0.22623494   0.21780285   0.07744549   0.15321752]]
[37m[1m[2023-06-25 01:35:52,401][129146] Max Reward on eval: 173.2449045953811
[37m[1m[2023-06-25 01:35:52,402][129146] Min Reward on eval: 173.2449045953811
[37m[1m[2023-06-25 01:35:52,402][129146] Mean Reward across all agents: 173.2449045953811
[37m[1m[2023-06-25 01:35:52,402][129146] Average Trajectory Length: 254.00733333333332
[36m[2023-06-25 01:35:57,988][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:35:57,994][129146] Reward + Measures: [[14.98546073  0.27833056  0.24901378  0.04949599  0.23683952]
[37m[1m [23.27595759  0.35590076  0.26532128  0.01176471  0.16702075]
[37m[1m [81.92263387  0.30332521  0.24321452  0.01131176  0.15377538]
[37m[1m ...
[37m[1m [18.49733334  0.28019062  0.2382777   0.07121168  0.17143647]
[37m[1m [58.86525201  0.24553724  0.20979531  0.08542069  0.19833994]
[37m[1m [32.74047935  0.26963803  0.25918055  0.05837877  0.18707742]]
[37m[1m[2023-06-25 01:35:57,994][129146] Max Reward on eval: 322.04487088702155
[37m[1m[2023-06-25 01:35:57,995][129146] Min Reward on eval: -53.00342896999791
[37m[1m[2023-06-25 01:35:57,995][129146] Mean Reward across all agents: 86.98428365559785
[37m[1m[2023-06-25 01:35:57,995][129146] Average Trajectory Length: 216.73833333333332
[36m[2023-06-25 01:35:57,998][129146] mean_value=-44.19407632318232, max_value=770.8985798454611
[37m[1m[2023-06-25 01:35:58,000][129146] New mean coefficients: [[ 2.6488154   2.0673      0.500418   -1.0822397   0.06262618]]
[37m[1m[2023-06-25 01:35:58,001][129146] Moving the mean solution point...
[36m[2023-06-25 01:36:07,685][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 01:36:07,686][129146] FPS: 396599.27
[36m[2023-06-25 01:36:07,688][129146] itr=38, itrs=2000, Progress: 1.90%
[36m[2023-06-25 01:36:19,155][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 01:36:19,155][129146] FPS: 335282.08
[36m[2023-06-25 01:36:23,938][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:36:23,944][129146] Reward + Measures: [[189.01380027   0.22537768   0.21731013   0.07879712   0.14868236]]
[37m[1m[2023-06-25 01:36:23,944][129146] Max Reward on eval: 189.0138002699976
[37m[1m[2023-06-25 01:36:23,944][129146] Min Reward on eval: 189.0138002699976
[37m[1m[2023-06-25 01:36:23,945][129146] Mean Reward across all agents: 189.0138002699976
[37m[1m[2023-06-25 01:36:23,945][129146] Average Trajectory Length: 273.39799999999997
[36m[2023-06-25 01:36:29,561][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:36:29,562][129146] Reward + Measures: [[197.15840395   0.23388398   0.1782458    0.10248365   0.13485239]
[37m[1m [198.15236159   0.18939698   0.17224155   0.07271901   0.14545077]
[37m[1m [ 78.70189587   0.28151894   0.2326874    0.05321362   0.13091926]
[37m[1m ...
[37m[1m [153.914566     0.26280364   0.21237652   0.04616155   0.15449706]
[37m[1m [ 22.70433741   0.26301572   0.23484063   0.07415409   0.18746126]
[37m[1m [ -2.58229786   0.3158733    0.28764975   0.01525335   0.15384161]]
[37m[1m[2023-06-25 01:36:29,562][129146] Max Reward on eval: 358.17451434251853
[37m[1m[2023-06-25 01:36:29,562][129146] Min Reward on eval: -14.218766263104044
[37m[1m[2023-06-25 01:36:29,563][129146] Mean Reward across all agents: 109.56793551835351
[37m[1m[2023-06-25 01:36:29,563][129146] Average Trajectory Length: 225.8093333333333
[36m[2023-06-25 01:36:29,566][129146] mean_value=-38.79061861420593, max_value=516.6985753915887
[37m[1m[2023-06-25 01:36:29,568][129146] New mean coefficients: [[ 2.8973043   2.7488256   0.51562333 -0.42793477  0.30928767]]
[37m[1m[2023-06-25 01:36:29,569][129146] Moving the mean solution point...
[36m[2023-06-25 01:36:39,287][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 01:36:39,287][129146] FPS: 395223.34
[36m[2023-06-25 01:36:39,289][129146] itr=39, itrs=2000, Progress: 1.95%
[36m[2023-06-25 01:36:50,718][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 01:36:50,719][129146] FPS: 336354.25
[36m[2023-06-25 01:36:55,457][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:36:55,458][129146] Reward + Measures: [[203.39837112   0.21720463   0.21337016   0.08252497   0.14811118]]
[37m[1m[2023-06-25 01:36:55,458][129146] Max Reward on eval: 203.39837112295794
[37m[1m[2023-06-25 01:36:55,458][129146] Min Reward on eval: 203.39837112295794
[37m[1m[2023-06-25 01:36:55,458][129146] Mean Reward across all agents: 203.39837112295794
[37m[1m[2023-06-25 01:36:55,458][129146] Average Trajectory Length: 294.2703333333333
[36m[2023-06-25 01:37:00,977][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:37:00,977][129146] Reward + Measures: [[ 76.37230297   0.30112085   0.26330635   0.09459104   0.14571252]
[37m[1m [246.44008851   0.20544878   0.18541279   0.09053659   0.15999785]
[37m[1m [194.25188189   0.24219058   0.20847726   0.08154849   0.14489996]
[37m[1m ...
[37m[1m [119.79665442   0.24121356   0.2237968    0.09558924   0.15878449]
[37m[1m [ 50.02801469   0.21544866   0.20920217   0.15519474   0.18909876]
[37m[1m [ 42.54143294   0.29510522   0.26998112   0.10420656   0.18712094]]
[37m[1m[2023-06-25 01:37:00,978][129146] Max Reward on eval: 356.9665859009954
[37m[1m[2023-06-25 01:37:00,978][129146] Min Reward on eval: -294.4880237168254
[37m[1m[2023-06-25 01:37:00,978][129146] Mean Reward across all agents: 105.59145359837083
[37m[1m[2023-06-25 01:37:00,978][129146] Average Trajectory Length: 326.62533333333334
[36m[2023-06-25 01:37:00,981][129146] mean_value=-46.94329102336686, max_value=485.85977592220775
[37m[1m[2023-06-25 01:37:00,984][129146] New mean coefficients: [[3.2055423  2.862122   0.51542073 0.5322212  0.3210572 ]]
[37m[1m[2023-06-25 01:37:00,985][129146] Moving the mean solution point...
[36m[2023-06-25 01:37:10,625][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 01:37:10,626][129146] FPS: 398388.77
[36m[2023-06-25 01:37:10,628][129146] itr=40, itrs=2000, Progress: 2.00%
[37m[1m[2023-06-25 01:37:12,757][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000020
[36m[2023-06-25 01:37:24,423][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 01:37:24,423][129146] FPS: 334931.60
[36m[2023-06-25 01:37:29,163][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:37:29,163][129146] Reward + Measures: [[215.9894321    0.21595336   0.21181893   0.07996489   0.14596441]]
[37m[1m[2023-06-25 01:37:29,164][129146] Max Reward on eval: 215.98943210163517
[37m[1m[2023-06-25 01:37:29,164][129146] Min Reward on eval: 215.98943210163517
[37m[1m[2023-06-25 01:37:29,164][129146] Mean Reward across all agents: 215.98943210163517
[37m[1m[2023-06-25 01:37:29,164][129146] Average Trajectory Length: 302.67766666666665
[36m[2023-06-25 01:37:34,557][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:37:34,615][129146] Reward + Measures: [[86.79787917  0.32259151  0.27703404  0.0335104   0.16755572]
[37m[1m [78.26702865  0.2797423   0.27497214  0.05994692  0.16233306]
[37m[1m [40.49247074  0.30628774  0.25377032  0.07016309  0.17281769]
[37m[1m ...
[37m[1m [63.55712794  0.26187149  0.2478029   0.09692712  0.21041639]
[37m[1m [25.66444112  0.3521938   0.28445825  0.01900641  0.1651345 ]
[37m[1m [83.81645598  0.2673597   0.28641436  0.08938935  0.15373038]]
[37m[1m[2023-06-25 01:37:34,615][129146] Max Reward on eval: 401.6314553881995
[37m[1m[2023-06-25 01:37:34,615][129146] Min Reward on eval: -13.701257508678827
[37m[1m[2023-06-25 01:37:34,616][129146] Mean Reward across all agents: 109.86872974195799
[37m[1m[2023-06-25 01:37:34,616][129146] Average Trajectory Length: 222.16733333333332
[36m[2023-06-25 01:37:34,618][129146] mean_value=-51.541611186539086, max_value=143.72993329799152
[37m[1m[2023-06-25 01:37:34,620][129146] New mean coefficients: [[2.7048562  2.2873306  0.64743185 0.74134827 0.03789485]]
[37m[1m[2023-06-25 01:37:34,621][129146] Moving the mean solution point...
[36m[2023-06-25 01:37:44,449][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 01:37:44,449][129146] FPS: 390779.19
[36m[2023-06-25 01:37:44,452][129146] itr=41, itrs=2000, Progress: 2.05%
[36m[2023-06-25 01:37:56,007][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 01:37:56,007][129146] FPS: 332707.83
[36m[2023-06-25 01:38:00,816][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:38:00,816][129146] Reward + Measures: [[224.17044257   0.21139632   0.21136792   0.08526512   0.14494285]]
[37m[1m[2023-06-25 01:38:00,817][129146] Max Reward on eval: 224.1704425732565
[37m[1m[2023-06-25 01:38:00,817][129146] Min Reward on eval: 224.1704425732565
[37m[1m[2023-06-25 01:38:00,817][129146] Mean Reward across all agents: 224.1704425732565
[37m[1m[2023-06-25 01:38:00,817][129146] Average Trajectory Length: 314.633
[36m[2023-06-25 01:38:06,249][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:38:06,249][129146] Reward + Measures: [[ 24.97006692   0.30273205   0.27265605   0.08978816   0.17887516]
[37m[1m [115.04446074   0.27582276   0.2367475    0.07577542   0.15140429]
[37m[1m [150.35578028   0.24990046   0.23449531   0.06448533   0.13907769]
[37m[1m ...
[37m[1m [202.82901866   0.24858046   0.22937797   0.08133043   0.13969047]
[37m[1m [190.62407655   0.23491816   0.20518222   0.05259869   0.16552672]
[37m[1m [ 61.92475159   0.29369542   0.22981802   0.05241434   0.1551235 ]]
[37m[1m[2023-06-25 01:38:06,249][129146] Max Reward on eval: 384.7531060382782
[37m[1m[2023-06-25 01:38:06,250][129146] Min Reward on eval: -25.160514234984294
[37m[1m[2023-06-25 01:38:06,250][129146] Mean Reward across all agents: 154.52718904239867
[37m[1m[2023-06-25 01:38:06,250][129146] Average Trajectory Length: 272.7803333333333
[36m[2023-06-25 01:38:06,253][129146] mean_value=-33.44489096510429, max_value=862.8149781358865
[37m[1m[2023-06-25 01:38:06,255][129146] New mean coefficients: [[2.6885145  1.1421117  0.83281153 1.0117257  0.03243902]]
[37m[1m[2023-06-25 01:38:06,256][129146] Moving the mean solution point...
[36m[2023-06-25 01:38:16,061][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 01:38:16,061][129146] FPS: 391711.15
[36m[2023-06-25 01:38:16,063][129146] itr=42, itrs=2000, Progress: 2.10%
[36m[2023-06-25 01:38:27,497][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 01:38:27,497][129146] FPS: 336221.39
[36m[2023-06-25 01:38:32,260][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:38:32,260][129146] Reward + Measures: [[243.8347303    0.20660675   0.21090642   0.08161181   0.13903022]]
[37m[1m[2023-06-25 01:38:32,260][129146] Max Reward on eval: 243.83473029830725
[37m[1m[2023-06-25 01:38:32,260][129146] Min Reward on eval: 243.83473029830725
[37m[1m[2023-06-25 01:38:32,261][129146] Mean Reward across all agents: 243.83473029830725
[37m[1m[2023-06-25 01:38:32,261][129146] Average Trajectory Length: 336.9823333333333
[36m[2023-06-25 01:38:37,724][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:38:37,724][129146] Reward + Measures: [[-91.96028473   0.27983889   0.31889066   0.18930219   0.20355923]
[37m[1m [ 35.76146939   0.21215641   0.1935143    0.05289853   0.21545248]
[37m[1m [ 32.22275963   0.30102953   0.31940418   0.09076373   0.14920674]
[37m[1m ...
[37m[1m [ 62.13112006   0.31287247   0.27274331   0.07571112   0.19153421]
[37m[1m [ 97.25522878   0.26795825   0.26429421   0.06201889   0.15671669]
[37m[1m [115.83112247   0.19041635   0.16151758   0.07976877   0.18146563]]
[37m[1m[2023-06-25 01:38:37,725][129146] Max Reward on eval: 381.09629082756584
[37m[1m[2023-06-25 01:38:37,725][129146] Min Reward on eval: -1092.4035496580414
[37m[1m[2023-06-25 01:38:37,725][129146] Mean Reward across all agents: 39.69092419288526
[37m[1m[2023-06-25 01:38:37,725][129146] Average Trajectory Length: 265.34366666666665
[36m[2023-06-25 01:38:37,729][129146] mean_value=-80.21553071715621, max_value=758.4496232657848
[37m[1m[2023-06-25 01:38:37,731][129146] New mean coefficients: [[2.9461665  0.86459136 1.0187322  1.6080067  0.07703364]]
[37m[1m[2023-06-25 01:38:37,733][129146] Moving the mean solution point...
[36m[2023-06-25 01:38:47,533][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 01:38:47,533][129146] FPS: 391882.77
[36m[2023-06-25 01:38:47,536][129146] itr=43, itrs=2000, Progress: 2.15%
[36m[2023-06-25 01:38:58,973][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 01:38:58,973][129146] FPS: 336161.91
[36m[2023-06-25 01:39:03,729][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:39:03,729][129146] Reward + Measures: [[250.63836432   0.20455298   0.21322872   0.08007495   0.13915628]]
[37m[1m[2023-06-25 01:39:03,730][129146] Max Reward on eval: 250.63836431840568
[37m[1m[2023-06-25 01:39:03,730][129146] Min Reward on eval: 250.63836431840568
[37m[1m[2023-06-25 01:39:03,730][129146] Mean Reward across all agents: 250.63836431840568
[37m[1m[2023-06-25 01:39:03,730][129146] Average Trajectory Length: 337.66133333333335
[36m[2023-06-25 01:39:09,351][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:39:09,352][129146] Reward + Measures: [[248.17110334   0.21797498   0.18270883   0.08082369   0.19451396]
[37m[1m [171.72592904   0.25164565   0.23488593   0.03630019   0.1326077 ]
[37m[1m [245.01467062   0.23786926   0.2030199    0.04469394   0.11455057]
[37m[1m ...
[37m[1m [141.32603534   0.28047886   0.2137322    0.06854413   0.15724187]
[37m[1m [101.17956524   0.25565702   0.21916588   0.06752563   0.18735351]
[37m[1m [227.8109444    0.22282489   0.18591586   0.07705943   0.15422155]]
[37m[1m[2023-06-25 01:39:09,352][129146] Max Reward on eval: 478.23553835588973
[37m[1m[2023-06-25 01:39:09,352][129146] Min Reward on eval: 4.200357125326991
[37m[1m[2023-06-25 01:39:09,353][129146] Mean Reward across all agents: 184.99441863018797
[37m[1m[2023-06-25 01:39:09,353][129146] Average Trajectory Length: 305.14366666666666
[36m[2023-06-25 01:39:09,356][129146] mean_value=-42.28643035569034, max_value=758.4325407953957
[37m[1m[2023-06-25 01:39:09,358][129146] New mean coefficients: [[3.0521362  0.78903425 1.3299848  1.9132963  0.37224916]]
[37m[1m[2023-06-25 01:39:09,359][129146] Moving the mean solution point...
[36m[2023-06-25 01:39:19,148][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 01:39:19,148][129146] FPS: 392361.19
[36m[2023-06-25 01:39:19,150][129146] itr=44, itrs=2000, Progress: 2.20%
[36m[2023-06-25 01:39:30,658][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 01:39:30,658][129146] FPS: 334086.24
[36m[2023-06-25 01:39:35,500][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:39:35,500][129146] Reward + Measures: [[281.83674868   0.19812593   0.20656493   0.08476436   0.13626377]]
[37m[1m[2023-06-25 01:39:35,500][129146] Max Reward on eval: 281.83674868041686
[37m[1m[2023-06-25 01:39:35,501][129146] Min Reward on eval: 281.83674868041686
[37m[1m[2023-06-25 01:39:35,501][129146] Mean Reward across all agents: 281.83674868041686
[37m[1m[2023-06-25 01:39:35,501][129146] Average Trajectory Length: 378.2846666666667
[36m[2023-06-25 01:39:41,078][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:39:41,078][129146] Reward + Measures: [[124.13402544   0.23747404   0.20080689   0.05697389   0.15797432]
[37m[1m [ 63.01289628   0.18431459   0.22777806   0.10149524   0.25141254]
[37m[1m [317.86545578   0.18805902   0.17816733   0.09440022   0.11384292]
[37m[1m ...
[37m[1m [ 50.38291638   0.24397472   0.24837966   0.07713141   0.19194995]
[37m[1m [ 75.67840178   0.2166049    0.2128716    0.1019668    0.18026547]
[37m[1m [ 49.67272703   0.22741042   0.24036668   0.10509688   0.18966721]]
[37m[1m[2023-06-25 01:39:41,078][129146] Max Reward on eval: 445.65177407863666
[37m[1m[2023-06-25 01:39:41,079][129146] Min Reward on eval: -353.2105466757901
[37m[1m[2023-06-25 01:39:41,079][129146] Mean Reward across all agents: 52.88679749496692
[37m[1m[2023-06-25 01:39:41,079][129146] Average Trajectory Length: 255.79466666666667
[36m[2023-06-25 01:39:41,081][129146] mean_value=-158.57005807768238, max_value=707.6820598589301
[37m[1m[2023-06-25 01:39:41,084][129146] New mean coefficients: [[2.7120595  0.76795566 1.2271602  0.47346628 0.5831591 ]]
[37m[1m[2023-06-25 01:39:41,085][129146] Moving the mean solution point...
[36m[2023-06-25 01:39:50,959][129146] train() took 9.87 seconds to complete
[36m[2023-06-25 01:39:50,960][129146] FPS: 388957.48
[36m[2023-06-25 01:39:50,962][129146] itr=45, itrs=2000, Progress: 2.25%
[36m[2023-06-25 01:40:02,741][129146] train() took 11.77 seconds to complete
[36m[2023-06-25 01:40:02,742][129146] FPS: 326354.66
[36m[2023-06-25 01:40:07,695][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:40:07,700][129146] Reward + Measures: [[304.40175051   0.19270669   0.20764071   0.08797269   0.13356322]]
[37m[1m[2023-06-25 01:40:07,701][129146] Max Reward on eval: 304.4017505054977
[37m[1m[2023-06-25 01:40:07,701][129146] Min Reward on eval: 304.4017505054977
[37m[1m[2023-06-25 01:40:07,701][129146] Mean Reward across all agents: 304.4017505054977
[37m[1m[2023-06-25 01:40:07,701][129146] Average Trajectory Length: 407.90666666666664
[36m[2023-06-25 01:40:13,255][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:40:13,255][129146] Reward + Measures: [[215.00558789   0.21913807   0.22819069   0.11041905   0.14538072]
[37m[1m [ 79.32878946   0.22951399   0.21551895   0.0826812    0.20970257]
[37m[1m [ 72.66381617   0.24110961   0.21605599   0.06255676   0.23207112]
[37m[1m ...
[37m[1m [-70.34410153   0.25727808   0.24480967   0.12106526   0.20046006]
[37m[1m [-31.28114892   0.3617011    0.26294407   0.29228354   0.20381685]
[37m[1m [-57.92864157   0.27272561   0.28486216   0.16788684   0.2093976 ]]
[37m[1m[2023-06-25 01:40:13,256][129146] Max Reward on eval: 492.9887478417251
[37m[1m[2023-06-25 01:40:13,256][129146] Min Reward on eval: -334.7070758337621
[37m[1m[2023-06-25 01:40:13,256][129146] Mean Reward across all agents: 81.29771173165284
[37m[1m[2023-06-25 01:40:13,256][129146] Average Trajectory Length: 263.90633333333335
[36m[2023-06-25 01:40:13,259][129146] mean_value=-103.57747302812011, max_value=992.9887478417252
[37m[1m[2023-06-25 01:40:13,261][129146] New mean coefficients: [[2.5885954 1.2461044 1.0179919 0.2141344 0.5416455]]
[37m[1m[2023-06-25 01:40:13,262][129146] Moving the mean solution point...
[36m[2023-06-25 01:40:23,168][129146] train() took 9.90 seconds to complete
[36m[2023-06-25 01:40:23,169][129146] FPS: 387696.89
[36m[2023-06-25 01:40:23,171][129146] itr=46, itrs=2000, Progress: 2.30%
[36m[2023-06-25 01:40:34,746][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 01:40:34,747][129146] FPS: 332102.88
[36m[2023-06-25 01:40:39,656][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:40:39,657][129146] Reward + Measures: [[309.41145616   0.19704211   0.21052909   0.08661393   0.13189164]]
[37m[1m[2023-06-25 01:40:39,657][129146] Max Reward on eval: 309.41145615580757
[37m[1m[2023-06-25 01:40:39,657][129146] Min Reward on eval: 309.41145615580757
[37m[1m[2023-06-25 01:40:39,657][129146] Mean Reward across all agents: 309.41145615580757
[37m[1m[2023-06-25 01:40:39,657][129146] Average Trajectory Length: 407.58866666666665
[36m[2023-06-25 01:40:45,148][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:40:45,148][129146] Reward + Measures: [[169.03419199   0.21111861   0.22424813   0.08896493   0.15149732]
[37m[1m [ 35.06824298   0.31104892   0.25816616   0.03597167   0.16829024]
[37m[1m [225.1710103    0.22408228   0.19434276   0.05087385   0.14802526]
[37m[1m ...
[37m[1m [180.36086839   0.25691089   0.18990135   0.09547665   0.17926188]
[37m[1m [268.77100792   0.19556622   0.17440374   0.07817083   0.15622   ]
[37m[1m [457.52072579   0.15871646   0.19054244   0.06442858   0.10103073]]
[37m[1m[2023-06-25 01:40:45,148][129146] Max Reward on eval: 513.3335987025406
[37m[1m[2023-06-25 01:40:45,149][129146] Min Reward on eval: 5.6613926370861005
[37m[1m[2023-06-25 01:40:45,149][129146] Mean Reward across all agents: 230.9822108683417
[37m[1m[2023-06-25 01:40:45,149][129146] Average Trajectory Length: 381.1863333333333
[36m[2023-06-25 01:40:45,152][129146] mean_value=-29.66440899283618, max_value=914.0347239183682
[37m[1m[2023-06-25 01:40:45,155][129146] New mean coefficients: [[2.1759484  1.6551101  0.75072783 0.06259936 0.59895325]]
[37m[1m[2023-06-25 01:40:45,156][129146] Moving the mean solution point...
[36m[2023-06-25 01:40:54,878][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 01:40:54,878][129146] FPS: 395059.29
[36m[2023-06-25 01:40:54,881][129146] itr=47, itrs=2000, Progress: 2.35%
[36m[2023-06-25 01:41:06,383][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 01:41:06,383][129146] FPS: 334228.60
[36m[2023-06-25 01:41:11,259][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:41:11,265][129146] Reward + Measures: [[337.60483746   0.19173433   0.21502437   0.08602039   0.12663886]]
[37m[1m[2023-06-25 01:41:11,265][129146] Max Reward on eval: 337.6048374628165
[37m[1m[2023-06-25 01:41:11,265][129146] Min Reward on eval: 337.6048374628165
[37m[1m[2023-06-25 01:41:11,265][129146] Mean Reward across all agents: 337.6048374628165
[37m[1m[2023-06-25 01:41:11,265][129146] Average Trajectory Length: 436.145
[36m[2023-06-25 01:41:16,747][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:41:16,748][129146] Reward + Measures: [[381.81499699   0.19385348   0.15795635   0.09218647   0.15191807]
[37m[1m [  4.78132766   0.21243201   0.25703809   0.08416535   0.21473005]
[37m[1m [  7.13302569   0.17829041   0.16785307   0.1332017    0.19659445]
[37m[1m ...
[37m[1m [254.81968365   0.18053004   0.17567135   0.11210507   0.14031516]
[37m[1m [138.64779698   0.24409543   0.19339602   0.09316447   0.18495527]
[37m[1m [345.75116938   0.18609433   0.1547365    0.04580407   0.10002428]]
[37m[1m[2023-06-25 01:41:16,748][129146] Max Reward on eval: 515.8336214891053
[37m[1m[2023-06-25 01:41:16,748][129146] Min Reward on eval: -1032.741349463025
[37m[1m[2023-06-25 01:41:16,749][129146] Mean Reward across all agents: 83.29444868700867
[37m[1m[2023-06-25 01:41:16,749][129146] Average Trajectory Length: 447.33566666666667
[36m[2023-06-25 01:41:16,753][129146] mean_value=-78.02918420331078, max_value=856.2261464125831
[37m[1m[2023-06-25 01:41:16,755][129146] New mean coefficients: [[ 2.1604893   1.5840645   0.4732832  -0.8568709   0.31927758]]
[37m[1m[2023-06-25 01:41:16,757][129146] Moving the mean solution point...
[36m[2023-06-25 01:41:26,550][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 01:41:26,550][129146] FPS: 392162.90
[36m[2023-06-25 01:41:26,553][129146] itr=48, itrs=2000, Progress: 2.40%
[36m[2023-06-25 01:41:37,948][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 01:41:37,948][129146] FPS: 337384.90
[36m[2023-06-25 01:41:42,760][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:41:42,761][129146] Reward + Measures: [[114.47685672   0.22530177   0.1688102    0.11009093   0.15040772]]
[37m[1m[2023-06-25 01:41:42,761][129146] Max Reward on eval: 114.4768567216515
[37m[1m[2023-06-25 01:41:42,761][129146] Min Reward on eval: 114.4768567216515
[37m[1m[2023-06-25 01:41:42,761][129146] Mean Reward across all agents: 114.4768567216515
[37m[1m[2023-06-25 01:41:42,762][129146] Average Trajectory Length: 479.48366666666664
[36m[2023-06-25 01:41:48,416][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:41:48,416][129146] Reward + Measures: [[  51.7388883     0.17011026    0.16369493    0.11782192    0.15354173]
[37m[1m [ -15.86747094    0.21515523    0.17727397    0.11971048    0.11804383]
[37m[1m [-110.26306812    0.24587403    0.20085119    0.10271265    0.16091397]
[37m[1m ...
[37m[1m [   2.65698441    0.26490355    0.19549109    0.0749782     0.1627323 ]
[37m[1m [  23.47826564    0.23585601    0.17537117    0.1410069     0.19965063]
[37m[1m [  68.32707931    0.23738794    0.17238161    0.10262521    0.1888134 ]]
[37m[1m[2023-06-25 01:41:48,416][129146] Max Reward on eval: 309.29043042297707
[37m[1m[2023-06-25 01:41:48,417][129146] Min Reward on eval: -173.10637651545693
[37m[1m[2023-06-25 01:41:48,417][129146] Mean Reward across all agents: 40.10699452300359
[37m[1m[2023-06-25 01:41:48,417][129146] Average Trajectory Length: 411.506
[36m[2023-06-25 01:41:48,419][129146] mean_value=-201.641189589736, max_value=565.5021788951533
[37m[1m[2023-06-25 01:41:48,422][129146] New mean coefficients: [[ 1.5135219  1.9937973  0.5104275 -0.6362477  0.5509975]]
[37m[1m[2023-06-25 01:41:48,423][129146] Moving the mean solution point...
[36m[2023-06-25 01:41:58,195][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 01:41:58,196][129146] FPS: 392996.24
[36m[2023-06-25 01:41:58,198][129146] itr=49, itrs=2000, Progress: 2.45%
[36m[2023-06-25 01:42:09,625][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 01:42:09,626][129146] FPS: 336429.85
[36m[2023-06-25 01:42:14,448][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:42:14,448][129146] Reward + Measures: [[149.49008039   0.21098588   0.16021492   0.10911218   0.14056526]]
[37m[1m[2023-06-25 01:42:14,449][129146] Max Reward on eval: 149.49008039471838
[37m[1m[2023-06-25 01:42:14,449][129146] Min Reward on eval: 149.49008039471838
[37m[1m[2023-06-25 01:42:14,449][129146] Mean Reward across all agents: 149.49008039471838
[37m[1m[2023-06-25 01:42:14,449][129146] Average Trajectory Length: 528.9653333333333
[36m[2023-06-25 01:42:20,000][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:42:20,000][129146] Reward + Measures: [[ 47.06555178   0.28194672   0.20065466   0.12553692   0.15556167]
[37m[1m [ 82.79425367   0.25738537   0.18709922   0.17418557   0.14557563]
[37m[1m [123.69671509   0.20787422   0.15346412   0.11970877   0.11730614]
[37m[1m ...
[37m[1m [ 73.28364543   0.25304452   0.17899056   0.14771645   0.14897838]
[37m[1m [225.23515719   0.18522525   0.13894176   0.09165627   0.14126182]
[37m[1m [203.45617034   0.19092663   0.13036568   0.09290325   0.13580854]]
[37m[1m[2023-06-25 01:42:20,000][129146] Max Reward on eval: 336.1561501125805
[37m[1m[2023-06-25 01:42:20,001][129146] Min Reward on eval: -16.581968543230325
[37m[1m[2023-06-25 01:42:20,001][129146] Mean Reward across all agents: 145.0772130942457
[37m[1m[2023-06-25 01:42:20,001][129146] Average Trajectory Length: 501.23866666666663
[36m[2023-06-25 01:42:20,003][129146] mean_value=-135.81309181258123, max_value=293.368884729824
[37m[1m[2023-06-25 01:42:20,006][129146] New mean coefficients: [[1.720242   2.1626985  0.19017681 0.13237858 0.52133733]]
[37m[1m[2023-06-25 01:42:20,007][129146] Moving the mean solution point...
[36m[2023-06-25 01:42:29,838][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 01:42:29,838][129146] FPS: 390646.85
[36m[2023-06-25 01:42:29,840][129146] itr=50, itrs=2000, Progress: 2.50%
[37m[1m[2023-06-25 01:42:31,978][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000030
[36m[2023-06-25 01:42:43,771][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 01:42:43,771][129146] FPS: 332847.71
[36m[2023-06-25 01:42:48,455][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:42:48,455][129146] Reward + Measures: [[175.61724544   0.20966141   0.15617776   0.11292312   0.13982353]]
[37m[1m[2023-06-25 01:42:48,455][129146] Max Reward on eval: 175.61724543690138
[37m[1m[2023-06-25 01:42:48,456][129146] Min Reward on eval: 175.61724543690138
[37m[1m[2023-06-25 01:42:48,456][129146] Mean Reward across all agents: 175.61724543690138
[37m[1m[2023-06-25 01:42:48,456][129146] Average Trajectory Length: 560.7189999999999
[36m[2023-06-25 01:42:53,882][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:42:53,882][129146] Reward + Measures: [[231.17121585   0.202713     0.14339468   0.09886063   0.13624488]
[37m[1m [193.69892      0.20477438   0.18329829   0.1283032    0.13451093]
[37m[1m [186.16543792   0.20174964   0.13480635   0.11304088   0.12173688]
[37m[1m ...
[37m[1m [232.29785305   0.14737228   0.09378215   0.09777319   0.09744891]
[37m[1m [201.65788965   0.22557831   0.16122913   0.12126674   0.129732  ]
[37m[1m [100.64246373   0.257761     0.23207946   0.10702467   0.16296279]]
[37m[1m[2023-06-25 01:42:53,883][129146] Max Reward on eval: 368.45709724371557
[37m[1m[2023-06-25 01:42:53,883][129146] Min Reward on eval: -36.72258936909493
[37m[1m[2023-06-25 01:42:53,883][129146] Mean Reward across all agents: 155.17722766110174
[37m[1m[2023-06-25 01:42:53,883][129146] Average Trajectory Length: 535.0086666666666
[36m[2023-06-25 01:42:53,885][129146] mean_value=-115.57686637583645, max_value=744.1259294192539
[37m[1m[2023-06-25 01:42:53,888][129146] New mean coefficients: [[ 1.9718565   1.935434   -0.13492197 -0.03038257  0.71510684]]
[37m[1m[2023-06-25 01:42:53,889][129146] Moving the mean solution point...
[36m[2023-06-25 01:43:03,659][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 01:43:03,659][129146] FPS: 393147.06
[36m[2023-06-25 01:43:03,661][129146] itr=51, itrs=2000, Progress: 2.55%
[36m[2023-06-25 01:43:15,325][129146] train() took 11.65 seconds to complete
[36m[2023-06-25 01:43:15,326][129146] FPS: 329574.91
[36m[2023-06-25 01:43:20,128][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:43:20,129][129146] Reward + Measures: [[195.56174403   0.2050344    0.15446202   0.11268342   0.133191  ]]
[37m[1m[2023-06-25 01:43:20,129][129146] Max Reward on eval: 195.56174403071722
[37m[1m[2023-06-25 01:43:20,129][129146] Min Reward on eval: 195.56174403071722
[37m[1m[2023-06-25 01:43:20,130][129146] Mean Reward across all agents: 195.56174403071722
[37m[1m[2023-06-25 01:43:20,130][129146] Average Trajectory Length: 586.0219999999999
[36m[2023-06-25 01:43:25,619][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:43:25,619][129146] Reward + Measures: [[ 59.50836257   0.27717501   0.19703865   0.07406902   0.18039893]
[37m[1m [265.06375008   0.2127652    0.1465138    0.10598367   0.12746277]
[37m[1m [226.43221194   0.18003674   0.13803923   0.08530196   0.1010617 ]
[37m[1m ...
[37m[1m [213.55021843   0.24081254   0.16561954   0.07976931   0.16889346]
[37m[1m [321.30424911   0.1624442    0.11541219   0.10061687   0.13271172]
[37m[1m [182.48843298   0.21426557   0.14847921   0.07960616   0.15369645]]
[37m[1m[2023-06-25 01:43:25,619][129146] Max Reward on eval: 411.34774180236565
[37m[1m[2023-06-25 01:43:25,620][129146] Min Reward on eval: -8.261170034753741
[37m[1m[2023-06-25 01:43:25,620][129146] Mean Reward across all agents: 178.05043015190338
[37m[1m[2023-06-25 01:43:25,620][129146] Average Trajectory Length: 571.2543333333333
[36m[2023-06-25 01:43:25,622][129146] mean_value=-108.39959930036765, max_value=623.1434084539435
[37m[1m[2023-06-25 01:43:25,625][129146] New mean coefficients: [[2.0198445  1.5562181  0.50062954 0.00992259 0.6721873 ]]
[37m[1m[2023-06-25 01:43:25,626][129146] Moving the mean solution point...
[36m[2023-06-25 01:43:35,347][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 01:43:35,347][129146] FPS: 395093.02
[36m[2023-06-25 01:43:35,349][129146] itr=52, itrs=2000, Progress: 2.60%
[36m[2023-06-25 01:43:46,815][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 01:43:46,815][129146] FPS: 335324.39
[36m[2023-06-25 01:43:51,699][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:43:51,699][129146] Reward + Measures: [[223.5089482    0.20653714   0.15474696   0.11024053   0.13072892]]
[37m[1m[2023-06-25 01:43:51,699][129146] Max Reward on eval: 223.50894820181026
[37m[1m[2023-06-25 01:43:51,700][129146] Min Reward on eval: 223.50894820181026
[37m[1m[2023-06-25 01:43:51,700][129146] Mean Reward across all agents: 223.50894820181026
[37m[1m[2023-06-25 01:43:51,700][129146] Average Trajectory Length: 595.5079999999999
[36m[2023-06-25 01:43:57,328][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:43:57,329][129146] Reward + Measures: [[208.12590042   0.2157443    0.14492121   0.11858469   0.13762332]
[37m[1m [397.38286184   0.12343333   0.10264444   0.08216666   0.08158889]
[37m[1m [263.26790538   0.18918562   0.1333883    0.11425686   0.11166479]
[37m[1m ...
[37m[1m [196.79020037   0.16985455   0.12488174   0.08093195   0.14690435]
[37m[1m [275.77623375   0.19876291   0.13536349   0.0779677    0.11731458]
[37m[1m [137.00367544   0.21066594   0.14607464   0.11783922   0.13673429]]
[37m[1m[2023-06-25 01:43:57,329][129146] Max Reward on eval: 432.06722573310833
[37m[1m[2023-06-25 01:43:57,329][129146] Min Reward on eval: -0.8440677627455443
[37m[1m[2023-06-25 01:43:57,329][129146] Mean Reward across all agents: 189.79822332451644
[37m[1m[2023-06-25 01:43:57,330][129146] Average Trajectory Length: 632.0533333333333
[36m[2023-06-25 01:43:57,332][129146] mean_value=-109.03796620799471, max_value=178.14547065721638
[37m[1m[2023-06-25 01:43:57,334][129146] New mean coefficients: [[1.9591037  1.3209502  0.44303757 0.28604555 0.88494337]]
[37m[1m[2023-06-25 01:43:57,335][129146] Moving the mean solution point...
[36m[2023-06-25 01:44:07,159][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 01:44:07,159][129146] FPS: 390978.06
[36m[2023-06-25 01:44:07,161][129146] itr=53, itrs=2000, Progress: 2.65%
[36m[2023-06-25 01:44:18,701][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 01:44:18,701][129146] FPS: 333135.66
[36m[2023-06-25 01:44:23,592][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:44:23,592][129146] Reward + Measures: [[254.46809196   0.19793257   0.14837795   0.11101502   0.12686196]]
[37m[1m[2023-06-25 01:44:23,592][129146] Max Reward on eval: 254.46809196132244
[37m[1m[2023-06-25 01:44:23,593][129146] Min Reward on eval: 254.46809196132244
[37m[1m[2023-06-25 01:44:23,593][129146] Mean Reward across all agents: 254.46809196132244
[37m[1m[2023-06-25 01:44:23,593][129146] Average Trajectory Length: 628.638
[36m[2023-06-25 01:44:29,183][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:44:29,184][129146] Reward + Measures: [[ 67.43000956   0.20017807   0.124381     0.17530495   0.17441623]
[37m[1m [234.96658558   0.21556588   0.16719249   0.15932183   0.13442656]
[37m[1m [125.74701075   0.17952168   0.13095771   0.14017187   0.13833569]
[37m[1m ...
[37m[1m [150.91288029   0.18917356   0.13612759   0.17675869   0.19827323]
[37m[1m [ -8.19276532   0.1426665    0.16059624   0.17947194   0.13135664]
[37m[1m [ 71.72458441   0.21677576   0.13852437   0.16072641   0.16116105]]
[37m[1m[2023-06-25 01:44:29,184][129146] Max Reward on eval: 435.10477764339305
[37m[1m[2023-06-25 01:44:29,184][129146] Min Reward on eval: -342.3637806429266
[37m[1m[2023-06-25 01:44:29,184][129146] Mean Reward across all agents: 115.86517215163676
[37m[1m[2023-06-25 01:44:29,185][129146] Average Trajectory Length: 649.2836666666666
[36m[2023-06-25 01:44:29,187][129146] mean_value=-111.28578531676122, max_value=935.104777643393
[37m[1m[2023-06-25 01:44:29,190][129146] New mean coefficients: [[1.7990503  1.3942814  0.34740204 0.533247   0.6902238 ]]
[37m[1m[2023-06-25 01:44:29,191][129146] Moving the mean solution point...
[36m[2023-06-25 01:44:38,991][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 01:44:38,991][129146] FPS: 391930.19
[36m[2023-06-25 01:44:38,993][129146] itr=54, itrs=2000, Progress: 2.70%
[36m[2023-06-25 01:44:50,549][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 01:44:50,549][129146] FPS: 332688.93
[36m[2023-06-25 01:44:55,394][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:44:55,395][129146] Reward + Measures: [[266.05400012   0.19946277   0.14925152   0.11316843   0.12752941]]
[37m[1m[2023-06-25 01:44:55,395][129146] Max Reward on eval: 266.0540001222305
[37m[1m[2023-06-25 01:44:55,395][129146] Min Reward on eval: 266.0540001222305
[37m[1m[2023-06-25 01:44:55,395][129146] Mean Reward across all agents: 266.0540001222305
[37m[1m[2023-06-25 01:44:55,396][129146] Average Trajectory Length: 632.776
[36m[2023-06-25 01:45:00,966][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:45:00,967][129146] Reward + Measures: [[161.40349293   0.24324325   0.17873497   0.09976896   0.16090217]
[37m[1m [165.36858202   0.22634879   0.16716237   0.1464072    0.1514985 ]
[37m[1m [199.48067775   0.17697112   0.1270662    0.1337353    0.11816619]
[37m[1m ...
[37m[1m [187.7561845    0.22832918   0.16315499   0.14582168   0.18757112]
[37m[1m [123.46531796   0.24920793   0.18960345   0.13521115   0.16519456]
[37m[1m [146.32995265   0.24279056   0.19171922   0.13812447   0.14279109]]
[37m[1m[2023-06-25 01:45:00,967][129146] Max Reward on eval: 457.36178647160415
[37m[1m[2023-06-25 01:45:00,967][129146] Min Reward on eval: 9.891595227381913
[37m[1m[2023-06-25 01:45:00,967][129146] Mean Reward across all agents: 224.57375871701907
[37m[1m[2023-06-25 01:45:00,968][129146] Average Trajectory Length: 632.569
[36m[2023-06-25 01:45:00,970][129146] mean_value=-68.52745862729913, max_value=480.62220543457727
[37m[1m[2023-06-25 01:45:00,972][129146] New mean coefficients: [[1.862374   1.0808476  0.50371933 0.54888004 0.7547285 ]]
[37m[1m[2023-06-25 01:45:00,974][129146] Moving the mean solution point...
[36m[2023-06-25 01:45:10,847][129146] train() took 9.87 seconds to complete
[36m[2023-06-25 01:45:10,847][129146] FPS: 389005.54
[36m[2023-06-25 01:45:10,850][129146] itr=55, itrs=2000, Progress: 2.75%
[36m[2023-06-25 01:45:22,379][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 01:45:22,379][129146] FPS: 333434.20
[36m[2023-06-25 01:45:27,146][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:45:27,146][129146] Reward + Measures: [[289.49072359   0.19553037   0.14987135   0.11384611   0.12595099]]
[37m[1m[2023-06-25 01:45:27,147][129146] Max Reward on eval: 289.49072358821826
[37m[1m[2023-06-25 01:45:27,147][129146] Min Reward on eval: 289.49072358821826
[37m[1m[2023-06-25 01:45:27,147][129146] Mean Reward across all agents: 289.49072358821826
[37m[1m[2023-06-25 01:45:27,147][129146] Average Trajectory Length: 644.8173333333333
[36m[2023-06-25 01:45:32,704][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:45:32,705][129146] Reward + Measures: [[289.57856777   0.18608871   0.12346589   0.09980812   0.11988457]
[37m[1m [405.5575979    0.13305315   0.09322477   0.0734246    0.10927626]
[37m[1m [340.36174941   0.17711018   0.13328575   0.12112005   0.14370255]
[37m[1m ...
[37m[1m [408.11371293   0.13340476   0.11499047   0.07396191   0.07571428]
[37m[1m [319.92075213   0.15753238   0.12042232   0.10401093   0.10869642]
[37m[1m [116.77100874   0.27522385   0.19646144   0.0896631    0.13094795]]
[37m[1m[2023-06-25 01:45:32,705][129146] Max Reward on eval: 488.515078247778
[37m[1m[2023-06-25 01:45:32,705][129146] Min Reward on eval: 73.78284912437667
[37m[1m[2023-06-25 01:45:32,706][129146] Mean Reward across all agents: 258.32053960057954
[37m[1m[2023-06-25 01:45:32,706][129146] Average Trajectory Length: 649.7936666666667
[36m[2023-06-25 01:45:32,708][129146] mean_value=-75.65540508106577, max_value=863.3297572816466
[37m[1m[2023-06-25 01:45:32,711][129146] New mean coefficients: [[ 1.8079306   0.6775593  -0.21029985  0.27453676  0.727595  ]]
[37m[1m[2023-06-25 01:45:32,712][129146] Moving the mean solution point...
[36m[2023-06-25 01:45:42,456][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 01:45:42,457][129146] FPS: 394126.37
[36m[2023-06-25 01:45:42,459][129146] itr=56, itrs=2000, Progress: 2.80%
[36m[2023-06-25 01:45:53,878][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 01:45:53,878][129146] FPS: 336717.13
[36m[2023-06-25 01:45:58,645][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:45:58,645][129146] Reward + Measures: [[329.24106651   0.18413222   0.14195599   0.1107941    0.11832592]]
[37m[1m[2023-06-25 01:45:58,645][129146] Max Reward on eval: 329.24106650690675
[37m[1m[2023-06-25 01:45:58,646][129146] Min Reward on eval: 329.24106650690675
[37m[1m[2023-06-25 01:45:58,646][129146] Mean Reward across all agents: 329.24106650690675
[37m[1m[2023-06-25 01:45:58,646][129146] Average Trajectory Length: 697.1043333333333
[36m[2023-06-25 01:46:04,168][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:46:04,169][129146] Reward + Measures: [[215.98318225   0.22490516   0.17539994   0.1379215    0.13327543]
[37m[1m [243.92577819   0.24924806   0.18599802   0.12598065   0.13345644]
[37m[1m [301.49977896   0.23045902   0.16125326   0.11145855   0.12106955]
[37m[1m ...
[37m[1m [347.02675646   0.20186234   0.15736507   0.11388457   0.10941558]
[37m[1m [478.72717211   0.13583235   0.09626176   0.07645883   0.10951471]
[37m[1m [ 59.91602883   0.28641039   0.23767729   0.11895523   0.18209894]]
[37m[1m[2023-06-25 01:46:04,169][129146] Max Reward on eval: 514.9151958955947
[37m[1m[2023-06-25 01:46:04,169][129146] Min Reward on eval: 59.916028826776895
[37m[1m[2023-06-25 01:46:04,169][129146] Mean Reward across all agents: 300.2522960285533
[37m[1m[2023-06-25 01:46:04,170][129146] Average Trajectory Length: 700.5493333333333
[36m[2023-06-25 01:46:04,172][129146] mean_value=-45.4412584225985, max_value=668.1974535493068
[37m[1m[2023-06-25 01:46:04,175][129146] New mean coefficients: [[ 1.5538757   0.96520674 -0.6744691   0.21097091  0.81194955]]
[37m[1m[2023-06-25 01:46:04,176][129146] Moving the mean solution point...
[36m[2023-06-25 01:46:13,961][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 01:46:13,961][129146] FPS: 392500.58
[36m[2023-06-25 01:46:13,963][129146] itr=57, itrs=2000, Progress: 2.85%
[36m[2023-06-25 01:46:25,419][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 01:46:25,419][129146] FPS: 335562.62
[36m[2023-06-25 01:46:30,143][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:46:30,144][129146] Reward + Measures: [[349.8737211    0.18238275   0.14135347   0.10618233   0.11777382]]
[37m[1m[2023-06-25 01:46:30,144][129146] Max Reward on eval: 349.8737211023229
[37m[1m[2023-06-25 01:46:30,144][129146] Min Reward on eval: 349.8737211023229
[37m[1m[2023-06-25 01:46:30,145][129146] Mean Reward across all agents: 349.8737211023229
[37m[1m[2023-06-25 01:46:30,145][129146] Average Trajectory Length: 695.3766666666667
[36m[2023-06-25 01:46:35,758][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:46:35,759][129146] Reward + Measures: [[343.85671329   0.18462418   0.13591255   0.12671773   0.12994632]
[37m[1m [459.85657032   0.15656583   0.11101849   0.07076471   0.09573277]
[37m[1m [201.70001655   0.26644355   0.192972     0.12112436   0.1607603 ]
[37m[1m ...
[37m[1m [228.75263056   0.25232098   0.18056908   0.15129618   0.15651634]
[37m[1m [327.92193493   0.19250324   0.15844284   0.07325248   0.13710271]
[37m[1m [493.74120436   0.12640227   0.12468977   0.09152159   0.06918068]]
[37m[1m[2023-06-25 01:46:35,759][129146] Max Reward on eval: 574.5558063164469
[37m[1m[2023-06-25 01:46:35,759][129146] Min Reward on eval: 37.391445140261204
[37m[1m[2023-06-25 01:46:35,759][129146] Mean Reward across all agents: 308.85933464476335
[37m[1m[2023-06-25 01:46:35,760][129146] Average Trajectory Length: 670.4803333333333
[36m[2023-06-25 01:46:35,763][129146] mean_value=-31.766814309907264, max_value=659.9068209241207
[37m[1m[2023-06-25 01:46:35,765][129146] New mean coefficients: [[ 1.6843405   1.3653929  -0.77589583  0.5870315   1.2319436 ]]
[37m[1m[2023-06-25 01:46:35,766][129146] Moving the mean solution point...
[36m[2023-06-25 01:46:45,514][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 01:46:45,514][129146] FPS: 394000.39
[36m[2023-06-25 01:46:45,517][129146] itr=58, itrs=2000, Progress: 2.90%
[36m[2023-06-25 01:46:56,955][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 01:46:56,955][129146] FPS: 336075.63
[36m[2023-06-25 01:47:01,716][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:47:01,717][129146] Reward + Measures: [[382.53444404   0.17841859   0.13697021   0.10502631   0.11641718]]
[37m[1m[2023-06-25 01:47:01,717][129146] Max Reward on eval: 382.5344440373376
[37m[1m[2023-06-25 01:47:01,717][129146] Min Reward on eval: 382.5344440373376
[37m[1m[2023-06-25 01:47:01,718][129146] Mean Reward across all agents: 382.5344440373376
[37m[1m[2023-06-25 01:47:01,718][129146] Average Trajectory Length: 716.9246666666667
[36m[2023-06-25 01:47:07,134][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:47:07,135][129146] Reward + Measures: [[339.91613684   0.20074622   0.15910061   0.09592559   0.13928434]
[37m[1m [187.64804599   0.29252091   0.2318904    0.10303885   0.17277853]
[37m[1m [466.70359695   0.1197122    0.11842928   0.0587       0.0677122 ]
[37m[1m ...
[37m[1m [156.32430587   0.25375175   0.23253052   0.0984416    0.14983171]
[37m[1m [377.64050218   0.11258888   0.1090493    0.09458125   0.1132882 ]
[37m[1m [398.84169739   0.15040889   0.11801491   0.08201691   0.11063037]]
[37m[1m[2023-06-25 01:47:07,135][129146] Max Reward on eval: 548.6954836446064
[37m[1m[2023-06-25 01:47:07,135][129146] Min Reward on eval: 39.69992259022547
[37m[1m[2023-06-25 01:47:07,136][129146] Mean Reward across all agents: 293.6576369749801
[37m[1m[2023-06-25 01:47:07,136][129146] Average Trajectory Length: 647.404
[36m[2023-06-25 01:47:07,138][129146] mean_value=-62.108863153640975, max_value=500.3655824158807
[37m[1m[2023-06-25 01:47:07,141][129146] New mean coefficients: [[ 1.5935397   1.1011816  -0.6827704   0.06292671  0.9197893 ]]
[37m[1m[2023-06-25 01:47:07,142][129146] Moving the mean solution point...
[36m[2023-06-25 01:47:16,887][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 01:47:16,888][129146] FPS: 394083.03
[36m[2023-06-25 01:47:16,890][129146] itr=59, itrs=2000, Progress: 2.95%
[36m[2023-06-25 01:47:28,299][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 01:47:28,299][129146] FPS: 336940.87
[36m[2023-06-25 01:47:33,045][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:47:33,045][129146] Reward + Measures: [[406.08949008   0.17697611   0.13473253   0.10627309   0.11529569]]
[37m[1m[2023-06-25 01:47:33,046][129146] Max Reward on eval: 406.0894900761403
[37m[1m[2023-06-25 01:47:33,046][129146] Min Reward on eval: 406.0894900761403
[37m[1m[2023-06-25 01:47:33,046][129146] Mean Reward across all agents: 406.0894900761403
[37m[1m[2023-06-25 01:47:33,046][129146] Average Trajectory Length: 734.002
[36m[2023-06-25 01:47:38,505][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:47:38,505][129146] Reward + Measures: [[275.38441858   0.20332567   0.15471666   0.12182808   0.12401264]
[37m[1m [311.76711967   0.19233701   0.14198665   0.11794354   0.1262757 ]
[37m[1m [304.26651062   0.21019416   0.15484297   0.10035612   0.12896933]
[37m[1m ...
[37m[1m [441.21642622   0.14861614   0.10519274   0.09408403   0.10809481]
[37m[1m [226.88438422   0.28491631   0.18167582   0.12197166   0.16056769]
[37m[1m [126.6011833    0.32792643   0.21655534   0.13934697   0.17379348]]
[37m[1m[2023-06-25 01:47:38,506][129146] Max Reward on eval: 628.7984983793227
[37m[1m[2023-06-25 01:47:38,506][129146] Min Reward on eval: 66.82613347001606
[37m[1m[2023-06-25 01:47:38,506][129146] Mean Reward across all agents: 336.8985066178073
[37m[1m[2023-06-25 01:47:38,506][129146] Average Trajectory Length: 697.8486666666666
[36m[2023-06-25 01:47:38,509][129146] mean_value=-41.395216227061866, max_value=948.9550448868188
[37m[1m[2023-06-25 01:47:38,511][129146] New mean coefficients: [[ 2.0782948   0.6805642  -0.5609797   0.28724968  0.617913  ]]
[37m[1m[2023-06-25 01:47:38,512][129146] Moving the mean solution point...
[36m[2023-06-25 01:47:48,349][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 01:47:48,349][129146] FPS: 390434.00
[36m[2023-06-25 01:47:48,352][129146] itr=60, itrs=2000, Progress: 3.00%
[37m[1m[2023-06-25 01:47:50,465][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000040
[36m[2023-06-25 01:48:02,186][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 01:48:02,187][129146] FPS: 335683.42
[36m[2023-06-25 01:48:07,027][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:48:07,028][129146] Reward + Measures: [[414.27442588   0.18108317   0.13403903   0.10701875   0.11771803]]
[37m[1m[2023-06-25 01:48:07,028][129146] Max Reward on eval: 414.2744258817155
[37m[1m[2023-06-25 01:48:07,028][129146] Min Reward on eval: 414.2744258817155
[37m[1m[2023-06-25 01:48:07,028][129146] Mean Reward across all agents: 414.2744258817155
[37m[1m[2023-06-25 01:48:07,029][129146] Average Trajectory Length: 737.8256666666666
[36m[2023-06-25 01:48:12,597][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:48:12,597][129146] Reward + Measures: [[448.24037919   0.16990601   0.12269551   0.10036816   0.10015543]
[37m[1m [241.01840491   0.22853279   0.17231388   0.14221774   0.16918902]
[37m[1m [345.75859033   0.16411041   0.1356561    0.12044414   0.11785655]
[37m[1m ...
[37m[1m [292.02888164   0.19623072   0.15712515   0.08579911   0.15556435]
[37m[1m [378.92359644   0.16175564   0.14406729   0.10225856   0.10679547]
[37m[1m [476.18515423   0.15744942   0.11374521   0.10844406   0.0939002 ]]
[37m[1m[2023-06-25 01:48:12,597][129146] Max Reward on eval: 612.9074001555506
[37m[1m[2023-06-25 01:48:12,598][129146] Min Reward on eval: 101.4732811592985
[37m[1m[2023-06-25 01:48:12,598][129146] Mean Reward across all agents: 353.33669341074796
[37m[1m[2023-06-25 01:48:12,598][129146] Average Trajectory Length: 705.3216666666666
[36m[2023-06-25 01:48:12,601][129146] mean_value=-30.863995039969147, max_value=826.7639842915614
[37m[1m[2023-06-25 01:48:12,604][129146] New mean coefficients: [[ 1.9421352   0.7839434  -0.34736425  0.292404    0.31025815]]
[37m[1m[2023-06-25 01:48:12,605][129146] Moving the mean solution point...
[36m[2023-06-25 01:48:22,370][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 01:48:22,370][129146] FPS: 393285.26
[36m[2023-06-25 01:48:22,373][129146] itr=61, itrs=2000, Progress: 3.05%
[36m[2023-06-25 01:48:33,786][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 01:48:33,786][129146] FPS: 336888.54
[36m[2023-06-25 01:48:38,612][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:48:38,613][129146] Reward + Measures: [[448.22711878   0.17604417   0.13216388   0.10275811   0.11466847]]
[37m[1m[2023-06-25 01:48:38,613][129146] Max Reward on eval: 448.22711877955675
[37m[1m[2023-06-25 01:48:38,613][129146] Min Reward on eval: 448.22711877955675
[37m[1m[2023-06-25 01:48:38,613][129146] Mean Reward across all agents: 448.22711877955675
[37m[1m[2023-06-25 01:48:38,614][129146] Average Trajectory Length: 756.297
[36m[2023-06-25 01:48:44,182][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:48:44,182][129146] Reward + Measures: [[321.57520072   0.23671643   0.16809465   0.10266321   0.1365464 ]
[37m[1m [353.45926065   0.22265863   0.15194607   0.14489639   0.15542535]
[37m[1m [365.82315575   0.21485101   0.16747484   0.08420145   0.12360938]
[37m[1m ...
[37m[1m [309.42168005   0.24622698   0.17196618   0.15311904   0.16625868]
[37m[1m [355.82787902   0.22500293   0.16164711   0.10889661   0.13083023]
[37m[1m [281.43196437   0.24791823   0.18735161   0.14327918   0.15461527]]
[37m[1m[2023-06-25 01:48:44,182][129146] Max Reward on eval: 689.2251062183175
[37m[1m[2023-06-25 01:48:44,183][129146] Min Reward on eval: 127.99688090975397
[37m[1m[2023-06-25 01:48:44,183][129146] Mean Reward across all agents: 408.3443120848243
[37m[1m[2023-06-25 01:48:44,183][129146] Average Trajectory Length: 749.02
[36m[2023-06-25 01:48:44,188][129146] mean_value=5.4362082841122294, max_value=880.2045341322828
[37m[1m[2023-06-25 01:48:44,191][129146] New mean coefficients: [[ 2.2152905   0.91421676 -0.50600064  0.70944923  0.51273215]]
[37m[1m[2023-06-25 01:48:44,191][129146] Moving the mean solution point...
[36m[2023-06-25 01:48:53,946][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 01:48:53,947][129146] FPS: 393711.65
[36m[2023-06-25 01:48:53,949][129146] itr=62, itrs=2000, Progress: 3.10%
[36m[2023-06-25 01:49:05,367][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 01:49:05,367][129146] FPS: 336722.85
[36m[2023-06-25 01:49:10,064][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:49:10,064][129146] Reward + Measures: [[480.76221465   0.17298962   0.12980202   0.10344756   0.11268357]]
[37m[1m[2023-06-25 01:49:10,065][129146] Max Reward on eval: 480.76221465262756
[37m[1m[2023-06-25 01:49:10,065][129146] Min Reward on eval: 480.76221465262756
[37m[1m[2023-06-25 01:49:10,065][129146] Mean Reward across all agents: 480.76221465262756
[37m[1m[2023-06-25 01:49:10,065][129146] Average Trajectory Length: 781.7946666666667
[36m[2023-06-25 01:49:15,730][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:49:15,731][129146] Reward + Measures: [[305.72044695   0.2494047    0.19501351   0.12054336   0.15747498]
[37m[1m [534.6213352    0.15355161   0.10640645   0.09099678   0.11149355]
[37m[1m [393.52234473   0.17866094   0.13615404   0.1145964    0.12978524]
[37m[1m ...
[37m[1m [152.54030648   0.30015352   0.21088497   0.1201444    0.20641218]
[37m[1m [562.72458208   0.14029616   0.08547308   0.09832115   0.10540384]
[37m[1m [339.9421086    0.19656324   0.15599458   0.13104625   0.13031462]]
[37m[1m[2023-06-25 01:49:15,731][129146] Max Reward on eval: 693.4157265899703
[37m[1m[2023-06-25 01:49:15,731][129146] Min Reward on eval: 152.54030648093467
[37m[1m[2023-06-25 01:49:15,731][129146] Mean Reward across all agents: 408.9240605902327
[37m[1m[2023-06-25 01:49:15,732][129146] Average Trajectory Length: 792.0653333333333
[36m[2023-06-25 01:49:15,734][129146] mean_value=-33.63673963609127, max_value=836.0003701796284
[37m[1m[2023-06-25 01:49:15,737][129146] New mean coefficients: [[ 1.7131553   0.8844835  -0.33741927  0.2753262   0.60997003]]
[37m[1m[2023-06-25 01:49:15,738][129146] Moving the mean solution point...
[36m[2023-06-25 01:49:25,569][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 01:49:25,569][129146] FPS: 390660.21
[36m[2023-06-25 01:49:25,572][129146] itr=63, itrs=2000, Progress: 3.15%
[36m[2023-06-25 01:49:37,065][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 01:49:37,065][129146] FPS: 334529.09
[36m[2023-06-25 01:49:41,936][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:49:41,941][129146] Reward + Measures: [[513.40525728   0.16629083   0.12437186   0.10008033   0.11483987]]
[37m[1m[2023-06-25 01:49:41,942][129146] Max Reward on eval: 513.405257283143
[37m[1m[2023-06-25 01:49:41,942][129146] Min Reward on eval: 513.405257283143
[37m[1m[2023-06-25 01:49:41,942][129146] Mean Reward across all agents: 513.405257283143
[37m[1m[2023-06-25 01:49:41,943][129146] Average Trajectory Length: 807.4803333333333
[36m[2023-06-25 01:49:47,430][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:49:47,431][129146] Reward + Measures: [[509.24323899   0.15902808   0.10227004   0.11764488   0.12897019]
[37m[1m [495.92507087   0.18887277   0.11785853   0.12550965   0.11852782]
[37m[1m [378.41048837   0.18728255   0.12727219   0.16799985   0.13707002]
[37m[1m ...
[37m[1m [515.19763276   0.16347267   0.11671584   0.08695582   0.0914017 ]
[37m[1m [419.87315001   0.18063708   0.14995396   0.11903902   0.1024395 ]
[37m[1m [429.28203667   0.18091784   0.12815785   0.12633787   0.12771967]]
[37m[1m[2023-06-25 01:49:47,431][129146] Max Reward on eval: 659.2251100939932
[37m[1m[2023-06-25 01:49:47,431][129146] Min Reward on eval: 195.80606811979087
[37m[1m[2023-06-25 01:49:47,431][129146] Mean Reward across all agents: 447.98563306866833
[37m[1m[2023-06-25 01:49:47,432][129146] Average Trajectory Length: 821.6503333333333
[36m[2023-06-25 01:49:47,435][129146] mean_value=-24.708734371867042, max_value=613.8097049677699
[37m[1m[2023-06-25 01:49:47,437][129146] New mean coefficients: [[ 1.3256141   0.8216081  -0.23318268  0.24437292  0.7791714 ]]
[37m[1m[2023-06-25 01:49:47,438][129146] Moving the mean solution point...
[36m[2023-06-25 01:49:57,047][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 01:49:57,047][129146] FPS: 399710.57
[36m[2023-06-25 01:49:57,049][129146] itr=64, itrs=2000, Progress: 3.20%
[36m[2023-06-25 01:50:08,495][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 01:50:08,495][129146] FPS: 335920.75
[36m[2023-06-25 01:50:13,286][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:50:13,287][129146] Reward + Measures: [[529.9626111    0.16780171   0.12438411   0.09892379   0.11608155]]
[37m[1m[2023-06-25 01:50:13,287][129146] Max Reward on eval: 529.9626111021635
[37m[1m[2023-06-25 01:50:13,287][129146] Min Reward on eval: 529.9626111021635
[37m[1m[2023-06-25 01:50:13,288][129146] Mean Reward across all agents: 529.9626111021635
[37m[1m[2023-06-25 01:50:13,288][129146] Average Trajectory Length: 806.437
[36m[2023-06-25 01:50:18,851][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:50:18,851][129146] Reward + Measures: [[412.24851569   0.15901391   0.10767221   0.15314876   0.15496878]
[37m[1m [535.45717209   0.14615309   0.10190815   0.10493636   0.10949291]
[37m[1m [493.90407449   0.19316553   0.12925315   0.09262716   0.11675642]
[37m[1m ...
[37m[1m [420.28467418   0.24711092   0.14178988   0.16231623   0.16939902]
[37m[1m [488.66594364   0.15263502   0.10735708   0.1068873    0.13546135]
[37m[1m [544.81147791   0.14570001   0.11455715   0.09274285   0.126     ]]
[37m[1m[2023-06-25 01:50:18,851][129146] Max Reward on eval: 678.1039310163993
[37m[1m[2023-06-25 01:50:18,852][129146] Min Reward on eval: 145.26885915204912
[37m[1m[2023-06-25 01:50:18,852][129146] Mean Reward across all agents: 458.1606106365602
[37m[1m[2023-06-25 01:50:18,852][129146] Average Trajectory Length: 760.8546666666666
[36m[2023-06-25 01:50:18,855][129146] mean_value=-40.82570070582305, max_value=191.86897962691285
[37m[1m[2023-06-25 01:50:18,858][129146] New mean coefficients: [[ 1.1568487   0.988335   -0.27736405  0.122665    0.9520463 ]]
[37m[1m[2023-06-25 01:50:18,859][129146] Moving the mean solution point...
[36m[2023-06-25 01:50:28,618][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 01:50:28,619][129146] FPS: 393513.24
[36m[2023-06-25 01:50:28,621][129146] itr=65, itrs=2000, Progress: 3.25%
[36m[2023-06-25 01:50:40,064][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 01:50:40,065][129146] FPS: 335931.79
[36m[2023-06-25 01:50:44,820][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:50:44,821][129146] Reward + Measures: [[534.17462658   0.16864739   0.12292584   0.09600933   0.12217163]]
[37m[1m[2023-06-25 01:50:44,821][129146] Max Reward on eval: 534.1746265821328
[37m[1m[2023-06-25 01:50:44,821][129146] Min Reward on eval: 534.1746265821328
[37m[1m[2023-06-25 01:50:44,821][129146] Mean Reward across all agents: 534.1746265821328
[37m[1m[2023-06-25 01:50:44,822][129146] Average Trajectory Length: 811.0433333333333
[36m[2023-06-25 01:50:50,359][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:50:50,360][129146] Reward + Measures: [[647.40400683   0.11772857   0.07684286   0.07012857   0.08172858]
[37m[1m [472.89211741   0.18163148   0.13304368   0.07930336   0.12325136]
[37m[1m [528.21714908   0.16225295   0.14895295   0.08448235   0.11440589]
[37m[1m ...
[37m[1m [500.65134862   0.18353476   0.12999605   0.0923503    0.14456211]
[37m[1m [336.92102413   0.26162848   0.18019049   0.13006465   0.17077085]
[37m[1m [440.50077584   0.23067656   0.14461301   0.13761702   0.17049479]]
[37m[1m[2023-06-25 01:50:50,360][129146] Max Reward on eval: 755.6695586889982
[37m[1m[2023-06-25 01:50:50,361][129146] Min Reward on eval: 180.70405787043273
[37m[1m[2023-06-25 01:50:50,361][129146] Mean Reward across all agents: 475.35126053555325
[37m[1m[2023-06-25 01:50:50,361][129146] Average Trajectory Length: 783.3413333333333
[36m[2023-06-25 01:50:50,364][129146] mean_value=-49.29607686471157, max_value=742.0124784203151
[37m[1m[2023-06-25 01:50:50,366][129146] New mean coefficients: [[ 1.3764504   0.80442333 -0.70825803 -0.10546773  0.85950893]]
[37m[1m[2023-06-25 01:50:50,367][129146] Moving the mean solution point...
[36m[2023-06-25 01:50:59,984][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 01:50:59,984][129146] FPS: 399372.58
[36m[2023-06-25 01:50:59,987][129146] itr=66, itrs=2000, Progress: 3.30%
[36m[2023-06-25 01:51:11,484][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 01:51:11,484][129146] FPS: 334388.55
[36m[2023-06-25 01:51:16,276][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:51:16,276][129146] Reward + Measures: [[549.75350921   0.16328619   0.11589075   0.09534185   0.13086869]]
[37m[1m[2023-06-25 01:51:16,277][129146] Max Reward on eval: 549.7535092116632
[37m[1m[2023-06-25 01:51:16,277][129146] Min Reward on eval: 549.7535092116632
[37m[1m[2023-06-25 01:51:16,277][129146] Mean Reward across all agents: 549.7535092116632
[37m[1m[2023-06-25 01:51:16,277][129146] Average Trajectory Length: 825.9163333333333
[36m[2023-06-25 01:51:21,659][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:51:21,660][129146] Reward + Measures: [[459.39055248   0.22640257   0.13837981   0.12707421   0.13925774]
[37m[1m [609.59269495   0.14243864   0.09737905   0.09453253   0.11635448]
[37m[1m [492.16587271   0.17110711   0.14508073   0.08111276   0.12373216]
[37m[1m ...
[37m[1m [610.22224428   0.14147036   0.08281852   0.09020741   0.09308148]
[37m[1m [642.87982028   0.14400536   0.09038671   0.08681352   0.13741982]
[37m[1m [537.1710524    0.17128764   0.12160449   0.07718314   0.1352427 ]]
[37m[1m[2023-06-25 01:51:21,660][129146] Max Reward on eval: 698.4247986098111
[37m[1m[2023-06-25 01:51:21,660][129146] Min Reward on eval: 229.97994860726757
[37m[1m[2023-06-25 01:51:21,661][129146] Mean Reward across all agents: 481.84137992249964
[37m[1m[2023-06-25 01:51:21,661][129146] Average Trajectory Length: 787.471
[36m[2023-06-25 01:51:21,663][129146] mean_value=-63.207050504304156, max_value=104.06631445831044
[37m[1m[2023-06-25 01:51:21,665][129146] New mean coefficients: [[ 1.3155804  0.6934419 -0.7983935 -0.5049847  1.1582494]]
[37m[1m[2023-06-25 01:51:21,666][129146] Moving the mean solution point...
[36m[2023-06-25 01:51:31,292][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 01:51:31,293][129146] FPS: 398985.91
[36m[2023-06-25 01:51:31,295][129146] itr=67, itrs=2000, Progress: 3.35%
[36m[2023-06-25 01:51:43,001][129146] train() took 11.69 seconds to complete
[36m[2023-06-25 01:51:43,002][129146] FPS: 328383.70
[36m[2023-06-25 01:51:47,772][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:51:47,772][129146] Reward + Measures: [[553.58591539   0.16570777   0.12028281   0.09217123   0.13347644]]
[37m[1m[2023-06-25 01:51:47,772][129146] Max Reward on eval: 553.5859153944465
[37m[1m[2023-06-25 01:51:47,772][129146] Min Reward on eval: 553.5859153944465
[37m[1m[2023-06-25 01:51:47,773][129146] Mean Reward across all agents: 553.5859153944465
[37m[1m[2023-06-25 01:51:47,773][129146] Average Trajectory Length: 809.8066666666666
[36m[2023-06-25 01:51:53,313][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:51:53,314][129146] Reward + Measures: [[335.54390137   0.22935171   0.16254507   0.10675035   0.17411432]
[37m[1m [484.41579374   0.18555275   0.10112917   0.10509493   0.14776993]
[37m[1m [540.2413375    0.14497544   0.12151354   0.0502       0.10467519]
[37m[1m ...
[37m[1m [557.72251887   0.14693224   0.10928065   0.0948258    0.14432903]
[37m[1m [461.54280262   0.19065985   0.13516457   0.1056182    0.13727713]
[37m[1m [632.90139744   0.11630447   0.08406911   0.08871344   0.1059555 ]]
[37m[1m[2023-06-25 01:51:53,314][129146] Max Reward on eval: 743.7871833403944
[37m[1m[2023-06-25 01:51:53,314][129146] Min Reward on eval: 129.15108285209863
[37m[1m[2023-06-25 01:51:53,315][129146] Mean Reward across all agents: 436.2465539362475
[37m[1m[2023-06-25 01:51:53,315][129146] Average Trajectory Length: 723.501
[36m[2023-06-25 01:51:53,318][129146] mean_value=-63.00322739694843, max_value=1218.3388218475739
[37m[1m[2023-06-25 01:51:53,320][129146] New mean coefficients: [[ 1.1488253   0.44715008 -0.43809107 -0.6806744   1.1744223 ]]
[37m[1m[2023-06-25 01:51:53,321][129146] Moving the mean solution point...
[36m[2023-06-25 01:52:03,030][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 01:52:03,030][129146] FPS: 395592.42
[36m[2023-06-25 01:52:03,032][129146] itr=68, itrs=2000, Progress: 3.40%
[36m[2023-06-25 01:52:14,595][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 01:52:14,595][129146] FPS: 332467.35
[36m[2023-06-25 01:52:19,385][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:52:19,386][129146] Reward + Measures: [[566.27595142   0.16812776   0.11748901   0.09217907   0.14117417]]
[37m[1m[2023-06-25 01:52:19,386][129146] Max Reward on eval: 566.27595142156
[37m[1m[2023-06-25 01:52:19,386][129146] Min Reward on eval: 566.27595142156
[37m[1m[2023-06-25 01:52:19,387][129146] Mean Reward across all agents: 566.27595142156
[37m[1m[2023-06-25 01:52:19,387][129146] Average Trajectory Length: 817.63
[36m[2023-06-25 01:52:24,853][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:52:24,853][129146] Reward + Measures: [[322.16383752   0.20397222   0.19286716   0.15512553   0.15684584]
[37m[1m [536.43538175   0.21115664   0.14174619   0.10579079   0.13365124]
[37m[1m [423.27407346   0.19689655   0.14443426   0.12964594   0.14206401]
[37m[1m ...
[37m[1m [472.66854423   0.18213412   0.16791096   0.05735271   0.10586479]
[37m[1m [647.0774539    0.11300754   0.09580753   0.07525484   0.06945484]
[37m[1m [654.83890003   0.15973479   0.09569132   0.09986522   0.14668696]]
[37m[1m[2023-06-25 01:52:24,854][129146] Max Reward on eval: 764.3291575602722
[37m[1m[2023-06-25 01:52:24,854][129146] Min Reward on eval: 200.94146637238447
[37m[1m[2023-06-25 01:52:24,854][129146] Mean Reward across all agents: 536.3962525515279
[37m[1m[2023-06-25 01:52:24,854][129146] Average Trajectory Length: 817.8843333333333
[36m[2023-06-25 01:52:24,857][129146] mean_value=-40.54501745939997, max_value=212.4488875524889
[37m[1m[2023-06-25 01:52:24,860][129146] New mean coefficients: [[ 1.4429555   0.5823691  -0.32169664 -0.5550655   1.2091873 ]]
[37m[1m[2023-06-25 01:52:24,861][129146] Moving the mean solution point...
[36m[2023-06-25 01:52:34,644][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 01:52:34,644][129146] FPS: 392578.35
[36m[2023-06-25 01:52:34,646][129146] itr=69, itrs=2000, Progress: 3.45%
[36m[2023-06-25 01:52:46,259][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 01:52:46,260][129146] FPS: 331020.91
[36m[2023-06-25 01:52:51,068][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:52:51,068][129146] Reward + Measures: [[587.06992652   0.16384842   0.11345547   0.09123658   0.15579481]]
[37m[1m[2023-06-25 01:52:51,069][129146] Max Reward on eval: 587.0699265225381
[37m[1m[2023-06-25 01:52:51,069][129146] Min Reward on eval: 587.0699265225381
[37m[1m[2023-06-25 01:52:51,069][129146] Mean Reward across all agents: 587.0699265225381
[37m[1m[2023-06-25 01:52:51,069][129146] Average Trajectory Length: 832.4326666666666
[36m[2023-06-25 01:52:56,595][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:52:56,596][129146] Reward + Measures: [[555.47353673   0.17930494   0.12932472   0.07791076   0.1183246 ]
[37m[1m [532.63235624   0.20221196   0.13988912   0.10916799   0.14066646]
[37m[1m [495.29750086   0.18075667   0.13378195   0.09537894   0.14071298]
[37m[1m ...
[37m[1m [625.6432682    0.12137143   0.08577143   0.0613       0.09512381]
[37m[1m [535.54904472   0.17902866   0.10932865   0.0871427    0.17758977]
[37m[1m [461.70295246   0.22304557   0.14394918   0.08568691   0.15605691]]
[37m[1m[2023-06-25 01:52:56,596][129146] Max Reward on eval: 776.4946789103793
[37m[1m[2023-06-25 01:52:56,596][129146] Min Reward on eval: 252.03671660382534
[37m[1m[2023-06-25 01:52:56,596][129146] Mean Reward across all agents: 537.7481708300821
[37m[1m[2023-06-25 01:52:56,597][129146] Average Trajectory Length: 796.8566666666667
[36m[2023-06-25 01:52:56,599][129146] mean_value=-30.490445509790575, max_value=1200.0282391477376
[37m[1m[2023-06-25 01:52:56,602][129146] New mean coefficients: [[ 1.3791538   0.65880734 -0.10108253 -0.47245485  1.2014213 ]]
[37m[1m[2023-06-25 01:52:56,603][129146] Moving the mean solution point...
[36m[2023-06-25 01:53:06,371][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 01:53:06,372][129146] FPS: 393172.56
[36m[2023-06-25 01:53:06,374][129146] itr=70, itrs=2000, Progress: 3.50%
[37m[1m[2023-06-25 01:53:08,501][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000050
[36m[2023-06-25 01:53:20,448][129146] train() took 11.66 seconds to complete
[36m[2023-06-25 01:53:20,448][129146] FPS: 329455.50
[36m[2023-06-25 01:53:25,034][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:53:25,035][129146] Reward + Measures: [[594.71586089   0.16381682   0.11148509   0.09352012   0.16861616]]
[37m[1m[2023-06-25 01:53:25,035][129146] Max Reward on eval: 594.7158608873713
[37m[1m[2023-06-25 01:53:25,036][129146] Min Reward on eval: 594.7158608873713
[37m[1m[2023-06-25 01:53:25,036][129146] Mean Reward across all agents: 594.7158608873713
[37m[1m[2023-06-25 01:53:25,036][129146] Average Trajectory Length: 847.208
[36m[2023-06-25 01:53:30,450][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:53:30,450][129146] Reward + Measures: [[659.04895744   0.13123222   0.10185764   0.09745593   0.1209678 ]
[37m[1m [552.70394744   0.18460867   0.11932526   0.08882136   0.1604058 ]
[37m[1m [336.51225928   0.28433281   0.1866546    0.14590068   0.15887101]
[37m[1m ...
[37m[1m [680.91781173   0.11959951   0.08102223   0.08870242   0.12202078]
[37m[1m [624.45380428   0.17347872   0.08701845   0.09903404   0.15920921]
[37m[1m [602.83303949   0.14563857   0.10708251   0.10503542   0.11224798]]
[37m[1m[2023-06-25 01:53:30,450][129146] Max Reward on eval: 773.569977357192
[37m[1m[2023-06-25 01:53:30,451][129146] Min Reward on eval: 257.33766012322155
[37m[1m[2023-06-25 01:53:30,451][129146] Mean Reward across all agents: 534.8234131459178
[37m[1m[2023-06-25 01:53:30,451][129146] Average Trajectory Length: 843.3656666666666
[36m[2023-06-25 01:53:30,454][129146] mean_value=-20.98553169545318, max_value=1074.949781817827
[37m[1m[2023-06-25 01:53:30,456][129146] New mean coefficients: [[ 1.4336864   0.6654005  -0.30089438  0.08254308  0.9891628 ]]
[37m[1m[2023-06-25 01:53:30,457][129146] Moving the mean solution point...
[36m[2023-06-25 01:53:40,068][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 01:53:40,069][129146] FPS: 399612.85
[36m[2023-06-25 01:53:40,071][129146] itr=71, itrs=2000, Progress: 3.55%
[36m[2023-06-25 01:53:51,481][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 01:53:51,482][129146] FPS: 336905.84
[36m[2023-06-25 01:53:56,236][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:53:56,236][129146] Reward + Measures: [[611.03798465   0.1646571    0.11122936   0.0911481    0.17124918]]
[37m[1m[2023-06-25 01:53:56,236][129146] Max Reward on eval: 611.0379846463853
[37m[1m[2023-06-25 01:53:56,237][129146] Min Reward on eval: 611.0379846463853
[37m[1m[2023-06-25 01:53:56,237][129146] Mean Reward across all agents: 611.0379846463853
[37m[1m[2023-06-25 01:53:56,237][129146] Average Trajectory Length: 849.778
[36m[2023-06-25 01:54:01,602][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:54:01,603][129146] Reward + Measures: [[636.15391807   0.13861668   0.08691666   0.10294167   0.18412916]
[37m[1m [641.52480156   0.14780001   0.0945       0.08810001   0.1751    ]
[37m[1m [616.88729615   0.17576742   0.11677152   0.11509655   0.15991448]
[37m[1m ...
[37m[1m [390.22956814   0.25152084   0.1721193    0.12756039   0.18258445]
[37m[1m [436.16873564   0.23356961   0.1573215    0.11737119   0.20755219]
[37m[1m [749.64097926   0.13150001   0.0874       0.0856       0.1415    ]]
[37m[1m[2023-06-25 01:54:01,603][129146] Max Reward on eval: 787.5478896444198
[37m[1m[2023-06-25 01:54:01,603][129146] Min Reward on eval: 306.7739506996819
[37m[1m[2023-06-25 01:54:01,603][129146] Mean Reward across all agents: 573.4357030214879
[37m[1m[2023-06-25 01:54:01,604][129146] Average Trajectory Length: 865.381
[36m[2023-06-25 01:54:01,607][129146] mean_value=20.183824903897026, max_value=1200.079068633821
[37m[1m[2023-06-25 01:54:01,610][129146] New mean coefficients: [[ 1.6764686   0.6170051  -0.17030881  0.14132845  1.0281427 ]]
[37m[1m[2023-06-25 01:54:01,611][129146] Moving the mean solution point...
[36m[2023-06-25 01:54:11,115][129146] train() took 9.50 seconds to complete
[36m[2023-06-25 01:54:11,116][129146] FPS: 404085.86
[36m[2023-06-25 01:54:11,118][129146] itr=72, itrs=2000, Progress: 3.60%
[36m[2023-06-25 01:54:22,515][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 01:54:22,516][129146] FPS: 337287.96
[36m[2023-06-25 01:54:27,220][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:54:27,220][129146] Reward + Measures: [[628.09772553   0.1610523    0.10848559   0.08643976   0.182377  ]]
[37m[1m[2023-06-25 01:54:27,220][129146] Max Reward on eval: 628.0977255348644
[37m[1m[2023-06-25 01:54:27,221][129146] Min Reward on eval: 628.0977255348644
[37m[1m[2023-06-25 01:54:27,221][129146] Mean Reward across all agents: 628.0977255348644
[37m[1m[2023-06-25 01:54:27,221][129146] Average Trajectory Length: 856.5046666666666
[36m[2023-06-25 01:54:32,818][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:54:32,818][129146] Reward + Measures: [[451.79331076   0.1864305    0.13398634   0.08732976   0.17578883]
[37m[1m [353.94165758   0.27962929   0.22077338   0.10600019   0.17318469]
[37m[1m [611.46737143   0.18642294   0.1111089    0.11983466   0.13608244]
[37m[1m ...
[37m[1m [455.68962506   0.21155305   0.16266719   0.08401605   0.21291144]
[37m[1m [487.54465444   0.20274194   0.17071371   0.07907607   0.16067056]
[37m[1m [651.56742109   0.1652143    0.10125715   0.09174287   0.13017143]]
[37m[1m[2023-06-25 01:54:32,818][129146] Max Reward on eval: 786.6272402290022
[37m[1m[2023-06-25 01:54:32,819][129146] Min Reward on eval: 247.46942509653164
[37m[1m[2023-06-25 01:54:32,819][129146] Mean Reward across all agents: 536.7100294643545
[37m[1m[2023-06-25 01:54:32,819][129146] Average Trajectory Length: 799.7636666666666
[36m[2023-06-25 01:54:32,822][129146] mean_value=-63.05320623577813, max_value=948.110001332027
[37m[1m[2023-06-25 01:54:32,825][129146] New mean coefficients: [[1.3711015  0.59719706 0.08050211 0.05953231 0.86238116]]
[37m[1m[2023-06-25 01:54:32,826][129146] Moving the mean solution point...
[36m[2023-06-25 01:54:42,580][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 01:54:42,580][129146] FPS: 393742.43
[36m[2023-06-25 01:54:42,583][129146] itr=73, itrs=2000, Progress: 3.65%
[36m[2023-06-25 01:54:54,137][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 01:54:54,138][129146] FPS: 332686.22
[36m[2023-06-25 01:54:59,009][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:54:59,010][129146] Reward + Measures: [[637.32333321   0.16748582   0.11254838   0.09364495   0.18247758]]
[37m[1m[2023-06-25 01:54:59,010][129146] Max Reward on eval: 637.3233332113563
[37m[1m[2023-06-25 01:54:59,010][129146] Min Reward on eval: 637.3233332113563
[37m[1m[2023-06-25 01:54:59,011][129146] Mean Reward across all agents: 637.3233332113563
[37m[1m[2023-06-25 01:54:59,011][129146] Average Trajectory Length: 871.4193333333333
[36m[2023-06-25 01:55:04,567][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:55:04,567][129146] Reward + Measures: [[719.14052487   0.13880001   0.08529999   0.0887       0.15580001]
[37m[1m [164.46599992   0.29080001   0.1645       0.1847       0.21270001]
[37m[1m [347.41090494   0.2581524    0.19563943   0.08659547   0.18617487]
[37m[1m ...
[37m[1m [439.50404459   0.19709399   0.14446568   0.09650654   0.14405969]
[37m[1m [389.43216615   0.2222625    0.14531802   0.14402573   0.19293553]
[37m[1m [483.2423138    0.19596414   0.11886038   0.11476038   0.2023    ]]
[37m[1m[2023-06-25 01:55:04,567][129146] Max Reward on eval: 780.8309235424501
[37m[1m[2023-06-25 01:55:04,568][129146] Min Reward on eval: -16.79143692611251
[37m[1m[2023-06-25 01:55:04,568][129146] Mean Reward across all agents: 485.5415710407253
[37m[1m[2023-06-25 01:55:04,568][129146] Average Trajectory Length: 779.6216666666667
[36m[2023-06-25 01:55:04,571][129146] mean_value=-67.42461404025302, max_value=870.4372638449163
[37m[1m[2023-06-25 01:55:04,574][129146] New mean coefficients: [[0.861048   0.5034483  0.10190088 0.34890103 0.78005415]]
[37m[1m[2023-06-25 01:55:04,575][129146] Moving the mean solution point...
[36m[2023-06-25 01:55:14,311][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 01:55:14,312][129146] FPS: 394457.47
[36m[2023-06-25 01:55:14,314][129146] itr=74, itrs=2000, Progress: 3.70%
[36m[2023-06-25 01:55:25,821][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 01:55:25,822][129146] FPS: 334064.55
[36m[2023-06-25 01:55:30,752][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:55:30,753][129146] Reward + Measures: [[643.37996725   0.17012897   0.11392387   0.09490003   0.19934028]]
[37m[1m[2023-06-25 01:55:30,753][129146] Max Reward on eval: 643.3799672492088
[37m[1m[2023-06-25 01:55:30,753][129146] Min Reward on eval: 643.3799672492088
[37m[1m[2023-06-25 01:55:30,754][129146] Mean Reward across all agents: 643.3799672492088
[37m[1m[2023-06-25 01:55:30,754][129146] Average Trajectory Length: 885.5403333333333
[36m[2023-06-25 01:55:36,112][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:55:36,112][129146] Reward + Measures: [[533.20149003   0.20312093   0.12126138   0.13177556   0.22243737]
[37m[1m [748.43173148   0.12310001   0.0777       0.0686       0.21660002]
[37m[1m [603.10883847   0.17110291   0.10376065   0.1176916    0.16003294]
[37m[1m ...
[37m[1m [622.00321256   0.15207392   0.08565743   0.09326766   0.15531254]
[37m[1m [505.63970107   0.21781969   0.1445612    0.1414825    0.15744679]
[37m[1m [546.16657224   0.20432857   0.11092222   0.1345619    0.1478778 ]]
[37m[1m[2023-06-25 01:55:36,112][129146] Max Reward on eval: 792.2355270402855
[37m[1m[2023-06-25 01:55:36,113][129146] Min Reward on eval: 313.21080091366895
[37m[1m[2023-06-25 01:55:36,113][129146] Mean Reward across all agents: 573.3532166385805
[37m[1m[2023-06-25 01:55:36,113][129146] Average Trajectory Length: 866.4783333333334
[36m[2023-06-25 01:55:36,117][129146] mean_value=-27.305598132436064, max_value=870.3588913492455
[37m[1m[2023-06-25 01:55:36,119][129146] New mean coefficients: [[0.37907583 0.37726432 0.23074184 0.47130877 0.70008385]]
[37m[1m[2023-06-25 01:55:36,120][129146] Moving the mean solution point...
[36m[2023-06-25 01:55:45,881][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 01:55:45,881][129146] FPS: 393498.93
[36m[2023-06-25 01:55:45,883][129146] itr=75, itrs=2000, Progress: 3.75%
[36m[2023-06-25 01:55:57,539][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 01:55:57,539][129146] FPS: 329812.87
[36m[2023-06-25 01:56:02,478][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:56:02,478][129146] Reward + Measures: [[642.70327099   0.18193334   0.11685492   0.10536967   0.20810404]]
[37m[1m[2023-06-25 01:56:02,479][129146] Max Reward on eval: 642.7032709875218
[37m[1m[2023-06-25 01:56:02,479][129146] Min Reward on eval: 642.7032709875218
[37m[1m[2023-06-25 01:56:02,479][129146] Mean Reward across all agents: 642.7032709875218
[37m[1m[2023-06-25 01:56:02,479][129146] Average Trajectory Length: 901.058
[36m[2023-06-25 01:56:08,079][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:56:08,079][129146] Reward + Measures: [[502.40900804   0.22245657   0.16519684   0.12907194   0.16627194]
[37m[1m [500.66440775   0.22548656   0.14833085   0.12527134   0.21565591]
[37m[1m [738.27887614   0.1428       0.1072       0.0827       0.1824    ]
[37m[1m ...
[37m[1m [674.34298455   0.17840944   0.10701864   0.11579734   0.25867096]
[37m[1m [679.65962891   0.13610001   0.072        0.077        0.26500002]
[37m[1m [683.76321441   0.13160001   0.08100001   0.07739999   0.24729998]]
[37m[1m[2023-06-25 01:56:08,079][129146] Max Reward on eval: 808.2620117523242
[37m[1m[2023-06-25 01:56:08,080][129146] Min Reward on eval: 318.4092313012341
[37m[1m[2023-06-25 01:56:08,080][129146] Mean Reward across all agents: 598.4565428542609
[37m[1m[2023-06-25 01:56:08,080][129146] Average Trajectory Length: 894.5649999999999
[36m[2023-06-25 01:56:08,084][129146] mean_value=-7.179636017341175, max_value=1113.4669733445917
[37m[1m[2023-06-25 01:56:08,086][129146] New mean coefficients: [[0.01559043 0.5319791  0.21174079 0.38915068 0.67641044]]
[37m[1m[2023-06-25 01:56:08,087][129146] Moving the mean solution point...
[36m[2023-06-25 01:56:17,816][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 01:56:17,817][129146] FPS: 394766.43
[36m[2023-06-25 01:56:17,819][129146] itr=76, itrs=2000, Progress: 3.80%
[36m[2023-06-25 01:56:29,480][129146] train() took 11.65 seconds to complete
[36m[2023-06-25 01:56:29,480][129146] FPS: 329665.57
[36m[2023-06-25 01:56:34,266][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:56:34,266][129146] Reward + Measures: [[624.02153898   0.19275163   0.12297997   0.1150779    0.22063854]]
[37m[1m[2023-06-25 01:56:34,266][129146] Max Reward on eval: 624.0215389823908
[37m[1m[2023-06-25 01:56:34,266][129146] Min Reward on eval: 624.0215389823908
[37m[1m[2023-06-25 01:56:34,266][129146] Mean Reward across all agents: 624.0215389823908
[37m[1m[2023-06-25 01:56:34,267][129146] Average Trajectory Length: 901.8526666666667
[36m[2023-06-25 01:56:39,878][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:56:39,879][129146] Reward + Measures: [[599.43339835   0.21400002   0.1303       0.1402       0.1877    ]
[37m[1m [608.33626728   0.17506258   0.11276863   0.1147212    0.20422097]
[37m[1m [556.11437571   0.1878421    0.12267895   0.1056       0.16538949]
[37m[1m ...
[37m[1m [733.47713535   0.14267582   0.10459268   0.06767002   0.11925592]
[37m[1m [589.32863519   0.18682881   0.1025339    0.11214916   0.2727983 ]
[37m[1m [553.42139062   0.24250291   0.13513723   0.12840518   0.19766989]]
[37m[1m[2023-06-25 01:56:39,879][129146] Max Reward on eval: 783.984079348878
[37m[1m[2023-06-25 01:56:39,879][129146] Min Reward on eval: 349.04614401360743
[37m[1m[2023-06-25 01:56:39,880][129146] Mean Reward across all agents: 595.1137012059401
[37m[1m[2023-06-25 01:56:39,880][129146] Average Trajectory Length: 909.288
[36m[2023-06-25 01:56:39,884][129146] mean_value=18.413852920715385, max_value=1264.9411824947688
[37m[1m[2023-06-25 01:56:39,886][129146] New mean coefficients: [[0.14148402 0.79312825 0.27722895 0.56410265 1.0130138 ]]
[37m[1m[2023-06-25 01:56:39,888][129146] Moving the mean solution point...
[36m[2023-06-25 01:56:49,643][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 01:56:49,643][129146] FPS: 393699.42
[36m[2023-06-25 01:56:49,645][129146] itr=77, itrs=2000, Progress: 3.85%
[36m[2023-06-25 01:57:01,275][129146] train() took 11.62 seconds to complete
[36m[2023-06-25 01:57:01,275][129146] FPS: 330587.88
[36m[2023-06-25 01:57:06,081][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:57:06,081][129146] Reward + Measures: [[314.06624882   0.30795375   0.21285617   0.13580391   0.23430945]]
[37m[1m[2023-06-25 01:57:06,081][129146] Max Reward on eval: 314.06624882325826
[37m[1m[2023-06-25 01:57:06,082][129146] Min Reward on eval: 314.06624882325826
[37m[1m[2023-06-25 01:57:06,082][129146] Mean Reward across all agents: 314.06624882325826
[37m[1m[2023-06-25 01:57:06,082][129146] Average Trajectory Length: 954.274
[36m[2023-06-25 01:57:11,614][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:57:11,615][129146] Reward + Measures: [[265.10651087   0.32690001   0.24679999   0.14780001   0.23179999]
[37m[1m [274.75788238   0.28107506   0.18843006   0.10652288   0.23857479]
[37m[1m [228.09697693   0.30950963   0.24402311   0.12494531   0.20174097]
[37m[1m ...
[37m[1m [340.31813244   0.2814976    0.18145421   0.12581085   0.2305759 ]
[37m[1m [220.19215231   0.26284531   0.17662136   0.11475986   0.219919  ]
[37m[1m [359.33940367   0.26809999   0.17310001   0.1072       0.2217    ]]
[37m[1m[2023-06-25 01:57:11,615][129146] Max Reward on eval: 427.55110993281414
[37m[1m[2023-06-25 01:57:11,615][129146] Min Reward on eval: 118.03607496899204
[37m[1m[2023-06-25 01:57:11,615][129146] Mean Reward across all agents: 288.5045991025314
[37m[1m[2023-06-25 01:57:11,616][129146] Average Trajectory Length: 948.4883333333333
[36m[2023-06-25 01:57:11,618][129146] mean_value=-82.24928728496275, max_value=782.4868003504968
[37m[1m[2023-06-25 01:57:11,621][129146] New mean coefficients: [[0.30292386 0.9772135  0.6564698  0.60484505 0.92038256]]
[37m[1m[2023-06-25 01:57:11,622][129146] Moving the mean solution point...
[36m[2023-06-25 01:57:21,374][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 01:57:21,374][129146] FPS: 393852.85
[36m[2023-06-25 01:57:21,376][129146] itr=78, itrs=2000, Progress: 3.90%
[36m[2023-06-25 01:57:32,895][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 01:57:32,895][129146] FPS: 333748.83
[36m[2023-06-25 01:57:37,587][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:57:37,587][129146] Reward + Measures: [[331.51940553   0.32857731   0.22235432   0.14549458   0.24299732]]
[37m[1m[2023-06-25 01:57:37,587][129146] Max Reward on eval: 331.51940553043784
[37m[1m[2023-06-25 01:57:37,588][129146] Min Reward on eval: 331.51940553043784
[37m[1m[2023-06-25 01:57:37,588][129146] Mean Reward across all agents: 331.51940553043784
[37m[1m[2023-06-25 01:57:37,588][129146] Average Trajectory Length: 959.2189999999999
[36m[2023-06-25 01:57:43,073][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:57:43,073][129146] Reward + Measures: [[386.2408918    0.40107307   0.2605603    0.18172279   0.24461651]
[37m[1m [252.89577454   0.33173728   0.22610453   0.14544357   0.23235758]
[37m[1m [278.56853888   0.29304594   0.2031002    0.1069827    0.2139695 ]
[37m[1m ...
[37m[1m [391.49261994   0.31219998   0.21469998   0.1393       0.21789999]
[37m[1m [327.82009255   0.35830966   0.21012859   0.1815459    0.22571301]
[37m[1m [386.47744227   0.31979999   0.21540003   0.13         0.22849999]]
[37m[1m[2023-06-25 01:57:43,073][129146] Max Reward on eval: 450.7105512320122
[37m[1m[2023-06-25 01:57:43,074][129146] Min Reward on eval: 179.9568811380246
[37m[1m[2023-06-25 01:57:43,074][129146] Mean Reward across all agents: 325.547444969227
[37m[1m[2023-06-25 01:57:43,074][129146] Average Trajectory Length: 933.443
[36m[2023-06-25 01:57:43,077][129146] mean_value=0.1356733585496719, max_value=914.2978642955829
[37m[1m[2023-06-25 01:57:43,079][129146] New mean coefficients: [[-0.43114424  1.0583179   0.9411792   1.0813751   0.7899565 ]]
[37m[1m[2023-06-25 01:57:43,081][129146] Moving the mean solution point...
[36m[2023-06-25 01:57:52,694][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 01:57:52,695][129146] FPS: 399497.71
[36m[2023-06-25 01:57:52,697][129146] itr=79, itrs=2000, Progress: 3.95%
[36m[2023-06-25 01:58:04,227][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 01:58:04,228][129146] FPS: 333387.04
[36m[2023-06-25 01:58:09,191][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:58:09,191][129146] Reward + Measures: [[294.47457012   0.36687183   0.22525692   0.17536987   0.24987008]]
[37m[1m[2023-06-25 01:58:09,191][129146] Max Reward on eval: 294.47457012443255
[37m[1m[2023-06-25 01:58:09,192][129146] Min Reward on eval: 294.47457012443255
[37m[1m[2023-06-25 01:58:09,192][129146] Mean Reward across all agents: 294.47457012443255
[37m[1m[2023-06-25 01:58:09,192][129146] Average Trajectory Length: 969.5753333333333
[36m[2023-06-25 01:58:14,686][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:58:14,686][129146] Reward + Measures: [[150.93275055   0.45000002   0.2464       0.25820002   0.2474    ]
[37m[1m [200.86167172   0.43150002   0.23059998   0.22740002   0.23670001]
[37m[1m [232.59795891   0.45910001   0.25440001   0.25139999   0.2352    ]
[37m[1m ...
[37m[1m [222.78816537   0.45229998   0.2782       0.22430001   0.25400001]
[37m[1m [242.30343606   0.41459998   0.25250003   0.21370001   0.24730001]
[37m[1m [193.90819743   0.42430001   0.29440004   0.205        0.26730001]]
[37m[1m[2023-06-25 01:58:14,686][129146] Max Reward on eval: 408.796434111672
[37m[1m[2023-06-25 01:58:14,687][129146] Min Reward on eval: 44.9534987514664
[37m[1m[2023-06-25 01:58:14,687][129146] Mean Reward across all agents: 252.30297755372712
[37m[1m[2023-06-25 01:58:14,687][129146] Average Trajectory Length: 973.4256666666666
[36m[2023-06-25 01:58:14,691][129146] mean_value=74.09733501908575, max_value=717.8069127352443
[37m[1m[2023-06-25 01:58:14,694][129146] New mean coefficients: [[-0.84792256  1.0723772   0.84942025  1.6391397   0.78876287]]
[37m[1m[2023-06-25 01:58:14,695][129146] Moving the mean solution point...
[36m[2023-06-25 01:58:24,686][129146] train() took 9.99 seconds to complete
[36m[2023-06-25 01:58:24,686][129146] FPS: 384425.25
[36m[2023-06-25 01:58:24,688][129146] itr=80, itrs=2000, Progress: 4.00%
[37m[1m[2023-06-25 01:58:26,850][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000060
[36m[2023-06-25 01:58:38,756][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 01:58:38,757][129146] FPS: 330913.50
[36m[2023-06-25 01:58:43,593][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:58:43,593][129146] Reward + Measures: [[-201.61765629    0.48779953    0.16441341    0.35143617    0.19093436]]
[37m[1m[2023-06-25 01:58:43,593][129146] Max Reward on eval: -201.61765628527814
[37m[1m[2023-06-25 01:58:43,594][129146] Min Reward on eval: -201.61765628527814
[37m[1m[2023-06-25 01:58:43,594][129146] Mean Reward across all agents: -201.61765628527814
[37m[1m[2023-06-25 01:58:43,594][129146] Average Trajectory Length: 949.2653333333333
[36m[2023-06-25 01:58:49,304][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:58:49,304][129146] Reward + Measures: [[ -99.49332821    0.42512131    0.16924171    0.2822417     0.18678592]
[37m[1m [ -91.9312721     0.46340004    0.169         0.33469999    0.1943    ]
[37m[1m [-234.40029276    0.46264753    0.21134947    0.30805871    0.20123605]
[37m[1m ...
[37m[1m [-127.02765439    0.45658073    0.21433809    0.2937479     0.1817252 ]
[37m[1m [-148.23915763    0.47399998    0.1877        0.33179998    0.2017    ]
[37m[1m [-122.12325546    0.3924        0.18410002    0.2045        0.20479999]]
[37m[1m[2023-06-25 01:58:49,304][129146] Max Reward on eval: 17.25475027291977
[37m[1m[2023-06-25 01:58:49,305][129146] Min Reward on eval: -312.1220947015565
[37m[1m[2023-06-25 01:58:49,305][129146] Mean Reward across all agents: -162.22375410905508
[37m[1m[2023-06-25 01:58:49,305][129146] Average Trajectory Length: 931.977
[36m[2023-06-25 01:58:49,313][129146] mean_value=193.86745578464667, max_value=431.5709036548236
[37m[1m[2023-06-25 01:58:49,316][129146] New mean coefficients: [[-1.4701973  1.1958476  1.0207133  1.697186   1.4886875]]
[37m[1m[2023-06-25 01:58:49,317][129146] Moving the mean solution point...
[36m[2023-06-25 01:58:59,169][129146] train() took 9.85 seconds to complete
[36m[2023-06-25 01:58:59,170][129146] FPS: 389817.81
[36m[2023-06-25 01:58:59,172][129146] itr=81, itrs=2000, Progress: 4.05%
[36m[2023-06-25 01:59:10,772][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 01:59:10,772][129146] FPS: 331442.91
[36m[2023-06-25 01:59:15,642][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:59:15,643][129146] Reward + Measures: [[-246.04618398    0.52623504    0.1628761     0.39315259    0.19460665]]
[37m[1m[2023-06-25 01:59:15,643][129146] Max Reward on eval: -246.0461839833918
[37m[1m[2023-06-25 01:59:15,643][129146] Min Reward on eval: -246.0461839833918
[37m[1m[2023-06-25 01:59:15,643][129146] Mean Reward across all agents: -246.0461839833918
[37m[1m[2023-06-25 01:59:15,643][129146] Average Trajectory Length: 963.3786666666666
[36m[2023-06-25 01:59:21,119][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:59:21,120][129146] Reward + Measures: [[-250.81864375    0.50155389    0.17722309    0.35839233    0.19547693]
[37m[1m [-316.69987798    0.57896984    0.16752265    0.42828494    0.20827925]
[37m[1m [-321.8867226     0.55340308    0.1744522     0.41627607    0.2184258 ]
[37m[1m ...
[37m[1m [-298.47730487    0.52535313    0.20062689    0.38531238    0.22186318]
[37m[1m [-349.72758336    0.58610004    0.13470002    0.44660002    0.199     ]
[37m[1m [-346.53033373    0.63012451    0.17042246    0.49365923    0.22772245]]
[37m[1m[2023-06-25 01:59:21,120][129146] Max Reward on eval: -110.17974543194286
[37m[1m[2023-06-25 01:59:21,120][129146] Min Reward on eval: -510.9075355608249
[37m[1m[2023-06-25 01:59:21,120][129146] Mean Reward across all agents: -304.58492456308437
[37m[1m[2023-06-25 01:59:21,121][129146] Average Trajectory Length: 949.4943333333333
[36m[2023-06-25 01:59:21,124][129146] mean_value=-27.32517473693835, max_value=267.63527763763807
[37m[1m[2023-06-25 01:59:21,127][129146] New mean coefficients: [[-1.1880195   0.7155164   0.91243935  1.3762425   1.5273206 ]]
[37m[1m[2023-06-25 01:59:21,127][129146] Moving the mean solution point...
[36m[2023-06-25 01:59:30,889][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 01:59:30,889][129146] FPS: 393460.43
[36m[2023-06-25 01:59:30,891][129146] itr=82, itrs=2000, Progress: 4.10%
[36m[2023-06-25 01:59:42,441][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 01:59:42,441][129146] FPS: 332888.25
[36m[2023-06-25 01:59:47,158][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:59:47,159][129146] Reward + Measures: [[-507.3063946     0.63796836    0.12851571    0.51870328    0.20866962]]
[37m[1m[2023-06-25 01:59:47,159][129146] Max Reward on eval: -507.30639459940744
[37m[1m[2023-06-25 01:59:47,159][129146] Min Reward on eval: -507.30639459940744
[37m[1m[2023-06-25 01:59:47,159][129146] Mean Reward across all agents: -507.30639459940744
[37m[1m[2023-06-25 01:59:47,159][129146] Average Trajectory Length: 981.6203333333333
[36m[2023-06-25 01:59:52,514][129146] Finished Evaluation Step
[37m[1m[2023-06-25 01:59:52,515][129146] Reward + Measures: [[-549.1609552     0.69200003    0.12820001    0.59650004    0.22480002]
[37m[1m [-684.57188598    0.71040004    0.0993        0.62550002    0.25119999]
[37m[1m [-859.63319209    0.76870006    0.0683        0.72760004    0.40130001]
[37m[1m ...
[37m[1m [-608.61904801    0.62629998    0.0911        0.55840003    0.22879998]
[37m[1m [-379.8695887     0.5068        0.1284        0.38080001    0.18450001]
[37m[1m [-629.57615947    0.63467503    0.08327166    0.60368663    0.27903175]]
[37m[1m[2023-06-25 01:59:52,515][129146] Max Reward on eval: -271.10084581517293
[37m[1m[2023-06-25 01:59:52,515][129146] Min Reward on eval: -859.6331920870056
[37m[1m[2023-06-25 01:59:52,516][129146] Mean Reward across all agents: -597.0248759331935
[37m[1m[2023-06-25 01:59:52,516][129146] Average Trajectory Length: 974.5973333333333
[36m[2023-06-25 01:59:52,519][129146] mean_value=-144.75870442451912, max_value=103.21599417598509
[37m[1m[2023-06-25 01:59:52,522][129146] New mean coefficients: [[-1.3281912   0.17143196  1.3139791   1.2181267   1.400688  ]]
[37m[1m[2023-06-25 01:59:52,523][129146] Moving the mean solution point...
[36m[2023-06-25 02:00:02,233][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 02:00:02,233][129146] FPS: 395533.65
[36m[2023-06-25 02:00:02,236][129146] itr=83, itrs=2000, Progress: 4.15%
[36m[2023-06-25 02:00:13,695][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 02:00:13,695][129146] FPS: 335518.92
[36m[2023-06-25 02:00:18,388][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:00:18,388][129146] Reward + Measures: [[-548.06121568    0.66016054    0.12241156    0.55441552    0.21781588]]
[37m[1m[2023-06-25 02:00:18,389][129146] Max Reward on eval: -548.0612156826438
[37m[1m[2023-06-25 02:00:18,389][129146] Min Reward on eval: -548.0612156826438
[37m[1m[2023-06-25 02:00:18,389][129146] Mean Reward across all agents: -548.0612156826438
[37m[1m[2023-06-25 02:00:18,389][129146] Average Trajectory Length: 983.0633333333333
[36m[2023-06-25 02:00:23,882][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:00:23,883][129146] Reward + Measures: [[-711.88229511    0.77240002    0.089         0.69530004    0.2775    ]
[37m[1m [-445.02002544    0.56569999    0.1604        0.44660002    0.21159999]
[37m[1m [-484.82385819    0.57924974    0.11078379    0.48450407    0.22545381]
[37m[1m ...
[37m[1m [-664.68703091    0.76510006    0.10089999    0.69          0.24940002]
[37m[1m [-559.34947297    0.62040001    0.16150001    0.51599997    0.24949999]
[37m[1m [-645.8700258     0.65350002    0.13569999    0.53850001    0.23450001]]
[37m[1m[2023-06-25 02:00:23,883][129146] Max Reward on eval: -276.7897163535381
[37m[1m[2023-06-25 02:00:23,883][129146] Min Reward on eval: -974.9299113677698
[37m[1m[2023-06-25 02:00:23,883][129146] Mean Reward across all agents: -636.0077327721186
[37m[1m[2023-06-25 02:00:23,884][129146] Average Trajectory Length: 976.0866666666666
[36m[2023-06-25 02:00:23,886][129146] mean_value=-204.48539896689073, max_value=69.18405277382533
[37m[1m[2023-06-25 02:00:23,888][129146] New mean coefficients: [[-1.4587659  -0.19668308  1.0874858   0.9107282   0.9837941 ]]
[37m[1m[2023-06-25 02:00:23,889][129146] Moving the mean solution point...
[36m[2023-06-25 02:00:33,527][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 02:00:33,527][129146] FPS: 398505.08
[36m[2023-06-25 02:00:33,529][129146] itr=84, itrs=2000, Progress: 4.20%
[36m[2023-06-25 02:00:45,046][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 02:00:45,046][129146] FPS: 333846.95
[36m[2023-06-25 02:00:49,919][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:00:49,919][129146] Reward + Measures: [[-796.6940219     0.78058058    0.08117179    0.72505176    0.29496053]]
[37m[1m[2023-06-25 02:00:49,919][129146] Max Reward on eval: -796.6940218978586
[37m[1m[2023-06-25 02:00:49,919][129146] Min Reward on eval: -796.6940218978586
[37m[1m[2023-06-25 02:00:49,920][129146] Mean Reward across all agents: -796.6940218978586
[37m[1m[2023-06-25 02:00:49,920][129146] Average Trajectory Length: 987.0616666666666
[36m[2023-06-25 02:00:55,410][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:00:55,411][129146] Reward + Measures: [[-430.95389943    0.56900001    0.11369999    0.51520008    0.25040004]
[37m[1m [-826.98202867    0.83109999    0.0573        0.78350008    0.31309998]
[37m[1m [-918.78201171    0.86079997    0.052         0.82770008    0.41089997]
[37m[1m ...
[37m[1m [-638.41991629    0.73899996    0.1128        0.66260004    0.23819999]
[37m[1m [-736.65934726    0.77147734    0.09226289    0.71826929    0.29843163]
[37m[1m [-964.04017785    0.82740003    0.0517        0.8035        0.44390002]]
[37m[1m[2023-06-25 02:00:55,411][129146] Max Reward on eval: -430.95389943184273
[37m[1m[2023-06-25 02:00:55,411][129146] Min Reward on eval: -1181.982176547288
[37m[1m[2023-06-25 02:00:55,411][129146] Mean Reward across all agents: -817.6592365589132
[37m[1m[2023-06-25 02:00:55,412][129146] Average Trajectory Length: 981.7803333333333
[36m[2023-06-25 02:00:55,414][129146] mean_value=-320.41604548551385, max_value=62.16496937181296
[37m[1m[2023-06-25 02:00:55,416][129146] New mean coefficients: [[-0.4439386  -0.45553175  1.6242652   0.43785766  1.5034021 ]]
[37m[1m[2023-06-25 02:00:55,417][129146] Moving the mean solution point...
[36m[2023-06-25 02:01:05,276][129146] train() took 9.86 seconds to complete
[36m[2023-06-25 02:01:05,276][129146] FPS: 389569.64
[36m[2023-06-25 02:01:05,278][129146] itr=85, itrs=2000, Progress: 4.25%
[36m[2023-06-25 02:01:16,744][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 02:01:16,744][129146] FPS: 335347.25
[36m[2023-06-25 02:01:21,665][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:01:21,665][129146] Reward + Measures: [[-1009.81848079     0.86945564     0.03996163     0.84512663
[37m[1m      0.40689659]]
[37m[1m[2023-06-25 02:01:21,666][129146] Max Reward on eval: -1009.818480790752
[37m[1m[2023-06-25 02:01:21,666][129146] Min Reward on eval: -1009.818480790752
[37m[1m[2023-06-25 02:01:21,666][129146] Mean Reward across all agents: -1009.818480790752
[37m[1m[2023-06-25 02:01:21,666][129146] Average Trajectory Length: 994.615
[36m[2023-06-25 02:01:27,362][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:01:27,362][129146] Reward + Measures: [[-1066.46312342     0.90399998     0.033          0.88390011
[37m[1m      0.45080003]
[37m[1m [ -819.33129719     0.84156752     0.05394595     0.80310363
[37m[1m      0.44478646]
[37m[1m [ -859.25672698     0.86009997     0.0489         0.81900007
[37m[1m      0.37400001]
[37m[1m ...
[37m[1m [ -649.19731446     0.73710001     0.0999         0.65539998
[37m[1m      0.26440001]
[37m[1m [ -870.05158352     0.83939999     0.06600001     0.78960001
[37m[1m      0.32480001]
[37m[1m [ -909.68603311     0.83642751     0.06668407     0.80148411
[37m[1m      0.41133767]]
[37m[1m[2023-06-25 02:01:27,362][129146] Max Reward on eval: -292.3879820811679
[37m[1m[2023-06-25 02:01:27,363][129146] Min Reward on eval: -1099.033747697505
[37m[1m[2023-06-25 02:01:27,363][129146] Mean Reward across all agents: -770.1134776114475
[37m[1m[2023-06-25 02:01:27,363][129146] Average Trajectory Length: 980.8456666666666
[36m[2023-06-25 02:01:27,366][129146] mean_value=-283.2161766766665, max_value=49.978344076032386
[37m[1m[2023-06-25 02:01:27,369][129146] New mean coefficients: [[ 0.15932804 -0.75242424  2.5879996   0.7557159   1.5707368 ]]
[37m[1m[2023-06-25 02:01:27,370][129146] Moving the mean solution point...
[36m[2023-06-25 02:01:37,089][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 02:01:37,089][129146] FPS: 395189.56
[36m[2023-06-25 02:01:37,091][129146] itr=86, itrs=2000, Progress: 4.30%
[36m[2023-06-25 02:01:48,733][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 02:01:48,733][129146] FPS: 330256.77
[36m[2023-06-25 02:01:53,612][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:01:53,612][129146] Reward + Measures: [[-1116.83033152     0.90904051     0.02400155     0.89018792
[37m[1m      0.49029177]]
[37m[1m[2023-06-25 02:01:53,612][129146] Max Reward on eval: -1116.8303315244
[37m[1m[2023-06-25 02:01:53,613][129146] Min Reward on eval: -1116.8303315244
[37m[1m[2023-06-25 02:01:53,613][129146] Mean Reward across all agents: -1116.8303315244
[37m[1m[2023-06-25 02:01:53,613][129146] Average Trajectory Length: 995.6023333333333
[36m[2023-06-25 02:01:59,014][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:01:59,015][129146] Reward + Measures: [[ -871.11146737     0.83070004     0.06750001     0.78850001
[37m[1m      0.3766    ]
[37m[1m [ -774.28076841     0.76770002     0.0846         0.70929998
[37m[1m      0.30399999]
[37m[1m [-1159.65618423     0.90320009     0.0364         0.86440003
[37m[1m      0.45090005]
[37m[1m ...
[37m[1m [-1012.22967241     0.92369998     0.0257         0.90939999
[37m[1m      0.57779998]
[37m[1m [-1045.73814568     0.82770008     0.0416         0.78249997
[37m[1m      0.37380001]
[37m[1m [-1091.50586562     0.88599998     0.0329         0.84419996
[37m[1m      0.39690003]]
[37m[1m[2023-06-25 02:01:59,015][129146] Max Reward on eval: -205.94442226220855
[37m[1m[2023-06-25 02:01:59,015][129146] Min Reward on eval: -1249.2952076256975
[37m[1m[2023-06-25 02:01:59,016][129146] Mean Reward across all agents: -833.8594688886558
[37m[1m[2023-06-25 02:01:59,016][129146] Average Trajectory Length: 990.1766666666666
[36m[2023-06-25 02:01:59,018][129146] mean_value=-347.58638739159073, max_value=124.91774603706983
[37m[1m[2023-06-25 02:01:59,020][129146] New mean coefficients: [[ 1.7975028  -0.64644265  3.3173995   0.6260908   1.564552  ]]
[37m[1m[2023-06-25 02:01:59,021][129146] Moving the mean solution point...
[36m[2023-06-25 02:02:08,894][129146] train() took 9.87 seconds to complete
[36m[2023-06-25 02:02:08,894][129146] FPS: 389025.93
[36m[2023-06-25 02:02:08,896][129146] itr=87, itrs=2000, Progress: 4.35%
[36m[2023-06-25 02:02:20,583][129146] train() took 11.67 seconds to complete
[36m[2023-06-25 02:02:20,583][129146] FPS: 328950.21
[36m[2023-06-25 02:02:25,478][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:02:25,478][129146] Reward + Measures: [[-430.18585436    0.64518464    0.12128967    0.56141603    0.23239359]]
[37m[1m[2023-06-25 02:02:25,478][129146] Max Reward on eval: -430.1858543635393
[37m[1m[2023-06-25 02:02:25,479][129146] Min Reward on eval: -430.1858543635393
[37m[1m[2023-06-25 02:02:25,479][129146] Mean Reward across all agents: -430.1858543635393
[37m[1m[2023-06-25 02:02:25,479][129146] Average Trajectory Length: 978.6719999999999
[36m[2023-06-25 02:02:31,021][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:02:31,021][129146] Reward + Measures: [[-175.14304677    0.4990367     0.18316796    0.3859888     0.20908804]
[37m[1m [-103.29174813    0.42928705    0.1810483     0.31532201    0.20277779]
[37m[1m [-259.66105756    0.57427537    0.16827956    0.4872511     0.23655605]
[37m[1m ...
[37m[1m [-293.0179779     0.51493853    0.14949426    0.44891867    0.21036711]
[37m[1m [-244.78280055    0.57450002    0.1769        0.46360001    0.2483    ]
[37m[1m [-273.07857675    0.53661948    0.12320941    0.47069198    0.21116646]]
[37m[1m[2023-06-25 02:02:31,021][129146] Max Reward on eval: 25.899211036157794
[37m[1m[2023-06-25 02:02:31,022][129146] Min Reward on eval: -537.3841918765568
[37m[1m[2023-06-25 02:02:31,022][129146] Mean Reward across all agents: -210.6237990353209
[37m[1m[2023-06-25 02:02:31,022][129146] Average Trajectory Length: 928.4053333333333
[36m[2023-06-25 02:02:31,028][129146] mean_value=34.3670475150305, max_value=405.3433886126324
[37m[1m[2023-06-25 02:02:31,030][129146] New mean coefficients: [[ 2.4562595  -0.51963025  3.9984174   0.6419582   1.6095217 ]]
[37m[1m[2023-06-25 02:02:31,031][129146] Moving the mean solution point...
[36m[2023-06-25 02:02:40,721][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 02:02:40,721][129146] FPS: 396360.12
[36m[2023-06-25 02:02:40,723][129146] itr=88, itrs=2000, Progress: 4.40%
[36m[2023-06-25 02:02:52,453][129146] train() took 11.72 seconds to complete
[36m[2023-06-25 02:02:52,454][129146] FPS: 327764.90
[36m[2023-06-25 02:02:57,213][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:02:57,214][129146] Reward + Measures: [[-349.27201922    0.58978277    0.14190611    0.4902519     0.22011144]]
[37m[1m[2023-06-25 02:02:57,214][129146] Max Reward on eval: -349.27201921549374
[37m[1m[2023-06-25 02:02:57,214][129146] Min Reward on eval: -349.27201921549374
[37m[1m[2023-06-25 02:02:57,215][129146] Mean Reward across all agents: -349.27201921549374
[37m[1m[2023-06-25 02:02:57,215][129146] Average Trajectory Length: 967.1373333333333
[36m[2023-06-25 02:03:02,705][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:03:02,706][129146] Reward + Measures: [[-188.21461492    0.40150315    0.22787817    0.26014522    0.21814452]
[37m[1m [-227.65035253    0.4032        0.1565        0.31290001    0.20559998]
[37m[1m [   7.90392453    0.28800002    0.13460001    0.17900001    0.1908    ]
[37m[1m ...
[37m[1m [-126.08212274    0.33812353    0.17065375    0.19909535    0.19672166]
[37m[1m [-221.88239229    0.38541779    0.16469437    0.27371335    0.22531405]
[37m[1m [ -93.67417841    0.37965021    0.18097198    0.23861444    0.18688719]]
[37m[1m[2023-06-25 02:03:02,706][129146] Max Reward on eval: 42.9760218150157
[37m[1m[2023-06-25 02:03:02,706][129146] Min Reward on eval: -654.2400417765136
[37m[1m[2023-06-25 02:03:02,706][129146] Mean Reward across all agents: -179.65467295561376
[37m[1m[2023-06-25 02:03:02,707][129146] Average Trajectory Length: 920.7869999999999
[36m[2023-06-25 02:03:02,709][129146] mean_value=-259.7650765638769, max_value=253.53064200003692
[37m[1m[2023-06-25 02:03:02,711][129146] New mean coefficients: [[ 1.7424114  -0.32979506  3.5555863   0.73396736  1.6062222 ]]
[37m[1m[2023-06-25 02:03:02,712][129146] Moving the mean solution point...
[36m[2023-06-25 02:03:12,451][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 02:03:12,451][129146] FPS: 394357.34
[36m[2023-06-25 02:03:12,453][129146] itr=89, itrs=2000, Progress: 4.45%
[36m[2023-06-25 02:03:24,037][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 02:03:24,037][129146] FPS: 331918.25
[36m[2023-06-25 02:03:28,844][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:03:28,844][129146] Reward + Measures: [[-249.40378323    0.5198496     0.15934075    0.41356891    0.21068558]]
[37m[1m[2023-06-25 02:03:28,845][129146] Max Reward on eval: -249.40378322835716
[37m[1m[2023-06-25 02:03:28,845][129146] Min Reward on eval: -249.40378322835716
[37m[1m[2023-06-25 02:03:28,845][129146] Mean Reward across all agents: -249.40378322835716
[37m[1m[2023-06-25 02:03:28,845][129146] Average Trajectory Length: 955.1276666666666
[36m[2023-06-25 02:03:34,433][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:03:34,434][129146] Reward + Measures: [[ -14.46324046    0.37606075    0.2611796     0.20428343    0.24022293]
[37m[1m [-305.59274424    0.57639998    0.17290001    0.47919998    0.25580001]
[37m[1m [-474.37308468    0.59059995    0.1593        0.47220001    0.24980001]
[37m[1m ...
[37m[1m [-145.66976795    0.42172286    0.17817461    0.31013963    0.20422743]
[37m[1m [-289.43114864    0.57826161    0.18538482    0.46852931    0.25495955]
[37m[1m [-156.38625335    0.53780001    0.2277        0.36190003    0.2289    ]]
[37m[1m[2023-06-25 02:03:34,434][129146] Max Reward on eval: 79.58465773076168
[37m[1m[2023-06-25 02:03:34,434][129146] Min Reward on eval: -599.1205666947993
[37m[1m[2023-06-25 02:03:34,435][129146] Mean Reward across all agents: -220.83518485126388
[37m[1m[2023-06-25 02:03:34,435][129146] Average Trajectory Length: 920.6893333333333
[36m[2023-06-25 02:03:34,438][129146] mean_value=-138.490693216857, max_value=329.6601141646807
[37m[1m[2023-06-25 02:03:34,440][129146] New mean coefficients: [[ 2.0668104  -0.14389206  3.0523825   1.202687    1.0765986 ]]
[37m[1m[2023-06-25 02:03:34,441][129146] Moving the mean solution point...
[36m[2023-06-25 02:03:44,157][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 02:03:44,157][129146] FPS: 395310.42
[36m[2023-06-25 02:03:44,159][129146] itr=90, itrs=2000, Progress: 4.50%
[37m[1m[2023-06-25 02:03:46,358][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000070
[36m[2023-06-25 02:03:58,270][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 02:03:58,270][129146] FPS: 331160.49
[36m[2023-06-25 02:04:03,019][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:04:03,019][129146] Reward + Measures: [[-195.89744193    0.48035279    0.16776918    0.37155777    0.20579596]]
[37m[1m[2023-06-25 02:04:03,020][129146] Max Reward on eval: -195.8974419271655
[37m[1m[2023-06-25 02:04:03,020][129146] Min Reward on eval: -195.8974419271655
[37m[1m[2023-06-25 02:04:03,020][129146] Mean Reward across all agents: -195.8974419271655
[37m[1m[2023-06-25 02:04:03,020][129146] Average Trajectory Length: 952.4146666666667
[36m[2023-06-25 02:04:08,407][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:04:08,408][129146] Reward + Measures: [[-164.0835026     0.48718396    0.18091162    0.34848097    0.21423297]
[37m[1m [  23.3041181     0.28789997    0.17307732    0.19391182    0.18518822]
[37m[1m [ -30.71454494    0.40681282    0.21056123    0.28762406    0.20669551]
[37m[1m ...
[37m[1m [ -19.04078895    0.35026979    0.19531684    0.22662747    0.20659795]
[37m[1m [ -74.49936577    0.33760005    0.16040002    0.2314        0.20539999]
[37m[1m [ -83.03350723    0.4666        0.1706        0.34600002    0.2208    ]]
[37m[1m[2023-06-25 02:04:08,408][129146] Max Reward on eval: 72.80641595855123
[37m[1m[2023-06-25 02:04:08,408][129146] Min Reward on eval: -400.0020813123207
[37m[1m[2023-06-25 02:04:08,409][129146] Mean Reward across all agents: -107.82821010632001
[37m[1m[2023-06-25 02:04:08,409][129146] Average Trajectory Length: 950.9446666666666
[36m[2023-06-25 02:04:08,412][129146] mean_value=-46.24049637094609, max_value=253.23475479983864
[37m[1m[2023-06-25 02:04:08,415][129146] New mean coefficients: [[2.5041676 0.0708154 3.2541165 1.4964273 1.443296 ]]
[37m[1m[2023-06-25 02:04:08,416][129146] Moving the mean solution point...
[36m[2023-06-25 02:04:18,077][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 02:04:18,077][129146] FPS: 397537.95
[36m[2023-06-25 02:04:18,080][129146] itr=91, itrs=2000, Progress: 4.55%
[36m[2023-06-25 02:04:29,560][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 02:04:29,560][129146] FPS: 334858.90
[36m[2023-06-25 02:04:34,390][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:04:34,390][129146] Reward + Measures: [[-136.11136681    0.45518699    0.17135331    0.34337997    0.20401309]]
[37m[1m[2023-06-25 02:04:34,390][129146] Max Reward on eval: -136.11136680871573
[37m[1m[2023-06-25 02:04:34,391][129146] Min Reward on eval: -136.11136680871573
[37m[1m[2023-06-25 02:04:34,391][129146] Mean Reward across all agents: -136.11136680871573
[37m[1m[2023-06-25 02:04:34,391][129146] Average Trajectory Length: 955.616
[36m[2023-06-25 02:04:39,873][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:04:39,873][129146] Reward + Measures: [[-135.05563257    0.48569998    0.21500002    0.36290002    0.22070001]
[37m[1m [ -92.63384177    0.49527436    0.21634924    0.34035465    0.21907528]
[37m[1m [ -72.06137615    0.4522        0.1869        0.3418        0.21390001]
[37m[1m ...
[37m[1m [  -6.78052553    0.45725271    0.21467434    0.32787704    0.21612433]
[37m[1m [ -61.31957985    0.43239999    0.17730001    0.30899999    0.2124    ]
[37m[1m [ -29.20363876    0.33020002    0.16000001    0.2244        0.1786    ]]
[37m[1m[2023-06-25 02:04:39,873][129146] Max Reward on eval: 103.38632227816561
[37m[1m[2023-06-25 02:04:39,874][129146] Min Reward on eval: -347.98179420784584
[37m[1m[2023-06-25 02:04:39,874][129146] Mean Reward across all agents: -73.73731056976561
[37m[1m[2023-06-25 02:04:39,874][129146] Average Trajectory Length: 965.6563333333334
[36m[2023-06-25 02:04:39,877][129146] mean_value=-48.32272269631427, max_value=471.2255411409277
[37m[1m[2023-06-25 02:04:39,880][129146] New mean coefficients: [[3.6665251 0.2641455 3.5108194 1.524781  1.1685694]]
[37m[1m[2023-06-25 02:04:39,881][129146] Moving the mean solution point...
[36m[2023-06-25 02:04:49,630][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 02:04:49,630][129146] FPS: 393947.37
[36m[2023-06-25 02:04:49,632][129146] itr=92, itrs=2000, Progress: 4.60%
[36m[2023-06-25 02:05:01,132][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 02:05:01,132][129146] FPS: 334340.10
[36m[2023-06-25 02:05:06,022][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:05:06,023][129146] Reward + Measures: [[-97.53116969   0.43547371   0.17020603   0.32116097   0.20019157]]
[37m[1m[2023-06-25 02:05:06,023][129146] Max Reward on eval: -97.5311696877818
[37m[1m[2023-06-25 02:05:06,023][129146] Min Reward on eval: -97.5311696877818
[37m[1m[2023-06-25 02:05:06,023][129146] Mean Reward across all agents: -97.5311696877818
[37m[1m[2023-06-25 02:05:06,023][129146] Average Trajectory Length: 955.249
[36m[2023-06-25 02:05:11,573][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:05:11,574][129146] Reward + Measures: [[-136.70406419    0.5072        0.22059999    0.37059999    0.21660002]
[37m[1m [ -28.87636642    0.3874        0.21869998    0.24850002    0.22020002]
[37m[1m [  44.96610293    0.34300002    0.1602        0.22740002    0.1937    ]
[37m[1m ...
[37m[1m [-139.47870151    0.44280002    0.1824        0.3355        0.21329999]
[37m[1m [ 108.8568515     0.34731415    0.17595445    0.22484294    0.21106911]
[37m[1m [-112.80202061    0.48801097    0.21111093    0.35483727    0.22063375]]
[37m[1m[2023-06-25 02:05:11,574][129146] Max Reward on eval: 205.4341348252463
[37m[1m[2023-06-25 02:05:11,574][129146] Min Reward on eval: -222.03391469204797
[37m[1m[2023-06-25 02:05:11,574][129146] Mean Reward across all agents: -10.383143437036324
[37m[1m[2023-06-25 02:05:11,575][129146] Average Trajectory Length: 956.5803333333333
[36m[2023-06-25 02:05:11,578][129146] mean_value=-116.11562779455133, max_value=297.3422306278402
[37m[1m[2023-06-25 02:05:11,580][129146] New mean coefficients: [[3.6220787 0.2380203 3.0916257 1.5911765 1.190274 ]]
[37m[1m[2023-06-25 02:05:11,581][129146] Moving the mean solution point...
[36m[2023-06-25 02:05:21,517][129146] train() took 9.93 seconds to complete
[36m[2023-06-25 02:05:21,518][129146] FPS: 386520.51
[36m[2023-06-25 02:05:21,520][129146] itr=93, itrs=2000, Progress: 4.65%
[36m[2023-06-25 02:05:33,138][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 02:05:33,138][129146] FPS: 330878.88
[36m[2023-06-25 02:05:38,103][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:05:38,103][129146] Reward + Measures: [[-33.68882141   0.41745564   0.17981435   0.29584038   0.19914323]]
[37m[1m[2023-06-25 02:05:38,103][129146] Max Reward on eval: -33.68882141288425
[37m[1m[2023-06-25 02:05:38,104][129146] Min Reward on eval: -33.68882141288425
[37m[1m[2023-06-25 02:05:38,104][129146] Mean Reward across all agents: -33.68882141288425
[37m[1m[2023-06-25 02:05:38,104][129146] Average Trajectory Length: 958.698
[36m[2023-06-25 02:05:43,600][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:05:43,600][129146] Reward + Measures: [[ -0.96128175   0.36789313   0.30789676   0.18345037   0.23046489]
[37m[1m [-24.02629618   0.43526259   0.28298458   0.29586586   0.24584226]
[37m[1m [-15.60008604   0.40109998   0.19000001   0.30410001   0.2053    ]
[37m[1m ...
[37m[1m [ 42.02151019   0.32038897   0.23015283   0.18587275   0.21970713]
[37m[1m [124.48685426   0.35101032   0.26453865   0.19998376   0.21486607]
[37m[1m [-20.35041557   0.38212729   0.26325455   0.23014848   0.23110302]]
[37m[1m[2023-06-25 02:05:43,600][129146] Max Reward on eval: 172.66671433295704
[37m[1m[2023-06-25 02:05:43,601][129146] Min Reward on eval: -227.65433284853643
[37m[1m[2023-06-25 02:05:43,601][129146] Mean Reward across all agents: 37.49689772583103
[37m[1m[2023-06-25 02:05:43,601][129146] Average Trajectory Length: 949.505
[36m[2023-06-25 02:05:43,603][129146] mean_value=-246.43991050455844, max_value=282.4505784260793
[37m[1m[2023-06-25 02:05:43,606][129146] New mean coefficients: [[3.171434   0.24352065 3.2748532  2.332177   1.4966111 ]]
[37m[1m[2023-06-25 02:05:43,607][129146] Moving the mean solution point...
[36m[2023-06-25 02:05:53,431][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 02:05:53,431][129146] FPS: 390937.63
[36m[2023-06-25 02:05:53,433][129146] itr=94, itrs=2000, Progress: 4.70%
[36m[2023-06-25 02:06:04,930][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 02:06:04,930][129146] FPS: 334421.78
[36m[2023-06-25 02:06:09,741][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:06:09,741][129146] Reward + Measures: [[11.82334456  0.40289453  0.17955029  0.27913538  0.19919699]]
[37m[1m[2023-06-25 02:06:09,741][129146] Max Reward on eval: 11.823344561749424
[37m[1m[2023-06-25 02:06:09,742][129146] Min Reward on eval: 11.823344561749424
[37m[1m[2023-06-25 02:06:09,742][129146] Mean Reward across all agents: 11.823344561749424
[37m[1m[2023-06-25 02:06:09,742][129146] Average Trajectory Length: 953.5976666666667
[36m[2023-06-25 02:06:15,303][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:06:15,304][129146] Reward + Measures: [[ 80.17626788   0.40472579   0.1954309    0.26217934   0.19693539]
[37m[1m [163.28641133   0.26690003   0.1974       0.1699       0.17040001]
[37m[1m [130.05421498   0.27015969   0.21039401   0.1463484    0.2008379 ]
[37m[1m ...
[37m[1m [ 30.98397333   0.36055562   0.22314501   0.23410586   0.21108997]
[37m[1m [ 56.96458951   0.29128382   0.21747516   0.1937172    0.20902677]
[37m[1m [132.29756631   0.41119051   0.24002342   0.26211476   0.22297302]]
[37m[1m[2023-06-25 02:06:15,304][129146] Max Reward on eval: 192.74121131903084
[37m[1m[2023-06-25 02:06:15,304][129146] Min Reward on eval: -122.67921287732607
[37m[1m[2023-06-25 02:06:15,304][129146] Mean Reward across all agents: 70.15929235976024
[37m[1m[2023-06-25 02:06:15,305][129146] Average Trajectory Length: 931.3829999999999
[36m[2023-06-25 02:06:15,306][129146] mean_value=-327.678352213313, max_value=125.77630216368517
[37m[1m[2023-06-25 02:06:15,309][129146] New mean coefficients: [[3.003175  0.250595  2.4000528 2.5534992 0.9625653]]
[37m[1m[2023-06-25 02:06:15,310][129146] Moving the mean solution point...
[36m[2023-06-25 02:06:25,022][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 02:06:25,023][129146] FPS: 395420.17
[36m[2023-06-25 02:06:25,025][129146] itr=95, itrs=2000, Progress: 4.75%
[36m[2023-06-25 02:06:36,424][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 02:06:36,424][129146] FPS: 337241.28
[36m[2023-06-25 02:06:41,179][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:06:41,180][129146] Reward + Measures: [[40.38413047  0.41572261  0.18729597  0.28627515  0.20374438]]
[37m[1m[2023-06-25 02:06:41,180][129146] Max Reward on eval: 40.38413047424983
[37m[1m[2023-06-25 02:06:41,180][129146] Min Reward on eval: 40.38413047424983
[37m[1m[2023-06-25 02:06:41,180][129146] Mean Reward across all agents: 40.38413047424983
[37m[1m[2023-06-25 02:06:41,181][129146] Average Trajectory Length: 959.3576666666667
[36m[2023-06-25 02:06:46,592][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:06:46,593][129146] Reward + Measures: [[ 33.43958314   0.38884363   0.22199523   0.24727857   0.17535159]
[37m[1m [ 60.37267601   0.3723       0.19140001   0.25070003   0.20700002]
[37m[1m [  6.72411794   0.43010002   0.1937       0.30679998   0.184     ]
[37m[1m ...
[37m[1m [ 30.51189143   0.40196109   0.20242639   0.26210627   0.21206303]
[37m[1m [ 32.27646611   0.38274118   0.18411766   0.25705293   0.20122941]
[37m[1m [131.29943583   0.2702789    0.16890551   0.14722477   0.16800642]]
[37m[1m[2023-06-25 02:06:46,593][129146] Max Reward on eval: 188.79345011875267
[37m[1m[2023-06-25 02:06:46,593][129146] Min Reward on eval: -114.08014363865077
[37m[1m[2023-06-25 02:06:46,593][129146] Mean Reward across all agents: 51.6175987335181
[37m[1m[2023-06-25 02:06:46,594][129146] Average Trajectory Length: 956.4343333333333
[36m[2023-06-25 02:06:46,597][129146] mean_value=-142.9714511632787, max_value=357.8594666369462
[37m[1m[2023-06-25 02:06:46,599][129146] New mean coefficients: [[2.4339879  0.2990321  2.0077586  2.7931674  0.30281883]]
[37m[1m[2023-06-25 02:06:46,600][129146] Moving the mean solution point...
[36m[2023-06-25 02:06:56,278][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 02:06:56,278][129146] FPS: 396847.44
[36m[2023-06-25 02:06:56,281][129146] itr=96, itrs=2000, Progress: 4.80%
[36m[2023-06-25 02:07:07,698][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 02:07:07,699][129146] FPS: 336743.09
[36m[2023-06-25 02:07:12,451][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:07:12,452][129146] Reward + Measures: [[87.89599629  0.41143635  0.19128618  0.28008774  0.20507836]]
[37m[1m[2023-06-25 02:07:12,452][129146] Max Reward on eval: 87.89599628665381
[37m[1m[2023-06-25 02:07:12,452][129146] Min Reward on eval: 87.89599628665381
[37m[1m[2023-06-25 02:07:12,452][129146] Mean Reward across all agents: 87.89599628665381
[37m[1m[2023-06-25 02:07:12,452][129146] Average Trajectory Length: 956.0126666666666
[36m[2023-06-25 02:07:17,875][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:07:17,876][129146] Reward + Measures: [[ -7.8906879    0.50110215   0.23787506   0.35161325   0.23021518]
[37m[1m [ 81.78520057   0.36631015   0.22437155   0.22220187   0.22194195]
[37m[1m [-33.47387373   0.47940001   0.18719999   0.33919999   0.20120001]
[37m[1m ...
[37m[1m [-63.47693941   0.53789997   0.1944       0.40920001   0.22049999]
[37m[1m [ 76.42944136   0.41389999   0.2261       0.2974       0.21069999]
[37m[1m [102.05242013   0.33000746   0.15434049   0.2310711    0.205981  ]]
[37m[1m[2023-06-25 02:07:17,876][129146] Max Reward on eval: 233.83864941377252
[37m[1m[2023-06-25 02:07:17,876][129146] Min Reward on eval: -287.7619079158292
[37m[1m[2023-06-25 02:07:17,876][129146] Mean Reward across all agents: 43.32498962407533
[37m[1m[2023-06-25 02:07:17,876][129146] Average Trajectory Length: 969.4496666666666
[36m[2023-06-25 02:07:17,880][129146] mean_value=-81.05566067875941, max_value=358.6114375069452
[37m[1m[2023-06-25 02:07:17,883][129146] New mean coefficients: [[2.0665574  0.21696483 1.3929565  3.20292    0.11569038]]
[37m[1m[2023-06-25 02:07:17,884][129146] Moving the mean solution point...
[36m[2023-06-25 02:07:27,582][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 02:07:27,582][129146] FPS: 396032.58
[36m[2023-06-25 02:07:27,584][129146] itr=97, itrs=2000, Progress: 4.85%
[36m[2023-06-25 02:07:39,174][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 02:07:39,174][129146] FPS: 331693.98
[36m[2023-06-25 02:07:43,961][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:07:43,962][129146] Reward + Measures: [[112.84794769   0.43485382   0.19289319   0.29904053   0.20898645]]
[37m[1m[2023-06-25 02:07:43,962][129146] Max Reward on eval: 112.84794768710395
[37m[1m[2023-06-25 02:07:43,962][129146] Min Reward on eval: 112.84794768710395
[37m[1m[2023-06-25 02:07:43,962][129146] Mean Reward across all agents: 112.84794768710395
[37m[1m[2023-06-25 02:07:43,963][129146] Average Trajectory Length: 969.063
[36m[2023-06-25 02:07:49,435][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:07:49,436][129146] Reward + Measures: [[167.00123775   0.454        0.20750001   0.2931       0.21510001]
[37m[1m [196.087089     0.41578254   0.20002222   0.25226191   0.19462381]
[37m[1m [117.73518357   0.39250001   0.22850001   0.27710003   0.22400001]
[37m[1m ...
[37m[1m [221.31227953   0.3321       0.1753       0.21870001   0.21789999]
[37m[1m [ 47.02045781   0.53470004   0.21110001   0.37939999   0.2174    ]
[37m[1m [108.85030374   0.47372466   0.21604204   0.3092348    0.25693044]]
[37m[1m[2023-06-25 02:07:49,436][129146] Max Reward on eval: 367.7371100494638
[37m[1m[2023-06-25 02:07:49,436][129146] Min Reward on eval: -172.55550753131973
[37m[1m[2023-06-25 02:07:49,437][129146] Mean Reward across all agents: 135.9012544390712
[37m[1m[2023-06-25 02:07:49,437][129146] Average Trajectory Length: 973.2546666666666
[36m[2023-06-25 02:07:49,442][129146] mean_value=14.93028424999778, max_value=640.3784389275067
[37m[1m[2023-06-25 02:07:49,445][129146] New mean coefficients: [[ 1.7566122   0.25693318  1.3781334   3.4593635  -0.03564993]]
[37m[1m[2023-06-25 02:07:49,446][129146] Moving the mean solution point...
[36m[2023-06-25 02:07:59,251][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 02:07:59,251][129146] FPS: 391717.68
[36m[2023-06-25 02:07:59,253][129146] itr=98, itrs=2000, Progress: 4.90%
[36m[2023-06-25 02:08:10,817][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 02:08:10,817][129146] FPS: 332513.63
[36m[2023-06-25 02:08:15,577][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:08:15,577][129146] Reward + Measures: [[102.03635746   0.4636392    0.19119041   0.33106065   0.20991622]]
[37m[1m[2023-06-25 02:08:15,578][129146] Max Reward on eval: 102.03635746162264
[37m[1m[2023-06-25 02:08:15,578][129146] Min Reward on eval: 102.03635746162264
[37m[1m[2023-06-25 02:08:15,578][129146] Mean Reward across all agents: 102.03635746162264
[37m[1m[2023-06-25 02:08:15,578][129146] Average Trajectory Length: 977.371
[36m[2023-06-25 02:08:21,156][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:08:21,157][129146] Reward + Measures: [[ -43.42153441    0.54210001    0.21860002    0.4224        0.23190001]
[37m[1m [-224.76139744    0.69230002    0.16600001    0.5941        0.22360002]
[37m[1m [  73.87650914    0.52200001    0.1997        0.39410001    0.21949999]
[37m[1m ...
[37m[1m [  96.63028415    0.46300003    0.21239999    0.35249999    0.1996    ]
[37m[1m [  20.57384821    0.52450001    0.19609998    0.41640002    0.21760002]
[37m[1m [  23.48626331    0.55970001    0.16440001    0.45240003    0.21200001]]
[37m[1m[2023-06-25 02:08:21,157][129146] Max Reward on eval: 327.07627690248194
[37m[1m[2023-06-25 02:08:21,157][129146] Min Reward on eval: -224.76139744116517
[37m[1m[2023-06-25 02:08:21,158][129146] Mean Reward across all agents: 82.24958036015953
[37m[1m[2023-06-25 02:08:21,158][129146] Average Trajectory Length: 978.2719999999999
[36m[2023-06-25 02:08:21,163][129146] mean_value=50.033972225528125, max_value=561.568798945143
[37m[1m[2023-06-25 02:08:21,166][129146] New mean coefficients: [[ 1.6274606   0.10276513  0.39988244  3.5186293  -0.58758986]]
[37m[1m[2023-06-25 02:08:21,167][129146] Moving the mean solution point...
[36m[2023-06-25 02:08:30,982][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 02:08:30,982][129146] FPS: 391320.72
[36m[2023-06-25 02:08:30,984][129146] itr=99, itrs=2000, Progress: 4.95%
[36m[2023-06-25 02:08:42,645][129146] train() took 11.65 seconds to complete
[36m[2023-06-25 02:08:42,645][129146] FPS: 329685.23
[36m[2023-06-25 02:08:47,590][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:08:47,590][129146] Reward + Measures: [[128.49082766   0.48057878   0.19386132   0.34492204   0.21189164]]
[37m[1m[2023-06-25 02:08:47,590][129146] Max Reward on eval: 128.4908276648587
[37m[1m[2023-06-25 02:08:47,591][129146] Min Reward on eval: 128.4908276648587
[37m[1m[2023-06-25 02:08:47,591][129146] Mean Reward across all agents: 128.4908276648587
[37m[1m[2023-06-25 02:08:47,591][129146] Average Trajectory Length: 976.8979999999999
[36m[2023-06-25 02:08:53,112][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:08:53,112][129146] Reward + Measures: [[ 25.78421241   0.46410003   0.19860001   0.35409999   0.22669998]
[37m[1m [ 63.98499421   0.49589998   0.20299999   0.3626       0.23029999]
[37m[1m [188.4248901    0.34426129   0.1778301    0.23472194   0.20111568]
[37m[1m ...
[37m[1m [-21.38081675   0.50941581   0.21984215   0.38063768   0.25026295]
[37m[1m [-80.46138905   0.49949357   0.2026484    0.36901289   0.23012257]
[37m[1m [ 96.00616393   0.41980001   0.19430001   0.273        0.21930002]]
[37m[1m[2023-06-25 02:08:53,112][129146] Max Reward on eval: 239.69756812438283
[37m[1m[2023-06-25 02:08:53,113][129146] Min Reward on eval: -319.23794838223
[37m[1m[2023-06-25 02:08:53,113][129146] Mean Reward across all agents: 39.870743077565955
[37m[1m[2023-06-25 02:08:53,113][129146] Average Trajectory Length: 972.7783333333333
[36m[2023-06-25 02:08:53,115][129146] mean_value=-91.39496522799432, max_value=220.5500045998928
[37m[1m[2023-06-25 02:08:53,118][129146] New mean coefficients: [[ 0.94487214  0.44070983 -0.19002676  3.1650128  -0.47639054]]
[37m[1m[2023-06-25 02:08:53,119][129146] Moving the mean solution point...
[36m[2023-06-25 02:09:03,039][129146] train() took 9.92 seconds to complete
[36m[2023-06-25 02:09:03,040][129146] FPS: 387153.94
[36m[2023-06-25 02:09:03,042][129146] itr=100, itrs=2000, Progress: 5.00%
[37m[1m[2023-06-25 02:09:05,260][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000080
[36m[2023-06-25 02:09:17,310][129146] train() took 11.73 seconds to complete
[36m[2023-06-25 02:09:17,311][129146] FPS: 327434.70
[36m[2023-06-25 02:09:22,206][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:09:22,206][129146] Reward + Measures: [[66.70792828  0.53919083  0.18263434  0.41364896  0.21448274]]
[37m[1m[2023-06-25 02:09:22,206][129146] Max Reward on eval: 66.70792828479945
[37m[1m[2023-06-25 02:09:22,207][129146] Min Reward on eval: 66.70792828479945
[37m[1m[2023-06-25 02:09:22,207][129146] Mean Reward across all agents: 66.70792828479945
[37m[1m[2023-06-25 02:09:22,207][129146] Average Trajectory Length: 981.8766666666667
[36m[2023-06-25 02:09:27,711][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:09:27,711][129146] Reward + Measures: [[ -10.19191314    0.50129998    0.1648        0.37970001    0.2191    ]
[37m[1m [-123.22675284    0.62370008    0.1928        0.51389998    0.22150002]
[37m[1m [ -56.36868526    0.51470006    0.1868        0.41890001    0.2141    ]
[37m[1m ...
[37m[1m [ -35.61393604    0.55649024    0.22106829    0.43647319    0.22853418]
[37m[1m [  48.22860714    0.55459994    0.1851        0.43000004    0.223     ]
[37m[1m [  86.85928185    0.47709998    0.18609999    0.35600001    0.21069999]]
[37m[1m[2023-06-25 02:09:27,712][129146] Max Reward on eval: 141.55107418064145
[37m[1m[2023-06-25 02:09:27,712][129146] Min Reward on eval: -164.31085401963793
[37m[1m[2023-06-25 02:09:27,712][129146] Mean Reward across all agents: -24.545996476338328
[37m[1m[2023-06-25 02:09:27,712][129146] Average Trajectory Length: 987.6863333333333
[36m[2023-06-25 02:09:27,716][129146] mean_value=0.1846780107726651, max_value=463.0628894955924
[37m[1m[2023-06-25 02:09:27,718][129146] New mean coefficients: [[ 0.36531603  0.4045397  -0.66490245  2.8616948  -0.8214243 ]]
[37m[1m[2023-06-25 02:09:27,719][129146] Moving the mean solution point...
[36m[2023-06-25 02:09:37,441][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 02:09:37,441][129146] FPS: 395062.39
[36m[2023-06-25 02:09:37,444][129146] itr=101, itrs=2000, Progress: 5.05%
[36m[2023-06-25 02:09:48,864][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 02:09:48,865][129146] FPS: 336598.81
[36m[2023-06-25 02:09:53,656][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:09:53,657][129146] Reward + Measures: [[-2.74325304  0.62187797  0.16299924  0.51511908  0.22615799]]
[37m[1m[2023-06-25 02:09:53,657][129146] Max Reward on eval: -2.7432530410337326
[37m[1m[2023-06-25 02:09:53,657][129146] Min Reward on eval: -2.7432530410337326
[37m[1m[2023-06-25 02:09:53,657][129146] Mean Reward across all agents: -2.7432530410337326
[37m[1m[2023-06-25 02:09:53,657][129146] Average Trajectory Length: 991.8223333333333
[36m[2023-06-25 02:09:59,224][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:09:59,225][129146] Reward + Measures: [[-200.0393618     0.73789996    0.1189        0.65110004    0.2712    ]
[37m[1m [-333.67103087    0.82240003    0.0859        0.77869999    0.36340004]
[37m[1m [-140.76050428    0.60149997    0.14670001    0.52100003    0.221     ]
[37m[1m ...
[37m[1m [-124.37055339    0.68209994    0.14669999    0.5891        0.22020002]
[37m[1m [-287.04271889    0.75550002    0.11099999    0.69769996    0.2167    ]
[37m[1m [-308.66698796    0.75840008    0.10179999    0.71220005    0.3175    ]]
[37m[1m[2023-06-25 02:09:59,225][129146] Max Reward on eval: 140.00425866352452
[37m[1m[2023-06-25 02:09:59,225][129146] Min Reward on eval: -594.8256122219958
[37m[1m[2023-06-25 02:09:59,226][129146] Mean Reward across all agents: -259.4859929054276
[37m[1m[2023-06-25 02:09:59,226][129146] Average Trajectory Length: 996.8746666666666
[36m[2023-06-25 02:09:59,235][129146] mean_value=142.64680573136727, max_value=397.6444394677743
[37m[1m[2023-06-25 02:09:59,238][129146] New mean coefficients: [[ 0.5844646   0.05874848 -0.42095563  2.2616265  -1.082612  ]]
[37m[1m[2023-06-25 02:09:59,239][129146] Moving the mean solution point...
[36m[2023-06-25 02:10:09,022][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 02:10:09,022][129146] FPS: 392593.94
[36m[2023-06-25 02:10:09,024][129146] itr=102, itrs=2000, Progress: 5.10%
[36m[2023-06-25 02:10:20,534][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 02:10:20,534][129146] FPS: 334048.96
[36m[2023-06-25 02:10:25,366][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:10:25,367][129146] Reward + Measures: [[-88.99968469   0.69203091   0.13565613   0.60627824   0.2421445 ]]
[37m[1m[2023-06-25 02:10:25,367][129146] Max Reward on eval: -88.99968468751744
[37m[1m[2023-06-25 02:10:25,367][129146] Min Reward on eval: -88.99968468751744
[37m[1m[2023-06-25 02:10:25,367][129146] Mean Reward across all agents: -88.99968468751744
[37m[1m[2023-06-25 02:10:25,367][129146] Average Trajectory Length: 994.4453333333333
[36m[2023-06-25 02:10:30,808][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:10:30,809][129146] Reward + Measures: [[-144.3089299     0.7536        0.12830001    0.66480011    0.2385    ]
[37m[1m [-151.85107659    0.741         0.1384041     0.6727857     0.24471822]
[37m[1m [ 111.58777656    0.60206032    0.17503193    0.48937914    0.21995123]
[37m[1m ...
[37m[1m [ -55.80265268    0.63679999    0.19350001    0.53200001    0.24430001]
[37m[1m [  45.61066606    0.6487        0.1604        0.54299998    0.2246    ]
[37m[1m [ -49.38131246    0.62001431    0.18882857    0.53578568    0.22190002]]
[37m[1m[2023-06-25 02:10:30,809][129146] Max Reward on eval: 332.4768201372237
[37m[1m[2023-06-25 02:10:30,809][129146] Min Reward on eval: -323.0470704335021
[37m[1m[2023-06-25 02:10:30,809][129146] Mean Reward across all agents: -30.880124196765575
[37m[1m[2023-06-25 02:10:30,810][129146] Average Trajectory Length: 988.3396666666666
[36m[2023-06-25 02:10:30,816][129146] mean_value=99.91889932467593, max_value=395.4403508288973
[37m[1m[2023-06-25 02:10:30,819][129146] New mean coefficients: [[ 0.47346652  0.25460196 -0.6297425   2.540987   -0.87605804]]
[37m[1m[2023-06-25 02:10:30,820][129146] Moving the mean solution point...
[36m[2023-06-25 02:10:40,451][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 02:10:40,451][129146] FPS: 398772.22
[36m[2023-06-25 02:10:40,454][129146] itr=103, itrs=2000, Progress: 5.15%
[36m[2023-06-25 02:10:51,867][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 02:10:51,867][129146] FPS: 336873.78
[36m[2023-06-25 02:10:56,643][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:10:56,644][129146] Reward + Measures: [[-170.14281451    0.74865347    0.11314279    0.67930466    0.25588048]]
[37m[1m[2023-06-25 02:10:56,644][129146] Max Reward on eval: -170.14281451377337
[37m[1m[2023-06-25 02:10:56,644][129146] Min Reward on eval: -170.14281451377337
[37m[1m[2023-06-25 02:10:56,644][129146] Mean Reward across all agents: -170.14281451377337
[37m[1m[2023-06-25 02:10:56,644][129146] Average Trajectory Length: 995.9353333333333
[36m[2023-06-25 02:11:02,238][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:11:02,239][129146] Reward + Measures: [[ -93.39589696    0.70859998    0.10290001    0.64420003    0.22189999]
[37m[1m [ -89.23789329    0.61409998    0.13060002    0.55159998    0.20999999]
[37m[1m [ -76.44359848    0.6785        0.1349        0.59350002    0.2287    ]
[37m[1m ...
[37m[1m [ -46.3144597     0.66300005    0.13170001    0.58420002    0.2119    ]
[37m[1m [-119.90134846    0.7209        0.14300001    0.63870001    0.24819998]
[37m[1m [ -62.26877866    0.55050004    0.14039999    0.49899998    0.24170004]]
[37m[1m[2023-06-25 02:11:02,239][129146] Max Reward on eval: 225.6059471187691
[37m[1m[2023-06-25 02:11:02,239][129146] Min Reward on eval: -313.81662217930426
[37m[1m[2023-06-25 02:11:02,240][129146] Mean Reward across all agents: -95.97766134616694
[37m[1m[2023-06-25 02:11:02,240][129146] Average Trajectory Length: 995.7816666666666
[36m[2023-06-25 02:11:02,244][129146] mean_value=17.471045807568192, max_value=456.50901485892246
[37m[1m[2023-06-25 02:11:02,247][129146] New mean coefficients: [[ 0.03112364  0.38722938 -0.87093216  2.6330767  -0.6984168 ]]
[37m[1m[2023-06-25 02:11:02,248][129146] Moving the mean solution point...
[36m[2023-06-25 02:11:11,933][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 02:11:11,933][129146] FPS: 396569.91
[36m[2023-06-25 02:11:11,935][129146] itr=104, itrs=2000, Progress: 5.20%
[36m[2023-06-25 02:11:23,360][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 02:11:23,361][129146] FPS: 336493.63
[36m[2023-06-25 02:11:28,188][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:11:28,188][129146] Reward + Measures: [[-300.53831995    0.84378874    0.06619364    0.80955869    0.33804309]]
[37m[1m[2023-06-25 02:11:28,189][129146] Max Reward on eval: -300.5383199508951
[37m[1m[2023-06-25 02:11:28,189][129146] Min Reward on eval: -300.5383199508951
[37m[1m[2023-06-25 02:11:28,189][129146] Mean Reward across all agents: -300.5383199508951
[37m[1m[2023-06-25 02:11:28,189][129146] Average Trajectory Length: 997.9253333333334
[36m[2023-06-25 02:11:33,690][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:11:33,690][129146] Reward + Measures: [[-424.29144634    0.89190006    0.0505        0.86849993    0.43859997]
[37m[1m [-368.98490513    0.81999999    0.0611        0.82480001    0.33309999]
[37m[1m [-360.3083915     0.86729997    0.0692        0.82010001    0.396     ]
[37m[1m ...
[37m[1m [-443.65251019    0.90479994    0.0421        0.8901        0.45060006]
[37m[1m [-459.83997156    0.88440001    0.0534        0.86580002    0.4434    ]
[37m[1m [-360.65327347    0.85500002    0.0654        0.84060001    0.34259999]]
[37m[1m[2023-06-25 02:11:33,690][129146] Max Reward on eval: -10.959465038217605
[37m[1m[2023-06-25 02:11:33,691][129146] Min Reward on eval: -494.3199895337806
[37m[1m[2023-06-25 02:11:33,691][129146] Mean Reward across all agents: -376.1921933375351
[37m[1m[2023-06-25 02:11:33,691][129146] Average Trajectory Length: 998.6486666666666
[36m[2023-06-25 02:11:33,696][129146] mean_value=29.281299480906018, max_value=241.0425369738502
[37m[1m[2023-06-25 02:11:33,699][129146] New mean coefficients: [[ 0.06388924  0.5086695  -0.28700805  2.9689603  -0.32022732]]
[37m[1m[2023-06-25 02:11:33,700][129146] Moving the mean solution point...
[36m[2023-06-25 02:11:43,400][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 02:11:43,401][129146] FPS: 395920.72
[36m[2023-06-25 02:11:43,403][129146] itr=105, itrs=2000, Progress: 5.25%
[36m[2023-06-25 02:11:54,865][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 02:11:54,866][129146] FPS: 335376.02
[36m[2023-06-25 02:11:59,544][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:11:59,544][129146] Reward + Measures: [[-429.92345548    0.91046196    0.03296633    0.89672267    0.44724166]]
[37m[1m[2023-06-25 02:11:59,544][129146] Max Reward on eval: -429.92345548477084
[37m[1m[2023-06-25 02:11:59,545][129146] Min Reward on eval: -429.92345548477084
[37m[1m[2023-06-25 02:11:59,545][129146] Mean Reward across all agents: -429.92345548477084
[37m[1m[2023-06-25 02:11:59,545][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 02:12:04,893][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:12:04,894][129146] Reward + Measures: [[-287.30353632    0.80489999    0.0593        0.77290004    0.3495    ]
[37m[1m [-427.60476393    0.92399997    0.0306        0.90199995    0.44670001]
[37m[1m [-439.85801651    0.90029997    0.0442        0.88819999    0.35059997]
[37m[1m ...
[37m[1m [-338.13414396    0.85230011    0.036         0.82490009    0.39280003]
[37m[1m [-241.59124724    0.80730003    0.0797        0.77030003    0.33850002]
[37m[1m [-431.47859702    0.90339994    0.0393        0.88170004    0.38530001]]
[37m[1m[2023-06-25 02:12:04,894][129146] Max Reward on eval: 58.39049275198486
[37m[1m[2023-06-25 02:12:04,894][129146] Min Reward on eval: -537.5689354960166
[37m[1m[2023-06-25 02:12:04,894][129146] Mean Reward across all agents: -364.810214947075
[37m[1m[2023-06-25 02:12:04,895][129146] Average Trajectory Length: 997.7476666666666
[36m[2023-06-25 02:12:04,899][129146] mean_value=4.334528462256761, max_value=265.21601019224386
[37m[1m[2023-06-25 02:12:04,902][129146] New mean coefficients: [[ 1.3590314   0.8232099  -0.1185668   2.764526   -0.30330977]]
[37m[1m[2023-06-25 02:12:04,903][129146] Moving the mean solution point...
[36m[2023-06-25 02:12:14,658][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 02:12:14,658][129146] FPS: 393711.07
[36m[2023-06-25 02:12:14,660][129146] itr=106, itrs=2000, Progress: 5.30%
[36m[2023-06-25 02:12:26,176][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 02:12:26,176][129146] FPS: 333835.92
[36m[2023-06-25 02:12:30,889][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:12:30,889][129146] Reward + Measures: [[-423.63138939    0.92697901    0.024527      0.91430026    0.47776237]]
[37m[1m[2023-06-25 02:12:30,889][129146] Max Reward on eval: -423.63138938501936
[37m[1m[2023-06-25 02:12:30,890][129146] Min Reward on eval: -423.63138938501936
[37m[1m[2023-06-25 02:12:30,890][129146] Mean Reward across all agents: -423.63138938501936
[37m[1m[2023-06-25 02:12:30,890][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 02:12:36,270][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:12:36,271][129146] Reward + Measures: [[-367.93613027    0.84259999    0.07120001    0.80779999    0.36810002]
[37m[1m [-329.15522692    0.85179996    0.0788        0.80720007    0.30399999]
[37m[1m [-396.89166117    0.87530005    0.0509        0.85230011    0.38329998]
[37m[1m ...
[37m[1m [-330.76893947    0.8775        0.06299999    0.8488        0.32100001]
[37m[1m [-385.20636539    0.86339998    0.0654        0.83490002    0.32600001]
[37m[1m [-351.54609638    0.8186        0.0883        0.77910006    0.31479999]]
[37m[1m[2023-06-25 02:12:36,271][129146] Max Reward on eval: -18.006179746455746
[37m[1m[2023-06-25 02:12:36,271][129146] Min Reward on eval: -447.9755406162352
[37m[1m[2023-06-25 02:12:36,271][129146] Mean Reward across all agents: -336.89151598416163
[37m[1m[2023-06-25 02:12:36,272][129146] Average Trajectory Length: 998.8539999999999
[36m[2023-06-25 02:12:36,276][129146] mean_value=-6.562294713298661, max_value=244.37230573072446
[37m[1m[2023-06-25 02:12:36,279][129146] New mean coefficients: [[ 1.238132    0.5740949  -0.09901351  2.805022   -0.16564599]]
[37m[1m[2023-06-25 02:12:36,280][129146] Moving the mean solution point...
[36m[2023-06-25 02:12:45,876][129146] train() took 9.59 seconds to complete
[36m[2023-06-25 02:12:45,876][129146] FPS: 400210.37
[36m[2023-06-25 02:12:45,879][129146] itr=107, itrs=2000, Progress: 5.35%
[36m[2023-06-25 02:12:57,391][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 02:12:57,392][129146] FPS: 333925.96
[36m[2023-06-25 02:13:02,112][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:13:02,112][129146] Reward + Measures: [[-411.05543816    0.94106239    0.01980161    0.92917788    0.51444942]]
[37m[1m[2023-06-25 02:13:02,112][129146] Max Reward on eval: -411.0554381598755
[37m[1m[2023-06-25 02:13:02,112][129146] Min Reward on eval: -411.0554381598755
[37m[1m[2023-06-25 02:13:02,113][129146] Mean Reward across all agents: -411.0554381598755
[37m[1m[2023-06-25 02:13:02,113][129146] Average Trajectory Length: 999.8686666666666
[36m[2023-06-25 02:13:07,624][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:13:07,625][129146] Reward + Measures: [[-378.85416682    0.9005        0.0489        0.87880003    0.49940005]
[37m[1m [ -50.81017471    0.73723334    0.13753335    0.64990002    0.2342    ]
[37m[1m [ -49.33713768    0.71870005    0.13340001    0.63239998    0.2414    ]
[37m[1m ...
[37m[1m [-236.74084595    0.75770003    0.10750001    0.72380006    0.38330004]
[37m[1m [ -55.13087114    0.7166        0.1017        0.65210003    0.2498    ]
[37m[1m [-173.03970054    0.7877        0.12410001    0.71850002    0.27300003]]
[37m[1m[2023-06-25 02:13:07,625][129146] Max Reward on eval: 165.18755957487738
[37m[1m[2023-06-25 02:13:07,625][129146] Min Reward on eval: -494.879654709954
[37m[1m[2023-06-25 02:13:07,626][129146] Mean Reward across all agents: -289.9400012147734
[37m[1m[2023-06-25 02:13:07,626][129146] Average Trajectory Length: 995.6753333333334
[36m[2023-06-25 02:13:07,633][129146] mean_value=49.19850007767327, max_value=590.1110557228734
[37m[1m[2023-06-25 02:13:07,636][129146] New mean coefficients: [[2.0333169  0.7737145  0.39731398 2.9890063  0.47204027]]
[37m[1m[2023-06-25 02:13:07,637][129146] Moving the mean solution point...
[36m[2023-06-25 02:13:17,350][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 02:13:17,350][129146] FPS: 395419.73
[36m[2023-06-25 02:13:17,352][129146] itr=108, itrs=2000, Progress: 5.40%
[36m[2023-06-25 02:13:28,813][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 02:13:28,813][129146] FPS: 335422.32
[36m[2023-06-25 02:13:33,732][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:13:33,733][129146] Reward + Measures: [[-349.33995752    0.94563758    0.01897467    0.93401074    0.51821595]]
[37m[1m[2023-06-25 02:13:33,733][129146] Max Reward on eval: -349.339957517722
[37m[1m[2023-06-25 02:13:33,733][129146] Min Reward on eval: -349.339957517722
[37m[1m[2023-06-25 02:13:33,733][129146] Mean Reward across all agents: -349.339957517722
[37m[1m[2023-06-25 02:13:33,733][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 02:13:39,208][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:13:39,209][129146] Reward + Measures: [[-297.74326277    0.92609996    0.0339        0.91409999    0.51630002]
[37m[1m [-344.66193609    0.92379999    0.0361        0.89380008    0.55419999]
[37m[1m [-289.55147385    0.92430001    0.0318        0.88819999    0.34320003]
[37m[1m ...
[37m[1m [-372.02683571    0.9497        0.0321        0.93809998    0.75040007]
[37m[1m [-275.89121819    0.85639995    0.0904        0.82100004    0.53549999]
[37m[1m [-322.07373691    0.92159998    0.0367        0.8998        0.4549    ]]
[37m[1m[2023-06-25 02:13:39,209][129146] Max Reward on eval: -0.8525792884174734
[37m[1m[2023-06-25 02:13:39,209][129146] Min Reward on eval: -433.8182831534999
[37m[1m[2023-06-25 02:13:39,210][129146] Mean Reward across all agents: -309.3332649081437
[37m[1m[2023-06-25 02:13:39,210][129146] Average Trajectory Length: 999.7196666666666
[36m[2023-06-25 02:13:39,216][129146] mean_value=60.69044092955563, max_value=227.90921487495982
[37m[1m[2023-06-25 02:13:39,219][129146] New mean coefficients: [[ 1.3702447   0.52385056 -0.29354522  3.3602948   1.1617508 ]]
[37m[1m[2023-06-25 02:13:39,220][129146] Moving the mean solution point...
[36m[2023-06-25 02:13:48,924][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 02:13:48,924][129146] FPS: 395791.24
[36m[2023-06-25 02:13:48,926][129146] itr=109, itrs=2000, Progress: 5.45%
[36m[2023-06-25 02:14:00,404][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 02:14:00,404][129146] FPS: 334940.05
[36m[2023-06-25 02:14:05,286][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:14:05,287][129146] Reward + Measures: [[-332.08227997    0.95924461    0.01535303    0.94941241    0.6137675 ]]
[37m[1m[2023-06-25 02:14:05,287][129146] Max Reward on eval: -332.0822799656333
[37m[1m[2023-06-25 02:14:05,287][129146] Min Reward on eval: -332.0822799656333
[37m[1m[2023-06-25 02:14:05,287][129146] Mean Reward across all agents: -332.0822799656333
[37m[1m[2023-06-25 02:14:05,288][129146] Average Trajectory Length: 999.7063333333333
[36m[2023-06-25 02:14:10,808][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:14:10,809][129146] Reward + Measures: [[-254.28565338    0.87420005    0.0552        0.85159999    0.44430003]
[37m[1m [-228.29122952    0.84240001    0.079         0.8082        0.40970001]
[37m[1m [-219.81696494    0.9228        0.0442        0.9052        0.6753    ]
[37m[1m ...
[37m[1m [-211.00777181    0.90570015    0.0504        0.88829994    0.59759998]
[37m[1m [-147.27091324    0.79350001    0.10380001    0.74970007    0.38      ]
[37m[1m [-262.21610018    0.93379992    0.0355        0.91589993    0.58069998]]
[37m[1m[2023-06-25 02:14:10,809][129146] Max Reward on eval: 133.13197710555397
[37m[1m[2023-06-25 02:14:10,809][129146] Min Reward on eval: -374.2826247173478
[37m[1m[2023-06-25 02:14:10,810][129146] Mean Reward across all agents: -240.0203495340681
[37m[1m[2023-06-25 02:14:10,810][129146] Average Trajectory Length: 998.9333333333333
[36m[2023-06-25 02:14:10,819][129146] mean_value=95.16123377718502, max_value=456.12194507858504
[37m[1m[2023-06-25 02:14:10,822][129146] New mean coefficients: [[ 0.29231894  0.871259   -0.04191691  4.4680986   2.17974   ]]
[37m[1m[2023-06-25 02:14:10,823][129146] Moving the mean solution point...
[36m[2023-06-25 02:14:20,629][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 02:14:20,629][129146] FPS: 391664.71
[36m[2023-06-25 02:14:20,632][129146] itr=110, itrs=2000, Progress: 5.50%
[37m[1m[2023-06-25 02:14:22,844][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000090
[36m[2023-06-25 02:14:34,805][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 02:14:34,806][129146] FPS: 329996.49
[36m[2023-06-25 02:14:39,702][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:14:39,703][129146] Reward + Measures: [[-348.85519634    0.96520001    0.014609      0.95476401    0.70806599]]
[37m[1m[2023-06-25 02:14:39,703][129146] Max Reward on eval: -348.8551963389458
[37m[1m[2023-06-25 02:14:39,703][129146] Min Reward on eval: -348.8551963389458
[37m[1m[2023-06-25 02:14:39,703][129146] Mean Reward across all agents: -348.8551963389458
[37m[1m[2023-06-25 02:14:39,704][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 02:14:45,142][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:14:45,142][129146] Reward + Measures: [[-355.11791107    0.94530004    0.0173        0.9472        0.62160003]
[37m[1m [-355.09851764    0.9738        0.0137        0.95970005    0.76450002]
[37m[1m [-264.80007697    0.93660003    0.0318        0.90900004    0.62800002]
[37m[1m ...
[37m[1m [-381.74245885    0.90679997    0.0311        0.89270002    0.48809996]
[37m[1m [-339.6037172     0.96789998    0.0136        0.96420002    0.80820006]
[37m[1m [-215.12941299    0.81660002    0.0953        0.78440005    0.465     ]]
[37m[1m[2023-06-25 02:14:45,143][129146] Max Reward on eval: 113.82905523814262
[37m[1m[2023-06-25 02:14:45,143][129146] Min Reward on eval: -548.6332954339101
[37m[1m[2023-06-25 02:14:45,143][129146] Mean Reward across all agents: -338.3713066852309
[37m[1m[2023-06-25 02:14:45,143][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 02:14:45,146][129146] mean_value=-44.05373167618682, max_value=300.6619699418311
[37m[1m[2023-06-25 02:14:45,149][129146] New mean coefficients: [[ 1.3739219   0.58095807 -0.9057732   3.9494038   1.5660671 ]]
[37m[1m[2023-06-25 02:14:45,150][129146] Moving the mean solution point...
[36m[2023-06-25 02:14:54,928][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 02:14:54,928][129146] FPS: 392797.23
[36m[2023-06-25 02:14:54,931][129146] itr=111, itrs=2000, Progress: 5.55%
[36m[2023-06-25 02:15:06,410][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 02:15:06,410][129146] FPS: 334950.87
[36m[2023-06-25 02:15:11,257][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:15:11,258][129146] Reward + Measures: [[-359.98685781    0.97168767    0.013147      0.96187496    0.85392636]]
[37m[1m[2023-06-25 02:15:11,258][129146] Max Reward on eval: -359.986857813958
[37m[1m[2023-06-25 02:15:11,258][129146] Min Reward on eval: -359.986857813958
[37m[1m[2023-06-25 02:15:11,258][129146] Mean Reward across all agents: -359.986857813958
[37m[1m[2023-06-25 02:15:11,258][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 02:15:16,700][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:15:16,701][129146] Reward + Measures: [[-471.64767573    0.97390002    0.0117        0.96730006    0.87690002]
[37m[1m [-511.47362595    0.96310008    0.0163        0.95639992    0.8502    ]
[37m[1m [-346.50474526    0.9544        0.0253        0.93160003    0.67640001]
[37m[1m ...
[37m[1m [-403.35368493    0.96730006    0.0144        0.95450002    0.83129996]
[37m[1m [-357.22470531    0.85470003    0.0369        0.82170004    0.58780003]
[37m[1m [-483.73950818    0.97259998    0.0137        0.96380007    0.91000003]]
[37m[1m[2023-06-25 02:15:16,701][129146] Max Reward on eval: -112.23490499367472
[37m[1m[2023-06-25 02:15:16,701][129146] Min Reward on eval: -763.817277351732
[37m[1m[2023-06-25 02:15:16,702][129146] Mean Reward across all agents: -453.4100440847575
[37m[1m[2023-06-25 02:15:16,702][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 02:15:16,704][129146] mean_value=-105.30434522624113, max_value=245.93501881221994
[37m[1m[2023-06-25 02:15:16,707][129146] New mean coefficients: [[ 2.2694576   0.24403027 -0.20065248  3.0069036   1.6236473 ]]
[37m[1m[2023-06-25 02:15:16,708][129146] Moving the mean solution point...
[36m[2023-06-25 02:15:26,451][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 02:15:26,452][129146] FPS: 394176.13
[36m[2023-06-25 02:15:26,454][129146] itr=112, itrs=2000, Progress: 5.60%
[36m[2023-06-25 02:15:37,879][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 02:15:37,879][129146] FPS: 336494.48
[36m[2023-06-25 02:15:42,700][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:15:42,701][129146] Reward + Measures: [[-384.12149059    0.97551626    0.01183667    0.96650934    0.9111017 ]]
[37m[1m[2023-06-25 02:15:42,701][129146] Max Reward on eval: -384.1214905891131
[37m[1m[2023-06-25 02:15:42,701][129146] Min Reward on eval: -384.1214905891131
[37m[1m[2023-06-25 02:15:42,702][129146] Mean Reward across all agents: -384.1214905891131
[37m[1m[2023-06-25 02:15:42,702][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 02:15:48,333][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:15:48,333][129146] Reward + Measures: [[ -81.2693597     0.76310003    0.12980001    0.70430005    0.39800003]
[37m[1m [-272.12074061    0.94869995    0.027         0.93879998    0.7913    ]
[37m[1m [-339.3604185     0.97399998    0.0124        0.96900004    0.92680007]
[37m[1m ...
[37m[1m [-376.82837894    0.97410005    0.0114        0.96649998    0.90640002]
[37m[1m [-239.01553583    0.96490002    0.0256        0.94919997    0.77350003]
[37m[1m [-197.14431328    0.87160009    0.0759        0.84780008    0.7374    ]]
[37m[1m[2023-06-25 02:15:48,333][129146] Max Reward on eval: 91.7982735502068
[37m[1m[2023-06-25 02:15:48,334][129146] Min Reward on eval: -470.9730152092758
[37m[1m[2023-06-25 02:15:48,334][129146] Mean Reward across all agents: -245.91769709267274
[37m[1m[2023-06-25 02:15:48,334][129146] Average Trajectory Length: 999.447
[36m[2023-06-25 02:15:48,342][129146] mean_value=131.8182870650447, max_value=591.7982735502068
[37m[1m[2023-06-25 02:15:48,345][129146] New mean coefficients: [[2.763396  0.5855074 1.0475296 4.171014  2.3467784]]
[37m[1m[2023-06-25 02:15:48,346][129146] Moving the mean solution point...
[36m[2023-06-25 02:15:58,107][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 02:15:58,107][129146] FPS: 393480.37
[36m[2023-06-25 02:15:58,110][129146] itr=113, itrs=2000, Progress: 5.65%
[36m[2023-06-25 02:16:09,541][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 02:16:09,542][129146] FPS: 336297.19
[36m[2023-06-25 02:16:14,382][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:16:14,383][129146] Reward + Measures: [[-303.79101029    0.9741478     0.01219008    0.96443307    0.89785874]]
[37m[1m[2023-06-25 02:16:14,383][129146] Max Reward on eval: -303.79101029281145
[37m[1m[2023-06-25 02:16:14,383][129146] Min Reward on eval: -303.79101029281145
[37m[1m[2023-06-25 02:16:14,383][129146] Mean Reward across all agents: -303.79101029281145
[37m[1m[2023-06-25 02:16:14,384][129146] Average Trajectory Length: 999.8156666666666
[36m[2023-06-25 02:16:19,809][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:16:19,810][129146] Reward + Measures: [[-160.17404438    0.86660004    0.0728        0.83939999    0.32190001]
[37m[1m [ -56.29709358    0.64710003    0.15259999    0.63080001    0.24089999]
[37m[1m [ -71.37305783    0.88210005    0.0666        0.84820002    0.33370003]
[37m[1m ...
[37m[1m [-173.30134675    0.89460003    0.0489        0.86800003    0.44859996]
[37m[1m [-196.11141146    0.95679998    0.0232        0.94340003    0.61939996]
[37m[1m [ 231.20193837    0.58499998    0.2507        0.42670003    0.25750002]]
[37m[1m[2023-06-25 02:16:19,810][129146] Max Reward on eval: 337.5557693584822
[37m[1m[2023-06-25 02:16:19,811][129146] Min Reward on eval: -359.5025108647067
[37m[1m[2023-06-25 02:16:19,811][129146] Mean Reward across all agents: -129.4768838233481
[37m[1m[2023-06-25 02:16:19,811][129146] Average Trajectory Length: 999.8076666666666
[36m[2023-06-25 02:16:19,819][129146] mean_value=95.3668613122854, max_value=837.5557693584822
[37m[1m[2023-06-25 02:16:19,822][129146] New mean coefficients: [[4.503328  0.6175722 2.1549926 4.0421305 2.7970247]]
[37m[1m[2023-06-25 02:16:19,823][129146] Moving the mean solution point...
[36m[2023-06-25 02:16:29,424][129146] train() took 9.60 seconds to complete
[36m[2023-06-25 02:16:29,425][129146] FPS: 400010.61
[36m[2023-06-25 02:16:29,427][129146] itr=114, itrs=2000, Progress: 5.70%
[36m[2023-06-25 02:16:40,990][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 02:16:40,990][129146] FPS: 332451.85
[36m[2023-06-25 02:16:45,744][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:16:45,744][129146] Reward + Measures: [[-228.4905386     0.97537029    0.01127067    0.96630597    0.9110871 ]]
[37m[1m[2023-06-25 02:16:45,745][129146] Max Reward on eval: -228.49053860112357
[37m[1m[2023-06-25 02:16:45,745][129146] Min Reward on eval: -228.49053860112357
[37m[1m[2023-06-25 02:16:45,745][129146] Mean Reward across all agents: -228.49053860112357
[37m[1m[2023-06-25 02:16:45,745][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 02:16:51,272][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:16:51,273][129146] Reward + Measures: [[ -83.87859312    0.88329995    0.0597        0.85320008    0.53030002]
[37m[1m [  35.97671652    0.78249997    0.1103        0.7324        0.44930002]
[37m[1m [-113.03399146    0.8186        0.0844        0.81440002    0.43689999]
[37m[1m ...
[37m[1m [  48.46112363    0.70359993    0.15530001    0.65620005    0.32010004]
[37m[1m [ 387.10579976    0.44620004    0.3193        0.27450001    0.25320002]
[37m[1m [ 147.70925941    0.68430007    0.1777        0.58540004    0.32430002]]
[37m[1m[2023-06-25 02:16:51,273][129146] Max Reward on eval: 502.01985410646887
[37m[1m[2023-06-25 02:16:51,273][129146] Min Reward on eval: -187.10343118271558
[37m[1m[2023-06-25 02:16:51,274][129146] Mean Reward across all agents: 60.017573616595925
[37m[1m[2023-06-25 02:16:51,274][129146] Average Trajectory Length: 999.1379999999999
[36m[2023-06-25 02:16:51,284][129146] mean_value=258.7922888535532, max_value=913.4644178205053
[37m[1m[2023-06-25 02:16:51,287][129146] New mean coefficients: [[6.043621   0.35249835 2.986396   3.9111357  3.4031596 ]]
[37m[1m[2023-06-25 02:16:51,288][129146] Moving the mean solution point...
[36m[2023-06-25 02:17:01,149][129146] train() took 9.86 seconds to complete
[36m[2023-06-25 02:17:01,149][129146] FPS: 389471.25
[36m[2023-06-25 02:17:01,151][129146] itr=115, itrs=2000, Progress: 5.75%
[36m[2023-06-25 02:17:12,843][129146] train() took 11.68 seconds to complete
[36m[2023-06-25 02:17:12,843][129146] FPS: 328872.49
[36m[2023-06-25 02:17:17,627][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:17:17,628][129146] Reward + Measures: [[-141.32370762    0.97698027    0.01081367    0.96770233    0.92841536]]
[37m[1m[2023-06-25 02:17:17,628][129146] Max Reward on eval: -141.3237076175687
[37m[1m[2023-06-25 02:17:17,628][129146] Min Reward on eval: -141.3237076175687
[37m[1m[2023-06-25 02:17:17,628][129146] Mean Reward across all agents: -141.3237076175687
[37m[1m[2023-06-25 02:17:17,629][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 02:17:23,106][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:17:23,106][129146] Reward + Measures: [[ -99.12619927    0.95550007    0.0211        0.93510002    0.70349997]
[37m[1m [-159.22290854    0.87919998    0.0365        0.8799001     0.43290001]
[37m[1m [-148.19477355    0.8775        0.0369        0.86040002    0.41809997]
[37m[1m ...
[37m[1m [ -28.52419633    0.83420002    0.0636        0.79339999    0.38410002]
[37m[1m [ -76.4039908     0.83490002    0.069         0.80860007    0.3414    ]
[37m[1m [ -68.40776846    0.88430005    0.0508        0.83560002    0.35330001]]
[37m[1m[2023-06-25 02:17:23,106][129146] Max Reward on eval: 442.50741788040614
[37m[1m[2023-06-25 02:17:23,107][129146] Min Reward on eval: -241.02475881834397
[37m[1m[2023-06-25 02:17:23,107][129146] Mean Reward across all agents: 43.62560578739436
[37m[1m[2023-06-25 02:17:23,107][129146] Average Trajectory Length: 999.9873333333333
[36m[2023-06-25 02:17:23,114][129146] mean_value=143.40486400528732, max_value=893.875519259891
[37m[1m[2023-06-25 02:17:23,117][129146] New mean coefficients: [[ 7.762927   -0.07006487  3.7188885   2.9969504   2.908441  ]]
[37m[1m[2023-06-25 02:17:23,118][129146] Moving the mean solution point...
[36m[2023-06-25 02:17:32,890][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 02:17:32,890][129146] FPS: 393050.29
[36m[2023-06-25 02:17:32,892][129146] itr=116, itrs=2000, Progress: 5.80%
[36m[2023-06-25 02:17:44,543][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 02:17:44,543][129146] FPS: 329994.35
[36m[2023-06-25 02:17:49,279][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:17:49,279][129146] Reward + Measures: [[6.12081474 0.84190762 0.08842433 0.77831429 0.34340867]]
[37m[1m[2023-06-25 02:17:49,279][129146] Max Reward on eval: 6.120814736955828
[37m[1m[2023-06-25 02:17:49,280][129146] Min Reward on eval: 6.120814736955828
[37m[1m[2023-06-25 02:17:49,280][129146] Mean Reward across all agents: 6.120814736955828
[37m[1m[2023-06-25 02:17:49,280][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 02:17:54,714][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:17:54,714][129146] Reward + Measures: [[ 28.07613173   0.69550002   0.1531       0.65250003   0.32079998]
[37m[1m [171.81287417   0.50030005   0.1974       0.42870003   0.24589999]
[37m[1m [170.94902516   0.65030003   0.17729999   0.57700008   0.2375    ]
[37m[1m ...
[37m[1m [101.27845982   0.63660002   0.184        0.56280005   0.29250002]
[37m[1m [142.8792609    0.57910001   0.2009       0.51140004   0.2746    ]
[37m[1m [-14.21294069   0.79399997   0.1294       0.7489       0.33530003]]
[37m[1m[2023-06-25 02:17:54,714][129146] Max Reward on eval: 329.68688998159485
[37m[1m[2023-06-25 02:17:54,715][129146] Min Reward on eval: -95.75749129446922
[37m[1m[2023-06-25 02:17:54,715][129146] Mean Reward across all agents: 155.68158503339848
[37m[1m[2023-06-25 02:17:54,715][129146] Average Trajectory Length: 995.8483333333332
[36m[2023-06-25 02:17:54,720][129146] mean_value=71.5028578688846, max_value=788.9018517128018
[37m[1m[2023-06-25 02:17:54,723][129146] New mean coefficients: [[7.9088483  0.05015299 3.8494408  3.2460394  2.6293995 ]]
[37m[1m[2023-06-25 02:17:54,724][129146] Moving the mean solution point...
[36m[2023-06-25 02:18:04,443][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 02:18:04,444][129146] FPS: 395165.41
[36m[2023-06-25 02:18:04,446][129146] itr=117, itrs=2000, Progress: 5.85%
[36m[2023-06-25 02:18:16,036][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 02:18:16,036][129146] FPS: 331694.80
[36m[2023-06-25 02:18:20,853][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:18:20,853][129146] Reward + Measures: [[87.68155157  0.82847536  0.09936448  0.75530189  0.37152305]]
[37m[1m[2023-06-25 02:18:20,854][129146] Max Reward on eval: 87.68155157448169
[37m[1m[2023-06-25 02:18:20,854][129146] Min Reward on eval: 87.68155157448169
[37m[1m[2023-06-25 02:18:20,854][129146] Mean Reward across all agents: 87.68155157448169
[37m[1m[2023-06-25 02:18:20,854][129146] Average Trajectory Length: 999.7116666666666
[36m[2023-06-25 02:18:26,328][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:18:26,328][129146] Reward + Measures: [[ 195.55325876    0.30972958    0.29381523    0.10380268    0.26388207]
[37m[1m [ 142.44952228    0.26618612    0.28226385    0.08639195    0.23065035]
[37m[1m [ 199.67516815    0.4463        0.2969        0.29280001    0.27190003]
[37m[1m ...
[37m[1m [-278.35056343    0.32863519    0.36067602    0.14072375    0.28960881]
[37m[1m [ 177.17639705    0.34688219    0.3255671     0.13479228    0.27551278]
[37m[1m [  59.68433135    0.31754598    0.33228612    0.12948905    0.26073724]]
[37m[1m[2023-06-25 02:18:26,329][129146] Max Reward on eval: 349.7285363521543
[37m[1m[2023-06-25 02:18:26,329][129146] Min Reward on eval: -525.8035615578294
[37m[1m[2023-06-25 02:18:26,329][129146] Mean Reward across all agents: 61.04294322059482
[37m[1m[2023-06-25 02:18:26,329][129146] Average Trajectory Length: 958.3883333333333
[36m[2023-06-25 02:18:26,334][129146] mean_value=-14.496604714211596, max_value=409.4257270100344
[37m[1m[2023-06-25 02:18:26,337][129146] New mean coefficients: [[ 7.3351865  -0.14122902  3.5770001   3.3024359   2.8767555 ]]
[37m[1m[2023-06-25 02:18:26,338][129146] Moving the mean solution point...
[36m[2023-06-25 02:18:35,976][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 02:18:35,976][129146] FPS: 398485.20
[36m[2023-06-25 02:18:35,979][129146] itr=118, itrs=2000, Progress: 5.90%
[36m[2023-06-25 02:18:47,385][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 02:18:47,386][129146] FPS: 337014.89
[36m[2023-06-25 02:18:52,104][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:18:52,105][129146] Reward + Measures: [[174.24462849   0.82235032   0.10994166   0.74402833   0.44018269]]
[37m[1m[2023-06-25 02:18:52,105][129146] Max Reward on eval: 174.2446284899653
[37m[1m[2023-06-25 02:18:52,105][129146] Min Reward on eval: 174.2446284899653
[37m[1m[2023-06-25 02:18:52,106][129146] Mean Reward across all agents: 174.2446284899653
[37m[1m[2023-06-25 02:18:52,106][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 02:18:57,586][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:18:57,592][129146] Reward + Measures: [[509.62533231   0.50414032   0.3003543    0.30499515   0.27767029]
[37m[1m [454.4363126    0.39380002   0.27790001   0.2103       0.25620002]
[37m[1m [386.81686121   0.39740005   0.34130001   0.1655       0.28369999]
[37m[1m ...
[37m[1m [295.7241785    0.39970002   0.35980001   0.18470001   0.28519997]
[37m[1m [399.60690524   0.5715       0.26020002   0.39949998   0.2667    ]
[37m[1m [437.50439526   0.43350002   0.24060002   0.2782       0.2333    ]]
[37m[1m[2023-06-25 02:18:57,592][129146] Max Reward on eval: 558.1791886831518
[37m[1m[2023-06-25 02:18:57,592][129146] Min Reward on eval: 132.34623552891426
[37m[1m[2023-06-25 02:18:57,593][129146] Mean Reward across all agents: 382.1034923974417
[37m[1m[2023-06-25 02:18:57,593][129146] Average Trajectory Length: 995.557
[36m[2023-06-25 02:18:57,600][129146] mean_value=129.84968224169324, max_value=1011.8428375406307
[37m[1m[2023-06-25 02:18:57,602][129146] New mean coefficients: [[ 6.4727902  -0.12607864  2.8250058   3.0543966   2.3734128 ]]
[37m[1m[2023-06-25 02:18:57,604][129146] Moving the mean solution point...
[36m[2023-06-25 02:19:07,370][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 02:19:07,370][129146] FPS: 393247.93
[36m[2023-06-25 02:19:07,373][129146] itr=119, itrs=2000, Progress: 5.95%
[36m[2023-06-25 02:19:19,028][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 02:19:19,028][129146] FPS: 329829.43
[36m[2023-06-25 02:19:23,948][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:19:23,948][129146] Reward + Measures: [[216.59800975   0.84556508   0.0944517    0.77966666   0.56616718]]
[37m[1m[2023-06-25 02:19:23,949][129146] Max Reward on eval: 216.59800975012473
[37m[1m[2023-06-25 02:19:23,949][129146] Min Reward on eval: 216.59800975012473
[37m[1m[2023-06-25 02:19:23,949][129146] Mean Reward across all agents: 216.59800975012473
[37m[1m[2023-06-25 02:19:23,949][129146] Average Trajectory Length: 999.6556666666667
[36m[2023-06-25 02:19:29,587][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:19:29,588][129146] Reward + Measures: [[352.94263447   0.29229999   0.41299996   0.1894       0.37860003]
[37m[1m [264.07366613   0.3091       0.41349998   0.20990001   0.37380001]
[37m[1m [336.98280662   0.3321       0.33140001   0.17950001   0.32609999]
[37m[1m ...
[37m[1m [252.53735309   0.3125       0.33570001   0.21350001   0.33330002]
[37m[1m [374.47590535   0.34170002   0.30040002   0.19969998   0.30689999]
[37m[1m [ 79.54240907   0.28079998   0.24499999   0.1543       0.27630001]]
[37m[1m[2023-06-25 02:19:29,588][129146] Max Reward on eval: 804.0899730688776
[37m[1m[2023-06-25 02:19:29,589][129146] Min Reward on eval: -495.84354646386345
[37m[1m[2023-06-25 02:19:29,589][129146] Mean Reward across all agents: 334.8481435290548
[37m[1m[2023-06-25 02:19:29,589][129146] Average Trajectory Length: 993.554
[36m[2023-06-25 02:19:29,597][129146] mean_value=409.3704475964626, max_value=1157.064575534474
[37m[1m[2023-06-25 02:19:29,600][129146] New mean coefficients: [[ 7.2223763  -0.04390816  3.235378    2.5053792   2.8969564 ]]
[37m[1m[2023-06-25 02:19:29,601][129146] Moving the mean solution point...
[36m[2023-06-25 02:19:39,496][129146] train() took 9.89 seconds to complete
[36m[2023-06-25 02:19:39,496][129146] FPS: 388142.98
[36m[2023-06-25 02:19:39,499][129146] itr=120, itrs=2000, Progress: 6.00%
[37m[1m[2023-06-25 02:19:41,777][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000100
[36m[2023-06-25 02:19:53,639][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 02:19:53,639][129146] FPS: 332966.61
[36m[2023-06-25 02:19:58,405][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:19:58,406][129146] Reward + Measures: [[286.08207314   0.84198809   0.08931237   0.78171873   0.62287652]]
[37m[1m[2023-06-25 02:19:58,406][129146] Max Reward on eval: 286.08207314026714
[37m[1m[2023-06-25 02:19:58,406][129146] Min Reward on eval: 286.08207314026714
[37m[1m[2023-06-25 02:19:58,406][129146] Mean Reward across all agents: 286.08207314026714
[37m[1m[2023-06-25 02:19:58,407][129146] Average Trajectory Length: 999.5
[36m[2023-06-25 02:20:03,879][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:20:03,932][129146] Reward + Measures: [[291.63577314   0.61400002   0.20120001   0.50749999   0.26750001]
[37m[1m [445.24487806   0.53350002   0.27830002   0.37330002   0.25770003]
[37m[1m [199.11347084   0.67229998   0.21010001   0.55380005   0.2386    ]
[37m[1m ...
[37m[1m [176.35819861   0.70900005   0.1891       0.62620002   0.31169999]
[37m[1m [204.57679763   0.76750004   0.1556       0.68729997   0.44120002]
[37m[1m [337.58697136   0.5794       0.24009998   0.44619998   0.27149999]]
[37m[1m[2023-06-25 02:20:03,932][129146] Max Reward on eval: 445.2448780604842
[37m[1m[2023-06-25 02:20:03,932][129146] Min Reward on eval: 56.963437622482886
[37m[1m[2023-06-25 02:20:03,932][129146] Mean Reward across all agents: 231.99031426713822
[37m[1m[2023-06-25 02:20:03,933][129146] Average Trajectory Length: 998.1466666666666
[36m[2023-06-25 02:20:03,938][129146] mean_value=60.170799338783276, max_value=672.6994642861223
[37m[1m[2023-06-25 02:20:03,940][129146] New mean coefficients: [[ 5.6330705  -0.05043352  2.50029     2.2078235   2.6502914 ]]
[37m[1m[2023-06-25 02:20:03,941][129146] Moving the mean solution point...
[36m[2023-06-25 02:20:13,593][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 02:20:13,594][129146] FPS: 397910.18
[36m[2023-06-25 02:20:13,596][129146] itr=121, itrs=2000, Progress: 6.05%
[36m[2023-06-25 02:20:24,992][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 02:20:24,992][129146] FPS: 337369.98
[36m[2023-06-25 02:20:29,722][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:20:29,722][129146] Reward + Measures: [[350.43504484   0.83944172   0.08575812   0.78231227   0.65934867]]
[37m[1m[2023-06-25 02:20:29,722][129146] Max Reward on eval: 350.43504484196285
[37m[1m[2023-06-25 02:20:29,723][129146] Min Reward on eval: 350.43504484196285
[37m[1m[2023-06-25 02:20:29,723][129146] Mean Reward across all agents: 350.43504484196285
[37m[1m[2023-06-25 02:20:29,723][129146] Average Trajectory Length: 999.8879999999999
[36m[2023-06-25 02:20:35,408][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:20:35,409][129146] Reward + Measures: [[-1070.2262045      0.17244671     0.18647671     0.15997921
[37m[1m      0.22846213]
[37m[1m [ -796.92409709     0.29762796     0.2748861      0.14857332
[37m[1m      0.3108345 ]
[37m[1m [ -479.01284124     0.21970883     0.21983556     0.21292751
[37m[1m      0.26518974]
[37m[1m ...
[37m[1m [ -151.76013663     0.25362659     0.24975617     0.17525524
[37m[1m      0.26276737]
[37m[1m [ -984.49913868     0.20007439     0.19693397     0.14542061
[37m[1m      0.25656408]
[37m[1m [   87.56009259     0.43590003     0.24130002     0.29709998
[37m[1m      0.3673    ]]
[37m[1m[2023-06-25 02:20:35,409][129146] Max Reward on eval: 459.83931067216906
[37m[1m[2023-06-25 02:20:35,409][129146] Min Reward on eval: -1551.6140353166381
[37m[1m[2023-06-25 02:20:35,409][129146] Mean Reward across all agents: -710.7245012601525
[37m[1m[2023-06-25 02:20:35,410][129146] Average Trajectory Length: 738.563
[36m[2023-06-25 02:20:35,412][129146] mean_value=-937.9107773701612, max_value=959.8393106721691
[37m[1m[2023-06-25 02:20:35,415][129146] New mean coefficients: [[2.8094819  0.03364558 1.5366244  2.5091338  2.5858338 ]]
[37m[1m[2023-06-25 02:20:35,416][129146] Moving the mean solution point...
[36m[2023-06-25 02:20:45,296][129146] train() took 9.88 seconds to complete
[36m[2023-06-25 02:20:45,296][129146] FPS: 388709.54
[36m[2023-06-25 02:20:45,299][129146] itr=122, itrs=2000, Progress: 6.10%
[36m[2023-06-25 02:20:57,003][129146] train() took 11.69 seconds to complete
[36m[2023-06-25 02:20:57,003][129146] FPS: 328443.26
[36m[2023-06-25 02:21:01,816][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:21:01,816][129146] Reward + Measures: [[381.50975404   0.8540433    0.06379357   0.81769222   0.73513687]]
[37m[1m[2023-06-25 02:21:01,816][129146] Max Reward on eval: 381.5097540420474
[37m[1m[2023-06-25 02:21:01,817][129146] Min Reward on eval: 381.5097540420474
[37m[1m[2023-06-25 02:21:01,817][129146] Mean Reward across all agents: 381.5097540420474
[37m[1m[2023-06-25 02:21:01,817][129146] Average Trajectory Length: 996.794
[36m[2023-06-25 02:21:07,217][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:21:07,218][129146] Reward + Measures: [[303.61335769   0.89069998   0.059        0.85810006   0.74680007]
[37m[1m [199.64145878   0.84650004   0.10110001   0.80030006   0.61220008]
[37m[1m [473.73956344   0.73550004   0.13780001   0.6505       0.58630002]
[37m[1m ...
[37m[1m [302.83761626   0.83750004   0.0983       0.77969998   0.64289999]
[37m[1m [153.14606441   0.80140001   0.16390002   0.68990004   0.38350001]
[37m[1m [129.44231216   0.58700001   0.257        0.41490003   0.24779999]]
[37m[1m[2023-06-25 02:21:07,218][129146] Max Reward on eval: 539.4045646388898
[37m[1m[2023-06-25 02:21:07,218][129146] Min Reward on eval: -117.86988851939677
[37m[1m[2023-06-25 02:21:07,219][129146] Mean Reward across all agents: 207.33650161813725
[37m[1m[2023-06-25 02:21:07,219][129146] Average Trajectory Length: 999.4839999999999
[36m[2023-06-25 02:21:07,227][129146] mean_value=250.35407063314696, max_value=1039.40456463889
[37m[1m[2023-06-25 02:21:07,229][129146] New mean coefficients: [[ 2.3776083  -0.20840827  1.2053914   1.7173834   1.6451381 ]]
[37m[1m[2023-06-25 02:21:07,230][129146] Moving the mean solution point...
[36m[2023-06-25 02:21:16,956][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 02:21:16,956][129146] FPS: 394904.84
[36m[2023-06-25 02:21:16,958][129146] itr=123, itrs=2000, Progress: 6.15%
[36m[2023-06-25 02:21:28,552][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 02:21:28,553][129146] FPS: 331625.45
[36m[2023-06-25 02:21:33,262][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:21:33,262][129146] Reward + Measures: [[410.20617063   0.87290305   0.05574904   0.84071535   0.77173418]]
[37m[1m[2023-06-25 02:21:33,262][129146] Max Reward on eval: 410.2061706305295
[37m[1m[2023-06-25 02:21:33,263][129146] Min Reward on eval: 410.2061706305295
[37m[1m[2023-06-25 02:21:33,263][129146] Mean Reward across all agents: 410.2061706305295
[37m[1m[2023-06-25 02:21:33,263][129146] Average Trajectory Length: 998.2233333333332
[36m[2023-06-25 02:21:38,714][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:21:38,719][129146] Reward + Measures: [[409.02511564   0.78179997   0.0923       0.73200005   0.60340005]
[37m[1m [547.24283227   0.70430005   0.13680001   0.61800003   0.49590001]
[37m[1m [454.41673067   0.67883509   0.11182252   0.61069548   0.5188036 ]
[37m[1m ...
[37m[1m [240.38877304   0.64579999   0.22189999   0.51120001   0.29390001]
[37m[1m [462.77915452   0.65880001   0.13789999   0.56810004   0.46790001]
[37m[1m [280.31781894   0.74230003   0.08530001   0.70780003   0.58630002]]
[37m[1m[2023-06-25 02:21:38,719][129146] Max Reward on eval: 681.5085747306584
[37m[1m[2023-06-25 02:21:38,720][129146] Min Reward on eval: -10.780920262634755
[37m[1m[2023-06-25 02:21:38,720][129146] Mean Reward across all agents: 319.81978749401424
[37m[1m[2023-06-25 02:21:38,720][129146] Average Trajectory Length: 996.3456666666666
[36m[2023-06-25 02:21:38,728][129146] mean_value=221.15273711364378, max_value=1058.3762321462796
[37m[1m[2023-06-25 02:21:38,731][129146] New mean coefficients: [[ 2.8900833  -0.35615128  1.1045797   0.8571534   1.2665886 ]]
[37m[1m[2023-06-25 02:21:38,732][129146] Moving the mean solution point...
[36m[2023-06-25 02:21:48,452][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 02:21:48,452][129146] FPS: 395141.04
[36m[2023-06-25 02:21:48,454][129146] itr=124, itrs=2000, Progress: 6.20%
[36m[2023-06-25 02:22:00,213][129146] train() took 11.75 seconds to complete
[36m[2023-06-25 02:22:00,214][129146] FPS: 326911.81
[36m[2023-06-25 02:22:04,940][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:22:04,940][129146] Reward + Measures: [[447.93663251   0.87430418   0.05272768   0.84240472   0.78757972]]
[37m[1m[2023-06-25 02:22:04,940][129146] Max Reward on eval: 447.9366325127744
[37m[1m[2023-06-25 02:22:04,940][129146] Min Reward on eval: 447.9366325127744
[37m[1m[2023-06-25 02:22:04,941][129146] Mean Reward across all agents: 447.9366325127744
[37m[1m[2023-06-25 02:22:04,941][129146] Average Trajectory Length: 996.2969999999999
[36m[2023-06-25 02:22:10,358][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:22:10,364][129146] Reward + Measures: [[ 284.78551651    0.33230001    0.2825        0.1552        0.2476    ]
[37m[1m [ 316.35187668    0.84009999    0.0382        0.81440002    0.7791    ]
[37m[1m [ 486.53411499    0.82030004    0.0841        0.76350003    0.66780007]
[37m[1m ...
[37m[1m [ 420.64686723    0.71990001    0.14030002    0.6304        0.54220003]
[37m[1m [-171.34433457    0.46616936    0.17662369    0.41147479    0.26747537]
[37m[1m [ 227.55903927    0.7816        0.0945        0.73660004    0.64210004]]
[37m[1m[2023-06-25 02:22:10,364][129146] Max Reward on eval: 655.8106438028335
[37m[1m[2023-06-25 02:22:10,365][129146] Min Reward on eval: -485.0355917472858
[37m[1m[2023-06-25 02:22:10,365][129146] Mean Reward across all agents: 259.7788877791492
[37m[1m[2023-06-25 02:22:10,365][129146] Average Trajectory Length: 977.4386666666667
[36m[2023-06-25 02:22:10,371][129146] mean_value=70.2972453390041, max_value=1024.9946429950883
[37m[1m[2023-06-25 02:22:10,373][129146] New mean coefficients: [[ 2.4252872  -0.35046777  1.055177    0.89566565  1.0355456 ]]
[37m[1m[2023-06-25 02:22:10,374][129146] Moving the mean solution point...
[36m[2023-06-25 02:22:20,021][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 02:22:20,021][129146] FPS: 398142.34
[36m[2023-06-25 02:22:20,023][129146] itr=125, itrs=2000, Progress: 6.25%
[36m[2023-06-25 02:22:31,591][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 02:22:31,591][129146] FPS: 332329.51
[36m[2023-06-25 02:22:36,426][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:22:36,427][129146] Reward + Measures: [[512.52582359   0.88169295   0.05133217   0.84918392   0.80440933]]
[37m[1m[2023-06-25 02:22:36,427][129146] Max Reward on eval: 512.5258235948068
[37m[1m[2023-06-25 02:22:36,427][129146] Min Reward on eval: 512.5258235948068
[37m[1m[2023-06-25 02:22:36,427][129146] Mean Reward across all agents: 512.5258235948068
[37m[1m[2023-06-25 02:22:36,427][129146] Average Trajectory Length: 997.9786666666666
[36m[2023-06-25 02:22:41,900][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:22:41,906][129146] Reward + Measures: [[568.89719082   0.64869994   0.2465       0.45699999   0.2613    ]
[37m[1m [408.16221395   0.76559997   0.122        0.70500004   0.5535    ]
[37m[1m [541.2263511    0.75580001   0.0668       0.72799999   0.68559998]
[37m[1m ...
[37m[1m [529.39728576   0.66570002   0.1569       0.5747       0.43470001]
[37m[1m [402.04980556   0.77460003   0.0923       0.72290003   0.6433    ]
[37m[1m [601.46443765   0.77170002   0.11790001   0.71259999   0.61970001]]
[37m[1m[2023-06-25 02:22:41,906][129146] Max Reward on eval: 705.6779797756578
[37m[1m[2023-06-25 02:22:41,906][129146] Min Reward on eval: -39.84531506735366
[37m[1m[2023-06-25 02:22:41,906][129146] Mean Reward across all agents: 462.003302739778
[37m[1m[2023-06-25 02:22:41,907][129146] Average Trajectory Length: 997.1519999999999
[36m[2023-06-25 02:22:41,916][129146] mean_value=254.2406709164606, max_value=1144.75866678406
[37m[1m[2023-06-25 02:22:41,919][129146] New mean coefficients: [[ 2.487705   -0.8074137   0.73858666 -0.03971833  0.32356435]]
[37m[1m[2023-06-25 02:22:41,920][129146] Moving the mean solution point...
[36m[2023-06-25 02:22:51,700][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 02:22:51,700][129146] FPS: 392683.34
[36m[2023-06-25 02:22:51,703][129146] itr=126, itrs=2000, Progress: 6.30%
[36m[2023-06-25 02:23:03,265][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 02:23:03,265][129146] FPS: 332551.31
[36m[2023-06-25 02:23:08,055][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:23:08,055][129146] Reward + Measures: [[582.45385997   0.86133021   0.05914225   0.82291943   0.79065806]]
[37m[1m[2023-06-25 02:23:08,055][129146] Max Reward on eval: 582.4538599716127
[37m[1m[2023-06-25 02:23:08,055][129146] Min Reward on eval: 582.4538599716127
[37m[1m[2023-06-25 02:23:08,056][129146] Mean Reward across all agents: 582.4538599716127
[37m[1m[2023-06-25 02:23:08,056][129146] Average Trajectory Length: 997.384
[36m[2023-06-25 02:23:13,637][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:23:13,643][129146] Reward + Measures: [[344.96712632   0.86120003   0.08880001   0.79680002   0.63000005]
[37m[1m [500.34656667   0.68830007   0.0933       0.6347       0.57620001]
[37m[1m [588.54013974   0.60589999   0.1736       0.50550002   0.48950002]
[37m[1m ...
[37m[1m [294.30478309   0.86070007   0.10520001   0.78820002   0.60390002]
[37m[1m [550.78740341   0.80830002   0.0624       0.778        0.72850001]
[37m[1m [473.40890382   0.79870003   0.0851       0.7367       0.64889997]]
[37m[1m[2023-06-25 02:23:13,643][129146] Max Reward on eval: 684.5290025289753
[37m[1m[2023-06-25 02:23:13,644][129146] Min Reward on eval: 73.39343274694401
[37m[1m[2023-06-25 02:23:13,644][129146] Mean Reward across all agents: 472.151949544893
[37m[1m[2023-06-25 02:23:13,644][129146] Average Trajectory Length: 996.0419999999999
[36m[2023-06-25 02:23:13,650][129146] mean_value=135.0901871357012, max_value=1142.9694857336349
[37m[1m[2023-06-25 02:23:13,652][129146] New mean coefficients: [[ 2.0528247  -0.9598452   0.7055268  -0.60528266  0.09916539]]
[37m[1m[2023-06-25 02:23:13,653][129146] Moving the mean solution point...
[36m[2023-06-25 02:23:23,352][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 02:23:23,352][129146] FPS: 395994.73
[36m[2023-06-25 02:23:23,355][129146] itr=127, itrs=2000, Progress: 6.35%
[36m[2023-06-25 02:23:35,173][129146] train() took 11.80 seconds to complete
[36m[2023-06-25 02:23:35,173][129146] FPS: 325324.38
[36m[2023-06-25 02:23:39,861][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:23:39,861][129146] Reward + Measures: [[620.66274959   0.84978342   0.06126404   0.81085676   0.78857362]]
[37m[1m[2023-06-25 02:23:39,861][129146] Max Reward on eval: 620.6627495893882
[37m[1m[2023-06-25 02:23:39,862][129146] Min Reward on eval: 620.6627495893882
[37m[1m[2023-06-25 02:23:39,862][129146] Mean Reward across all agents: 620.6627495893882
[37m[1m[2023-06-25 02:23:39,862][129146] Average Trajectory Length: 996.644
[36m[2023-06-25 02:23:45,333][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:23:45,333][129146] Reward + Measures: [[632.3652249    0.77160007   0.07740001   0.72500002   0.72069997]
[37m[1m [578.37360722   0.66820002   0.09340001   0.60470003   0.57969999]
[37m[1m [668.76547771   0.45280004   0.2185       0.33249998   0.43000004]
[37m[1m ...
[37m[1m [510.64243145   0.50200003   0.1365       0.43280002   0.42199999]
[37m[1m [504.74072      0.82300007   0.1032       0.76969999   0.72310001]
[37m[1m [697.94664691   0.89569998   0.0442       0.86639994   0.84450006]]
[37m[1m[2023-06-25 02:23:45,334][129146] Max Reward on eval: 900.774064259103
[37m[1m[2023-06-25 02:23:45,334][129146] Min Reward on eval: 233.7075808462454
[37m[1m[2023-06-25 02:23:45,334][129146] Mean Reward across all agents: 593.189438305159
[37m[1m[2023-06-25 02:23:45,334][129146] Average Trajectory Length: 993.29
[36m[2023-06-25 02:23:45,343][129146] mean_value=399.7651409958063, max_value=1201.1089798408561
[37m[1m[2023-06-25 02:23:45,346][129146] New mean coefficients: [[ 2.585228   -1.3464072   1.1461034  -0.92818576 -0.45321965]]
[37m[1m[2023-06-25 02:23:45,347][129146] Moving the mean solution point...
[36m[2023-06-25 02:23:55,060][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 02:23:55,061][129146] FPS: 395380.55
[36m[2023-06-25 02:23:55,063][129146] itr=128, itrs=2000, Progress: 6.40%
[36m[2023-06-25 02:24:06,797][129146] train() took 11.72 seconds to complete
[36m[2023-06-25 02:24:06,797][129146] FPS: 327664.63
[36m[2023-06-25 02:24:11,702][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:24:11,702][129146] Reward + Measures: [[672.16012711   0.75659353   0.10006329   0.69410169   0.68135244]]
[37m[1m[2023-06-25 02:24:11,702][129146] Max Reward on eval: 672.1601271138527
[37m[1m[2023-06-25 02:24:11,703][129146] Min Reward on eval: 672.1601271138527
[37m[1m[2023-06-25 02:24:11,703][129146] Mean Reward across all agents: 672.1601271138527
[37m[1m[2023-06-25 02:24:11,703][129146] Average Trajectory Length: 996.891
[36m[2023-06-25 02:24:17,199][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:24:17,199][129146] Reward + Measures: [[559.41959259   0.78689998   0.0921       0.72790003   0.69150001]
[37m[1m [274.6907571    0.245923     0.20610733   0.15840986   0.2194494 ]
[37m[1m [148.58800834   0.33912817   0.2116835    0.2193806    0.23804274]
[37m[1m ...
[37m[1m [-90.96949268   0.23788534   0.18484487   0.14667922   0.14605592]
[37m[1m [641.75784752   0.7610001    0.1209       0.69250005   0.66180003]
[37m[1m [598.41191219   0.73860008   0.14140001   0.6602       0.63210005]]
[37m[1m[2023-06-25 02:24:17,199][129146] Max Reward on eval: 731.8992232831573
[37m[1m[2023-06-25 02:24:17,199][129146] Min Reward on eval: -487.8722248253995
[37m[1m[2023-06-25 02:24:17,200][129146] Mean Reward across all agents: 242.94169702974483
[37m[1m[2023-06-25 02:24:17,200][129146] Average Trajectory Length: 894.7726666666666
[36m[2023-06-25 02:24:17,205][129146] mean_value=-145.6228628372604, max_value=1163.7813714027404
[37m[1m[2023-06-25 02:24:17,208][129146] New mean coefficients: [[ 2.8076286  -1.0139132   1.662817    0.42182976 -0.13688534]]
[37m[1m[2023-06-25 02:24:17,209][129146] Moving the mean solution point...
[36m[2023-06-25 02:24:27,036][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 02:24:27,036][129146] FPS: 390820.28
[36m[2023-06-25 02:24:27,038][129146] itr=129, itrs=2000, Progress: 6.45%
[36m[2023-06-25 02:24:38,468][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 02:24:38,469][129146] FPS: 336420.11
[36m[2023-06-25 02:24:43,267][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:24:43,268][129146] Reward + Measures: [[722.3770081    0.6378265    0.15417232   0.54151016   0.54806572]]
[37m[1m[2023-06-25 02:24:43,268][129146] Max Reward on eval: 722.3770081018085
[37m[1m[2023-06-25 02:24:43,268][129146] Min Reward on eval: 722.3770081018085
[37m[1m[2023-06-25 02:24:43,268][129146] Mean Reward across all agents: 722.3770081018085
[37m[1m[2023-06-25 02:24:43,269][129146] Average Trajectory Length: 996.2793333333333
[36m[2023-06-25 02:24:48,665][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:24:48,670][129146] Reward + Measures: [[579.47513159   0.52200001   0.2181       0.40120003   0.27920002]
[37m[1m [526.8771991    0.39769998   0.28999999   0.193        0.27669999]
[37m[1m [297.90928884   0.50419998   0.22119999   0.42220002   0.26139998]
[37m[1m ...
[37m[1m [428.60825437   0.36069998   0.27500001   0.16069999   0.26390001]
[37m[1m [359.13111081   0.58690757   0.19188617   0.50666416   0.31512591]
[37m[1m [668.24840613   0.42130002   0.27540001   0.28150001   0.3633    ]]
[37m[1m[2023-06-25 02:24:48,670][129146] Max Reward on eval: 832.9214491308434
[37m[1m[2023-06-25 02:24:48,671][129146] Min Reward on eval: -156.69442376532533
[37m[1m[2023-06-25 02:24:48,671][129146] Mean Reward across all agents: 481.4811637401294
[37m[1m[2023-06-25 02:24:48,671][129146] Average Trajectory Length: 980.0806666666666
[36m[2023-06-25 02:24:48,678][129146] mean_value=100.81258136716642, max_value=1014.0631739542524
[37m[1m[2023-06-25 02:24:48,680][129146] New mean coefficients: [[ 2.432716   -1.1698927   1.9801259   0.58755654 -0.07897213]]
[37m[1m[2023-06-25 02:24:48,681][129146] Moving the mean solution point...
[36m[2023-06-25 02:24:58,393][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 02:24:58,393][129146] FPS: 395472.16
[36m[2023-06-25 02:24:58,395][129146] itr=130, itrs=2000, Progress: 6.50%
[37m[1m[2023-06-25 02:25:00,716][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000110
[36m[2023-06-25 02:25:12,689][129146] train() took 11.65 seconds to complete
[36m[2023-06-25 02:25:12,689][129146] FPS: 329684.97
[36m[2023-06-25 02:25:17,428][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:25:17,429][129146] Reward + Measures: [[823.64935113   0.48197329   0.22267126   0.34131241   0.38742036]]
[37m[1m[2023-06-25 02:25:17,429][129146] Max Reward on eval: 823.6493511291377
[37m[1m[2023-06-25 02:25:17,429][129146] Min Reward on eval: 823.6493511291377
[37m[1m[2023-06-25 02:25:17,429][129146] Mean Reward across all agents: 823.6493511291377
[37m[1m[2023-06-25 02:25:17,429][129146] Average Trajectory Length: 995.5346666666667
[36m[2023-06-25 02:25:23,016][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:25:23,017][129146] Reward + Measures: [[730.65723942   0.3572       0.28940001   0.17260002   0.26680002]
[37m[1m [ -5.13502491   0.47320005   0.26970002   0.33830002   0.33649999]
[37m[1m [576.61485755   0.42880902   0.26620576   0.25640616   0.30224198]
[37m[1m ...
[37m[1m [466.82939864   0.35330001   0.31723332   0.2066       0.24483334]
[37m[1m [132.5724268    0.45889997   0.23120001   0.33090004   0.37690002]
[37m[1m [-26.54597216   0.2996982    0.33084652   0.12522475   0.2936047 ]]
[37m[1m[2023-06-25 02:25:23,017][129146] Max Reward on eval: 934.2939045358332
[37m[1m[2023-06-25 02:25:23,017][129146] Min Reward on eval: -336.35579723085976
[37m[1m[2023-06-25 02:25:23,017][129146] Mean Reward across all agents: 344.11050430180717
[37m[1m[2023-06-25 02:25:23,018][129146] Average Trajectory Length: 970.1186666666666
[36m[2023-06-25 02:25:23,022][129146] mean_value=-124.5723287550069, max_value=686.330763840787
[37m[1m[2023-06-25 02:25:23,025][129146] New mean coefficients: [[ 2.3091135  -1.2795895   1.8021722   0.1800299  -0.24159323]]
[37m[1m[2023-06-25 02:25:23,026][129146] Moving the mean solution point...
[36m[2023-06-25 02:25:32,817][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 02:25:32,818][129146] FPS: 392242.03
[36m[2023-06-25 02:25:32,820][129146] itr=131, itrs=2000, Progress: 6.55%
[36m[2023-06-25 02:25:44,458][129146] train() took 11.62 seconds to complete
[36m[2023-06-25 02:25:44,458][129146] FPS: 330389.29
[36m[2023-06-25 02:25:49,252][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:25:49,253][129146] Reward + Measures: [[878.72308197   0.41527238   0.25331107   0.25480384   0.32230365]]
[37m[1m[2023-06-25 02:25:49,253][129146] Max Reward on eval: 878.7230819683839
[37m[1m[2023-06-25 02:25:49,253][129146] Min Reward on eval: 878.7230819683839
[37m[1m[2023-06-25 02:25:49,253][129146] Mean Reward across all agents: 878.7230819683839
[37m[1m[2023-06-25 02:25:49,254][129146] Average Trajectory Length: 993.2923333333333
[36m[2023-06-25 02:25:54,709][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:25:54,715][129146] Reward + Measures: [[567.35470973   0.3407       0.2139       0.20079999   0.24479997]
[37m[1m [822.69764746   0.3044       0.24749999   0.15100001   0.2572    ]
[37m[1m [778.70314448   0.43780002   0.34199998   0.191        0.28169999]
[37m[1m ...
[37m[1m [979.06581101   0.3355       0.28120002   0.17549999   0.29140002]
[37m[1m [721.83943707   0.34258965   0.27910104   0.14243679   0.26122853]
[37m[1m [531.29504656   0.4382       0.25780001   0.24439998   0.27970001]]
[37m[1m[2023-06-25 02:25:54,716][129146] Max Reward on eval: 1151.270667004434
[37m[1m[2023-06-25 02:25:54,716][129146] Min Reward on eval: -295.2026375668589
[37m[1m[2023-06-25 02:25:54,717][129146] Mean Reward across all agents: 637.0644657617999
[37m[1m[2023-06-25 02:25:54,717][129146] Average Trajectory Length: 984.9843333333333
[36m[2023-06-25 02:25:54,727][129146] mean_value=57.22694901840604, max_value=1267.547588795513
[37m[1m[2023-06-25 02:25:54,731][129146] New mean coefficients: [[ 1.887135   -1.4846425   1.739077   -0.16275781 -0.3003836 ]]
[37m[1m[2023-06-25 02:25:54,732][129146] Moving the mean solution point...
[36m[2023-06-25 02:26:04,573][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 02:26:04,574][129146] FPS: 390279.33
[36m[2023-06-25 02:26:04,576][129146] itr=132, itrs=2000, Progress: 6.60%
[36m[2023-06-25 02:26:16,233][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 02:26:16,233][129146] FPS: 329780.15
[36m[2023-06-25 02:26:21,111][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:26:21,111][129146] Reward + Measures: [[926.39365544   0.38336647   0.27725491   0.20594218   0.29407462]]
[37m[1m[2023-06-25 02:26:21,111][129146] Max Reward on eval: 926.3936554440783
[37m[1m[2023-06-25 02:26:21,112][129146] Min Reward on eval: 926.3936554440783
[37m[1m[2023-06-25 02:26:21,112][129146] Mean Reward across all agents: 926.3936554440783
[37m[1m[2023-06-25 02:26:21,112][129146] Average Trajectory Length: 993.5073333333333
[36m[2023-06-25 02:26:26,650][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:26:26,651][129146] Reward + Measures: [[866.70823322   0.32020003   0.2755       0.14310001   0.26690003]
[37m[1m [711.00737641   0.34440106   0.25467119   0.19842356   0.25980052]
[37m[1m [568.85092384   0.39757764   0.29808977   0.23043935   0.29395419]
[37m[1m ...
[37m[1m [880.60418443   0.34709999   0.30520001   0.1547       0.25120002]
[37m[1m [750.66491199   0.24459998   0.2218       0.13530001   0.20840001]
[37m[1m [740.89645848   0.42040005   0.22790001   0.28379998   0.33270001]]
[37m[1m[2023-06-25 02:26:26,651][129146] Max Reward on eval: 1029.0373329829424
[37m[1m[2023-06-25 02:26:26,652][129146] Min Reward on eval: 478.35239470425296
[37m[1m[2023-06-25 02:26:26,652][129146] Mean Reward across all agents: 801.3771065905551
[37m[1m[2023-06-25 02:26:26,652][129146] Average Trajectory Length: 986.7429999999999
[36m[2023-06-25 02:26:26,656][129146] mean_value=7.040692004872168, max_value=1215.1478142845435
[37m[1m[2023-06-25 02:26:26,658][129146] New mean coefficients: [[ 1.6799269  -1.1306093   1.4669306  -0.09022121 -0.02580824]]
[37m[1m[2023-06-25 02:26:26,659][129146] Moving the mean solution point...
[36m[2023-06-25 02:26:36,349][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 02:26:36,349][129146] FPS: 396374.12
[36m[2023-06-25 02:26:36,351][129146] itr=133, itrs=2000, Progress: 6.65%
[36m[2023-06-25 02:26:47,916][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 02:26:47,916][129146] FPS: 332473.39
[36m[2023-06-25 02:26:52,708][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:26:52,708][129146] Reward + Measures: [[993.64668945   0.34456834   0.28335214   0.1631346    0.27843478]]
[37m[1m[2023-06-25 02:26:52,708][129146] Max Reward on eval: 993.6466894473424
[37m[1m[2023-06-25 02:26:52,709][129146] Min Reward on eval: 993.6466894473424
[37m[1m[2023-06-25 02:26:52,709][129146] Mean Reward across all agents: 993.6466894473424
[37m[1m[2023-06-25 02:26:52,709][129146] Average Trajectory Length: 990.8033333333333
[36m[2023-06-25 02:26:58,227][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:26:58,228][129146] Reward + Measures: [[921.94134777   0.31550002   0.30029997   0.1155       0.27090001]
[37m[1m [674.14368293   0.3989       0.27599999   0.25830001   0.29440004]
[37m[1m [768.87498906   0.54358113   0.22438207   0.39869571   0.39241686]
[37m[1m ...
[37m[1m [849.9574914    0.26120001   0.2554       0.11980001   0.25400001]
[37m[1m [702.55491611   0.39750001   0.24089999   0.27680001   0.28369999]
[37m[1m [-63.58669507   0.24006711   0.23081918   0.12753837   0.22131388]]
[37m[1m[2023-06-25 02:26:58,228][129146] Max Reward on eval: 1106.9597719412645
[37m[1m[2023-06-25 02:26:58,228][129146] Min Reward on eval: -178.1278635404189
[37m[1m[2023-06-25 02:26:58,229][129146] Mean Reward across all agents: 565.6223529697828
[37m[1m[2023-06-25 02:26:58,229][129146] Average Trajectory Length: 966.858
[36m[2023-06-25 02:26:58,235][129146] mean_value=11.403736983136453, max_value=928.9311839130986
[37m[1m[2023-06-25 02:26:58,238][129146] New mean coefficients: [[ 1.9183958  -1.2890682   1.3723001  -0.55621535 -0.20978133]]
[37m[1m[2023-06-25 02:26:58,239][129146] Moving the mean solution point...
[36m[2023-06-25 02:27:08,054][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 02:27:08,054][129146] FPS: 391302.59
[36m[2023-06-25 02:27:08,056][129146] itr=134, itrs=2000, Progress: 6.70%
[36m[2023-06-25 02:27:19,718][129146] train() took 11.65 seconds to complete
[36m[2023-06-25 02:27:19,718][129146] FPS: 329714.38
[36m[2023-06-25 02:27:24,506][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:27:24,506][129146] Reward + Measures: [[1029.4384802     0.32990873    0.27713153    0.14974324    0.26971093]]
[37m[1m[2023-06-25 02:27:24,506][129146] Max Reward on eval: 1029.4384802028787
[37m[1m[2023-06-25 02:27:24,507][129146] Min Reward on eval: 1029.4384802028787
[37m[1m[2023-06-25 02:27:24,507][129146] Mean Reward across all agents: 1029.4384802028787
[37m[1m[2023-06-25 02:27:24,507][129146] Average Trajectory Length: 990.1406666666667
[36m[2023-06-25 02:27:29,949][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:27:29,954][129146] Reward + Measures: [[911.81655937   0.28420001   0.2665       0.13570002   0.2755    ]
[37m[1m [937.34248686   0.28410003   0.28280002   0.0972       0.27159998]
[37m[1m [901.78726118   0.29530001   0.27559999   0.11930001   0.2642    ]
[37m[1m ...
[37m[1m [920.96301703   0.24600001   0.24160002   0.11369999   0.27169999]
[37m[1m [939.38481858   0.2595       0.25049999   0.0958       0.25310001]
[37m[1m [769.32577691   0.28599998   0.23029999   0.15570001   0.27589998]]
[37m[1m[2023-06-25 02:27:29,954][129146] Max Reward on eval: 1225.5628765900387
[37m[1m[2023-06-25 02:27:29,955][129146] Min Reward on eval: 466.2641303097131
[37m[1m[2023-06-25 02:27:29,955][129146] Mean Reward across all agents: 941.5693683782939
[37m[1m[2023-06-25 02:27:29,955][129146] Average Trajectory Length: 991.3463333333333
[36m[2023-06-25 02:27:29,960][129146] mean_value=132.91172119605943, max_value=965.0825660654763
[37m[1m[2023-06-25 02:27:29,963][129146] New mean coefficients: [[ 3.0150628  -1.4927113   1.1280041  -1.4323442  -0.23084041]]
[37m[1m[2023-06-25 02:27:29,964][129146] Moving the mean solution point...
[36m[2023-06-25 02:27:39,738][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 02:27:39,738][129146] FPS: 392957.09
[36m[2023-06-25 02:27:39,740][129146] itr=135, itrs=2000, Progress: 6.75%
[36m[2023-06-25 02:27:51,406][129146] train() took 11.65 seconds to complete
[36m[2023-06-25 02:27:51,406][129146] FPS: 329551.51
[36m[2023-06-25 02:27:56,349][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:27:56,350][129146] Reward + Measures: [[1081.91537748    0.31458181    0.27514172    0.13488583    0.26619038]]
[37m[1m[2023-06-25 02:27:56,350][129146] Max Reward on eval: 1081.9153774771694
[37m[1m[2023-06-25 02:27:56,350][129146] Min Reward on eval: 1081.9153774771694
[37m[1m[2023-06-25 02:27:56,350][129146] Mean Reward across all agents: 1081.9153774771694
[37m[1m[2023-06-25 02:27:56,351][129146] Average Trajectory Length: 987.8606666666666
[36m[2023-06-25 02:28:01,979][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:28:01,985][129146] Reward + Measures: [[1236.71461916    0.36289999    0.28080001    0.1339        0.27150002]
[37m[1m [ 128.84141237    0.37559998    0.48720002    0.1072        0.41079998]
[37m[1m [ 159.690466      0.34060001    0.40970001    0.18519999    0.3373    ]
[37m[1m ...
[37m[1m [1007.32436632    0.33970001    0.33409998    0.15410002    0.2845    ]
[37m[1m [ 381.89898215    0.4535        0.34010002    0.28299999    0.37970001]
[37m[1m [1098.5311595     0.31300002    0.2987        0.1103        0.2579    ]]
[37m[1m[2023-06-25 02:28:01,985][129146] Max Reward on eval: 1236.7146191637148
[37m[1m[2023-06-25 02:28:01,986][129146] Min Reward on eval: -672.9006610484096
[37m[1m[2023-06-25 02:28:01,986][129146] Mean Reward across all agents: 437.2265997967549
[37m[1m[2023-06-25 02:28:01,986][129146] Average Trajectory Length: 988.3406666666666
[36m[2023-06-25 02:28:01,991][129146] mean_value=7.620211369011716, max_value=1123.7365899537224
[37m[1m[2023-06-25 02:28:01,994][129146] New mean coefficients: [[ 3.018929  -1.7358823  1.0761247 -1.9296601 -1.0459288]]
[37m[1m[2023-06-25 02:28:01,995][129146] Moving the mean solution point...
[36m[2023-06-25 02:28:11,793][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 02:28:11,794][129146] FPS: 391967.48
[36m[2023-06-25 02:28:11,796][129146] itr=136, itrs=2000, Progress: 6.80%
[36m[2023-06-25 02:28:23,427][129146] train() took 11.62 seconds to complete
[36m[2023-06-25 02:28:23,427][129146] FPS: 330542.33
[36m[2023-06-25 02:28:28,261][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:28:28,261][129146] Reward + Measures: [[1128.63905246    0.30503488    0.27330005    0.12497436    0.26541603]]
[37m[1m[2023-06-25 02:28:28,261][129146] Max Reward on eval: 1128.639052457833
[37m[1m[2023-06-25 02:28:28,261][129146] Min Reward on eval: 1128.639052457833
[37m[1m[2023-06-25 02:28:28,262][129146] Mean Reward across all agents: 1128.639052457833
[37m[1m[2023-06-25 02:28:28,262][129146] Average Trajectory Length: 982.7606666666667
[36m[2023-06-25 02:28:33,818][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:28:33,819][129146] Reward + Measures: [[ 923.79393172    0.22545706    0.23314393    0.10622185    0.22604559]
[37m[1m [ 872.54744381    0.30280003    0.25650001    0.14920001    0.23380001]
[37m[1m [ 961.17006934    0.28182328    0.30187884    0.12887935    0.26186138]
[37m[1m ...
[37m[1m [1072.4913064     0.33452708    0.2798489     0.1219677     0.26260263]
[37m[1m [1040.16598771    0.26429999    0.26229998    0.0984        0.2289    ]
[37m[1m [ 827.47133875    0.21469998    0.21040002    0.10500001    0.22239999]]
[37m[1m[2023-06-25 02:28:33,819][129146] Max Reward on eval: 1255.151512164343
[37m[1m[2023-06-25 02:28:33,819][129146] Min Reward on eval: 464.7660079172289
[37m[1m[2023-06-25 02:28:33,820][129146] Mean Reward across all agents: 914.0888862710834
[37m[1m[2023-06-25 02:28:33,820][129146] Average Trajectory Length: 946.836
[36m[2023-06-25 02:28:33,823][129146] mean_value=-24.229188445814785, max_value=1112.1018417417974
[37m[1m[2023-06-25 02:28:33,826][129146] New mean coefficients: [[ 2.5459697 -1.4620452  0.6641664 -1.236385  -1.215608 ]]
[37m[1m[2023-06-25 02:28:33,827][129146] Moving the mean solution point...
[36m[2023-06-25 02:28:43,759][129146] train() took 9.93 seconds to complete
[36m[2023-06-25 02:28:43,760][129146] FPS: 386687.00
[36m[2023-06-25 02:28:43,762][129146] itr=137, itrs=2000, Progress: 6.85%
[36m[2023-06-25 02:28:55,322][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 02:28:55,322][129146] FPS: 332553.63
[36m[2023-06-25 02:29:00,154][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:29:00,154][129146] Reward + Measures: [[1202.78762076    0.29719502    0.27010444    0.11303985    0.26275456]]
[37m[1m[2023-06-25 02:29:00,155][129146] Max Reward on eval: 1202.787620759103
[37m[1m[2023-06-25 02:29:00,155][129146] Min Reward on eval: 1202.787620759103
[37m[1m[2023-06-25 02:29:00,155][129146] Mean Reward across all agents: 1202.787620759103
[37m[1m[2023-06-25 02:29:00,155][129146] Average Trajectory Length: 982.0413333333333
[36m[2023-06-25 02:29:05,663][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:29:05,664][129146] Reward + Measures: [[1048.02342042    0.30819535    0.26945814    0.13808373    0.25970697]
[37m[1m [ 789.63735011    0.24345362    0.24074839    0.08921106    0.21798651]
[37m[1m [1120.90525302    0.26678929    0.25027773    0.07608213    0.23106289]
[37m[1m ...
[37m[1m [1308.10359733    0.285         0.27630001    0.0841        0.2807    ]
[37m[1m [1085.89140519    0.24277328    0.22140642    0.08739634    0.23914233]
[37m[1m [1040.03093917    0.26550001    0.22649999    0.11570001    0.26400003]]
[37m[1m[2023-06-25 02:29:05,664][129146] Max Reward on eval: 1420.9436116411466
[37m[1m[2023-06-25 02:29:05,665][129146] Min Reward on eval: 531.7801748012076
[37m[1m[2023-06-25 02:29:05,665][129146] Mean Reward across all agents: 1029.4693381025052
[37m[1m[2023-06-25 02:29:05,665][129146] Average Trajectory Length: 942.1156666666666
[36m[2023-06-25 02:29:05,671][129146] mean_value=81.37824301800596, max_value=849.6219669175507
[37m[1m[2023-06-25 02:29:05,674][129146] New mean coefficients: [[ 1.7696694  -1.5641317   0.23844895 -1.5081207  -1.5556345 ]]
[37m[1m[2023-06-25 02:29:05,675][129146] Moving the mean solution point...
[36m[2023-06-25 02:29:15,544][129146] train() took 9.87 seconds to complete
[36m[2023-06-25 02:29:15,544][129146] FPS: 389173.10
[36m[2023-06-25 02:29:15,546][129146] itr=138, itrs=2000, Progress: 6.90%
[36m[2023-06-25 02:29:27,092][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 02:29:27,092][129146] FPS: 332958.00
[36m[2023-06-25 02:29:31,876][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:29:31,877][129146] Reward + Measures: [[1253.76076617    0.29490629    0.26901421    0.10533811    0.2594142 ]]
[37m[1m[2023-06-25 02:29:31,877][129146] Max Reward on eval: 1253.7607661664572
[37m[1m[2023-06-25 02:29:31,877][129146] Min Reward on eval: 1253.7607661664572
[37m[1m[2023-06-25 02:29:31,878][129146] Mean Reward across all agents: 1253.7607661664572
[37m[1m[2023-06-25 02:29:31,878][129146] Average Trajectory Length: 978.507
[36m[2023-06-25 02:29:37,360][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:29:37,361][129146] Reward + Measures: [[-175.81068631    0.30174729    0.27446875    0.17307954    0.24189854]
[37m[1m [1117.69690011    0.289         0.25050002    0.15390001    0.26970002]
[37m[1m [  -4.04800194    0.2427725     0.21425472    0.13052614    0.17100845]
[37m[1m ...
[37m[1m [ 754.08385322    0.33970004    0.2527        0.14999999    0.2251    ]
[37m[1m [ 141.5384749     0.20777757    0.18217865    0.12641664    0.21947877]
[37m[1m [ 353.69478092    0.23886918    0.21497814    0.10925212    0.18759738]]
[37m[1m[2023-06-25 02:29:37,361][129146] Max Reward on eval: 1389.9495147491368
[37m[1m[2023-06-25 02:29:37,361][129146] Min Reward on eval: -747.3011181406094
[37m[1m[2023-06-25 02:29:37,361][129146] Mean Reward across all agents: 215.86703323081016
[37m[1m[2023-06-25 02:29:37,362][129146] Average Trajectory Length: 855.81
[36m[2023-06-25 02:29:37,364][129146] mean_value=-670.9015597281657, max_value=876.0969132965693
[37m[1m[2023-06-25 02:29:37,366][129146] New mean coefficients: [[ 1.5912191  -1.1003447   0.72204006 -0.21874094 -0.7641833 ]]
[37m[1m[2023-06-25 02:29:37,367][129146] Moving the mean solution point...
[36m[2023-06-25 02:29:47,129][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 02:29:47,129][129146] FPS: 393445.55
[36m[2023-06-25 02:29:47,131][129146] itr=139, itrs=2000, Progress: 6.95%
[36m[2023-06-25 02:29:58,603][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 02:29:58,603][129146] FPS: 335132.85
[36m[2023-06-25 02:30:03,418][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:30:03,419][129146] Reward + Measures: [[1289.5440409     0.28385964    0.26405352    0.0990037     0.25603697]]
[37m[1m[2023-06-25 02:30:03,419][129146] Max Reward on eval: 1289.544040901158
[37m[1m[2023-06-25 02:30:03,419][129146] Min Reward on eval: 1289.544040901158
[37m[1m[2023-06-25 02:30:03,419][129146] Mean Reward across all agents: 1289.544040901158
[37m[1m[2023-06-25 02:30:03,420][129146] Average Trajectory Length: 977.5413333333333
[36m[2023-06-25 02:30:09,162][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:30:09,163][129146] Reward + Measures: [[1248.17387697    0.2881        0.25110003    0.1           0.2581    ]
[37m[1m [1486.65179275    0.30669999    0.30360001    0.0865        0.27879998]
[37m[1m [1157.11577705    0.29211363    0.31106591    0.07757045    0.28430459]
[37m[1m ...
[37m[1m [1318.94154951    0.29499999    0.30430004    0.0844        0.26409999]
[37m[1m [1077.36233277    0.35440001    0.29930001    0.18920001    0.23450001]
[37m[1m [ 757.92504556    0.34650001    0.2516        0.1925        0.2247    ]]
[37m[1m[2023-06-25 02:30:09,163][129146] Max Reward on eval: 1486.6517927534064
[37m[1m[2023-06-25 02:30:09,163][129146] Min Reward on eval: 393.32459656047865
[37m[1m[2023-06-25 02:30:09,163][129146] Mean Reward across all agents: 1120.0638944257973
[37m[1m[2023-06-25 02:30:09,163][129146] Average Trajectory Length: 981.3576666666667
[36m[2023-06-25 02:30:09,169][129146] mean_value=44.77516871044198, max_value=631.8130041650884
[37m[1m[2023-06-25 02:30:09,171][129146] New mean coefficients: [[ 1.8298221  -1.258345    0.46996894 -0.58995074 -0.8057455 ]]
[37m[1m[2023-06-25 02:30:09,172][129146] Moving the mean solution point...
[36m[2023-06-25 02:30:19,186][129146] train() took 10.01 seconds to complete
[36m[2023-06-25 02:30:19,186][129146] FPS: 383544.71
[36m[2023-06-25 02:30:19,189][129146] itr=140, itrs=2000, Progress: 7.00%
[37m[1m[2023-06-25 02:30:21,579][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000120
[36m[2023-06-25 02:30:33,630][129146] train() took 11.72 seconds to complete
[36m[2023-06-25 02:30:33,630][129146] FPS: 327575.05
[36m[2023-06-25 02:30:38,449][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:30:38,449][129146] Reward + Measures: [[1309.94715078    0.27451396    0.25955188    0.09264962    0.25020719]]
[37m[1m[2023-06-25 02:30:38,449][129146] Max Reward on eval: 1309.9471507800179
[37m[1m[2023-06-25 02:30:38,449][129146] Min Reward on eval: 1309.9471507800179
[37m[1m[2023-06-25 02:30:38,450][129146] Mean Reward across all agents: 1309.9471507800179
[37m[1m[2023-06-25 02:30:38,450][129146] Average Trajectory Length: 965.3466666666666
[36m[2023-06-25 02:30:43,889][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:30:43,889][129146] Reward + Measures: [[ 130.00420651    0.23655334    0.25686499    0.15355664    0.17984556]
[37m[1m [ -16.26720926    0.19957356    0.25554174    0.15714383    0.18150751]
[37m[1m [ 549.45292454    0.23701687    0.27335021    0.09680025    0.17426433]
[37m[1m ...
[37m[1m [ 862.59526478    0.30045715    0.32025716    0.10625239    0.24139524]
[37m[1m [1235.78183279    0.28337246    0.27374059    0.11539855    0.24840143]
[37m[1m [ 487.46782985    0.32440001    0.34549999    0.17550001    0.25390002]]
[37m[1m[2023-06-25 02:30:43,889][129146] Max Reward on eval: 1571.3702077065943
[37m[1m[2023-06-25 02:30:43,890][129146] Min Reward on eval: -313.6823058415728
[37m[1m[2023-06-25 02:30:43,890][129146] Mean Reward across all agents: 723.8232615067961
[37m[1m[2023-06-25 02:30:43,890][129146] Average Trajectory Length: 955.3756666666667
[36m[2023-06-25 02:30:43,893][129146] mean_value=-352.76749737154177, max_value=908.0368560594245
[37m[1m[2023-06-25 02:30:43,896][129146] New mean coefficients: [[ 1.8343607  -1.122571    0.60542405 -0.1362626  -0.6306735 ]]
[37m[1m[2023-06-25 02:30:43,897][129146] Moving the mean solution point...
[36m[2023-06-25 02:30:53,646][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 02:30:53,647][129146] FPS: 393941.22
[36m[2023-06-25 02:30:53,649][129146] itr=141, itrs=2000, Progress: 7.05%
[36m[2023-06-25 02:31:05,092][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 02:31:05,092][129146] FPS: 335969.31
[36m[2023-06-25 02:31:09,855][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:31:09,855][129146] Reward + Measures: [[1354.29862046    0.27123731    0.2563239     0.08531036    0.24487874]]
[37m[1m[2023-06-25 02:31:09,855][129146] Max Reward on eval: 1354.2986204643378
[37m[1m[2023-06-25 02:31:09,856][129146] Min Reward on eval: 1354.2986204643378
[37m[1m[2023-06-25 02:31:09,856][129146] Mean Reward across all agents: 1354.2986204643378
[37m[1m[2023-06-25 02:31:09,856][129146] Average Trajectory Length: 961.068
[36m[2023-06-25 02:31:15,404][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:31:15,405][129146] Reward + Measures: [[ 631.94713846    0.32989678    0.29492831    0.18369927    0.22304343]
[37m[1m [ 969.63371935    0.25446153    0.23927949    0.11517949    0.21115127]
[37m[1m [ 789.8985633     0.29391807    0.223985      0.09102645    0.23228951]
[37m[1m ...
[37m[1m [1147.7596616     0.33809999    0.3159        0.14690001    0.2642    ]
[37m[1m [ 585.45305557    0.26810002    0.26920003    0.15179999    0.18740001]
[37m[1m [ 578.49859672    0.31440002    0.2881        0.16820002    0.22379999]]
[37m[1m[2023-06-25 02:31:15,405][129146] Max Reward on eval: 1568.522169438959
[37m[1m[2023-06-25 02:31:15,405][129146] Min Reward on eval: -100.97329064552468
[37m[1m[2023-06-25 02:31:15,405][129146] Mean Reward across all agents: 880.1571173792779
[37m[1m[2023-06-25 02:31:15,406][129146] Average Trajectory Length: 948.6116666666667
[36m[2023-06-25 02:31:15,408][129146] mean_value=-213.12487910095336, max_value=748.0810638109484
[37m[1m[2023-06-25 02:31:15,411][129146] New mean coefficients: [[ 1.801826   -1.0313991   0.22384363 -0.4089282  -0.96333754]]
[37m[1m[2023-06-25 02:31:15,412][129146] Moving the mean solution point...
[36m[2023-06-25 02:31:25,031][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 02:31:25,032][129146] FPS: 399282.87
[36m[2023-06-25 02:31:25,034][129146] itr=142, itrs=2000, Progress: 7.10%
[36m[2023-06-25 02:31:36,689][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 02:31:36,689][129146] FPS: 329891.49
[36m[2023-06-25 02:31:41,431][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:31:41,431][129146] Reward + Measures: [[1410.65851952    0.27241296    0.25793049    0.08475805    0.24518025]]
[37m[1m[2023-06-25 02:31:41,431][129146] Max Reward on eval: 1410.6585195211924
[37m[1m[2023-06-25 02:31:41,431][129146] Min Reward on eval: 1410.6585195211924
[37m[1m[2023-06-25 02:31:41,432][129146] Mean Reward across all agents: 1410.6585195211924
[37m[1m[2023-06-25 02:31:41,432][129146] Average Trajectory Length: 963.2016666666666
[36m[2023-06-25 02:31:46,892][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:31:46,892][129146] Reward + Measures: [[1151.11920281    0.26659998    0.27770001    0.10399999    0.25840002]
[37m[1m [ -13.38045604    0.30209431    0.23173836    0.10220414    0.14097582]
[37m[1m [  33.04290741    0.34954703    0.22640489    0.14360693    0.16194779]
[37m[1m ...
[37m[1m [ 211.33175692    0.29730004    0.16630001    0.12          0.1767    ]
[37m[1m [ 468.06133104    0.26040003    0.16779999    0.0997        0.1319    ]
[37m[1m [1116.94582098    0.23048396    0.20456639    0.09867994    0.22976446]]
[37m[1m[2023-06-25 02:31:46,892][129146] Max Reward on eval: 1564.2832246455248
[37m[1m[2023-06-25 02:31:46,893][129146] Min Reward on eval: -217.24021731028915
[37m[1m[2023-06-25 02:31:46,893][129146] Mean Reward across all agents: 680.1861479924022
[37m[1m[2023-06-25 02:31:46,893][129146] Average Trajectory Length: 945.0523333333333
[36m[2023-06-25 02:31:46,897][129146] mean_value=-127.121702744187, max_value=763.8022627010914
[37m[1m[2023-06-25 02:31:46,899][129146] New mean coefficients: [[ 1.3623658  -0.96692306 -0.24658594 -0.35504004 -1.4347229 ]]
[37m[1m[2023-06-25 02:31:46,901][129146] Moving the mean solution point...
[36m[2023-06-25 02:31:56,670][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 02:31:56,670][129146] FPS: 393164.70
[36m[2023-06-25 02:31:56,672][129146] itr=143, itrs=2000, Progress: 7.15%
[36m[2023-06-25 02:32:08,356][129146] train() took 11.67 seconds to complete
[36m[2023-06-25 02:32:08,356][129146] FPS: 329076.69
[36m[2023-06-25 02:32:13,164][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:32:13,164][129146] Reward + Measures: [[1439.02544018    0.26801828    0.25213256    0.08097016    0.24073978]]
[37m[1m[2023-06-25 02:32:13,165][129146] Max Reward on eval: 1439.02544017623
[37m[1m[2023-06-25 02:32:13,165][129146] Min Reward on eval: 1439.02544017623
[37m[1m[2023-06-25 02:32:13,165][129146] Mean Reward across all agents: 1439.02544017623
[37m[1m[2023-06-25 02:32:13,165][129146] Average Trajectory Length: 952.4146666666667
[36m[2023-06-25 02:32:18,710][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:32:18,711][129146] Reward + Measures: [[1116.87495462    0.26813743    0.2578164     0.10368901    0.24282932]
[37m[1m [1228.28492162    0.23389998    0.23930001    0.0936        0.2264    ]
[37m[1m [1325.45704786    0.2814        0.2818        0.0974        0.26659998]
[37m[1m ...
[37m[1m [  96.39823182    0.22010083    0.24022995    0.17052415    0.22095664]
[37m[1m [1279.20362565    0.21299998    0.22250001    0.10340001    0.21710001]
[37m[1m [ 452.27015851    0.25514841    0.26067969    0.11879405    0.21813062]]
[37m[1m[2023-06-25 02:32:18,711][129146] Max Reward on eval: 1514.6608301923843
[37m[1m[2023-06-25 02:32:18,711][129146] Min Reward on eval: -605.5123184255324
[37m[1m[2023-06-25 02:32:18,712][129146] Mean Reward across all agents: 842.7874460286434
[37m[1m[2023-06-25 02:32:18,712][129146] Average Trajectory Length: 941.8103333333333
[36m[2023-06-25 02:32:18,715][129146] mean_value=-237.10459937024424, max_value=919.7616811635912
[37m[1m[2023-06-25 02:32:18,718][129146] New mean coefficients: [[ 1.2959335  -0.59712595  0.10952893  0.56444824 -1.2153305 ]]
[37m[1m[2023-06-25 02:32:18,719][129146] Moving the mean solution point...
[36m[2023-06-25 02:32:28,462][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 02:32:28,462][129146] FPS: 394205.37
[36m[2023-06-25 02:32:28,464][129146] itr=144, itrs=2000, Progress: 7.20%
[36m[2023-06-25 02:32:40,030][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 02:32:40,030][129146] FPS: 332394.95
[36m[2023-06-25 02:32:44,780][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:32:44,780][129146] Reward + Measures: [[1491.87338608    0.26576576    0.25359052    0.07814992    0.23885205]]
[37m[1m[2023-06-25 02:32:44,780][129146] Max Reward on eval: 1491.87338607799
[37m[1m[2023-06-25 02:32:44,781][129146] Min Reward on eval: 1491.87338607799
[37m[1m[2023-06-25 02:32:44,781][129146] Mean Reward across all agents: 1491.87338607799
[37m[1m[2023-06-25 02:32:44,781][129146] Average Trajectory Length: 955.828
[36m[2023-06-25 02:32:50,164][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:32:50,165][129146] Reward + Measures: [[ 370.44968274    0.56669998    0.22750001    0.42270002    0.3707    ]
[37m[1m [ 754.74788997    0.24599349    0.29715267    0.10277851    0.26001617]
[37m[1m [ 520.02694524    0.25939998    0.29260001    0.1236        0.23169999]
[37m[1m ...
[37m[1m [ 511.42080196    0.23968868    0.23318616    0.1277654     0.23679686]
[37m[1m [ 715.24467262    0.21273048    0.17760515    0.10115451    0.1989219 ]
[37m[1m [1330.36973912    0.2735942     0.23949905    0.06122518    0.2514821 ]]
[37m[1m[2023-06-25 02:32:50,165][129146] Max Reward on eval: 1691.2992779659573
[37m[1m[2023-06-25 02:32:50,165][129146] Min Reward on eval: -128.60434833943145
[37m[1m[2023-06-25 02:32:50,166][129146] Mean Reward across all agents: 703.4368376714916
[37m[1m[2023-06-25 02:32:50,166][129146] Average Trajectory Length: 928.9846666666666
[36m[2023-06-25 02:32:50,168][129146] mean_value=-528.2618240566503, max_value=549.9777940358482
[37m[1m[2023-06-25 02:32:50,171][129146] New mean coefficients: [[ 0.97489107 -0.23795143  0.26447177  0.7813028  -0.2775277 ]]
[37m[1m[2023-06-25 02:32:50,172][129146] Moving the mean solution point...
[36m[2023-06-25 02:32:59,835][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 02:32:59,835][129146] FPS: 397455.18
[36m[2023-06-25 02:32:59,838][129146] itr=145, itrs=2000, Progress: 7.25%
[36m[2023-06-25 02:33:11,393][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 02:33:11,394][129146] FPS: 332686.57
[36m[2023-06-25 02:33:16,079][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:33:16,079][129146] Reward + Measures: [[1541.58799478    0.26726282    0.25530127    0.07442944    0.24135859]]
[37m[1m[2023-06-25 02:33:16,079][129146] Max Reward on eval: 1541.587994781962
[37m[1m[2023-06-25 02:33:16,080][129146] Min Reward on eval: 1541.587994781962
[37m[1m[2023-06-25 02:33:16,080][129146] Mean Reward across all agents: 1541.587994781962
[37m[1m[2023-06-25 02:33:16,080][129146] Average Trajectory Length: 957.9466666666666
[36m[2023-06-25 02:33:21,408][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:33:21,409][129146] Reward + Measures: [[1054.18697272    0.23671816    0.21699511    0.09812918    0.20992465]
[37m[1m [1380.69421108    0.29771227    0.24550457    0.07477152    0.2448328 ]
[37m[1m [ 239.26196058    0.27811584    0.24646024    0.13460155    0.22037111]
[37m[1m ...
[37m[1m [1231.33833352    0.27867982    0.23029172    0.0996101     0.24322294]
[37m[1m [1160.83017768    0.23569873    0.22377649    0.09150117    0.20281194]
[37m[1m [1043.53614005    0.26751545    0.25744498    0.08379758    0.24800296]]
[37m[1m[2023-06-25 02:33:21,409][129146] Max Reward on eval: 1739.5416698335437
[37m[1m[2023-06-25 02:33:21,409][129146] Min Reward on eval: 64.60331948726089
[37m[1m[2023-06-25 02:33:21,410][129146] Mean Reward across all agents: 1065.5654449652322
[37m[1m[2023-06-25 02:33:21,410][129146] Average Trajectory Length: 941.9793333333333
[36m[2023-06-25 02:33:21,412][129146] mean_value=-278.01231824386247, max_value=563.9576155317227
[37m[1m[2023-06-25 02:33:21,415][129146] New mean coefficients: [[ 0.8838392  -0.2800091   0.11768743  0.540148    0.05536002]]
[37m[1m[2023-06-25 02:33:21,416][129146] Moving the mean solution point...
[36m[2023-06-25 02:33:31,078][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 02:33:31,078][129146] FPS: 397495.72
[36m[2023-06-25 02:33:31,080][129146] itr=146, itrs=2000, Progress: 7.30%
[36m[2023-06-25 02:33:42,502][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 02:33:42,502][129146] FPS: 336616.08
[36m[2023-06-25 02:33:47,333][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:33:47,333][129146] Reward + Measures: [[1596.77460872    0.26564535    0.25423872    0.07135428    0.24065746]]
[37m[1m[2023-06-25 02:33:47,334][129146] Max Reward on eval: 1596.774608722979
[37m[1m[2023-06-25 02:33:47,334][129146] Min Reward on eval: 1596.774608722979
[37m[1m[2023-06-25 02:33:47,334][129146] Mean Reward across all agents: 1596.774608722979
[37m[1m[2023-06-25 02:33:47,334][129146] Average Trajectory Length: 953.2626666666666
[36m[2023-06-25 02:33:52,841][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:33:52,841][129146] Reward + Measures: [[1290.06873158    0.24114478    0.24804941    0.09645432    0.25701863]
[37m[1m [ 151.05262737    0.82450008    0.079         0.78780001    0.6983    ]
[37m[1m [ 522.38104706    0.2775        0.2026        0.22650002    0.2299    ]
[37m[1m ...
[37m[1m [-431.71434117    0.62323183    0.14123489    0.59935272    0.52220696]
[37m[1m [ 342.04071218    0.2687        0.30180001    0.1321        0.1982    ]
[37m[1m [ 913.6357228     0.29930001    0.29480001    0.1122        0.25      ]]
[37m[1m[2023-06-25 02:33:52,842][129146] Max Reward on eval: 1784.891000314278
[37m[1m[2023-06-25 02:33:52,842][129146] Min Reward on eval: -903.9705123952474
[37m[1m[2023-06-25 02:33:52,842][129146] Mean Reward across all agents: 510.15625678565937
[37m[1m[2023-06-25 02:33:52,842][129146] Average Trajectory Length: 950.3863333333333
[36m[2023-06-25 02:33:52,847][129146] mean_value=-256.4453674817287, max_value=1268.8289720622738
[37m[1m[2023-06-25 02:33:52,850][129146] New mean coefficients: [[ 0.8463023  -0.29408693 -0.01404397  0.5402567  -0.24594924]]
[37m[1m[2023-06-25 02:33:52,851][129146] Moving the mean solution point...
[36m[2023-06-25 02:34:02,502][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 02:34:02,502][129146] FPS: 397972.17
[36m[2023-06-25 02:34:02,504][129146] itr=147, itrs=2000, Progress: 7.35%
[36m[2023-06-25 02:34:13,920][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 02:34:13,920][129146] FPS: 336772.60
[36m[2023-06-25 02:34:18,678][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:34:18,678][129146] Reward + Measures: [[1636.20660108    0.26364985    0.25476143    0.06951405    0.24170002]]
[37m[1m[2023-06-25 02:34:18,679][129146] Max Reward on eval: 1636.206601081386
[37m[1m[2023-06-25 02:34:18,679][129146] Min Reward on eval: 1636.206601081386
[37m[1m[2023-06-25 02:34:18,679][129146] Mean Reward across all agents: 1636.206601081386
[37m[1m[2023-06-25 02:34:18,679][129146] Average Trajectory Length: 950.3323333333333
[36m[2023-06-25 02:34:24,178][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:34:24,178][129146] Reward + Measures: [[ 520.30165132    0.38371345    0.32959244    0.21606807    0.2447311 ]
[37m[1m [1045.69884206    0.33460003    0.26910004    0.16429999    0.25299999]
[37m[1m [1125.8025045     0.24232872    0.24408202    0.086735      0.23522691]
[37m[1m ...
[37m[1m [ 267.75609735    0.27440003    0.1698        0.14579999    0.18069999]
[37m[1m [1359.07976022    0.2349        0.1901        0.0659        0.23900001]
[37m[1m [1250.25622333    0.29770002    0.29679999    0.1124        0.2502    ]]
[37m[1m[2023-06-25 02:34:24,179][129146] Max Reward on eval: 1843.9875586423325
[37m[1m[2023-06-25 02:34:24,179][129146] Min Reward on eval: -263.97816818712164
[37m[1m[2023-06-25 02:34:24,179][129146] Mean Reward across all agents: 1023.8023765971802
[37m[1m[2023-06-25 02:34:24,179][129146] Average Trajectory Length: 964.6676666666666
[36m[2023-06-25 02:34:24,182][129146] mean_value=-259.79583088742066, max_value=1062.6664938142435
[37m[1m[2023-06-25 02:34:24,185][129146] New mean coefficients: [[ 0.7887415  -0.30944237 -0.0955909   0.10614848 -0.39626396]]
[37m[1m[2023-06-25 02:34:24,186][129146] Moving the mean solution point...
[36m[2023-06-25 02:34:33,831][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 02:34:33,832][129146] FPS: 398166.80
[36m[2023-06-25 02:34:33,834][129146] itr=148, itrs=2000, Progress: 7.40%
[36m[2023-06-25 02:34:45,310][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 02:34:45,310][129146] FPS: 335057.94
[36m[2023-06-25 02:34:50,150][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:34:50,150][129146] Reward + Measures: [[1663.46237102    0.25487298    0.24815056    0.06903282    0.23954785]]
[37m[1m[2023-06-25 02:34:50,150][129146] Max Reward on eval: 1663.462371020178
[37m[1m[2023-06-25 02:34:50,151][129146] Min Reward on eval: 1663.462371020178
[37m[1m[2023-06-25 02:34:50,151][129146] Mean Reward across all agents: 1663.462371020178
[37m[1m[2023-06-25 02:34:50,151][129146] Average Trajectory Length: 950.567
[36m[2023-06-25 02:34:55,740][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:34:55,741][129146] Reward + Measures: [[1316.96336715    0.2929        0.29249999    0.09060001    0.27580002]
[37m[1m [1786.34456932    0.26512465    0.25985429    0.08971301    0.26144227]
[37m[1m [1223.74289581    0.26551446    0.27441442    0.08176021    0.25481194]
[37m[1m ...
[37m[1m [ 891.080134      0.221         0.23310001    0.09729999    0.2251    ]
[37m[1m [ 583.0725001     0.34130001    0.34540001    0.16410001    0.28620002]
[37m[1m [1243.93104119    0.26486155    0.21411002    0.11908114    0.21366428]]
[37m[1m[2023-06-25 02:34:55,741][129146] Max Reward on eval: 1842.8974767938023
[37m[1m[2023-06-25 02:34:55,741][129146] Min Reward on eval: -230.13854647062254
[37m[1m[2023-06-25 02:34:55,741][129146] Mean Reward across all agents: 949.2476272145191
[37m[1m[2023-06-25 02:34:55,742][129146] Average Trajectory Length: 905.4586666666667
[36m[2023-06-25 02:34:55,745][129146] mean_value=-188.5723587251258, max_value=1051.4463082661853
[37m[1m[2023-06-25 02:34:55,748][129146] New mean coefficients: [[ 0.3719256  -0.08848658  0.22008736  0.2764208  -0.23289141]]
[37m[1m[2023-06-25 02:34:55,749][129146] Moving the mean solution point...
[36m[2023-06-25 02:35:05,508][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 02:35:05,508][129146] FPS: 393544.70
[36m[2023-06-25 02:35:05,511][129146] itr=149, itrs=2000, Progress: 7.45%
[36m[2023-06-25 02:35:16,966][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 02:35:16,966][129146] FPS: 335653.41
[36m[2023-06-25 02:35:21,860][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:35:21,861][129146] Reward + Measures: [[1697.21269636    0.25625819    0.25003606    0.06711329    0.23835896]]
[37m[1m[2023-06-25 02:35:21,861][129146] Max Reward on eval: 1697.212696355639
[37m[1m[2023-06-25 02:35:21,861][129146] Min Reward on eval: 1697.212696355639
[37m[1m[2023-06-25 02:35:21,861][129146] Mean Reward across all agents: 1697.212696355639
[37m[1m[2023-06-25 02:35:21,862][129146] Average Trajectory Length: 946.597
[36m[2023-06-25 02:35:27,353][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:35:27,354][129146] Reward + Measures: [[1462.58804782    0.27101278    0.28227487    0.08019163    0.2400075 ]
[37m[1m [1565.6197358     0.34190002    0.26199999    0.14570001    0.27959999]
[37m[1m [ 993.99516917    0.34570003    0.3021        0.17850001    0.2942    ]
[37m[1m ...
[37m[1m [ 954.00350982    0.33790001    0.2481        0.18810001    0.23720001]
[37m[1m [1055.42628867    0.23615122    0.25604147    0.08041578    0.23320737]
[37m[1m [1209.31787438    0.35709998    0.26499999    0.1851        0.24980001]]
[37m[1m[2023-06-25 02:35:27,354][129146] Max Reward on eval: 1959.021154253022
[37m[1m[2023-06-25 02:35:27,354][129146] Min Reward on eval: 465.62160249410664
[37m[1m[2023-06-25 02:35:27,355][129146] Mean Reward across all agents: 1280.740795828022
[37m[1m[2023-06-25 02:35:27,355][129146] Average Trajectory Length: 935.2016666666666
[36m[2023-06-25 02:35:27,358][129146] mean_value=-169.00875593203386, max_value=673.3675441108985
[37m[1m[2023-06-25 02:35:27,361][129146] New mean coefficients: [[ 0.60676885  0.31809464  0.06372587  0.62115806 -0.71429646]]
[37m[1m[2023-06-25 02:35:27,362][129146] Moving the mean solution point...
[36m[2023-06-25 02:35:37,061][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 02:35:37,061][129146] FPS: 395989.52
[36m[2023-06-25 02:35:37,064][129146] itr=150, itrs=2000, Progress: 7.50%
[37m[1m[2023-06-25 02:35:39,423][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000130
[36m[2023-06-25 02:35:51,218][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 02:35:51,219][129146] FPS: 335130.21
[36m[2023-06-25 02:35:55,906][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:35:55,906][129146] Reward + Measures: [[1765.3596554     0.25657099    0.25045452    0.06861737    0.24049465]]
[37m[1m[2023-06-25 02:35:55,907][129146] Max Reward on eval: 1765.359655398681
[37m[1m[2023-06-25 02:35:55,907][129146] Min Reward on eval: 1765.359655398681
[37m[1m[2023-06-25 02:35:55,907][129146] Mean Reward across all agents: 1765.359655398681
[37m[1m[2023-06-25 02:35:55,907][129146] Average Trajectory Length: 947.482
[36m[2023-06-25 02:36:01,502][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:36:01,502][129146] Reward + Measures: [[1392.53034216    0.2793        0.24349999    0.12660001    0.28920001]
[37m[1m [1701.78038321    0.27770001    0.24310003    0.0948        0.25180003]
[37m[1m [1475.59779813    0.26592967    0.24180244    0.06721082    0.23787013]
[37m[1m ...
[37m[1m [1234.02684853    0.24580307    0.21239983    0.06238049    0.18191241]
[37m[1m [1686.17757321    0.27169999    0.2703        0.07739999    0.2687    ]
[37m[1m [1440.45617077    0.28011438    0.27378231    0.11269815    0.2837753 ]]
[37m[1m[2023-06-25 02:36:01,598][129146] Max Reward on eval: 1937.1302548453446
[37m[1m[2023-06-25 02:36:01,598][129146] Min Reward on eval: -236.8167298122251
[37m[1m[2023-06-25 02:36:01,599][129146] Mean Reward across all agents: 1181.3879001275939
[37m[1m[2023-06-25 02:36:01,599][129146] Average Trajectory Length: 950.799
[36m[2023-06-25 02:36:01,603][129146] mean_value=-34.88476935493241, max_value=1369.9606321001197
[37m[1m[2023-06-25 02:36:01,606][129146] New mean coefficients: [[ 0.55742615  0.64266443  0.01290345  1.5444927  -0.52787113]]
[37m[1m[2023-06-25 02:36:01,607][129146] Moving the mean solution point...
[36m[2023-06-25 02:36:11,280][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 02:36:11,280][129146] FPS: 397078.22
[36m[2023-06-25 02:36:11,282][129146] itr=151, itrs=2000, Progress: 7.55%
[36m[2023-06-25 02:36:22,730][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 02:36:22,730][129146] FPS: 335836.18
[36m[2023-06-25 02:36:27,539][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:36:27,539][129146] Reward + Measures: [[1798.60308626    0.25744647    0.24973404    0.06904326    0.23872828]]
[37m[1m[2023-06-25 02:36:27,540][129146] Max Reward on eval: 1798.6030862578061
[37m[1m[2023-06-25 02:36:27,540][129146] Min Reward on eval: 1798.6030862578061
[37m[1m[2023-06-25 02:36:27,540][129146] Mean Reward across all agents: 1798.6030862578061
[37m[1m[2023-06-25 02:36:27,540][129146] Average Trajectory Length: 946.418
[36m[2023-06-25 02:36:33,047][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:36:33,048][129146] Reward + Measures: [[1896.16832322    0.28918067    0.27204195    0.06548387    0.25027096]
[37m[1m [1914.20239959    0.28889999    0.2746        0.0784        0.25920001]
[37m[1m [1751.15033292    0.27119318    0.27040511    0.07525396    0.25423825]
[37m[1m ...
[37m[1m [1720.79625777    0.23233639    0.22411366    0.05983618    0.2142061 ]
[37m[1m [1028.56870683    0.39910004    0.31420001    0.1552        0.27779999]
[37m[1m [1738.86194617    0.30827484    0.25616199    0.08197116    0.24229018]]
[37m[1m[2023-06-25 02:36:33,048][129146] Max Reward on eval: 2125.7088604199234
[37m[1m[2023-06-25 02:36:33,048][129146] Min Reward on eval: 1028.5687068330008
[37m[1m[2023-06-25 02:36:33,049][129146] Mean Reward across all agents: 1679.8703747942538
[37m[1m[2023-06-25 02:36:33,049][129146] Average Trajectory Length: 950.2826666666666
[36m[2023-06-25 02:36:33,052][129146] mean_value=-86.80805853598649, max_value=747.2850205265638
[37m[1m[2023-06-25 02:36:33,055][129146] New mean coefficients: [[ 0.831932    0.4549716   0.302136    1.2290283  -0.22597349]]
[37m[1m[2023-06-25 02:36:33,056][129146] Moving the mean solution point...
[36m[2023-06-25 02:36:42,878][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 02:36:42,878][129146] FPS: 391028.06
[36m[2023-06-25 02:36:42,880][129146] itr=152, itrs=2000, Progress: 7.60%
[36m[2023-06-25 02:36:54,347][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 02:36:54,347][129146] FPS: 335332.90
[36m[2023-06-25 02:36:59,103][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:36:59,103][129146] Reward + Measures: [[1898.94203547    0.2596451     0.25083789    0.07080263    0.23933716]]
[37m[1m[2023-06-25 02:36:59,104][129146] Max Reward on eval: 1898.9420354731276
[37m[1m[2023-06-25 02:36:59,104][129146] Min Reward on eval: 1898.9420354731276
[37m[1m[2023-06-25 02:36:59,104][129146] Mean Reward across all agents: 1898.9420354731276
[37m[1m[2023-06-25 02:36:59,105][129146] Average Trajectory Length: 961.8856666666667
[36m[2023-06-25 02:37:04,492][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:37:04,493][129146] Reward + Measures: [[ 227.18860368    0.35210001    0.31150001    0.22149999    0.28620002]
[37m[1m [ 774.83409741    0.229         0.23189998    0.12250002    0.24330001]
[37m[1m [1598.02433795    0.28260002    0.26719999    0.10700001    0.26280001]
[37m[1m ...
[37m[1m [1495.77410435    0.23145257    0.22576495    0.07783324    0.212235  ]
[37m[1m [ 397.15702516    0.2323        0.21040002    0.147         0.2362    ]
[37m[1m [ 631.48526517    0.19450001    0.2264        0.12639999    0.25240001]]
[37m[1m[2023-06-25 02:37:04,493][129146] Max Reward on eval: 2060.870939492527
[37m[1m[2023-06-25 02:37:04,493][129146] Min Reward on eval: -312.0306062129559
[37m[1m[2023-06-25 02:37:04,494][129146] Mean Reward across all agents: 935.4184391514103
[37m[1m[2023-06-25 02:37:04,494][129146] Average Trajectory Length: 957.3276666666667
[36m[2023-06-25 02:37:04,496][129146] mean_value=-646.6301728801487, max_value=406.8271529339586
[37m[1m[2023-06-25 02:37:04,498][129146] New mean coefficients: [[ 0.90238756  0.13272381  0.14263044  0.24209398 -0.08785929]]
[37m[1m[2023-06-25 02:37:04,499][129146] Moving the mean solution point...
[36m[2023-06-25 02:37:14,330][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 02:37:14,330][129146] FPS: 390674.31
[36m[2023-06-25 02:37:14,332][129146] itr=153, itrs=2000, Progress: 7.65%
[36m[2023-06-25 02:37:25,909][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 02:37:25,909][129146] FPS: 332101.13
[36m[2023-06-25 02:37:30,788][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:37:30,788][129146] Reward + Measures: [[1966.98054374    0.25583592    0.25069043    0.06611433    0.23932637]]
[37m[1m[2023-06-25 02:37:30,788][129146] Max Reward on eval: 1966.9805437427406
[37m[1m[2023-06-25 02:37:30,789][129146] Min Reward on eval: 1966.9805437427406
[37m[1m[2023-06-25 02:37:30,789][129146] Mean Reward across all agents: 1966.9805437427406
[37m[1m[2023-06-25 02:37:30,789][129146] Average Trajectory Length: 956.5223333333333
[36m[2023-06-25 02:37:36,279][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:37:36,280][129146] Reward + Measures: [[1458.251143      0.30440003    0.25050002    0.138         0.25670001]
[37m[1m [1159.51727246    0.26440001    0.3066        0.10750001    0.2712    ]
[37m[1m [2049.5745808     0.26230001    0.27140003    0.06510001    0.245     ]
[37m[1m ...
[37m[1m [  45.8423718     0.21826671    0.19723333    0.18063334    0.1462    ]
[37m[1m [1569.12759099    0.28210002    0.23199999    0.1133        0.23270002]
[37m[1m [ 793.32123365    0.24380003    0.28459999    0.12319999    0.24700001]]
[37m[1m[2023-06-25 02:37:36,285][129146] Max Reward on eval: 2196.89767047402
[37m[1m[2023-06-25 02:37:36,285][129146] Min Reward on eval: -340.0028183664661
[37m[1m[2023-06-25 02:37:36,286][129146] Mean Reward across all agents: 1359.9048285578133
[37m[1m[2023-06-25 02:37:36,286][129146] Average Trajectory Length: 963.629
[36m[2023-06-25 02:37:36,289][129146] mean_value=-234.90097736864962, max_value=681.2595005232414
[37m[1m[2023-06-25 02:37:36,291][129146] New mean coefficients: [[1.197504   0.40514085 0.6004877  0.6869778  0.22519611]]
[37m[1m[2023-06-25 02:37:36,293][129146] Moving the mean solution point...
[36m[2023-06-25 02:37:45,971][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 02:37:45,971][129146] FPS: 396840.47
[36m[2023-06-25 02:37:45,974][129146] itr=154, itrs=2000, Progress: 7.70%
[36m[2023-06-25 02:37:57,425][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 02:37:57,426][129146] FPS: 335754.70
[36m[2023-06-25 02:38:02,195][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:38:02,195][129146] Reward + Measures: [[2005.83375719    0.25635511    0.25067249    0.06321332    0.23974389]]
[37m[1m[2023-06-25 02:38:02,195][129146] Max Reward on eval: 2005.8337571856812
[37m[1m[2023-06-25 02:38:02,196][129146] Min Reward on eval: 2005.8337571856812
[37m[1m[2023-06-25 02:38:02,196][129146] Mean Reward across all agents: 2005.8337571856812
[37m[1m[2023-06-25 02:38:02,196][129146] Average Trajectory Length: 942.2023333333333
[36m[2023-06-25 02:38:07,698][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:38:07,698][129146] Reward + Measures: [[1766.00629996    0.27379999    0.30379999    0.1248        0.28530002]
[37m[1m [1443.83143093    0.24671049    0.216711      0.05259264    0.21750875]
[37m[1m [1198.22116233    0.23726058    0.20970169    0.06379513    0.19705646]
[37m[1m ...
[37m[1m [1601.63019534    0.2877        0.31209999    0.1196        0.27000001]
[37m[1m [1324.990503      0.32499999    0.30990002    0.15280001    0.26550001]
[37m[1m [ 960.0216442     0.36390004    0.31830001    0.2053        0.2744    ]]
[37m[1m[2023-06-25 02:38:07,699][129146] Max Reward on eval: 2328.620939033106
[37m[1m[2023-06-25 02:38:07,699][129146] Min Reward on eval: 126.17856894269353
[37m[1m[2023-06-25 02:38:07,699][129146] Mean Reward across all agents: 1602.4623721132962
[37m[1m[2023-06-25 02:38:07,699][129146] Average Trajectory Length: 959.443
[36m[2023-06-25 02:38:07,703][129146] mean_value=-138.809877230639, max_value=1436.9431854972845
[37m[1m[2023-06-25 02:38:07,706][129146] New mean coefficients: [[0.6791077  0.47376117 0.416027   0.88294685 0.07859087]]
[37m[1m[2023-06-25 02:38:07,707][129146] Moving the mean solution point...
[36m[2023-06-25 02:38:17,398][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 02:38:17,398][129146] FPS: 396313.92
[36m[2023-06-25 02:38:17,401][129146] itr=155, itrs=2000, Progress: 7.75%
[36m[2023-06-25 02:38:28,844][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 02:38:28,845][129146] FPS: 335990.94
[36m[2023-06-25 02:38:33,624][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:38:33,624][129146] Reward + Measures: [[2083.68811705    0.25436735    0.24919869    0.06534       0.24051416]]
[37m[1m[2023-06-25 02:38:33,624][129146] Max Reward on eval: 2083.6881170481633
[37m[1m[2023-06-25 02:38:33,624][129146] Min Reward on eval: 2083.6881170481633
[37m[1m[2023-06-25 02:38:33,625][129146] Mean Reward across all agents: 2083.6881170481633
[37m[1m[2023-06-25 02:38:33,625][129146] Average Trajectory Length: 955.6426666666666
[36m[2023-06-25 02:38:39,103][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:38:39,103][129146] Reward + Measures: [[1611.7647195     0.28183576    0.2344429     0.08238204    0.22595011]
[37m[1m [2134.99442961    0.26891619    0.2910637     0.07028127    0.26756033]
[37m[1m [2178.72135517    0.26970002    0.25559998    0.06879999    0.25      ]
[37m[1m ...
[37m[1m [1897.81504981    0.26482567    0.25876153    0.06889231    0.22778462]
[37m[1m [1973.840827      0.25342175    0.24843912    0.07960869    0.24938695]
[37m[1m [1755.84340026    0.28493643    0.24550362    0.06305643    0.256201  ]]
[37m[1m[2023-06-25 02:38:39,104][129146] Max Reward on eval: 2364.487344305974
[37m[1m[2023-06-25 02:38:39,104][129146] Min Reward on eval: 1096.4073143085814
[37m[1m[2023-06-25 02:38:39,104][129146] Mean Reward across all agents: 1956.9632342189054
[37m[1m[2023-06-25 02:38:39,104][129146] Average Trajectory Length: 953.6813333333333
[36m[2023-06-25 02:38:39,108][129146] mean_value=-81.51885892838486, max_value=536.5392104438095
[37m[1m[2023-06-25 02:38:39,110][129146] New mean coefficients: [[1.1220722  0.1808109  1.0348678  0.73601305 0.07150078]]
[37m[1m[2023-06-25 02:38:39,111][129146] Moving the mean solution point...
[36m[2023-06-25 02:38:48,754][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 02:38:48,754][129146] FPS: 398298.97
[36m[2023-06-25 02:38:48,757][129146] itr=156, itrs=2000, Progress: 7.80%
[36m[2023-06-25 02:39:00,265][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 02:39:00,265][129146] FPS: 334110.44
[36m[2023-06-25 02:39:04,995][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:39:04,996][129146] Reward + Measures: [[2181.20449129    0.25616708    0.25401467    0.06822769    0.24708626]]
[37m[1m[2023-06-25 02:39:04,996][129146] Max Reward on eval: 2181.204491290902
[37m[1m[2023-06-25 02:39:04,996][129146] Min Reward on eval: 2181.204491290902
[37m[1m[2023-06-25 02:39:04,996][129146] Mean Reward across all agents: 2181.204491290902
[37m[1m[2023-06-25 02:39:04,997][129146] Average Trajectory Length: 963.304
[36m[2023-06-25 02:39:10,431][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:39:10,431][129146] Reward + Measures: [[1572.95947554    0.37309998    0.31900001    0.15800002    0.28770003]
[37m[1m [1695.57442043    0.3743        0.30580002    0.1411        0.27950001]
[37m[1m [1944.33842011    0.2843        0.25850001    0.08419999    0.2649    ]
[37m[1m ...
[37m[1m [1348.8960449     0.36660001    0.29159999    0.156         0.25510001]
[37m[1m [1257.71271644    0.42500001    0.32910001    0.16910002    0.27200001]
[37m[1m [1970.83235657    0.30276424    0.26289511    0.07207222    0.25976363]]
[37m[1m[2023-06-25 02:39:10,431][129146] Max Reward on eval: 2365.53848929432
[37m[1m[2023-06-25 02:39:10,432][129146] Min Reward on eval: 332.74134073321477
[37m[1m[2023-06-25 02:39:10,432][129146] Mean Reward across all agents: 1539.4428080343994
[37m[1m[2023-06-25 02:39:10,432][129146] Average Trajectory Length: 973.3143333333333
[36m[2023-06-25 02:39:10,435][129146] mean_value=-165.73836721255063, max_value=509.0809876506762
[37m[1m[2023-06-25 02:39:10,438][129146] New mean coefficients: [[0.99318814 0.06923073 1.0318942  0.74276453 0.18129572]]
[37m[1m[2023-06-25 02:39:10,439][129146] Moving the mean solution point...
[36m[2023-06-25 02:39:20,154][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 02:39:20,154][129146] FPS: 395325.71
[36m[2023-06-25 02:39:20,156][129146] itr=157, itrs=2000, Progress: 7.85%
[36m[2023-06-25 02:39:31,612][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 02:39:31,612][129146] FPS: 335597.80
[36m[2023-06-25 02:39:36,334][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:39:36,334][129146] Reward + Measures: [[2173.79764092    0.2513341     0.24917927    0.06678813    0.24306788]]
[37m[1m[2023-06-25 02:39:36,334][129146] Max Reward on eval: 2173.797640921035
[37m[1m[2023-06-25 02:39:36,335][129146] Min Reward on eval: 2173.797640921035
[37m[1m[2023-06-25 02:39:36,335][129146] Mean Reward across all agents: 2173.797640921035
[37m[1m[2023-06-25 02:39:36,335][129146] Average Trajectory Length: 952.9246666666667
[36m[2023-06-25 02:39:41,805][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:39:41,806][129146] Reward + Measures: [[1311.88431715    0.29429999    0.26070002    0.1365        0.23819999]
[37m[1m [ 438.16096559    0.34960002    0.25729999    0.2543        0.27100003]
[37m[1m [ 959.07254144    0.22490001    0.2464        0.10236666    0.20420001]
[37m[1m ...
[37m[1m [1655.81843628    0.24553514    0.23632884    0.08880721    0.24723426]
[37m[1m [ 897.74727598    0.24549393    0.30998716    0.13965832    0.1917123 ]
[37m[1m [1868.6166269     0.23459999    0.21669999    0.0858        0.2264    ]]
[37m[1m[2023-06-25 02:39:41,806][129146] Max Reward on eval: 2413.714903171966
[37m[1m[2023-06-25 02:39:41,806][129146] Min Reward on eval: -88.41162719832792
[37m[1m[2023-06-25 02:39:41,806][129146] Mean Reward across all agents: 1150.8890784520008
[37m[1m[2023-06-25 02:39:41,806][129146] Average Trajectory Length: 961.28
[36m[2023-06-25 02:39:41,809][129146] mean_value=-535.5011737457929, max_value=1247.6908775811662
[37m[1m[2023-06-25 02:39:41,811][129146] New mean coefficients: [[ 1.0372329  -0.15760657  1.2247437   0.7498308   0.3272246 ]]
[37m[1m[2023-06-25 02:39:41,812][129146] Moving the mean solution point...
[36m[2023-06-25 02:39:51,456][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 02:39:51,457][129146] FPS: 398235.41
[36m[2023-06-25 02:39:51,459][129146] itr=158, itrs=2000, Progress: 7.90%
[36m[2023-06-25 02:40:02,992][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 02:40:02,992][129146] FPS: 333333.00
[36m[2023-06-25 02:40:07,744][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:40:07,745][129146] Reward + Measures: [[2276.65159092    0.24949779    0.25168779    0.06419816    0.24674617]]
[37m[1m[2023-06-25 02:40:07,745][129146] Max Reward on eval: 2276.6515909173772
[37m[1m[2023-06-25 02:40:07,745][129146] Min Reward on eval: 2276.6515909173772
[37m[1m[2023-06-25 02:40:07,745][129146] Mean Reward across all agents: 2276.6515909173772
[37m[1m[2023-06-25 02:40:07,745][129146] Average Trajectory Length: 964.5086666666666
[36m[2023-06-25 02:40:13,295][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:40:13,295][129146] Reward + Measures: [[1124.25791932    0.24773513    0.24780369    0.10866766    0.24310684]
[37m[1m [ 994.94663113    0.20900002    0.21430002    0.08670001    0.18890001]
[37m[1m [ 719.27423208    0.1970398     0.18216237    0.14263441    0.21506989]
[37m[1m ...
[37m[1m [ 647.52564656    0.23832993    0.21613947    0.10731836    0.20595852]
[37m[1m [ 909.27041967    0.31511509    0.24911979    0.1155652     0.22335656]
[37m[1m [1055.15876795    0.32679191    0.25884417    0.12505119    0.21620086]]
[37m[1m[2023-06-25 02:40:13,296][129146] Max Reward on eval: 2479.9269918570994
[37m[1m[2023-06-25 02:40:13,296][129146] Min Reward on eval: -174.14824865010306
[37m[1m[2023-06-25 02:40:13,296][129146] Mean Reward across all agents: 1247.442809903863
[37m[1m[2023-06-25 02:40:13,296][129146] Average Trajectory Length: 935.7113333333333
[36m[2023-06-25 02:40:13,299][129146] mean_value=-584.7383363582309, max_value=1561.1920889021271
[37m[1m[2023-06-25 02:40:13,302][129146] New mean coefficients: [[ 0.8779965  -0.10743862  0.69501925  0.7040195  -0.33099252]]
[37m[1m[2023-06-25 02:40:13,303][129146] Moving the mean solution point...
[36m[2023-06-25 02:40:23,066][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 02:40:23,067][129146] FPS: 393358.03
[36m[2023-06-25 02:40:23,069][129146] itr=159, itrs=2000, Progress: 7.95%
[36m[2023-06-25 02:40:34,607][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 02:40:34,608][129146] FPS: 333184.44
[36m[2023-06-25 02:40:39,341][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:40:39,342][129146] Reward + Measures: [[2368.51523549    0.24835241    0.25514355    0.06140316    0.24942167]]
[37m[1m[2023-06-25 02:40:39,342][129146] Max Reward on eval: 2368.5152354927727
[37m[1m[2023-06-25 02:40:39,342][129146] Min Reward on eval: 2368.5152354927727
[37m[1m[2023-06-25 02:40:39,342][129146] Mean Reward across all agents: 2368.5152354927727
[37m[1m[2023-06-25 02:40:39,343][129146] Average Trajectory Length: 967.6363333333333
[36m[2023-06-25 02:40:44,787][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:40:44,788][129146] Reward + Measures: [[-34.46271239   0.34050566   0.23110239   0.21699503   0.25289553]
[37m[1m [405.91349156   0.24721019   0.19831555   0.13543628   0.20477287]
[37m[1m [561.63090549   0.35780001   0.25120002   0.22049999   0.26910001]
[37m[1m ...
[37m[1m [808.67215884   0.2674       0.20320001   0.15460001   0.20910001]
[37m[1m [716.65140875   0.2859       0.21529999   0.1556       0.23280001]
[37m[1m [968.7795853    0.29100001   0.2112       0.1648       0.27070004]]
[37m[1m[2023-06-25 02:40:44,788][129146] Max Reward on eval: 2589.7021835503865
[37m[1m[2023-06-25 02:40:44,788][129146] Min Reward on eval: -248.15187687967847
[37m[1m[2023-06-25 02:40:44,788][129146] Mean Reward across all agents: 1235.043727245829
[37m[1m[2023-06-25 02:40:44,789][129146] Average Trajectory Length: 955.2093333333333
[36m[2023-06-25 02:40:44,791][129146] mean_value=-513.1207173810344, max_value=389.37418814524244
[37m[1m[2023-06-25 02:40:44,793][129146] New mean coefficients: [[ 0.73426163 -0.2053839   0.5837892   0.47408706 -0.3736457 ]]
[37m[1m[2023-06-25 02:40:44,794][129146] Moving the mean solution point...
[36m[2023-06-25 02:40:54,574][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 02:40:54,575][129146] FPS: 392716.96
[36m[2023-06-25 02:40:54,577][129146] itr=160, itrs=2000, Progress: 8.00%
[37m[1m[2023-06-25 02:40:56,960][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000140
[36m[2023-06-25 02:41:08,826][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 02:41:08,826][129146] FPS: 332979.01
[36m[2023-06-25 02:41:13,595][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:41:13,596][129146] Reward + Measures: [[2410.04666477    0.24689843    0.25344157    0.05767787    0.24595398]]
[37m[1m[2023-06-25 02:41:13,596][129146] Max Reward on eval: 2410.046664765028
[37m[1m[2023-06-25 02:41:13,596][129146] Min Reward on eval: 2410.046664765028
[37m[1m[2023-06-25 02:41:13,596][129146] Mean Reward across all agents: 2410.046664765028
[37m[1m[2023-06-25 02:41:13,597][129146] Average Trajectory Length: 962.088
[36m[2023-06-25 02:41:18,996][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:41:19,001][129146] Reward + Measures: [[1099.23504682    0.24009816    0.26148579    0.07006554    0.21565132]
[37m[1m [ 887.61264414    0.34634978    0.28639397    0.18473029    0.25139353]
[37m[1m [1044.67209875    0.3856        0.29370001    0.27669999    0.3233    ]
[37m[1m ...
[37m[1m [1784.78190236    0.27121326    0.26638171    0.10193601    0.25083038]
[37m[1m [1722.10554965    0.3407        0.3105        0.1516        0.2771    ]
[37m[1m [2118.76292783    0.2886        0.3249        0.1135        0.29249999]]
[37m[1m[2023-06-25 02:41:19,002][129146] Max Reward on eval: 2509.579502999247
[37m[1m[2023-06-25 02:41:19,002][129146] Min Reward on eval: 295.09578664105794
[37m[1m[2023-06-25 02:41:19,002][129146] Mean Reward across all agents: 1723.6329898410206
[37m[1m[2023-06-25 02:41:19,002][129146] Average Trajectory Length: 961.2803333333333
[36m[2023-06-25 02:41:19,006][129146] mean_value=-177.31351624272403, max_value=1202.9067040832715
[37m[1m[2023-06-25 02:41:19,009][129146] New mean coefficients: [[ 0.7637186   0.04572257  0.6228708   0.9632087  -0.789272  ]]
[37m[1m[2023-06-25 02:41:19,010][129146] Moving the mean solution point...
[36m[2023-06-25 02:41:28,636][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 02:41:28,637][129146] FPS: 398994.95
[36m[2023-06-25 02:41:28,639][129146] itr=161, itrs=2000, Progress: 8.05%
[36m[2023-06-25 02:41:40,054][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 02:41:40,054][129146] FPS: 336791.68
[36m[2023-06-25 02:41:44,677][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:41:44,677][129146] Reward + Measures: [[2445.14842942    0.24580322    0.25440758    0.05672994    0.24451226]]
[37m[1m[2023-06-25 02:41:44,678][129146] Max Reward on eval: 2445.148429420999
[37m[1m[2023-06-25 02:41:44,678][129146] Min Reward on eval: 2445.148429420999
[37m[1m[2023-06-25 02:41:44,678][129146] Mean Reward across all agents: 2445.148429420999
[37m[1m[2023-06-25 02:41:44,678][129146] Average Trajectory Length: 962.8263333333333
[36m[2023-06-25 02:41:50,226][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:41:50,227][129146] Reward + Measures: [[1727.09472254    0.23510002    0.22760001    0.08640001    0.22789998]
[37m[1m [1686.27646477    0.24633108    0.22260323    0.08026024    0.24296799]
[37m[1m [1038.29178354    0.38240001    0.27410001    0.1804        0.25250003]
[37m[1m ...
[37m[1m [2364.05408298    0.25979999    0.2737        0.0674        0.25619999]
[37m[1m [1540.54548364    0.26719999    0.25659999    0.1171        0.2402    ]
[37m[1m [1837.93703257    0.24101429    0.23081426    0.08702858    0.24175715]]
[37m[1m[2023-06-25 02:41:50,227][129146] Max Reward on eval: 2666.2194638574497
[37m[1m[2023-06-25 02:41:50,228][129146] Min Reward on eval: 552.8333050381596
[37m[1m[2023-06-25 02:41:50,228][129146] Mean Reward across all agents: 1809.1822082746262
[37m[1m[2023-06-25 02:41:50,228][129146] Average Trajectory Length: 957.2156666666666
[36m[2023-06-25 02:41:50,230][129146] mean_value=-429.48243768905206, max_value=979.5690183102924
[37m[1m[2023-06-25 02:41:50,233][129146] New mean coefficients: [[ 0.60727966 -0.16038881  0.6518418   0.40985256 -0.43340397]]
[37m[1m[2023-06-25 02:41:50,234][129146] Moving the mean solution point...
[36m[2023-06-25 02:41:59,972][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 02:41:59,973][129146] FPS: 394369.68
[36m[2023-06-25 02:41:59,975][129146] itr=162, itrs=2000, Progress: 8.10%
[36m[2023-06-25 02:42:11,582][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 02:42:11,583][129146] FPS: 331194.41
[36m[2023-06-25 02:42:16,407][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:42:16,408][129146] Reward + Measures: [[2516.89326741    0.24091177    0.25458106    0.05485024    0.24351586]]
[37m[1m[2023-06-25 02:42:16,408][129146] Max Reward on eval: 2516.8932674086386
[37m[1m[2023-06-25 02:42:16,408][129146] Min Reward on eval: 2516.8932674086386
[37m[1m[2023-06-25 02:42:16,409][129146] Mean Reward across all agents: 2516.8932674086386
[37m[1m[2023-06-25 02:42:16,409][129146] Average Trajectory Length: 963.2926666666666
[36m[2023-06-25 02:42:21,959][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:42:21,959][129146] Reward + Measures: [[1196.40812796    0.39379999    0.2626        0.24100001    0.27880001]
[37m[1m [2033.9195285     0.25533375    0.23433943    0.06058725    0.2396394 ]
[37m[1m [2617.74740862    0.26000002    0.26630002    0.0632        0.26440001]
[37m[1m ...
[37m[1m [1378.10432314    0.27213573    0.33480135    0.06896798    0.21603604]
[37m[1m [ 288.61351504    0.35315189    0.32793283    0.14565115    0.2527    ]
[37m[1m [1046.39146914    0.25520226    0.22362347    0.07119866    0.22774117]]
[37m[1m[2023-06-25 02:42:21,960][129146] Max Reward on eval: 2680.5437280947344
[37m[1m[2023-06-25 02:42:21,960][129146] Min Reward on eval: 42.676914514007514
[37m[1m[2023-06-25 02:42:21,960][129146] Mean Reward across all agents: 1407.7948753467406
[37m[1m[2023-06-25 02:42:21,960][129146] Average Trajectory Length: 936.5166666666667
[36m[2023-06-25 02:42:21,963][129146] mean_value=-685.8124919992862, max_value=1157.1323579350606
[37m[1m[2023-06-25 02:42:21,966][129146] New mean coefficients: [[ 0.51289606 -0.57822293  0.6532667  -0.28952992  0.19716555]]
[37m[1m[2023-06-25 02:42:21,967][129146] Moving the mean solution point...
[36m[2023-06-25 02:42:31,786][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 02:42:31,787][129146] FPS: 391139.61
[36m[2023-06-25 02:42:31,789][129146] itr=163, itrs=2000, Progress: 8.15%
[36m[2023-06-25 02:42:43,364][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 02:42:43,364][129146] FPS: 332118.74
[36m[2023-06-25 02:42:48,169][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:42:48,169][129146] Reward + Measures: [[2579.42431098    0.23959374    0.25571823    0.05334289    0.24349958]]
[37m[1m[2023-06-25 02:42:48,170][129146] Max Reward on eval: 2579.4243109797185
[37m[1m[2023-06-25 02:42:48,170][129146] Min Reward on eval: 2579.4243109797185
[37m[1m[2023-06-25 02:42:48,170][129146] Mean Reward across all agents: 2579.4243109797185
[37m[1m[2023-06-25 02:42:48,170][129146] Average Trajectory Length: 967.5856666666666
[36m[2023-06-25 02:42:53,654][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:42:53,655][129146] Reward + Measures: [[ 546.98048726    0.22077329    0.27795717    0.0952593     0.23071352]
[37m[1m [1108.74809604    0.35249996    0.35209998    0.13010001    0.23190001]
[37m[1m [1018.21506912    0.22146843    0.24377847    0.09078437    0.15096872]
[37m[1m ...
[37m[1m [2555.47062856    0.2208        0.25500003    0.0533        0.22789998]
[37m[1m [1207.55868677    0.19353433    0.23330128    0.09256911    0.21108027]
[37m[1m [ 997.33449345    0.20693333    0.22924104    0.0692        0.21084104]]
[37m[1m[2023-06-25 02:42:53,655][129146] Max Reward on eval: 2708.2947213049047
[37m[1m[2023-06-25 02:42:53,655][129146] Min Reward on eval: -208.84389477407095
[37m[1m[2023-06-25 02:42:53,656][129146] Mean Reward across all agents: 1589.804633667971
[37m[1m[2023-06-25 02:42:53,656][129146] Average Trajectory Length: 912.2176666666667
[36m[2023-06-25 02:42:53,658][129146] mean_value=-669.6818894147011, max_value=1437.3602387365004
[37m[1m[2023-06-25 02:42:53,661][129146] New mean coefficients: [[ 0.95159864 -0.44339222  0.60553986 -0.12052967 -0.26602435]]
[37m[1m[2023-06-25 02:42:53,662][129146] Moving the mean solution point...
[36m[2023-06-25 02:43:03,428][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 02:43:03,429][129146] FPS: 393247.38
[36m[2023-06-25 02:43:03,431][129146] itr=164, itrs=2000, Progress: 8.20%
[36m[2023-06-25 02:43:14,882][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 02:43:14,883][129146] FPS: 335719.36
[36m[2023-06-25 02:43:19,827][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:43:19,827][129146] Reward + Measures: [[2674.36382653    0.23274662    0.2565974     0.05260412    0.24619222]]
[37m[1m[2023-06-25 02:43:19,827][129146] Max Reward on eval: 2674.3638265321
[37m[1m[2023-06-25 02:43:19,827][129146] Min Reward on eval: 2674.3638265321
[37m[1m[2023-06-25 02:43:19,828][129146] Mean Reward across all agents: 2674.3638265321
[37m[1m[2023-06-25 02:43:19,828][129146] Average Trajectory Length: 969.6906666666666
[36m[2023-06-25 02:43:25,239][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:43:25,239][129146] Reward + Measures: [[2835.67087002    0.24249999    0.25920001    0.0454        0.24789999]
[37m[1m [1671.86838488    0.23470215    0.242553      0.05324689    0.20640002]
[37m[1m [2628.35495841    0.23544617    0.27226922    0.05834615    0.2590231 ]
[37m[1m ...
[37m[1m [2223.35380554    0.23433582    0.24100088    0.05409875    0.23569603]
[37m[1m [2592.48017356    0.23139696    0.2553606     0.05788788    0.2493697 ]
[37m[1m [2613.16517728    0.2617        0.2626        0.0671        0.2538    ]]
[37m[1m[2023-06-25 02:43:25,240][129146] Max Reward on eval: 2869.602766994806
[37m[1m[2023-06-25 02:43:25,240][129146] Min Reward on eval: 1671.868384877604
[37m[1m[2023-06-25 02:43:25,240][129146] Mean Reward across all agents: 2438.63648153459
[37m[1m[2023-06-25 02:43:25,240][129146] Average Trajectory Length: 957.1673333333333
[36m[2023-06-25 02:43:25,244][129146] mean_value=-11.103300335111708, max_value=1268.5015241687088
[37m[1m[2023-06-25 02:43:25,247][129146] New mean coefficients: [[ 0.92466897 -0.2636205   0.6189164  -0.18947664  0.30387408]]
[37m[1m[2023-06-25 02:43:25,248][129146] Moving the mean solution point...
[36m[2023-06-25 02:43:34,851][129146] train() took 9.60 seconds to complete
[36m[2023-06-25 02:43:34,851][129146] FPS: 399942.91
[36m[2023-06-25 02:43:34,854][129146] itr=165, itrs=2000, Progress: 8.25%
[36m[2023-06-25 02:43:46,272][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 02:43:46,272][129146] FPS: 336684.00
[36m[2023-06-25 02:43:50,997][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:43:50,997][129146] Reward + Measures: [[2747.08454307    0.22905655    0.25598684    0.04833079    0.24526559]]
[37m[1m[2023-06-25 02:43:50,997][129146] Max Reward on eval: 2747.0845430688482
[37m[1m[2023-06-25 02:43:50,998][129146] Min Reward on eval: 2747.0845430688482
[37m[1m[2023-06-25 02:43:50,998][129146] Mean Reward across all agents: 2747.0845430688482
[37m[1m[2023-06-25 02:43:50,998][129146] Average Trajectory Length: 969.7906666666667
[36m[2023-06-25 02:43:56,407][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:43:56,408][129146] Reward + Measures: [[2337.37205614    0.23450164    0.25760588    0.06599922    0.23935835]
[37m[1m [1796.33276863    0.21820001    0.2313        0.0823        0.2007    ]
[37m[1m [2355.87662774    0.23685245    0.25682166    0.06238415    0.24140273]
[37m[1m ...
[37m[1m [2546.11016506    0.23915873    0.26237005    0.048957      0.24127863]
[37m[1m [2422.25844609    0.25812203    0.27263734    0.06625694    0.25905263]
[37m[1m [1727.25732411    0.22472751    0.24865332    0.07696287    0.24442966]]
[37m[1m[2023-06-25 02:43:56,408][129146] Max Reward on eval: 2968.4809485582637
[37m[1m[2023-06-25 02:43:56,408][129146] Min Reward on eval: 1080.4210674311064
[37m[1m[2023-06-25 02:43:56,409][129146] Mean Reward across all agents: 2476.8209220628983
[37m[1m[2023-06-25 02:43:56,409][129146] Average Trajectory Length: 950.809
[36m[2023-06-25 02:43:56,412][129146] mean_value=-130.86831904110744, max_value=1299.76000959768
[37m[1m[2023-06-25 02:43:56,415][129146] New mean coefficients: [[ 0.35076576 -0.08196187 -0.08020264 -0.0825305   0.13248381]]
[37m[1m[2023-06-25 02:43:56,416][129146] Moving the mean solution point...
[36m[2023-06-25 02:44:06,094][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 02:44:06,094][129146] FPS: 396851.74
[36m[2023-06-25 02:44:06,096][129146] itr=166, itrs=2000, Progress: 8.30%
[36m[2023-06-25 02:44:17,498][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 02:44:17,499][129146] FPS: 337221.70
[36m[2023-06-25 02:44:22,309][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:44:22,310][129146] Reward + Measures: [[2814.36787808    0.22829321    0.25703919    0.04429558    0.24563272]]
[37m[1m[2023-06-25 02:44:22,310][129146] Max Reward on eval: 2814.367878077513
[37m[1m[2023-06-25 02:44:22,310][129146] Min Reward on eval: 2814.367878077513
[37m[1m[2023-06-25 02:44:22,310][129146] Mean Reward across all agents: 2814.367878077513
[37m[1m[2023-06-25 02:44:22,310][129146] Average Trajectory Length: 975.1179999999999
[36m[2023-06-25 02:44:27,923][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:44:27,924][129146] Reward + Measures: [[ -37.16472813    0.51050001    0.58289999    0.31320003    0.59040004]
[37m[1m [1478.58952617    0.3448        0.29620001    0.0742        0.2378    ]
[37m[1m [1897.95216595    0.20110002    0.22909999    0.0794        0.23699999]
[37m[1m ...
[37m[1m [1162.45557812    0.28230944    0.26457065    0.15754429    0.23695324]
[37m[1m [1993.34802818    0.27863583    0.25156319    0.03648508    0.23270729]
[37m[1m [2222.89874346    0.26429999    0.2696        0.11670001    0.2617    ]]
[37m[1m[2023-06-25 02:44:27,924][129146] Max Reward on eval: 2914.9479649094164
[37m[1m[2023-06-25 02:44:27,924][129146] Min Reward on eval: -745.8029010407278
[37m[1m[2023-06-25 02:44:27,924][129146] Mean Reward across all agents: 1383.434653162482
[37m[1m[2023-06-25 02:44:27,925][129146] Average Trajectory Length: 956.7873333333333
[36m[2023-06-25 02:44:27,928][129146] mean_value=-670.1474998602408, max_value=1133.1156763376239
[37m[1m[2023-06-25 02:44:27,930][129146] New mean coefficients: [[-0.15001273  0.16880777 -1.3003559  -0.0394235  -0.18807945]]
[37m[1m[2023-06-25 02:44:27,931][129146] Moving the mean solution point...
[36m[2023-06-25 02:44:37,635][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 02:44:37,635][129146] FPS: 395811.09
[36m[2023-06-25 02:44:37,637][129146] itr=167, itrs=2000, Progress: 8.35%
[36m[2023-06-25 02:44:49,128][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 02:44:49,128][129146] FPS: 334644.79
[36m[2023-06-25 02:44:54,010][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:44:54,010][129146] Reward + Measures: [[2794.69603106    0.23256889    0.25501972    0.04121509    0.24473958]]
[37m[1m[2023-06-25 02:44:54,010][129146] Max Reward on eval: 2794.69603105769
[37m[1m[2023-06-25 02:44:54,011][129146] Min Reward on eval: 2794.69603105769
[37m[1m[2023-06-25 02:44:54,011][129146] Mean Reward across all agents: 2794.69603105769
[37m[1m[2023-06-25 02:44:54,011][129146] Average Trajectory Length: 971.6469999999999
[36m[2023-06-25 02:44:59,557][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:44:59,558][129146] Reward + Measures: [[2284.06461807    0.2863        0.28          0.0742        0.26089999]
[37m[1m [ 793.02105917    0.2308743     0.20639837    0.11046939    0.22969714]
[37m[1m [ 864.74329054    0.23259392    0.23987031    0.12416523    0.1908989 ]
[37m[1m ...
[37m[1m [ 815.64072158    0.31300002    0.33400002    0.11240001    0.25079998]
[37m[1m [2123.54018701    0.26922664    0.25023624    0.0685892     0.23760617]
[37m[1m [2599.16236494    0.24039851    0.25923666    0.03542932    0.24197386]]
[37m[1m[2023-06-25 02:44:59,558][129146] Max Reward on eval: 2826.22591989066
[37m[1m[2023-06-25 02:44:59,558][129146] Min Reward on eval: -179.77417313358745
[37m[1m[2023-06-25 02:44:59,559][129146] Mean Reward across all agents: 1696.4324856123158
[37m[1m[2023-06-25 02:44:59,559][129146] Average Trajectory Length: 955.212
[36m[2023-06-25 02:44:59,561][129146] mean_value=-824.0312481767081, max_value=390.8609328342943
[37m[1m[2023-06-25 02:44:59,563][129146] New mean coefficients: [[-0.0318578   0.26191053 -1.1513718   0.10859973 -0.06333786]]
[37m[1m[2023-06-25 02:44:59,564][129146] Moving the mean solution point...
[36m[2023-06-25 02:45:09,317][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 02:45:09,317][129146] FPS: 393825.63
[36m[2023-06-25 02:45:09,319][129146] itr=168, itrs=2000, Progress: 8.40%
[36m[2023-06-25 02:45:20,943][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 02:45:20,943][129146] FPS: 330803.71
[36m[2023-06-25 02:45:25,773][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:45:25,774][129146] Reward + Measures: [[2501.75836246    0.25108346    0.24682032    0.05511919    0.23292978]]
[37m[1m[2023-06-25 02:45:25,774][129146] Max Reward on eval: 2501.7583624612257
[37m[1m[2023-06-25 02:45:25,774][129146] Min Reward on eval: 2501.7583624612257
[37m[1m[2023-06-25 02:45:25,775][129146] Mean Reward across all agents: 2501.7583624612257
[37m[1m[2023-06-25 02:45:25,775][129146] Average Trajectory Length: 944.1246666666666
[36m[2023-06-25 02:45:31,237][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:45:31,237][129146] Reward + Measures: [[1359.29609671    0.35680002    0.25040001    0.19090001    0.2053    ]
[37m[1m [1944.69416331    0.29010516    0.2567744     0.07770512    0.2598795 ]
[37m[1m [ 995.95049927    0.46900001    0.24679999    0.26430002    0.2181    ]
[37m[1m ...
[37m[1m [1450.16287998    0.27790001    0.20850001    0.1096        0.20650001]
[37m[1m [2205.02775607    0.25489998    0.24560001    0.0663        0.2414    ]
[37m[1m [1585.75069967    0.26847905    0.25487694    0.1185348     0.24593078]]
[37m[1m[2023-06-25 02:45:31,237][129146] Max Reward on eval: 2792.1061455101008
[37m[1m[2023-06-25 02:45:31,238][129146] Min Reward on eval: 398.9051022356492
[37m[1m[2023-06-25 02:45:31,238][129146] Mean Reward across all agents: 1853.183570255034
[37m[1m[2023-06-25 02:45:31,238][129146] Average Trajectory Length: 947.2376666666667
[36m[2023-06-25 02:45:31,241][129146] mean_value=-538.0951377237794, max_value=1191.950956545344
[37m[1m[2023-06-25 02:45:31,244][129146] New mean coefficients: [[-0.32454443  0.50104046 -1.8104973   0.3484062  -0.88261074]]
[37m[1m[2023-06-25 02:45:31,245][129146] Moving the mean solution point...
[36m[2023-06-25 02:45:41,110][129146] train() took 9.86 seconds to complete
[36m[2023-06-25 02:45:41,110][129146] FPS: 389310.81
[36m[2023-06-25 02:45:41,112][129146] itr=169, itrs=2000, Progress: 8.45%
[36m[2023-06-25 02:45:52,766][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 02:45:52,767][129146] FPS: 329878.40
[36m[2023-06-25 02:45:57,682][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:45:57,683][129146] Reward + Measures: [[2415.73929253    0.25851282    0.24623372    0.06495458    0.23160076]]
[37m[1m[2023-06-25 02:45:57,683][129146] Max Reward on eval: 2415.739292532432
[37m[1m[2023-06-25 02:45:57,683][129146] Min Reward on eval: 2415.739292532432
[37m[1m[2023-06-25 02:45:57,683][129146] Mean Reward across all agents: 2415.739292532432
[37m[1m[2023-06-25 02:45:57,684][129146] Average Trajectory Length: 951.7139999999999
[36m[2023-06-25 02:46:03,207][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:46:03,208][129146] Reward + Measures: [[1019.75730497    0.21830001    0.2181        0.17209999    0.26120001]
[37m[1m [2402.46012611    0.27199998    0.25370002    0.0774        0.26590002]
[37m[1m [2144.33861042    0.30247617    0.23726881    0.09448624    0.22020276]
[37m[1m ...
[37m[1m [ 895.00843248    0.5223        0.26110002    0.29659998    0.23269999]
[37m[1m [ 977.05575741    0.31657985    0.22446619    0.15858167    0.22598724]
[37m[1m [2032.71735354    0.287         0.2198        0.0979        0.24679999]]
[37m[1m[2023-06-25 02:46:03,208][129146] Max Reward on eval: 2625.8815748251277
[37m[1m[2023-06-25 02:46:03,208][129146] Min Reward on eval: -133.32904114039556
[37m[1m[2023-06-25 02:46:03,208][129146] Mean Reward across all agents: 1484.5493291993002
[37m[1m[2023-06-25 02:46:03,209][129146] Average Trajectory Length: 967.1286666666666
[36m[2023-06-25 02:46:03,212][129146] mean_value=-189.2675606421166, max_value=1380.0995412418883
[37m[1m[2023-06-25 02:46:03,215][129146] New mean coefficients: [[-0.7158356   0.8916931  -2.4362683   0.55057925 -1.3146702 ]]
[37m[1m[2023-06-25 02:46:03,216][129146] Moving the mean solution point...
[36m[2023-06-25 02:46:13,029][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 02:46:13,029][129146] FPS: 391383.40
[36m[2023-06-25 02:46:13,031][129146] itr=170, itrs=2000, Progress: 8.50%
[37m[1m[2023-06-25 02:46:15,431][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000150
[36m[2023-06-25 02:46:27,230][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 02:46:27,230][129146] FPS: 334895.15
[36m[2023-06-25 02:46:31,981][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:46:31,981][129146] Reward + Measures: [[2219.9126907     0.28114334    0.24564213    0.07952786    0.22510897]]
[37m[1m[2023-06-25 02:46:31,981][129146] Max Reward on eval: 2219.9126906963065
[37m[1m[2023-06-25 02:46:31,981][129146] Min Reward on eval: 2219.9126906963065
[37m[1m[2023-06-25 02:46:31,982][129146] Mean Reward across all agents: 2219.9126906963065
[37m[1m[2023-06-25 02:46:31,982][129146] Average Trajectory Length: 956.453
[36m[2023-06-25 02:46:37,572][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:46:37,572][129146] Reward + Measures: [[2207.37905831    0.25845024    0.22959332    0.06601586    0.25195026]
[37m[1m [ 579.22125624    0.25565735    0.25157085    0.15153539    0.26656649]
[37m[1m [1518.89143963    0.30669999    0.2527        0.1303        0.23840001]
[37m[1m ...
[37m[1m [1239.03625676    0.2757        0.2494        0.12819999    0.24609999]
[37m[1m [1184.73225556    0.29994687    0.23399425    0.15666525    0.24598818]
[37m[1m [1414.22522354    0.28215137    0.24645793    0.1200541     0.21656886]]
[37m[1m[2023-06-25 02:46:37,572][129146] Max Reward on eval: 2644.0234848818045
[37m[1m[2023-06-25 02:46:37,573][129146] Min Reward on eval: 160.75510195033857
[37m[1m[2023-06-25 02:46:37,573][129146] Mean Reward across all agents: 1278.42395685684
[37m[1m[2023-06-25 02:46:37,573][129146] Average Trajectory Length: 944.5236666666666
[36m[2023-06-25 02:46:37,575][129146] mean_value=-1196.4353747269543, max_value=466.8479406056208
[37m[1m[2023-06-25 02:46:37,577][129146] New mean coefficients: [[-0.5238476   0.8247032  -1.894757    0.38625354 -0.51489145]]
[37m[1m[2023-06-25 02:46:37,578][129146] Moving the mean solution point...
[36m[2023-06-25 02:46:47,243][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 02:46:47,243][129146] FPS: 397396.34
[36m[2023-06-25 02:46:47,245][129146] itr=171, itrs=2000, Progress: 8.55%
[36m[2023-06-25 02:46:58,637][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 02:46:58,637][129146] FPS: 337482.15
[36m[2023-06-25 02:47:03,429][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:47:03,430][129146] Reward + Measures: [[2059.08939805    0.30123255    0.243604      0.1005903     0.22391056]]
[37m[1m[2023-06-25 02:47:03,430][129146] Max Reward on eval: 2059.0893980511128
[37m[1m[2023-06-25 02:47:03,430][129146] Min Reward on eval: 2059.0893980511128
[37m[1m[2023-06-25 02:47:03,430][129146] Mean Reward across all agents: 2059.0893980511128
[37m[1m[2023-06-25 02:47:03,430][129146] Average Trajectory Length: 956.528
[36m[2023-06-25 02:47:08,897][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:47:08,898][129146] Reward + Measures: [[1138.73216267    0.34029999    0.23910001    0.15259999    0.21710001]
[37m[1m [1855.91788056    0.33555165    0.23824029    0.09919621    0.21831183]
[37m[1m [1831.85053227    0.32500002    0.25570002    0.11079999    0.23109999]
[37m[1m ...
[37m[1m [1529.54831355    0.38547489    0.2609075     0.16652687    0.24183039]
[37m[1m [ 584.51016455    0.5449        0.3062        0.3251        0.30379999]
[37m[1m [1161.43949217    0.39270002    0.2651        0.18560001    0.22649999]]
[37m[1m[2023-06-25 02:47:08,898][129146] Max Reward on eval: 2272.789809432521
[37m[1m[2023-06-25 02:47:08,898][129146] Min Reward on eval: -120.25248521545436
[37m[1m[2023-06-25 02:47:08,899][129146] Mean Reward across all agents: 1200.5331716785984
[37m[1m[2023-06-25 02:47:08,899][129146] Average Trajectory Length: 970.1959999999999
[36m[2023-06-25 02:47:08,904][129146] mean_value=-166.8708870057278, max_value=1218.901705749921
[37m[1m[2023-06-25 02:47:08,906][129146] New mean coefficients: [[-0.6212929   1.2822688  -2.1479862   0.8298737  -0.44130123]]
[37m[1m[2023-06-25 02:47:08,907][129146] Moving the mean solution point...
[36m[2023-06-25 02:47:18,589][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 02:47:18,590][129146] FPS: 396680.17
[36m[2023-06-25 02:47:18,592][129146] itr=172, itrs=2000, Progress: 8.60%
[36m[2023-06-25 02:47:30,064][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 02:47:30,065][129146] FPS: 335120.13
[36m[2023-06-25 02:47:34,826][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:47:34,827][129146] Reward + Measures: [[1788.46872166    0.3434602     0.24072148    0.14689381    0.22078022]]
[37m[1m[2023-06-25 02:47:34,827][129146] Max Reward on eval: 1788.4687216591549
[37m[1m[2023-06-25 02:47:34,827][129146] Min Reward on eval: 1788.4687216591549
[37m[1m[2023-06-25 02:47:34,827][129146] Mean Reward across all agents: 1788.4687216591549
[37m[1m[2023-06-25 02:47:34,827][129146] Average Trajectory Length: 952.8536666666666
[36m[2023-06-25 02:47:40,284][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:47:40,284][129146] Reward + Measures: [[1517.27262099    0.43689266    0.25301287    0.24043854    0.23973027]
[37m[1m [1270.53611711    0.44779998    0.2441        0.26260003    0.21359999]
[37m[1m [1651.96720493    0.33730003    0.23770002    0.1601        0.21870001]
[37m[1m ...
[37m[1m [ 933.53486101    0.46680003    0.2256        0.30550003    0.20490001]
[37m[1m [1478.8781717     0.35680002    0.22809999    0.1894        0.20030001]
[37m[1m [1628.11903293    0.37040004    0.24099998    0.18359999    0.20830002]]
[37m[1m[2023-06-25 02:47:40,284][129146] Max Reward on eval: 2181.062019904144
[37m[1m[2023-06-25 02:47:40,285][129146] Min Reward on eval: 641.9858189429448
[37m[1m[2023-06-25 02:47:40,285][129146] Mean Reward across all agents: 1430.1015077324737
[37m[1m[2023-06-25 02:47:40,285][129146] Average Trajectory Length: 976.711
[36m[2023-06-25 02:47:40,289][129146] mean_value=-60.61865035391471, max_value=1478.8883757968665
[37m[1m[2023-06-25 02:47:40,291][129146] New mean coefficients: [[-0.606143    1.833272   -2.5112581   1.1468545  -0.75476795]]
[37m[1m[2023-06-25 02:47:40,292][129146] Moving the mean solution point...
[36m[2023-06-25 02:47:50,016][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 02:47:50,016][129146] FPS: 394994.47
[36m[2023-06-25 02:47:50,018][129146] itr=173, itrs=2000, Progress: 8.65%
[36m[2023-06-25 02:48:01,440][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 02:48:01,440][129146] FPS: 336574.15
[36m[2023-06-25 02:48:06,207][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:48:06,207][129146] Reward + Measures: [[1412.1872728     0.43294665    0.23854032    0.24175252    0.22303067]]
[37m[1m[2023-06-25 02:48:06,208][129146] Max Reward on eval: 1412.187272803663
[37m[1m[2023-06-25 02:48:06,208][129146] Min Reward on eval: 1412.187272803663
[37m[1m[2023-06-25 02:48:06,208][129146] Mean Reward across all agents: 1412.187272803663
[37m[1m[2023-06-25 02:48:06,208][129146] Average Trajectory Length: 962.747
[36m[2023-06-25 02:48:11,769][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:48:11,770][129146] Reward + Measures: [[ 214.53512238    0.75299996    0.21729998    0.60050005    0.26550004]
[37m[1m [1028.80500033    0.5774647     0.23953715    0.38077307    0.22165449]
[37m[1m [ 109.81072858    0.83190006    0.19229999    0.67899996    0.29790002]
[37m[1m ...
[37m[1m [ 151.0464336     0.7700001     0.20279999    0.56690001    0.29480001]
[37m[1m [ 993.18920091    0.45413953    0.2288934     0.27675167    0.22587691]
[37m[1m [1012.91846371    0.49404797    0.2246577     0.28519592    0.19806424]]
[37m[1m[2023-06-25 02:48:11,770][129146] Max Reward on eval: 1858.5088151240489
[37m[1m[2023-06-25 02:48:11,770][129146] Min Reward on eval: 58.40216949461028
[37m[1m[2023-06-25 02:48:11,771][129146] Mean Reward across all agents: 464.9847197337245
[37m[1m[2023-06-25 02:48:11,771][129146] Average Trajectory Length: 990.145
[36m[2023-06-25 02:48:11,779][129146] mean_value=293.49080465106005, max_value=1352.0646195814159
[37m[1m[2023-06-25 02:48:11,782][129146] New mean coefficients: [[-0.24445519  1.9817014  -2.6666858   1.3300078  -0.5433085 ]]
[37m[1m[2023-06-25 02:48:11,783][129146] Moving the mean solution point...
[36m[2023-06-25 02:48:21,796][129146] train() took 10.01 seconds to complete
[36m[2023-06-25 02:48:21,797][129146] FPS: 383548.23
[36m[2023-06-25 02:48:21,799][129146] itr=174, itrs=2000, Progress: 8.70%
[36m[2023-06-25 02:48:33,423][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 02:48:33,423][129146] FPS: 330778.48
[36m[2023-06-25 02:48:38,242][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:48:38,242][129146] Reward + Measures: [[1062.65796005    0.4894565     0.21991146    0.31853744    0.2985706 ]]
[37m[1m[2023-06-25 02:48:38,242][129146] Max Reward on eval: 1062.6579600543218
[37m[1m[2023-06-25 02:48:38,243][129146] Min Reward on eval: 1062.6579600543218
[37m[1m[2023-06-25 02:48:38,243][129146] Mean Reward across all agents: 1062.6579600543218
[37m[1m[2023-06-25 02:48:38,243][129146] Average Trajectory Length: 993.0113333333333
[36m[2023-06-25 02:48:43,897][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:48:43,898][129146] Reward + Measures: [[601.74337612   0.63260001   0.14460002   0.49219999   0.31480002]
[37m[1m [751.9596462    0.62900001   0.18169999   0.46720001   0.3136    ]
[37m[1m [340.40510854   0.72349995   0.1328       0.58969998   0.3276    ]
[37m[1m ...
[37m[1m [536.84112215   0.69420004   0.1547       0.54470009   0.32270002]
[37m[1m [597.22502594   0.65379995   0.14910001   0.49990001   0.32530001]
[37m[1m [601.21489144   0.6925       0.1556       0.5438       0.32769999]]
[37m[1m[2023-06-25 02:48:43,903][129146] Max Reward on eval: 1171.8328251925996
[37m[1m[2023-06-25 02:48:43,903][129146] Min Reward on eval: 252.84591918478253
[37m[1m[2023-06-25 02:48:43,904][129146] Mean Reward across all agents: 558.9731250388668
[37m[1m[2023-06-25 02:48:43,904][129146] Average Trajectory Length: 997.6743333333333
[36m[2023-06-25 02:48:43,910][129146] mean_value=264.1815317638861, max_value=1339.2896756564962
[37m[1m[2023-06-25 02:48:43,913][129146] New mean coefficients: [[-0.48931068  2.092018   -2.8827581   1.3559374  -0.5315169 ]]
[37m[1m[2023-06-25 02:48:43,914][129146] Moving the mean solution point...
[36m[2023-06-25 02:48:53,748][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 02:48:53,748][129146] FPS: 390531.11
[36m[2023-06-25 02:48:53,751][129146] itr=175, itrs=2000, Progress: 8.75%
[36m[2023-06-25 02:49:05,214][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 02:49:05,214][129146] FPS: 335408.92
[36m[2023-06-25 02:49:10,005][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:49:10,006][129146] Reward + Measures: [[753.16157069   0.60773194   0.17619886   0.47242883   0.32041174]]
[37m[1m[2023-06-25 02:49:10,006][129146] Max Reward on eval: 753.1615706893143
[37m[1m[2023-06-25 02:49:10,006][129146] Min Reward on eval: 753.1615706893143
[37m[1m[2023-06-25 02:49:10,006][129146] Mean Reward across all agents: 753.1615706893143
[37m[1m[2023-06-25 02:49:10,006][129146] Average Trajectory Length: 997.5156666666667
[36m[2023-06-25 02:49:15,421][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:49:15,421][129146] Reward + Measures: [[ 61.23434231   0.83659995   0.0988       0.7985       0.3637    ]
[37m[1m [179.08750659   0.8524       0.109        0.77100003   0.35700002]
[37m[1m [159.82877552   0.83170003   0.08459999   0.78999996   0.37079999]
[37m[1m ...
[37m[1m [385.91434507   0.74510002   0.13940001   0.62889999   0.32600001]
[37m[1m [ 44.12749918   0.84530002   0.0679       0.83590001   0.3883    ]
[37m[1m [176.77343112   0.83020002   0.09150001   0.75889999   0.36719999]]
[37m[1m[2023-06-25 02:49:15,422][129146] Max Reward on eval: 1155.0707327774376
[37m[1m[2023-06-25 02:49:15,422][129146] Min Reward on eval: -310.53606684319675
[37m[1m[2023-06-25 02:49:15,422][129146] Mean Reward across all agents: 226.88680997793253
[37m[1m[2023-06-25 02:49:15,422][129146] Average Trajectory Length: 998.8783333333333
[36m[2023-06-25 02:49:15,430][129146] mean_value=116.23314723019939, max_value=814.9564061496991
[37m[1m[2023-06-25 02:49:15,432][129146] New mean coefficients: [[-0.36785743  2.2647216  -2.8909965   1.3247551  -0.43079   ]]
[37m[1m[2023-06-25 02:49:15,433][129146] Moving the mean solution point...
[36m[2023-06-25 02:49:25,139][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 02:49:25,140][129146] FPS: 395689.10
[36m[2023-06-25 02:49:25,142][129146] itr=176, itrs=2000, Progress: 8.80%
[36m[2023-06-25 02:49:36,559][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 02:49:36,559][129146] FPS: 336727.51
[36m[2023-06-25 02:49:41,375][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:49:41,376][129146] Reward + Measures: [[501.62654936   0.72333455   0.12243929   0.62880164   0.3587063 ]]
[37m[1m[2023-06-25 02:49:41,376][129146] Max Reward on eval: 501.6265493561191
[37m[1m[2023-06-25 02:49:41,376][129146] Min Reward on eval: 501.6265493561191
[37m[1m[2023-06-25 02:49:41,376][129146] Mean Reward across all agents: 501.6265493561191
[37m[1m[2023-06-25 02:49:41,377][129146] Average Trajectory Length: 999.3679999999999
[36m[2023-06-25 02:49:46,816][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:49:46,816][129146] Reward + Measures: [[805.41613041   0.55650002   0.17819999   0.42280003   0.31299999]
[37m[1m [122.22469826   0.89099997   0.0422       0.85089999   0.4197    ]
[37m[1m [ 32.1713173    0.9174       0.0261       0.88709992   0.4501    ]
[37m[1m ...
[37m[1m [ 99.29156287   0.88810009   0.0278       0.86199999   0.47730002]
[37m[1m [ 53.96314477   0.91609997   0.0367       0.88180012   0.44170004]
[37m[1m [ 77.28647171   0.91210002   0.0304       0.87740004   0.47219998]]
[37m[1m[2023-06-25 02:49:46,817][129146] Max Reward on eval: 805.4161304146692
[37m[1m[2023-06-25 02:49:46,817][129146] Min Reward on eval: -65.51775029463461
[37m[1m[2023-06-25 02:49:46,817][129146] Mean Reward across all agents: 111.20712395152908
[37m[1m[2023-06-25 02:49:46,817][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 02:49:46,824][129146] mean_value=201.14248778721594, max_value=636.7399210516887
[37m[1m[2023-06-25 02:49:46,827][129146] New mean coefficients: [[ 0.35897335  1.699317   -1.6597109   0.9622715   0.27008504]]
[37m[1m[2023-06-25 02:49:46,828][129146] Moving the mean solution point...
[36m[2023-06-25 02:49:56,525][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 02:49:56,525][129146] FPS: 396078.71
[36m[2023-06-25 02:49:56,527][129146] itr=177, itrs=2000, Progress: 8.85%
[36m[2023-06-25 02:50:07,959][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 02:50:07,959][129146] FPS: 336288.88
[36m[2023-06-25 02:50:12,812][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:50:12,813][129146] Reward + Measures: [[-689.88982068    0.98910165    0.00504033    0.99193698    0.90735227]]
[37m[1m[2023-06-25 02:50:12,813][129146] Max Reward on eval: -689.8898206821495
[37m[1m[2023-06-25 02:50:12,813][129146] Min Reward on eval: -689.8898206821495
[37m[1m[2023-06-25 02:50:12,813][129146] Mean Reward across all agents: -689.8898206821495
[37m[1m[2023-06-25 02:50:12,814][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 02:50:18,286][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:50:18,286][129146] Reward + Measures: [[-705.8483644     0.99069995    0.0028        0.99680007    0.92329997]
[37m[1m [-720.55453651    0.9903        0.0043        0.99379998    0.92810005]
[37m[1m [-680.93102541    0.99220002    0.0006        0.99589998    0.91670001]
[37m[1m ...
[37m[1m [-689.8093625     0.98939991    0.0043        0.99379998    0.92690003]
[37m[1m [-676.72859955    0.99040002    0.0035        0.99480003    0.93239993]
[37m[1m [-742.45623021    0.991         0.0044        0.99169999    0.92509997]]
[37m[1m[2023-06-25 02:50:18,286][129146] Max Reward on eval: -596.1783229118213
[37m[1m[2023-06-25 02:50:18,287][129146] Min Reward on eval: -896.2041744176997
[37m[1m[2023-06-25 02:50:18,287][129146] Mean Reward across all agents: -725.0131588835162
[37m[1m[2023-06-25 02:50:18,287][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 02:50:18,288][129146] mean_value=-916.2535215769013, max_value=-530.9453530643303
[36m[2023-06-25 02:50:18,290][129146] XNES is restarting with a new solution whose measures are [0.60748833 0.30119613 0.39316314 0.32501942] and objective is 479.7546885408577
[36m[2023-06-25 02:50:18,291][129146] Emitter restarted. Changing the mean agent...
[37m[1m[2023-06-25 02:50:18,293][129146] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]]
[37m[1m[2023-06-25 02:50:18,294][129146] Moving the mean solution point...
[36m[2023-06-25 02:50:28,119][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 02:50:28,119][129146] FPS: 390908.75
[36m[2023-06-25 02:50:28,121][129146] itr=178, itrs=2000, Progress: 8.90%
[36m[2023-06-25 02:50:39,535][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 02:50:39,535][129146] FPS: 336839.93
[36m[2023-06-25 02:50:44,333][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:50:44,334][129146] Reward + Measures: [[557.03989569   0.54556727   0.29455259   0.33489978   0.2990962 ]]
[37m[1m[2023-06-25 02:50:44,334][129146] Max Reward on eval: 557.0398956860477
[37m[1m[2023-06-25 02:50:44,334][129146] Min Reward on eval: 557.0398956860477
[37m[1m[2023-06-25 02:50:44,334][129146] Mean Reward across all agents: 557.0398956860477
[37m[1m[2023-06-25 02:50:44,334][129146] Average Trajectory Length: 993.318
[36m[2023-06-25 02:50:49,790][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:50:49,790][129146] Reward + Measures: [[ 413.7300051     0.62510008    0.20149998    0.59630001    0.44249997]
[37m[1m [-405.7159567     0.24777041    0.19444947    0.13348179    0.1454085 ]
[37m[1m [-214.93817802    0.29829308    0.1609693     0.24189286    0.13864867]
[37m[1m ...
[37m[1m [ 378.20865964    0.38070002    0.2253        0.19850001    0.20739999]
[37m[1m [-620.16511644    0.20556925    0.1464154     0.15497692    0.11894616]
[37m[1m [-319.93377478    0.17110001    0.31780002    0.26060003    0.38270003]]
[37m[1m[2023-06-25 02:50:49,790][129146] Max Reward on eval: 1135.2424992070185
[37m[1m[2023-06-25 02:50:49,791][129146] Min Reward on eval: -806.7236509022187
[37m[1m[2023-06-25 02:50:49,791][129146] Mean Reward across all agents: 100.4291712342483
[37m[1m[2023-06-25 02:50:49,791][129146] Average Trajectory Length: 943.2143333333333
[36m[2023-06-25 02:50:49,799][129146] mean_value=-422.405267004453, max_value=1002.6342877703253
[37m[1m[2023-06-25 02:50:49,802][129146] New mean coefficients: [[-1.0861166  -0.57338285 -0.79217976 -2.9190207  -1.421625  ]]
[37m[1m[2023-06-25 02:50:49,803][129146] Moving the mean solution point...
[36m[2023-06-25 02:50:59,730][129146] train() took 9.93 seconds to complete
[36m[2023-06-25 02:50:59,730][129146] FPS: 386897.01
[36m[2023-06-25 02:50:59,733][129146] itr=179, itrs=2000, Progress: 8.95%
[36m[2023-06-25 02:51:11,589][129146] train() took 11.84 seconds to complete
[36m[2023-06-25 02:51:11,589][129146] FPS: 324286.64
[36m[2023-06-25 02:51:16,503][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:51:16,503][129146] Reward + Measures: [[541.66889649   0.49841657   0.30342162   0.27879265   0.28723744]]
[37m[1m[2023-06-25 02:51:16,504][129146] Max Reward on eval: 541.668896486648
[37m[1m[2023-06-25 02:51:16,504][129146] Min Reward on eval: 541.668896486648
[37m[1m[2023-06-25 02:51:16,504][129146] Mean Reward across all agents: 541.668896486648
[37m[1m[2023-06-25 02:51:16,504][129146] Average Trajectory Length: 991.7096666666666
[36m[2023-06-25 02:51:22,090][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:51:22,091][129146] Reward + Measures: [[ 546.12740431    0.37020001    0.31200001    0.229         0.29930001]
[37m[1m [-353.10977444    0.3337        0.26280001    0.35100004    0.37550002]
[37m[1m [ 295.526984      0.34198406    0.31338546    0.1324632     0.24473484]
[37m[1m ...
[37m[1m [-268.00009209    0.34120247    0.2703416     0.14564221    0.24988401]
[37m[1m [ 427.45052886    0.36140004    0.35570002    0.18430001    0.28760001]
[37m[1m [ 851.77170373    0.35694313    0.2665112     0.1385469     0.25004107]]
[37m[1m[2023-06-25 02:51:22,096][129146] Max Reward on eval: 1345.0167985447856
[37m[1m[2023-06-25 02:51:22,096][129146] Min Reward on eval: -1002.3577628514147
[37m[1m[2023-06-25 02:51:22,097][129146] Mean Reward across all agents: 154.70509098016757
[37m[1m[2023-06-25 02:51:22,097][129146] Average Trajectory Length: 961.6113333333333
[36m[2023-06-25 02:51:22,102][129146] mean_value=-503.289836543047, max_value=930.4678741386742
[37m[1m[2023-06-25 02:51:22,105][129146] New mean coefficients: [[-2.0108383 -1.0163941 -1.2775261 -2.3670845 -1.5064245]]
[37m[1m[2023-06-25 02:51:22,106][129146] Moving the mean solution point...
[36m[2023-06-25 02:51:31,910][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 02:51:31,910][129146] FPS: 391779.53
[36m[2023-06-25 02:51:31,912][129146] itr=180, itrs=2000, Progress: 9.00%
[37m[1m[2023-06-25 02:51:34,376][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000160
[36m[2023-06-25 02:51:46,322][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 02:51:46,322][129146] FPS: 330825.99
[36m[2023-06-25 02:51:51,200][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:51:51,201][129146] Reward + Measures: [[516.38719445   0.46770665   0.30742383   0.24694206   0.27669567]]
[37m[1m[2023-06-25 02:51:51,201][129146] Max Reward on eval: 516.3871944541113
[37m[1m[2023-06-25 02:51:51,201][129146] Min Reward on eval: 516.3871944541113
[37m[1m[2023-06-25 02:51:51,201][129146] Mean Reward across all agents: 516.3871944541113
[37m[1m[2023-06-25 02:51:51,202][129146] Average Trajectory Length: 989.9613333333333
[36m[2023-06-25 02:51:56,713][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:51:56,714][129146] Reward + Measures: [[-454.60529581    0.32604751    0.18779202    0.25682035    0.13909671]
[37m[1m [  13.54249298    0.22680001    0.24120001    0.20749998    0.1181    ]
[37m[1m [-379.84196329    0.23616605    0.1769421     0.17121588    0.12323699]
[37m[1m ...
[37m[1m [ 938.4018617     0.26530001    0.29060003    0.1592        0.31019998]
[37m[1m [ 250.20840931    0.40699998    0.42490003    0.29980001    0.45169997]
[37m[1m [-486.30231632    0.37547004    0.1618        0.32805449    0.15589283]]
[37m[1m[2023-06-25 02:51:56,714][129146] Max Reward on eval: 1522.3681954043336
[37m[1m[2023-06-25 02:51:56,714][129146] Min Reward on eval: -966.5798528401181
[37m[1m[2023-06-25 02:51:56,714][129146] Mean Reward across all agents: 174.7101183071038
[37m[1m[2023-06-25 02:51:56,715][129146] Average Trajectory Length: 917.3633333333333
[36m[2023-06-25 02:51:56,717][129146] mean_value=-713.6433168726, max_value=937.7807153225638
[37m[1m[2023-06-25 02:51:56,719][129146] New mean coefficients: [[-0.8869237  -0.15906453 -2.1161141  -1.805834   -2.3028672 ]]
[37m[1m[2023-06-25 02:51:56,720][129146] Moving the mean solution point...
[36m[2023-06-25 02:52:06,450][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 02:52:06,451][129146] FPS: 394723.35
[36m[2023-06-25 02:52:06,453][129146] itr=181, itrs=2000, Progress: 9.05%
[36m[2023-06-25 02:52:17,974][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 02:52:17,974][129146] FPS: 333746.55
[36m[2023-06-25 02:52:22,773][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:52:22,773][129146] Reward + Measures: [[436.19585681   0.44992554   0.29695338   0.22866304   0.25786856]]
[37m[1m[2023-06-25 02:52:22,773][129146] Max Reward on eval: 436.1958568051243
[37m[1m[2023-06-25 02:52:22,773][129146] Min Reward on eval: 436.1958568051243
[37m[1m[2023-06-25 02:52:22,774][129146] Mean Reward across all agents: 436.1958568051243
[37m[1m[2023-06-25 02:52:22,774][129146] Average Trajectory Length: 986.8199999999999
[36m[2023-06-25 02:52:28,261][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:52:28,261][129146] Reward + Measures: [[493.15684423   0.52810001   0.24440001   0.35190001   0.23740001]
[37m[1m [294.9849465    0.59639996   0.29969999   0.45109996   0.30850002]
[37m[1m [745.65225055   0.35803232   0.2310313    0.11474537   0.21929465]
[37m[1m ...
[37m[1m [365.72937661   0.48047256   0.20689702   0.33384117   0.22117357]
[37m[1m [217.93796746   0.4375       0.2597       0.23240001   0.2333    ]
[37m[1m [749.6845842    0.33611387   0.23840673   0.08944029   0.23012678]]
[37m[1m[2023-06-25 02:52:28,262][129146] Max Reward on eval: 1263.0587255665916
[37m[1m[2023-06-25 02:52:28,262][129146] Min Reward on eval: -217.82628773605103
[37m[1m[2023-06-25 02:52:28,262][129146] Mean Reward across all agents: 437.5609052110868
[37m[1m[2023-06-25 02:52:28,262][129146] Average Trajectory Length: 952.3233333333333
[36m[2023-06-25 02:52:28,265][129146] mean_value=-740.3131758858208, max_value=849.6272177185526
[37m[1m[2023-06-25 02:52:28,267][129146] New mean coefficients: [[-2.5260396   0.61091304 -1.9060133  -1.4671369  -2.2931619 ]]
[37m[1m[2023-06-25 02:52:28,268][129146] Moving the mean solution point...
[36m[2023-06-25 02:52:38,186][129146] train() took 9.92 seconds to complete
[36m[2023-06-25 02:52:38,186][129146] FPS: 387264.92
[36m[2023-06-25 02:52:38,188][129146] itr=182, itrs=2000, Progress: 9.10%
[36m[2023-06-25 02:52:49,819][129146] train() took 11.62 seconds to complete
[36m[2023-06-25 02:52:49,819][129146] FPS: 330556.69
[36m[2023-06-25 02:52:54,628][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:52:54,629][129146] Reward + Measures: [[319.31508713   0.40550146   0.27484366   0.19422631   0.23359799]]
[37m[1m[2023-06-25 02:52:54,629][129146] Max Reward on eval: 319.3150871254846
[37m[1m[2023-06-25 02:52:54,629][129146] Min Reward on eval: 319.3150871254846
[37m[1m[2023-06-25 02:52:54,629][129146] Mean Reward across all agents: 319.3150871254846
[37m[1m[2023-06-25 02:52:54,630][129146] Average Trajectory Length: 984.4096666666667
[36m[2023-06-25 02:53:00,133][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:53:00,134][129146] Reward + Measures: [[  65.16823711    0.23591962    0.21363738    0.17433831    0.14970373]
[37m[1m [-713.19781997    0.14964305    0.12115569    0.11080886    0.07335823]
[37m[1m [-206.3089557     0.1742        0.13950001    0.0964        0.0623    ]
[37m[1m ...
[37m[1m [ 463.11906526    0.38550004    0.25180003    0.19589999    0.2199    ]
[37m[1m [ 455.41623676    0.33939999    0.28280002    0.14420001    0.26110002]
[37m[1m [-304.41180022    0.2543        0.1873        0.1252        0.1089    ]]
[37m[1m[2023-06-25 02:53:00,134][129146] Max Reward on eval: 1287.7249201854224
[37m[1m[2023-06-25 02:53:00,134][129146] Min Reward on eval: -1208.5741549975937
[37m[1m[2023-06-25 02:53:00,135][129146] Mean Reward across all agents: 97.41934131056298
[37m[1m[2023-06-25 02:53:00,135][129146] Average Trajectory Length: 939.4403333333333
[36m[2023-06-25 02:53:00,137][129146] mean_value=-928.174170609558, max_value=1338.3576225277125
[37m[1m[2023-06-25 02:53:00,139][129146] New mean coefficients: [[-1.8485315   1.3879626  -1.0061316  -0.93530786 -2.9408903 ]]
[37m[1m[2023-06-25 02:53:00,140][129146] Moving the mean solution point...
[36m[2023-06-25 02:53:09,839][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 02:53:09,839][129146] FPS: 396011.03
[36m[2023-06-25 02:53:09,841][129146] itr=183, itrs=2000, Progress: 9.15%
[36m[2023-06-25 02:53:21,268][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 02:53:21,268][129146] FPS: 336450.06
[36m[2023-06-25 02:53:26,079][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:53:26,079][129146] Reward + Measures: [[233.74387976   0.38823658   0.25914448   0.19397719   0.21830338]]
[37m[1m[2023-06-25 02:53:26,079][129146] Max Reward on eval: 233.74387975690274
[37m[1m[2023-06-25 02:53:26,080][129146] Min Reward on eval: 233.74387975690274
[37m[1m[2023-06-25 02:53:26,080][129146] Mean Reward across all agents: 233.74387975690274
[37m[1m[2023-06-25 02:53:26,080][129146] Average Trajectory Length: 985.9376666666666
[36m[2023-06-25 02:53:31,788][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:53:31,788][129146] Reward + Measures: [[-729.78401484    0.27134529    0.20120148    0.16120417    0.15415138]
[37m[1m [-679.31415968    0.23776737    0.20381306    0.16196766    0.16811652]
[37m[1m [-776.89379363    0.29954621    0.16958086    0.19609323    0.16276412]
[37m[1m ...
[37m[1m [-190.82536559    0.22289999    0.16070001    0.15260001    0.18779999]
[37m[1m [ 499.01086453    0.32376155    0.24167033    0.17507583    0.22072308]
[37m[1m [-142.84382074    0.33536813    0.20662175    0.17429744    0.26813903]]
[37m[1m[2023-06-25 02:53:31,788][129146] Max Reward on eval: 1477.098874766915
[37m[1m[2023-06-25 02:53:31,789][129146] Min Reward on eval: -1017.7091410213383
[37m[1m[2023-06-25 02:53:31,789][129146] Mean Reward across all agents: 86.35697872487954
[37m[1m[2023-06-25 02:53:31,789][129146] Average Trajectory Length: 926.5376666666666
[36m[2023-06-25 02:53:31,791][129146] mean_value=-1293.5311781293276, max_value=750.2712137066992
[37m[1m[2023-06-25 02:53:31,793][129146] New mean coefficients: [[-0.64317477  0.97536105 -1.7317584   0.25565684 -3.4457912 ]]
[37m[1m[2023-06-25 02:53:31,794][129146] Moving the mean solution point...
[36m[2023-06-25 02:53:41,596][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 02:53:41,596][129146] FPS: 391851.45
[36m[2023-06-25 02:53:41,598][129146] itr=184, itrs=2000, Progress: 9.20%
[36m[2023-06-25 02:53:53,199][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 02:53:53,199][129146] FPS: 331391.19
[36m[2023-06-25 02:53:58,074][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:53:58,074][129146] Reward + Measures: [[133.013833     0.38969633   0.25764468   0.20059501   0.20873056]]
[37m[1m[2023-06-25 02:53:58,075][129146] Max Reward on eval: 133.0138329965447
[37m[1m[2023-06-25 02:53:58,075][129146] Min Reward on eval: 133.0138329965447
[37m[1m[2023-06-25 02:53:58,075][129146] Mean Reward across all agents: 133.0138329965447
[37m[1m[2023-06-25 02:53:58,076][129146] Average Trajectory Length: 989.661
[36m[2023-06-25 02:54:03,676][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:54:03,676][129146] Reward + Measures: [[ 347.27219761    0.34029999    0.26089999    0.21570002    0.22980002]
[37m[1m [ 140.57341837    0.29029998    0.17050001    0.1652        0.13970001]
[37m[1m [ 138.63970473    0.3144215     0.20752616    0.14200281    0.19559532]
[37m[1m ...
[37m[1m [  28.99862402    0.41764566    0.26153436    0.23146315    0.19735356]
[37m[1m [-562.11284147    0.19966359    0.14628361    0.17326473    0.13776951]
[37m[1m [  70.89935694    0.38180467    0.19558372    0.21467443    0.21315698]]
[37m[1m[2023-06-25 02:54:03,677][129146] Max Reward on eval: 681.1531425673398
[37m[1m[2023-06-25 02:54:03,677][129146] Min Reward on eval: -1027.6569897166803
[37m[1m[2023-06-25 02:54:03,677][129146] Mean Reward across all agents: 42.60229411849382
[37m[1m[2023-06-25 02:54:03,677][129146] Average Trajectory Length: 926.65
[36m[2023-06-25 02:54:03,680][129146] mean_value=-675.4500363956384, max_value=726.8522386811401
[37m[1m[2023-06-25 02:54:03,682][129146] New mean coefficients: [[ 1.3911761  0.5130122 -2.295158   1.2375121 -3.946221 ]]
[37m[1m[2023-06-25 02:54:03,683][129146] Moving the mean solution point...
[36m[2023-06-25 02:54:13,682][129146] train() took 10.00 seconds to complete
[36m[2023-06-25 02:54:13,683][129146] FPS: 384099.42
[36m[2023-06-25 02:54:13,685][129146] itr=185, itrs=2000, Progress: 9.25%
[36m[2023-06-25 02:54:25,365][129146] train() took 11.67 seconds to complete
[36m[2023-06-25 02:54:25,365][129146] FPS: 329153.18
[36m[2023-06-25 02:54:30,284][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:54:30,284][129146] Reward + Measures: [[138.01481218   0.45830655   0.262806     0.24296664   0.22033635]]
[37m[1m[2023-06-25 02:54:30,284][129146] Max Reward on eval: 138.014812183259
[37m[1m[2023-06-25 02:54:30,285][129146] Min Reward on eval: 138.014812183259
[37m[1m[2023-06-25 02:54:30,285][129146] Mean Reward across all agents: 138.014812183259
[37m[1m[2023-06-25 02:54:30,285][129146] Average Trajectory Length: 993.9056666666667
[36m[2023-06-25 02:54:35,778][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:54:35,779][129146] Reward + Measures: [[160.30190104   0.40850002   0.2362       0.23040001   0.1956    ]
[37m[1m [145.61110995   0.40739998   0.21250001   0.2471       0.1591    ]
[37m[1m [564.08640152   0.47740003   0.26690003   0.22949998   0.2247    ]
[37m[1m ...
[37m[1m [192.54632369   0.6724       0.2647       0.44040003   0.2933    ]
[37m[1m [755.59917734   0.31652296   0.1817801    0.13992484   0.21760325]
[37m[1m [618.86096905   0.29168829   0.19528012   0.13857661   0.20413391]]
[37m[1m[2023-06-25 02:54:35,779][129146] Max Reward on eval: 1048.7155534461606
[37m[1m[2023-06-25 02:54:35,779][129146] Min Reward on eval: -48.40693533217418
[37m[1m[2023-06-25 02:54:35,780][129146] Mean Reward across all agents: 310.9942750802939
[37m[1m[2023-06-25 02:54:35,780][129146] Average Trajectory Length: 988.3586666666666
[36m[2023-06-25 02:54:35,783][129146] mean_value=-496.1953541541627, max_value=1255.386294000399
[37m[1m[2023-06-25 02:54:35,785][129146] New mean coefficients: [[ 2.3055816   0.50829804 -3.623943    2.34417    -3.1193118 ]]
[37m[1m[2023-06-25 02:54:35,786][129146] Moving the mean solution point...
[36m[2023-06-25 02:54:45,511][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 02:54:45,511][129146] FPS: 394942.49
[36m[2023-06-25 02:54:45,513][129146] itr=186, itrs=2000, Progress: 9.30%
[36m[2023-06-25 02:54:57,043][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 02:54:57,043][129146] FPS: 333493.77
[36m[2023-06-25 02:55:01,867][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:55:01,868][129146] Reward + Measures: [[187.79601782   0.60597152   0.18994193   0.41833651   0.22667956]]
[37m[1m[2023-06-25 02:55:01,868][129146] Max Reward on eval: 187.79601781782588
[37m[1m[2023-06-25 02:55:01,868][129146] Min Reward on eval: 187.79601781782588
[37m[1m[2023-06-25 02:55:01,868][129146] Mean Reward across all agents: 187.79601781782588
[37m[1m[2023-06-25 02:55:01,868][129146] Average Trajectory Length: 997.5053333333333
[36m[2023-06-25 02:55:07,263][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:55:07,264][129146] Reward + Measures: [[ -243.61545399     0.31050003     0.2045         0.32200003
[37m[1m      0.15449999]
[37m[1m [-1226.10516605     0.11611934     0.1951915      0.18349032
[37m[1m      0.17725472]
[37m[1m [ -355.89044852     0.2106         0.1671         0.26989999
[37m[1m      0.1295    ]
[37m[1m ...
[37m[1m [  116.580421       0.58969998     0.18570001     0.44980001
[37m[1m      0.22710001]
[37m[1m [  214.30046163     0.5521         0.17819999     0.42659998
[37m[1m      0.22930001]
[37m[1m [   22.01441563     0.69100004     0.1821         0.54610002
[37m[1m      0.25009999]]
[37m[1m[2023-06-25 02:55:07,264][129146] Max Reward on eval: 266.1872952139296
[37m[1m[2023-06-25 02:55:07,264][129146] Min Reward on eval: -1470.6160900360264
[37m[1m[2023-06-25 02:55:07,265][129146] Mean Reward across all agents: -409.23409700055623
[37m[1m[2023-06-25 02:55:07,265][129146] Average Trajectory Length: 914.3183333333333
[36m[2023-06-25 02:55:07,269][129146] mean_value=-450.1603231616711, max_value=681.4262535275018
[37m[1m[2023-06-25 02:55:07,271][129146] New mean coefficients: [[ 0.87825704  0.8469965  -3.5559962   2.0631442  -3.0681458 ]]
[37m[1m[2023-06-25 02:55:07,272][129146] Moving the mean solution point...
[36m[2023-06-25 02:55:16,984][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 02:55:16,984][129146] FPS: 395473.25
[36m[2023-06-25 02:55:16,987][129146] itr=187, itrs=2000, Progress: 9.35%
[36m[2023-06-25 02:55:28,611][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 02:55:28,612][129146] FPS: 330761.43
[36m[2023-06-25 02:55:33,427][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:55:33,428][129146] Reward + Measures: [[252.28623294   0.63162184   0.18225616   0.43684465   0.22412309]]
[37m[1m[2023-06-25 02:55:33,428][129146] Max Reward on eval: 252.2862329360844
[37m[1m[2023-06-25 02:55:33,428][129146] Min Reward on eval: 252.2862329360844
[37m[1m[2023-06-25 02:55:33,428][129146] Mean Reward across all agents: 252.2862329360844
[37m[1m[2023-06-25 02:55:33,428][129146] Average Trajectory Length: 996.5793333333334
[36m[2023-06-25 02:55:38,949][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:55:38,950][129146] Reward + Measures: [[-161.28289431    0.30070001    0.17279999    0.25670001    0.1948    ]
[37m[1m [-273.60632796    0.23229197    0.17508967    0.19556205    0.16540726]
[37m[1m [-322.76937711    0.22214113    0.15270624    0.16811712    0.1580929 ]
[37m[1m ...
[37m[1m [ 164.1394809     0.64659995    0.17380001    0.47469997    0.24489999]
[37m[1m [ 185.19590632    0.59039998    0.22590001    0.3863        0.2098    ]
[37m[1m [  18.38620545    0.58640003    0.1402        0.53820002    0.29070002]]
[37m[1m[2023-06-25 02:55:38,950][129146] Max Reward on eval: 353.6299473120882
[37m[1m[2023-06-25 02:55:38,950][129146] Min Reward on eval: -656.6929107231438
[37m[1m[2023-06-25 02:55:38,951][129146] Mean Reward across all agents: -30.97582535460543
[37m[1m[2023-06-25 02:55:38,951][129146] Average Trajectory Length: 951.0153333333333
[36m[2023-06-25 02:55:38,953][129146] mean_value=-504.75413363959535, max_value=620.7533107941097
[37m[1m[2023-06-25 02:55:38,956][129146] New mean coefficients: [[-0.20739365  0.9220945  -3.5423687   2.2891216  -2.8449855 ]]
[37m[1m[2023-06-25 02:55:38,957][129146] Moving the mean solution point...
[36m[2023-06-25 02:55:48,773][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 02:55:48,774][129146] FPS: 391239.46
[36m[2023-06-25 02:55:48,776][129146] itr=188, itrs=2000, Progress: 9.40%
[36m[2023-06-25 02:56:00,383][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 02:56:00,383][129146] FPS: 331222.22
[36m[2023-06-25 02:56:05,242][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:56:05,243][129146] Reward + Measures: [[217.80622208   0.67534888   0.17108206   0.48560116   0.22800016]]
[37m[1m[2023-06-25 02:56:05,243][129146] Max Reward on eval: 217.80622207977464
[37m[1m[2023-06-25 02:56:05,243][129146] Min Reward on eval: 217.80622207977464
[37m[1m[2023-06-25 02:56:05,243][129146] Mean Reward across all agents: 217.80622207977464
[37m[1m[2023-06-25 02:56:05,244][129146] Average Trajectory Length: 998.7379999999999
[36m[2023-06-25 02:56:10,909][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:56:10,910][129146] Reward + Measures: [[ 214.36168001    0.69160002    0.15969999    0.49950001    0.2095    ]
[37m[1m [ 275.921633      0.498         0.16779999    0.34059998    0.21440001]
[37m[1m [ -76.43844617    0.7798        0.14960001    0.6692        0.25530002]
[37m[1m ...
[37m[1m [-381.13696606    0.73799998    0.14260001    0.68040007    0.28299999]
[37m[1m [  75.75177942    0.73699999    0.18080001    0.55139995    0.23290001]
[37m[1m [ -92.70293973    0.2749097     0.13962077    0.2007426     0.17873584]]
[37m[1m[2023-06-25 02:56:10,910][129146] Max Reward on eval: 471.9368896493921
[37m[1m[2023-06-25 02:56:10,910][129146] Min Reward on eval: -721.8014327452286
[37m[1m[2023-06-25 02:56:10,910][129146] Mean Reward across all agents: 26.59471197879959
[37m[1m[2023-06-25 02:56:10,911][129146] Average Trajectory Length: 947.43
[36m[2023-06-25 02:56:10,913][129146] mean_value=-333.150881095055, max_value=734.64065637868
[37m[1m[2023-06-25 02:56:10,916][129146] New mean coefficients: [[-0.05878744  1.0349159  -3.7134886   3.3890772  -2.0325851 ]]
[37m[1m[2023-06-25 02:56:10,917][129146] Moving the mean solution point...
[36m[2023-06-25 02:56:20,661][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 02:56:20,662][129146] FPS: 394141.40
[36m[2023-06-25 02:56:20,664][129146] itr=189, itrs=2000, Progress: 9.45%
[36m[2023-06-25 02:56:32,146][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 02:56:32,146][129146] FPS: 334815.92
[36m[2023-06-25 02:56:36,931][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:56:36,932][129146] Reward + Measures: [[154.99597442   0.72089702   0.15610147   0.54155862   0.23883314]]
[37m[1m[2023-06-25 02:56:36,932][129146] Max Reward on eval: 154.99597442313026
[37m[1m[2023-06-25 02:56:36,932][129146] Min Reward on eval: 154.99597442313026
[37m[1m[2023-06-25 02:56:36,933][129146] Mean Reward across all agents: 154.99597442313026
[37m[1m[2023-06-25 02:56:36,933][129146] Average Trajectory Length: 999.0673333333333
[36m[2023-06-25 02:56:42,440][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:56:42,441][129146] Reward + Measures: [[103.28917141   0.69730002   0.15090001   0.55200005   0.26700002]
[37m[1m [ 37.22583275   0.8247       0.097        0.65500003   0.27919999]
[37m[1m [ 99.18143697   0.67340004   0.18970001   0.46669999   0.1837    ]
[37m[1m ...
[37m[1m [-39.9331478    0.78580004   0.12150001   0.6735       0.30199999]
[37m[1m [ 74.90859096   0.78240007   0.1441       0.62959999   0.27110001]
[37m[1m [ 37.53250373   0.77490002   0.11800001   0.61409998   0.2631    ]]
[37m[1m[2023-06-25 02:56:42,441][129146] Max Reward on eval: 520.1059388999987
[37m[1m[2023-06-25 02:56:42,442][129146] Min Reward on eval: -113.08745662928558
[37m[1m[2023-06-25 02:56:42,442][129146] Mean Reward across all agents: 105.03138639536259
[37m[1m[2023-06-25 02:56:42,442][129146] Average Trajectory Length: 999.6039999999999
[36m[2023-06-25 02:56:42,445][129146] mean_value=-179.31911246262277, max_value=670.2099971242599
[37m[1m[2023-06-25 02:56:42,447][129146] New mean coefficients: [[ 0.961879   -0.18517447 -2.5800152   3.494832   -2.6110573 ]]
[37m[1m[2023-06-25 02:56:42,448][129146] Moving the mean solution point...
[36m[2023-06-25 02:56:52,103][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 02:56:52,103][129146] FPS: 397821.13
[36m[2023-06-25 02:56:52,105][129146] itr=190, itrs=2000, Progress: 9.50%
[37m[1m[2023-06-25 02:56:54,597][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000170
[36m[2023-06-25 02:57:06,519][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 02:57:06,520][129146] FPS: 331511.12
[36m[2023-06-25 02:57:11,242][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:57:11,243][129146] Reward + Measures: [[166.74409622   0.75297719   0.14262673   0.58548146   0.2438295 ]]
[37m[1m[2023-06-25 02:57:11,243][129146] Max Reward on eval: 166.7440962169193
[37m[1m[2023-06-25 02:57:11,243][129146] Min Reward on eval: 166.7440962169193
[37m[1m[2023-06-25 02:57:11,243][129146] Mean Reward across all agents: 166.7440962169193
[37m[1m[2023-06-25 02:57:11,243][129146] Average Trajectory Length: 999.6783333333333
[36m[2023-06-25 02:57:16,720][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:57:16,721][129146] Reward + Measures: [[ 177.12166961    0.64580005    0.20100001    0.49860001    0.27340001]
[37m[1m [-697.93928994    0.35341606    0.18181013    0.28660098    0.17313164]
[37m[1m [ 228.98660519    0.74800003    0.16290002    0.5485        0.2669    ]
[37m[1m ...
[37m[1m [  53.62625528    0.78920001    0.131         0.63600004    0.2516    ]
[37m[1m [-524.28995841    0.30111289    0.34019634    0.17377239    0.2738454 ]
[37m[1m [-331.46705627    0.47839999    0.70999998    0.0795        0.63330001]]
[37m[1m[2023-06-25 02:57:16,721][129146] Max Reward on eval: 444.4326914090663
[37m[1m[2023-06-25 02:57:16,721][129146] Min Reward on eval: -972.8095341463224
[37m[1m[2023-06-25 02:57:16,722][129146] Mean Reward across all agents: 60.15337459806918
[37m[1m[2023-06-25 02:57:16,722][129146] Average Trajectory Length: 991.2669999999999
[36m[2023-06-25 02:57:16,727][129146] mean_value=-234.1054498884337, max_value=803.9916949248087
[37m[1m[2023-06-25 02:57:16,730][129146] New mean coefficients: [[ 2.4417725  -0.07380111 -2.7495534   3.7618556  -2.8262453 ]]
[37m[1m[2023-06-25 02:57:16,731][129146] Moving the mean solution point...
[36m[2023-06-25 02:57:26,442][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 02:57:26,442][129146] FPS: 395495.27
[36m[2023-06-25 02:57:26,444][129146] itr=191, itrs=2000, Progress: 9.55%
[36m[2023-06-25 02:57:37,854][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 02:57:37,854][129146] FPS: 336950.15
[36m[2023-06-25 02:57:42,574][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:57:42,575][129146] Reward + Measures: [[183.7111113    0.7867654    0.12309749   0.63562763   0.24162976]]
[37m[1m[2023-06-25 02:57:42,575][129146] Max Reward on eval: 183.71111129611148
[37m[1m[2023-06-25 02:57:42,575][129146] Min Reward on eval: 183.71111129611148
[37m[1m[2023-06-25 02:57:42,575][129146] Mean Reward across all agents: 183.71111129611148
[37m[1m[2023-06-25 02:57:42,576][129146] Average Trajectory Length: 999.2153333333333
[36m[2023-06-25 02:57:47,932][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:57:47,933][129146] Reward + Measures: [[ 62.96279903   0.45100003   0.33440003   0.43889999   0.38429999]
[37m[1m [ 51.70811368   0.50480002   0.32269999   0.43360001   0.3802    ]
[37m[1m [203.57659594   0.40580001   0.27110001   0.30809999   0.31110001]
[37m[1m ...
[37m[1m [ 46.84116689   0.33240002   0.25890002   0.2465       0.27500001]
[37m[1m [103.00602456   0.34699997   0.25600001   0.333        0.33839998]
[37m[1m [116.93844734   0.83050007   0.17279999   0.71570003   0.34369999]]
[37m[1m[2023-06-25 02:57:47,933][129146] Max Reward on eval: 469.35673598778084
[37m[1m[2023-06-25 02:57:47,933][129146] Min Reward on eval: -698.1605835333233
[37m[1m[2023-06-25 02:57:47,933][129146] Mean Reward across all agents: 13.708908250633394
[37m[1m[2023-06-25 02:57:47,934][129146] Average Trajectory Length: 968.1143333333333
[36m[2023-06-25 02:57:47,937][129146] mean_value=-351.00643228066997, max_value=706.5682035341772
[37m[1m[2023-06-25 02:57:47,940][129146] New mean coefficients: [[ 1.385731   0.7129394 -3.4001367  3.207271  -3.1915011]]
[37m[1m[2023-06-25 02:57:47,941][129146] Moving the mean solution point...
[36m[2023-06-25 02:57:57,461][129146] train() took 9.52 seconds to complete
[36m[2023-06-25 02:57:57,461][129146] FPS: 403420.04
[36m[2023-06-25 02:57:57,463][129146] itr=192, itrs=2000, Progress: 9.60%
[36m[2023-06-25 02:58:09,047][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 02:58:09,048][129146] FPS: 331913.43
[36m[2023-06-25 02:58:13,803][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:58:13,803][129146] Reward + Measures: [[192.0562474    0.81319237   0.10997459   0.67470711   0.24236687]]
[37m[1m[2023-06-25 02:58:13,803][129146] Max Reward on eval: 192.0562474012476
[37m[1m[2023-06-25 02:58:13,804][129146] Min Reward on eval: 192.0562474012476
[37m[1m[2023-06-25 02:58:13,804][129146] Mean Reward across all agents: 192.0562474012476
[37m[1m[2023-06-25 02:58:13,804][129146] Average Trajectory Length: 999.8689999999999
[36m[2023-06-25 02:58:19,259][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:58:19,260][129146] Reward + Measures: [[156.04206666   0.815        0.08670001   0.70569998   0.26490003]
[37m[1m [108.28986873   0.90270007   0.08800001   0.83479995   0.32820001]
[37m[1m [ 75.27256209   0.80200005   0.1115       0.71860003   0.30019999]
[37m[1m ...
[37m[1m [ 73.72437181   0.67379999   0.12980001   0.57370007   0.2667    ]
[37m[1m [ 53.35555821   0.88519996   0.0896       0.7784       0.3152    ]
[37m[1m [274.24160025   0.85140002   0.1093       0.69440001   0.2454    ]]
[37m[1m[2023-06-25 02:58:19,260][129146] Max Reward on eval: 520.1861025715829
[37m[1m[2023-06-25 02:58:19,260][129146] Min Reward on eval: -47.045023916939684
[37m[1m[2023-06-25 02:58:19,260][129146] Mean Reward across all agents: 89.96367004701752
[37m[1m[2023-06-25 02:58:19,260][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 02:58:19,263][129146] mean_value=-70.37545415983136, max_value=333.0280216743297
[37m[1m[2023-06-25 02:58:19,266][129146] New mean coefficients: [[ 0.97201455  2.3996031  -1.6232055   1.4382203  -3.451674  ]]
[37m[1m[2023-06-25 02:58:19,267][129146] Moving the mean solution point...
[36m[2023-06-25 02:58:28,969][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 02:58:28,969][129146] FPS: 395879.16
[36m[2023-06-25 02:58:28,971][129146] itr=193, itrs=2000, Progress: 9.65%
[36m[2023-06-25 02:58:40,439][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 02:58:40,439][129146] FPS: 335294.20
[36m[2023-06-25 02:58:45,252][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:58:45,253][129146] Reward + Measures: [[216.22041528   0.82596296   0.10480333   0.68308634   0.219512  ]]
[37m[1m[2023-06-25 02:58:45,253][129146] Max Reward on eval: 216.22041527617972
[37m[1m[2023-06-25 02:58:45,253][129146] Min Reward on eval: 216.22041527617972
[37m[1m[2023-06-25 02:58:45,253][129146] Mean Reward across all agents: 216.22041527617972
[37m[1m[2023-06-25 02:58:45,253][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 02:58:50,887][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:58:50,887][129146] Reward + Measures: [[158.97935318   0.62869996   0.18750001   0.50349998   0.26320001]
[37m[1m [523.93739922   0.72209996   0.1636       0.4876       0.17510001]
[37m[1m [508.97022102   0.59200001   0.20179999   0.412        0.1753    ]
[37m[1m ...
[37m[1m [550.62082666   0.61109996   0.2296       0.38770005   0.22420001]
[37m[1m [373.13936854   0.6027       0.21640001   0.46700001   0.20729999]
[37m[1m [404.85708752   0.7313       0.1653       0.54540002   0.1965    ]]
[37m[1m[2023-06-25 02:58:50,888][129146] Max Reward on eval: 899.7544591710903
[37m[1m[2023-06-25 02:58:50,888][129146] Min Reward on eval: -5.648026156565175
[37m[1m[2023-06-25 02:58:50,888][129146] Mean Reward across all agents: 393.0297218297742
[37m[1m[2023-06-25 02:58:50,888][129146] Average Trajectory Length: 999.2013333333333
[36m[2023-06-25 02:58:50,895][129146] mean_value=176.28335109889503, max_value=1048.3024728186429
[37m[1m[2023-06-25 02:58:50,898][129146] New mean coefficients: [[ 1.0281924   3.2860432  -0.9494407   0.31912827 -3.332276  ]]
[37m[1m[2023-06-25 02:58:50,899][129146] Moving the mean solution point...
[36m[2023-06-25 02:59:00,539][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 02:59:00,539][129146] FPS: 398402.91
[36m[2023-06-25 02:59:00,541][129146] itr=194, itrs=2000, Progress: 9.70%
[36m[2023-06-25 02:59:11,979][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 02:59:11,980][129146] FPS: 336126.99
[36m[2023-06-25 02:59:16,771][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:59:16,771][129146] Reward + Measures: [[254.75679732   0.84210938   0.10508367   0.68414569   0.19638   ]]
[37m[1m[2023-06-25 02:59:16,772][129146] Max Reward on eval: 254.75679731867027
[37m[1m[2023-06-25 02:59:16,772][129146] Min Reward on eval: 254.75679731867027
[37m[1m[2023-06-25 02:59:16,772][129146] Mean Reward across all agents: 254.75679731867027
[37m[1m[2023-06-25 02:59:16,772][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 02:59:22,201][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:59:22,201][129146] Reward + Measures: [[145.59481341   0.88300002   0.1213       0.76109999   0.2343    ]
[37m[1m [242.55089266   0.75050002   0.14950001   0.57230002   0.18670002]
[37m[1m [216.86608083   0.8405       0.116        0.73200005   0.18350001]
[37m[1m ...
[37m[1m [253.23574992   0.81490004   0.1103       0.70469999   0.1919    ]
[37m[1m [196.82198866   0.75         0.1145       0.7119       0.21690002]
[37m[1m [201.02529118   0.81539994   0.097        0.77939999   0.22720002]]
[37m[1m[2023-06-25 02:59:22,202][129146] Max Reward on eval: 544.8120081504225
[37m[1m[2023-06-25 02:59:22,202][129146] Min Reward on eval: -595.3193163036907
[37m[1m[2023-06-25 02:59:22,202][129146] Mean Reward across all agents: 76.7531800488224
[37m[1m[2023-06-25 02:59:22,202][129146] Average Trajectory Length: 984.3453333333333
[36m[2023-06-25 02:59:22,207][129146] mean_value=-56.699789396167105, max_value=798.6166707386146
[37m[1m[2023-06-25 02:59:22,210][129146] New mean coefficients: [[ 0.25542867  3.3582253  -0.58763117  1.0850396  -4.159283  ]]
[37m[1m[2023-06-25 02:59:22,211][129146] Moving the mean solution point...
[36m[2023-06-25 02:59:31,904][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 02:59:31,904][129146] FPS: 396233.06
[36m[2023-06-25 02:59:31,906][129146] itr=195, itrs=2000, Progress: 9.75%
[36m[2023-06-25 02:59:43,306][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 02:59:43,306][129146] FPS: 337230.66
[36m[2023-06-25 02:59:48,165][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:59:48,166][129146] Reward + Measures: [[223.77350457   0.86702764   0.10310033   0.7179653    0.18245666]]
[37m[1m[2023-06-25 02:59:48,166][129146] Max Reward on eval: 223.77350457036056
[37m[1m[2023-06-25 02:59:48,166][129146] Min Reward on eval: 223.77350457036056
[37m[1m[2023-06-25 02:59:48,167][129146] Mean Reward across all agents: 223.77350457036056
[37m[1m[2023-06-25 02:59:48,167][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 02:59:53,645][129146] Finished Evaluation Step
[37m[1m[2023-06-25 02:59:53,646][129146] Reward + Measures: [[281.39059616   0.81680006   0.12260001   0.63100004   0.21960001]
[37m[1m [214.63171709   0.81829995   0.0896       0.62300009   0.2529    ]
[37m[1m [ 54.38057207   0.67319995   0.19629999   0.5808       0.31370002]
[37m[1m ...
[37m[1m [237.36281009   0.7863       0.15260001   0.57709998   0.22420001]
[37m[1m [146.14808601   0.81980002   0.10040001   0.67159998   0.21939997]
[37m[1m [175.29954038   0.84009999   0.11900001   0.70859998   0.28010002]]
[37m[1m[2023-06-25 02:59:53,646][129146] Max Reward on eval: 342.6992499973159
[37m[1m[2023-06-25 02:59:53,646][129146] Min Reward on eval: -705.3990604613267
[37m[1m[2023-06-25 02:59:53,647][129146] Mean Reward across all agents: 81.5237130846435
[37m[1m[2023-06-25 02:59:53,647][129146] Average Trajectory Length: 995.2956666666666
[36m[2023-06-25 02:59:53,650][129146] mean_value=-143.20096698010474, max_value=765.278101022495
[37m[1m[2023-06-25 02:59:53,652][129146] New mean coefficients: [[-0.32042098  2.8414984  -1.309536    0.74912786 -3.1452146 ]]
[37m[1m[2023-06-25 02:59:53,653][129146] Moving the mean solution point...
[36m[2023-06-25 03:00:03,446][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 03:00:03,447][129146] FPS: 392164.89
[36m[2023-06-25 03:00:03,449][129146] itr=196, itrs=2000, Progress: 9.80%
[36m[2023-06-25 03:00:14,974][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 03:00:14,974][129146] FPS: 333627.73
[36m[2023-06-25 03:00:19,873][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:00:19,874][129146] Reward + Measures: [[45.89538405  0.85823101  0.13834234  0.73468357  0.14670999]]
[37m[1m[2023-06-25 03:00:19,874][129146] Max Reward on eval: 45.895384046489085
[37m[1m[2023-06-25 03:00:19,874][129146] Min Reward on eval: 45.895384046489085
[37m[1m[2023-06-25 03:00:19,874][129146] Mean Reward across all agents: 45.895384046489085
[37m[1m[2023-06-25 03:00:19,875][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:00:25,393][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:00:25,394][129146] Reward + Measures: [[ 44.44560461   0.89019996   0.10399999   0.75410002   0.19160001]
[37m[1m [  5.2502663    0.7877       0.2158       0.6505       0.27360001]
[37m[1m [-50.46105235   0.71569997   0.23769999   0.57880002   0.28220001]
[37m[1m ...
[37m[1m [ 66.12073585   0.85199994   0.14479999   0.70819998   0.189     ]
[37m[1m [-41.74156647   0.83200008   0.16849999   0.67989999   0.18099999]
[37m[1m [ 56.27455874   0.7651       0.1705       0.58510005   0.1929    ]]
[37m[1m[2023-06-25 03:00:25,394][129146] Max Reward on eval: 260.90191148566663
[37m[1m[2023-06-25 03:00:25,394][129146] Min Reward on eval: -157.87972732702036
[37m[1m[2023-06-25 03:00:25,394][129146] Mean Reward across all agents: 29.92793274903677
[37m[1m[2023-06-25 03:00:25,395][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:00:25,397][129146] mean_value=-130.54217326462756, max_value=672.715149554587
[37m[1m[2023-06-25 03:00:25,399][129146] New mean coefficients: [[ 0.27652675  3.693901   -0.4698909   0.78108484 -2.40425   ]]
[37m[1m[2023-06-25 03:00:25,400][129146] Moving the mean solution point...
[36m[2023-06-25 03:00:35,198][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 03:00:35,199][129146] FPS: 391970.30
[36m[2023-06-25 03:00:35,201][129146] itr=197, itrs=2000, Progress: 9.85%
[36m[2023-06-25 03:00:46,669][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 03:00:46,669][129146] FPS: 335300.41
[36m[2023-06-25 03:00:51,402][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:00:51,403][129146] Reward + Measures: [[39.0350829   0.8871997   0.14015299  0.74897695  0.130806  ]]
[37m[1m[2023-06-25 03:00:51,403][129146] Max Reward on eval: 39.03508289838351
[37m[1m[2023-06-25 03:00:51,403][129146] Min Reward on eval: 39.03508289838351
[37m[1m[2023-06-25 03:00:51,404][129146] Mean Reward across all agents: 39.03508289838351
[37m[1m[2023-06-25 03:00:51,404][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:00:57,087][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:00:57,087][129146] Reward + Measures: [[ 144.92125566    0.86910003    0.11130001    0.72149998    0.1693    ]
[37m[1m [-137.04398606    0.40864155    0.1697063     0.41790566    0.24444151]
[37m[1m [ 142.45130001    0.81919998    0.16690001    0.67250007    0.1744    ]
[37m[1m ...
[37m[1m [ 218.44752839    0.58499998    0.18249999    0.46799999    0.22119999]
[37m[1m [-229.06816511    0.49893433    0.15759657    0.37022749    0.19375408]
[37m[1m [  26.64675519    0.88319999    0.11920001    0.77610004    0.14400001]]
[37m[1m[2023-06-25 03:00:57,088][129146] Max Reward on eval: 348.9859784732413
[37m[1m[2023-06-25 03:00:57,088][129146] Min Reward on eval: -229.06816511019716
[37m[1m[2023-06-25 03:00:57,088][129146] Mean Reward across all agents: 77.77728729403297
[37m[1m[2023-06-25 03:00:57,088][129146] Average Trajectory Length: 999.909
[36m[2023-06-25 03:00:57,093][129146] mean_value=25.07189057566137, max_value=731.3631333187222
[37m[1m[2023-06-25 03:00:57,096][129146] New mean coefficients: [[ 0.83336556  3.48888     0.2412374   2.321777   -2.752904  ]]
[37m[1m[2023-06-25 03:00:57,097][129146] Moving the mean solution point...
[36m[2023-06-25 03:01:06,818][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 03:01:06,818][129146] FPS: 395063.79
[36m[2023-06-25 03:01:06,820][129146] itr=198, itrs=2000, Progress: 9.90%
[36m[2023-06-25 03:01:18,265][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 03:01:18,265][129146] FPS: 335940.30
[36m[2023-06-25 03:01:23,204][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:01:23,205][129146] Reward + Measures: [[32.3818403   0.90381038  0.14427833  0.77961099  0.11305933]]
[37m[1m[2023-06-25 03:01:23,205][129146] Max Reward on eval: 32.38184030282813
[37m[1m[2023-06-25 03:01:23,205][129146] Min Reward on eval: 32.38184030282813
[37m[1m[2023-06-25 03:01:23,205][129146] Mean Reward across all agents: 32.38184030282813
[37m[1m[2023-06-25 03:01:23,206][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:01:28,686][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:01:28,687][129146] Reward + Measures: [[248.65128651   0.88570005   0.1314       0.68969995   0.1558    ]
[37m[1m [ -5.14580632   0.921        0.07219999   0.77810001   0.23370002]
[37m[1m [ 48.70241583   0.87940007   0.1506       0.77060002   0.1752    ]
[37m[1m ...
[37m[1m [ -6.06033207   0.49850002   0.20650001   0.40970001   0.14279999]
[37m[1m [ 29.83660151   0.90460008   0.09160001   0.76539999   0.19090001]
[37m[1m [255.35129001   0.85129994   0.1434       0.70630002   0.1229    ]]
[37m[1m[2023-06-25 03:01:28,687][129146] Max Reward on eval: 467.9578258262365
[37m[1m[2023-06-25 03:01:28,687][129146] Min Reward on eval: -488.36623721944636
[37m[1m[2023-06-25 03:01:28,687][129146] Mean Reward across all agents: 135.8603076264655
[37m[1m[2023-06-25 03:01:28,688][129146] Average Trajectory Length: 997.0516666666666
[36m[2023-06-25 03:01:28,692][129146] mean_value=180.9814271027832, max_value=967.9578258262366
[37m[1m[2023-06-25 03:01:28,695][129146] New mean coefficients: [[ 0.74191695  4.2694793  -0.15094593  1.4842262  -3.6149662 ]]
[37m[1m[2023-06-25 03:01:28,696][129146] Moving the mean solution point...
[36m[2023-06-25 03:01:38,461][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 03:01:38,461][129146] FPS: 393296.49
[36m[2023-06-25 03:01:38,464][129146] itr=199, itrs=2000, Progress: 9.95%
[36m[2023-06-25 03:01:49,892][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 03:01:49,893][129146] FPS: 336433.95
[36m[2023-06-25 03:01:54,639][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:01:54,639][129146] Reward + Measures: [[59.56408721  0.92203164  0.15734766  0.79844034  0.09267734]]
[37m[1m[2023-06-25 03:01:54,640][129146] Max Reward on eval: 59.56408720635415
[37m[1m[2023-06-25 03:01:54,640][129146] Min Reward on eval: 59.56408720635415
[37m[1m[2023-06-25 03:01:54,640][129146] Mean Reward across all agents: 59.56408720635415
[37m[1m[2023-06-25 03:01:54,640][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:02:00,114][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:02:00,114][129146] Reward + Measures: [[  384.60761491     0.75480002     0.1714         0.50950003
[37m[1m      0.167     ]
[37m[1m [ -183.2376798      0.25547293     0.17430322     0.27442965
[37m[1m      0.10201492]
[37m[1m [ -278.95467979     0.28661853     0.19500554     0.20414853
[37m[1m      0.18149301]
[37m[1m ...
[37m[1m [-1014.02103339     0.11409012     0.09460741     0.16635557
[37m[1m      0.07635679]
[37m[1m [ -482.58799346     0.16266404     0.13440917     0.19491833
[37m[1m      0.12546077]
[37m[1m [ -137.16164296     0.94000006     0.1319         0.83890003
[37m[1m      0.16150001]]
[37m[1m[2023-06-25 03:02:00,114][129146] Max Reward on eval: 734.9951853902661
[37m[1m[2023-06-25 03:02:00,115][129146] Min Reward on eval: -1196.1239320277411
[37m[1m[2023-06-25 03:02:00,115][129146] Mean Reward across all agents: -227.30275638107085
[37m[1m[2023-06-25 03:02:00,115][129146] Average Trajectory Length: 850.1333333333333
[36m[2023-06-25 03:02:00,119][129146] mean_value=-407.610725290354, max_value=657.0738820261788
[37m[1m[2023-06-25 03:02:00,122][129146] New mean coefficients: [[-0.26833135  4.1423855   0.0528599   1.0972216  -2.1596327 ]]
[37m[1m[2023-06-25 03:02:00,123][129146] Moving the mean solution point...
[36m[2023-06-25 03:02:09,847][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 03:02:09,847][129146] FPS: 394975.72
[36m[2023-06-25 03:02:09,849][129146] itr=200, itrs=2000, Progress: 10.00%
[37m[1m[2023-06-25 03:02:12,411][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000180
[36m[2023-06-25 03:02:24,326][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 03:02:24,326][129146] FPS: 331647.93
[36m[2023-06-25 03:02:29,068][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:02:29,069][129146] Reward + Measures: [[1.30827593 0.93828338 0.17960332 0.82484561 0.085201  ]]
[37m[1m[2023-06-25 03:02:29,069][129146] Max Reward on eval: 1.3082759297564626
[37m[1m[2023-06-25 03:02:29,069][129146] Min Reward on eval: 1.3082759297564626
[37m[1m[2023-06-25 03:02:29,070][129146] Mean Reward across all agents: 1.3082759297564626
[37m[1m[2023-06-25 03:02:29,070][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:02:34,479][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:02:34,480][129146] Reward + Measures: [[ -23.71401523    0.98240006    0.0184        0.94489998    0.4118    ]
[37m[1m [ -33.10214477    0.97730011    0.0562        0.9176001     0.33159995]
[37m[1m [ -64.96409726    0.93570006    0.38600001    0.85009998    0.06519999]
[37m[1m ...
[37m[1m [-133.10698836    0.95759994    0.0609        0.88160002    0.3687    ]
[37m[1m [ -25.80886809    0.91349995    0.21329999    0.84079999    0.0861    ]
[37m[1m [ -36.10141465    0.9637        0.0304        0.94150001    0.51890004]]
[37m[1m[2023-06-25 03:02:34,480][129146] Max Reward on eval: 508.1726149904774
[37m[1m[2023-06-25 03:02:34,480][129146] Min Reward on eval: -203.7658332555904
[37m[1m[2023-06-25 03:02:34,481][129146] Mean Reward across all agents: -4.332036833348133
[37m[1m[2023-06-25 03:02:34,481][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:02:34,488][129146] mean_value=224.5438305598267, max_value=607.2013717137137
[37m[1m[2023-06-25 03:02:34,491][129146] New mean coefficients: [[-0.06314781  3.830166    0.9447355   1.0460389  -1.862956  ]]
[37m[1m[2023-06-25 03:02:34,492][129146] Moving the mean solution point...
[36m[2023-06-25 03:02:44,349][129146] train() took 9.85 seconds to complete
[36m[2023-06-25 03:02:44,349][129146] FPS: 389673.70
[36m[2023-06-25 03:02:44,351][129146] itr=201, itrs=2000, Progress: 10.05%
[36m[2023-06-25 03:02:55,913][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 03:02:55,913][129146] FPS: 332531.66
[36m[2023-06-25 03:03:00,794][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:03:00,794][129146] Reward + Measures: [[-28.24725879   0.95914096   0.32720199   0.88322967   0.05135567]]
[37m[1m[2023-06-25 03:03:00,795][129146] Max Reward on eval: -28.247258788233786
[37m[1m[2023-06-25 03:03:00,795][129146] Min Reward on eval: -28.247258788233786
[37m[1m[2023-06-25 03:03:00,795][129146] Mean Reward across all agents: -28.247258788233786
[37m[1m[2023-06-25 03:03:00,795][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:03:06,430][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:03:06,431][129146] Reward + Measures: [[ -3.26816112   0.93300003   0.11570001   0.84100002   0.19319999]
[37m[1m [-11.04644884   0.92089999   0.21529999   0.83430004   0.10110001]
[37m[1m [ 44.62187203   0.89230007   0.17900001   0.8021       0.1152    ]
[37m[1m ...
[37m[1m [-17.75325826   0.91140002   0.1337       0.83880007   0.15440001]
[37m[1m [ 66.04493948   0.85970002   0.16510001   0.80629998   0.28      ]
[37m[1m [ 52.03946411   0.83219999   0.19130002   0.73920006   0.12620001]]
[37m[1m[2023-06-25 03:03:06,431][129146] Max Reward on eval: 237.1324244635063
[37m[1m[2023-06-25 03:03:06,431][129146] Min Reward on eval: -76.1830470909481
[37m[1m[2023-06-25 03:03:06,432][129146] Mean Reward across all agents: 40.58001849035973
[37m[1m[2023-06-25 03:03:06,432][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:03:06,437][129146] mean_value=43.76853736952793, max_value=524.9434091743291
[37m[1m[2023-06-25 03:03:06,440][129146] New mean coefficients: [[-0.37626085  3.7829037   1.0819297   0.18405032 -1.5052886 ]]
[37m[1m[2023-06-25 03:03:06,441][129146] Moving the mean solution point...
[36m[2023-06-25 03:03:16,368][129146] train() took 9.93 seconds to complete
[36m[2023-06-25 03:03:16,369][129146] FPS: 386874.95
[36m[2023-06-25 03:03:16,371][129146] itr=202, itrs=2000, Progress: 10.10%
[36m[2023-06-25 03:03:28,034][129146] train() took 11.65 seconds to complete
[36m[2023-06-25 03:03:28,034][129146] FPS: 329685.53
[36m[2023-06-25 03:03:32,922][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:03:32,923][129146] Reward + Measures: [[-116.98510957    0.98036391    0.64742333    0.94255799    0.02487467]]
[37m[1m[2023-06-25 03:03:32,923][129146] Max Reward on eval: -116.98510956749732
[37m[1m[2023-06-25 03:03:32,923][129146] Min Reward on eval: -116.98510956749732
[37m[1m[2023-06-25 03:03:32,923][129146] Mean Reward across all agents: -116.98510956749732
[37m[1m[2023-06-25 03:03:32,924][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:03:38,364][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:03:38,364][129146] Reward + Measures: [[-170.30708478    0.95970005    0.36930001    0.87080002    0.0672    ]
[37m[1m [  98.58419431    0.92810005    0.1751        0.80490011    0.0735    ]
[37m[1m [-137.33501398    0.95450002    0.45240003    0.91409999    0.047     ]
[37m[1m ...
[37m[1m [ -77.02086353    0.93020004    0.21589999    0.83590001    0.0744    ]
[37m[1m [-181.1883691     0.96149999    0.56240004    0.91830009    0.0367    ]
[37m[1m [-157.9874899     0.96149999    0.52360004    0.92600006    0.0839    ]]
[37m[1m[2023-06-25 03:03:38,364][129146] Max Reward on eval: 215.67575760814944
[37m[1m[2023-06-25 03:03:38,365][129146] Min Reward on eval: -276.96551556282213
[37m[1m[2023-06-25 03:03:38,365][129146] Mean Reward across all agents: -105.48354011146415
[37m[1m[2023-06-25 03:03:38,365][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:03:38,370][129146] mean_value=43.09677150402072, max_value=608.345147515554
[37m[1m[2023-06-25 03:03:38,373][129146] New mean coefficients: [[-0.4612347   3.8552024   1.4687753  -1.4557041  -0.06553137]]
[37m[1m[2023-06-25 03:03:38,373][129146] Moving the mean solution point...
[36m[2023-06-25 03:03:48,162][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 03:03:48,162][129146] FPS: 392374.74
[36m[2023-06-25 03:03:48,164][129146] itr=203, itrs=2000, Progress: 10.15%
[36m[2023-06-25 03:03:59,718][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 03:03:59,719][129146] FPS: 332793.94
[36m[2023-06-25 03:04:04,542][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:04:04,542][129146] Reward + Measures: [[-260.84355541    0.96560204    0.35256433    0.89212596    0.079003  ]]
[37m[1m[2023-06-25 03:04:04,542][129146] Max Reward on eval: -260.84355540645765
[37m[1m[2023-06-25 03:04:04,543][129146] Min Reward on eval: -260.84355540645765
[37m[1m[2023-06-25 03:04:04,543][129146] Mean Reward across all agents: -260.84355540645765
[37m[1m[2023-06-25 03:04:04,543][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:04:09,988][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:04:09,989][129146] Reward + Measures: [[-302.54276801    0.97430003    0.4849        0.91149998    0.08350001]
[37m[1m [-240.60302863    0.84380001    0.14860001    0.73960006    0.23500001]
[37m[1m [-397.56554596    0.97470009    0.07970001    0.91169995    0.24249999]
[37m[1m ...
[37m[1m [-294.13435541    0.96940005    0.0434        0.91849995    0.34039998]
[37m[1m [-108.81454733    0.91180003    0.0971        0.83090001    0.25839999]
[37m[1m [ -93.03207409    0.89370006    0.34010002    0.77630001    0.0794    ]]
[37m[1m[2023-06-25 03:04:09,989][129146] Max Reward on eval: 62.75981615971541
[37m[1m[2023-06-25 03:04:09,989][129146] Min Reward on eval: -522.5173889652593
[37m[1m[2023-06-25 03:04:09,990][129146] Mean Reward across all agents: -257.8071973681139
[37m[1m[2023-06-25 03:04:09,990][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:04:09,992][129146] mean_value=-245.88139355119333, max_value=406.96792591479607
[37m[1m[2023-06-25 03:04:09,994][129146] New mean coefficients: [[ 0.19021863  3.389852    0.66131884 -0.70113945  0.45797324]]
[37m[1m[2023-06-25 03:04:09,995][129146] Moving the mean solution point...
[36m[2023-06-25 03:04:19,573][129146] train() took 9.58 seconds to complete
[36m[2023-06-25 03:04:19,573][129146] FPS: 400998.31
[36m[2023-06-25 03:04:19,576][129146] itr=204, itrs=2000, Progress: 10.20%
[36m[2023-06-25 03:04:31,254][129146] train() took 11.66 seconds to complete
[36m[2023-06-25 03:04:31,254][129146] FPS: 329265.96
[36m[2023-06-25 03:04:36,008][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:04:36,009][129146] Reward + Measures: [[-239.78120897    0.97737002    0.6049003     0.92550999    0.03920567]]
[37m[1m[2023-06-25 03:04:36,009][129146] Max Reward on eval: -239.78120897016302
[37m[1m[2023-06-25 03:04:36,009][129146] Min Reward on eval: -239.78120897016302
[37m[1m[2023-06-25 03:04:36,010][129146] Mean Reward across all agents: -239.78120897016302
[37m[1m[2023-06-25 03:04:36,010][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:04:41,413][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:04:41,421][129146] Reward + Measures: [[-376.30983649    0.95650005    0.07270001    0.88800001    0.31      ]
[37m[1m [ -57.5110613     0.81700003    0.17690001    0.69960004    0.28049999]
[37m[1m [-163.39429461    0.97979993    0.62090003    0.93290007    0.0198    ]
[37m[1m ...
[37m[1m [-138.79565899    0.93659991    0.19090001    0.77710003    0.1056    ]
[37m[1m [-224.93079446    0.9738        0.72069997    0.93219995    0.0134    ]
[37m[1m [-217.42300965    0.94960004    0.4711        0.88339996    0.10640001]]
[37m[1m[2023-06-25 03:04:41,421][129146] Max Reward on eval: 155.32825324467848
[37m[1m[2023-06-25 03:04:41,422][129146] Min Reward on eval: -491.44216034048003
[37m[1m[2023-06-25 03:04:41,422][129146] Mean Reward across all agents: -225.7715036341126
[37m[1m[2023-06-25 03:04:41,422][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:04:41,426][129146] mean_value=-96.10082468540482, max_value=472.7671567284735
[37m[1m[2023-06-25 03:04:41,429][129146] New mean coefficients: [[ 1.6790574   2.8640366   0.11080563  0.35648692 -0.2061168 ]]
[37m[1m[2023-06-25 03:04:41,430][129146] Moving the mean solution point...
[36m[2023-06-25 03:04:51,244][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 03:04:51,244][129146] FPS: 391344.64
[36m[2023-06-25 03:04:51,246][129146] itr=205, itrs=2000, Progress: 10.25%
[36m[2023-06-25 03:05:02,870][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 03:05:02,870][129146] FPS: 330759.09
[36m[2023-06-25 03:05:07,698][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:05:07,698][129146] Reward + Measures: [[-168.49602885    0.98275161    0.75553566    0.93965936    0.02477967]]
[37m[1m[2023-06-25 03:05:07,698][129146] Max Reward on eval: -168.49602884890285
[37m[1m[2023-06-25 03:05:07,698][129146] Min Reward on eval: -168.49602884890285
[37m[1m[2023-06-25 03:05:07,699][129146] Mean Reward across all agents: -168.49602884890285
[37m[1m[2023-06-25 03:05:07,699][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:05:13,258][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:05:13,258][129146] Reward + Measures: [[ -47.79357973    0.912         0.39639997    0.7827        0.07590001]
[37m[1m [-184.55491405    0.91660005    0.14470001    0.78099996    0.1815    ]
[37m[1m [-252.0612526     0.97900003    0.6419        0.93500006    0.0388    ]
[37m[1m ...
[37m[1m [-253.70161186    0.9738        0.7209        0.91829997    0.0304    ]
[37m[1m [-170.21350977    0.98339999    0.75489998    0.95240003    0.0157    ]
[37m[1m [-212.94911145    0.98029995    0.72059995    0.93480009    0.0218    ]]
[37m[1m[2023-06-25 03:05:13,258][129146] Max Reward on eval: 127.07903332343558
[37m[1m[2023-06-25 03:05:13,259][129146] Min Reward on eval: -397.35155485221185
[37m[1m[2023-06-25 03:05:13,259][129146] Mean Reward across all agents: -197.99652374277096
[37m[1m[2023-06-25 03:05:13,259][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:05:13,264][129146] mean_value=49.50187897151676, max_value=465.84273552446683
[37m[1m[2023-06-25 03:05:13,267][129146] New mean coefficients: [[ 1.4035246   2.87915     1.2831635  -0.09566164  0.5521265 ]]
[37m[1m[2023-06-25 03:05:13,268][129146] Moving the mean solution point...
[36m[2023-06-25 03:05:23,033][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 03:05:23,033][129146] FPS: 393311.48
[36m[2023-06-25 03:05:23,036][129146] itr=206, itrs=2000, Progress: 10.30%
[36m[2023-06-25 03:05:34,577][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 03:05:34,577][129146] FPS: 333151.76
[36m[2023-06-25 03:05:39,393][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:05:39,393][129146] Reward + Measures: [[-141.15680154    0.99024361    0.84298134    0.95657891    0.01432467]]
[37m[1m[2023-06-25 03:05:39,394][129146] Max Reward on eval: -141.1568015403723
[37m[1m[2023-06-25 03:05:39,394][129146] Min Reward on eval: -141.1568015403723
[37m[1m[2023-06-25 03:05:39,394][129146] Mean Reward across all agents: -141.1568015403723
[37m[1m[2023-06-25 03:05:39,395][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:05:45,117][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:05:45,117][129146] Reward + Measures: [[-155.78890642    0.9774        0.63800001    0.91240007    0.029     ]
[37m[1m [-134.9338424     0.98670006    0.7748        0.94099998    0.0188    ]
[37m[1m [-191.07711104    0.94099998    0.38890001    0.84110004    0.0666    ]
[37m[1m ...
[37m[1m [ -72.52680158    0.97570002    0.77109998    0.92060006    0.0282    ]
[37m[1m [-225.37155919    0.98140001    0.3705        0.92810005    0.19890001]
[37m[1m [-104.5753784     0.98509997    0.81490004    0.95180005    0.014     ]]
[37m[1m[2023-06-25 03:05:45,118][129146] Max Reward on eval: 36.526627454184926
[37m[1m[2023-06-25 03:05:45,118][129146] Min Reward on eval: -341.48734005668666
[37m[1m[2023-06-25 03:05:45,118][129146] Mean Reward across all agents: -168.7613942101757
[37m[1m[2023-06-25 03:05:45,118][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:05:45,122][129146] mean_value=7.042034493987321, max_value=500.4235278518172
[37m[1m[2023-06-25 03:05:45,124][129146] New mean coefficients: [[ 2.0460446   1.9877042   1.1411998   1.1026975  -0.89274335]]
[37m[1m[2023-06-25 03:05:45,125][129146] Moving the mean solution point...
[36m[2023-06-25 03:05:54,883][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 03:05:54,884][129146] FPS: 393583.11
[36m[2023-06-25 03:05:54,886][129146] itr=207, itrs=2000, Progress: 10.35%
[36m[2023-06-25 03:06:06,401][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 03:06:06,402][129146] FPS: 333857.58
[36m[2023-06-25 03:06:11,175][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:06:11,175][129146] Reward + Measures: [[-71.49633589   0.98880327   0.86339533   0.95640594   0.01069233]]
[37m[1m[2023-06-25 03:06:11,176][129146] Max Reward on eval: -71.49633588987868
[37m[1m[2023-06-25 03:06:11,176][129146] Min Reward on eval: -71.49633588987868
[37m[1m[2023-06-25 03:06:11,176][129146] Mean Reward across all agents: -71.49633588987868
[37m[1m[2023-06-25 03:06:11,176][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:06:16,623][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:06:16,629][129146] Reward + Measures: [[-901.58889315    0.66177982    0.11542348    0.54714364    0.21070519]
[37m[1m [-209.57953991    0.86709994    0.1201        0.67440003    0.19579999]
[37m[1m [-187.75266371    0.96259993    0.12890001    0.81259996    0.1772    ]
[37m[1m ...
[37m[1m [-428.67676704    0.80410004    0.12039999    0.64250004    0.18710001]
[37m[1m [ -48.0484266     0.94090003    0.0848        0.79269999    0.2225    ]
[37m[1m [-273.15312859    0.92200005    0.12910001    0.76739997    0.18740001]]
[37m[1m[2023-06-25 03:06:16,629][129146] Max Reward on eval: 262.8653335535666
[37m[1m[2023-06-25 03:06:16,629][129146] Min Reward on eval: -901.5888931494904
[37m[1m[2023-06-25 03:06:16,629][129146] Mean Reward across all agents: -213.532428563528
[37m[1m[2023-06-25 03:06:16,630][129146] Average Trajectory Length: 999.3389999999999
[36m[2023-06-25 03:06:16,633][129146] mean_value=-240.76382298722854, max_value=605.5449166059552
[37m[1m[2023-06-25 03:06:16,636][129146] New mean coefficients: [[ 0.19505656  2.5684123   1.7397908  -0.33619344  0.11544096]]
[37m[1m[2023-06-25 03:06:16,637][129146] Moving the mean solution point...
[36m[2023-06-25 03:06:26,495][129146] train() took 9.86 seconds to complete
[36m[2023-06-25 03:06:26,495][129146] FPS: 389604.71
[36m[2023-06-25 03:06:26,498][129146] itr=208, itrs=2000, Progress: 10.40%
[36m[2023-06-25 03:06:37,932][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 03:06:37,932][129146] FPS: 336244.02
[36m[2023-06-25 03:06:42,709][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:06:42,710][129146] Reward + Measures: [[-34.75413251   0.98472297   0.87360561   0.95539731   0.012022  ]]
[37m[1m[2023-06-25 03:06:42,710][129146] Max Reward on eval: -34.754132512208265
[37m[1m[2023-06-25 03:06:42,710][129146] Min Reward on eval: -34.754132512208265
[37m[1m[2023-06-25 03:06:42,710][129146] Mean Reward across all agents: -34.754132512208265
[37m[1m[2023-06-25 03:06:42,711][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:06:48,179][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:06:48,180][129146] Reward + Measures: [[-183.6991925     0.98439997    0.87720007    0.958         0.0106    ]
[37m[1m [-140.89476263    0.98390007    0.9073        0.96789998    0.0064    ]
[37m[1m [-483.07157913    0.97049999    0.89650005    0.94339991    0.0155    ]
[37m[1m ...
[37m[1m [  47.00209514    0.9629001     0.70310003    0.87770003    0.0452    ]
[37m[1m [-128.70507763    0.9853999     0.85659999    0.9637        0.0133    ]
[37m[1m [ -31.50186445    0.97280008    0.61320001    0.90970004    0.0302    ]]
[37m[1m[2023-06-25 03:06:48,180][129146] Max Reward on eval: 161.68801928011234
[37m[1m[2023-06-25 03:06:48,180][129146] Min Reward on eval: -657.2259213556069
[37m[1m[2023-06-25 03:06:48,180][129146] Mean Reward across all agents: -161.93358525830007
[37m[1m[2023-06-25 03:06:48,181][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:06:48,185][129146] mean_value=-2.382296165357196, max_value=470.90267228817777
[37m[1m[2023-06-25 03:06:48,188][129146] New mean coefficients: [[ 0.5872842   1.7196907   1.6267202   0.2519558  -0.72832185]]
[37m[1m[2023-06-25 03:06:48,189][129146] Moving the mean solution point...
[36m[2023-06-25 03:06:57,906][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 03:06:57,907][129146] FPS: 395227.33
[36m[2023-06-25 03:06:57,909][129146] itr=209, itrs=2000, Progress: 10.45%
[36m[2023-06-25 03:07:09,414][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 03:07:09,414][129146] FPS: 334169.16
[36m[2023-06-25 03:07:14,297][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:07:14,298][129146] Reward + Measures: [[7.50645209 0.98814136 0.88625127 0.96202701 0.01000533]]
[37m[1m[2023-06-25 03:07:14,298][129146] Max Reward on eval: 7.50645209266208
[37m[1m[2023-06-25 03:07:14,298][129146] Min Reward on eval: 7.50645209266208
[37m[1m[2023-06-25 03:07:14,298][129146] Mean Reward across all agents: 7.50645209266208
[37m[1m[2023-06-25 03:07:14,298][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:07:19,759][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:07:19,759][129146] Reward + Measures: [[  60.35628019    0.98460001    0.79119998    0.95249999    0.0278    ]
[37m[1m [  44.81732038    0.98509997    0.84800005    0.95770007    0.0129    ]
[37m[1m [-189.03411491    0.7155        0.41230002    0.62849998    0.21920002]
[37m[1m ...
[37m[1m [ -47.69447264    0.9156        0.52790004    0.81040001    0.0691    ]
[37m[1m [  47.36009676    0.98850006    0.65300006    0.96670002    0.0107    ]
[37m[1m [ -69.90807196    0.47849998    0.4772        0.42069998    0.41430002]]
[37m[1m[2023-06-25 03:07:19,760][129146] Max Reward on eval: 258.53823821267576
[37m[1m[2023-06-25 03:07:19,760][129146] Min Reward on eval: -424.07022077018627
[37m[1m[2023-06-25 03:07:19,760][129146] Mean Reward across all agents: -39.13178323802864
[37m[1m[2023-06-25 03:07:19,760][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:07:19,770][129146] mean_value=74.9304730007527, max_value=657.3284793693456
[37m[1m[2023-06-25 03:07:19,773][129146] New mean coefficients: [[ 0.05058134  1.9468625   2.5855632  -0.55062014 -0.41013646]]
[37m[1m[2023-06-25 03:07:19,774][129146] Moving the mean solution point...
[36m[2023-06-25 03:07:29,632][129146] train() took 9.86 seconds to complete
[36m[2023-06-25 03:07:29,632][129146] FPS: 389616.89
[36m[2023-06-25 03:07:29,634][129146] itr=210, itrs=2000, Progress: 10.50%
[37m[1m[2023-06-25 03:07:32,355][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000190
[36m[2023-06-25 03:07:44,280][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 03:07:44,280][129146] FPS: 331449.14
[36m[2023-06-25 03:07:49,209][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:07:49,210][129146] Reward + Measures: [[13.36859458  0.98986292  0.89616323  0.96544206  0.009575  ]]
[37m[1m[2023-06-25 03:07:49,210][129146] Max Reward on eval: 13.368594580495852
[37m[1m[2023-06-25 03:07:49,210][129146] Min Reward on eval: 13.368594580495852
[37m[1m[2023-06-25 03:07:49,210][129146] Mean Reward across all agents: 13.368594580495852
[37m[1m[2023-06-25 03:07:49,210][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:07:54,809][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:07:54,809][129146] Reward + Measures: [[ -54.84680082    0.85979998    0.0764        0.90450001    0.51789999]
[37m[1m [-230.20869889    0.87439996    0.13310002    0.66720003    0.21930002]
[37m[1m [-163.89843437    0.97659999    0.5535        0.93269998    0.0405    ]
[37m[1m ...
[37m[1m [ 100.6590571     0.93660003    0.67049998    0.86329997    0.0455    ]
[37m[1m [ 114.02968369    0.42810002    0.29179999    0.43760005    0.11179999]
[37m[1m [ -12.28310114    0.58140004    0.18950002    0.54800004    0.21619999]]
[37m[1m[2023-06-25 03:07:54,809][129146] Max Reward on eval: 231.53018693311896
[37m[1m[2023-06-25 03:07:54,810][129146] Min Reward on eval: -514.7688517583243
[37m[1m[2023-06-25 03:07:54,810][129146] Mean Reward across all agents: -28.963023832019882
[37m[1m[2023-06-25 03:07:54,810][129146] Average Trajectory Length: 998.1936666666667
[36m[2023-06-25 03:07:54,816][129146] mean_value=-16.78259962998812, max_value=619.2318122219701
[37m[1m[2023-06-25 03:07:54,819][129146] New mean coefficients: [[ 0.29544497  1.2516851   3.1137066  -1.054357   -0.67006993]]
[37m[1m[2023-06-25 03:07:54,820][129146] Moving the mean solution point...
[36m[2023-06-25 03:08:04,433][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 03:08:04,433][129146] FPS: 399521.70
[36m[2023-06-25 03:08:04,435][129146] itr=211, itrs=2000, Progress: 10.55%
[36m[2023-06-25 03:08:16,000][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 03:08:16,000][129146] FPS: 332493.48
[36m[2023-06-25 03:08:20,769][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:08:20,770][129146] Reward + Measures: [[0.51196211 0.991575   0.9117527  0.97112757 0.007779  ]]
[37m[1m[2023-06-25 03:08:20,770][129146] Max Reward on eval: 0.5119621105456414
[37m[1m[2023-06-25 03:08:20,770][129146] Min Reward on eval: 0.5119621105456414
[37m[1m[2023-06-25 03:08:20,770][129146] Mean Reward across all agents: 0.5119621105456414
[37m[1m[2023-06-25 03:08:20,771][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:08:26,258][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:08:26,259][129146] Reward + Measures: [[ -18.99231865    0.90529996    0.49680001    0.7683        0.0565    ]
[37m[1m [-108.42968533    0.96800005    0.73509997    0.92510003    0.0344    ]
[37m[1m [   8.41840921    0.97240001    0.80229998    0.93479997    0.0357    ]
[37m[1m ...
[37m[1m [-149.06396389    0.93679994    0.31890002    0.82690001    0.10520001]
[37m[1m [ -76.01680184    0.90030003    0.35800001    0.78599995    0.17150001]
[37m[1m [ -33.45910435    0.99209994    0.90250009    0.97510004    0.0075    ]]
[37m[1m[2023-06-25 03:08:26,259][129146] Max Reward on eval: 182.31861626569008
[37m[1m[2023-06-25 03:08:26,259][129146] Min Reward on eval: -184.84719230654883
[37m[1m[2023-06-25 03:08:26,260][129146] Mean Reward across all agents: -39.15451792117727
[37m[1m[2023-06-25 03:08:26,260][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:08:26,263][129146] mean_value=-43.23271627581988, max_value=593.7932453100511
[37m[1m[2023-06-25 03:08:26,265][129146] New mean coefficients: [[ 0.47763735  0.93599284  3.9150493  -1.575737   -1.2362583 ]]
[37m[1m[2023-06-25 03:08:26,266][129146] Moving the mean solution point...
[36m[2023-06-25 03:08:36,122][129146] train() took 9.85 seconds to complete
[36m[2023-06-25 03:08:36,123][129146] FPS: 389670.90
[36m[2023-06-25 03:08:36,125][129146] itr=212, itrs=2000, Progress: 10.60%
[36m[2023-06-25 03:08:47,546][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 03:08:47,547][129146] FPS: 336712.35
[36m[2023-06-25 03:08:52,379][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:08:52,379][129146] Reward + Measures: [[24.4620522   0.99217594  0.92207599  0.97396266  0.00658467]]
[37m[1m[2023-06-25 03:08:52,379][129146] Max Reward on eval: 24.462052201298075
[37m[1m[2023-06-25 03:08:52,379][129146] Min Reward on eval: 24.462052201298075
[37m[1m[2023-06-25 03:08:52,379][129146] Mean Reward across all agents: 24.462052201298075
[37m[1m[2023-06-25 03:08:52,380][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:08:57,861][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:08:57,862][129146] Reward + Measures: [[-152.3047319     0.92800009    0.182         0.79580003    0.1612    ]
[37m[1m [ -12.58306588    0.77520001    0.2349        0.59250003    0.22640002]
[37m[1m [ -61.35107435    0.97179997    0.54869998    0.91880006    0.0248    ]
[37m[1m ...
[37m[1m [-173.87051662    0.97530001    0.68780005    0.93769997    0.0343    ]
[37m[1m [-130.26139119    0.99200004    0.92460006    0.97760004    0.0067    ]
[37m[1m [-181.8398908     0.9787001     0.52710003    0.94340003    0.0244    ]]
[37m[1m[2023-06-25 03:08:57,862][129146] Max Reward on eval: 166.05607054670108
[37m[1m[2023-06-25 03:08:57,862][129146] Min Reward on eval: -300.9551081099431
[37m[1m[2023-06-25 03:08:57,862][129146] Mean Reward across all agents: -111.03576007142634
[37m[1m[2023-06-25 03:08:57,863][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:08:57,864][129146] mean_value=-141.35537322281206, max_value=246.33648262372964
[37m[1m[2023-06-25 03:08:57,867][129146] New mean coefficients: [[ 0.67592466  1.3575388   2.2119236  -0.11848736  0.33347905]]
[37m[1m[2023-06-25 03:08:57,868][129146] Moving the mean solution point...
[36m[2023-06-25 03:09:07,613][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 03:09:07,618][129146] FPS: 394082.11
[36m[2023-06-25 03:09:07,625][129146] itr=213, itrs=2000, Progress: 10.65%
[36m[2023-06-25 03:09:19,057][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 03:09:19,057][129146] FPS: 336587.50
[36m[2023-06-25 03:09:23,780][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:09:23,781][129146] Reward + Measures: [[59.50871704  0.99313098  0.92303532  0.97519499  0.00686167]]
[37m[1m[2023-06-25 03:09:23,781][129146] Max Reward on eval: 59.50871703785532
[37m[1m[2023-06-25 03:09:23,781][129146] Min Reward on eval: 59.50871703785532
[37m[1m[2023-06-25 03:09:23,781][129146] Mean Reward across all agents: 59.50871703785532
[37m[1m[2023-06-25 03:09:23,781][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:09:29,251][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:09:29,252][129146] Reward + Measures: [[-115.7337231     0.98500007    0.91860002    0.97290003    0.0051    ]
[37m[1m [ 104.9335138     0.99220002    0.9332        0.98269999    0.0053    ]
[37m[1m [ -10.90733241    0.98229992    0.89250004    0.96110004    0.0089    ]
[37m[1m ...
[37m[1m [  45.69103553    0.98610002    0.90220004    0.96940005    0.0061    ]
[37m[1m [ 139.84095128    0.99290007    0.92799997    0.97680008    0.0055    ]
[37m[1m [ -82.65035172    0.98839998    0.79240006    0.96509999    0.016     ]]
[37m[1m[2023-06-25 03:09:29,252][129146] Max Reward on eval: 339.1203971162264
[37m[1m[2023-06-25 03:09:29,252][129146] Min Reward on eval: -317.10520213880807
[37m[1m[2023-06-25 03:09:29,252][129146] Mean Reward across all agents: 9.28628149479549
[37m[1m[2023-06-25 03:09:29,253][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:09:29,258][129146] mean_value=4.07810374286392, max_value=712.0586080229791
[37m[1m[2023-06-25 03:09:29,260][129146] New mean coefficients: [[ 1.0476072   2.820293    2.7377174  -0.77298194 -0.54787457]]
[37m[1m[2023-06-25 03:09:29,261][129146] Moving the mean solution point...
[36m[2023-06-25 03:09:38,925][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 03:09:38,925][129146] FPS: 397424.18
[36m[2023-06-25 03:09:38,928][129146] itr=214, itrs=2000, Progress: 10.70%
[36m[2023-06-25 03:09:50,368][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 03:09:50,368][129146] FPS: 336054.63
[36m[2023-06-25 03:09:55,142][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:09:55,142][129146] Reward + Measures: [[60.57361282  0.99357569  0.92997897  0.97698998  0.00616067]]
[37m[1m[2023-06-25 03:09:55,143][129146] Max Reward on eval: 60.57361281525885
[37m[1m[2023-06-25 03:09:55,143][129146] Min Reward on eval: 60.57361281525885
[37m[1m[2023-06-25 03:09:55,143][129146] Mean Reward across all agents: 60.57361281525885
[37m[1m[2023-06-25 03:09:55,143][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:10:00,760][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:10:00,761][129146] Reward + Measures: [[ 140.41426945    0.99550003    0.94109994    0.98309994    0.0024    ]
[37m[1m [ -89.97842184    0.98039991    0.67919999    0.95559996    0.0372    ]
[37m[1m [  49.01724306    0.9932        0.91009998    0.97399998    0.0095    ]
[37m[1m ...
[37m[1m [  41.37297993    0.9835        0.86690009    0.95760006    0.0131    ]
[37m[1m [ 104.79243998    0.98620003    0.88190001    0.96800005    0.0095    ]
[37m[1m [-448.79114306    0.79170001    0.16580002    0.63819999    0.25040004]]
[37m[1m[2023-06-25 03:10:00,761][129146] Max Reward on eval: 282.174553624168
[37m[1m[2023-06-25 03:10:00,761][129146] Min Reward on eval: -503.11095134014033
[37m[1m[2023-06-25 03:10:00,762][129146] Mean Reward across all agents: -54.250406504932215
[37m[1m[2023-06-25 03:10:00,762][129146] Average Trajectory Length: 999.6476666666666
[36m[2023-06-25 03:10:00,769][129146] mean_value=26.222698614510808, max_value=576.7798649823642
[37m[1m[2023-06-25 03:10:00,772][129146] New mean coefficients: [[ 1.8584033   2.8831646   3.5375907  -0.59295726 -1.4400074 ]]
[37m[1m[2023-06-25 03:10:00,773][129146] Moving the mean solution point...
[36m[2023-06-25 03:10:10,442][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 03:10:10,443][129146] FPS: 397196.19
[36m[2023-06-25 03:10:10,445][129146] itr=215, itrs=2000, Progress: 10.75%
[36m[2023-06-25 03:10:21,860][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 03:10:21,861][129146] FPS: 336782.75
[36m[2023-06-25 03:10:26,731][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:10:26,731][129146] Reward + Measures: [[91.89112139  0.99407309  0.94602495  0.98243463  0.00477467]]
[37m[1m[2023-06-25 03:10:26,731][129146] Max Reward on eval: 91.89112139138007
[37m[1m[2023-06-25 03:10:26,732][129146] Min Reward on eval: 91.89112139138007
[37m[1m[2023-06-25 03:10:26,732][129146] Mean Reward across all agents: 91.89112139138007
[37m[1m[2023-06-25 03:10:26,732][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:10:32,219][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:10:32,219][129146] Reward + Measures: [[381.37815525   0.64799994   0.25839999   0.46190006   0.18679999]
[37m[1m [132.77129401   0.84170002   0.41170001   0.72389996   0.089     ]
[37m[1m [ 10.65743924   0.98439997   0.90910006   0.97180003   0.0082    ]
[37m[1m ...
[37m[1m [  8.74733181   0.9938001    0.93400002   0.98119992   0.0056    ]
[37m[1m [-21.06561282   0.95349997   0.8648001    0.92189997   0.0255    ]
[37m[1m [-26.73491999   0.99150002   0.92830002   0.98050004   0.0056    ]]
[37m[1m[2023-06-25 03:10:32,219][129146] Max Reward on eval: 413.24492179081426
[37m[1m[2023-06-25 03:10:32,220][129146] Min Reward on eval: -256.0522473312682
[37m[1m[2023-06-25 03:10:32,220][129146] Mean Reward across all agents: -21.93985790481461
[37m[1m[2023-06-25 03:10:32,220][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:10:32,223][129146] mean_value=-69.65382534763918, max_value=823.4885581578184
[37m[1m[2023-06-25 03:10:32,226][129146] New mean coefficients: [[ 1.6805047  4.706221   4.465096  -2.1899786 -1.8300321]]
[37m[1m[2023-06-25 03:10:32,227][129146] Moving the mean solution point...
[36m[2023-06-25 03:10:41,914][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 03:10:41,915][129146] FPS: 396464.07
[36m[2023-06-25 03:10:41,917][129146] itr=216, itrs=2000, Progress: 10.80%
[36m[2023-06-25 03:10:53,366][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 03:10:53,367][129146] FPS: 335813.47
[36m[2023-06-25 03:10:58,139][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:10:58,139][129146] Reward + Measures: [[120.10250371   0.99293369   0.94224429   0.98169065   0.00594533]]
[37m[1m[2023-06-25 03:10:58,140][129146] Max Reward on eval: 120.10250371169433
[37m[1m[2023-06-25 03:10:58,140][129146] Min Reward on eval: 120.10250371169433
[37m[1m[2023-06-25 03:10:58,140][129146] Mean Reward across all agents: 120.10250371169433
[37m[1m[2023-06-25 03:10:58,140][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:11:03,560][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:11:03,560][129146] Reward + Measures: [[  29.41766507    0.91730005    0.57300001    0.84779996    0.0789    ]
[37m[1m [  62.26805093    0.99160004    0.8738001     0.97410005    0.0079    ]
[37m[1m [-211.83910009    0.87169999    0.26990002    0.72070003    0.13540001]
[37m[1m ...
[37m[1m [  -3.91619188    0.82660007    0.25229999    0.66430008    0.0954    ]
[37m[1m [  35.68837679    0.85420001    0.56850004    0.77829999    0.10110001]
[37m[1m [   5.7798005     0.98920006    0.89370006    0.97510004    0.015     ]]
[37m[1m[2023-06-25 03:11:03,560][129146] Max Reward on eval: 252.64314335860544
[37m[1m[2023-06-25 03:11:03,561][129146] Min Reward on eval: -275.2004130143905
[37m[1m[2023-06-25 03:11:03,561][129146] Mean Reward across all agents: -20.69221519387616
[37m[1m[2023-06-25 03:11:03,561][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:11:03,564][129146] mean_value=-42.06557199946151, max_value=736.31039138973
[37m[1m[2023-06-25 03:11:03,567][129146] New mean coefficients: [[ 1.9177034   4.187047    5.734417   -3.44576    -0.23724449]]
[37m[1m[2023-06-25 03:11:03,567][129146] Moving the mean solution point...
[36m[2023-06-25 03:11:13,479][129146] train() took 9.91 seconds to complete
[36m[2023-06-25 03:11:13,479][129146] FPS: 387488.94
[36m[2023-06-25 03:11:13,482][129146] itr=217, itrs=2000, Progress: 10.85%
[36m[2023-06-25 03:11:25,057][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 03:11:25,057][129146] FPS: 332203.44
[36m[2023-06-25 03:11:29,841][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:11:29,842][129146] Reward + Measures: [[184.25389251   0.9936893    0.94692367   0.98298365   0.005192  ]]
[37m[1m[2023-06-25 03:11:29,842][129146] Max Reward on eval: 184.25389251348997
[37m[1m[2023-06-25 03:11:29,842][129146] Min Reward on eval: 184.25389251348997
[37m[1m[2023-06-25 03:11:29,842][129146] Mean Reward across all agents: 184.25389251348997
[37m[1m[2023-06-25 03:11:29,843][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:11:35,253][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:11:35,253][129146] Reward + Measures: [[ -314.94972991     0.57160002     0.3134         0.48559999
[37m[1m      0.32550001]
[37m[1m [-1301.16922929     0.30415714     0.24692857     0.40600714
[37m[1m      0.2579    ]
[37m[1m [  281.65410053     0.32269999     0.20419998     0.21599999
[37m[1m      0.193     ]
[37m[1m ...
[37m[1m [ -885.21199886     0.37830001     0.44510004     0.40270001
[37m[1m      0.44390002]
[37m[1m [ -516.62666308     0.25494432     0.62526602     0.33062062
[37m[1m      0.50396395]
[37m[1m [ -855.23402247     0.2818         0.40120003     0.27600002
[37m[1m      0.36620003]]
[37m[1m[2023-06-25 03:11:35,253][129146] Max Reward on eval: 570.58275079999
[37m[1m[2023-06-25 03:11:35,254][129146] Min Reward on eval: -1597.4577317935182
[37m[1m[2023-06-25 03:11:35,254][129146] Mean Reward across all agents: -268.5017109859431
[37m[1m[2023-06-25 03:11:35,254][129146] Average Trajectory Length: 999.2883333333333
[36m[2023-06-25 03:11:35,262][129146] mean_value=-163.54061678057235, max_value=842.3846344942227
[37m[1m[2023-06-25 03:11:35,264][129146] New mean coefficients: [[ 0.8845296   3.540759    7.883872   -4.313246   -0.41701788]]
[37m[1m[2023-06-25 03:11:35,265][129146] Moving the mean solution point...
[36m[2023-06-25 03:11:45,117][129146] train() took 9.85 seconds to complete
[36m[2023-06-25 03:11:45,117][129146] FPS: 389838.36
[36m[2023-06-25 03:11:45,119][129146] itr=218, itrs=2000, Progress: 10.90%
[36m[2023-06-25 03:11:56,769][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 03:11:56,769][129146] FPS: 330035.37
[36m[2023-06-25 03:12:01,659][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:12:01,659][129146] Reward + Measures: [[213.04310319   0.99462998   0.95122427   0.98421293   0.00448833]]
[37m[1m[2023-06-25 03:12:01,659][129146] Max Reward on eval: 213.04310318759897
[37m[1m[2023-06-25 03:12:01,660][129146] Min Reward on eval: 213.04310318759897
[37m[1m[2023-06-25 03:12:01,660][129146] Mean Reward across all agents: 213.04310318759897
[37m[1m[2023-06-25 03:12:01,660][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:12:07,124][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:12:07,125][129146] Reward + Measures: [[-41.00613562   0.99360001   0.82089996   0.98250008   0.0043    ]
[37m[1m [107.21197828   0.93940002   0.79570001   0.89389992   0.0528    ]
[37m[1m [ 44.95716049   0.94460005   0.78660005   0.91149998   0.0358    ]
[37m[1m ...
[37m[1m [113.06959756   0.98629999   0.91090006   0.97169989   0.0141    ]
[37m[1m [174.97709884   0.639        0.2771       0.52570003   0.27509999]
[37m[1m [127.50714237   0.81899995   0.6498       0.76789999   0.1327    ]]
[37m[1m[2023-06-25 03:12:07,125][129146] Max Reward on eval: 313.43243163502774
[37m[1m[2023-06-25 03:12:07,125][129146] Min Reward on eval: -216.06143088511308
[37m[1m[2023-06-25 03:12:07,126][129146] Mean Reward across all agents: 68.90523773290283
[37m[1m[2023-06-25 03:12:07,126][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:12:07,131][129146] mean_value=52.15878592195924, max_value=693.4217807949753
[37m[1m[2023-06-25 03:12:07,134][129146] New mean coefficients: [[-0.5043802  3.9280732  8.799009  -5.913177   0.4135257]]
[37m[1m[2023-06-25 03:12:07,134][129146] Moving the mean solution point...
[36m[2023-06-25 03:12:17,000][129146] train() took 9.86 seconds to complete
[36m[2023-06-25 03:12:17,000][129146] FPS: 389309.38
[36m[2023-06-25 03:12:17,002][129146] itr=219, itrs=2000, Progress: 10.95%
[36m[2023-06-25 03:12:28,496][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 03:12:28,496][129146] FPS: 334530.71
[36m[2023-06-25 03:12:33,214][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:12:33,215][129146] Reward + Measures: [[170.77140055   0.99385864   0.95733267   0.98797965   0.00242233]]
[37m[1m[2023-06-25 03:12:33,215][129146] Max Reward on eval: 170.77140054684193
[37m[1m[2023-06-25 03:12:33,215][129146] Min Reward on eval: 170.77140054684193
[37m[1m[2023-06-25 03:12:33,215][129146] Mean Reward across all agents: 170.77140054684193
[37m[1m[2023-06-25 03:12:33,216][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:12:38,860][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:12:38,860][129146] Reward + Measures: [[ -47.89082461    0.83799994    0.4883        0.7931        0.1146    ]
[37m[1m [ -92.14377674    0.85540003    0.48879996    0.80470002    0.1178    ]
[37m[1m [ -62.42585662    0.94440001    0.72670007    0.92020005    0.0264    ]
[37m[1m ...
[37m[1m [ -39.71159957    0.58920002    0.33649999    0.43540001    0.21879999]
[37m[1m [-124.50704796    0.71560001    0.25130001    0.64029998    0.18710001]
[37m[1m [-229.47980375    0.71600002    0.23940001    0.69080007    0.0925    ]]
[37m[1m[2023-06-25 03:12:38,861][129146] Max Reward on eval: 282.43394599709865
[37m[1m[2023-06-25 03:12:38,861][129146] Min Reward on eval: -1079.8575343476027
[37m[1m[2023-06-25 03:12:38,861][129146] Mean Reward across all agents: -178.1097181225923
[37m[1m[2023-06-25 03:12:38,861][129146] Average Trajectory Length: 990.894
[36m[2023-06-25 03:12:38,867][129146] mean_value=-193.6625908052368, max_value=710.8804024585618
[37m[1m[2023-06-25 03:12:38,870][129146] New mean coefficients: [[-0.20600787  3.7192595   7.7025075  -6.6462097   2.0438514 ]]
[37m[1m[2023-06-25 03:12:38,871][129146] Moving the mean solution point...
[36m[2023-06-25 03:12:48,563][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 03:12:48,563][129146] FPS: 396281.96
[36m[2023-06-25 03:12:48,565][129146] itr=220, itrs=2000, Progress: 11.00%
[37m[1m[2023-06-25 03:12:51,347][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000200
[36m[2023-06-25 03:13:03,308][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 03:13:03,308][129146] FPS: 329835.68
[36m[2023-06-25 03:13:07,969][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:13:07,970][129146] Reward + Measures: [[147.6433875    0.9939943    0.95769435   0.98796004   0.00246233]]
[37m[1m[2023-06-25 03:13:07,970][129146] Max Reward on eval: 147.64338750250246
[37m[1m[2023-06-25 03:13:07,970][129146] Min Reward on eval: 147.64338750250246
[37m[1m[2023-06-25 03:13:07,971][129146] Mean Reward across all agents: 147.64338750250246
[37m[1m[2023-06-25 03:13:07,971][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:13:13,275][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:13:13,275][129146] Reward + Measures: [[ -99.70114101    0.33790001    0.35969999    0.1796        0.36030003]
[37m[1m [ 305.11236266    0.46679997    0.4152        0.2428        0.25830001]
[37m[1m [ -32.30967875    0.90380001    0.57240003    0.8538        0.0419    ]
[37m[1m ...
[37m[1m [ -10.72797854    0.77709997    0.51010001    0.69460005    0.0979    ]
[37m[1m [-115.93274058    0.4375        0.57919997    0.1628        0.50100005]
[37m[1m [-163.49483045    0.35600001    0.4664        0.15120001    0.3996    ]]
[37m[1m[2023-06-25 03:13:13,276][129146] Max Reward on eval: 333.34549449912737
[37m[1m[2023-06-25 03:13:13,276][129146] Min Reward on eval: -309.23032770393183
[37m[1m[2023-06-25 03:13:13,276][129146] Mean Reward across all agents: -48.317870186112685
[37m[1m[2023-06-25 03:13:13,276][129146] Average Trajectory Length: 995.511
[36m[2023-06-25 03:13:13,282][129146] mean_value=-149.93306007498822, max_value=678.8748824148613
[37m[1m[2023-06-25 03:13:13,285][129146] New mean coefficients: [[-0.2548849  3.194788   6.6704683 -6.319551   2.7147615]]
[37m[1m[2023-06-25 03:13:13,286][129146] Moving the mean solution point...
[36m[2023-06-25 03:13:23,047][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 03:13:23,047][129146] FPS: 393455.36
[36m[2023-06-25 03:13:23,050][129146] itr=221, itrs=2000, Progress: 11.05%
[36m[2023-06-25 03:13:34,657][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 03:13:34,657][129146] FPS: 331269.57
[36m[2023-06-25 03:13:39,551][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:13:39,551][129146] Reward + Measures: [[137.57958433   0.99577302   0.95021933   0.98732001   0.00513133]]
[37m[1m[2023-06-25 03:13:39,551][129146] Max Reward on eval: 137.57958432847545
[37m[1m[2023-06-25 03:13:39,552][129146] Min Reward on eval: 137.57958432847545
[37m[1m[2023-06-25 03:13:39,552][129146] Mean Reward across all agents: 137.57958432847545
[37m[1m[2023-06-25 03:13:39,552][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:13:44,980][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:13:44,981][129146] Reward + Measures: [[-54.36045619   0.93269998   0.44959998   0.88120002   0.0535    ]
[37m[1m [ 95.94248524   0.88640004   0.55150002   0.815        0.05950001]
[37m[1m [ 53.71909254   0.98410004   0.79030001   0.96660006   0.0099    ]
[37m[1m ...
[37m[1m [264.72469134   0.74529994   0.26020002   0.6207       0.1573    ]
[37m[1m [229.32407926   0.93409997   0.83500004   0.88880008   0.0368    ]
[37m[1m [ 77.90675051   0.87709999   0.53870004   0.8113001    0.0681    ]]
[37m[1m[2023-06-25 03:13:44,981][129146] Max Reward on eval: 677.4663496312569
[37m[1m[2023-06-25 03:13:44,981][129146] Min Reward on eval: -131.1284503830888
[37m[1m[2023-06-25 03:13:44,981][129146] Mean Reward across all agents: 94.58064818948128
[37m[1m[2023-06-25 03:13:44,982][129146] Average Trajectory Length: 999.831
[36m[2023-06-25 03:13:44,987][129146] mean_value=64.21165698460477, max_value=830.8305912222075
[37m[1m[2023-06-25 03:13:44,990][129146] New mean coefficients: [[ 0.2627179  2.6270401  7.2467875 -8.248289   3.0502164]]
[37m[1m[2023-06-25 03:13:44,991][129146] Moving the mean solution point...
[36m[2023-06-25 03:13:54,827][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 03:13:54,827][129146] FPS: 390468.62
[36m[2023-06-25 03:13:54,829][129146] itr=222, itrs=2000, Progress: 11.10%
[36m[2023-06-25 03:14:06,307][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 03:14:06,307][129146] FPS: 335033.64
[36m[2023-06-25 03:14:11,157][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:14:11,157][129146] Reward + Measures: [[110.04461924   0.99470538   0.95569533   0.98811769   0.00350333]]
[37m[1m[2023-06-25 03:14:11,158][129146] Max Reward on eval: 110.04461923531943
[37m[1m[2023-06-25 03:14:11,158][129146] Min Reward on eval: 110.04461923531943
[37m[1m[2023-06-25 03:14:11,158][129146] Mean Reward across all agents: 110.04461923531943
[37m[1m[2023-06-25 03:14:11,158][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:14:16,817][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:14:16,818][129146] Reward + Measures: [[-144.2666263     0.92500001    0.57000005    0.92720002    0.0132    ]
[37m[1m [-195.8588192     0.91370004    0.67480004    0.89039993    0.0464    ]
[37m[1m [ -63.41450432    0.98880005    0.75459999    0.98380005    0.0004    ]
[37m[1m ...
[37m[1m [ -81.97296127    0.60420001    0.56730002    0.52039999    0.40089998]
[37m[1m [-164.49220685    0.9497        0.61160004    0.94939995    0.0084    ]
[37m[1m [-177.44876584    0.9285        0.5643        0.93599999    0.0081    ]]
[37m[1m[2023-06-25 03:14:16,818][129146] Max Reward on eval: 333.63848238732317
[37m[1m[2023-06-25 03:14:16,818][129146] Min Reward on eval: -344.00826952509817
[37m[1m[2023-06-25 03:14:16,819][129146] Mean Reward across all agents: -77.69053246539539
[37m[1m[2023-06-25 03:14:16,819][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:14:16,825][129146] mean_value=-17.212982961323725, max_value=546.0442342563008
[37m[1m[2023-06-25 03:14:16,828][129146] New mean coefficients: [[ 0.9606888  2.1227207  9.300817  -7.3378305  2.515786 ]]
[37m[1m[2023-06-25 03:14:16,829][129146] Moving the mean solution point...
[36m[2023-06-25 03:14:26,612][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 03:14:26,612][129146] FPS: 392584.58
[36m[2023-06-25 03:14:26,615][129146] itr=223, itrs=2000, Progress: 11.15%
[36m[2023-06-25 03:14:38,031][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 03:14:38,031][129146] FPS: 336778.26
[36m[2023-06-25 03:14:42,926][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:14:42,926][129146] Reward + Measures: [[120.25854017   0.9958244    0.96184397   0.98879391   0.002306  ]]
[37m[1m[2023-06-25 03:14:42,926][129146] Max Reward on eval: 120.25854016987728
[37m[1m[2023-06-25 03:14:42,927][129146] Min Reward on eval: 120.25854016987728
[37m[1m[2023-06-25 03:14:42,927][129146] Mean Reward across all agents: 120.25854016987728
[37m[1m[2023-06-25 03:14:42,927][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:14:48,448][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:14:48,449][129146] Reward + Measures: [[-258.60183239    0.97210008    0.78089994    0.95640004    0.0188    ]
[37m[1m [-102.61554746    0.89829999    0.49509999    0.84849995    0.1086    ]
[37m[1m [ -96.10258087    0.477         0.40790001    0.2631        0.33220002]
[37m[1m ...
[37m[1m [-213.8629692     0.98850006    0.92989999    0.98460001    0.0028    ]
[37m[1m [-196.59433729    0.88259995    0.49910003    0.83280003    0.0732    ]
[37m[1m [ 135.59032106    0.49380001    0.46269998    0.2773        0.36489999]]
[37m[1m[2023-06-25 03:14:48,449][129146] Max Reward on eval: 337.47632811205114
[37m[1m[2023-06-25 03:14:48,449][129146] Min Reward on eval: -409.82974952888907
[37m[1m[2023-06-25 03:14:48,450][129146] Mean Reward across all agents: -126.03317928582231
[37m[1m[2023-06-25 03:14:48,450][129146] Average Trajectory Length: 999.644
[36m[2023-06-25 03:14:48,455][129146] mean_value=-154.0138678669329, max_value=747.2034661990101
[37m[1m[2023-06-25 03:14:48,457][129146] New mean coefficients: [[ 1.3970447  3.263952  13.305483  -5.909341   1.7487336]]
[37m[1m[2023-06-25 03:14:48,458][129146] Moving the mean solution point...
[36m[2023-06-25 03:14:58,340][129146] train() took 9.88 seconds to complete
[36m[2023-06-25 03:14:58,340][129146] FPS: 388658.03
[36m[2023-06-25 03:14:58,343][129146] itr=224, itrs=2000, Progress: 11.20%
[36m[2023-06-25 03:15:10,044][129146] train() took 11.69 seconds to complete
[36m[2023-06-25 03:15:10,045][129146] FPS: 328615.11
[36m[2023-06-25 03:15:14,838][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:15:14,839][129146] Reward + Measures: [[127.21701554   0.996916     0.96585733   0.99022132   0.00133267]]
[37m[1m[2023-06-25 03:15:14,839][129146] Max Reward on eval: 127.21701553617002
[37m[1m[2023-06-25 03:15:14,839][129146] Min Reward on eval: 127.21701553617002
[37m[1m[2023-06-25 03:15:14,840][129146] Mean Reward across all agents: 127.21701553617002
[37m[1m[2023-06-25 03:15:14,840][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:15:20,285][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:15:20,286][129146] Reward + Measures: [[-253.82849215    0.85720009    0.169         0.77790004    0.2278    ]
[37m[1m [ -21.81022733    0.53450006    0.42320004    0.39000002    0.3335    ]
[37m[1m [ -88.20125006    0.68100005    0.3222        0.53369999    0.28980002]
[37m[1m ...
[37m[1m [-135.53887116    0.78390008    0.22280002    0.71350002    0.27400002]
[37m[1m [ -55.44555369    0.52819997    0.28079998    0.41669998    0.29060003]
[37m[1m [ -88.84985957    0.48109999    0.26100001    0.42460003    0.29300004]]
[37m[1m[2023-06-25 03:15:20,286][129146] Max Reward on eval: 154.47370421797967
[37m[1m[2023-06-25 03:15:20,286][129146] Min Reward on eval: -581.4382413533807
[37m[1m[2023-06-25 03:15:20,287][129146] Mean Reward across all agents: -157.15097080789894
[37m[1m[2023-06-25 03:15:20,287][129146] Average Trajectory Length: 994.3296666666666
[36m[2023-06-25 03:15:20,290][129146] mean_value=-304.6222654408317, max_value=392.39843696563037
[37m[1m[2023-06-25 03:15:20,293][129146] New mean coefficients: [[ 0.08237016  3.054263   14.028026   -6.129302    1.7061611 ]]
[37m[1m[2023-06-25 03:15:20,294][129146] Moving the mean solution point...
[36m[2023-06-25 03:15:30,130][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 03:15:30,130][129146] FPS: 390468.33
[36m[2023-06-25 03:15:30,132][129146] itr=225, itrs=2000, Progress: 11.25%
[36m[2023-06-25 03:15:41,632][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 03:15:41,633][129146] FPS: 334374.75
[36m[2023-06-25 03:15:46,497][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:15:46,497][129146] Reward + Measures: [[96.67633505  0.99728596  0.96701401  0.99052238  0.00090033]]
[37m[1m[2023-06-25 03:15:46,497][129146] Max Reward on eval: 96.67633504539393
[37m[1m[2023-06-25 03:15:46,498][129146] Min Reward on eval: 96.67633504539393
[37m[1m[2023-06-25 03:15:46,498][129146] Mean Reward across all agents: 96.67633504539393
[37m[1m[2023-06-25 03:15:46,498][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:15:52,024][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:15:52,024][129146] Reward + Measures: [[  17.22688111    0.67720002    0.34680003    0.51640004    0.1734    ]
[37m[1m [  15.27621457    0.75279999    0.27830002    0.61210001    0.27790001]
[37m[1m [ -75.54694343    0.85159999    0.41160002    0.78190011    0.087     ]
[37m[1m ...
[37m[1m [ -11.59088944    0.98729992    0.8294        0.98040003    0.0056    ]
[37m[1m [  91.03188801    0.59310001    0.34769997    0.43530002    0.20999999]
[37m[1m [-129.31566295    0.87760001    0.20720001    0.78580004    0.16669999]]
[37m[1m[2023-06-25 03:15:52,024][129146] Max Reward on eval: 347.9187462153262
[37m[1m[2023-06-25 03:15:52,025][129146] Min Reward on eval: -293.35933712924015
[37m[1m[2023-06-25 03:15:52,025][129146] Mean Reward across all agents: -63.76378897795993
[37m[1m[2023-06-25 03:15:52,025][129146] Average Trajectory Length: 999.8643333333333
[36m[2023-06-25 03:15:52,028][129146] mean_value=-136.30246157176362, max_value=734.9731637996854
[37m[1m[2023-06-25 03:15:52,031][129146] New mean coefficients: [[ 0.41102216  0.8074343  15.586593   -4.1256714   0.66099346]]
[37m[1m[2023-06-25 03:15:52,032][129146] Moving the mean solution point...
[36m[2023-06-25 03:16:01,757][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 03:16:01,758][129146] FPS: 394901.30
[36m[2023-06-25 03:16:01,760][129146] itr=226, itrs=2000, Progress: 11.30%
[36m[2023-06-25 03:16:13,219][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 03:16:13,219][129146] FPS: 335510.66
[36m[2023-06-25 03:16:17,943][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:16:17,943][129146] Reward + Measures: [[62.99756323  0.99392396  0.95920432  0.98841465  0.00226233]]
[37m[1m[2023-06-25 03:16:17,944][129146] Max Reward on eval: 62.99756323020686
[37m[1m[2023-06-25 03:16:17,944][129146] Min Reward on eval: 62.99756323020686
[37m[1m[2023-06-25 03:16:17,944][129146] Mean Reward across all agents: 62.99756323020686
[37m[1m[2023-06-25 03:16:17,944][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:16:23,418][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:16:23,419][129146] Reward + Measures: [[173.01158462   0.45370004   0.49659997   0.2105       0.37960002]
[37m[1m [251.24719466   0.48459998   0.46639997   0.303        0.39340001]
[37m[1m [120.26631107   0.50730002   0.4226       0.30870003   0.29660001]
[37m[1m ...
[37m[1m [-36.63882707   0.82579994   0.24029998   0.71929997   0.22410002]
[37m[1m [146.55565242   0.3867       0.57110006   0.1469       0.38590002]
[37m[1m [316.90574841   0.57840002   0.39269999   0.37349999   0.36629999]]
[37m[1m[2023-06-25 03:16:23,419][129146] Max Reward on eval: 498.5886689394247
[37m[1m[2023-06-25 03:16:23,419][129146] Min Reward on eval: -483.1588899405906
[37m[1m[2023-06-25 03:16:23,420][129146] Mean Reward across all agents: 201.74373025074664
[37m[1m[2023-06-25 03:16:23,420][129146] Average Trajectory Length: 999.7786666666666
[36m[2023-06-25 03:16:23,427][129146] mean_value=211.8808448216302, max_value=783.2238631045213
[37m[1m[2023-06-25 03:16:23,429][129146] New mean coefficients: [[ 0.8356723   0.12986487 15.905427   -4.290305    0.49462208]]
[37m[1m[2023-06-25 03:16:23,430][129146] Moving the mean solution point...
[36m[2023-06-25 03:16:33,125][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 03:16:33,125][129146] FPS: 396162.79
[36m[2023-06-25 03:16:33,127][129146] itr=227, itrs=2000, Progress: 11.35%
[36m[2023-06-25 03:16:44,554][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 03:16:44,554][129146] FPS: 336498.48
[36m[2023-06-25 03:16:49,332][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:16:49,333][129146] Reward + Measures: [[81.45258257  0.99361002  0.96218634  0.98845732  0.00221433]]
[37m[1m[2023-06-25 03:16:49,333][129146] Max Reward on eval: 81.4525825698743
[37m[1m[2023-06-25 03:16:49,333][129146] Min Reward on eval: 81.4525825698743
[37m[1m[2023-06-25 03:16:49,334][129146] Mean Reward across all agents: 81.4525825698743
[37m[1m[2023-06-25 03:16:49,334][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:16:54,857][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:16:54,858][129146] Reward + Measures: [[ -87.27030312    0.51980001    0.32479998    0.3962        0.30340001]
[37m[1m [ -76.99965068    0.64659995    0.26209998    0.54529995    0.2586    ]
[37m[1m [-128.4483995     0.3292        0.48629999    0.16970001    0.37629995]
[37m[1m ...
[37m[1m [ 173.52966477    0.48610002    0.36660001    0.28120002    0.28760001]
[37m[1m [  -7.10122375    0.68909997    0.2969        0.61620009    0.29389998]
[37m[1m [-126.02894859    0.54510003    0.3231        0.3626        0.30090001]]
[37m[1m[2023-06-25 03:16:54,858][129146] Max Reward on eval: 386.592181109995
[37m[1m[2023-06-25 03:16:54,858][129146] Min Reward on eval: -527.2653506290866
[37m[1m[2023-06-25 03:16:54,859][129146] Mean Reward across all agents: -14.234857060997692
[37m[1m[2023-06-25 03:16:54,859][129146] Average Trajectory Length: 999.476
[36m[2023-06-25 03:16:54,863][129146] mean_value=-161.97536023297465, max_value=567.3345082497518
[37m[1m[2023-06-25 03:16:54,865][129146] New mean coefficients: [[ 1.0160829  -0.91064984 14.539342   -3.0525382   2.3705502 ]]
[37m[1m[2023-06-25 03:16:54,866][129146] Moving the mean solution point...
[36m[2023-06-25 03:17:04,633][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 03:17:04,633][129146] FPS: 393235.10
[36m[2023-06-25 03:17:04,636][129146] itr=228, itrs=2000, Progress: 11.40%
[36m[2023-06-25 03:17:16,045][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 03:17:16,045][129146] FPS: 336992.99
[36m[2023-06-25 03:17:20,928][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:17:20,929][129146] Reward + Measures: [[97.96394716  0.99517173  0.96643424  0.98954201  0.001293  ]]
[37m[1m[2023-06-25 03:17:20,929][129146] Max Reward on eval: 97.96394715648105
[37m[1m[2023-06-25 03:17:20,929][129146] Min Reward on eval: 97.96394715648105
[37m[1m[2023-06-25 03:17:20,930][129146] Mean Reward across all agents: 97.96394715648105
[37m[1m[2023-06-25 03:17:20,930][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:17:26,462][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:17:26,463][129146] Reward + Measures: [[ -83.47602036    0.89600009    0.43169999    0.8495        0.07380001]
[37m[1m [-115.27573941    0.84289998    0.2974        0.75239998    0.1789    ]
[37m[1m [  61.71057077    0.46040002    0.36200002    0.29730001    0.28830001]
[37m[1m ...
[37m[1m [  63.39368105    0.78389996    0.3048        0.6322        0.1425    ]
[37m[1m [  59.04892846    0.5442        0.3831        0.38339999    0.26250002]
[37m[1m [ 112.03613046    0.61220002    0.48470002    0.49460003    0.2622    ]]
[37m[1m[2023-06-25 03:17:26,463][129146] Max Reward on eval: 489.1879150076944
[37m[1m[2023-06-25 03:17:26,463][129146] Min Reward on eval: -267.11924732961927
[37m[1m[2023-06-25 03:17:26,463][129146] Mean Reward across all agents: 34.03427367779884
[37m[1m[2023-06-25 03:17:26,464][129146] Average Trajectory Length: 999.506
[36m[2023-06-25 03:17:26,468][129146] mean_value=-91.54992575995902, max_value=577.5997391345841
[37m[1m[2023-06-25 03:17:26,471][129146] New mean coefficients: [[ 0.98346704 -0.94226325 13.08759    -5.107806    2.1802328 ]]
[37m[1m[2023-06-25 03:17:26,472][129146] Moving the mean solution point...
[36m[2023-06-25 03:17:36,249][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 03:17:36,249][129146] FPS: 392810.51
[36m[2023-06-25 03:17:36,251][129146] itr=229, itrs=2000, Progress: 11.45%
[36m[2023-06-25 03:17:47,771][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 03:17:47,771][129146] FPS: 333761.92
[36m[2023-06-25 03:17:52,578][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:17:52,578][129146] Reward + Measures: [[88.11797943  0.99514067  0.96497202  0.98896098  0.00176667]]
[37m[1m[2023-06-25 03:17:52,579][129146] Max Reward on eval: 88.11797942994397
[37m[1m[2023-06-25 03:17:52,579][129146] Min Reward on eval: 88.11797942994397
[37m[1m[2023-06-25 03:17:52,579][129146] Mean Reward across all agents: 88.11797942994397
[37m[1m[2023-06-25 03:17:52,579][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:17:57,991][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:17:57,991][129146] Reward + Measures: [[-313.50110326    0.97550005    0.45280004    0.949         0.0078    ]
[37m[1m [ -86.74319907    0.98269999    0.69629997    0.96740001    0.0042    ]
[37m[1m [-190.59529014    0.99110001    0.94880009    0.98950005    0.0007    ]
[37m[1m ...
[37m[1m [-276.81776626    0.9898001     0.81400007    0.98780006    0.0007    ]
[37m[1m [-223.21283694    0.90780002    0.46849999    0.88690007    0.0771    ]
[37m[1m [-145.91941166    0.98009998    0.63009995    0.96190006    0.0082    ]]
[37m[1m[2023-06-25 03:17:57,991][129146] Max Reward on eval: 99.1618902259972
[37m[1m[2023-06-25 03:17:57,992][129146] Min Reward on eval: -558.3127821570495
[37m[1m[2023-06-25 03:17:57,992][129146] Mean Reward across all agents: -255.33344393474093
[37m[1m[2023-06-25 03:17:57,992][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:17:57,995][129146] mean_value=-229.36287987863884, max_value=347.42062230104347
[37m[1m[2023-06-25 03:17:57,997][129146] New mean coefficients: [[ 1.2556126   0.08424807 14.733496   -5.6636143   2.2713754 ]]
[37m[1m[2023-06-25 03:17:57,998][129146] Moving the mean solution point...
[36m[2023-06-25 03:18:07,701][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 03:18:07,701][129146] FPS: 395831.79
[36m[2023-06-25 03:18:07,703][129146] itr=230, itrs=2000, Progress: 11.50%
[37m[1m[2023-06-25 03:18:10,610][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000210
[36m[2023-06-25 03:18:22,594][129146] train() took 11.66 seconds to complete
[36m[2023-06-25 03:18:22,594][129146] FPS: 329379.31
[36m[2023-06-25 03:18:27,404][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:18:27,404][129146] Reward + Measures: [[141.67316316   0.99572372   0.96926999   0.99000031   0.00118367]]
[37m[1m[2023-06-25 03:18:27,405][129146] Max Reward on eval: 141.67316316417984
[37m[1m[2023-06-25 03:18:27,405][129146] Min Reward on eval: 141.67316316417984
[37m[1m[2023-06-25 03:18:27,405][129146] Mean Reward across all agents: 141.67316316417984
[37m[1m[2023-06-25 03:18:27,406][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:18:33,103][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:18:33,104][129146] Reward + Measures: [[-246.49090276    0.90009993    0.24969999    0.82849997    0.1392    ]
[37m[1m [-234.94288445    0.95859998    0.62400001    0.95190001    0.0192    ]
[37m[1m [-113.0362214     0.99300003    0.94659996    0.98929995    0.0024    ]
[37m[1m ...
[37m[1m [-253.65635392    0.91140002    0.60589999    0.90300006    0.05610001]
[37m[1m [ -53.67701785    0.33939999    0.51780003    0.33080003    0.45570001]
[37m[1m [-133.72364962    0.91390002    0.391         0.89449996    0.0641    ]]
[37m[1m[2023-06-25 03:18:33,104][129146] Max Reward on eval: 80.69856678994839
[37m[1m[2023-06-25 03:18:33,104][129146] Min Reward on eval: -352.2218660808518
[37m[1m[2023-06-25 03:18:33,104][129146] Mean Reward across all agents: -157.2641837456265
[37m[1m[2023-06-25 03:18:33,105][129146] Average Trajectory Length: 999.4879999999999
[36m[2023-06-25 03:18:33,109][129146] mean_value=-136.96338260045047, max_value=515.4894674013765
[37m[1m[2023-06-25 03:18:33,112][129146] New mean coefficients: [[ 1.4762075 -2.2153344 16.799616  -4.8145905  3.5203   ]]
[37m[1m[2023-06-25 03:18:33,112][129146] Moving the mean solution point...
[36m[2023-06-25 03:18:42,802][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 03:18:42,802][129146] FPS: 396370.64
[36m[2023-06-25 03:18:42,805][129146] itr=231, itrs=2000, Progress: 11.55%
[36m[2023-06-25 03:18:54,320][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 03:18:54,320][129146] FPS: 333903.32
[36m[2023-06-25 03:18:59,064][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:18:59,064][129146] Reward + Measures: [[181.93964581   0.99142635   0.95160866   0.98271435   0.00310667]]
[37m[1m[2023-06-25 03:18:59,064][129146] Max Reward on eval: 181.93964580508054
[37m[1m[2023-06-25 03:18:59,064][129146] Min Reward on eval: 181.93964580508054
[37m[1m[2023-06-25 03:18:59,064][129146] Mean Reward across all agents: 181.93964580508054
[37m[1m[2023-06-25 03:18:59,065][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:19:04,427][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:19:04,427][129146] Reward + Measures: [[143.84768977   0.41770002   0.35420001   0.32319999   0.27019998]
[37m[1m [  5.07645635   0.2509       0.41240001   0.14289999   0.33070001]
[37m[1m [276.04759566   0.41430002   0.583        0.39250001   0.4443    ]
[37m[1m ...
[37m[1m [270.07992602   0.46420002   0.43070003   0.26069999   0.2633    ]
[37m[1m [-93.00847789   0.28670001   0.50629997   0.17690001   0.36820003]
[37m[1m [468.01640329   0.47600004   0.49470004   0.25740001   0.27959999]]
[37m[1m[2023-06-25 03:19:04,428][129146] Max Reward on eval: 593.9568954301765
[37m[1m[2023-06-25 03:19:04,428][129146] Min Reward on eval: -509.01507938703287
[37m[1m[2023-06-25 03:19:04,428][129146] Mean Reward across all agents: 121.86319109649395
[37m[1m[2023-06-25 03:19:04,428][129146] Average Trajectory Length: 999.554
[36m[2023-06-25 03:19:04,440][129146] mean_value=217.30560239355665, max_value=1093.9568954301765
[37m[1m[2023-06-25 03:19:04,443][129146] New mean coefficients: [[ 1.1814071 -2.5031106 12.918187  -6.5061626  4.4071302]]
[37m[1m[2023-06-25 03:19:04,443][129146] Moving the mean solution point...
[36m[2023-06-25 03:19:14,107][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 03:19:14,107][129146] FPS: 397443.42
[36m[2023-06-25 03:19:14,110][129146] itr=232, itrs=2000, Progress: 11.60%
[36m[2023-06-25 03:19:25,567][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 03:19:25,567][129146] FPS: 335610.37
[36m[2023-06-25 03:19:30,429][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:19:30,429][129146] Reward + Measures: [[192.59763932   0.98992628   0.94998097   0.98104596   0.00391967]]
[37m[1m[2023-06-25 03:19:30,430][129146] Max Reward on eval: 192.5976393194565
[37m[1m[2023-06-25 03:19:30,430][129146] Min Reward on eval: 192.5976393194565
[37m[1m[2023-06-25 03:19:30,430][129146] Mean Reward across all agents: 192.5976393194565
[37m[1m[2023-06-25 03:19:30,430][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:19:35,859][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:19:35,859][129146] Reward + Measures: [[180.79000719   0.27809998   0.39910004   0.16239999   0.27939999]
[37m[1m [704.94589225   0.40079999   0.52360004   0.14659999   0.2854    ]
[37m[1m [ 69.56454606   0.35680002   0.52860004   0.07560001   0.255     ]
[37m[1m ...
[37m[1m [350.6364899    0.32230002   0.54699999   0.1592       0.38519999]
[37m[1m [570.3030281    0.40180001   0.4488       0.21269999   0.30140001]
[37m[1m [305.79569499   0.33919999   0.37920004   0.2419       0.33070001]]
[37m[1m[2023-06-25 03:19:35,859][129146] Max Reward on eval: 973.9250982735073
[37m[1m[2023-06-25 03:19:35,860][129146] Min Reward on eval: -147.88035502317362
[37m[1m[2023-06-25 03:19:35,860][129146] Mean Reward across all agents: 338.6484436778794
[37m[1m[2023-06-25 03:19:35,860][129146] Average Trajectory Length: 999.5086666666666
[36m[2023-06-25 03:19:35,867][129146] mean_value=295.50447288572155, max_value=1473.9250982735073
[37m[1m[2023-06-25 03:19:35,870][129146] New mean coefficients: [[ 0.9977237 -3.994305   8.749414  -7.4066534  4.0104184]]
[37m[1m[2023-06-25 03:19:35,871][129146] Moving the mean solution point...
[36m[2023-06-25 03:19:45,483][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 03:19:45,484][129146] FPS: 399533.74
[36m[2023-06-25 03:19:45,486][129146] itr=233, itrs=2000, Progress: 11.65%
[36m[2023-06-25 03:19:56,924][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 03:19:56,924][129146] FPS: 336135.32
[36m[2023-06-25 03:20:01,790][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:20:01,791][129146] Reward + Measures: [[184.91546681   0.99056071   0.95094866   0.9813146    0.00370467]]
[37m[1m[2023-06-25 03:20:01,791][129146] Max Reward on eval: 184.91546680652516
[37m[1m[2023-06-25 03:20:01,791][129146] Min Reward on eval: 184.91546680652516
[37m[1m[2023-06-25 03:20:01,791][129146] Mean Reward across all agents: 184.91546680652516
[37m[1m[2023-06-25 03:20:01,791][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:20:07,335][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:20:07,335][129146] Reward + Measures: [[479.63055037   0.40349999   0.48740003   0.22239999   0.31760001]
[37m[1m [537.41364333   0.43560001   0.4664       0.24590002   0.25279999]
[37m[1m [257.76211024   0.6225       0.45790002   0.47330004   0.1364    ]
[37m[1m ...
[37m[1m [533.56464708   0.33829999   0.53549999   0.11490001   0.3193    ]
[37m[1m [307.10078887   0.69620001   0.59880006   0.64029998   0.16610001]
[37m[1m [188.81454388   0.31479999   0.47350001   0.17119999   0.32260001]]
[37m[1m[2023-06-25 03:20:07,336][129146] Max Reward on eval: 951.4604552854668
[37m[1m[2023-06-25 03:20:07,336][129146] Min Reward on eval: -144.92546665718547
[37m[1m[2023-06-25 03:20:07,336][129146] Mean Reward across all agents: 436.4671427960195
[37m[1m[2023-06-25 03:20:07,336][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:20:07,347][129146] mean_value=378.8992529488101, max_value=1279.7549494297243
[37m[1m[2023-06-25 03:20:07,350][129146] New mean coefficients: [[ 1.2356384 -3.0316622  8.324373  -5.70359    3.229487 ]]
[37m[1m[2023-06-25 03:20:07,351][129146] Moving the mean solution point...
[36m[2023-06-25 03:20:17,030][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 03:20:17,030][129146] FPS: 396825.59
[36m[2023-06-25 03:20:17,032][129146] itr=234, itrs=2000, Progress: 11.70%
[36m[2023-06-25 03:20:28,468][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 03:20:28,468][129146] FPS: 336265.20
[36m[2023-06-25 03:20:33,275][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:20:33,275][129146] Reward + Measures: [[242.37788114   0.98996025   0.95482159   0.98134959   0.00281467]]
[37m[1m[2023-06-25 03:20:33,275][129146] Max Reward on eval: 242.3778811420989
[37m[1m[2023-06-25 03:20:33,275][129146] Min Reward on eval: 242.3778811420989
[37m[1m[2023-06-25 03:20:33,275][129146] Mean Reward across all agents: 242.3778811420989
[37m[1m[2023-06-25 03:20:33,276][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:20:38,915][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:20:38,916][129146] Reward + Measures: [[667.51455117   0.41529998   0.44889998   0.17400001   0.20330003]
[37m[1m [589.69472116   0.55790007   0.47059998   0.31070003   0.17389999]
[37m[1m [702.14214292   0.39250001   0.4962       0.1171       0.33790001]
[37m[1m ...
[37m[1m [513.73977155   0.61199999   0.52270001   0.40279999   0.1508    ]
[37m[1m [184.57927392   0.35069999   0.62410009   0.26500002   0.43270001]
[37m[1m [240.00896696   0.40829998   0.52010006   0.1909       0.29789999]]
[37m[1m[2023-06-25 03:20:38,916][129146] Max Reward on eval: 976.0258130244795
[37m[1m[2023-06-25 03:20:38,916][129146] Min Reward on eval: -159.10048380801453
[37m[1m[2023-06-25 03:20:38,916][129146] Mean Reward across all agents: 441.0956079724824
[37m[1m[2023-06-25 03:20:38,917][129146] Average Trajectory Length: 999.8439999999999
[36m[2023-06-25 03:20:38,926][129146] mean_value=414.99206200294276, max_value=1209.23907956013
[37m[1m[2023-06-25 03:20:38,929][129146] New mean coefficients: [[ 1.5853919 -1.0466202  7.484652  -6.137262   3.2250133]]
[37m[1m[2023-06-25 03:20:38,930][129146] Moving the mean solution point...
[36m[2023-06-25 03:20:48,737][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 03:20:48,737][129146] FPS: 391632.88
[36m[2023-06-25 03:20:48,739][129146] itr=235, itrs=2000, Progress: 11.75%
[36m[2023-06-25 03:21:00,308][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 03:21:00,309][129146] FPS: 332375.14
[36m[2023-06-25 03:21:05,114][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:21:05,114][129146] Reward + Measures: [[277.22314875   0.98990232   0.95597267   0.98220962   0.00246933]]
[37m[1m[2023-06-25 03:21:05,115][129146] Max Reward on eval: 277.2231487472685
[37m[1m[2023-06-25 03:21:05,115][129146] Min Reward on eval: 277.2231487472685
[37m[1m[2023-06-25 03:21:05,115][129146] Mean Reward across all agents: 277.2231487472685
[37m[1m[2023-06-25 03:21:05,115][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:21:10,764][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:21:10,765][129146] Reward + Measures: [[360.08609415   0.38069999   0.58529997   0.0497       0.4131    ]
[37m[1m [233.52427624   0.9016       0.73820001   0.86160004   0.0859    ]
[37m[1m [303.28307995   0.49600002   0.67160004   0.0969       0.51279998]
[37m[1m ...
[37m[1m [270.19034871   0.36950001   0.55129999   0.1058       0.4463    ]
[37m[1m [732.44949612   0.3732       0.51739997   0.10220001   0.32570001]
[37m[1m [438.38094383   0.36919999   0.55680001   0.1517       0.32390001]]
[37m[1m[2023-06-25 03:21:10,765][129146] Max Reward on eval: 855.7809279582725
[37m[1m[2023-06-25 03:21:10,765][129146] Min Reward on eval: -181.44840062358418
[37m[1m[2023-06-25 03:21:10,766][129146] Mean Reward across all agents: 423.76438539774045
[37m[1m[2023-06-25 03:21:10,766][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:21:10,775][129146] mean_value=360.31688975789797, max_value=1149.124220220337
[37m[1m[2023-06-25 03:21:10,778][129146] New mean coefficients: [[ 1.2187514 -1.3186576  7.573478  -6.1990843  3.2443097]]
[37m[1m[2023-06-25 03:21:10,779][129146] Moving the mean solution point...
[36m[2023-06-25 03:21:20,577][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 03:21:20,577][129146] FPS: 391978.59
[36m[2023-06-25 03:21:20,579][129146] itr=236, itrs=2000, Progress: 11.80%
[36m[2023-06-25 03:21:32,126][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 03:21:32,126][129146] FPS: 332976.18
[36m[2023-06-25 03:21:37,011][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:21:37,012][129146] Reward + Measures: [[308.92482231   0.98689461   0.95473796   0.979276     0.00330967]]
[37m[1m[2023-06-25 03:21:37,012][129146] Max Reward on eval: 308.92482231149427
[37m[1m[2023-06-25 03:21:37,012][129146] Min Reward on eval: 308.92482231149427
[37m[1m[2023-06-25 03:21:37,013][129146] Mean Reward across all agents: 308.92482231149427
[37m[1m[2023-06-25 03:21:37,013][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:21:42,510][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:21:42,510][129146] Reward + Measures: [[286.64430003   0.21729998   0.71320003   0.21870001   0.61830002]
[37m[1m [524.55378025   0.29570001   0.47679996   0.0839       0.35600001]
[37m[1m [472.57959537   0.29350001   0.52100003   0.07690001   0.34230003]
[37m[1m ...
[37m[1m [461.82601173   0.27089998   0.57950002   0.1279       0.40790001]
[37m[1m [492.67645205   0.3062       0.5097       0.07790001   0.39000002]
[37m[1m [249.10832647   0.23360001   0.68510002   0.18500002   0.63      ]]
[37m[1m[2023-06-25 03:21:42,510][129146] Max Reward on eval: 826.4150806894643
[37m[1m[2023-06-25 03:21:42,511][129146] Min Reward on eval: -134.00863056572854
[37m[1m[2023-06-25 03:21:42,511][129146] Mean Reward across all agents: 308.9088845935824
[37m[1m[2023-06-25 03:21:42,511][129146] Average Trajectory Length: 999.6719999999999
[36m[2023-06-25 03:21:42,521][129146] mean_value=314.2088445657681, max_value=1112.8345683753025
[37m[1m[2023-06-25 03:21:42,524][129146] New mean coefficients: [[ 1.5431417 -2.676608   6.483124  -4.307754   4.656621 ]]
[37m[1m[2023-06-25 03:21:42,525][129146] Moving the mean solution point...
[36m[2023-06-25 03:21:52,463][129146] train() took 9.94 seconds to complete
[36m[2023-06-25 03:21:52,464][129146] FPS: 386449.38
[36m[2023-06-25 03:21:52,466][129146] itr=237, itrs=2000, Progress: 11.85%
[36m[2023-06-25 03:22:03,957][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 03:22:03,957][129146] FPS: 334660.70
[36m[2023-06-25 03:22:08,705][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:22:08,706][129146] Reward + Measures: [[329.49390548   0.97809958   0.94627702   0.96634138   0.008529  ]]
[37m[1m[2023-06-25 03:22:08,706][129146] Max Reward on eval: 329.49390547560336
[37m[1m[2023-06-25 03:22:08,706][129146] Min Reward on eval: 329.49390547560336
[37m[1m[2023-06-25 03:22:08,707][129146] Mean Reward across all agents: 329.49390547560336
[37m[1m[2023-06-25 03:22:08,707][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:22:14,190][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:22:14,191][129146] Reward + Measures: [[388.92589377   0.28029999   0.50940001   0.19149999   0.38669997]
[37m[1m [677.49672919   0.37639999   0.47170001   0.2217       0.3802    ]
[37m[1m [476.57767109   0.29790002   0.47989997   0.21370001   0.35380003]
[37m[1m ...
[37m[1m [876.6380486    0.40079999   0.41820002   0.1789       0.27770004]
[37m[1m [535.42491656   0.37020001   0.45809999   0.2027       0.2969    ]
[37m[1m [846.0832648    0.37909999   0.50299996   0.20899999   0.2861    ]]
[37m[1m[2023-06-25 03:22:14,191][129146] Max Reward on eval: 1062.9240556144505
[37m[1m[2023-06-25 03:22:14,191][129146] Min Reward on eval: 78.7522906500526
[37m[1m[2023-06-25 03:22:14,191][129146] Mean Reward across all agents: 711.3147287403975
[37m[1m[2023-06-25 03:22:14,192][129146] Average Trajectory Length: 999.716
[36m[2023-06-25 03:22:14,199][129146] mean_value=174.6011281572723, max_value=1186.6612631975509
[37m[1m[2023-06-25 03:22:14,202][129146] New mean coefficients: [[ 1.4933635 -2.5619295  5.492227  -3.8425794  3.9386601]]
[37m[1m[2023-06-25 03:22:14,203][129146] Moving the mean solution point...
[36m[2023-06-25 03:22:24,094][129146] train() took 9.89 seconds to complete
[36m[2023-06-25 03:22:24,095][129146] FPS: 388289.07
[36m[2023-06-25 03:22:24,097][129146] itr=238, itrs=2000, Progress: 11.90%
[36m[2023-06-25 03:22:35,750][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 03:22:35,750][129146] FPS: 329938.86
[36m[2023-06-25 03:22:40,488][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:22:40,489][129146] Reward + Measures: [[350.75598469   0.96557033   0.93970263   0.94974136   0.018872  ]]
[37m[1m[2023-06-25 03:22:40,489][129146] Max Reward on eval: 350.7559846896863
[37m[1m[2023-06-25 03:22:40,489][129146] Min Reward on eval: 350.7559846896863
[37m[1m[2023-06-25 03:22:40,489][129146] Mean Reward across all agents: 350.7559846896863
[37m[1m[2023-06-25 03:22:40,490][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:22:45,995][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:22:45,996][129146] Reward + Measures: [[368.17054783   0.3267       0.7033       0.13500001   0.62439996]
[37m[1m [236.08531696   0.40640002   0.63609999   0.22880001   0.53530002]
[37m[1m [318.3262595    0.32300001   0.68690002   0.17830001   0.58540004]
[37m[1m ...
[37m[1m [383.33797029   0.28830001   0.602        0.1382       0.4598    ]
[37m[1m [321.29902693   0.26630002   0.43710002   0.14560001   0.29980001]
[37m[1m [172.79671297   0.35020003   0.51249999   0.21700001   0.35549998]]
[37m[1m[2023-06-25 03:22:45,996][129146] Max Reward on eval: 937.8256223769392
[37m[1m[2023-06-25 03:22:45,996][129146] Min Reward on eval: -24.459292855905368
[37m[1m[2023-06-25 03:22:45,997][129146] Mean Reward across all agents: 378.08184069202247
[37m[1m[2023-06-25 03:22:45,997][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:22:46,006][129146] mean_value=285.2840636393352, max_value=1133.2643095999258
[37m[1m[2023-06-25 03:22:46,009][129146] New mean coefficients: [[ 1.5508448  -0.97456586  5.144489   -3.9457023   4.1298075 ]]
[37m[1m[2023-06-25 03:22:46,010][129146] Moving the mean solution point...
[36m[2023-06-25 03:22:55,854][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 03:22:55,854][129146] FPS: 390147.31
[36m[2023-06-25 03:22:55,857][129146] itr=239, itrs=2000, Progress: 11.95%
[36m[2023-06-25 03:23:07,428][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 03:23:07,428][129146] FPS: 332273.84
[36m[2023-06-25 03:23:12,294][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:23:12,294][129146] Reward + Measures: [[404.52145261   0.8972407    0.90950799   0.84730536   0.09249066]]
[37m[1m[2023-06-25 03:23:12,295][129146] Max Reward on eval: 404.52145260989636
[37m[1m[2023-06-25 03:23:12,295][129146] Min Reward on eval: 404.52145260989636
[37m[1m[2023-06-25 03:23:12,295][129146] Mean Reward across all agents: 404.52145260989636
[37m[1m[2023-06-25 03:23:12,295][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:23:17,866][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:23:17,867][129146] Reward + Measures: [[756.33623878   0.4693       0.52150005   0.19160001   0.28009999]
[37m[1m [582.60458294   0.71030003   0.52130002   0.56520003   0.15090001]
[37m[1m [759.39935706   0.49939999   0.44899997   0.33610001   0.29999998]
[37m[1m ...
[37m[1m [254.93158389   0.9867       0.93029994   0.97749996   0.004     ]
[37m[1m [759.857866     0.46560001   0.41200003   0.24850002   0.24799998]
[37m[1m [549.41560853   0.46010002   0.4357       0.19690001   0.2568    ]]
[37m[1m[2023-06-25 03:23:17,867][129146] Max Reward on eval: 1091.6063093058765
[37m[1m[2023-06-25 03:23:17,867][129146] Min Reward on eval: 154.7361968014273
[37m[1m[2023-06-25 03:23:17,867][129146] Mean Reward across all agents: 726.5422120796978
[37m[1m[2023-06-25 03:23:17,867][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:23:17,875][129146] mean_value=133.14676777984138, max_value=1251.1147398809203
[37m[1m[2023-06-25 03:23:17,878][129146] New mean coefficients: [[ 1.5739298   0.73148465  4.0415874  -2.718844    2.5481267 ]]
[37m[1m[2023-06-25 03:23:17,879][129146] Moving the mean solution point...
[36m[2023-06-25 03:23:27,669][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 03:23:27,670][129146] FPS: 392283.57
[36m[2023-06-25 03:23:27,672][129146] itr=240, itrs=2000, Progress: 12.00%
[37m[1m[2023-06-25 03:23:30,744][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000220
[36m[2023-06-25 03:23:42,558][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 03:23:42,559][129146] FPS: 334260.77
[36m[2023-06-25 03:23:47,456][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:23:47,456][129146] Reward + Measures: [[386.00962736   0.9041363    0.924447     0.81113136   0.14585333]]
[37m[1m[2023-06-25 03:23:47,457][129146] Max Reward on eval: 386.0096273559713
[37m[1m[2023-06-25 03:23:47,457][129146] Min Reward on eval: 386.0096273559713
[37m[1m[2023-06-25 03:23:47,457][129146] Mean Reward across all agents: 386.0096273559713
[37m[1m[2023-06-25 03:23:47,457][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:23:52,948][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:23:52,949][129146] Reward + Measures: [[601.34739766   0.55419999   0.83490002   0.15110001   0.70710003]
[37m[1m [407.98142838   0.46099997   0.44530001   0.2481       0.26859999]
[37m[1m [612.18152481   0.41640002   0.52089995   0.18360002   0.3414    ]
[37m[1m ...
[37m[1m [423.44591685   0.47589999   0.491        0.3371       0.33010003]
[37m[1m [467.52149389   0.75330001   0.69250005   0.57949996   0.23550001]
[37m[1m [478.66932008   0.56230003   0.41479999   0.37479997   0.3283    ]]
[37m[1m[2023-06-25 03:23:52,949][129146] Max Reward on eval: 1023.7295364810386
[37m[1m[2023-06-25 03:23:52,949][129146] Min Reward on eval: 77.44142009369098
[37m[1m[2023-06-25 03:23:52,950][129146] Mean Reward across all agents: 502.33275792892016
[37m[1m[2023-06-25 03:23:52,950][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:23:52,962][129146] mean_value=322.7004181776577, max_value=1382.1236885465332
[37m[1m[2023-06-25 03:23:52,965][129146] New mean coefficients: [[ 1.6726717  1.253011   0.8759134 -3.7272387  2.7486682]]
[37m[1m[2023-06-25 03:23:52,966][129146] Moving the mean solution point...
[36m[2023-06-25 03:24:02,643][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 03:24:02,643][129146] FPS: 396864.69
[36m[2023-06-25 03:24:02,646][129146] itr=241, itrs=2000, Progress: 12.05%
[36m[2023-06-25 03:24:14,203][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 03:24:14,204][129146] FPS: 332728.47
[36m[2023-06-25 03:24:19,055][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:24:19,055][129146] Reward + Measures: [[414.07180559   0.89277434   0.92105502   0.74805963   0.20194501]]
[37m[1m[2023-06-25 03:24:19,055][129146] Max Reward on eval: 414.0718055875941
[37m[1m[2023-06-25 03:24:19,056][129146] Min Reward on eval: 414.0718055875941
[37m[1m[2023-06-25 03:24:19,056][129146] Mean Reward across all agents: 414.0718055875941
[37m[1m[2023-06-25 03:24:19,056][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:24:24,601][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:24:24,606][129146] Reward + Measures: [[551.44527779   0.63730001   0.43660003   0.36720002   0.31220001]
[37m[1m [248.52933734   0.4506       0.54090005   0.171        0.3418    ]
[37m[1m [296.84674544   0.96390003   0.94470006   0.89020008   0.0851    ]
[37m[1m ...
[37m[1m [ 30.30606841   0.92640001   0.19340001   0.81549996   0.22750001]
[37m[1m [410.11985953   0.58289999   0.43449998   0.3396       0.34840003]
[37m[1m [262.9006893    0.9914999    0.96219999   0.9896       0.0015    ]]
[37m[1m[2023-06-25 03:24:24,606][129146] Max Reward on eval: 1065.7771519973176
[37m[1m[2023-06-25 03:24:24,607][129146] Min Reward on eval: -68.62662955889246
[37m[1m[2023-06-25 03:24:24,607][129146] Mean Reward across all agents: 414.51908805932044
[37m[1m[2023-06-25 03:24:24,607][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:24:24,620][129146] mean_value=290.5480104226231, max_value=1323.7521443960024
[37m[1m[2023-06-25 03:24:24,623][129146] New mean coefficients: [[ 1.5866166  1.7288656  1.2216741 -2.9840639  1.0449991]]
[37m[1m[2023-06-25 03:24:24,624][129146] Moving the mean solution point...
[36m[2023-06-25 03:24:34,543][129146] train() took 9.92 seconds to complete
[36m[2023-06-25 03:24:34,544][129146] FPS: 387194.21
[36m[2023-06-25 03:24:34,546][129146] itr=242, itrs=2000, Progress: 12.10%
[36m[2023-06-25 03:24:46,152][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 03:24:46,153][129146] FPS: 331269.92
[36m[2023-06-25 03:24:51,014][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:24:51,015][129146] Reward + Measures: [[460.54943329   0.87612665   0.9095394    0.63326329   0.30127999]]
[37m[1m[2023-06-25 03:24:51,015][129146] Max Reward on eval: 460.54943328871735
[37m[1m[2023-06-25 03:24:51,015][129146] Min Reward on eval: 460.54943328871735
[37m[1m[2023-06-25 03:24:51,015][129146] Mean Reward across all agents: 460.54943328871735
[37m[1m[2023-06-25 03:24:51,015][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:24:56,565][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:24:56,565][129146] Reward + Measures: [[574.54506557   0.72009999   0.84350008   0.1549       0.685     ]
[37m[1m [519.54898896   0.7445001    0.86250001   0.5054       0.3777    ]
[37m[1m [575.46409958   0.5733       0.3849       0.40669999   0.29470003]
[37m[1m ...
[37m[1m [535.85136587   0.3962       0.52950001   0.1173       0.41150004]
[37m[1m [191.32517259   0.88950008   0.8021       0.80660003   0.06420001]
[37m[1m [676.67318206   0.58280003   0.56450003   0.30780002   0.25279999]]
[37m[1m[2023-06-25 03:24:56,565][129146] Max Reward on eval: 947.4879910487682
[37m[1m[2023-06-25 03:24:56,566][129146] Min Reward on eval: 35.466850707796404
[37m[1m[2023-06-25 03:24:56,566][129146] Mean Reward across all agents: 499.96926914206665
[37m[1m[2023-06-25 03:24:56,566][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:24:56,579][129146] mean_value=462.78517360493345, max_value=1204.2663673352915
[37m[1m[2023-06-25 03:24:56,582][129146] New mean coefficients: [[ 1.3306406  0.6940999  0.9121847 -2.5256824  1.8662765]]
[37m[1m[2023-06-25 03:24:56,583][129146] Moving the mean solution point...
[36m[2023-06-25 03:25:06,483][129146] train() took 9.90 seconds to complete
[36m[2023-06-25 03:25:06,483][129146] FPS: 387940.83
[36m[2023-06-25 03:25:06,486][129146] itr=243, itrs=2000, Progress: 12.15%
[36m[2023-06-25 03:25:18,139][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 03:25:18,139][129146] FPS: 329952.72
[36m[2023-06-25 03:25:22,966][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:25:22,966][129146] Reward + Measures: [[509.29993456   0.89598662   0.90772575   0.55199969   0.37601367]]
[37m[1m[2023-06-25 03:25:22,966][129146] Max Reward on eval: 509.29993455770966
[37m[1m[2023-06-25 03:25:22,967][129146] Min Reward on eval: 509.29993455770966
[37m[1m[2023-06-25 03:25:22,967][129146] Mean Reward across all agents: 509.29993455770966
[37m[1m[2023-06-25 03:25:22,967][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:25:28,567][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:25:28,568][129146] Reward + Measures: [[336.27674245   0.88710004   0.84470004   0.7974       0.10960001]
[37m[1m [582.93788932   0.62260002   0.62819999   0.2359       0.45500001]
[37m[1m [658.02257394   0.73929995   0.88360006   0.0315       0.84619999]
[37m[1m ...
[37m[1m [506.30249702   0.77179998   0.86569995   0.33039999   0.55440003]
[37m[1m [171.19743824   0.95619994   0.71429998   0.91520005   0.0319    ]
[37m[1m [527.38789968   0.52080005   0.73949999   0.0814       0.63309997]]
[37m[1m[2023-06-25 03:25:28,568][129146] Max Reward on eval: 1028.94980300233
[37m[1m[2023-06-25 03:25:28,568][129146] Min Reward on eval: -1.2946543542784639
[37m[1m[2023-06-25 03:25:28,568][129146] Mean Reward across all agents: 455.2826537452163
[37m[1m[2023-06-25 03:25:28,568][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:25:28,582][129146] mean_value=509.5343836370882, max_value=1189.2673193447758
[37m[1m[2023-06-25 03:25:28,584][129146] New mean coefficients: [[ 1.395956   -0.47152364  1.0575378  -2.6543345   0.8127898 ]]
[37m[1m[2023-06-25 03:25:28,585][129146] Moving the mean solution point...
[36m[2023-06-25 03:25:38,330][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 03:25:38,330][129146] FPS: 394118.32
[36m[2023-06-25 03:25:38,332][129146] itr=244, itrs=2000, Progress: 12.20%
[36m[2023-06-25 03:25:49,969][129146] train() took 11.62 seconds to complete
[36m[2023-06-25 03:25:49,969][129146] FPS: 330444.81
[36m[2023-06-25 03:25:54,758][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:25:54,759][129146] Reward + Measures: [[561.29783388   0.88242668   0.89791858   0.42250237   0.49166101]]
[37m[1m[2023-06-25 03:25:54,759][129146] Max Reward on eval: 561.2978338803982
[37m[1m[2023-06-25 03:25:54,759][129146] Min Reward on eval: 561.2978338803982
[37m[1m[2023-06-25 03:25:54,760][129146] Mean Reward across all agents: 561.2978338803982
[37m[1m[2023-06-25 03:25:54,760][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:26:00,006][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:26:00,006][129146] Reward + Measures: [[465.52131109   0.53030002   0.62219995   0.19170001   0.58450001]
[37m[1m [675.66202739   0.70489997   0.73120004   0.3788       0.32210001]
[37m[1m [553.40406517   0.50240004   0.76579994   0.0792       0.73769999]
[37m[1m ...
[37m[1m [514.60920086   0.7022       0.90599996   0.1239       0.78850001]
[37m[1m [562.32756139   0.55770004   0.87170011   0.034        0.83759993]
[37m[1m [505.30768268   0.84000009   0.8908       0.39569998   0.49899998]]
[37m[1m[2023-06-25 03:26:00,007][129146] Max Reward on eval: 892.576339200139
[37m[1m[2023-06-25 03:26:00,007][129146] Min Reward on eval: 176.30452083297422
[37m[1m[2023-06-25 03:26:00,007][129146] Mean Reward across all agents: 551.6600369710876
[37m[1m[2023-06-25 03:26:00,007][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:26:00,019][129146] mean_value=633.6703659928925, max_value=1313.6298865830292
[37m[1m[2023-06-25 03:26:00,022][129146] New mean coefficients: [[ 1.2852174 -1.252712   1.6926256 -1.9608133  0.4775789]]
[37m[1m[2023-06-25 03:26:00,023][129146] Moving the mean solution point...
[36m[2023-06-25 03:26:09,782][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 03:26:09,782][129146] FPS: 393570.10
[36m[2023-06-25 03:26:09,784][129146] itr=245, itrs=2000, Progress: 12.25%
[36m[2023-06-25 03:26:21,415][129146] train() took 11.62 seconds to complete
[36m[2023-06-25 03:26:21,415][129146] FPS: 330576.72
[36m[2023-06-25 03:26:26,223][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:26:26,224][129146] Reward + Measures: [[603.5956533    0.86412865   0.89242303   0.33627599   0.57021469]]
[37m[1m[2023-06-25 03:26:26,224][129146] Max Reward on eval: 603.5956532960828
[37m[1m[2023-06-25 03:26:26,224][129146] Min Reward on eval: 603.5956532960828
[37m[1m[2023-06-25 03:26:26,225][129146] Mean Reward across all agents: 603.5956532960828
[37m[1m[2023-06-25 03:26:26,225][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:26:31,713][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:26:31,714][129146] Reward + Measures: [[563.01505617   0.39369997   0.56490004   0.1353       0.40949997]
[37m[1m [542.4909626    0.45240003   0.74550003   0.0533       0.71900004]
[37m[1m [622.83463118   0.40010005   0.55770004   0.13890001   0.44499999]
[37m[1m ...
[37m[1m [671.57142831   0.3698       0.43109998   0.0801       0.28979999]
[37m[1m [525.68638006   0.40299997   0.73259997   0.0831       0.69180006]
[37m[1m [753.50441257   0.3766       0.52430004   0.1173       0.35660002]]
[37m[1m[2023-06-25 03:26:31,714][129146] Max Reward on eval: 942.3404653846752
[37m[1m[2023-06-25 03:26:31,714][129146] Min Reward on eval: 106.91273079555249
[37m[1m[2023-06-25 03:26:31,715][129146] Mean Reward across all agents: 592.6086006933768
[37m[1m[2023-06-25 03:26:31,715][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:26:31,725][129146] mean_value=383.6426659820393, max_value=1158.4482676599641
[37m[1m[2023-06-25 03:26:31,727][129146] New mean coefficients: [[ 1.1327229  -1.2374812   0.0227046  -2.0647552   0.68857336]]
[37m[1m[2023-06-25 03:26:31,728][129146] Moving the mean solution point...
[36m[2023-06-25 03:26:41,383][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 03:26:41,383][129146] FPS: 397813.49
[36m[2023-06-25 03:26:41,385][129146] itr=246, itrs=2000, Progress: 12.30%
[36m[2023-06-25 03:26:52,847][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 03:26:52,848][129146] FPS: 335550.58
[36m[2023-06-25 03:26:57,726][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:26:57,727][129146] Reward + Measures: [[664.23473059   0.79833364   0.87525958   0.20845801   0.67756003]]
[37m[1m[2023-06-25 03:26:57,727][129146] Max Reward on eval: 664.2347305886716
[37m[1m[2023-06-25 03:26:57,727][129146] Min Reward on eval: 664.2347305886716
[37m[1m[2023-06-25 03:26:57,728][129146] Mean Reward across all agents: 664.2347305886716
[37m[1m[2023-06-25 03:26:57,728][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:27:03,273][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:27:03,274][129146] Reward + Measures: [[529.78396038   0.85269994   0.9070999    0.41220003   0.5079    ]
[37m[1m [448.59360471   0.70850003   0.465        0.54730004   0.17459999]
[37m[1m [600.84783107   0.46090004   0.61660004   0.0801       0.5359    ]
[37m[1m ...
[37m[1m [667.20037366   0.70160002   0.78330004   0.20819998   0.55989999]
[37m[1m [ 26.20711816   0.37390003   0.29100001   0.29080001   0.3285    ]
[37m[1m [ 79.47682501   0.35110003   0.30170003   0.36649999   0.36140001]]
[37m[1m[2023-06-25 03:27:03,274][129146] Max Reward on eval: 891.0736477230676
[37m[1m[2023-06-25 03:27:03,274][129146] Min Reward on eval: -1089.961054857145
[37m[1m[2023-06-25 03:27:03,274][129146] Mean Reward across all agents: 321.35671174666714
[37m[1m[2023-06-25 03:27:03,275][129146] Average Trajectory Length: 994.2566666666667
[36m[2023-06-25 03:27:03,283][129146] mean_value=-9.762918450658956, max_value=1089.0901434348025
[37m[1m[2023-06-25 03:27:03,286][129146] New mean coefficients: [[ 1.5532525  -0.47105587  0.4121952  -0.6401969   0.33859986]]
[37m[1m[2023-06-25 03:27:03,287][129146] Moving the mean solution point...
[36m[2023-06-25 03:27:13,253][129146] train() took 9.96 seconds to complete
[36m[2023-06-25 03:27:13,253][129146] FPS: 385370.20
[36m[2023-06-25 03:27:13,255][129146] itr=247, itrs=2000, Progress: 12.35%
[36m[2023-06-25 03:27:24,896][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 03:27:24,896][129146] FPS: 330312.93
[36m[2023-06-25 03:27:29,652][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:27:29,652][129146] Reward + Measures: [[731.14991585   0.76334697   0.85504895   0.14246401   0.71667796]]
[37m[1m[2023-06-25 03:27:29,652][129146] Max Reward on eval: 731.1499158513908
[37m[1m[2023-06-25 03:27:29,653][129146] Min Reward on eval: 731.1499158513908
[37m[1m[2023-06-25 03:27:29,653][129146] Mean Reward across all agents: 731.1499158513908
[37m[1m[2023-06-25 03:27:29,653][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:27:35,293][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:27:35,294][129146] Reward + Measures: [[ 281.3936771     0.43729997    0.49860001    0.1612        0.3053    ]
[37m[1m [-679.85359661    0.26449585    0.23784505    0.14810322    0.16501318]
[37m[1m [ 745.18957379    0.62250006    0.58160001    0.32619998    0.3468    ]
[37m[1m ...
[37m[1m [ 579.58931302    0.40110001    0.74430001    0.0972        0.6965    ]
[37m[1m [-150.3309483     0.5869        0.30780002    0.41929999    0.29710004]
[37m[1m [-314.24874141    0.22071567    0.24848194    0.24538794    0.24046385]]
[37m[1m[2023-06-25 03:27:35,294][129146] Max Reward on eval: 1021.2477747607743
[37m[1m[2023-06-25 03:27:35,294][129146] Min Reward on eval: -1065.6233716881775
[37m[1m[2023-06-25 03:27:35,294][129146] Mean Reward across all agents: 193.00175479095176
[37m[1m[2023-06-25 03:27:35,295][129146] Average Trajectory Length: 979.9133333333333
[36m[2023-06-25 03:27:35,303][129146] mean_value=-195.78905900979075, max_value=1377.7514797245617
[37m[1m[2023-06-25 03:27:35,306][129146] New mean coefficients: [[ 1.6279699   0.89658105  0.51921505 -0.73917174 -0.00183082]]
[37m[1m[2023-06-25 03:27:35,306][129146] Moving the mean solution point...
[36m[2023-06-25 03:27:45,037][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 03:27:45,038][129146] FPS: 394687.71
[36m[2023-06-25 03:27:45,040][129146] itr=248, itrs=2000, Progress: 12.40%
[36m[2023-06-25 03:27:56,617][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 03:27:56,618][129146] FPS: 332183.15
[36m[2023-06-25 03:28:01,436][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:28:01,436][129146] Reward + Measures: [[787.97745285   0.75456333   0.83240861   0.10331266   0.72442526]]
[37m[1m[2023-06-25 03:28:01,436][129146] Max Reward on eval: 787.9774528532519
[37m[1m[2023-06-25 03:28:01,436][129146] Min Reward on eval: 787.9774528532519
[37m[1m[2023-06-25 03:28:01,437][129146] Mean Reward across all agents: 787.9774528532519
[37m[1m[2023-06-25 03:28:01,437][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:28:06,926][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:28:06,927][129146] Reward + Measures: [[269.16220306   0.9346       0.79349995   0.88880008   0.0454    ]
[37m[1m [940.65274692   0.53120005   0.59369999   0.10930001   0.48199996]
[37m[1m [855.47290786   0.41169998   0.60700005   0.1322       0.41290003]
[37m[1m ...
[37m[1m [107.42719727   0.78060001   0.26440001   0.6214       0.3457    ]
[37m[1m [793.06810953   0.70699996   0.8136       0.0393       0.74730003]
[37m[1m [830.37804343   0.55919999   0.64530003   0.12150001   0.57319999]]
[37m[1m[2023-06-25 03:28:06,927][129146] Max Reward on eval: 1192.321334541845
[37m[1m[2023-06-25 03:28:06,927][129146] Min Reward on eval: 107.42719726688229
[37m[1m[2023-06-25 03:28:06,927][129146] Mean Reward across all agents: 774.5349241427024
[37m[1m[2023-06-25 03:28:06,928][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:28:06,942][129146] mean_value=490.8510112763852, max_value=1424.1502359614358
[37m[1m[2023-06-25 03:28:06,944][129146] New mean coefficients: [[ 1.834171    1.3250122   0.04676414 -1.2505467  -0.15859464]]
[37m[1m[2023-06-25 03:28:06,945][129146] Moving the mean solution point...
[36m[2023-06-25 03:28:16,796][129146] train() took 9.85 seconds to complete
[36m[2023-06-25 03:28:16,796][129146] FPS: 389896.83
[36m[2023-06-25 03:28:16,798][129146] itr=249, itrs=2000, Progress: 12.45%
[36m[2023-06-25 03:28:28,478][129146] train() took 11.67 seconds to complete
[36m[2023-06-25 03:28:28,479][129146] FPS: 329188.22
[36m[2023-06-25 03:28:33,386][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:28:33,387][129146] Reward + Measures: [[860.32837674   0.73620719   0.80420375   0.09111301   0.69727975]]
[37m[1m[2023-06-25 03:28:33,387][129146] Max Reward on eval: 860.3283767383882
[37m[1m[2023-06-25 03:28:33,387][129146] Min Reward on eval: 860.3283767383882
[37m[1m[2023-06-25 03:28:33,387][129146] Mean Reward across all agents: 860.3283767383882
[37m[1m[2023-06-25 03:28:33,387][129146] Average Trajectory Length: 999.7403333333333
[36m[2023-06-25 03:28:38,905][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:28:38,905][129146] Reward + Measures: [[118.59485733   0.63849998   0.54549998   0.56409997   0.5485    ]
[37m[1m [587.14786106   0.60810006   0.3547       0.40890002   0.33629999]
[37m[1m [702.89297109   0.62529999   0.69220001   0.46149999   0.27560002]
[37m[1m ...
[37m[1m [731.31137114   0.75740004   0.88840002   0.0253       0.84359998]
[37m[1m [392.1401957    0.57060003   0.38730001   0.40189996   0.35800001]
[37m[1m [165.36140692   0.65240002   0.56080002   0.56129998   0.54329997]]
[37m[1m[2023-06-25 03:28:38,905][129146] Max Reward on eval: 1162.715788587404
[37m[1m[2023-06-25 03:28:38,906][129146] Min Reward on eval: -1199.8024736611812
[37m[1m[2023-06-25 03:28:38,906][129146] Mean Reward across all agents: 311.8649675214041
[37m[1m[2023-06-25 03:28:38,906][129146] Average Trajectory Length: 998.5816666666666
[36m[2023-06-25 03:28:38,917][129146] mean_value=238.8899066094505, max_value=1302.3445840194647
[37m[1m[2023-06-25 03:28:38,920][129146] New mean coefficients: [[ 1.9281557   1.0643883  -0.26983082 -0.6361064  -0.14209431]]
[37m[1m[2023-06-25 03:28:38,921][129146] Moving the mean solution point...
[36m[2023-06-25 03:28:48,682][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 03:28:48,682][129146] FPS: 393498.37
[36m[2023-06-25 03:28:48,685][129146] itr=250, itrs=2000, Progress: 12.50%
[37m[1m[2023-06-25 03:28:51,910][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000230
[36m[2023-06-25 03:29:03,697][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 03:29:03,698][129146] FPS: 334604.61
[36m[2023-06-25 03:29:08,291][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:29:08,292][129146] Reward + Measures: [[982.98779101   0.692325     0.75478566   0.071493     0.64897501]]
[37m[1m[2023-06-25 03:29:08,292][129146] Max Reward on eval: 982.9877910060624
[37m[1m[2023-06-25 03:29:08,292][129146] Min Reward on eval: 982.9877910060624
[37m[1m[2023-06-25 03:29:08,292][129146] Mean Reward across all agents: 982.9877910060624
[37m[1m[2023-06-25 03:29:08,293][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:29:13,772][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:29:13,772][129146] Reward + Measures: [[970.46818144   0.65860003   0.72539997   0.05980001   0.65039998]
[37m[1m [419.76975848   0.2342       0.30720001   0.13219999   0.27340001]
[37m[1m [626.38856501   0.79320002   0.71270001   0.62870002   0.1103    ]
[37m[1m ...
[37m[1m [785.83904195   0.65790004   0.83260006   0.0465       0.75480002]
[37m[1m [800.09247468   0.64399999   0.69120002   0.15939999   0.4937    ]
[37m[1m [501.65240333   0.58880001   0.40500003   0.40410003   0.22500001]]
[37m[1m[2023-06-25 03:29:13,773][129146] Max Reward on eval: 1274.4291563846987
[37m[1m[2023-06-25 03:29:13,773][129146] Min Reward on eval: -672.4125886453069
[37m[1m[2023-06-25 03:29:13,773][129146] Mean Reward across all agents: 519.3041599836788
[37m[1m[2023-06-25 03:29:13,773][129146] Average Trajectory Length: 972.8746666666666
[36m[2023-06-25 03:29:13,784][129146] mean_value=-59.316649861147305, max_value=1371.9188641950302
[37m[1m[2023-06-25 03:29:13,787][129146] New mean coefficients: [[ 1.7730864   0.88510174 -0.45039627 -0.7211933  -0.41372707]]
[37m[1m[2023-06-25 03:29:13,788][129146] Moving the mean solution point...
[36m[2023-06-25 03:29:23,452][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 03:29:23,453][129146] FPS: 397388.33
[36m[2023-06-25 03:29:23,455][129146] itr=251, itrs=2000, Progress: 12.55%
[36m[2023-06-25 03:29:34,921][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 03:29:34,921][129146] FPS: 335335.20
[36m[2023-06-25 03:29:39,797][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:29:39,798][129146] Reward + Measures: [[1215.67938114    0.55984432    0.62003696    0.09645499    0.45522502]]
[37m[1m[2023-06-25 03:29:39,798][129146] Max Reward on eval: 1215.6793811379664
[37m[1m[2023-06-25 03:29:39,798][129146] Min Reward on eval: 1215.6793811379664
[37m[1m[2023-06-25 03:29:39,799][129146] Mean Reward across all agents: 1215.6793811379664
[37m[1m[2023-06-25 03:29:39,799][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:29:45,357][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:29:45,358][129146] Reward + Measures: [[-208.92677811    0.46496198    0.34826905    0.37070283    0.26429155]
[37m[1m [ 265.21803089    0.61219996    0.43189999    0.31350002    0.28439999]
[37m[1m [ 695.41065372    0.6146        0.6936        0.18609999    0.49760005]
[37m[1m ...
[37m[1m [1276.39478265    0.45159999    0.47600004    0.1098        0.3249    ]
[37m[1m [1203.87737665    0.4691        0.4165        0.1451        0.30419999]
[37m[1m [ 828.29100391    0.69840002    0.73800004    0.25710002    0.46120006]]
[37m[1m[2023-06-25 03:29:45,358][129146] Max Reward on eval: 1472.9923422430409
[37m[1m[2023-06-25 03:29:45,358][129146] Min Reward on eval: -679.1290871802078
[37m[1m[2023-06-25 03:29:45,358][129146] Mean Reward across all agents: 653.0005471369678
[37m[1m[2023-06-25 03:29:45,359][129146] Average Trajectory Length: 998.0136666666666
[36m[2023-06-25 03:29:45,369][129146] mean_value=250.57788741041867, max_value=1736.4474133603276
[37m[1m[2023-06-25 03:29:45,371][129146] New mean coefficients: [[ 1.8389095   1.5983598  -0.8484671  -0.37163123 -0.08441415]]
[37m[1m[2023-06-25 03:29:45,372][129146] Moving the mean solution point...
[36m[2023-06-25 03:29:55,126][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 03:29:55,127][129146] FPS: 393748.00
[36m[2023-06-25 03:29:55,129][129146] itr=252, itrs=2000, Progress: 12.60%
[36m[2023-06-25 03:30:06,766][129146] train() took 11.62 seconds to complete
[36m[2023-06-25 03:30:06,767][129146] FPS: 330498.10
[36m[2023-06-25 03:30:11,620][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:30:11,620][129146] Reward + Measures: [[1380.4539548     0.49736568    0.5456177     0.09515567    0.36477563]]
[37m[1m[2023-06-25 03:30:11,620][129146] Max Reward on eval: 1380.453954804108
[37m[1m[2023-06-25 03:30:11,621][129146] Min Reward on eval: 1380.453954804108
[37m[1m[2023-06-25 03:30:11,621][129146] Mean Reward across all agents: 1380.453954804108
[37m[1m[2023-06-25 03:30:11,621][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:30:17,162][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:30:17,163][129146] Reward + Measures: [[ 350.25905823    0.27690002    0.4111        0.16049999    0.35250002]
[37m[1m [-491.53250289    0.7924        0.34900001    0.73159999    0.32660004]
[37m[1m [-199.78420349    0.7299        0.2974        0.59750003    0.2543    ]
[37m[1m ...
[37m[1m [ 748.41629212    0.3599        0.41980001    0.0735        0.3046    ]
[37m[1m [-190.66936221    0.77380002    0.24150001    0.69680005    0.257     ]
[37m[1m [  90.2955352     0.3651        0.3732        0.2922        0.45089999]]
[37m[1m[2023-06-25 03:30:17,163][129146] Max Reward on eval: 1468.3062728963675
[37m[1m[2023-06-25 03:30:17,163][129146] Min Reward on eval: -1049.5238786373288
[37m[1m[2023-06-25 03:30:17,164][129146] Mean Reward across all agents: 554.5890324771156
[37m[1m[2023-06-25 03:30:17,164][129146] Average Trajectory Length: 995.276
[36m[2023-06-25 03:30:17,171][129146] mean_value=-28.91294799431135, max_value=1798.6720391426375
[37m[1m[2023-06-25 03:30:17,174][129146] New mean coefficients: [[ 1.7992988   1.855893   -1.0986019  -0.4337799  -0.32875448]]
[37m[1m[2023-06-25 03:30:17,175][129146] Moving the mean solution point...
[36m[2023-06-25 03:30:26,974][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 03:30:26,974][129146] FPS: 391926.70
[36m[2023-06-25 03:30:26,977][129146] itr=253, itrs=2000, Progress: 12.65%
[36m[2023-06-25 03:30:38,479][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 03:30:38,479][129146] FPS: 334353.54
[36m[2023-06-25 03:30:43,304][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:30:43,304][129146] Reward + Measures: [[1544.82942574    0.46094       0.48486164    0.09374201    0.29050967]]
[37m[1m[2023-06-25 03:30:43,305][129146] Max Reward on eval: 1544.82942573742
[37m[1m[2023-06-25 03:30:43,305][129146] Min Reward on eval: 1544.82942573742
[37m[1m[2023-06-25 03:30:43,305][129146] Mean Reward across all agents: 1544.82942573742
[37m[1m[2023-06-25 03:30:43,306][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:30:48,625][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:30:48,625][129146] Reward + Measures: [[ 838.76932778    0.75940001    0.68360001    0.55590004    0.12980001]
[37m[1m [1474.32963652    0.4244        0.45840001    0.094         0.2863    ]
[37m[1m [1549.21596309    0.48109999    0.47960004    0.0913        0.26159999]
[37m[1m ...
[37m[1m [1086.06032886    0.58140004    0.59380001    0.1901        0.35210004]
[37m[1m [1067.14405488    0.53239995    0.61849999    0.0409        0.50639999]
[37m[1m [ 704.74897632    0.57910001    0.41220003    0.31549999    0.26300001]]
[37m[1m[2023-06-25 03:30:48,626][129146] Max Reward on eval: 1610.3424347032328
[37m[1m[2023-06-25 03:30:48,626][129146] Min Reward on eval: 445.62725255440455
[37m[1m[2023-06-25 03:30:48,626][129146] Mean Reward across all agents: 1272.4430062872336
[37m[1m[2023-06-25 03:30:48,626][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:30:48,636][129146] mean_value=445.336300545634, max_value=1781.5585119450582
[37m[1m[2023-06-25 03:30:48,639][129146] New mean coefficients: [[ 1.9532411   2.1403682  -1.205353    0.6596876  -0.69290483]]
[37m[1m[2023-06-25 03:30:48,640][129146] Moving the mean solution point...
[36m[2023-06-25 03:30:58,229][129146] train() took 9.59 seconds to complete
[36m[2023-06-25 03:30:58,230][129146] FPS: 400481.11
[36m[2023-06-25 03:30:58,232][129146] itr=254, itrs=2000, Progress: 12.70%
[36m[2023-06-25 03:31:09,801][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 03:31:09,801][129146] FPS: 332436.81
[36m[2023-06-25 03:31:14,690][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:31:14,695][129146] Reward + Measures: [[1614.93589078    0.46550602    0.46701765    0.10494066    0.26415569]]
[37m[1m[2023-06-25 03:31:14,696][129146] Max Reward on eval: 1614.9358907807718
[37m[1m[2023-06-25 03:31:14,696][129146] Min Reward on eval: 1614.9358907807718
[37m[1m[2023-06-25 03:31:14,696][129146] Mean Reward across all agents: 1614.9358907807718
[37m[1m[2023-06-25 03:31:14,696][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:31:20,127][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:31:20,127][129146] Reward + Measures: [[-756.01539355    0.30645391    0.21468544    0.33423683    0.15060249]
[37m[1m [ 520.18168967    0.74410003    0.23500001    0.59630007    0.32099998]
[37m[1m [ 176.98444017    0.9393        0.58130008    0.91329998    0.0584    ]
[37m[1m ...
[37m[1m [  99.4023712     0.80720007    0.35999998    0.75990003    0.1202    ]
[37m[1m [  55.4840014     0.34160003    0.35730001    0.2474        0.38480002]
[37m[1m [1496.3742216     0.45590001    0.47499999    0.08549999    0.29410002]]
[37m[1m[2023-06-25 03:31:20,127][129146] Max Reward on eval: 1496.374221598974
[37m[1m[2023-06-25 03:31:20,128][129146] Min Reward on eval: -1143.9911044395762
[37m[1m[2023-06-25 03:31:20,128][129146] Mean Reward across all agents: 221.33670757610705
[37m[1m[2023-06-25 03:31:20,128][129146] Average Trajectory Length: 992.337
[36m[2023-06-25 03:31:20,137][129146] mean_value=-55.79514812208707, max_value=1784.2325913926586
[37m[1m[2023-06-25 03:31:20,140][129146] New mean coefficients: [[ 2.0228374  3.3439538 -1.3537947 -0.177706  -1.0993683]]
[37m[1m[2023-06-25 03:31:20,141][129146] Moving the mean solution point...
[36m[2023-06-25 03:31:29,802][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 03:31:29,802][129146] FPS: 397546.03
[36m[2023-06-25 03:31:29,805][129146] itr=255, itrs=2000, Progress: 12.75%
[36m[2023-06-25 03:31:41,332][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 03:31:41,333][129146] FPS: 333608.90
[36m[2023-06-25 03:31:46,079][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:31:46,079][129146] Reward + Measures: [[1680.94510369    0.47702369    0.44915569    0.09844999    0.25063866]]
[37m[1m[2023-06-25 03:31:46,079][129146] Max Reward on eval: 1680.9451036886267
[37m[1m[2023-06-25 03:31:46,079][129146] Min Reward on eval: 1680.9451036886267
[37m[1m[2023-06-25 03:31:46,080][129146] Mean Reward across all agents: 1680.9451036886267
[37m[1m[2023-06-25 03:31:46,080][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:31:51,522][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:31:51,523][129146] Reward + Measures: [[-182.08675012    0.28759384    0.27701002    0.14774376    0.27788314]
[37m[1m [ 310.74145403    0.40900001    0.33930001    0.16440001    0.28700003]
[37m[1m [ 779.41042969    0.4605        0.45559999    0.09170001    0.38      ]
[37m[1m ...
[37m[1m [ 122.72272515    0.317         0.30219999    0.1683        0.23900001]
[37m[1m [ 870.63069094    0.63620007    0.59860003    0.3644        0.26460001]
[37m[1m [ 206.16221445    0.36452007    0.34583408    0.13701208    0.30257624]]
[37m[1m[2023-06-25 03:31:51,523][129146] Max Reward on eval: 1609.2747278997208
[37m[1m[2023-06-25 03:31:51,523][129146] Min Reward on eval: -648.1714742166689
[37m[1m[2023-06-25 03:31:51,523][129146] Mean Reward across all agents: 528.451757730045
[37m[1m[2023-06-25 03:31:51,524][129146] Average Trajectory Length: 980.9586666666667
[36m[2023-06-25 03:31:51,531][129146] mean_value=-292.88652041180495, max_value=1872.0039316422074
[37m[1m[2023-06-25 03:31:51,534][129146] New mean coefficients: [[ 2.016324    3.519536   -1.6319067  -0.34205842  0.26903367]]
[37m[1m[2023-06-25 03:31:51,535][129146] Moving the mean solution point...
[36m[2023-06-25 03:32:01,223][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 03:32:01,223][129146] FPS: 396419.86
[36m[2023-06-25 03:32:01,226][129146] itr=256, itrs=2000, Progress: 12.80%
[36m[2023-06-25 03:32:12,666][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 03:32:12,666][129146] FPS: 336132.81
[36m[2023-06-25 03:32:17,447][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:32:17,448][129146] Reward + Measures: [[1729.23595747    0.49362832    0.43922997    0.09540933    0.24351265]]
[37m[1m[2023-06-25 03:32:17,448][129146] Max Reward on eval: 1729.2359574739555
[37m[1m[2023-06-25 03:32:17,448][129146] Min Reward on eval: 1729.2359574739555
[37m[1m[2023-06-25 03:32:17,448][129146] Mean Reward across all agents: 1729.2359574739555
[37m[1m[2023-06-25 03:32:17,448][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:32:22,936][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:32:22,936][129146] Reward + Measures: [[-105.9518807     0.41209999    0.33930001    0.3822        0.36759999]
[37m[1m [  -7.98717057    0.65630001    0.32839999    0.4145        0.1508    ]
[37m[1m [-226.41523659    0.4673        0.27150002    0.29620001    0.2545    ]
[37m[1m ...
[37m[1m [ 241.65081155    0.41619998    0.29550001    0.28570002    0.2421    ]
[37m[1m [ 700.1938829     0.44910002    0.3987        0.22480002    0.27500001]
[37m[1m [ 349.81150937    0.59020007    0.38620001    0.33269998    0.27540001]]
[37m[1m[2023-06-25 03:32:22,937][129146] Max Reward on eval: 1462.0222524890444
[37m[1m[2023-06-25 03:32:22,937][129146] Min Reward on eval: -946.7874372327612
[37m[1m[2023-06-25 03:32:22,937][129146] Mean Reward across all agents: 204.48058168050102
[37m[1m[2023-06-25 03:32:22,937][129146] Average Trajectory Length: 990.7656666666667
[36m[2023-06-25 03:32:22,944][129146] mean_value=-221.35784039461083, max_value=1422.1980158791118
[37m[1m[2023-06-25 03:32:22,946][129146] New mean coefficients: [[1.968037   3.3562932  0.1005075  0.20200813 0.3478949 ]]
[37m[1m[2023-06-25 03:32:22,947][129146] Moving the mean solution point...
[36m[2023-06-25 03:32:32,885][129146] train() took 9.94 seconds to complete
[36m[2023-06-25 03:32:32,886][129146] FPS: 386450.37
[36m[2023-06-25 03:32:32,888][129146] itr=257, itrs=2000, Progress: 12.85%
[36m[2023-06-25 03:32:44,392][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 03:32:44,393][129146] FPS: 334227.78
[36m[2023-06-25 03:32:49,145][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:32:49,145][129146] Reward + Measures: [[1817.35512579    0.50519252    0.42276689    0.09014823    0.2345378 ]]
[37m[1m[2023-06-25 03:32:49,146][129146] Max Reward on eval: 1817.3551257881804
[37m[1m[2023-06-25 03:32:49,146][129146] Min Reward on eval: 1817.3551257881804
[37m[1m[2023-06-25 03:32:49,146][129146] Mean Reward across all agents: 1817.3551257881804
[37m[1m[2023-06-25 03:32:49,146][129146] Average Trajectory Length: 999.9363333333333
[36m[2023-06-25 03:32:54,613][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:32:54,613][129146] Reward + Measures: [[ 420.28273338    0.43670002    0.3946        0.25409999    0.25770003]
[37m[1m [ 761.81516473    0.47060004    0.44310004    0.25240001    0.2247    ]
[37m[1m [ 961.60588124    0.50480002    0.47810003    0.25729999    0.2247    ]
[37m[1m ...
[37m[1m [ 521.5063578     0.43440005    0.4131        0.24010001    0.25220001]
[37m[1m [1165.08539068    0.51630002    0.454         0.2211        0.22739999]
[37m[1m [-191.2810483     0.78570002    0.20060001    0.76309997    0.1851    ]]
[37m[1m[2023-06-25 03:32:54,613][129146] Max Reward on eval: 1666.7816963775317
[37m[1m[2023-06-25 03:32:54,614][129146] Min Reward on eval: -1206.0417926205787
[37m[1m[2023-06-25 03:32:54,614][129146] Mean Reward across all agents: 463.32575870597736
[37m[1m[2023-06-25 03:32:54,614][129146] Average Trajectory Length: 992.502
[36m[2023-06-25 03:32:54,619][129146] mean_value=-255.3035968334454, max_value=2082.3779341228305
[37m[1m[2023-06-25 03:32:54,622][129146] New mean coefficients: [[ 1.9112295   3.6956913   0.46131578  0.29423362 -0.03770167]]
[37m[1m[2023-06-25 03:32:54,623][129146] Moving the mean solution point...
[36m[2023-06-25 03:33:04,254][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 03:33:04,254][129146] FPS: 398755.57
[36m[2023-06-25 03:33:04,257][129146] itr=258, itrs=2000, Progress: 12.90%
[36m[2023-06-25 03:33:15,670][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 03:33:15,671][129146] FPS: 336950.58
[36m[2023-06-25 03:33:20,534][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:33:20,535][129146] Reward + Measures: [[1873.41805938    0.52313167    0.41337165    0.086077      0.22833666]]
[37m[1m[2023-06-25 03:33:20,535][129146] Max Reward on eval: 1873.4180593760605
[37m[1m[2023-06-25 03:33:20,535][129146] Min Reward on eval: 1873.4180593760605
[37m[1m[2023-06-25 03:33:20,536][129146] Mean Reward across all agents: 1873.4180593760605
[37m[1m[2023-06-25 03:33:20,536][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:33:26,060][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:33:26,060][129146] Reward + Measures: [[  70.71816333    0.8854        0.32909998    0.81059998    0.23729999]
[37m[1m [1645.91441788    0.57720006    0.41750002    0.12060001    0.22379999]
[37m[1m [1819.95479648    0.51620001    0.4086        0.0947        0.2357    ]
[37m[1m ...
[37m[1m [ 783.02901026    0.78890002    0.31959999    0.48410001    0.21009998]
[37m[1m [1864.61747394    0.45280001    0.43800002    0.0638        0.2369    ]
[37m[1m [ 304.7779991     0.84209996    0.35609999    0.64810002    0.23400001]]
[37m[1m[2023-06-25 03:33:26,061][129146] Max Reward on eval: 1907.2714619600913
[37m[1m[2023-06-25 03:33:26,061][129146] Min Reward on eval: -68.24085043754894
[37m[1m[2023-06-25 03:33:26,061][129146] Mean Reward across all agents: 1110.5904932482315
[37m[1m[2023-06-25 03:33:26,061][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:33:26,071][129146] mean_value=521.9977114526303, max_value=2309.1119604426667
[37m[1m[2023-06-25 03:33:26,074][129146] New mean coefficients: [[ 1.7821833   3.935551   -0.08195004  0.32877284 -0.3554718 ]]
[37m[1m[2023-06-25 03:33:26,075][129146] Moving the mean solution point...
[36m[2023-06-25 03:33:36,026][129146] train() took 9.95 seconds to complete
[36m[2023-06-25 03:33:36,026][129146] FPS: 385932.99
[36m[2023-06-25 03:33:36,029][129146] itr=259, itrs=2000, Progress: 12.95%
[36m[2023-06-25 03:33:47,568][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 03:33:47,568][129146] FPS: 333248.16
[36m[2023-06-25 03:33:52,393][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:33:52,393][129146] Reward + Measures: [[1944.67117079    0.539666      0.412429      0.08764866    0.22631535]]
[37m[1m[2023-06-25 03:33:52,394][129146] Max Reward on eval: 1944.671170788674
[37m[1m[2023-06-25 03:33:52,394][129146] Min Reward on eval: 1944.671170788674
[37m[1m[2023-06-25 03:33:52,394][129146] Mean Reward across all agents: 1944.671170788674
[37m[1m[2023-06-25 03:33:52,394][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:33:58,036][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:33:58,037][129146] Reward + Measures: [[1469.42290787    0.45620003    0.41580001    0.1168        0.25060001]
[37m[1m [ 271.33260697    0.52680004    0.40050003    0.33489999    0.33109999]
[37m[1m [ 267.32588053    0.81730002    0.31399998    0.685         0.1471    ]
[37m[1m ...
[37m[1m [ 285.68262731    0.62580001    0.50240004    0.40289998    0.2782    ]
[37m[1m [ 382.09117045    0.66210002    0.54230005    0.54120004    0.145     ]
[37m[1m [ 177.0352687     0.57069999    0.55580002    0.43320003    0.45050001]]
[37m[1m[2023-06-25 03:33:58,037][129146] Max Reward on eval: 1991.622688453598
[37m[1m[2023-06-25 03:33:58,037][129146] Min Reward on eval: -486.10822130214075
[37m[1m[2023-06-25 03:33:58,038][129146] Mean Reward across all agents: 363.2333767911899
[37m[1m[2023-06-25 03:33:58,038][129146] Average Trajectory Length: 980.0656666666666
[36m[2023-06-25 03:33:58,045][129146] mean_value=-228.73446324375507, max_value=2070.62539767182
[37m[1m[2023-06-25 03:33:58,047][129146] New mean coefficients: [[ 2.0354857   3.8895583   1.0967785   0.38341248 -0.8485768 ]]
[37m[1m[2023-06-25 03:33:58,048][129146] Moving the mean solution point...
[36m[2023-06-25 03:34:07,685][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 03:34:07,685][129146] FPS: 398549.24
[36m[2023-06-25 03:34:07,688][129146] itr=260, itrs=2000, Progress: 13.00%
[37m[1m[2023-06-25 03:34:11,031][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000240
[36m[2023-06-25 03:34:22,873][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 03:34:22,873][129146] FPS: 332782.54
[36m[2023-06-25 03:34:27,746][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:34:27,747][129146] Reward + Measures: [[1999.26730585    0.55803829    0.42311499    0.08954298    0.22500099]]
[37m[1m[2023-06-25 03:34:27,747][129146] Max Reward on eval: 1999.2673058455275
[37m[1m[2023-06-25 03:34:27,747][129146] Min Reward on eval: 1999.2673058455275
[37m[1m[2023-06-25 03:34:27,747][129146] Mean Reward across all agents: 1999.2673058455275
[37m[1m[2023-06-25 03:34:27,747][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:34:33,217][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:34:33,218][129146] Reward + Measures: [[ 974.40472534    0.39079997    0.39770004    0.13380001    0.2307    ]
[37m[1m [ 639.83060997    0.51969999    0.42840001    0.30090001    0.37039995]
[37m[1m [ 357.81019999    0.57249999    0.4104        0.3662        0.35209998]
[37m[1m ...
[37m[1m [ 359.84488295    0.32730001    0.48920003    0.25430003    0.42589998]
[37m[1m [1161.9400588     0.4765        0.37989998    0.2376        0.24399999]
[37m[1m [ 550.97940008    0.61470002    0.3766        0.3452        0.25489998]]
[37m[1m[2023-06-25 03:34:33,218][129146] Max Reward on eval: 1778.2848400516436
[37m[1m[2023-06-25 03:34:33,218][129146] Min Reward on eval: -513.612232354586
[37m[1m[2023-06-25 03:34:33,218][129146] Mean Reward across all agents: 541.6152881751229
[37m[1m[2023-06-25 03:34:33,219][129146] Average Trajectory Length: 991.156
[36m[2023-06-25 03:34:33,225][129146] mean_value=-50.54804661855715, max_value=1714.001342477555
[37m[1m[2023-06-25 03:34:33,227][129146] New mean coefficients: [[ 1.7142279   3.5606716   0.20924926  0.10411909 -0.21163887]]
[37m[1m[2023-06-25 03:34:33,228][129146] Moving the mean solution point...
[36m[2023-06-25 03:34:42,920][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 03:34:42,921][129146] FPS: 396264.26
[36m[2023-06-25 03:34:42,923][129146] itr=261, itrs=2000, Progress: 13.05%
[36m[2023-06-25 03:34:54,552][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 03:34:54,552][129146] FPS: 330637.27
[36m[2023-06-25 03:34:59,375][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:34:59,375][129146] Reward + Measures: [[2088.53269714    0.56165797    0.42479992    0.08125423    0.22302432]]
[37m[1m[2023-06-25 03:34:59,376][129146] Max Reward on eval: 2088.5326971427635
[37m[1m[2023-06-25 03:34:59,376][129146] Min Reward on eval: 2088.5326971427635
[37m[1m[2023-06-25 03:34:59,376][129146] Mean Reward across all agents: 2088.5326971427635
[37m[1m[2023-06-25 03:34:59,376][129146] Average Trajectory Length: 999.798
[36m[2023-06-25 03:35:04,896][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:35:04,897][129146] Reward + Measures: [[ 270.8411992     0.81449997    0.16360001    0.67020005    0.3346    ]
[37m[1m [ 704.96559478    0.76390004    0.40710002    0.50419998    0.16530001]
[37m[1m [-122.69218946    0.35401112    0.25378892    0.25214446    0.15442221]
[37m[1m ...
[37m[1m [ 520.42032909    0.70899999    0.31009999    0.47539997    0.3179    ]
[37m[1m [-280.41086393    0.34830004    0.27850002    0.2163        0.20030001]
[37m[1m [ 110.45141151    0.35773718    0.26702645    0.27652565    0.22872396]]
[37m[1m[2023-06-25 03:35:04,897][129146] Max Reward on eval: 1756.723374892515
[37m[1m[2023-06-25 03:35:04,897][129146] Min Reward on eval: -579.9523614153383
[37m[1m[2023-06-25 03:35:04,897][129146] Mean Reward across all agents: 452.8523059267403
[37m[1m[2023-06-25 03:35:04,897][129146] Average Trajectory Length: 997.54
[36m[2023-06-25 03:35:04,906][129146] mean_value=-100.30880955807272, max_value=1736.8201424066724
[37m[1m[2023-06-25 03:35:04,908][129146] New mean coefficients: [[ 1.4266472   3.638901    0.45102698  0.260153   -0.13211861]]
[37m[1m[2023-06-25 03:35:04,909][129146] Moving the mean solution point...
[36m[2023-06-25 03:35:14,686][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 03:35:14,687][129146] FPS: 392820.29
[36m[2023-06-25 03:35:14,689][129146] itr=262, itrs=2000, Progress: 13.10%
[36m[2023-06-25 03:35:26,204][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 03:35:26,204][129146] FPS: 333919.48
[36m[2023-06-25 03:35:30,885][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:35:30,885][129146] Reward + Measures: [[2163.13068906    0.58041364    0.41997799    0.08157333    0.21840401]]
[37m[1m[2023-06-25 03:35:30,885][129146] Max Reward on eval: 2163.13068905817
[37m[1m[2023-06-25 03:35:30,885][129146] Min Reward on eval: 2163.13068905817
[37m[1m[2023-06-25 03:35:30,886][129146] Mean Reward across all agents: 2163.13068905817
[37m[1m[2023-06-25 03:35:30,886][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:35:35,737][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:35:35,738][129146] Reward + Measures: [[157.38660109   0.60859996   0.48610002   0.40150005   0.4738    ]
[37m[1m [210.15599617   0.27589998   0.36770001   0.2234       0.31460002]
[37m[1m [249.34106986   0.81539994   0.48800001   0.67220002   0.18429999]
[37m[1m ...
[37m[1m [-80.78306571   0.93300003   0.2393       0.88240004   0.26799998]
[37m[1m [979.94943407   0.58339995   0.52859998   0.33090001   0.245     ]
[37m[1m [540.396531     0.49400002   0.43059999   0.1549       0.29959998]]
[37m[1m[2023-06-25 03:35:35,738][129146] Max Reward on eval: 1963.8369300665333
[37m[1m[2023-06-25 03:35:35,738][129146] Min Reward on eval: -563.7314308625239
[37m[1m[2023-06-25 03:35:35,739][129146] Mean Reward across all agents: 417.7035834248171
[37m[1m[2023-06-25 03:35:35,739][129146] Average Trajectory Length: 994.048
[36m[2023-06-25 03:35:35,751][129146] mean_value=61.22846460058053, max_value=2089.5440592801897
[37m[1m[2023-06-25 03:35:35,754][129146] New mean coefficients: [[ 1.485841    3.5039017   0.35306412  0.25081906 -0.27222404]]
[37m[1m[2023-06-25 03:35:35,755][129146] Moving the mean solution point...
[36m[2023-06-25 03:35:44,759][129146] train() took 9.00 seconds to complete
[36m[2023-06-25 03:35:44,759][129146] FPS: 426558.46
[36m[2023-06-25 03:35:44,762][129146] itr=263, itrs=2000, Progress: 13.15%
[36m[2023-06-25 03:35:56,304][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 03:35:56,304][129146] FPS: 333140.34
[36m[2023-06-25 03:36:01,099][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:36:01,100][129146] Reward + Measures: [[2249.44137791    0.59006298    0.42191568    0.07717167    0.21524133]]
[37m[1m[2023-06-25 03:36:01,100][129146] Max Reward on eval: 2249.4413779053352
[37m[1m[2023-06-25 03:36:01,100][129146] Min Reward on eval: 2249.4413779053352
[37m[1m[2023-06-25 03:36:01,100][129146] Mean Reward across all agents: 2249.4413779053352
[37m[1m[2023-06-25 03:36:01,101][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:36:06,615][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:36:06,616][129146] Reward + Measures: [[ 299.88069028    0.46520001    0.2863        0.41270003    0.2395    ]
[37m[1m [  65.06162876    0.76109999    0.65020001    0.66120005    0.1046    ]
[37m[1m [ -60.86091293    0.31350002    0.31130001    0.146         0.18090001]
[37m[1m ...
[37m[1m [1175.63625121    0.42360002    0.38960001    0.1213        0.2192    ]
[37m[1m [ 686.8120399     0.44420001    0.39300001    0.0495        0.27080002]
[37m[1m [ 278.87626359    0.57240003    0.3378        0.37050003    0.27200001]]
[37m[1m[2023-06-25 03:36:06,616][129146] Max Reward on eval: 2186.1523175963666
[37m[1m[2023-06-25 03:36:06,616][129146] Min Reward on eval: -346.4908480573096
[37m[1m[2023-06-25 03:36:06,617][129146] Mean Reward across all agents: 701.3690658085927
[37m[1m[2023-06-25 03:36:06,617][129146] Average Trajectory Length: 995.706
[36m[2023-06-25 03:36:06,624][129146] mean_value=-102.0808212494707, max_value=2686.1523175963666
[37m[1m[2023-06-25 03:36:06,626][129146] New mean coefficients: [[ 1.2992986   3.4377923  -0.16733712  0.23943573  0.07162786]]
[37m[1m[2023-06-25 03:36:06,627][129146] Moving the mean solution point...
[36m[2023-06-25 03:36:16,456][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 03:36:16,457][129146] FPS: 390745.97
[36m[2023-06-25 03:36:16,459][129146] itr=264, itrs=2000, Progress: 13.20%
[36m[2023-06-25 03:36:27,919][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 03:36:27,919][129146] FPS: 335557.57
[36m[2023-06-25 03:36:32,650][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:36:32,651][129146] Reward + Measures: [[2284.56717033    0.62370133    0.42715132    0.07564867    0.21467999]]
[37m[1m[2023-06-25 03:36:32,651][129146] Max Reward on eval: 2284.5671703302605
[37m[1m[2023-06-25 03:36:32,651][129146] Min Reward on eval: 2284.5671703302605
[37m[1m[2023-06-25 03:36:32,651][129146] Mean Reward across all agents: 2284.5671703302605
[37m[1m[2023-06-25 03:36:32,651][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:36:37,925][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:36:37,925][129146] Reward + Measures: [[1570.61351501    0.4305        0.45099998    0.09730001    0.24960001]
[37m[1m [1356.02232248    0.58579999    0.43469998    0.1514        0.24419999]
[37m[1m [1731.79260727    0.63030005    0.4454        0.1346        0.2077    ]
[37m[1m ...
[37m[1m [ 195.58067208    0.74270004    0.35110003    0.54800004    0.338     ]
[37m[1m [ 747.98249957    0.45705262    0.34017113    0.20675318    0.22309943]
[37m[1m [ 386.19263244    0.3714        0.3098        0.20160003    0.22930001]]
[37m[1m[2023-06-25 03:36:37,925][129146] Max Reward on eval: 2292.3168180794455
[37m[1m[2023-06-25 03:36:37,926][129146] Min Reward on eval: -753.6629352071207
[37m[1m[2023-06-25 03:36:37,926][129146] Mean Reward across all agents: 760.2661010426384
[37m[1m[2023-06-25 03:36:37,926][129146] Average Trajectory Length: 991.526
[36m[2023-06-25 03:36:37,933][129146] mean_value=17.6881859625647, max_value=2523.701586319809
[37m[1m[2023-06-25 03:36:37,936][129146] New mean coefficients: [[ 1.1414162   3.2939093  -0.34614688 -0.26133257  0.32417125]]
[37m[1m[2023-06-25 03:36:37,937][129146] Moving the mean solution point...
[36m[2023-06-25 03:36:47,608][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 03:36:47,608][129146] FPS: 397125.54
[36m[2023-06-25 03:36:47,611][129146] itr=265, itrs=2000, Progress: 13.25%
[36m[2023-06-25 03:36:59,061][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 03:36:59,061][129146] FPS: 335831.33
[36m[2023-06-25 03:37:03,900][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:37:03,901][129146] Reward + Measures: [[2312.24073521    0.65241867    0.41069165    0.08294667    0.20967866]]
[37m[1m[2023-06-25 03:37:03,901][129146] Max Reward on eval: 2312.2407352135374
[37m[1m[2023-06-25 03:37:03,901][129146] Min Reward on eval: 2312.2407352135374
[37m[1m[2023-06-25 03:37:03,901][129146] Mean Reward across all agents: 2312.2407352135374
[37m[1m[2023-06-25 03:37:03,902][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:37:09,424][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:37:09,425][129146] Reward + Measures: [[ 344.64629824    0.47329998    0.47839999    0.34459996    0.3996    ]
[37m[1m [2282.73412285    0.57499999    0.37699997    0.0545        0.2145    ]
[37m[1m [ 526.64650957    0.22950001    0.36830002    0.17749999    0.37830001]
[37m[1m ...
[37m[1m [ 783.52839343    0.64210004    0.43070003    0.3152        0.2349    ]
[37m[1m [ 546.87464074    0.36830002    0.2933        0.2184        0.28060001]
[37m[1m [1252.04692729    0.40149999    0.28819999    0.2191        0.26580003]]
[37m[1m[2023-06-25 03:37:09,425][129146] Max Reward on eval: 2282.7341228515143
[37m[1m[2023-06-25 03:37:09,425][129146] Min Reward on eval: -843.8756743050996
[37m[1m[2023-06-25 03:37:09,425][129146] Mean Reward across all agents: 1008.7777452991816
[37m[1m[2023-06-25 03:37:09,426][129146] Average Trajectory Length: 999.2796666666667
[36m[2023-06-25 03:37:09,434][129146] mean_value=263.70287535212503, max_value=2782.7341228515143
[37m[1m[2023-06-25 03:37:09,436][129146] New mean coefficients: [[ 1.0984136   3.3241177  -0.1901645   0.1616922   0.36873856]]
[37m[1m[2023-06-25 03:37:09,437][129146] Moving the mean solution point...
[36m[2023-06-25 03:37:19,186][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 03:37:19,186][129146] FPS: 393981.99
[36m[2023-06-25 03:37:19,188][129146] itr=266, itrs=2000, Progress: 13.30%
[36m[2023-06-25 03:37:30,620][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 03:37:30,620][129146] FPS: 336358.33
[36m[2023-06-25 03:37:35,555][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:37:35,555][129146] Reward + Measures: [[2368.88791419    0.66544831    0.41105899    0.08018766    0.20675102]]
[37m[1m[2023-06-25 03:37:35,555][129146] Max Reward on eval: 2368.8879141890884
[37m[1m[2023-06-25 03:37:35,556][129146] Min Reward on eval: 2368.8879141890884
[37m[1m[2023-06-25 03:37:35,556][129146] Mean Reward across all agents: 2368.8879141890884
[37m[1m[2023-06-25 03:37:35,556][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:37:41,001][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:37:41,002][129146] Reward + Measures: [[1882.11551391    0.54979998    0.433         0.0714        0.2177    ]
[37m[1m [1040.2549423     0.45609999    0.36900002    0.19860001    0.29260001]
[37m[1m [2100.99264301    0.62510002    0.4533        0.1015        0.22050002]
[37m[1m ...
[37m[1m [ 911.98732155    0.5620591     0.43553907    0.17912012    0.22152102]
[37m[1m [ 272.50181574    0.58840001    0.33860001    0.51550001    0.38649997]
[37m[1m [ 651.99651126    0.59470004    0.4052        0.3037        0.23699999]]
[37m[1m[2023-06-25 03:37:41,002][129146] Max Reward on eval: 2254.9758438604886
[37m[1m[2023-06-25 03:37:41,002][129146] Min Reward on eval: -694.841068432061
[37m[1m[2023-06-25 03:37:41,003][129146] Mean Reward across all agents: 850.7937341999386
[37m[1m[2023-06-25 03:37:41,003][129146] Average Trajectory Length: 994.5663333333333
[36m[2023-06-25 03:37:41,011][129146] mean_value=70.51144557592735, max_value=2258.343311514597
[37m[1m[2023-06-25 03:37:41,013][129146] New mean coefficients: [[1.1623619  2.9269626  0.11528713 0.18252131 0.35511902]]
[37m[1m[2023-06-25 03:37:41,014][129146] Moving the mean solution point...
[36m[2023-06-25 03:37:50,634][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 03:37:50,634][129146] FPS: 399263.98
[36m[2023-06-25 03:37:50,636][129146] itr=267, itrs=2000, Progress: 13.35%
[36m[2023-06-25 03:38:02,164][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 03:38:02,164][129146] FPS: 333589.57
[36m[2023-06-25 03:38:07,087][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:38:07,087][129146] Reward + Measures: [[2475.89690441    0.66226131    0.42593732    0.069329      0.20703532]]
[37m[1m[2023-06-25 03:38:07,087][129146] Max Reward on eval: 2475.8969044093014
[37m[1m[2023-06-25 03:38:07,087][129146] Min Reward on eval: 2475.8969044093014
[37m[1m[2023-06-25 03:38:07,088][129146] Mean Reward across all agents: 2475.8969044093014
[37m[1m[2023-06-25 03:38:07,088][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:38:12,755][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:38:12,756][129146] Reward + Measures: [[ 241.4587868     0.5564        0.44390002    0.3881        0.41420004]
[37m[1m [1065.56287246    0.53380007    0.35349998    0.15570001    0.2148    ]
[37m[1m [1270.6788715     0.42609999    0.3971        0.1596        0.2791    ]
[37m[1m ...
[37m[1m [1785.3763295     0.68809998    0.3558        0.0967        0.2023    ]
[37m[1m [1504.37586058    0.69630003    0.3479        0.1188        0.2024    ]
[37m[1m [2007.82290338    0.45550004    0.51450002    0.0797        0.24989998]]
[37m[1m[2023-06-25 03:38:12,756][129146] Max Reward on eval: 2405.2793191414557
[37m[1m[2023-06-25 03:38:12,756][129146] Min Reward on eval: -770.7721824257751
[37m[1m[2023-06-25 03:38:12,757][129146] Mean Reward across all agents: 1081.3451790959807
[37m[1m[2023-06-25 03:38:12,757][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:38:12,763][129146] mean_value=151.49739921958587, max_value=2716.0403523223476
[37m[1m[2023-06-25 03:38:12,766][129146] New mean coefficients: [[1.1640172  2.6686625  0.17618056 0.22445688 0.21199994]]
[37m[1m[2023-06-25 03:38:12,766][129146] Moving the mean solution point...
[36m[2023-06-25 03:38:22,552][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 03:38:22,552][129146] FPS: 392480.95
[36m[2023-06-25 03:38:22,555][129146] itr=268, itrs=2000, Progress: 13.40%
[36m[2023-06-25 03:38:34,121][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 03:38:34,121][129146] FPS: 332442.99
[36m[2023-06-25 03:38:38,907][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:38:38,908][129146] Reward + Measures: [[2561.88263038    0.66283965    0.43170768    0.06533567    0.20656498]]
[37m[1m[2023-06-25 03:38:38,908][129146] Max Reward on eval: 2561.8826303757924
[37m[1m[2023-06-25 03:38:38,908][129146] Min Reward on eval: 2561.8826303757924
[37m[1m[2023-06-25 03:38:38,908][129146] Mean Reward across all agents: 2561.8826303757924
[37m[1m[2023-06-25 03:38:38,909][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:38:44,344][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:38:44,345][129146] Reward + Measures: [[ 960.46017551    0.41189995    0.46479997    0.24609999    0.3788    ]
[37m[1m [2053.08918124    0.47          0.3955        0.10749999    0.23940001]
[37m[1m [ 243.4775969     0.49160004    0.45450002    0.4535        0.48990002]
[37m[1m ...
[37m[1m [1502.90886489    0.67799997    0.4876        0.17709999    0.18190001]
[37m[1m [  90.664665      0.52899998    0.37470004    0.58570004    0.54049999]
[37m[1m [1151.6313664     0.46070001    0.4296        0.23530002    0.32170001]]
[37m[1m[2023-06-25 03:38:44,345][129146] Max Reward on eval: 2256.504046339914
[37m[1m[2023-06-25 03:38:44,346][129146] Min Reward on eval: -464.19126707290997
[37m[1m[2023-06-25 03:38:44,346][129146] Mean Reward across all agents: 1136.2032502403708
[37m[1m[2023-06-25 03:38:44,346][129146] Average Trajectory Length: 999.4866666666667
[36m[2023-06-25 03:38:44,355][129146] mean_value=269.8460414624807, max_value=2465.1332859304175
[37m[1m[2023-06-25 03:38:44,358][129146] New mean coefficients: [[ 0.7601036   2.834168   -0.06807847  0.09539205  0.29195562]]
[37m[1m[2023-06-25 03:38:44,359][129146] Moving the mean solution point...
[36m[2023-06-25 03:38:54,164][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 03:38:54,164][129146] FPS: 391703.30
[36m[2023-06-25 03:38:54,166][129146] itr=269, itrs=2000, Progress: 13.45%
[36m[2023-06-25 03:39:05,848][129146] train() took 11.67 seconds to complete
[36m[2023-06-25 03:39:05,848][129146] FPS: 329155.56
[36m[2023-06-25 03:39:10,783][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:39:10,784][129146] Reward + Measures: [[2596.51890063    0.68208933    0.42195702    0.06348366    0.204327  ]]
[37m[1m[2023-06-25 03:39:10,784][129146] Max Reward on eval: 2596.518900634884
[37m[1m[2023-06-25 03:39:10,784][129146] Min Reward on eval: 2596.518900634884
[37m[1m[2023-06-25 03:39:10,785][129146] Mean Reward across all agents: 2596.518900634884
[37m[1m[2023-06-25 03:39:10,785][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:39:16,191][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:39:16,192][129146] Reward + Measures: [[1303.87784229    0.54610002    0.41879997    0.22649999    0.25      ]
[37m[1m [ 630.21199482    0.40369996    0.39980003    0.17639999    0.26350001]
[37m[1m [ 450.97002316    0.6182        0.37040001    0.5837        0.41489998]
[37m[1m ...
[37m[1m [ 495.44161923    0.43490002    0.40190002    0.22950001    0.27260002]
[37m[1m [1348.40168641    0.47419998    0.43320003    0.19710003    0.2431    ]
[37m[1m [ 264.07178093    0.46930003    0.39880002    0.4409        0.47670004]]
[37m[1m[2023-06-25 03:39:16,192][129146] Max Reward on eval: 2556.828071939456
[37m[1m[2023-06-25 03:39:16,193][129146] Min Reward on eval: -636.2271077438724
[37m[1m[2023-06-25 03:39:16,193][129146] Mean Reward across all agents: 1191.5886769356812
[37m[1m[2023-06-25 03:39:16,193][129146] Average Trajectory Length: 999.7026666666667
[36m[2023-06-25 03:39:16,201][129146] mean_value=129.62950303278916, max_value=2785.7874282374514
[37m[1m[2023-06-25 03:39:16,204][129146] New mean coefficients: [[1.2057043  2.9199245  0.12816744 0.33948994 0.3262729 ]]
[37m[1m[2023-06-25 03:39:16,205][129146] Moving the mean solution point...
[36m[2023-06-25 03:39:26,042][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 03:39:26,042][129146] FPS: 390432.76
[36m[2023-06-25 03:39:26,044][129146] itr=270, itrs=2000, Progress: 13.50%
[37m[1m[2023-06-25 03:39:29,540][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000250
[36m[2023-06-25 03:39:41,502][129146] train() took 11.65 seconds to complete
[36m[2023-06-25 03:39:41,503][129146] FPS: 329564.89
[36m[2023-06-25 03:39:46,308][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:39:46,308][129146] Reward + Measures: [[2655.20221831    0.68604428    0.42591399    0.06121267    0.20214133]]
[37m[1m[2023-06-25 03:39:46,309][129146] Max Reward on eval: 2655.202218305321
[37m[1m[2023-06-25 03:39:46,309][129146] Min Reward on eval: 2655.202218305321
[37m[1m[2023-06-25 03:39:46,309][129146] Mean Reward across all agents: 2655.202218305321
[37m[1m[2023-06-25 03:39:46,309][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:39:51,947][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:39:51,947][129146] Reward + Measures: [[ 543.67254046    0.53290004    0.35010001    0.2278        0.27320001]
[37m[1m [1114.88559631    0.4792268     0.38994333    0.0636244     0.28786692]
[37m[1m [1447.57778633    0.38679999    0.47960001    0.1776        0.22860001]
[37m[1m ...
[37m[1m [2049.28135584    0.53490001    0.42840001    0.09020001    0.20739999]
[37m[1m [1841.37040691    0.43390003    0.46479997    0.07480001    0.26190001]
[37m[1m [1127.97348101    0.77430004    0.43259999    0.43240005    0.2199    ]]
[37m[1m[2023-06-25 03:39:51,947][129146] Max Reward on eval: 2481.941627893876
[37m[1m[2023-06-25 03:39:51,948][129146] Min Reward on eval: -255.44652735798155
[37m[1m[2023-06-25 03:39:51,948][129146] Mean Reward across all agents: 1223.6742108243775
[37m[1m[2023-06-25 03:39:51,948][129146] Average Trajectory Length: 996.9309999999999
[36m[2023-06-25 03:39:51,954][129146] mean_value=22.603574267979276, max_value=2478.4882243599186
[37m[1m[2023-06-25 03:39:51,957][129146] New mean coefficients: [[ 1.2811856   2.4635606  -0.63551915 -0.12144846  0.41170993]]
[37m[1m[2023-06-25 03:39:51,958][129146] Moving the mean solution point...
[36m[2023-06-25 03:40:01,848][129146] train() took 9.89 seconds to complete
[36m[2023-06-25 03:40:01,848][129146] FPS: 388338.47
[36m[2023-06-25 03:40:01,850][129146] itr=271, itrs=2000, Progress: 13.55%
[36m[2023-06-25 03:40:13,426][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 03:40:13,426][129146] FPS: 332215.62
[36m[2023-06-25 03:40:18,235][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:40:18,235][129146] Reward + Measures: [[2732.05923517    0.68498397    0.42635238    0.06068967    0.20546266]]
[37m[1m[2023-06-25 03:40:18,236][129146] Max Reward on eval: 2732.059235168729
[37m[1m[2023-06-25 03:40:18,236][129146] Min Reward on eval: 2732.059235168729
[37m[1m[2023-06-25 03:40:18,236][129146] Mean Reward across all agents: 2732.059235168729
[37m[1m[2023-06-25 03:40:18,236][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:40:23,843][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:40:23,843][129146] Reward + Measures: [[1204.60447831    0.47770005    0.37040001    0.045         0.18990001]
[37m[1m [2098.03445114    0.54269999    0.3953        0.0699        0.24150001]
[37m[1m [ 247.53025561    0.47310001    0.62210006    0.3215        0.49400002]
[37m[1m ...
[37m[1m [1074.59138319    0.52669996    0.39240003    0.23370002    0.22390001]
[37m[1m [1307.53168671    0.57080001    0.44139996    0.18980001    0.2211    ]
[37m[1m [ 997.58307123    0.3946        0.33670002    0.1719        0.23710001]]
[37m[1m[2023-06-25 03:40:23,843][129146] Max Reward on eval: 2554.632109142328
[37m[1m[2023-06-25 03:40:23,844][129146] Min Reward on eval: -590.2088652080972
[37m[1m[2023-06-25 03:40:23,844][129146] Mean Reward across all agents: 1035.787099142663
[37m[1m[2023-06-25 03:40:23,844][129146] Average Trajectory Length: 998.7636666666666
[36m[2023-06-25 03:40:23,848][129146] mean_value=-346.783982955809, max_value=1731.1707145006035
[37m[1m[2023-06-25 03:40:23,850][129146] New mean coefficients: [[ 1.2895341   2.7747753  -0.10096616  0.04511291  0.10280302]]
[37m[1m[2023-06-25 03:40:23,851][129146] Moving the mean solution point...
[36m[2023-06-25 03:40:33,716][129146] train() took 9.86 seconds to complete
[36m[2023-06-25 03:40:33,717][129146] FPS: 389311.63
[36m[2023-06-25 03:40:33,719][129146] itr=272, itrs=2000, Progress: 13.60%
[36m[2023-06-25 03:40:45,222][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 03:40:45,222][129146] FPS: 334272.16
[36m[2023-06-25 03:40:50,012][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:40:50,012][129146] Reward + Measures: [[1267.24801512    0.56759131    0.42969698    0.20006834    0.22623734]]
[37m[1m[2023-06-25 03:40:50,013][129146] Max Reward on eval: 1267.2480151154457
[37m[1m[2023-06-25 03:40:50,013][129146] Min Reward on eval: 1267.2480151154457
[37m[1m[2023-06-25 03:40:50,013][129146] Mean Reward across all agents: 1267.2480151154457
[37m[1m[2023-06-25 03:40:50,014][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:40:55,437][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:40:55,438][129146] Reward + Measures: [[-337.54295347    0.37549999    0.2457        0.2057        0.18859999]
[37m[1m [ 761.96868636    0.42480001    0.45100003    0.20609999    0.2316    ]
[37m[1m [ 867.27693771    0.58310002    0.46750003    0.2278        0.20280002]
[37m[1m ...
[37m[1m [ 388.11203992    0.37729999    0.46180001    0.24250002    0.43309999]
[37m[1m [-237.54959308    0.55550003    0.3549        0.22880001    0.18789999]
[37m[1m [ 684.87685395    0.52629995    0.42039999    0.20829999    0.2141    ]]
[37m[1m[2023-06-25 03:40:55,438][129146] Max Reward on eval: 1460.28856343512
[37m[1m[2023-06-25 03:40:55,438][129146] Min Reward on eval: -508.89371664035133
[37m[1m[2023-06-25 03:40:55,438][129146] Mean Reward across all agents: 473.09919086727626
[37m[1m[2023-06-25 03:40:55,439][129146] Average Trajectory Length: 997.665
[36m[2023-06-25 03:40:55,441][129146] mean_value=-848.9601961477772, max_value=1720.9990748810505
[37m[1m[2023-06-25 03:40:55,443][129146] New mean coefficients: [[-0.07819319  2.2635841  -0.3958707  -0.0905782   0.339081  ]]
[37m[1m[2023-06-25 03:40:55,444][129146] Moving the mean solution point...
[36m[2023-06-25 03:41:05,115][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 03:41:05,115][129146] FPS: 397121.11
[36m[2023-06-25 03:41:05,118][129146] itr=273, itrs=2000, Progress: 13.65%
[36m[2023-06-25 03:41:16,634][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 03:41:16,634][129146] FPS: 333961.08
[36m[2023-06-25 03:41:21,406][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:41:21,406][129146] Reward + Measures: [[1191.99036159    0.62970465    0.41258833    0.20904267    0.22048567]]
[37m[1m[2023-06-25 03:41:21,406][129146] Max Reward on eval: 1191.9903615888068
[37m[1m[2023-06-25 03:41:21,407][129146] Min Reward on eval: 1191.9903615888068
[37m[1m[2023-06-25 03:41:21,407][129146] Mean Reward across all agents: 1191.9903615888068
[37m[1m[2023-06-25 03:41:21,407][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:41:26,954][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:41:26,955][129146] Reward + Measures: [[793.91287662   0.60669994   0.51419997   0.24259999   0.20149998]
[37m[1m [108.25296813   0.62110007   0.3486       0.37330002   0.25939998]
[37m[1m [122.21257001   0.34722182   0.27896449   0.16964953   0.24152999]
[37m[1m ...
[37m[1m [913.58760027   0.63199997   0.5478       0.24879999   0.1991    ]
[37m[1m [485.38454272   0.42280003   0.30679998   0.19130002   0.245     ]
[37m[1m [960.45380652   0.64379996   0.3698       0.25850001   0.20799999]]
[37m[1m[2023-06-25 03:41:26,955][129146] Max Reward on eval: 1509.900702948228
[37m[1m[2023-06-25 03:41:26,955][129146] Min Reward on eval: -893.0099754726631
[37m[1m[2023-06-25 03:41:26,955][129146] Mean Reward across all agents: 586.3005744021161
[37m[1m[2023-06-25 03:41:26,956][129146] Average Trajectory Length: 987.627
[36m[2023-06-25 03:41:26,958][129146] mean_value=-485.29231884086573, max_value=1532.6492162031354
[37m[1m[2023-06-25 03:41:26,961][129146] New mean coefficients: [[-0.6033031   3.1510372  -0.359589   -0.06434883  0.34198177]]
[37m[1m[2023-06-25 03:41:26,962][129146] Moving the mean solution point...
[36m[2023-06-25 03:41:36,603][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 03:41:36,603][129146] FPS: 398365.42
[36m[2023-06-25 03:41:36,606][129146] itr=274, itrs=2000, Progress: 13.70%
[36m[2023-06-25 03:41:48,116][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 03:41:48,116][129146] FPS: 334145.68
[36m[2023-06-25 03:41:52,418][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:41:52,419][129146] Reward + Measures: [[1059.47179576    0.66377234    0.39219299    0.21430431    0.214644  ]]
[37m[1m[2023-06-25 03:41:52,419][129146] Max Reward on eval: 1059.4717957631576
[37m[1m[2023-06-25 03:41:52,419][129146] Min Reward on eval: 1059.4717957631576
[37m[1m[2023-06-25 03:41:52,419][129146] Mean Reward across all agents: 1059.4717957631576
[37m[1m[2023-06-25 03:41:52,420][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:41:57,958][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:41:57,958][129146] Reward + Measures: [[530.44680959   0.66930002   0.48160002   0.39780003   0.167     ]
[37m[1m [256.91322833   0.52500004   0.4515       0.2534       0.26630002]
[37m[1m [657.31220643   0.51349998   0.30270001   0.20439999   0.19149999]
[37m[1m ...
[37m[1m [535.87871643   0.51550001   0.44919997   0.25190002   0.14220001]
[37m[1m [ 27.70780411   0.63239998   0.47300002   0.29840001   0.20630001]
[37m[1m [623.60253434   0.67930001   0.53899997   0.2211       0.2142    ]]
[37m[1m[2023-06-25 03:41:57,958][129146] Max Reward on eval: 1147.384209932934
[37m[1m[2023-06-25 03:41:57,959][129146] Min Reward on eval: -305.10175656846985
[37m[1m[2023-06-25 03:41:57,959][129146] Mean Reward across all agents: 457.3019864080145
[37m[1m[2023-06-25 03:41:57,959][129146] Average Trajectory Length: 946.8036666666667
[36m[2023-06-25 03:41:57,962][129146] mean_value=-383.600495830261, max_value=1195.2524059733346
[37m[1m[2023-06-25 03:41:57,965][129146] New mean coefficients: [[ 0.11160952  3.3045108  -0.14732675 -0.00978054  0.79691416]]
[37m[1m[2023-06-25 03:41:57,965][129146] Moving the mean solution point...
[36m[2023-06-25 03:42:07,659][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 03:42:07,659][129146] FPS: 396201.34
[36m[2023-06-25 03:42:07,662][129146] itr=275, itrs=2000, Progress: 13.75%
[36m[2023-06-25 03:42:19,083][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 03:42:19,083][129146] FPS: 336745.74
[36m[2023-06-25 03:42:23,885][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:42:23,886][129146] Reward + Measures: [[1029.82254049    0.69833332    0.38449234    0.21118265    0.21110664]]
[37m[1m[2023-06-25 03:42:23,886][129146] Max Reward on eval: 1029.8225404938507
[37m[1m[2023-06-25 03:42:23,886][129146] Min Reward on eval: 1029.8225404938507
[37m[1m[2023-06-25 03:42:23,886][129146] Mean Reward across all agents: 1029.8225404938507
[37m[1m[2023-06-25 03:42:23,887][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:42:29,295][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:42:29,295][129146] Reward + Measures: [[1213.95104571    0.45739999    0.4594        0.18310001    0.23439999]
[37m[1m [  85.14269881    0.48930001    0.41          0.4025        0.19560002]
[37m[1m [ 688.64899806    0.52229995    0.41290003    0.20110002    0.24370001]
[37m[1m ...
[37m[1m [ 335.72520565    0.36560002    0.53170007    0.22950001    0.40629998]
[37m[1m [ 894.24145244    0.46269998    0.37979999    0.22359999    0.23529999]
[37m[1m [ 719.43480188    0.42809996    0.41850001    0.2379        0.27600002]]
[37m[1m[2023-06-25 03:42:29,300][129146] Max Reward on eval: 1542.6866341294256
[37m[1m[2023-06-25 03:42:29,301][129146] Min Reward on eval: -411.1129509029677
[37m[1m[2023-06-25 03:42:29,301][129146] Mean Reward across all agents: 577.0481209506389
[37m[1m[2023-06-25 03:42:29,301][129146] Average Trajectory Length: 998.9076666666666
[36m[2023-06-25 03:42:29,304][129146] mean_value=-532.9292757382703, max_value=760.6804470821401
[37m[1m[2023-06-25 03:42:29,307][129146] New mean coefficients: [[1.2593086  3.258587   0.14625855 0.14978987 0.5586121 ]]
[37m[1m[2023-06-25 03:42:29,308][129146] Moving the mean solution point...
[36m[2023-06-25 03:42:39,017][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 03:42:39,017][129146] FPS: 395571.85
[36m[2023-06-25 03:42:39,020][129146] itr=276, itrs=2000, Progress: 13.80%
[36m[2023-06-25 03:42:50,670][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 03:42:50,670][129146] FPS: 330118.70
[36m[2023-06-25 03:42:55,524][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:42:55,524][129146] Reward + Measures: [[1095.10020023    0.72531801    0.38452932    0.21431567    0.208809  ]]
[37m[1m[2023-06-25 03:42:55,524][129146] Max Reward on eval: 1095.1002002277926
[37m[1m[2023-06-25 03:42:55,525][129146] Min Reward on eval: 1095.1002002277926
[37m[1m[2023-06-25 03:42:55,525][129146] Mean Reward across all agents: 1095.1002002277926
[37m[1m[2023-06-25 03:42:55,525][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:43:01,068][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:43:01,069][129146] Reward + Measures: [[ 349.94886129    0.67589998    0.4138        0.22660001    0.2036    ]
[37m[1m [ -26.08486881    0.34295473    0.30284926    0.33913311    0.29576164]
[37m[1m [ 115.88231915    0.48989996    0.32360002    0.29449999    0.24620001]
[37m[1m ...
[37m[1m [-195.64512116    0.39649996    0.31300002    0.41850001    0.38589999]
[37m[1m [ 451.98148036    0.41150004    0.37720001    0.2474        0.2467    ]
[37m[1m [1086.82988998    0.67329997    0.43660003    0.16950002    0.20299999]]
[37m[1m[2023-06-25 03:43:01,069][129146] Max Reward on eval: 1675.0471759220236
[37m[1m[2023-06-25 03:43:01,070][129146] Min Reward on eval: -759.675714127766
[37m[1m[2023-06-25 03:43:01,070][129146] Mean Reward across all agents: 461.20767965159166
[37m[1m[2023-06-25 03:43:01,070][129146] Average Trajectory Length: 995.2396666666666
[36m[2023-06-25 03:43:01,072][129146] mean_value=-597.8181092625865, max_value=741.9782636383757
[37m[1m[2023-06-25 03:43:01,075][129146] New mean coefficients: [[1.4313862  3.2343237  0.5460565  0.18109094 0.55339795]]
[37m[1m[2023-06-25 03:43:01,076][129146] Moving the mean solution point...
[36m[2023-06-25 03:43:10,945][129146] train() took 9.87 seconds to complete
[36m[2023-06-25 03:43:10,945][129146] FPS: 389137.84
[36m[2023-06-25 03:43:10,948][129146] itr=277, itrs=2000, Progress: 13.85%
[36m[2023-06-25 03:43:22,591][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 03:43:22,591][129146] FPS: 330287.13
[36m[2023-06-25 03:43:27,246][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:43:27,246][129146] Reward + Measures: [[319.4580895    0.76722425   0.54500133   0.67143768   0.070936  ]]
[37m[1m[2023-06-25 03:43:27,247][129146] Max Reward on eval: 319.458089496544
[37m[1m[2023-06-25 03:43:27,247][129146] Min Reward on eval: 319.458089496544
[37m[1m[2023-06-25 03:43:27,247][129146] Mean Reward across all agents: 319.458089496544
[37m[1m[2023-06-25 03:43:27,247][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:43:32,649][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:43:32,650][129146] Reward + Measures: [[  46.67464343    0.921         0.58649999    0.88199997    0.0802    ]
[37m[1m [-218.9752059     0.90100002    0.29210001    0.84669989    0.20479999]
[37m[1m [ 325.70477784    0.54759997    0.49060002    0.36089998    0.20840001]
[37m[1m ...
[37m[1m [  69.8959162     0.56689996    0.30889997    0.39850003    0.2271    ]
[37m[1m [-393.26701531    0.96490002    0.78380007    0.94769996    0.0281    ]
[37m[1m [  80.17502035    0.86630005    0.45619997    0.78229994    0.0612    ]]
[37m[1m[2023-06-25 03:43:32,650][129146] Max Reward on eval: 781.8204635256494
[37m[1m[2023-06-25 03:43:32,650][129146] Min Reward on eval: -674.3130751896009
[37m[1m[2023-06-25 03:43:32,650][129146] Mean Reward across all agents: 114.05755029635611
[37m[1m[2023-06-25 03:43:32,650][129146] Average Trajectory Length: 999.4879999999999
[36m[2023-06-25 03:43:32,655][129146] mean_value=-187.4866298676327, max_value=1281.8204635256493
[37m[1m[2023-06-25 03:43:32,658][129146] New mean coefficients: [[-0.4235251   3.1724355   0.33060756  0.16779548  0.35068774]]
[37m[1m[2023-06-25 03:43:32,659][129146] Moving the mean solution point...
[36m[2023-06-25 03:43:42,334][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 03:43:42,334][129146] FPS: 396956.33
[36m[2023-06-25 03:43:42,337][129146] itr=278, itrs=2000, Progress: 13.90%
[36m[2023-06-25 03:43:53,817][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 03:43:53,818][129146] FPS: 334969.85
[36m[2023-06-25 03:43:58,727][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:43:58,728][129146] Reward + Measures: [[0.94397735 0.97247362 0.96395296 0.98342872 0.00059633]]
[37m[1m[2023-06-25 03:43:58,728][129146] Max Reward on eval: 0.9439773463605137
[37m[1m[2023-06-25 03:43:58,728][129146] Min Reward on eval: 0.9439773463605137
[37m[1m[2023-06-25 03:43:58,728][129146] Mean Reward across all agents: 0.9439773463605137
[37m[1m[2023-06-25 03:43:58,728][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:44:04,186][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:44:04,186][129146] Reward + Measures: [[-113.96503193    0.9479        0.94810003    0.98880005    0.        ]
[37m[1m [ -10.75800877    0.48815686    0.34342691    0.46035466    0.25752163]
[37m[1m [-702.68943298    0.96870005    0.96859998    0.98549998    0.        ]
[37m[1m ...
[37m[1m [-611.82040056    0.77770007    0.82700008    0.93220007    0.0006    ]
[37m[1m [-220.52904228    0.77940005    0.7288        0.93529999    0.001     ]
[37m[1m [-324.19919782    0.64300007    0.66290003    0.79020005    0.0216    ]]
[37m[1m[2023-06-25 03:44:04,187][129146] Max Reward on eval: 492.798337557388
[37m[1m[2023-06-25 03:44:04,187][129146] Min Reward on eval: -1272.9064881812315
[37m[1m[2023-06-25 03:44:04,187][129146] Mean Reward across all agents: -61.376194094120436
[37m[1m[2023-06-25 03:44:04,187][129146] Average Trajectory Length: 997.0966666666666
[36m[2023-06-25 03:44:04,193][129146] mean_value=-198.03766096843927, max_value=688.4671009391291
[37m[1m[2023-06-25 03:44:04,195][129146] New mean coefficients: [[-1.2146101   3.5213778   0.29369813  0.3514276   0.20497938]]
[37m[1m[2023-06-25 03:44:04,196][129146] Moving the mean solution point...
[36m[2023-06-25 03:44:14,003][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 03:44:14,004][129146] FPS: 391627.32
[36m[2023-06-25 03:44:14,006][129146] itr=279, itrs=2000, Progress: 13.95%
[36m[2023-06-25 03:44:25,464][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 03:44:25,465][129146] FPS: 335586.28
[36m[2023-06-25 03:44:30,301][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:44:30,301][129146] Reward + Measures: [[-707.08363264    0.98576665    0.97507191    0.9907046     0.000038  ]]
[37m[1m[2023-06-25 03:44:30,302][129146] Max Reward on eval: -707.0836326355402
[37m[1m[2023-06-25 03:44:30,302][129146] Min Reward on eval: -707.0836326355402
[37m[1m[2023-06-25 03:44:30,302][129146] Mean Reward across all agents: -707.0836326355402
[37m[1m[2023-06-25 03:44:30,302][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:44:35,982][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:44:35,982][129146] Reward + Measures: [[-564.00097388    0.98270005    0.97229999    0.98790008    0.        ]
[37m[1m [-532.54499604    0.98070002    0.96970004    0.98969996    0.        ]
[37m[1m [-538.44973218    0.96310008    0.94510001    0.96470004    0.0126    ]
[37m[1m ...
[37m[1m [-982.22639761    0.97130007    0.95909995    0.98780006    0.        ]
[37m[1m [-669.6533161     0.84330004    0.75349993    0.86320001    0.0276    ]
[37m[1m [-510.74283112    0.98549998    0.96490002    0.99050009    0.0007    ]]
[37m[1m[2023-06-25 03:44:35,983][129146] Max Reward on eval: -229.45510971042094
[37m[1m[2023-06-25 03:44:35,983][129146] Min Reward on eval: -1044.982538574448
[37m[1m[2023-06-25 03:44:35,983][129146] Mean Reward across all agents: -622.2939077475853
[37m[1m[2023-06-25 03:44:35,983][129146] Average Trajectory Length: 999.5989999999999
[36m[2023-06-25 03:44:35,985][129146] mean_value=-760.3161232731063, max_value=256.42137408283304
[37m[1m[2023-06-25 03:44:35,987][129146] New mean coefficients: [[-2.907126    3.8355622   0.17924416  0.502118    0.19166084]]
[37m[1m[2023-06-25 03:44:35,988][129146] Moving the mean solution point...
[36m[2023-06-25 03:44:45,705][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 03:44:45,706][129146] FPS: 395233.60
[36m[2023-06-25 03:44:45,708][129146] itr=280, itrs=2000, Progress: 14.00%
[37m[1m[2023-06-25 03:44:49,215][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000260
[36m[2023-06-25 03:45:01,024][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 03:45:01,024][129146] FPS: 333777.04
[36m[2023-06-25 03:45:05,605][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:45:05,605][129146] Reward + Measures: [[-1397.56437953     0.98496759     0.97943538     0.99030101
[37m[1m      0.00000433]]
[37m[1m[2023-06-25 03:45:05,605][129146] Max Reward on eval: -1397.5643795312408
[37m[1m[2023-06-25 03:45:05,606][129146] Min Reward on eval: -1397.5643795312408
[37m[1m[2023-06-25 03:45:05,606][129146] Mean Reward across all agents: -1397.5643795312408
[37m[1m[2023-06-25 03:45:05,606][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:45:11,041][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:45:11,042][129146] Reward + Measures: [[-1532.83165689     0.91899997     0.89230007     0.83630002
[37m[1m      0.1657    ]
[37m[1m [-1551.43248694     0.87860006     0.92480004     0.64730006
[37m[1m      0.32539999]
[37m[1m [ -581.54140174     0.81070006     0.75220001     0.88520002
[37m[1m      0.0226    ]
[37m[1m ...
[37m[1m [-1565.50307084     0.98890001     0.9781         0.99069995
[37m[1m      0.0001    ]
[37m[1m [-1534.02465175     0.94390005     0.86630005     0.86490005
[37m[1m      0.16129999]
[37m[1m [-1287.33610471     0.92390007     0.93269998     0.81550008
[37m[1m      0.15910001]]
[37m[1m[2023-06-25 03:45:11,042][129146] Max Reward on eval: -581.541401739372
[37m[1m[2023-06-25 03:45:11,043][129146] Min Reward on eval: -2020.8199260722845
[37m[1m[2023-06-25 03:45:11,043][129146] Mean Reward across all agents: -1592.4515429774474
[37m[1m[2023-06-25 03:45:11,043][129146] Average Trajectory Length: 999.187
[36m[2023-06-25 03:45:11,045][129146] mean_value=-1554.6333600501196, max_value=-246.7906904796809
[36m[2023-06-25 03:45:11,047][129146] XNES is restarting with a new solution whose measures are [0.52810001 0.74110001 0.28780001 0.4587    ] and objective is 746.1909106878272
[36m[2023-06-25 03:45:11,048][129146] Emitter restarted. Changing the mean agent...
[37m[1m[2023-06-25 03:45:11,050][129146] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]]
[37m[1m[2023-06-25 03:45:11,051][129146] Moving the mean solution point...
[36m[2023-06-25 03:45:20,769][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 03:45:20,769][129146] FPS: 395205.97
[36m[2023-06-25 03:45:20,771][129146] itr=281, itrs=2000, Progress: 14.05%
[36m[2023-06-25 03:45:32,215][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 03:45:32,216][129146] FPS: 336067.88
[36m[2023-06-25 03:45:36,980][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:45:36,980][129146] Reward + Measures: [[767.10314129   0.67059267   0.750166     0.35662565   0.3946237 ]]
[37m[1m[2023-06-25 03:45:36,980][129146] Max Reward on eval: 767.1031412886618
[37m[1m[2023-06-25 03:45:36,980][129146] Min Reward on eval: 767.1031412886618
[37m[1m[2023-06-25 03:45:36,981][129146] Mean Reward across all agents: 767.1031412886618
[37m[1m[2023-06-25 03:45:36,981][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:45:42,362][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:45:42,362][129146] Reward + Measures: [[  804.62175178     0.41589999     0.66480005     0.11390001
[37m[1m      0.51380002]
[37m[1m [ -348.23976016     0.2148025      0.20420602     0.08161552
[37m[1m      0.18393199]
[37m[1m [ -565.71009452     0.23140001     0.20110002     0.074
[37m[1m      0.18970001]
[37m[1m ...
[37m[1m [  504.21101144     0.465          0.38050002     0.29850003
[37m[1m      0.2651    ]
[37m[1m [-1181.57952729     0.17894466     0.15416794     0.09206398
[37m[1m      0.17927356]
[37m[1m [ -540.84371659     0.24949999     0.17209999     0.1274
[37m[1m      0.1533    ]]
[37m[1m[2023-06-25 03:45:42,362][129146] Max Reward on eval: 1128.8827765628696
[37m[1m[2023-06-25 03:45:42,362][129146] Min Reward on eval: -1624.6607374892221
[37m[1m[2023-06-25 03:45:42,363][129146] Mean Reward across all agents: -263.7667079561053
[37m[1m[2023-06-25 03:45:42,363][129146] Average Trajectory Length: 936.2516666666667
[36m[2023-06-25 03:45:42,366][129146] mean_value=-1234.6247188435625, max_value=765.6280220936844
[37m[1m[2023-06-25 03:45:42,368][129146] New mean coefficients: [[ 0.37713236 -0.5939644   0.6282277  -0.05538452 -1.5063062 ]]
[37m[1m[2023-06-25 03:45:42,369][129146] Moving the mean solution point...
[36m[2023-06-25 03:45:51,990][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 03:45:51,991][129146] FPS: 399193.66
[36m[2023-06-25 03:45:51,993][129146] itr=282, itrs=2000, Progress: 14.10%
[36m[2023-06-25 03:46:03,468][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 03:46:03,468][129146] FPS: 335138.20
[36m[2023-06-25 03:46:08,293][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:46:08,293][129146] Reward + Measures: [[401.27744657   0.54519302   0.42184001   0.347635     0.34399834]]
[37m[1m[2023-06-25 03:46:08,293][129146] Max Reward on eval: 401.2774465659611
[37m[1m[2023-06-25 03:46:08,294][129146] Min Reward on eval: 401.2774465659611
[37m[1m[2023-06-25 03:46:08,294][129146] Mean Reward across all agents: 401.2774465659611
[37m[1m[2023-06-25 03:46:08,294][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:46:13,953][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:46:13,954][129146] Reward + Measures: [[-586.96208395    0.17039995    0.17541923    0.15695117    0.0988135 ]
[37m[1m [-334.14693931    0.63480002    0.22309999    0.491         0.14240001]
[37m[1m [-509.38886961    0.20500647    0.20761772    0.14876945    0.15745845]
[37m[1m ...
[37m[1m [-501.52676863    0.27047068    0.28981256    0.2654542     0.19484276]
[37m[1m [-555.38406515    0.21710001    0.25639999    0.14390002    0.20200001]
[37m[1m [ -66.47609109    0.28509998    0.32670003    0.25440001    0.2405    ]]
[37m[1m[2023-06-25 03:46:13,954][129146] Max Reward on eval: 693.2673183538485
[37m[1m[2023-06-25 03:46:13,954][129146] Min Reward on eval: -1357.1780799822068
[37m[1m[2023-06-25 03:46:13,955][129146] Mean Reward across all agents: -280.12522080777177
[37m[1m[2023-06-25 03:46:13,955][129146] Average Trajectory Length: 855.5526666666666
[36m[2023-06-25 03:46:13,957][129146] mean_value=-1034.4068504616896, max_value=877.3487415887299
[37m[1m[2023-06-25 03:46:13,959][129146] New mean coefficients: [[ 2.0379918  -0.7027402   1.0951648  -0.35185447  0.11118007]]
[37m[1m[2023-06-25 03:46:13,960][129146] Moving the mean solution point...
[36m[2023-06-25 03:46:23,765][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 03:46:23,765][129146] FPS: 391687.88
[36m[2023-06-25 03:46:23,768][129146] itr=283, itrs=2000, Progress: 14.15%
[36m[2023-06-25 03:46:35,356][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 03:46:35,356][129146] FPS: 331807.00
[36m[2023-06-25 03:46:40,092][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:46:40,093][129146] Reward + Measures: [[701.79398759   0.59453809   0.61041087   0.08845545   0.50619596]]
[37m[1m[2023-06-25 03:46:40,093][129146] Max Reward on eval: 701.7939875895241
[37m[1m[2023-06-25 03:46:40,093][129146] Min Reward on eval: 701.7939875895241
[37m[1m[2023-06-25 03:46:40,094][129146] Mean Reward across all agents: 701.7939875895241
[37m[1m[2023-06-25 03:46:40,094][129146] Average Trajectory Length: 999.4843333333333
[36m[2023-06-25 03:46:45,471][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:46:45,472][129146] Reward + Measures: [[  79.62047625    0.4237        0.47559997    0.1505        0.46310002]
[37m[1m [-539.62452028    0.21701801    0.21012418    0.21213828    0.20500088]
[37m[1m [ -52.3948407     0.31070003    0.4014        0.1576        0.35510001]
[37m[1m ...
[37m[1m [-580.9180447     0.28390118    0.20515423    0.16697705    0.15797769]
[37m[1m [-717.9016366     0.27706698    0.12964985    0.13622822    0.10108665]
[37m[1m [ -96.402438      0.41024572    0.32551369    0.20530081    0.23116903]]
[37m[1m[2023-06-25 03:46:45,472][129146] Max Reward on eval: 1016.2259882506362
[37m[1m[2023-06-25 03:46:45,472][129146] Min Reward on eval: -1326.4528091463144
[37m[1m[2023-06-25 03:46:45,472][129146] Mean Reward across all agents: -121.90909974892178
[37m[1m[2023-06-25 03:46:45,473][129146] Average Trajectory Length: 941.134
[36m[2023-06-25 03:46:45,475][129146] mean_value=-1061.8732178726643, max_value=1156.5779754416667
[37m[1m[2023-06-25 03:46:45,478][129146] New mean coefficients: [[ 0.9903414  -0.51475656  1.2502395   0.2916771   1.2525482 ]]
[37m[1m[2023-06-25 03:46:45,478][129146] Moving the mean solution point...
[36m[2023-06-25 03:46:55,212][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 03:46:55,212][129146] FPS: 394579.72
[36m[2023-06-25 03:46:55,215][129146] itr=284, itrs=2000, Progress: 14.20%
[36m[2023-06-25 03:47:06,837][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 03:47:06,837][129146] FPS: 330872.38
[36m[2023-06-25 03:47:11,757][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:47:11,757][129146] Reward + Measures: [[1016.48109172    0.46264568    0.54826796    0.15873933    0.37527335]]
[37m[1m[2023-06-25 03:47:11,757][129146] Max Reward on eval: 1016.48109172406
[37m[1m[2023-06-25 03:47:11,758][129146] Min Reward on eval: 1016.48109172406
[37m[1m[2023-06-25 03:47:11,758][129146] Mean Reward across all agents: 1016.48109172406
[37m[1m[2023-06-25 03:47:11,758][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:47:17,324][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:47:17,325][129146] Reward + Measures: [[-708.17674034    0.27179635    0.24172269    0.17690656    0.16615184]
[37m[1m [ 711.49375762    0.41480002    0.39750001    0.17200001    0.22980002]
[37m[1m [-790.18003327    0.23474741    0.22550905    0.18220012    0.19200952]
[37m[1m ...
[37m[1m [-502.04948442    0.29258224    0.23816888    0.24323367    0.14795777]
[37m[1m [ 732.49902895    0.40980005    0.44700003    0.1849        0.366     ]
[37m[1m [ -94.98653228    0.50326985    0.41503817    0.40887189    0.36271256]]
[37m[1m[2023-06-25 03:47:17,325][129146] Max Reward on eval: 1022.5238696992513
[37m[1m[2023-06-25 03:47:17,325][129146] Min Reward on eval: -1147.471486541559
[37m[1m[2023-06-25 03:47:17,326][129146] Mean Reward across all agents: -163.65443323243977
[37m[1m[2023-06-25 03:47:17,326][129146] Average Trajectory Length: 917.135
[36m[2023-06-25 03:47:17,328][129146] mean_value=-997.0727001590872, max_value=891.7206175090012
[37m[1m[2023-06-25 03:47:17,330][129146] New mean coefficients: [[ 0.76120466  0.62067235  0.7207499  -0.51150835  1.5317132 ]]
[37m[1m[2023-06-25 03:47:17,331][129146] Moving the mean solution point...
[36m[2023-06-25 03:47:27,019][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 03:47:27,019][129146] FPS: 396431.22
[36m[2023-06-25 03:47:27,021][129146] itr=285, itrs=2000, Progress: 14.25%
[36m[2023-06-25 03:47:38,469][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 03:47:38,469][129146] FPS: 335897.75
[36m[2023-06-25 03:47:43,332][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:47:43,332][129146] Reward + Measures: [[1014.50055503    0.47683331    0.564022      0.14364468    0.40965432]]
[37m[1m[2023-06-25 03:47:43,332][129146] Max Reward on eval: 1014.5005550264831
[37m[1m[2023-06-25 03:47:43,332][129146] Min Reward on eval: 1014.5005550264831
[37m[1m[2023-06-25 03:47:43,333][129146] Mean Reward across all agents: 1014.5005550264831
[37m[1m[2023-06-25 03:47:43,333][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:47:48,739][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:47:48,740][129146] Reward + Measures: [[ 673.58707202    0.51230001    0.55579996    0.21700001    0.41139999]
[37m[1m [ -91.1279295     0.41370001    0.27959999    0.25710002    0.2304    ]
[37m[1m [1054.36750457    0.40019998    0.48800001    0.12320001    0.35929999]
[37m[1m ...
[37m[1m [ 276.35015053    0.51540005    0.37509999    0.3585        0.39499998]
[37m[1m [-300.80735434    0.34300002    0.62560004    0.54750007    0.54869998]
[37m[1m [ 320.93912424    0.24130002    0.30900002    0.1013        0.26359996]]
[37m[1m[2023-06-25 03:47:48,740][129146] Max Reward on eval: 1061.7633025063842
[37m[1m[2023-06-25 03:47:48,740][129146] Min Reward on eval: -1212.0786226998898
[37m[1m[2023-06-25 03:47:48,740][129146] Mean Reward across all agents: 30.605731513348402
[37m[1m[2023-06-25 03:47:48,741][129146] Average Trajectory Length: 967.9483333333333
[36m[2023-06-25 03:47:48,744][129146] mean_value=-711.747218547505, max_value=761.1197129188827
[37m[1m[2023-06-25 03:47:48,746][129146] New mean coefficients: [[ 2.0187523   0.49190944  0.65744776 -0.29152524  1.6289428 ]]
[37m[1m[2023-06-25 03:47:48,747][129146] Moving the mean solution point...
[36m[2023-06-25 03:47:58,368][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 03:47:58,368][129146] FPS: 399192.44
[36m[2023-06-25 03:47:58,370][129146] itr=286, itrs=2000, Progress: 14.30%
[36m[2023-06-25 03:48:09,854][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 03:48:09,854][129146] FPS: 334851.75
[36m[2023-06-25 03:48:14,675][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:48:14,681][129146] Reward + Measures: [[1032.18160164    0.48500332    0.56832063    0.13865867    0.41477466]]
[37m[1m[2023-06-25 03:48:14,681][129146] Max Reward on eval: 1032.1816016442403
[37m[1m[2023-06-25 03:48:14,681][129146] Min Reward on eval: 1032.1816016442403
[37m[1m[2023-06-25 03:48:14,682][129146] Mean Reward across all agents: 1032.1816016442403
[37m[1m[2023-06-25 03:48:14,682][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:48:20,265][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:48:20,266][129146] Reward + Measures: [[  546.51236406     0.419          0.49070001     0.14219999
[37m[1m      0.49130002]
[37m[1m [ -292.49437063     0.2218812      0.22299023     0.20513232
[37m[1m      0.20766543]
[37m[1m [-1257.37755941     0.46310958     0.26797235     0.38603511
[37m[1m      0.20503192]
[37m[1m ...
[37m[1m [   75.8213212      0.55220002     0.66600001     0.0837
[37m[1m      0.63770002]
[37m[1m [  371.00167652     0.3646         0.46870002     0.20390001
[37m[1m      0.31830001]
[37m[1m [ -182.79481033     0.45440003     0.53679991     0.1779
[37m[1m      0.47840005]]
[37m[1m[2023-06-25 03:48:20,266][129146] Max Reward on eval: 1141.7840139894397
[37m[1m[2023-06-25 03:48:20,266][129146] Min Reward on eval: -1257.3775594110134
[37m[1m[2023-06-25 03:48:20,267][129146] Mean Reward across all agents: 101.45324087712837
[37m[1m[2023-06-25 03:48:20,267][129146] Average Trajectory Length: 931.3186666666667
[36m[2023-06-25 03:48:20,270][129146] mean_value=-629.1177498778356, max_value=940.0643390423173
[37m[1m[2023-06-25 03:48:20,273][129146] New mean coefficients: [[ 0.942518    1.0786835   0.47185308 -0.57420886  1.2548057 ]]
[37m[1m[2023-06-25 03:48:20,274][129146] Moving the mean solution point...
[36m[2023-06-25 03:48:30,114][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 03:48:30,114][129146] FPS: 390325.72
[36m[2023-06-25 03:48:30,116][129146] itr=287, itrs=2000, Progress: 14.35%
[36m[2023-06-25 03:48:41,531][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 03:48:41,531][129146] FPS: 336912.45
[36m[2023-06-25 03:48:46,332][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:48:46,332][129146] Reward + Measures: [[1044.58937908    0.50467128    0.58006567    0.12915367    0.43764466]]
[37m[1m[2023-06-25 03:48:46,332][129146] Max Reward on eval: 1044.589379082366
[37m[1m[2023-06-25 03:48:46,333][129146] Min Reward on eval: 1044.589379082366
[37m[1m[2023-06-25 03:48:46,333][129146] Mean Reward across all agents: 1044.589379082366
[37m[1m[2023-06-25 03:48:46,333][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:48:51,867][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:48:51,873][129146] Reward + Measures: [[ 334.83557508    0.46650001    0.50929999    0.1999        0.3502    ]
[37m[1m [ 120.26605295    0.64900005    0.72140008    0.0572        0.68160003]
[37m[1m [-447.40809853    0.20950806    0.24430101    0.20754218    0.17345253]
[37m[1m ...
[37m[1m [-390.66336597    0.24138017    0.34360534    0.15763772    0.12841311]
[37m[1m [ -92.5774377     0.22543052    0.26771197    0.23711573    0.18545409]
[37m[1m [-351.24488823    0.26202205    0.22006078    0.17346179    0.2098964 ]]
[37m[1m[2023-06-25 03:48:51,873][129146] Max Reward on eval: 1132.0146095908713
[37m[1m[2023-06-25 03:48:51,873][129146] Min Reward on eval: -875.3001371541293
[37m[1m[2023-06-25 03:48:51,874][129146] Mean Reward across all agents: 271.9476373756696
[37m[1m[2023-06-25 03:48:51,874][129146] Average Trajectory Length: 855.81
[36m[2023-06-25 03:48:51,877][129146] mean_value=-524.5984673847245, max_value=1442.0747998136328
[37m[1m[2023-06-25 03:48:51,880][129146] New mean coefficients: [[ 0.33886725  1.0823036   0.05915669 -0.26356417  0.9330696 ]]
[37m[1m[2023-06-25 03:48:51,881][129146] Moving the mean solution point...
[36m[2023-06-25 03:49:01,550][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 03:49:01,550][129146] FPS: 397212.70
[36m[2023-06-25 03:49:01,552][129146] itr=288, itrs=2000, Progress: 14.40%
[36m[2023-06-25 03:49:12,993][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 03:49:12,994][129146] FPS: 336091.01
[36m[2023-06-25 03:49:17,787][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:49:17,788][129146] Reward + Measures: [[998.06473126   0.54804933   0.61528999   0.10966901   0.50233567]]
[37m[1m[2023-06-25 03:49:17,788][129146] Max Reward on eval: 998.0647312581182
[37m[1m[2023-06-25 03:49:17,788][129146] Min Reward on eval: 998.0647312581182
[37m[1m[2023-06-25 03:49:17,788][129146] Mean Reward across all agents: 998.0647312581182
[37m[1m[2023-06-25 03:49:17,789][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:49:23,234][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:49:23,234][129146] Reward + Measures: [[ 448.01967688    0.33100003    0.4499        0.1926        0.34279999]
[37m[1m [ 869.42160243    0.50130004    0.60550004    0.1586        0.4262    ]
[37m[1m [  83.85834877    0.46650001    0.37610003    0.28929999    0.3114    ]
[37m[1m ...
[37m[1m [-311.92139483    0.22217737    0.31506863    0.19413796    0.39762774]
[37m[1m [-704.1033173     0.3152        0.18450001    0.1938        0.15100001]
[37m[1m [  78.77503397    0.31860003    0.35250002    0.228         0.3256    ]]
[37m[1m[2023-06-25 03:49:23,234][129146] Max Reward on eval: 991.0029937663581
[37m[1m[2023-06-25 03:49:23,235][129146] Min Reward on eval: -1124.1569031727036
[37m[1m[2023-06-25 03:49:23,235][129146] Mean Reward across all agents: 87.1724956239634
[37m[1m[2023-06-25 03:49:23,235][129146] Average Trajectory Length: 983.102
[36m[2023-06-25 03:49:23,238][129146] mean_value=-741.7930076051196, max_value=1089.5413893430027
[37m[1m[2023-06-25 03:49:23,241][129146] New mean coefficients: [[ 0.00122684  1.0729387  -0.52540874  0.38419217  0.21534479]]
[37m[1m[2023-06-25 03:49:23,242][129146] Moving the mean solution point...
[36m[2023-06-25 03:49:32,902][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 03:49:32,903][129146] FPS: 397560.80
[36m[2023-06-25 03:49:32,905][129146] itr=289, itrs=2000, Progress: 14.45%
[36m[2023-06-25 03:49:44,376][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 03:49:44,377][129146] FPS: 335190.69
[36m[2023-06-25 03:49:49,197][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:49:49,197][129146] Reward + Measures: [[862.35323541   0.64123946   0.68732625   0.08389232   0.61588967]]
[37m[1m[2023-06-25 03:49:49,198][129146] Max Reward on eval: 862.3532354129786
[37m[1m[2023-06-25 03:49:49,198][129146] Min Reward on eval: 862.3532354129786
[37m[1m[2023-06-25 03:49:49,198][129146] Mean Reward across all agents: 862.3532354129786
[37m[1m[2023-06-25 03:49:49,199][129146] Average Trajectory Length: 999.752
[36m[2023-06-25 03:49:54,711][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:49:54,712][129146] Reward + Measures: [[ 798.07318686    0.41930005    0.46939999    0.16580001    0.32880002]
[37m[1m [ 257.87059452    0.73110002    0.7913        0.0532        0.75699997]
[37m[1m [ 143.47327275    0.52019995    0.44250003    0.32530001    0.46110001]
[37m[1m ...
[37m[1m [ 705.11011665    0.42070004    0.46560001    0.2149        0.47100002]
[37m[1m [ -20.09001781    0.6415        0.70290005    0.10649999    0.57460004]
[37m[1m [-666.5198504     0.66262168    0.75163478    0.06878261    0.64650005]]
[37m[1m[2023-06-25 03:49:54,712][129146] Max Reward on eval: 1100.1873862938955
[37m[1m[2023-06-25 03:49:54,713][129146] Min Reward on eval: -1041.67339750611
[37m[1m[2023-06-25 03:49:54,713][129146] Mean Reward across all agents: 217.2940827666929
[37m[1m[2023-06-25 03:49:54,713][129146] Average Trajectory Length: 982.8946666666666
[36m[2023-06-25 03:49:54,718][129146] mean_value=-457.28885387768486, max_value=953.1614072861724
[37m[1m[2023-06-25 03:49:54,720][129146] New mean coefficients: [[-0.08604847  0.70960605 -0.84547293  0.18257563  0.7159703 ]]
[37m[1m[2023-06-25 03:49:54,722][129146] Moving the mean solution point...
[36m[2023-06-25 03:50:04,657][129146] train() took 9.93 seconds to complete
[36m[2023-06-25 03:50:04,658][129146] FPS: 386542.74
[36m[2023-06-25 03:50:04,660][129146] itr=290, itrs=2000, Progress: 14.50%
[37m[1m[2023-06-25 03:50:08,370][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000270
[36m[2023-06-25 03:50:20,188][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 03:50:20,188][129146] FPS: 333517.20
[36m[2023-06-25 03:50:24,967][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:50:24,968][129146] Reward + Measures: [[810.39373741   0.67048234   0.70856136   0.08447032   0.64581901]]
[37m[1m[2023-06-25 03:50:24,968][129146] Max Reward on eval: 810.3937374071802
[37m[1m[2023-06-25 03:50:24,968][129146] Min Reward on eval: 810.3937374071802
[37m[1m[2023-06-25 03:50:24,969][129146] Mean Reward across all agents: 810.3937374071802
[37m[1m[2023-06-25 03:50:24,969][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:50:30,394][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:50:30,395][129146] Reward + Measures: [[ -460.54447383     0.26088181     0.25021821     0.17789352
[37m[1m      0.14402667]
[37m[1m [ -262.29374465     0.27574983     0.26974115     0.19495554
[37m[1m      0.10802223]
[37m[1m [ -228.97717612     0.23461683     0.24035573     0.15788738
[37m[1m      0.13352387]
[37m[1m ...
[37m[1m [  -24.76651302     0.31890002     0.3303         0.21230002
[37m[1m      0.20539999]
[37m[1m [ -274.08637104     0.25703534     0.26317191     0.18918546
[37m[1m      0.15758677]
[37m[1m [-1499.16483751     0.78109998     0.52890003     0.72630006
[37m[1m      0.1734    ]]
[37m[1m[2023-06-25 03:50:30,395][129146] Max Reward on eval: 1106.7087614190066
[37m[1m[2023-06-25 03:50:30,395][129146] Min Reward on eval: -1536.6928245021495
[37m[1m[2023-06-25 03:50:30,396][129146] Mean Reward across all agents: -70.40461017673992
[37m[1m[2023-06-25 03:50:30,396][129146] Average Trajectory Length: 920.641
[36m[2023-06-25 03:50:30,401][129146] mean_value=-516.3474718132655, max_value=999.1674651125562
[37m[1m[2023-06-25 03:50:30,404][129146] New mean coefficients: [[-0.26676688  0.42411843 -0.7567962   0.48926818  0.64729655]]
[37m[1m[2023-06-25 03:50:30,405][129146] Moving the mean solution point...
[36m[2023-06-25 03:50:40,228][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 03:50:40,228][129146] FPS: 390979.58
[36m[2023-06-25 03:50:40,230][129146] itr=291, itrs=2000, Progress: 14.55%
[36m[2023-06-25 03:50:51,739][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 03:50:51,740][129146] FPS: 334194.63
[36m[2023-06-25 03:50:56,553][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:50:56,554][129146] Reward + Measures: [[681.77645682   0.7220307    0.73738104   0.07657433   0.71056932]]
[37m[1m[2023-06-25 03:50:56,554][129146] Max Reward on eval: 681.7764568220463
[37m[1m[2023-06-25 03:50:56,554][129146] Min Reward on eval: 681.7764568220463
[37m[1m[2023-06-25 03:50:56,554][129146] Mean Reward across all agents: 681.7764568220463
[37m[1m[2023-06-25 03:50:56,555][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:51:02,131][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:51:02,131][129146] Reward + Measures: [[404.12400864   0.41050002   0.31310001   0.29990003   0.2498    ]
[37m[1m [603.85076247   0.54570001   0.48899999   0.37830001   0.22720002]
[37m[1m [740.90068989   0.4501       0.47470003   0.2545       0.27069998]
[37m[1m ...
[37m[1m [844.40234781   0.4269       0.51730007   0.21370001   0.37180001]
[37m[1m [583.95239672   0.35610002   0.51599997   0.13759999   0.3414    ]
[37m[1m [629.50620241   0.52349997   0.42990002   0.22090001   0.31150001]]
[37m[1m[2023-06-25 03:51:02,131][129146] Max Reward on eval: 1031.8839034690056
[37m[1m[2023-06-25 03:51:02,132][129146] Min Reward on eval: -302.4161768877064
[37m[1m[2023-06-25 03:51:02,132][129146] Mean Reward across all agents: 469.0692230125319
[37m[1m[2023-06-25 03:51:02,132][129146] Average Trajectory Length: 992.4266666666666
[36m[2023-06-25 03:51:02,136][129146] mean_value=-292.1031893312279, max_value=1075.2705535270122
[37m[1m[2023-06-25 03:51:02,139][129146] New mean coefficients: [[ 0.01511234  0.03192297 -1.5559951   0.28252554  0.71026975]]
[37m[1m[2023-06-25 03:51:02,140][129146] Moving the mean solution point...
[36m[2023-06-25 03:51:11,981][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 03:51:11,981][129146] FPS: 390256.66
[36m[2023-06-25 03:51:11,984][129146] itr=292, itrs=2000, Progress: 14.60%
[36m[2023-06-25 03:51:23,637][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 03:51:23,637][129146] FPS: 330039.53
[36m[2023-06-25 03:51:28,475][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:51:28,476][129146] Reward + Measures: [[656.7588738    0.70541203   0.71270734   0.07634667   0.70418203]]
[37m[1m[2023-06-25 03:51:28,476][129146] Max Reward on eval: 656.7588737983374
[37m[1m[2023-06-25 03:51:28,476][129146] Min Reward on eval: 656.7588737983374
[37m[1m[2023-06-25 03:51:28,476][129146] Mean Reward across all agents: 656.7588737983374
[37m[1m[2023-06-25 03:51:28,476][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:51:33,908][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:51:33,913][129146] Reward + Measures: [[705.14767025   0.56800002   0.53740001   0.12980001   0.49130002]
[37m[1m [130.53101849   0.43730003   0.64169997   0.28490001   0.57929999]
[37m[1m [393.33120584   0.42350003   0.39970002   0.21130002   0.294     ]
[37m[1m ...
[37m[1m [110.44859114   0.50229996   0.41960001   0.40459999   0.41570002]
[37m[1m [304.81887983   0.37576595   0.40923095   0.22820316   0.36045191]
[37m[1m [517.82290219   0.55669999   0.79030001   0.1056       0.73629999]]
[37m[1m[2023-06-25 03:51:33,913][129146] Max Reward on eval: 1133.8688831424574
[37m[1m[2023-06-25 03:51:33,914][129146] Min Reward on eval: -658.5357854676607
[37m[1m[2023-06-25 03:51:33,914][129146] Mean Reward across all agents: 331.30736025188355
[37m[1m[2023-06-25 03:51:33,914][129146] Average Trajectory Length: 988.4276666666666
[36m[2023-06-25 03:51:33,920][129146] mean_value=-109.93998061144516, max_value=1123.5726292740437
[37m[1m[2023-06-25 03:51:33,923][129146] New mean coefficients: [[ 0.17930372 -0.04612186 -1.6183136   0.03607734  1.2564878 ]]
[37m[1m[2023-06-25 03:51:33,924][129146] Moving the mean solution point...
[36m[2023-06-25 03:51:43,744][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 03:51:43,744][129146] FPS: 391122.39
[36m[2023-06-25 03:51:43,747][129146] itr=293, itrs=2000, Progress: 14.65%
[36m[2023-06-25 03:51:55,297][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 03:51:55,297][129146] FPS: 332910.55
[36m[2023-06-25 03:52:00,181][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:52:00,182][129146] Reward + Measures: [[629.40536966   0.65888166   0.6869117    0.071105     0.71337694]]
[37m[1m[2023-06-25 03:52:00,182][129146] Max Reward on eval: 629.4053696594544
[37m[1m[2023-06-25 03:52:00,182][129146] Min Reward on eval: 629.4053696594544
[37m[1m[2023-06-25 03:52:00,183][129146] Mean Reward across all agents: 629.4053696594544
[37m[1m[2023-06-25 03:52:00,183][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:52:05,711][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:52:05,712][129146] Reward + Measures: [[ 952.1800071     0.38820001    0.56750005    0.14040001    0.49670002]
[37m[1m [-101.46946557    0.52969998    0.54490006    0.0872        0.56940001]
[37m[1m [ 545.20419542    0.32319999    0.66510004    0.14310001    0.62169999]
[37m[1m ...
[37m[1m [ 356.15913514    0.3908        0.69940001    0.06870001    0.77910006]
[37m[1m [ 581.36436925    0.29319999    0.48750001    0.19410001    0.38000003]
[37m[1m [ -16.17251255    0.23290001    0.46160004    0.0619        0.4666    ]]
[37m[1m[2023-06-25 03:52:05,712][129146] Max Reward on eval: 1104.6402649081779
[37m[1m[2023-06-25 03:52:05,712][129146] Min Reward on eval: -1209.6175135084195
[37m[1m[2023-06-25 03:52:05,712][129146] Mean Reward across all agents: 343.5971303244502
[37m[1m[2023-06-25 03:52:05,712][129146] Average Trajectory Length: 989.0613333333333
[36m[2023-06-25 03:52:05,722][129146] mean_value=68.0006823043202, max_value=1108.733660048165
[37m[1m[2023-06-25 03:52:05,724][129146] New mean coefficients: [[ 0.40791    -0.33619672 -1.5710366  -0.35362273  1.1458875 ]]
[37m[1m[2023-06-25 03:52:05,725][129146] Moving the mean solution point...
[36m[2023-06-25 03:52:15,493][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 03:52:15,493][129146] FPS: 393200.98
[36m[2023-06-25 03:52:15,495][129146] itr=294, itrs=2000, Progress: 14.70%
[36m[2023-06-25 03:52:27,049][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 03:52:27,049][129146] FPS: 332869.37
[36m[2023-06-25 03:52:31,869][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:52:31,875][129146] Reward + Measures: [[626.84075888   0.61224318   0.65483266   0.07676958   0.68913656]]
[37m[1m[2023-06-25 03:52:31,876][129146] Max Reward on eval: 626.840758880237
[37m[1m[2023-06-25 03:52:31,876][129146] Min Reward on eval: 626.840758880237
[37m[1m[2023-06-25 03:52:31,877][129146] Mean Reward across all agents: 626.840758880237
[37m[1m[2023-06-25 03:52:31,877][129146] Average Trajectory Length: 999.765
[36m[2023-06-25 03:52:37,509][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:52:37,509][129146] Reward + Measures: [[355.5390284    0.3822       0.51599997   0.18730001   0.41240001]
[37m[1m [494.25685151   0.87110007   0.86370003   0.0464       0.83850002]
[37m[1m [479.22709668   0.58790004   0.72080004   0.0558       0.71180004]
[37m[1m ...
[37m[1m [333.3607089    0.78000003   0.79450005   0.0731       0.74169999]
[37m[1m [556.82072185   0.73879999   0.7191       0.066        0.7529    ]
[37m[1m [533.00831646   0.67580003   0.76280004   0.0526       0.79520005]]
[37m[1m[2023-06-25 03:52:37,509][129146] Max Reward on eval: 873.5880198506871
[37m[1m[2023-06-25 03:52:37,509][129146] Min Reward on eval: -348.81060778562386
[37m[1m[2023-06-25 03:52:37,510][129146] Mean Reward across all agents: 397.6494146626332
[37m[1m[2023-06-25 03:52:37,510][129146] Average Trajectory Length: 986.4723333333333
[36m[2023-06-25 03:52:37,515][129146] mean_value=-86.54849156652296, max_value=1055.3277395142452
[37m[1m[2023-06-25 03:52:37,517][129146] New mean coefficients: [[ 0.02574489  0.3217855  -2.6710978   0.2955823   0.5807443 ]]
[37m[1m[2023-06-25 03:52:37,518][129146] Moving the mean solution point...
[36m[2023-06-25 03:52:47,379][129146] train() took 9.86 seconds to complete
[36m[2023-06-25 03:52:47,379][129146] FPS: 389502.55
[36m[2023-06-25 03:52:47,381][129146] itr=295, itrs=2000, Progress: 14.75%
[36m[2023-06-25 03:52:58,833][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 03:52:58,833][129146] FPS: 335785.88
[36m[2023-06-25 03:53:03,734][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:53:03,735][129146] Reward + Measures: [[626.72002498   0.57440698   0.6112473    0.08795734   0.64920068]]
[37m[1m[2023-06-25 03:53:03,735][129146] Max Reward on eval: 626.7200249847999
[37m[1m[2023-06-25 03:53:03,735][129146] Min Reward on eval: 626.7200249847999
[37m[1m[2023-06-25 03:53:03,735][129146] Mean Reward across all agents: 626.7200249847999
[37m[1m[2023-06-25 03:53:03,735][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:53:09,305][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:53:09,306][129146] Reward + Measures: [[301.54107663   0.50400001   0.57889998   0.0885       0.62330002]
[37m[1m [786.96662461   0.52829999   0.63980001   0.10680001   0.53540003]
[37m[1m [109.69866299   0.59770006   0.54150003   0.27939999   0.47690001]
[37m[1m ...
[37m[1m [449.32696287   0.47960001   0.44689998   0.25610003   0.4179    ]
[37m[1m [487.83198305   0.58860004   0.68980002   0.10160001   0.56059998]
[37m[1m [617.62736019   0.55370003   0.45570001   0.2581       0.32430002]]
[37m[1m[2023-06-25 03:53:09,306][129146] Max Reward on eval: 1033.6516767818946
[37m[1m[2023-06-25 03:53:09,306][129146] Min Reward on eval: -394.1798687647621
[37m[1m[2023-06-25 03:53:09,306][129146] Mean Reward across all agents: 369.80394632382604
[37m[1m[2023-06-25 03:53:09,307][129146] Average Trajectory Length: 997.9623333333333
[36m[2023-06-25 03:53:09,310][129146] mean_value=-282.12940941075414, max_value=875.467255966639
[37m[1m[2023-06-25 03:53:09,313][129146] New mean coefficients: [[-0.02412927  0.18369508 -3.0660913   0.14319429  0.27752635]]
[37m[1m[2023-06-25 03:53:09,314][129146] Moving the mean solution point...
[36m[2023-06-25 03:53:19,118][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 03:53:19,118][129146] FPS: 391760.40
[36m[2023-06-25 03:53:19,120][129146] itr=296, itrs=2000, Progress: 14.80%
[36m[2023-06-25 03:53:30,540][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 03:53:30,541][129146] FPS: 336796.77
[36m[2023-06-25 03:53:35,356][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:53:35,356][129146] Reward + Measures: [[692.45702106   0.55277723   0.57860601   0.0925807    0.60092205]]
[37m[1m[2023-06-25 03:53:35,357][129146] Max Reward on eval: 692.4570210628083
[37m[1m[2023-06-25 03:53:35,357][129146] Min Reward on eval: 692.4570210628083
[37m[1m[2023-06-25 03:53:35,357][129146] Mean Reward across all agents: 692.4570210628083
[37m[1m[2023-06-25 03:53:35,357][129146] Average Trajectory Length: 999.8106666666666
[36m[2023-06-25 03:53:40,804][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:53:40,809][129146] Reward + Measures: [[120.85987379   0.4993       0.40939999   0.303        0.37870002]
[37m[1m [439.8615122    0.57359999   0.61309999   0.07300001   0.66169995]
[37m[1m [692.39181733   0.64750004   0.64289999   0.09550001   0.59810007]
[37m[1m ...
[37m[1m [719.08984374   0.4684       0.54580003   0.13309999   0.42300001]
[37m[1m [852.56031075   0.46070004   0.50229996   0.1201       0.419     ]
[37m[1m [661.99256377   0.44980001   0.53619999   0.1293       0.46580002]]
[37m[1m[2023-06-25 03:53:40,810][129146] Max Reward on eval: 1025.7399976769461
[37m[1m[2023-06-25 03:53:40,810][129146] Min Reward on eval: -223.142169406591
[37m[1m[2023-06-25 03:53:40,810][129146] Mean Reward across all agents: 440.9463950528439
[37m[1m[2023-06-25 03:53:40,811][129146] Average Trajectory Length: 998.213
[36m[2023-06-25 03:53:40,815][129146] mean_value=-41.60975521365598, max_value=1264.5837096302305
[37m[1m[2023-06-25 03:53:40,818][129146] New mean coefficients: [[-0.37108633  0.49701563 -3.735877   -0.29746556 -0.259146  ]]
[37m[1m[2023-06-25 03:53:40,819][129146] Moving the mean solution point...
[36m[2023-06-25 03:53:50,633][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 03:53:50,633][129146] FPS: 391348.49
[36m[2023-06-25 03:53:50,636][129146] itr=297, itrs=2000, Progress: 14.85%
[36m[2023-06-25 03:54:02,288][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 03:54:02,288][129146] FPS: 330058.30
[36m[2023-06-25 03:54:07,041][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:54:07,042][129146] Reward + Measures: [[791.65588442   0.51454246   0.53109741   0.1029662    0.52621424]]
[37m[1m[2023-06-25 03:54:07,042][129146] Max Reward on eval: 791.6558844209987
[37m[1m[2023-06-25 03:54:07,042][129146] Min Reward on eval: 791.6558844209987
[37m[1m[2023-06-25 03:54:07,043][129146] Mean Reward across all agents: 791.6558844209987
[37m[1m[2023-06-25 03:54:07,043][129146] Average Trajectory Length: 999.7389999999999
[36m[2023-06-25 03:54:12,450][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:54:12,451][129146] Reward + Measures: [[842.91517494   0.59630007   0.54220003   0.0868       0.51340002]
[37m[1m [715.2652299    0.57580006   0.45159999   0.20840001   0.34080002]
[37m[1m [538.4716261    0.419        0.3662       0.11090001   0.30320001]
[37m[1m ...
[37m[1m [601.67069823   0.5194       0.55109996   0.0916       0.57249999]
[37m[1m [424.47760133   0.51679999   0.55040002   0.16690001   0.56950003]
[37m[1m [605.6356254    0.54680002   0.52109998   0.0535       0.49329996]]
[37m[1m[2023-06-25 03:54:12,451][129146] Max Reward on eval: 1210.3263154653832
[37m[1m[2023-06-25 03:54:12,451][129146] Min Reward on eval: -352.4366481889272
[37m[1m[2023-06-25 03:54:12,452][129146] Mean Reward across all agents: 549.9255473404384
[37m[1m[2023-06-25 03:54:12,452][129146] Average Trajectory Length: 995.3856666666667
[36m[2023-06-25 03:54:12,456][129146] mean_value=-239.121292637642, max_value=1441.5115201699548
[37m[1m[2023-06-25 03:54:12,458][129146] New mean coefficients: [[ 0.23966905  0.7472737  -3.8097744   0.04505301 -0.07068391]]
[37m[1m[2023-06-25 03:54:12,459][129146] Moving the mean solution point...
[36m[2023-06-25 03:54:22,168][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 03:54:22,168][129146] FPS: 395583.15
[36m[2023-06-25 03:54:22,171][129146] itr=298, itrs=2000, Progress: 14.90%
[36m[2023-06-25 03:54:33,630][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 03:54:33,631][129146] FPS: 335545.57
[36m[2023-06-25 03:54:38,363][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:54:38,363][129146] Reward + Measures: [[932.96539237   0.48010567   0.48084289   0.12076424   0.42905319]]
[37m[1m[2023-06-25 03:54:38,363][129146] Max Reward on eval: 932.965392366759
[37m[1m[2023-06-25 03:54:38,364][129146] Min Reward on eval: 932.965392366759
[37m[1m[2023-06-25 03:54:38,364][129146] Mean Reward across all agents: 932.965392366759
[37m[1m[2023-06-25 03:54:38,364][129146] Average Trajectory Length: 999.5113333333333
[36m[2023-06-25 03:54:43,962][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:54:43,963][129146] Reward + Measures: [[307.66735006   0.34369999   0.42090002   0.10600001   0.41810003]
[37m[1m [881.56994141   0.52969998   0.45469999   0.19160001   0.32370001]
[37m[1m [877.3592814    0.41960001   0.42379999   0.12119999   0.37910005]
[37m[1m ...
[37m[1m [286.58059906   0.44600001   0.3712       0.21680002   0.26289999]
[37m[1m [-55.93456803   0.18813157   0.16787896   0.15563157   0.12580526]
[37m[1m [607.69576342   0.43990001   0.53590006   0.06130001   0.48990002]]
[37m[1m[2023-06-25 03:54:43,963][129146] Max Reward on eval: 1205.6139317871653
[37m[1m[2023-06-25 03:54:43,963][129146] Min Reward on eval: -126.49389483294217
[37m[1m[2023-06-25 03:54:43,964][129146] Mean Reward across all agents: 654.9089078237431
[37m[1m[2023-06-25 03:54:43,964][129146] Average Trajectory Length: 990.5066666666667
[36m[2023-06-25 03:54:43,966][129146] mean_value=-450.0739817482203, max_value=892.250375903632
[37m[1m[2023-06-25 03:54:43,968][129146] New mean coefficients: [[-0.2618316   0.70367527 -2.337201    0.12986861 -0.16803306]]
[37m[1m[2023-06-25 03:54:43,969][129146] Moving the mean solution point...
[36m[2023-06-25 03:54:53,625][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 03:54:53,625][129146] FPS: 397772.46
[36m[2023-06-25 03:54:53,627][129146] itr=299, itrs=2000, Progress: 14.95%
[36m[2023-06-25 03:55:05,064][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 03:55:05,064][129146] FPS: 336215.82
[36m[2023-06-25 03:55:09,935][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:55:09,935][129146] Reward + Measures: [[997.88184298   0.46940973   0.44781396   0.12870157   0.38702327]]
[37m[1m[2023-06-25 03:55:09,935][129146] Max Reward on eval: 997.8818429842685
[37m[1m[2023-06-25 03:55:09,936][129146] Min Reward on eval: 997.8818429842685
[37m[1m[2023-06-25 03:55:09,936][129146] Mean Reward across all agents: 997.8818429842685
[37m[1m[2023-06-25 03:55:09,936][129146] Average Trajectory Length: 999.6993333333334
[36m[2023-06-25 03:55:15,429][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:55:15,429][129146] Reward + Measures: [[ 705.9612501     0.44729996    0.4815        0.0675        0.52880001]
[37m[1m [ 495.60833593    0.52950001    0.39070001    0.2622        0.36390001]
[37m[1m [ 742.29901784    0.5413        0.32259998    0.30930004    0.26799998]
[37m[1m ...
[37m[1m [1068.1950695     0.51680005    0.43220001    0.18910001    0.28219998]
[37m[1m [ 910.20120569    0.48820001    0.46529999    0.146         0.31609997]
[37m[1m [ 307.22770381    0.43400002    0.39590001    0.24120001    0.32389998]]
[37m[1m[2023-06-25 03:55:15,429][129146] Max Reward on eval: 1216.5605979775894
[37m[1m[2023-06-25 03:55:15,430][129146] Min Reward on eval: -139.74489973559102
[37m[1m[2023-06-25 03:55:15,430][129146] Mean Reward across all agents: 657.5707561485796
[37m[1m[2023-06-25 03:55:15,430][129146] Average Trajectory Length: 998.8686666666666
[36m[2023-06-25 03:55:15,433][129146] mean_value=-168.29746040644318, max_value=1149.8083987380678
[37m[1m[2023-06-25 03:55:15,436][129146] New mean coefficients: [[ 0.5739      1.5529537  -2.2991922  -0.22474585  0.8323911 ]]
[37m[1m[2023-06-25 03:55:15,437][129146] Moving the mean solution point...
[36m[2023-06-25 03:55:25,157][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 03:55:25,157][129146] FPS: 395138.84
[36m[2023-06-25 03:55:25,159][129146] itr=300, itrs=2000, Progress: 15.00%
[37m[1m[2023-06-25 03:55:28,817][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000280
[36m[2023-06-25 03:55:40,631][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 03:55:40,631][129146] FPS: 333897.81
[36m[2023-06-25 03:55:45,447][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:55:45,447][129146] Reward + Measures: [[1087.0370767     0.47157535    0.4297373     0.140728      0.35204598]]
[37m[1m[2023-06-25 03:55:45,448][129146] Max Reward on eval: 1087.0370766958351
[37m[1m[2023-06-25 03:55:45,448][129146] Min Reward on eval: 1087.0370766958351
[37m[1m[2023-06-25 03:55:45,448][129146] Mean Reward across all agents: 1087.0370766958351
[37m[1m[2023-06-25 03:55:45,448][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:55:50,920][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:55:50,921][129146] Reward + Measures: [[1040.8373315     0.4808        0.45170003    0.18960002    0.3057    ]
[37m[1m [ 681.55641901    0.57950002    0.41440001    0.30669999    0.28660002]
[37m[1m [ 177.79012453    0.43930003    0.31850001    0.33040002    0.31329998]
[37m[1m ...
[37m[1m [ 838.1624947     0.54530001    0.43280002    0.24890001    0.28290001]
[37m[1m [-138.09444958    0.33160001    0.18990001    0.25170001    0.1943    ]
[37m[1m [ 951.51682383    0.42140004    0.41990003    0.1567        0.29840001]]
[37m[1m[2023-06-25 03:55:50,921][129146] Max Reward on eval: 1347.5041589612142
[37m[1m[2023-06-25 03:55:50,921][129146] Min Reward on eval: -186.70035310142558
[37m[1m[2023-06-25 03:55:50,922][129146] Mean Reward across all agents: 746.179228855055
[37m[1m[2023-06-25 03:55:50,922][129146] Average Trajectory Length: 998.7073333333333
[36m[2023-06-25 03:55:50,924][129146] mean_value=-487.5468960628472, max_value=431.94329364810056
[37m[1m[2023-06-25 03:55:50,926][129146] New mean coefficients: [[ 0.20402119  1.7681532  -3.244536   -0.5550237  -0.05575359]]
[37m[1m[2023-06-25 03:55:50,927][129146] Moving the mean solution point...
[36m[2023-06-25 03:56:00,659][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 03:56:00,659][129146] FPS: 394650.89
[36m[2023-06-25 03:56:00,661][129146] itr=301, itrs=2000, Progress: 15.05%
[36m[2023-06-25 03:56:12,098][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 03:56:12,098][129146] FPS: 336225.79
[36m[2023-06-25 03:56:16,873][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:56:16,879][129146] Reward + Measures: [[1172.80036008    0.47350332    0.39820704    0.15656032    0.30959401]]
[37m[1m[2023-06-25 03:56:16,880][129146] Max Reward on eval: 1172.8003600839222
[37m[1m[2023-06-25 03:56:16,881][129146] Min Reward on eval: 1172.8003600839222
[37m[1m[2023-06-25 03:56:16,881][129146] Mean Reward across all agents: 1172.8003600839222
[37m[1m[2023-06-25 03:56:16,882][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 03:56:22,474][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:56:22,475][129146] Reward + Measures: [[830.60067859   0.42080003   0.3856       0.18249999   0.31809998]
[37m[1m [838.85794274   0.396        0.32969999   0.1517       0.31479999]
[37m[1m [267.3341888    0.28541055   0.27087155   0.10672175   0.26064381]
[37m[1m ...
[37m[1m [166.13917658   0.53265452   0.28102729   0.31126365   0.20879093]
[37m[1m [946.71560954   0.40089998   0.32030001   0.17         0.26250002]
[37m[1m [281.18183537   0.329        0.40100002   0.10760001   0.35030001]]
[37m[1m[2023-06-25 03:56:22,475][129146] Max Reward on eval: 1215.7602675203816
[37m[1m[2023-06-25 03:56:22,475][129146] Min Reward on eval: -438.89576548053884
[37m[1m[2023-06-25 03:56:22,476][129146] Mean Reward across all agents: 538.7549143639606
[37m[1m[2023-06-25 03:56:22,476][129146] Average Trajectory Length: 981.7333333333333
[36m[2023-06-25 03:56:22,478][129146] mean_value=-567.8372459622561, max_value=885.7940601488526
[37m[1m[2023-06-25 03:56:22,481][129146] New mean coefficients: [[ 0.44090402  1.7500474  -2.5031407  -0.11795172 -0.21065311]]
[37m[1m[2023-06-25 03:56:22,482][129146] Moving the mean solution point...
[36m[2023-06-25 03:56:32,080][129146] train() took 9.60 seconds to complete
[36m[2023-06-25 03:56:32,080][129146] FPS: 400129.14
[36m[2023-06-25 03:56:32,083][129146] itr=302, itrs=2000, Progress: 15.10%
[36m[2023-06-25 03:56:43,496][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 03:56:43,497][129146] FPS: 336897.02
[36m[2023-06-25 03:56:48,218][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:56:48,218][129146] Reward + Measures: [[1204.83790378    0.48637944    0.37678197    0.16747421    0.28626356]]
[37m[1m[2023-06-25 03:56:48,219][129146] Max Reward on eval: 1204.837903778973
[37m[1m[2023-06-25 03:56:48,219][129146] Min Reward on eval: 1204.837903778973
[37m[1m[2023-06-25 03:56:48,219][129146] Mean Reward across all agents: 1204.837903778973
[37m[1m[2023-06-25 03:56:48,219][129146] Average Trajectory Length: 999.4556666666666
[36m[2023-06-25 03:56:53,716][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:56:53,716][129146] Reward + Measures: [[ 650.36916707    0.36250001    0.3197        0.13110001    0.2199    ]
[37m[1m [ 701.05817438    0.62480003    0.47010002    0.2924        0.37349999]
[37m[1m [1103.00375153    0.48610002    0.40159997    0.15910001    0.27770001]
[37m[1m ...
[37m[1m [-248.94246863    0.92919999    0.42210004    0.89239997    0.38720003]
[37m[1m [1046.35341187    0.50979996    0.42080003    0.16869999    0.35530001]
[37m[1m [ 509.36771907    0.3436        0.28040001    0.11370001    0.19400001]]
[37m[1m[2023-06-25 03:56:53,717][129146] Max Reward on eval: 1345.8843881792388
[37m[1m[2023-06-25 03:56:53,717][129146] Min Reward on eval: -262.17760178725
[37m[1m[2023-06-25 03:56:53,717][129146] Mean Reward across all agents: 644.4466947178356
[37m[1m[2023-06-25 03:56:53,717][129146] Average Trajectory Length: 998.2706666666667
[36m[2023-06-25 03:56:53,723][129146] mean_value=-149.42140457521984, max_value=1000.9289355464559
[37m[1m[2023-06-25 03:56:53,726][129146] New mean coefficients: [[-0.46532673  1.5297203  -2.8039877  -0.3806044  -0.96609175]]
[37m[1m[2023-06-25 03:56:53,727][129146] Moving the mean solution point...
[36m[2023-06-25 03:57:03,346][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 03:57:03,347][129146] FPS: 399253.79
[36m[2023-06-25 03:57:03,349][129146] itr=303, itrs=2000, Progress: 15.15%
[36m[2023-06-25 03:57:14,755][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 03:57:14,755][129146] FPS: 337123.59
[36m[2023-06-25 03:57:19,467][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:57:19,467][129146] Reward + Measures: [[1201.31932594    0.49833813    0.35690349    0.17969832    0.26542395]]
[37m[1m[2023-06-25 03:57:19,467][129146] Max Reward on eval: 1201.3193259351094
[37m[1m[2023-06-25 03:57:19,468][129146] Min Reward on eval: 1201.3193259351094
[37m[1m[2023-06-25 03:57:19,468][129146] Mean Reward across all agents: 1201.3193259351094
[37m[1m[2023-06-25 03:57:19,468][129146] Average Trajectory Length: 999.3403333333333
[36m[2023-06-25 03:57:24,796][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:57:24,796][129146] Reward + Measures: [[787.0444858    0.52560002   0.42200002   0.23400001   0.34519997]
[37m[1m [506.7605427    0.58990002   0.41879997   0.30770001   0.3802    ]
[37m[1m [648.02101909   0.38610002   0.31570002   0.133        0.20829999]
[37m[1m ...
[37m[1m [547.85374805   0.54969996   0.42350003   0.21859999   0.35310003]
[37m[1m [348.27345564   0.3301       0.32940003   0.0793       0.28280002]
[37m[1m [435.89367222   0.30410001   0.2764       0.1035       0.1989    ]]
[37m[1m[2023-06-25 03:57:24,796][129146] Max Reward on eval: 1295.974640877411
[37m[1m[2023-06-25 03:57:24,797][129146] Min Reward on eval: -144.32939898938494
[37m[1m[2023-06-25 03:57:24,797][129146] Mean Reward across all agents: 796.2716912798196
[37m[1m[2023-06-25 03:57:24,797][129146] Average Trajectory Length: 988.7353333333333
[36m[2023-06-25 03:57:24,799][129146] mean_value=-800.597482822817, max_value=1500.2529163846402
[37m[1m[2023-06-25 03:57:24,801][129146] New mean coefficients: [[-0.07522112  1.3659456  -2.5121508  -0.13542324 -0.37197924]]
[37m[1m[2023-06-25 03:57:24,802][129146] Moving the mean solution point...
[36m[2023-06-25 03:57:34,415][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 03:57:34,415][129146] FPS: 399534.94
[36m[2023-06-25 03:57:34,418][129146] itr=304, itrs=2000, Progress: 15.20%
[36m[2023-06-25 03:57:46,086][129146] train() took 11.65 seconds to complete
[36m[2023-06-25 03:57:46,086][129146] FPS: 329623.51
[36m[2023-06-25 03:57:50,905][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:57:50,905][129146] Reward + Measures: [[1141.69810828    0.52083606    0.34056523    0.21248439    0.2499108 ]]
[37m[1m[2023-06-25 03:57:50,906][129146] Max Reward on eval: 1141.6981082843167
[37m[1m[2023-06-25 03:57:50,906][129146] Min Reward on eval: 1141.6981082843167
[37m[1m[2023-06-25 03:57:50,906][129146] Mean Reward across all agents: 1141.6981082843167
[37m[1m[2023-06-25 03:57:50,906][129146] Average Trajectory Length: 999.8306666666666
[36m[2023-06-25 03:57:56,381][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:57:56,381][129146] Reward + Measures: [[ 821.25753909    0.55669999    0.39920002    0.2868        0.2687    ]
[37m[1m [1069.84609389    0.51809996    0.33950004    0.22790001    0.25640002]
[37m[1m [1133.23673412    0.51459998    0.33449998    0.14520001    0.28530002]
[37m[1m ...
[37m[1m [1167.18372546    0.52460003    0.32570001    0.18660001    0.2606    ]
[37m[1m [ 702.94473913    0.62849998    0.28579998    0.34740001    0.23740001]
[37m[1m [ -69.8738301     0.54190004    0.35390002    0.36300001    0.38789999]]
[37m[1m[2023-06-25 03:57:56,381][129146] Max Reward on eval: 1302.4855232634582
[37m[1m[2023-06-25 03:57:56,382][129146] Min Reward on eval: -542.2565558155999
[37m[1m[2023-06-25 03:57:56,382][129146] Mean Reward across all agents: 620.4202151484832
[37m[1m[2023-06-25 03:57:56,382][129146] Average Trajectory Length: 998.6913333333333
[36m[2023-06-25 03:57:56,385][129146] mean_value=-180.50905603954283, max_value=1513.7630441923043
[37m[1m[2023-06-25 03:57:56,388][129146] New mean coefficients: [[-0.6948232   1.4505228  -2.4539843  -0.13929012 -0.00889435]]
[37m[1m[2023-06-25 03:57:56,389][129146] Moving the mean solution point...
[36m[2023-06-25 03:58:06,138][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 03:58:06,138][129146] FPS: 393946.08
[36m[2023-06-25 03:58:06,140][129146] itr=305, itrs=2000, Progress: 15.25%
[36m[2023-06-25 03:58:17,576][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 03:58:17,576][129146] FPS: 336264.84
[36m[2023-06-25 03:58:22,368][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:58:22,373][129146] Reward + Measures: [[982.492112     0.56785309   0.31940359   0.28554261   0.23022106]]
[37m[1m[2023-06-25 03:58:22,373][129146] Max Reward on eval: 982.4921120013082
[37m[1m[2023-06-25 03:58:22,374][129146] Min Reward on eval: 982.4921120013082
[37m[1m[2023-06-25 03:58:22,374][129146] Mean Reward across all agents: 982.4921120013082
[37m[1m[2023-06-25 03:58:22,374][129146] Average Trajectory Length: 998.683
[36m[2023-06-25 03:58:27,817][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:58:27,818][129146] Reward + Measures: [[805.71783176   0.49880004   0.36499998   0.2096       0.27580002]
[37m[1m [942.9890831    0.4777       0.33969998   0.2066       0.30679998]
[37m[1m [370.77069044   0.46739998   0.33290002   0.22350001   0.26610002]
[37m[1m ...
[37m[1m [998.14007571   0.5474       0.36660001   0.2237       0.245     ]
[37m[1m [258.15755234   0.86360008   0.308        0.74750006   0.1076    ]
[37m[1m [809.11858453   0.40809998   0.33759999   0.1769       0.2561    ]]
[37m[1m[2023-06-25 03:58:27,818][129146] Max Reward on eval: 1231.9757091533393
[37m[1m[2023-06-25 03:58:27,818][129146] Min Reward on eval: -632.2202384139061
[37m[1m[2023-06-25 03:58:27,818][129146] Mean Reward across all agents: 462.64176637091686
[37m[1m[2023-06-25 03:58:27,819][129146] Average Trajectory Length: 997.567
[36m[2023-06-25 03:58:27,822][129146] mean_value=-372.8446719179947, max_value=703.1551544217124
[37m[1m[2023-06-25 03:58:27,825][129146] New mean coefficients: [[ 0.31151205  1.6960372  -2.2439406   0.26169065  0.21046264]]
[37m[1m[2023-06-25 03:58:27,826][129146] Moving the mean solution point...
[36m[2023-06-25 03:58:37,519][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 03:58:37,519][129146] FPS: 396240.47
[36m[2023-06-25 03:58:37,521][129146] itr=306, itrs=2000, Progress: 15.30%
[36m[2023-06-25 03:58:48,951][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 03:58:48,951][129146] FPS: 336429.32
[36m[2023-06-25 03:58:53,778][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:58:53,778][129146] Reward + Measures: [[836.61006845   0.61096561   0.29780734   0.36009046   0.22010913]]
[37m[1m[2023-06-25 03:58:53,779][129146] Max Reward on eval: 836.6100684488919
[37m[1m[2023-06-25 03:58:53,779][129146] Min Reward on eval: 836.6100684488919
[37m[1m[2023-06-25 03:58:53,779][129146] Mean Reward across all agents: 836.6100684488919
[37m[1m[2023-06-25 03:58:53,779][129146] Average Trajectory Length: 999.352
[36m[2023-06-25 03:58:59,354][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:58:59,354][129146] Reward + Measures: [[ 759.57215566    0.56960005    0.36210003    0.34720001    0.28460002]
[37m[1m [ 258.06939537    0.53560001    0.4323        0.21900001    0.30739999]
[37m[1m [ 892.16563729    0.34777254    0.29121763    0.10360473    0.28430519]
[37m[1m ...
[37m[1m [1143.50751856    0.48533794    0.37358275    0.24433449    0.20617242]
[37m[1m [ 393.9408857     0.57120001    0.39540002    0.38150001    0.40599999]
[37m[1m [ 833.57245589    0.58470005    0.34349999    0.26750001    0.2712    ]]
[37m[1m[2023-06-25 03:58:59,354][129146] Max Reward on eval: 1229.666829267994
[37m[1m[2023-06-25 03:58:59,355][129146] Min Reward on eval: -68.18533485668013
[37m[1m[2023-06-25 03:58:59,355][129146] Mean Reward across all agents: 612.931155136841
[37m[1m[2023-06-25 03:58:59,355][129146] Average Trajectory Length: 995.7406666666666
[36m[2023-06-25 03:58:59,360][129146] mean_value=-140.6761078183042, max_value=759.8050147280694
[37m[1m[2023-06-25 03:58:59,363][129146] New mean coefficients: [[ 0.2660614   1.7916319  -1.8951988   0.44247773  0.6331185 ]]
[37m[1m[2023-06-25 03:58:59,363][129146] Moving the mean solution point...
[36m[2023-06-25 03:59:09,061][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 03:59:09,062][129146] FPS: 396030.27
[36m[2023-06-25 03:59:09,064][129146] itr=307, itrs=2000, Progress: 15.35%
[36m[2023-06-25 03:59:20,492][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 03:59:20,492][129146] FPS: 336501.72
[36m[2023-06-25 03:59:25,273][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:59:25,274][129146] Reward + Measures: [[599.03392333   0.68104738   0.26983735   0.4784807    0.20717467]]
[37m[1m[2023-06-25 03:59:25,274][129146] Max Reward on eval: 599.0339233264982
[37m[1m[2023-06-25 03:59:25,274][129146] Min Reward on eval: 599.0339233264982
[37m[1m[2023-06-25 03:59:25,274][129146] Mean Reward across all agents: 599.0339233264982
[37m[1m[2023-06-25 03:59:25,275][129146] Average Trajectory Length: 999.3286666666667
[36m[2023-06-25 03:59:30,601][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:59:30,601][129146] Reward + Measures: [[ 564.53482636    0.44779998    0.46949998    0.13780001    0.44180003]
[37m[1m [ 223.62001899    0.3529        0.29570001    0.21089999    0.2983    ]
[37m[1m [1149.66696421    0.49400002    0.30490002    0.19509999    0.24059999]
[37m[1m ...
[37m[1m [1179.27194313    0.47          0.37549999    0.08880001    0.28600001]
[37m[1m [ 406.96001948    0.76490003    0.35029998    0.66950005    0.1225    ]
[37m[1m [ 344.12179506    0.49580002    0.51000005    0.13450001    0.56490004]]
[37m[1m[2023-06-25 03:59:30,602][129146] Max Reward on eval: 1306.4476987567032
[37m[1m[2023-06-25 03:59:30,602][129146] Min Reward on eval: 3.1393264694837852
[37m[1m[2023-06-25 03:59:30,602][129146] Mean Reward across all agents: 637.7847494220288
[37m[1m[2023-06-25 03:59:30,602][129146] Average Trajectory Length: 998.225
[36m[2023-06-25 03:59:30,606][129146] mean_value=-132.35670072989035, max_value=949.125039326548
[37m[1m[2023-06-25 03:59:30,609][129146] New mean coefficients: [[ 0.501512    2.614027   -2.683381    0.4108893   0.50640416]]
[37m[1m[2023-06-25 03:59:30,610][129146] Moving the mean solution point...
[36m[2023-06-25 03:59:40,297][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 03:59:40,297][129146] FPS: 396492.78
[36m[2023-06-25 03:59:40,300][129146] itr=308, itrs=2000, Progress: 15.40%
[36m[2023-06-25 03:59:51,841][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 03:59:51,842][129146] FPS: 333192.26
[36m[2023-06-25 03:59:56,711][129146] Finished Evaluation Step
[37m[1m[2023-06-25 03:59:56,711][129146] Reward + Measures: [[254.03923306   0.78132761   0.23237646   0.65078467   0.21781124]]
[37m[1m[2023-06-25 03:59:56,711][129146] Max Reward on eval: 254.03923306072784
[37m[1m[2023-06-25 03:59:56,712][129146] Min Reward on eval: 254.03923306072784
[37m[1m[2023-06-25 03:59:56,712][129146] Mean Reward across all agents: 254.03923306072784
[37m[1m[2023-06-25 03:59:56,712][129146] Average Trajectory Length: 999.8703333333333
[36m[2023-06-25 04:00:02,316][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:00:02,317][129146] Reward + Measures: [[301.25765773   0.75949997   0.28059998   0.6354       0.21689999]
[37m[1m [352.58049445   0.58320004   0.36360002   0.40120003   0.33969998]
[37m[1m [448.59819152   0.6541       0.28920004   0.49200001   0.29560003]
[37m[1m ...
[37m[1m [ 26.27810716   0.84860003   0.21160002   0.72619992   0.14650001]
[37m[1m [ 54.27034847   0.79730004   0.2342       0.68740004   0.14380001]
[37m[1m [-20.38690807   0.86079997   0.18630001   0.77439994   0.12360001]]
[37m[1m[2023-06-25 04:00:02,317][129146] Max Reward on eval: 644.4902123381034
[37m[1m[2023-06-25 04:00:02,317][129146] Min Reward on eval: -126.5978673730162
[37m[1m[2023-06-25 04:00:02,318][129146] Mean Reward across all agents: 191.70211285022503
[37m[1m[2023-06-25 04:00:02,318][129146] Average Trajectory Length: 999.6983333333333
[36m[2023-06-25 04:00:02,321][129146] mean_value=-60.131294765544496, max_value=275.05285844794605
[37m[1m[2023-06-25 04:00:02,323][129146] New mean coefficients: [[ 1.113975    1.9823154  -2.0170708   0.38514858  0.41208297]]
[37m[1m[2023-06-25 04:00:02,324][129146] Moving the mean solution point...
[36m[2023-06-25 04:00:12,226][129146] train() took 9.90 seconds to complete
[36m[2023-06-25 04:00:12,227][129146] FPS: 387864.71
[36m[2023-06-25 04:00:12,229][129146] itr=309, itrs=2000, Progress: 15.45%
[36m[2023-06-25 04:00:23,970][129146] train() took 11.72 seconds to complete
[36m[2023-06-25 04:00:23,970][129146] FPS: 327573.67
[36m[2023-06-25 04:00:28,677][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:00:28,678][129146] Reward + Measures: [[199.30540112   0.81890631   0.28714165   0.73377031   0.19769533]]
[37m[1m[2023-06-25 04:00:28,678][129146] Max Reward on eval: 199.30540111593123
[37m[1m[2023-06-25 04:00:28,678][129146] Min Reward on eval: 199.30540111593123
[37m[1m[2023-06-25 04:00:28,678][129146] Mean Reward across all agents: 199.30540111593123
[37m[1m[2023-06-25 04:00:28,679][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:00:34,111][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:00:34,112][129146] Reward + Measures: [[ 486.94568986    0.64099997    0.26019999    0.47049999    0.33340001]
[37m[1m [ 344.83037134    0.53899997    0.40419999    0.47669998    0.45930001]
[37m[1m [-340.69203678    0.2900289     0.18833052    0.2138654     0.12593596]
[37m[1m ...
[37m[1m [ 170.37938938    0.824         0.30350003    0.759         0.19140001]
[37m[1m [ 195.47692854    0.62120003    0.44550005    0.56960005    0.456     ]
[37m[1m [  27.1557786     0.56910002    0.2976        0.34810004    0.27480003]]
[37m[1m[2023-06-25 04:00:34,112][129146] Max Reward on eval: 608.2307508195169
[37m[1m[2023-06-25 04:00:34,112][129146] Min Reward on eval: -731.5862514875334
[37m[1m[2023-06-25 04:00:34,112][129146] Mean Reward across all agents: 158.8355807390107
[37m[1m[2023-06-25 04:00:34,113][129146] Average Trajectory Length: 998.4973333333332
[36m[2023-06-25 04:00:34,119][129146] mean_value=-103.0482692102354, max_value=721.3825498067047
[37m[1m[2023-06-25 04:00:34,121][129146] New mean coefficients: [[ 1.7657967   2.8427672  -1.6991248   0.31760573  0.51335275]]
[37m[1m[2023-06-25 04:00:34,122][129146] Moving the mean solution point...
[36m[2023-06-25 04:00:43,826][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 04:00:43,827][129146] FPS: 395766.36
[36m[2023-06-25 04:00:43,829][129146] itr=310, itrs=2000, Progress: 15.50%
[37m[1m[2023-06-25 04:00:47,689][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000290
[36m[2023-06-25 04:00:59,640][129146] train() took 11.65 seconds to complete
[36m[2023-06-25 04:00:59,641][129146] FPS: 329555.57
[36m[2023-06-25 04:01:04,595][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:01:04,596][129146] Reward + Measures: [[672.96032791   0.50798011   0.3297298    0.28538081   0.24126402]]
[37m[1m[2023-06-25 04:01:04,596][129146] Max Reward on eval: 672.9603279081184
[37m[1m[2023-06-25 04:01:04,596][129146] Min Reward on eval: 672.9603279081184
[37m[1m[2023-06-25 04:01:04,596][129146] Mean Reward across all agents: 672.9603279081184
[37m[1m[2023-06-25 04:01:04,596][129146] Average Trajectory Length: 999.218
[36m[2023-06-25 04:01:10,157][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:01:10,158][129146] Reward + Measures: [[-542.7120117     0.35238242    0.32794577    0.3144016     0.28334591]
[37m[1m [ 303.21629609    0.53430003    0.34909996    0.41890001    0.20840001]
[37m[1m [ 508.50983076    0.62950003    0.26420003    0.46129999    0.1873    ]
[37m[1m ...
[37m[1m [-138.31736869    0.44709998    0.32210001    0.32750002    0.23770002]
[37m[1m [ 360.56509307    0.69639999    0.24299999    0.52249998    0.2034    ]
[37m[1m [ 269.14551769    0.53939998    0.29449999    0.4165        0.1816    ]]
[37m[1m[2023-06-25 04:01:10,158][129146] Max Reward on eval: 800.6816171979299
[37m[1m[2023-06-25 04:01:10,158][129146] Min Reward on eval: -623.3444208801491
[37m[1m[2023-06-25 04:01:10,158][129146] Mean Reward across all agents: 279.727246530185
[37m[1m[2023-06-25 04:01:10,159][129146] Average Trajectory Length: 992.381
[36m[2023-06-25 04:01:10,161][129146] mean_value=-356.13796893390463, max_value=746.8677651349338
[37m[1m[2023-06-25 04:01:10,164][129146] New mean coefficients: [[ 2.011662    2.8164756  -1.411228   -0.00442302  0.50924504]]
[37m[1m[2023-06-25 04:01:10,165][129146] Moving the mean solution point...
[36m[2023-06-25 04:01:19,857][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 04:01:19,858][129146] FPS: 396254.10
[36m[2023-06-25 04:01:19,860][129146] itr=311, itrs=2000, Progress: 15.55%
[36m[2023-06-25 04:01:31,294][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 04:01:31,295][129146] FPS: 336290.68
[36m[2023-06-25 04:01:36,026][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:01:36,026][129146] Reward + Measures: [[764.4084117    0.51235735   0.33566061   0.2821036    0.2400741 ]]
[37m[1m[2023-06-25 04:01:36,027][129146] Max Reward on eval: 764.4084116990048
[37m[1m[2023-06-25 04:01:36,027][129146] Min Reward on eval: 764.4084116990048
[37m[1m[2023-06-25 04:01:36,027][129146] Mean Reward across all agents: 764.4084116990048
[37m[1m[2023-06-25 04:01:36,027][129146] Average Trajectory Length: 999.5079999999999
[36m[2023-06-25 04:01:41,506][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:01:41,507][129146] Reward + Measures: [[425.20862821   0.47259998   0.3423       0.38870001   0.20320001]
[37m[1m [650.32288768   0.42530003   0.3549       0.30779999   0.20460001]
[37m[1m [252.13578914   0.44459996   0.3247       0.23050001   0.31240001]
[37m[1m ...
[37m[1m [442.30995024   0.42350003   0.40709996   0.33110002   0.23029999]
[37m[1m [447.66365395   0.46159998   0.3506       0.34630001   0.1875    ]
[37m[1m [312.86980786   0.66429996   0.24620001   0.53019994   0.14240001]]
[37m[1m[2023-06-25 04:01:41,507][129146] Max Reward on eval: 954.2741039049113
[37m[1m[2023-06-25 04:01:41,507][129146] Min Reward on eval: -735.6028786324896
[37m[1m[2023-06-25 04:01:41,507][129146] Mean Reward across all agents: 300.54759412199024
[37m[1m[2023-06-25 04:01:41,508][129146] Average Trajectory Length: 994.8533333333334
[36m[2023-06-25 04:01:41,511][129146] mean_value=-152.6861093253873, max_value=690.019206879579
[37m[1m[2023-06-25 04:01:41,513][129146] New mean coefficients: [[ 2.115552    2.7467895  -1.6201053   0.05233222  0.61948967]]
[37m[1m[2023-06-25 04:01:41,514][129146] Moving the mean solution point...
[36m[2023-06-25 04:01:51,253][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 04:01:51,253][129146] FPS: 394363.96
[36m[2023-06-25 04:01:51,255][129146] itr=312, itrs=2000, Progress: 15.60%
[36m[2023-06-25 04:02:02,793][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 04:02:02,794][129146] FPS: 333389.92
[36m[2023-06-25 04:02:07,590][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:02:07,591][129146] Reward + Measures: [[845.08123172   0.50623471   0.3480832    0.26282185   0.24276155]]
[37m[1m[2023-06-25 04:02:07,591][129146] Max Reward on eval: 845.0812317164266
[37m[1m[2023-06-25 04:02:07,591][129146] Min Reward on eval: 845.0812317164266
[37m[1m[2023-06-25 04:02:07,591][129146] Mean Reward across all agents: 845.0812317164266
[37m[1m[2023-06-25 04:02:07,592][129146] Average Trajectory Length: 999.324
[36m[2023-06-25 04:02:13,031][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:02:13,032][129146] Reward + Measures: [[884.25759408   0.52020001   0.37600002   0.23870002   0.27910003]
[37m[1m [724.6447384    0.52380002   0.30669999   0.27010003   0.25570002]
[37m[1m [886.35352724   0.52529997   0.39110002   0.25319999   0.27379999]
[37m[1m ...
[37m[1m [792.62154977   0.52019995   0.40170002   0.2692       0.289     ]
[37m[1m [708.31210324   0.48610002   0.3468       0.25040001   0.2895    ]
[37m[1m [978.72074218   0.51719999   0.39449999   0.26120001   0.26010001]]
[37m[1m[2023-06-25 04:02:13,032][129146] Max Reward on eval: 978.7207421763684
[37m[1m[2023-06-25 04:02:13,032][129146] Min Reward on eval: 52.386498149152615
[37m[1m[2023-06-25 04:02:13,032][129146] Mean Reward across all agents: 702.5553719966028
[37m[1m[2023-06-25 04:02:13,033][129146] Average Trajectory Length: 999.3223333333333
[36m[2023-06-25 04:02:13,034][129146] mean_value=-254.1708490626048, max_value=434.84935116420735
[37m[1m[2023-06-25 04:02:13,037][129146] New mean coefficients: [[ 2.1883304   2.976449   -1.498058    0.54134154  0.71283984]]
[37m[1m[2023-06-25 04:02:13,038][129146] Moving the mean solution point...
[36m[2023-06-25 04:02:22,757][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 04:02:22,758][129146] FPS: 395140.40
[36m[2023-06-25 04:02:22,760][129146] itr=313, itrs=2000, Progress: 15.65%
[36m[2023-06-25 04:02:34,359][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 04:02:34,359][129146] FPS: 331588.00
[36m[2023-06-25 04:02:39,099][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:02:39,100][129146] Reward + Measures: [[882.95811538   0.51778388   0.3384935    0.27467957   0.24143369]]
[37m[1m[2023-06-25 04:02:39,100][129146] Max Reward on eval: 882.9581153774081
[37m[1m[2023-06-25 04:02:39,100][129146] Min Reward on eval: 882.9581153774081
[37m[1m[2023-06-25 04:02:39,101][129146] Mean Reward across all agents: 882.9581153774081
[37m[1m[2023-06-25 04:02:39,101][129146] Average Trajectory Length: 999.3996666666667
[36m[2023-06-25 04:02:44,644][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:02:44,644][129146] Reward + Measures: [[   1.21567611    0.42891479    0.17020655    0.35505244    0.25474593]
[37m[1m [ 426.4465407     0.62099999    0.33920002    0.44530001    0.25760001]
[37m[1m [ 694.08588673    0.46160004    0.26370001    0.28210002    0.2349    ]
[37m[1m ...
[37m[1m [ 129.44515357    0.81919998    0.0886        0.76519996    0.30599999]
[37m[1m [  55.45174859    0.3698        0.2465        0.24510001    0.29380003]
[37m[1m [-413.78781366    0.32140002    0.19709998    0.2397        0.28230003]]
[37m[1m[2023-06-25 04:02:44,649][129146] Max Reward on eval: 1212.6815272960346
[37m[1m[2023-06-25 04:02:44,649][129146] Min Reward on eval: -667.2910569932312
[37m[1m[2023-06-25 04:02:44,650][129146] Mean Reward across all agents: 315.8742504385061
[37m[1m[2023-06-25 04:02:44,650][129146] Average Trajectory Length: 991.029
[36m[2023-06-25 04:02:44,652][129146] mean_value=-634.8053715919633, max_value=483.87368286416523
[37m[1m[2023-06-25 04:02:44,655][129146] New mean coefficients: [[ 0.93580365  3.324202   -2.1399364   0.75281125  0.7509195 ]]
[37m[1m[2023-06-25 04:02:44,656][129146] Moving the mean solution point...
[36m[2023-06-25 04:02:54,352][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 04:02:54,352][129146] FPS: 396105.80
[36m[2023-06-25 04:02:54,354][129146] itr=314, itrs=2000, Progress: 15.70%
[36m[2023-06-25 04:03:06,004][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 04:03:06,004][129146] FPS: 330144.63
[36m[2023-06-25 04:03:10,872][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:03:10,873][129146] Reward + Measures: [[754.67384594   0.59760684   0.27998772   0.38947216   0.24726032]]
[37m[1m[2023-06-25 04:03:10,873][129146] Max Reward on eval: 754.6738459412625
[37m[1m[2023-06-25 04:03:10,874][129146] Min Reward on eval: 754.6738459412625
[37m[1m[2023-06-25 04:03:10,874][129146] Mean Reward across all agents: 754.6738459412625
[37m[1m[2023-06-25 04:03:10,874][129146] Average Trajectory Length: 999.5963333333333
[36m[2023-06-25 04:03:16,293][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:03:16,294][129146] Reward + Measures: [[911.16176672   0.48820001   0.31420001   0.25610003   0.2368    ]
[37m[1m [-90.83129289   0.34635639   0.19281399   0.21860655   0.18035866]
[37m[1m [730.57953556   0.55980003   0.3202       0.3184       0.2034    ]
[37m[1m ...
[37m[1m [420.94677704   0.39182529   0.36053768   0.17998011   0.18837498]
[37m[1m [ 97.59047564   0.78459996   0.11800001   0.74449998   0.22409999]
[37m[1m [458.52811549   0.41446576   0.27562705   0.22667387   0.19207929]]
[37m[1m[2023-06-25 04:03:16,294][129146] Max Reward on eval: 1035.1234975347265
[37m[1m[2023-06-25 04:03:16,294][129146] Min Reward on eval: -326.0090643152682
[37m[1m[2023-06-25 04:03:16,295][129146] Mean Reward across all agents: 398.4545334709508
[37m[1m[2023-06-25 04:03:16,295][129146] Average Trajectory Length: 951.8536666666666
[36m[2023-06-25 04:03:16,297][129146] mean_value=-491.32929036556646, max_value=517.6403054088238
[37m[1m[2023-06-25 04:03:16,300][129146] New mean coefficients: [[ 0.8064443   3.6066823  -2.1534054   0.659283   -0.26479775]]
[37m[1m[2023-06-25 04:03:16,300][129146] Moving the mean solution point...
[36m[2023-06-25 04:03:26,073][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 04:03:26,073][129146] FPS: 393008.94
[36m[2023-06-25 04:03:26,076][129146] itr=315, itrs=2000, Progress: 15.75%
[36m[2023-06-25 04:03:37,759][129146] train() took 11.67 seconds to complete
[36m[2023-06-25 04:03:37,760][129146] FPS: 329168.85
[36m[2023-06-25 04:03:42,623][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:03:42,626][129146] Reward + Measures: [[408.6374607    0.77441543   0.15077315   0.6609484    0.28669655]]
[37m[1m[2023-06-25 04:03:42,627][129146] Max Reward on eval: 408.63746069644316
[37m[1m[2023-06-25 04:03:42,627][129146] Min Reward on eval: 408.63746069644316
[37m[1m[2023-06-25 04:03:42,627][129146] Mean Reward across all agents: 408.63746069644316
[37m[1m[2023-06-25 04:03:42,627][129146] Average Trajectory Length: 999.356
[36m[2023-06-25 04:03:47,894][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:03:47,899][129146] Reward + Measures: [[263.3802065    0.83820003   0.11310001   0.7367       0.29229999]
[37m[1m [ 65.08328991   0.80410004   0.0834       0.76300001   0.2538    ]
[37m[1m [286.68362294   0.84619999   0.0969       0.77790004   0.30089998]
[37m[1m ...
[37m[1m [224.76922587   0.82709998   0.24249998   0.75279999   0.1053    ]
[37m[1m [800.68880402   0.66029996   0.24829999   0.44569999   0.2545    ]
[37m[1m [612.37126252   0.58939999   0.34470001   0.38690001   0.24700001]]
[37m[1m[2023-06-25 04:03:47,899][129146] Max Reward on eval: 1118.655970152479
[37m[1m[2023-06-25 04:03:47,900][129146] Min Reward on eval: 65.08328990745358
[37m[1m[2023-06-25 04:03:47,900][129146] Mean Reward across all agents: 429.30184667938244
[37m[1m[2023-06-25 04:03:47,900][129146] Average Trajectory Length: 999.8783333333333
[36m[2023-06-25 04:03:47,907][129146] mean_value=68.39263397137756, max_value=576.894793001941
[37m[1m[2023-06-25 04:03:47,910][129146] New mean coefficients: [[ 0.99789745  3.64515    -2.714448    0.7445232   0.02075255]]
[37m[1m[2023-06-25 04:03:47,911][129146] Moving the mean solution point...
[36m[2023-06-25 04:03:57,694][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 04:03:57,695][129146] FPS: 392583.44
[36m[2023-06-25 04:03:57,697][129146] itr=316, itrs=2000, Progress: 15.80%
[36m[2023-06-25 04:04:09,325][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 04:04:09,325][129146] FPS: 330737.02
[36m[2023-06-25 04:04:14,071][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:04:14,076][129146] Reward + Measures: [[169.41069122   0.91086149   0.05790315   0.86427194   0.38963833]]
[37m[1m[2023-06-25 04:04:14,077][129146] Max Reward on eval: 169.41069121730294
[37m[1m[2023-06-25 04:04:14,077][129146] Min Reward on eval: 169.41069121730294
[37m[1m[2023-06-25 04:04:14,077][129146] Mean Reward across all agents: 169.41069121730294
[37m[1m[2023-06-25 04:04:14,077][129146] Average Trajectory Length: 999.703
[36m[2023-06-25 04:04:19,549][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:04:19,555][129146] Reward + Measures: [[ 85.11963167   0.95109999   0.0246       0.92869997   0.41549999]
[37m[1m [182.80684241   0.91149998   0.0794       0.86089993   0.26799998]
[37m[1m [209.6118732    0.91009998   0.0688       0.85150003   0.42290002]
[37m[1m ...
[37m[1m [230.40660399   0.89370006   0.078        0.8405       0.32109997]
[37m[1m [ 93.49583789   0.93990004   0.0438       0.91909999   0.3276    ]
[37m[1m [292.64995373   0.85650009   0.10040001   0.7802       0.3664    ]]
[37m[1m[2023-06-25 04:04:19,556][129146] Max Reward on eval: 697.8635894064792
[37m[1m[2023-06-25 04:04:19,557][129146] Min Reward on eval: -17.116105590993538
[37m[1m[2023-06-25 04:04:19,557][129146] Mean Reward across all agents: 155.34281990056982
[37m[1m[2023-06-25 04:04:19,558][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:04:19,569][129146] mean_value=58.866268596569476, max_value=606.67617000208
[37m[1m[2023-06-25 04:04:19,573][129146] New mean coefficients: [[ 0.96594286  4.138716   -2.4376428   0.6176359  -0.17956059]]
[37m[1m[2023-06-25 04:04:19,574][129146] Moving the mean solution point...
[36m[2023-06-25 04:04:29,315][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 04:04:29,315][129146] FPS: 394298.52
[36m[2023-06-25 04:04:29,317][129146] itr=317, itrs=2000, Progress: 15.85%
[36m[2023-06-25 04:04:40,765][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 04:04:40,765][129146] FPS: 335971.28
[36m[2023-06-25 04:04:45,474][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:04:45,474][129146] Reward + Measures: [[47.65897935  0.98134935  0.009773    0.96983063  0.56846929]]
[37m[1m[2023-06-25 04:04:45,474][129146] Max Reward on eval: 47.658979353107185
[37m[1m[2023-06-25 04:04:45,475][129146] Min Reward on eval: 47.658979353107185
[37m[1m[2023-06-25 04:04:45,475][129146] Mean Reward across all agents: 47.658979353107185
[37m[1m[2023-06-25 04:04:45,475][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:04:50,933][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:04:50,934][129146] Reward + Measures: [[661.1830302    0.68479997   0.24110003   0.54229999   0.45250002]
[37m[1m [-11.40554077   0.96450007   0.0423       0.94640011   0.3522    ]
[37m[1m [-28.0283548    0.99140006   0.0051       0.98229998   0.67520005]
[37m[1m ...
[37m[1m [ 24.20337989   0.95290005   0.0461       0.93349999   0.27270001]
[37m[1m [167.39998673   0.92060006   0.0539       0.88049996   0.34830004]
[37m[1m [235.56306896   0.69390005   0.1842       0.56190002   0.24609999]]
[37m[1m[2023-06-25 04:04:50,939][129146] Max Reward on eval: 1034.533478205977
[37m[1m[2023-06-25 04:04:50,940][129146] Min Reward on eval: -963.3223810455645
[37m[1m[2023-06-25 04:04:50,940][129146] Mean Reward across all agents: 141.87068772453046
[37m[1m[2023-06-25 04:04:50,941][129146] Average Trajectory Length: 980.5396666666667
[36m[2023-06-25 04:04:50,949][129146] mean_value=-141.25957854428225, max_value=880.9144969337747
[37m[1m[2023-06-25 04:04:50,953][129146] New mean coefficients: [[ 0.7097212   4.137105   -2.855975    0.43968454 -0.34830642]]
[37m[1m[2023-06-25 04:04:50,955][129146] Moving the mean solution point...
[36m[2023-06-25 04:05:00,709][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 04:05:00,709][129146] FPS: 393774.02
[36m[2023-06-25 04:05:00,711][129146] itr=318, itrs=2000, Progress: 15.90%
[36m[2023-06-25 04:05:12,173][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 04:05:12,174][129146] FPS: 335489.01
[36m[2023-06-25 04:05:16,931][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:05:16,936][129146] Reward + Measures: [[64.16450473  0.98559165  0.00814367  0.9740926   0.51050162]]
[37m[1m[2023-06-25 04:05:16,936][129146] Max Reward on eval: 64.16450472955681
[37m[1m[2023-06-25 04:05:16,937][129146] Min Reward on eval: 64.16450472955681
[37m[1m[2023-06-25 04:05:16,937][129146] Mean Reward across all agents: 64.16450472955681
[37m[1m[2023-06-25 04:05:16,937][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:05:22,531][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:05:22,532][129146] Reward + Measures: [[328.65176193   0.77399999   0.1953       0.66589999   0.1849    ]
[37m[1m [249.66345304   0.7482       0.25240001   0.5284       0.17470001]
[37m[1m [240.15415814   0.49495035   0.25125575   0.39516434   0.30247405]
[37m[1m ...
[37m[1m [153.82175039   0.54330009   0.17789999   0.44400007   0.26030001]
[37m[1m [205.55088771   0.81580001   0.1328       0.80310005   0.1847    ]
[37m[1m [276.16346946   0.65189999   0.17010002   0.63099998   0.2947    ]]
[37m[1m[2023-06-25 04:05:22,532][129146] Max Reward on eval: 681.9932829596801
[37m[1m[2023-06-25 04:05:22,532][129146] Min Reward on eval: -204.9243265035446
[37m[1m[2023-06-25 04:05:22,532][129146] Mean Reward across all agents: 232.814603891166
[37m[1m[2023-06-25 04:05:22,533][129146] Average Trajectory Length: 998.2603333333333
[36m[2023-06-25 04:05:22,538][129146] mean_value=-60.503952610580065, max_value=765.323324559955
[37m[1m[2023-06-25 04:05:22,541][129146] New mean coefficients: [[ 0.9550804   4.3278503  -3.0886064   0.5927268  -0.18519193]]
[37m[1m[2023-06-25 04:05:22,543][129146] Moving the mean solution point...
[36m[2023-06-25 04:05:32,291][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 04:05:32,291][129146] FPS: 393987.12
[36m[2023-06-25 04:05:32,293][129146] itr=319, itrs=2000, Progress: 15.95%
[36m[2023-06-25 04:05:43,687][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 04:05:43,688][129146] FPS: 337516.65
[36m[2023-06-25 04:05:48,530][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:05:48,530][129146] Reward + Measures: [[84.18586346  0.99435264  0.002793    0.98820066  0.60747129]]
[37m[1m[2023-06-25 04:05:48,531][129146] Max Reward on eval: 84.18586345748744
[37m[1m[2023-06-25 04:05:48,531][129146] Min Reward on eval: 84.18586345748744
[37m[1m[2023-06-25 04:05:48,531][129146] Mean Reward across all agents: 84.18586345748744
[37m[1m[2023-06-25 04:05:48,531][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:05:53,951][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:05:53,952][129146] Reward + Measures: [[143.43886801   0.99720001   0.0012       0.99110001   0.83490002]
[37m[1m [ 50.14375763   0.91050005   0.0779       0.88669997   0.1908    ]
[37m[1m [ 11.18243453   0.93600005   0.0659       0.91520005   0.21729998]
[37m[1m ...
[37m[1m [ 71.36985729   0.98780006   0.0148       0.9781       0.44970003]
[37m[1m [142.76375808   0.99239999   0.0025       0.98340005   0.67990005]
[37m[1m [322.26090185   0.79879999   0.1007       0.71789998   0.4289    ]]
[37m[1m[2023-06-25 04:05:53,952][129146] Max Reward on eval: 373.20210804676174
[37m[1m[2023-06-25 04:05:53,952][129146] Min Reward on eval: -63.90141119665932
[37m[1m[2023-06-25 04:05:53,953][129146] Mean Reward across all agents: 92.08624536032382
[37m[1m[2023-06-25 04:05:53,953][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:05:53,958][129146] mean_value=90.86207681341111, max_value=605.4343363394406
[37m[1m[2023-06-25 04:05:53,960][129146] New mean coefficients: [[ 1.3038794   3.7567968  -3.035028    0.46118125  0.33817434]]
[37m[1m[2023-06-25 04:05:53,961][129146] Moving the mean solution point...
[36m[2023-06-25 04:06:03,584][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 04:06:03,585][129146] FPS: 399126.88
[36m[2023-06-25 04:06:03,587][129146] itr=320, itrs=2000, Progress: 16.00%
[37m[1m[2023-06-25 04:06:07,274][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000300
[36m[2023-06-25 04:06:19,106][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 04:06:19,106][129146] FPS: 332979.75
[36m[2023-06-25 04:06:23,770][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:06:23,770][129146] Reward + Measures: [[185.18956693   0.99843967   0.00178867   0.99441397   0.92989498]]
[37m[1m[2023-06-25 04:06:23,770][129146] Max Reward on eval: 185.18956692566792
[37m[1m[2023-06-25 04:06:23,771][129146] Min Reward on eval: 185.18956692566792
[37m[1m[2023-06-25 04:06:23,771][129146] Mean Reward across all agents: 185.18956692566792
[37m[1m[2023-06-25 04:06:23,771][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:06:29,167][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:06:29,167][129146] Reward + Measures: [[  132.57611455     0.88280004     0.18560003     0.85229999
[37m[1m      0.39629999]
[37m[1m [  124.8297938      0.38749999     0.26000002     0.32959998
[37m[1m      0.2227    ]
[37m[1m [-1282.28345028     0.99529999     0.0011         0.98909998
[37m[1m      0.80009997]
[37m[1m ...
[37m[1m [  432.67449781     0.52959996     0.31850001     0.42799997
[37m[1m      0.25670001]
[37m[1m [  121.4385981      0.73640001     0.26250002     0.62980002
[37m[1m      0.26760003]
[37m[1m [  346.60093982     0.59944606     0.28714603     0.44369522
[37m[1m      0.15292698]]
[37m[1m[2023-06-25 04:06:29,167][129146] Max Reward on eval: 646.1435624081757
[37m[1m[2023-06-25 04:06:29,168][129146] Min Reward on eval: -2042.6750943746417
[37m[1m[2023-06-25 04:06:29,168][129146] Mean Reward across all agents: -291.8370909116602
[37m[1m[2023-06-25 04:06:29,168][129146] Average Trajectory Length: 994.8679999999999
[36m[2023-06-25 04:06:29,172][129146] mean_value=-526.8784750288564, max_value=632.5761145549361
[37m[1m[2023-06-25 04:06:29,174][129146] New mean coefficients: [[ 1.8484778   4.1611743  -1.6503954   0.62296814  0.5623655 ]]
[37m[1m[2023-06-25 04:06:29,175][129146] Moving the mean solution point...
[36m[2023-06-25 04:06:38,960][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 04:06:38,960][129146] FPS: 392526.59
[36m[2023-06-25 04:06:38,962][129146] itr=321, itrs=2000, Progress: 16.05%
[36m[2023-06-25 04:06:50,631][129146] train() took 11.65 seconds to complete
[36m[2023-06-25 04:06:50,631][129146] FPS: 329645.81
[36m[2023-06-25 04:06:55,451][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:06:55,451][129146] Reward + Measures: [[459.07477934   0.5994367    0.3785533    0.32363367   0.25000134]]
[37m[1m[2023-06-25 04:06:55,451][129146] Max Reward on eval: 459.0747793406018
[37m[1m[2023-06-25 04:06:55,452][129146] Min Reward on eval: 459.0747793406018
[37m[1m[2023-06-25 04:06:55,452][129146] Mean Reward across all agents: 459.0747793406018
[37m[1m[2023-06-25 04:06:55,452][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:07:01,002][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:07:01,003][129146] Reward + Measures: [[ 44.38447129   0.72060007   0.21540001   0.70360005   0.1364    ]
[37m[1m [  7.12201867   0.76069999   0.2404       0.61840004   0.1373    ]
[37m[1m [ 65.65886374   0.74039996   0.2158       0.60650003   0.14229999]
[37m[1m ...
[37m[1m [176.5057129    0.73509997   0.30170003   0.53079998   0.17370002]
[37m[1m [661.71567073   0.54250002   0.44639999   0.2823       0.32429999]
[37m[1m [  8.15759796   0.74780005   0.23029999   0.69209999   0.127     ]]
[37m[1m[2023-06-25 04:07:01,008][129146] Max Reward on eval: 661.7156707262736
[37m[1m[2023-06-25 04:07:01,008][129146] Min Reward on eval: -55.20034468259401
[37m[1m[2023-06-25 04:07:01,009][129146] Mean Reward across all agents: 137.79491759851547
[37m[1m[2023-06-25 04:07:01,009][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:07:01,011][129146] mean_value=-200.08495733943258, max_value=594.3109624274763
[37m[1m[2023-06-25 04:07:01,013][129146] New mean coefficients: [[ 2.0597496   4.8535748  -2.5012465   1.0399158   0.61064583]]
[37m[1m[2023-06-25 04:07:01,014][129146] Moving the mean solution point...
[36m[2023-06-25 04:07:10,670][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 04:07:10,671][129146] FPS: 397735.51
[36m[2023-06-25 04:07:10,673][129146] itr=322, itrs=2000, Progress: 16.10%
[36m[2023-06-25 04:07:22,343][129146] train() took 11.65 seconds to complete
[36m[2023-06-25 04:07:22,343][129146] FPS: 329562.01
[36m[2023-06-25 04:07:27,157][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:07:27,157][129146] Reward + Measures: [[484.91859592   0.63405299   0.36011067   0.36530501   0.228044  ]]
[37m[1m[2023-06-25 04:07:27,157][129146] Max Reward on eval: 484.9185959201272
[37m[1m[2023-06-25 04:07:27,157][129146] Min Reward on eval: 484.9185959201272
[37m[1m[2023-06-25 04:07:27,158][129146] Mean Reward across all agents: 484.9185959201272
[37m[1m[2023-06-25 04:07:27,158][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:07:32,692][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:07:32,698][129146] Reward + Measures: [[695.8267557    0.59709996   0.33100003   0.28280002   0.2395    ]
[37m[1m [578.02785879   0.62700003   0.3039       0.3233       0.2314    ]
[37m[1m [606.64535195   0.6455       0.34710002   0.35700002   0.25040001]
[37m[1m ...
[37m[1m [760.72390214   0.63890004   0.3574       0.29340002   0.2454    ]
[37m[1m [534.49783943   0.59465182   0.27126795   0.2854479    0.22749086]
[37m[1m [740.31043518   0.59729999   0.36419997   0.28480002   0.271     ]]
[37m[1m[2023-06-25 04:07:32,698][129146] Max Reward on eval: 808.4788781381445
[37m[1m[2023-06-25 04:07:32,698][129146] Min Reward on eval: -60.442953742464304
[37m[1m[2023-06-25 04:07:32,699][129146] Mean Reward across all agents: 560.4780842629781
[37m[1m[2023-06-25 04:07:32,699][129146] Average Trajectory Length: 991.9603333333333
[36m[2023-06-25 04:07:32,701][129146] mean_value=-240.59973370904618, max_value=1078.323657303746
[37m[1m[2023-06-25 04:07:32,704][129146] New mean coefficients: [[ 2.0818012   5.1142473  -2.852536    0.98033434  0.46095386]]
[37m[1m[2023-06-25 04:07:32,705][129146] Moving the mean solution point...
[36m[2023-06-25 04:07:42,587][129146] train() took 9.88 seconds to complete
[36m[2023-06-25 04:07:42,587][129146] FPS: 388645.30
[36m[2023-06-25 04:07:42,589][129146] itr=323, itrs=2000, Progress: 16.15%
[36m[2023-06-25 04:07:54,165][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 04:07:54,165][129146] FPS: 332215.66
[36m[2023-06-25 04:07:58,955][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:07:58,955][129146] Reward + Measures: [[530.21756323   0.64870268   0.34678435   0.37375668   0.23067465]]
[37m[1m[2023-06-25 04:07:58,955][129146] Max Reward on eval: 530.2175632282848
[37m[1m[2023-06-25 04:07:58,956][129146] Min Reward on eval: 530.2175632282848
[37m[1m[2023-06-25 04:07:58,956][129146] Mean Reward across all agents: 530.2175632282848
[37m[1m[2023-06-25 04:07:58,956][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:08:04,441][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:08:04,442][129146] Reward + Measures: [[ 54.35468558   0.8002001    0.2395       0.7022       0.1177    ]
[37m[1m [-31.16488782   0.71500009   0.1793       0.61610001   0.16139999]
[37m[1m [ 11.79395561   0.77279997   0.18000001   0.69310004   0.18429999]
[37m[1m ...
[37m[1m [319.64560865   0.62950003   0.2359       0.45860001   0.2105    ]
[37m[1m [-30.88206127   0.73070002   0.1963       0.65970004   0.1463    ]
[37m[1m [ 60.40684278   0.74400008   0.21170001   0.63529998   0.15989999]]
[37m[1m[2023-06-25 04:08:04,442][129146] Max Reward on eval: 738.5328361286723
[37m[1m[2023-06-25 04:08:04,442][129146] Min Reward on eval: -50.764926285145336
[37m[1m[2023-06-25 04:08:04,443][129146] Mean Reward across all agents: 189.50536247228254
[37m[1m[2023-06-25 04:08:04,443][129146] Average Trajectory Length: 999.7183333333332
[36m[2023-06-25 04:08:04,444][129146] mean_value=-239.40166690098306, max_value=309.5773651776196
[37m[1m[2023-06-25 04:08:04,447][129146] New mean coefficients: [[ 2.3656797  5.178584  -2.2317467  1.0770844  0.5308918]]
[37m[1m[2023-06-25 04:08:04,448][129146] Moving the mean solution point...
[36m[2023-06-25 04:08:14,167][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 04:08:14,167][129146] FPS: 395159.93
[36m[2023-06-25 04:08:14,170][129146] itr=324, itrs=2000, Progress: 16.20%
[36m[2023-06-25 04:08:25,695][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 04:08:25,695][129146] FPS: 333661.15
[36m[2023-06-25 04:08:30,491][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:08:30,491][129146] Reward + Measures: [[575.68272015   0.66179574   0.33329844   0.38294059   0.23315206]]
[37m[1m[2023-06-25 04:08:30,491][129146] Max Reward on eval: 575.6827201527248
[37m[1m[2023-06-25 04:08:30,492][129146] Min Reward on eval: 575.6827201527248
[37m[1m[2023-06-25 04:08:30,492][129146] Mean Reward across all agents: 575.6827201527248
[37m[1m[2023-06-25 04:08:30,492][129146] Average Trajectory Length: 999.9186666666666
[36m[2023-06-25 04:08:35,918][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:08:35,918][129146] Reward + Measures: [[352.27308219   0.63339996   0.27430001   0.4598       0.2538    ]
[37m[1m [606.13906523   0.625        0.36239997   0.35750002   0.28580004]
[37m[1m [306.22461425   0.73140001   0.24330001   0.53549999   0.2357    ]
[37m[1m ...
[37m[1m [238.77170502   0.69140005   0.22669999   0.53700006   0.30969998]
[37m[1m [250.97509541   0.70110005   0.28029999   0.52699995   0.24609999]
[37m[1m [366.03698145   0.71500009   0.2595       0.53330004   0.2845    ]]
[37m[1m[2023-06-25 04:08:35,919][129146] Max Reward on eval: 726.3372955123486
[37m[1m[2023-06-25 04:08:35,919][129146] Min Reward on eval: -20.488290146843063
[37m[1m[2023-06-25 04:08:35,919][129146] Mean Reward across all agents: 219.1431650389857
[37m[1m[2023-06-25 04:08:35,919][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:08:35,921][129146] mean_value=-215.66575915624682, max_value=542.7447037797189
[37m[1m[2023-06-25 04:08:35,924][129146] New mean coefficients: [[ 2.1670518  5.1491914 -3.0544312  1.2244618  0.9343551]]
[37m[1m[2023-06-25 04:08:35,925][129146] Moving the mean solution point...
[36m[2023-06-25 04:08:45,608][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 04:08:45,608][129146] FPS: 396629.56
[36m[2023-06-25 04:08:45,610][129146] itr=325, itrs=2000, Progress: 16.25%
[36m[2023-06-25 04:08:57,044][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 04:08:57,045][129146] FPS: 336376.74
[36m[2023-06-25 04:09:01,814][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:09:01,815][129146] Reward + Measures: [[602.24534717   0.68289369   0.31059232   0.39170295   0.237579  ]]
[37m[1m[2023-06-25 04:09:01,815][129146] Max Reward on eval: 602.24534717354
[37m[1m[2023-06-25 04:09:01,815][129146] Min Reward on eval: 602.24534717354
[37m[1m[2023-06-25 04:09:01,816][129146] Mean Reward across all agents: 602.24534717354
[37m[1m[2023-06-25 04:09:01,816][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:09:07,269][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:09:07,270][129146] Reward + Measures: [[259.09526652   0.78739995   0.33090001   0.54510003   0.1319    ]
[37m[1m [626.66448342   0.69580001   0.3407       0.38         0.2332    ]
[37m[1m [189.51557732   0.8082       0.2343       0.63129997   0.13940001]
[37m[1m ...
[37m[1m [592.64955051   0.71239996   0.30859998   0.42250004   0.22469997]
[37m[1m [167.59071235   0.80100006   0.26300001   0.57520002   0.1331    ]
[37m[1m [420.91786871   0.75020003   0.34290001   0.48810002   0.17279999]]
[37m[1m[2023-06-25 04:09:07,270][129146] Max Reward on eval: 804.4216825528332
[37m[1m[2023-06-25 04:09:07,270][129146] Min Reward on eval: -9.91687104346929
[37m[1m[2023-06-25 04:09:07,270][129146] Mean Reward across all agents: 314.4059922105402
[37m[1m[2023-06-25 04:09:07,271][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:09:07,275][129146] mean_value=-6.8124408714956495, max_value=624.8752892299304
[37m[1m[2023-06-25 04:09:07,278][129146] New mean coefficients: [[ 2.5574455  5.619687  -2.0505846  1.2988794  0.6888152]]
[37m[1m[2023-06-25 04:09:07,279][129146] Moving the mean solution point...
[36m[2023-06-25 04:09:17,008][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 04:09:17,008][129146] FPS: 394760.67
[36m[2023-06-25 04:09:17,010][129146] itr=326, itrs=2000, Progress: 16.30%
[36m[2023-06-25 04:09:28,620][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 04:09:28,620][129146] FPS: 331257.61
[36m[2023-06-25 04:09:33,422][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:09:33,422][129146] Reward + Measures: [[684.00452039   0.68957603   0.30193132   0.38908234   0.24772833]]
[37m[1m[2023-06-25 04:09:33,423][129146] Max Reward on eval: 684.0045203865243
[37m[1m[2023-06-25 04:09:33,423][129146] Min Reward on eval: 684.0045203865243
[37m[1m[2023-06-25 04:09:33,423][129146] Mean Reward across all agents: 684.0045203865243
[37m[1m[2023-06-25 04:09:33,423][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:09:39,158][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:09:39,159][129146] Reward + Measures: [[425.94111989   0.7701       0.22729997   0.48020002   0.2464    ]
[37m[1m [199.79019041   0.79350001   0.18740001   0.61060005   0.24770001]
[37m[1m [629.98553979   0.66470003   0.30210003   0.38320002   0.2339    ]
[37m[1m ...
[37m[1m [293.00132675   0.78879994   0.16950001   0.5783       0.30370003]
[37m[1m [427.46189053   0.75300008   0.29800001   0.49049997   0.206     ]
[37m[1m [580.25755536   0.69659996   0.36320001   0.4312       0.2402    ]]
[37m[1m[2023-06-25 04:09:39,159][129146] Max Reward on eval: 723.7058388579055
[37m[1m[2023-06-25 04:09:39,160][129146] Min Reward on eval: 21.054841386328917
[37m[1m[2023-06-25 04:09:39,160][129146] Mean Reward across all agents: 325.94879299286396
[37m[1m[2023-06-25 04:09:39,160][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:09:39,163][129146] mean_value=-41.34690716060856, max_value=1053.6419373638578
[37m[1m[2023-06-25 04:09:39,165][129146] New mean coefficients: [[ 1.0379858   5.7460246  -1.9699286   1.1716874   0.14171988]]
[37m[1m[2023-06-25 04:09:39,166][129146] Moving the mean solution point...
[36m[2023-06-25 04:09:49,003][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 04:09:49,003][129146] FPS: 390428.97
[36m[2023-06-25 04:09:49,006][129146] itr=327, itrs=2000, Progress: 16.35%
[36m[2023-06-25 04:10:00,538][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 04:10:00,538][129146] FPS: 333517.84
[36m[2023-06-25 04:10:05,387][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:10:05,387][129146] Reward + Measures: [[546.78037394   0.75276762   0.24203566   0.45676899   0.23172098]]
[37m[1m[2023-06-25 04:10:05,387][129146] Max Reward on eval: 546.7803739410873
[37m[1m[2023-06-25 04:10:05,387][129146] Min Reward on eval: 546.7803739410873
[37m[1m[2023-06-25 04:10:05,388][129146] Mean Reward across all agents: 546.7803739410873
[37m[1m[2023-06-25 04:10:05,388][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:10:10,927][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:10:10,927][129146] Reward + Measures: [[136.41343326   0.85670006   0.19329999   0.64449996   0.18359999]
[37m[1m [411.17217132   0.78330004   0.27270001   0.4948       0.1707    ]
[37m[1m [576.05793888   0.76970005   0.24010001   0.43420002   0.23670001]
[37m[1m ...
[37m[1m [236.02485864   0.81239998   0.26660001   0.59540004   0.1259    ]
[37m[1m [163.34857515   0.8355999    0.2084       0.67119998   0.20020001]
[37m[1m [374.56783975   0.7985       0.30580002   0.5503       0.1288    ]]
[37m[1m[2023-06-25 04:10:10,927][129146] Max Reward on eval: 775.2154478412006
[37m[1m[2023-06-25 04:10:10,928][129146] Min Reward on eval: 99.61609167659772
[37m[1m[2023-06-25 04:10:10,928][129146] Mean Reward across all agents: 333.31432156370875
[37m[1m[2023-06-25 04:10:10,928][129146] Average Trajectory Length: 999.8926666666666
[36m[2023-06-25 04:10:10,934][129146] mean_value=172.67946971346714, max_value=1081.115476314124
[37m[1m[2023-06-25 04:10:10,937][129146] New mean coefficients: [[-0.13169003  5.6253295  -1.9505944   1.3075258   0.05115935]]
[37m[1m[2023-06-25 04:10:10,938][129146] Moving the mean solution point...
[36m[2023-06-25 04:10:20,695][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 04:10:20,695][129146] FPS: 393628.52
[36m[2023-06-25 04:10:20,697][129146] itr=328, itrs=2000, Progress: 16.40%
[36m[2023-06-25 04:10:32,110][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 04:10:32,110][129146] FPS: 336967.09
[36m[2023-06-25 04:10:36,828][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:10:36,828][129146] Reward + Measures: [[65.797801    0.87748635  0.16124001  0.68672866  0.19419166]]
[37m[1m[2023-06-25 04:10:36,829][129146] Max Reward on eval: 65.79780100100007
[37m[1m[2023-06-25 04:10:36,829][129146] Min Reward on eval: 65.79780100100007
[37m[1m[2023-06-25 04:10:36,829][129146] Mean Reward across all agents: 65.79780100100007
[37m[1m[2023-06-25 04:10:36,830][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:10:42,250][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:10:42,251][129146] Reward + Measures: [[172.58660094   0.86310005   0.26010001   0.64210004   0.1217    ]
[37m[1m [ 27.19054216   0.82859993   0.1556       0.73939997   0.1821    ]
[37m[1m [ 22.85586729   0.86499995   0.14130001   0.72630006   0.2095    ]
[37m[1m ...
[37m[1m [ 64.17204852   0.77940005   0.12149999   0.71959996   0.24919999]
[37m[1m [ 86.48309772   0.84609997   0.19799998   0.66180003   0.16739999]
[37m[1m [ 75.54975329   0.87690002   0.28500003   0.69309998   0.0717    ]]
[37m[1m[2023-06-25 04:10:42,251][129146] Max Reward on eval: 407.4974377548671
[37m[1m[2023-06-25 04:10:42,251][129146] Min Reward on eval: -10.909466749033891
[37m[1m[2023-06-25 04:10:42,252][129146] Mean Reward across all agents: 70.0674723337647
[37m[1m[2023-06-25 04:10:42,252][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:10:42,254][129146] mean_value=-139.72094260946696, max_value=629.0680726861261
[37m[1m[2023-06-25 04:10:42,256][129146] New mean coefficients: [[-1.6387388   5.5948534  -0.7742605   1.3171057  -0.34951147]]
[37m[1m[2023-06-25 04:10:42,257][129146] Moving the mean solution point...
[36m[2023-06-25 04:10:52,014][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 04:10:52,014][129146] FPS: 393644.36
[36m[2023-06-25 04:10:52,017][129146] itr=329, itrs=2000, Progress: 16.45%
[36m[2023-06-25 04:11:03,599][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 04:11:03,599][129146] FPS: 332021.92
[36m[2023-06-25 04:11:08,537][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:11:08,537][129146] Reward + Measures: [[6.47106981 0.90466398 0.201849   0.72754532 0.13110766]]
[37m[1m[2023-06-25 04:11:08,537][129146] Max Reward on eval: 6.471069810039456
[37m[1m[2023-06-25 04:11:08,537][129146] Min Reward on eval: 6.471069810039456
[37m[1m[2023-06-25 04:11:08,538][129146] Mean Reward across all agents: 6.471069810039456
[37m[1m[2023-06-25 04:11:08,538][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:11:14,017][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:11:14,018][129146] Reward + Measures: [[-50.23605038   0.84429997   0.17279999   0.74260002   0.1838    ]
[37m[1m [-53.05281448   0.85529995   0.21400002   0.69150007   0.1294    ]
[37m[1m [-23.16918486   0.84939998   0.1902       0.67180002   0.15030001]
[37m[1m ...
[37m[1m [-60.82096574   0.85710001   0.17479999   0.66289997   0.14210001]
[37m[1m [ 42.82253525   0.86329997   0.21720003   0.71970004   0.1098    ]
[37m[1m [ -6.2139361    0.78259993   0.24330001   0.58169997   0.0795    ]]
[37m[1m[2023-06-25 04:11:14,018][129146] Max Reward on eval: 107.72270243867533
[37m[1m[2023-06-25 04:11:14,018][129146] Min Reward on eval: -130.54596608190914
[37m[1m[2023-06-25 04:11:14,019][129146] Mean Reward across all agents: -42.75292040744327
[37m[1m[2023-06-25 04:11:14,019][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:11:14,021][129146] mean_value=-167.9004102992048, max_value=607.7227024386754
[37m[1m[2023-06-25 04:11:14,023][129146] New mean coefficients: [[-1.2180226  6.132374  -2.4928312  1.3329204 -0.316072 ]]
[37m[1m[2023-06-25 04:11:14,024][129146] Moving the mean solution point...
[36m[2023-06-25 04:11:23,905][129146] train() took 9.88 seconds to complete
[36m[2023-06-25 04:11:23,905][129146] FPS: 388722.98
[36m[2023-06-25 04:11:23,907][129146] itr=330, itrs=2000, Progress: 16.50%
[37m[1m[2023-06-25 04:11:27,870][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000310
[36m[2023-06-25 04:11:39,750][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 04:11:39,750][129146] FPS: 332110.99
[36m[2023-06-25 04:11:44,526][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:11:44,526][129146] Reward + Measures: [[-21.26310386   0.92436194   0.21806432   0.73945493   0.10124367]]
[37m[1m[2023-06-25 04:11:44,527][129146] Max Reward on eval: -21.263103864662646
[37m[1m[2023-06-25 04:11:44,527][129146] Min Reward on eval: -21.263103864662646
[37m[1m[2023-06-25 04:11:44,527][129146] Mean Reward across all agents: -21.263103864662646
[37m[1m[2023-06-25 04:11:44,527][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:11:50,061][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:11:50,105][129146] Reward + Measures: [[273.62047499   0.69919997   0.33230001   0.45190001   0.1409    ]
[37m[1m [ -3.24008646   0.8599       0.23110001   0.69         0.09960001]
[37m[1m [642.77484285   0.62199992   0.30700001   0.38050005   0.3105    ]
[37m[1m ...
[37m[1m [-95.88437965   0.6171       0.32340002   0.37920001   0.09280001]
[37m[1m [-33.92035359   0.82840008   0.3493       0.60170001   0.0342    ]
[37m[1m [ 83.29362183   0.74729997   0.40940005   0.5104       0.0524    ]]
[37m[1m[2023-06-25 04:11:50,106][129146] Max Reward on eval: 810.9010037738365
[37m[1m[2023-06-25 04:11:50,106][129146] Min Reward on eval: -443.0501969744684
[37m[1m[2023-06-25 04:11:50,106][129146] Mean Reward across all agents: 66.67885010452679
[37m[1m[2023-06-25 04:11:50,106][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:11:50,111][129146] mean_value=81.16198655401976, max_value=698.6054529771384
[37m[1m[2023-06-25 04:11:50,114][129146] New mean coefficients: [[-2.0488431  6.3296585 -2.0051808  1.1687874 -0.4979489]]
[37m[1m[2023-06-25 04:11:50,115][129146] Moving the mean solution point...
[36m[2023-06-25 04:11:59,775][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 04:11:59,775][129146] FPS: 397552.07
[36m[2023-06-25 04:11:59,778][129146] itr=331, itrs=2000, Progress: 16.55%
[36m[2023-06-25 04:12:11,208][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 04:12:11,209][129146] FPS: 336484.30
[36m[2023-06-25 04:12:16,077][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:12:16,078][129146] Reward + Measures: [[-89.2482129    0.94107497   0.186519     0.75762928   0.10743233]]
[37m[1m[2023-06-25 04:12:16,078][129146] Max Reward on eval: -89.24821290225691
[37m[1m[2023-06-25 04:12:16,078][129146] Min Reward on eval: -89.24821290225691
[37m[1m[2023-06-25 04:12:16,078][129146] Mean Reward across all agents: -89.24821290225691
[37m[1m[2023-06-25 04:12:16,079][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:12:21,587][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:12:21,587][129146] Reward + Measures: [[-38.51716861   0.94119996   0.1628       0.85319996   0.17660001]
[37m[1m [ 32.08029039   0.90280002   0.0414       0.9368       0.53140002]
[37m[1m [ 44.99299416   0.91450006   0.06720001   0.91219997   0.40450001]
[37m[1m ...
[37m[1m [ 39.02340504   0.9235       0.0217       0.96709996   0.65029997]
[37m[1m [  1.3393782    0.93610001   0.0954       0.90529996   0.34690002]
[37m[1m [ -5.08491755   0.92229998   0.2911       0.7762       0.0545    ]]
[37m[1m[2023-06-25 04:12:21,600][129146] Max Reward on eval: 130.65493151711925
[37m[1m[2023-06-25 04:12:21,600][129146] Min Reward on eval: -104.353139809391
[37m[1m[2023-06-25 04:12:21,601][129146] Mean Reward across all agents: -7.685227168517494
[37m[1m[2023-06-25 04:12:21,601][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:12:21,603][129146] mean_value=-35.18515939545592, max_value=539.0234050351428
[37m[1m[2023-06-25 04:12:21,606][129146] New mean coefficients: [[-1.8593773   6.3026605  -2.2740443   1.030675   -0.58795655]]
[37m[1m[2023-06-25 04:12:21,607][129146] Moving the mean solution point...
[36m[2023-06-25 04:12:31,335][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 04:12:31,335][129146] FPS: 394806.03
[36m[2023-06-25 04:12:31,337][129146] itr=332, itrs=2000, Progress: 16.60%
[36m[2023-06-25 04:12:42,789][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 04:12:42,789][129146] FPS: 335790.94
[36m[2023-06-25 04:12:47,526][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:12:47,526][129146] Reward + Measures: [[-91.80731228   0.94922632   0.17861432   0.78471303   0.10301566]]
[37m[1m[2023-06-25 04:12:47,526][129146] Max Reward on eval: -91.80731228247626
[37m[1m[2023-06-25 04:12:47,526][129146] Min Reward on eval: -91.80731228247626
[37m[1m[2023-06-25 04:12:47,527][129146] Mean Reward across all agents: -91.80731228247626
[37m[1m[2023-06-25 04:12:47,527][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:12:53,074][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:12:53,075][129146] Reward + Measures: [[-42.03525764   0.88550007   0.11129999   0.68599999   0.23049998]
[37m[1m [ 92.23690233   0.85500002   0.1231       0.68970007   0.25860003]
[37m[1m [ 32.39530087   0.91750002   0.1776       0.77939999   0.149     ]
[37m[1m ...
[37m[1m [-26.18463451   0.91009998   0.1294       0.74550003   0.1884    ]
[37m[1m [ -2.49188175   0.93050003   0.16900001   0.76000005   0.15210001]
[37m[1m [315.83124337   0.81129998   0.24590002   0.56559998   0.18610001]]
[37m[1m[2023-06-25 04:12:53,075][129146] Max Reward on eval: 403.0419167935499
[37m[1m[2023-06-25 04:12:53,075][129146] Min Reward on eval: -143.48239186275168
[37m[1m[2023-06-25 04:12:53,075][129146] Mean Reward across all agents: 40.91913076711855
[37m[1m[2023-06-25 04:12:53,076][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:12:53,078][129146] mean_value=-122.3538595723207, max_value=632.0839759666123
[37m[1m[2023-06-25 04:12:53,081][129146] New mean coefficients: [[-0.8644635  5.556921  -1.9322317  1.0751522 -0.389082 ]]
[37m[1m[2023-06-25 04:12:53,082][129146] Moving the mean solution point...
[36m[2023-06-25 04:13:02,745][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 04:13:02,746][129146] FPS: 397429.75
[36m[2023-06-25 04:13:02,748][129146] itr=333, itrs=2000, Progress: 16.65%
[36m[2023-06-25 04:13:14,152][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 04:13:14,152][129146] FPS: 337198.85
[36m[2023-06-25 04:13:18,955][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:13:18,956][129146] Reward + Measures: [[-98.1410451    0.94606167   0.120069     0.78319627   0.15556733]]
[37m[1m[2023-06-25 04:13:18,956][129146] Max Reward on eval: -98.14104509721732
[37m[1m[2023-06-25 04:13:18,956][129146] Min Reward on eval: -98.14104509721732
[37m[1m[2023-06-25 04:13:18,956][129146] Mean Reward across all agents: -98.14104509721732
[37m[1m[2023-06-25 04:13:18,957][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:13:24,465][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:13:24,466][129146] Reward + Measures: [[-107.54737804    0.954         0.35760003    0.80330002    0.0187    ]
[37m[1m [-122.36688772    0.94209999    0.21270001    0.74970007    0.0697    ]
[37m[1m [ 176.74827447    0.7858001     0.17569999    0.46430001    0.22130001]
[37m[1m ...
[37m[1m [-126.11899756    0.95180005    0.2383        0.77319998    0.0506    ]
[37m[1m [ -57.12034058    0.93460006    0.20970002    0.6839        0.0954    ]
[37m[1m [ -58.4564888     0.95440006    0.47670004    0.87250006    0.0096    ]]
[37m[1m[2023-06-25 04:13:24,466][129146] Max Reward on eval: 745.856611083087
[37m[1m[2023-06-25 04:13:24,466][129146] Min Reward on eval: -195.3841651064693
[37m[1m[2023-06-25 04:13:24,466][129146] Mean Reward across all agents: -42.09384423180339
[37m[1m[2023-06-25 04:13:24,467][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:13:24,470][129146] mean_value=-60.749917890448586, max_value=1180.5112084213877
[37m[1m[2023-06-25 04:13:24,473][129146] New mean coefficients: [[-1.9142389   4.4337006  -0.86188996  1.0045836  -0.4629762 ]]
[37m[1m[2023-06-25 04:13:24,474][129146] Moving the mean solution point...
[36m[2023-06-25 04:13:34,182][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 04:13:34,183][129146] FPS: 395577.29
[36m[2023-06-25 04:13:34,185][129146] itr=334, itrs=2000, Progress: 16.70%
[36m[2023-06-25 04:13:45,621][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 04:13:45,622][129146] FPS: 336255.10
[36m[2023-06-25 04:13:50,351][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:13:50,352][129146] Reward + Measures: [[496.2159871    0.67237937   0.37014663   0.41737199   0.21076599]]
[37m[1m[2023-06-25 04:13:50,352][129146] Max Reward on eval: 496.21598709885797
[37m[1m[2023-06-25 04:13:50,352][129146] Min Reward on eval: 496.21598709885797
[37m[1m[2023-06-25 04:13:50,352][129146] Mean Reward across all agents: 496.21598709885797
[37m[1m[2023-06-25 04:13:50,353][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:13:55,742][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:13:55,742][129146] Reward + Measures: [[413.79643946   0.68400002   0.36310002   0.44730002   0.19250001]
[37m[1m [385.18677627   0.66280001   0.38639998   0.4316       0.18609999]
[37m[1m [337.96336105   0.72170007   0.37900004   0.45930001   0.1049    ]
[37m[1m ...
[37m[1m [335.67574647   0.67469996   0.34549999   0.39129999   0.22310002]
[37m[1m [406.61043462   0.6803       0.38600001   0.42990002   0.19860001]
[37m[1m [300.12002223   0.68559998   0.36539999   0.40949997   0.19719999]]
[37m[1m[2023-06-25 04:13:55,742][129146] Max Reward on eval: 599.629588073201
[37m[1m[2023-06-25 04:13:55,743][129146] Min Reward on eval: 199.8734524517553
[37m[1m[2023-06-25 04:13:55,743][129146] Mean Reward across all agents: 396.77463079973495
[37m[1m[2023-06-25 04:13:55,743][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:13:55,745][129146] mean_value=-164.32568097419468, max_value=331.572559475955
[37m[1m[2023-06-25 04:13:55,747][129146] New mean coefficients: [[-1.7918214   3.4056184  -0.2045474   0.8549457  -0.42578664]]
[37m[1m[2023-06-25 04:13:55,748][129146] Moving the mean solution point...
[36m[2023-06-25 04:14:05,481][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 04:14:05,482][129146] FPS: 394578.25
[36m[2023-06-25 04:14:05,484][129146] itr=335, itrs=2000, Progress: 16.75%
[36m[2023-06-25 04:14:17,003][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 04:14:17,003][129146] FPS: 333829.98
[36m[2023-06-25 04:14:21,868][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:14:21,869][129146] Reward + Measures: [[439.94767557   0.69100326   0.35578799   0.44259065   0.20282768]]
[37m[1m[2023-06-25 04:14:21,869][129146] Max Reward on eval: 439.9476755675333
[37m[1m[2023-06-25 04:14:21,869][129146] Min Reward on eval: 439.9476755675333
[37m[1m[2023-06-25 04:14:21,869][129146] Mean Reward across all agents: 439.9476755675333
[37m[1m[2023-06-25 04:14:21,870][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:14:27,317][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:14:27,318][129146] Reward + Measures: [[389.49874992   0.6979       0.37339997   0.43940002   0.20059998]
[37m[1m [340.96166497   0.71799994   0.30850002   0.52020001   0.1699    ]
[37m[1m [390.57883712   0.68970007   0.32389998   0.45280001   0.21250001]
[37m[1m ...
[37m[1m [391.28535722   0.69840002   0.354        0.46790001   0.1706    ]
[37m[1m [305.06299882   0.70640004   0.29800001   0.4725       0.2405    ]
[37m[1m [361.44856579   0.7101       0.30920002   0.47160003   0.2174    ]]
[37m[1m[2023-06-25 04:14:27,318][129146] Max Reward on eval: 491.35335810015675
[37m[1m[2023-06-25 04:14:27,318][129146] Min Reward on eval: 212.38333960694726
[37m[1m[2023-06-25 04:14:27,318][129146] Mean Reward across all agents: 364.5104561526177
[37m[1m[2023-06-25 04:14:27,319][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:14:27,320][129146] mean_value=-149.59835536729662, max_value=790.6946944641206
[37m[1m[2023-06-25 04:14:27,323][129146] New mean coefficients: [[-3.055684    2.003111    0.69164824  0.7134977  -0.6050188 ]]
[37m[1m[2023-06-25 04:14:27,324][129146] Moving the mean solution point...
[36m[2023-06-25 04:14:37,131][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 04:14:37,131][129146] FPS: 391606.19
[36m[2023-06-25 04:14:37,133][129146] itr=336, itrs=2000, Progress: 16.80%
[36m[2023-06-25 04:14:48,615][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 04:14:48,615][129146] FPS: 334941.33
[36m[2023-06-25 04:14:53,157][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:14:53,158][129146] Reward + Measures: [[372.08349459   0.69450128   0.35218567   0.45634198   0.19851001]]
[37m[1m[2023-06-25 04:14:53,158][129146] Max Reward on eval: 372.08349459234284
[37m[1m[2023-06-25 04:14:53,158][129146] Min Reward on eval: 372.08349459234284
[37m[1m[2023-06-25 04:14:53,158][129146] Mean Reward across all agents: 372.08349459234284
[37m[1m[2023-06-25 04:14:53,159][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:14:58,684][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:14:58,685][129146] Reward + Measures: [[304.5206687    0.67140001   0.3671       0.46779999   0.182     ]
[37m[1m [360.78457397   0.6724       0.3529       0.46939999   0.21339999]
[37m[1m [361.94412188   0.63690007   0.36740002   0.45650002   0.2158    ]
[37m[1m ...
[37m[1m [362.10242731   0.65679997   0.35819998   0.43850002   0.22400002]
[37m[1m [265.92594091   0.68780005   0.35600001   0.46880004   0.21869998]
[37m[1m [411.16571754   0.65950006   0.38850001   0.40279999   0.22260001]]
[37m[1m[2023-06-25 04:14:58,685][129146] Max Reward on eval: 454.42143406990215
[37m[1m[2023-06-25 04:14:58,685][129146] Min Reward on eval: 250.64455035803257
[37m[1m[2023-06-25 04:14:58,685][129146] Mean Reward across all agents: 369.4595140574334
[37m[1m[2023-06-25 04:14:58,686][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:14:58,688][129146] mean_value=-172.35703539701424, max_value=783.1600500482173
[37m[1m[2023-06-25 04:14:58,690][129146] New mean coefficients: [[-1.8834963   3.5154858  -2.4931364   0.52608895 -0.33174258]]
[37m[1m[2023-06-25 04:14:58,691][129146] Moving the mean solution point...
[36m[2023-06-25 04:15:08,409][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 04:15:08,409][129146] FPS: 395236.50
[36m[2023-06-25 04:15:08,411][129146] itr=337, itrs=2000, Progress: 16.85%
[36m[2023-06-25 04:15:19,923][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 04:15:19,923][129146] FPS: 334041.41
[36m[2023-06-25 04:15:24,710][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:15:24,711][129146] Reward + Measures: [[317.98432489   0.70557964   0.32841834   0.46854931   0.21074967]]
[37m[1m[2023-06-25 04:15:24,711][129146] Max Reward on eval: 317.98432489316343
[37m[1m[2023-06-25 04:15:24,711][129146] Min Reward on eval: 317.98432489316343
[37m[1m[2023-06-25 04:15:24,711][129146] Mean Reward across all agents: 317.98432489316343
[37m[1m[2023-06-25 04:15:24,711][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:15:30,127][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:15:30,127][129146] Reward + Measures: [[336.9383647    0.73680001   0.34109998   0.49169999   0.1573    ]
[37m[1m [318.57663447   0.73070002   0.35440001   0.51550001   0.16230001]
[37m[1m [383.79031413   0.72789997   0.33790001   0.47620001   0.17310001]
[37m[1m ...
[37m[1m [307.32094039   0.73110002   0.32440001   0.52430004   0.1362    ]
[37m[1m [151.83327106   0.69620001   0.42420003   0.39840004   0.32519999]
[37m[1m [230.90138437   0.70900005   0.37070003   0.4883       0.1753    ]]
[37m[1m[2023-06-25 04:15:30,127][129146] Max Reward on eval: 476.92692931508645
[37m[1m[2023-06-25 04:15:30,128][129146] Min Reward on eval: 84.31132518168015
[37m[1m[2023-06-25 04:15:30,128][129146] Mean Reward across all agents: 294.1268562056434
[37m[1m[2023-06-25 04:15:30,128][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:15:30,130][129146] mean_value=-115.8136773711758, max_value=652.101485163453
[37m[1m[2023-06-25 04:15:30,133][129146] New mean coefficients: [[-3.677167    4.277511   -3.6043365   0.37796783 -0.93451655]]
[37m[1m[2023-06-25 04:15:30,134][129146] Moving the mean solution point...
[36m[2023-06-25 04:15:39,838][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 04:15:39,838][129146] FPS: 395767.57
[36m[2023-06-25 04:15:39,841][129146] itr=338, itrs=2000, Progress: 16.90%
[36m[2023-06-25 04:15:51,397][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 04:15:51,397][129146] FPS: 332735.09
[36m[2023-06-25 04:15:56,347][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:15:56,347][129146] Reward + Measures: [[240.42525819   0.71184325   0.30888501   0.48475167   0.21177401]]
[37m[1m[2023-06-25 04:15:56,348][129146] Max Reward on eval: 240.42525819107456
[37m[1m[2023-06-25 04:15:56,348][129146] Min Reward on eval: 240.42525819107456
[37m[1m[2023-06-25 04:15:56,348][129146] Mean Reward across all agents: 240.42525819107456
[37m[1m[2023-06-25 04:15:56,348][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:16:01,984][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:16:01,985][129146] Reward + Measures: [[276.74765545   0.72540003   0.2809       0.55159998   0.1779    ]
[37m[1m [121.59291386   0.68550009   0.27849999   0.4831       0.19669999]
[37m[1m [215.45138135   0.73519999   0.3339       0.5298       0.133     ]
[37m[1m ...
[37m[1m [275.67321728   0.7123       0.29370001   0.4876       0.1772    ]
[37m[1m [300.7937806    0.72369999   0.30560002   0.4937       0.17639999]
[37m[1m [206.75303571   0.7295       0.29120001   0.49950001   0.22839999]]
[37m[1m[2023-06-25 04:16:01,990][129146] Max Reward on eval: 390.9628388778423
[37m[1m[2023-06-25 04:16:01,991][129146] Min Reward on eval: -60.59653097425471
[37m[1m[2023-06-25 04:16:01,992][129146] Mean Reward across all agents: 234.62989701919062
[37m[1m[2023-06-25 04:16:01,992][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:16:01,995][129146] mean_value=-193.49640170738644, max_value=146.63625151660844
[37m[1m[2023-06-25 04:16:02,000][129146] New mean coefficients: [[-4.0003834  3.4215293 -3.340545   0.4435436 -1.0423892]]
[37m[1m[2023-06-25 04:16:02,002][129146] Moving the mean solution point...
[36m[2023-06-25 04:16:11,829][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 04:16:11,830][129146] FPS: 390817.53
[36m[2023-06-25 04:16:11,832][129146] itr=339, itrs=2000, Progress: 16.95%
[36m[2023-06-25 04:16:23,327][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 04:16:23,327][129146] FPS: 334529.59
[36m[2023-06-25 04:16:28,063][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:16:28,063][129146] Reward + Measures: [[163.97255213   0.71675664   0.29404899   0.49594197   0.217262  ]]
[37m[1m[2023-06-25 04:16:28,063][129146] Max Reward on eval: 163.97255212759688
[37m[1m[2023-06-25 04:16:28,063][129146] Min Reward on eval: 163.97255212759688
[37m[1m[2023-06-25 04:16:28,064][129146] Mean Reward across all agents: 163.97255212759688
[37m[1m[2023-06-25 04:16:28,064][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:16:33,451][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:16:33,451][129146] Reward + Measures: [[121.31121577   0.75059998   0.25250003   0.51940006   0.2342    ]
[37m[1m [146.90830496   0.84440005   0.1349       0.72450006   0.1429    ]
[37m[1m [195.51255902   0.80170006   0.18020001   0.65350002   0.1451    ]
[37m[1m ...
[37m[1m [164.994803     0.77410001   0.21509998   0.56879997   0.2069    ]
[37m[1m [132.76959974   0.85320008   0.1365       0.72010005   0.15269999]
[37m[1m [135.23385666   0.71069998   0.28470001   0.46700001   0.2685    ]]
[37m[1m[2023-06-25 04:16:33,451][129146] Max Reward on eval: 239.31603981833905
[37m[1m[2023-06-25 04:16:33,452][129146] Min Reward on eval: -22.661804787028814
[37m[1m[2023-06-25 04:16:33,452][129146] Mean Reward across all agents: 157.44739126311876
[37m[1m[2023-06-25 04:16:33,452][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:16:33,454][129146] mean_value=-255.4970798299284, max_value=99.41234677408477
[37m[1m[2023-06-25 04:16:33,456][129146] New mean coefficients: [[-3.0141969   4.725426   -5.793517    0.12212363 -1.0087742 ]]
[37m[1m[2023-06-25 04:16:33,457][129146] Moving the mean solution point...
[36m[2023-06-25 04:16:43,096][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 04:16:43,096][129146] FPS: 398461.83
[36m[2023-06-25 04:16:43,098][129146] itr=340, itrs=2000, Progress: 17.00%
[37m[1m[2023-06-25 04:16:46,785][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000320
[36m[2023-06-25 04:16:58,571][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 04:16:58,572][129146] FPS: 334540.48
[36m[2023-06-25 04:17:03,393][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:17:03,394][129146] Reward + Measures: [[146.30983762   0.73416567   0.25925067   0.52688438   0.21094298]]
[37m[1m[2023-06-25 04:17:03,394][129146] Max Reward on eval: 146.3098376248551
[37m[1m[2023-06-25 04:17:03,394][129146] Min Reward on eval: 146.3098376248551
[37m[1m[2023-06-25 04:17:03,394][129146] Mean Reward across all agents: 146.3098376248551
[37m[1m[2023-06-25 04:17:03,395][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:17:08,922][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:17:08,923][129146] Reward + Measures: [[132.37454039   0.74169999   0.2404       0.58000004   0.1567    ]
[37m[1m [201.06622562   0.76360005   0.34909999   0.61269999   0.0499    ]
[37m[1m [161.77563867   0.75690001   0.2599       0.59700006   0.12840001]
[37m[1m ...
[37m[1m [222.18965386   0.75040001   0.34639999   0.59079999   0.1037    ]
[37m[1m [172.96684675   0.74180001   0.25229999   0.53369999   0.1856    ]
[37m[1m [158.56178375   0.78489995   0.2613       0.65630001   0.10030001]]
[37m[1m[2023-06-25 04:17:08,923][129146] Max Reward on eval: 237.97445674185875
[37m[1m[2023-06-25 04:17:08,923][129146] Min Reward on eval: -90.92388919472432
[37m[1m[2023-06-25 04:17:08,923][129146] Mean Reward across all agents: 148.52153803929818
[37m[1m[2023-06-25 04:17:08,924][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:17:08,926][129146] mean_value=-165.98167435282957, max_value=599.5978890649103
[37m[1m[2023-06-25 04:17:08,928][129146] New mean coefficients: [[-2.4383752   4.954088   -6.3691545  -0.01901345 -0.875836  ]]
[37m[1m[2023-06-25 04:17:08,929][129146] Moving the mean solution point...
[36m[2023-06-25 04:17:18,650][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 04:17:18,650][129146] FPS: 395112.51
[36m[2023-06-25 04:17:18,652][129146] itr=341, itrs=2000, Progress: 17.05%
[36m[2023-06-25 04:17:30,099][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 04:17:30,099][129146] FPS: 336017.18
[36m[2023-06-25 04:17:34,984][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:17:34,984][129146] Reward + Measures: [[157.31067215   0.75831169   0.22179866   0.56408167   0.19721167]]
[37m[1m[2023-06-25 04:17:34,984][129146] Max Reward on eval: 157.31067214641618
[37m[1m[2023-06-25 04:17:34,985][129146] Min Reward on eval: 157.31067214641618
[37m[1m[2023-06-25 04:17:34,985][129146] Mean Reward across all agents: 157.31067214641618
[37m[1m[2023-06-25 04:17:34,985][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:17:40,582][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:17:40,582][129146] Reward + Measures: [[-63.1415726    0.83470005   0.2807       0.58980006   0.0418    ]
[37m[1m [-16.20213168   0.77030003   0.183        0.57740003   0.0865    ]
[37m[1m [191.9342002    0.81229991   0.1732       0.6961       0.17120002]
[37m[1m ...
[37m[1m [ 73.54955643   0.82019997   0.27719998   0.61570007   0.0422    ]
[37m[1m [180.26404427   0.84530002   0.15710001   0.69749999   0.13609999]
[37m[1m [217.96360268   0.83929998   0.10720002   0.72200006   0.20650001]]
[37m[1m[2023-06-25 04:17:40,583][129146] Max Reward on eval: 259.871977999754
[37m[1m[2023-06-25 04:17:40,583][129146] Min Reward on eval: -310.61559622583445
[37m[1m[2023-06-25 04:17:40,583][129146] Mean Reward across all agents: 40.02711057430172
[37m[1m[2023-06-25 04:17:40,583][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:17:40,585][129146] mean_value=-148.79705981513433, max_value=402.28448951209384
[37m[1m[2023-06-25 04:17:40,587][129146] New mean coefficients: [[-3.0807648   3.8493733  -3.973276    0.23756991 -0.84096116]]
[37m[1m[2023-06-25 04:17:40,588][129146] Moving the mean solution point...
[36m[2023-06-25 04:17:50,288][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 04:17:50,288][129146] FPS: 395955.27
[36m[2023-06-25 04:17:50,291][129146] itr=342, itrs=2000, Progress: 17.10%
[36m[2023-06-25 04:18:01,840][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 04:18:01,840][129146] FPS: 332996.07
[36m[2023-06-25 04:18:06,724][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:18:06,724][129146] Reward + Measures: [[145.08317482   0.78991932   0.18076566   0.61777002   0.18109733]]
[37m[1m[2023-06-25 04:18:06,724][129146] Max Reward on eval: 145.08317482269896
[37m[1m[2023-06-25 04:18:06,724][129146] Min Reward on eval: 145.08317482269896
[37m[1m[2023-06-25 04:18:06,725][129146] Mean Reward across all agents: 145.08317482269896
[37m[1m[2023-06-25 04:18:06,725][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:18:12,187][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:18:12,188][129146] Reward + Measures: [[ 162.78599947    0.80740005    0.1209        0.66839999    0.19459999]
[37m[1m [-351.31409352    0.34507948    0.23589256    0.20744161    0.12402298]
[37m[1m [ 223.20536387    0.81850004    0.2138        0.62120003    0.103     ]
[37m[1m ...
[37m[1m [   6.70108771    0.78010005    0.25760004    0.45360002    0.0823    ]
[37m[1m [ 226.42660911    0.7762        0.26519999    0.53179997    0.0863    ]
[37m[1m [ 211.92450975    0.66780001    0.31400001    0.41949996    0.0831    ]]
[37m[1m[2023-06-25 04:18:12,188][129146] Max Reward on eval: 283.5912207136513
[37m[1m[2023-06-25 04:18:12,189][129146] Min Reward on eval: -490.8516013880377
[37m[1m[2023-06-25 04:18:12,189][129146] Mean Reward across all agents: 68.26531032095131
[37m[1m[2023-06-25 04:18:12,189][129146] Average Trajectory Length: 992.9336666666667
[36m[2023-06-25 04:18:12,194][129146] mean_value=118.05722533791285, max_value=783.5912207136513
[37m[1m[2023-06-25 04:18:12,197][129146] New mean coefficients: [[-3.7243066   3.3561406  -2.6418157   0.29455596 -0.8968853 ]]
[37m[1m[2023-06-25 04:18:12,198][129146] Moving the mean solution point...
[36m[2023-06-25 04:18:22,001][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 04:18:22,001][129146] FPS: 391791.30
[36m[2023-06-25 04:18:22,003][129146] itr=343, itrs=2000, Progress: 17.15%
[36m[2023-06-25 04:18:33,548][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 04:18:33,549][129146] FPS: 333077.72
[36m[2023-06-25 04:18:38,345][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:18:38,346][129146] Reward + Measures: [[124.49055279   0.80705607   0.16064265   0.65321165   0.17047967]]
[37m[1m[2023-06-25 04:18:38,346][129146] Max Reward on eval: 124.4905527871967
[37m[1m[2023-06-25 04:18:38,346][129146] Min Reward on eval: 124.4905527871967
[37m[1m[2023-06-25 04:18:38,347][129146] Mean Reward across all agents: 124.4905527871967
[37m[1m[2023-06-25 04:18:38,347][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:18:43,658][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:18:43,663][129146] Reward + Measures: [[ 77.85417066   0.79969996   0.1267       0.67509997   0.1635    ]
[37m[1m [170.4539399    0.78249997   0.16859999   0.61870003   0.15740001]
[37m[1m [131.60704178   0.84020007   0.11870001   0.69130003   0.15970001]
[37m[1m ...
[37m[1m [ 96.31356626   0.7604       0.20439999   0.54899997   0.1876    ]
[37m[1m [125.45808106   0.79119998   0.19149999   0.667        0.11229999]
[37m[1m [114.56631868   0.8186       0.13240001   0.63910002   0.16929999]]
[37m[1m[2023-06-25 04:18:43,663][129146] Max Reward on eval: 217.73264577608788
[37m[1m[2023-06-25 04:18:43,663][129146] Min Reward on eval: -60.77238579607801
[37m[1m[2023-06-25 04:18:43,664][129146] Mean Reward across all agents: 98.43875324466858
[37m[1m[2023-06-25 04:18:43,664][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:18:43,665][129146] mean_value=-236.84378629934275, max_value=631.9270487461123
[37m[1m[2023-06-25 04:18:43,668][129146] New mean coefficients: [[-2.4409232   3.0445344  -2.5442128   0.09784068 -0.5909997 ]]
[37m[1m[2023-06-25 04:18:43,669][129146] Moving the mean solution point...
[36m[2023-06-25 04:18:53,356][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 04:18:53,356][129146] FPS: 396454.20
[36m[2023-06-25 04:18:53,359][129146] itr=344, itrs=2000, Progress: 17.20%
[36m[2023-06-25 04:19:04,812][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 04:19:04,812][129146] FPS: 335847.12
[36m[2023-06-25 04:19:09,434][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:19:09,434][129146] Reward + Measures: [[115.78856862   0.82337564   0.13341834   0.68104005   0.17190832]]
[37m[1m[2023-06-25 04:19:09,434][129146] Max Reward on eval: 115.78856861565794
[37m[1m[2023-06-25 04:19:09,435][129146] Min Reward on eval: 115.78856861565794
[37m[1m[2023-06-25 04:19:09,435][129146] Mean Reward across all agents: 115.78856861565794
[37m[1m[2023-06-25 04:19:09,435][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:19:14,887][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:19:14,893][129146] Reward + Measures: [[114.70582795   0.80429995   0.18270001   0.62199998   0.1768    ]
[37m[1m [129.46772482   0.81140006   0.16330002   0.64520001   0.1707    ]
[37m[1m [188.7132156    0.85120004   0.11359999   0.69749999   0.19289999]
[37m[1m ...
[37m[1m [116.94865954   0.78010005   0.17519999   0.58380002   0.2462    ]
[37m[1m [ 88.69148072   0.79409999   0.1736       0.63029999   0.1865    ]
[37m[1m [144.29294812   0.83880007   0.14320001   0.67930001   0.15890001]]
[37m[1m[2023-06-25 04:19:14,893][129146] Max Reward on eval: 213.18837970277528
[37m[1m[2023-06-25 04:19:14,894][129146] Min Reward on eval: -72.68425077295615
[37m[1m[2023-06-25 04:19:14,894][129146] Mean Reward across all agents: 105.12012542323117
[37m[1m[2023-06-25 04:19:14,894][129146] Average Trajectory Length: 999.8963333333332
[36m[2023-06-25 04:19:14,895][129146] mean_value=-227.08652597267215, max_value=-54.441945211257995
[36m[2023-06-25 04:19:14,898][129146] XNES is restarting with a new solution whose measures are [0.92880005 0.37170002 0.83540004 0.2033    ] and objective is 206.6154127270798
[36m[2023-06-25 04:19:14,899][129146] Emitter restarted. Changing the mean agent...
[37m[1m[2023-06-25 04:19:14,901][129146] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]]
[37m[1m[2023-06-25 04:19:14,902][129146] Moving the mean solution point...
[36m[2023-06-25 04:19:24,540][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 04:19:24,540][129146] FPS: 398479.67
[36m[2023-06-25 04:19:24,543][129146] itr=345, itrs=2000, Progress: 17.25%
[36m[2023-06-25 04:19:36,080][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 04:19:36,081][129146] FPS: 333276.12
[36m[2023-06-25 04:19:40,782][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:19:40,782][129146] Reward + Measures: [[437.92067159   0.85504866   0.61973768   0.66757137   0.094267  ]]
[37m[1m[2023-06-25 04:19:40,782][129146] Max Reward on eval: 437.92067158781487
[37m[1m[2023-06-25 04:19:40,783][129146] Min Reward on eval: 437.92067158781487
[37m[1m[2023-06-25 04:19:40,783][129146] Mean Reward across all agents: 437.92067158781487
[37m[1m[2023-06-25 04:19:40,783][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:19:46,402][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:19:46,402][129146] Reward + Measures: [[ -103.75016059     0.77759999     0.0658         0.91860002
[37m[1m      0.48519999]
[37m[1m [ -798.52959295     0.33711347     0.5187785      0.18410385
[37m[1m      0.42314479]
[37m[1m [   37.01194916     0.3046         0.49910003     0.1829
[37m[1m      0.45200005]
[37m[1m ...
[37m[1m [ -520.16690719     0.43850002     0.4533         0.3888
[37m[1m      0.42920002]
[37m[1m [ -845.97532289     0.28479999     0.45460001     0.18499999
[37m[1m      0.4269    ]
[37m[1m [-1275.19568044     0.71099997     0.177          0.64959997
[37m[1m      0.2581    ]]
[37m[1m[2023-06-25 04:19:46,402][129146] Max Reward on eval: 1016.5041283103521
[37m[1m[2023-06-25 04:19:46,403][129146] Min Reward on eval: -1727.0993783068611
[37m[1m[2023-06-25 04:19:46,403][129146] Mean Reward across all agents: -526.3533190290701
[37m[1m[2023-06-25 04:19:46,403][129146] Average Trajectory Length: 971.9223333333333
[36m[2023-06-25 04:19:46,406][129146] mean_value=-763.1760488944594, max_value=790.9290470180356
[37m[1m[2023-06-25 04:19:46,408][129146] New mean coefficients: [[-0.17449096 -0.10598409  0.28887224 -1.4650792  -0.7192431 ]]
[37m[1m[2023-06-25 04:19:46,409][129146] Moving the mean solution point...
[36m[2023-06-25 04:19:56,078][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 04:19:56,078][129146] FPS: 397221.50
[36m[2023-06-25 04:19:56,081][129146] itr=346, itrs=2000, Progress: 17.30%
[36m[2023-06-25 04:20:07,520][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 04:20:07,521][129146] FPS: 336137.03
[36m[2023-06-25 04:20:12,385][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:20:12,386][129146] Reward + Measures: [[67.08362539  0.7109707   0.55386364  0.51524371  0.47730166]]
[37m[1m[2023-06-25 04:20:12,386][129146] Max Reward on eval: 67.08362539319866
[37m[1m[2023-06-25 04:20:12,386][129146] Min Reward on eval: 67.08362539319866
[37m[1m[2023-06-25 04:20:12,386][129146] Mean Reward across all agents: 67.08362539319866
[37m[1m[2023-06-25 04:20:12,387][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:20:17,893][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:20:17,898][129146] Reward + Measures: [[ -462.01049838     0.38249999     0.32339999     0.23720001
[37m[1m      0.28239998]
[37m[1m [ -472.0554878      0.46329999     0.18429999     0.2811
[37m[1m      0.26550004]
[37m[1m [ -777.8977614      0.30250198     0.3281357      0.2552903
[37m[1m      0.22861138]
[37m[1m ...
[37m[1m [-1366.56213988     0.56914133     0.24880461     0.512016
[37m[1m      0.11669087]
[37m[1m [ -801.02587775     0.34996948     0.34895501     0.29699823
[37m[1m      0.29226515]
[37m[1m [ -674.48858881     0.50716895     0.2659862      0.38013795
[37m[1m      0.13583449]]
[37m[1m[2023-06-25 04:20:17,898][129146] Max Reward on eval: 370.9635923948954
[37m[1m[2023-06-25 04:20:17,899][129146] Min Reward on eval: -1510.6910840889789
[37m[1m[2023-06-25 04:20:17,899][129146] Mean Reward across all agents: -449.72953041503484
[37m[1m[2023-06-25 04:20:17,899][129146] Average Trajectory Length: 884.9953333333333
[36m[2023-06-25 04:20:17,902][129146] mean_value=-1045.5550896878963, max_value=410.3057510371867
[37m[1m[2023-06-25 04:20:17,904][129146] New mean coefficients: [[-0.48318702  0.1946041  -1.0350537  -0.8520293  -0.650878  ]]
[37m[1m[2023-06-25 04:20:17,905][129146] Moving the mean solution point...
[36m[2023-06-25 04:20:27,702][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 04:20:27,702][129146] FPS: 392046.07
[36m[2023-06-25 04:20:27,704][129146] itr=347, itrs=2000, Progress: 17.35%
[36m[2023-06-25 04:20:39,228][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 04:20:39,228][129146] FPS: 333719.58
[36m[2023-06-25 04:20:44,137][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:20:44,137][129146] Reward + Measures: [[92.24555116  0.69638836  0.52668166  0.49399599  0.45317265]]
[37m[1m[2023-06-25 04:20:44,138][129146] Max Reward on eval: 92.24555116202403
[37m[1m[2023-06-25 04:20:44,138][129146] Min Reward on eval: 92.24555116202403
[37m[1m[2023-06-25 04:20:44,138][129146] Mean Reward across all agents: 92.24555116202403
[37m[1m[2023-06-25 04:20:44,138][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:20:49,599][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:20:49,600][129146] Reward + Measures: [[ 144.43152712    0.40090528    0.30254209    0.3469474     0.25643685]
[37m[1m [ -16.67827984    0.42060003    0.3177        0.23670001    0.27090001]
[37m[1m [-166.28261249    0.40190002    0.22749999    0.32179999    0.26890001]
[37m[1m ...
[37m[1m [-159.69488758    0.26984382    0.2052341     0.21433215    0.23381248]
[37m[1m [-536.74318844    0.32375094    0.20402811    0.20132314    0.19562699]
[37m[1m [-452.30498004    0.27294555    0.29902217    0.2909112     0.15989248]]
[37m[1m[2023-06-25 04:20:49,600][129146] Max Reward on eval: 474.84597995363873
[37m[1m[2023-06-25 04:20:49,600][129146] Min Reward on eval: -1037.4777774327727
[37m[1m[2023-06-25 04:20:49,600][129146] Mean Reward across all agents: -98.7282114028111
[37m[1m[2023-06-25 04:20:49,601][129146] Average Trajectory Length: 948.1966666666666
[36m[2023-06-25 04:20:49,603][129146] mean_value=-769.0453963889728, max_value=760.1827290521003
[37m[1m[2023-06-25 04:20:49,606][129146] New mean coefficients: [[-1.6802676   0.3516984  -0.922379   -0.9772623  -0.34848464]]
[37m[1m[2023-06-25 04:20:49,607][129146] Moving the mean solution point...
[36m[2023-06-25 04:20:59,269][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 04:20:59,269][129146] FPS: 397508.84
[36m[2023-06-25 04:20:59,272][129146] itr=348, itrs=2000, Progress: 17.40%
[36m[2023-06-25 04:21:10,740][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 04:21:10,740][129146] FPS: 335345.83
[36m[2023-06-25 04:21:15,523][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:21:15,523][129146] Reward + Measures: [[55.80887184  0.68977708  0.48671135  0.4770965   0.42891046]]
[37m[1m[2023-06-25 04:21:15,523][129146] Max Reward on eval: 55.80887183771878
[37m[1m[2023-06-25 04:21:15,524][129146] Min Reward on eval: 55.80887183771878
[37m[1m[2023-06-25 04:21:15,524][129146] Mean Reward across all agents: 55.80887183771878
[37m[1m[2023-06-25 04:21:15,524][129146] Average Trajectory Length: 999.8576666666667
[36m[2023-06-25 04:21:20,931][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:21:20,932][129146] Reward + Measures: [[-482.33509859    0.52740002    0.43099999    0.44110003    0.40790001]
[37m[1m [-536.79526499    0.5686        0.46259999    0.53280002    0.41479999]
[37m[1m [-887.84247779    0.52340001    0.2915        0.3766        0.35780001]
[37m[1m ...
[37m[1m [-695.30063095    0.30162138    0.25574988    0.2463907     0.25708985]
[37m[1m [-509.65672809    0.37734419    0.24832292    0.30381832    0.27433419]
[37m[1m [-432.25190958    0.32743335    0.27406669    0.20469999    0.18790001]]
[37m[1m[2023-06-25 04:21:20,932][129146] Max Reward on eval: 354.55368710833136
[37m[1m[2023-06-25 04:21:20,932][129146] Min Reward on eval: -1700.366845940263
[37m[1m[2023-06-25 04:21:20,933][129146] Mean Reward across all agents: -478.55797466393057
[37m[1m[2023-06-25 04:21:20,933][129146] Average Trajectory Length: 921.4653333333333
[36m[2023-06-25 04:21:20,936][129146] mean_value=-961.6886755590876, max_value=446.7789861838552
[37m[1m[2023-06-25 04:21:20,938][129146] New mean coefficients: [[-2.6894875   0.52265406 -0.1682533  -0.00331682  0.81565833]]
[37m[1m[2023-06-25 04:21:20,940][129146] Moving the mean solution point...
[36m[2023-06-25 04:21:30,616][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 04:21:30,616][129146] FPS: 396904.21
[36m[2023-06-25 04:21:30,619][129146] itr=349, itrs=2000, Progress: 17.45%
[36m[2023-06-25 04:21:42,054][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 04:21:42,054][129146] FPS: 336308.09
[36m[2023-06-25 04:21:46,827][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:21:46,833][129146] Reward + Measures: [[-116.42824742    0.5899021     0.43580621    0.33584455    0.36448583]]
[37m[1m[2023-06-25 04:21:46,833][129146] Max Reward on eval: -116.42824742163442
[37m[1m[2023-06-25 04:21:46,833][129146] Min Reward on eval: -116.42824742163442
[37m[1m[2023-06-25 04:21:46,834][129146] Mean Reward across all agents: -116.42824742163442
[37m[1m[2023-06-25 04:21:46,834][129146] Average Trajectory Length: 999.5963333333333
[36m[2023-06-25 04:21:52,356][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:21:52,357][129146] Reward + Measures: [[  11.55509515    0.52410001    0.4693        0.39400002    0.2854    ]
[37m[1m [-617.46670094    0.40229997    0.3788        0.39839998    0.3145    ]
[37m[1m [  88.09698262    0.39580002    0.45429999    0.35080001    0.36380002]
[37m[1m ...
[37m[1m [ 167.76370845    0.62199998    0.50089997    0.37130004    0.38      ]
[37m[1m [ 241.0833129     0.39910004    0.43360004    0.27630001    0.35320002]
[37m[1m [-263.61538857    0.41319999    0.36000001    0.38390002    0.43660003]]
[37m[1m[2023-06-25 04:21:52,357][129146] Max Reward on eval: 486.405876794178
[37m[1m[2023-06-25 04:21:52,357][129146] Min Reward on eval: -1424.1116175992415
[37m[1m[2023-06-25 04:21:52,357][129146] Mean Reward across all agents: -122.87775477000037
[37m[1m[2023-06-25 04:21:52,358][129146] Average Trajectory Length: 996.977
[36m[2023-06-25 04:21:52,360][129146] mean_value=-722.0862503431453, max_value=484.471005088439
[37m[1m[2023-06-25 04:21:52,362][129146] New mean coefficients: [[-3.0677035   0.86427486 -0.02504843  0.32282037  0.47250324]]
[37m[1m[2023-06-25 04:21:52,364][129146] Moving the mean solution point...
[36m[2023-06-25 04:22:02,207][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 04:22:02,208][129146] FPS: 390153.27
[36m[2023-06-25 04:22:02,210][129146] itr=350, itrs=2000, Progress: 17.50%
[37m[1m[2023-06-25 04:22:06,085][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000330
[36m[2023-06-25 04:22:17,920][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 04:22:17,920][129146] FPS: 333360.92
[36m[2023-06-25 04:22:22,652][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:22:22,652][129146] Reward + Measures: [[-132.8147476     0.58334303    0.43922344    0.34430566    0.36650842]]
[37m[1m[2023-06-25 04:22:22,652][129146] Max Reward on eval: -132.81474759569082
[37m[1m[2023-06-25 04:22:22,653][129146] Min Reward on eval: -132.81474759569082
[37m[1m[2023-06-25 04:22:22,653][129146] Mean Reward across all agents: -132.81474759569082
[37m[1m[2023-06-25 04:22:22,653][129146] Average Trajectory Length: 999.794
[36m[2023-06-25 04:22:28,152][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:22:28,153][129146] Reward + Measures: [[-1149.7025216      0.28327695     0.40247473     0.3929956
[37m[1m      0.35845712]
[37m[1m [-1350.76538712     0.25164548     0.36515149     0.33819696
[37m[1m      0.31453332]
[37m[1m [-1005.18403086     0.38139999     0.3849         0.45459995
[37m[1m      0.3134    ]
[37m[1m ...
[37m[1m [ -487.25002024     0.3917         0.49110004     0.56030005
[37m[1m      0.42179999]
[37m[1m [ -130.91834791     0.56730002     0.43630001     0.24890001
[37m[1m      0.30520001]
[37m[1m [ -487.98356027     0.37689999     0.49429998     0.57359999
[37m[1m      0.41339999]]
[37m[1m[2023-06-25 04:22:28,153][129146] Max Reward on eval: 404.4951640293933
[37m[1m[2023-06-25 04:22:28,153][129146] Min Reward on eval: -1392.5368367745773
[37m[1m[2023-06-25 04:22:28,153][129146] Mean Reward across all agents: -308.553551008809
[37m[1m[2023-06-25 04:22:28,154][129146] Average Trajectory Length: 981.5846666666666
[36m[2023-06-25 04:22:28,158][129146] mean_value=-435.57470884792946, max_value=623.7296621239162
[37m[1m[2023-06-25 04:22:28,160][129146] New mean coefficients: [[-3.0636067   0.87857765  0.2537184   0.37652057  0.43784237]]
[37m[1m[2023-06-25 04:22:28,161][129146] Moving the mean solution point...
[36m[2023-06-25 04:22:38,061][129146] train() took 9.90 seconds to complete
[36m[2023-06-25 04:22:38,061][129146] FPS: 387945.62
[36m[2023-06-25 04:22:38,064][129146] itr=351, itrs=2000, Progress: 17.55%
[36m[2023-06-25 04:22:49,695][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 04:22:49,696][129146] FPS: 330635.37
[36m[2023-06-25 04:22:54,540][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:22:54,541][129146] Reward + Measures: [[-57.79459634   0.48076636   0.40100399   0.16647467   0.34910533]]
[37m[1m[2023-06-25 04:22:54,541][129146] Max Reward on eval: -57.794596343661695
[37m[1m[2023-06-25 04:22:54,541][129146] Min Reward on eval: -57.794596343661695
[37m[1m[2023-06-25 04:22:54,541][129146] Mean Reward across all agents: -57.794596343661695
[37m[1m[2023-06-25 04:22:54,542][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:23:00,111][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:23:00,111][129146] Reward + Measures: [[ -73.18506658    0.36399999    0.27700001    0.13870001    0.27970001]
[37m[1m [-453.64250741    0.27568239    0.17645295    0.18072942    0.24207647]
[37m[1m [-486.90542834    0.28518397    0.22585331    0.18759771    0.25407451]
[37m[1m ...
[37m[1m [-423.84909792    0.39930001    0.3107        0.1785        0.27180001]
[37m[1m [-681.51222221    0.24670143    0.13270213    0.1593719     0.15943837]
[37m[1m [-520.1047086     0.34514317    0.30263489    0.15513907    0.25868818]]
[37m[1m[2023-06-25 04:23:00,111][129146] Max Reward on eval: 304.72673579595283
[37m[1m[2023-06-25 04:23:00,112][129146] Min Reward on eval: -984.0730099430308
[37m[1m[2023-06-25 04:23:00,112][129146] Mean Reward across all agents: -387.6819446813264
[37m[1m[2023-06-25 04:23:00,112][129146] Average Trajectory Length: 919.935
[36m[2023-06-25 04:23:00,113][129146] mean_value=-1942.6713851871789, max_value=-404.8682644360376
[36m[2023-06-25 04:23:00,116][129146] XNES is restarting with a new solution whose measures are [0.75500005 0.81689996 0.13160001 0.84349996] and objective is 416.24381819708503
[36m[2023-06-25 04:23:00,117][129146] Emitter restarted. Changing the mean agent...
[37m[1m[2023-06-25 04:23:00,119][129146] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]]
[37m[1m[2023-06-25 04:23:00,120][129146] Moving the mean solution point...
[36m[2023-06-25 04:23:09,932][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 04:23:09,932][129146] FPS: 391417.89
[36m[2023-06-25 04:23:09,934][129146] itr=352, itrs=2000, Progress: 17.60%
[36m[2023-06-25 04:23:21,581][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 04:23:21,581][129146] FPS: 330166.14
[36m[2023-06-25 04:23:26,404][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:23:26,404][129146] Reward + Measures: [[404.86132468   0.64195937   0.72845781   0.12182205   0.7762298 ]]
[37m[1m[2023-06-25 04:23:26,404][129146] Max Reward on eval: 404.8613246841587
[37m[1m[2023-06-25 04:23:26,405][129146] Min Reward on eval: 404.8613246841587
[37m[1m[2023-06-25 04:23:26,405][129146] Mean Reward across all agents: 404.8613246841587
[37m[1m[2023-06-25 04:23:26,405][129146] Average Trajectory Length: 999.987
[36m[2023-06-25 04:23:31,931][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:23:31,931][129146] Reward + Measures: [[383.94181509   0.66189998   0.74479997   0.1349       0.80120003]
[37m[1m [278.52394724   0.68080002   0.75169998   0.25050002   0.81189996]
[37m[1m [385.9917827    0.74590003   0.74000001   0.0196       0.79069996]
[37m[1m ...
[37m[1m [444.13787374   0.59820002   0.67659998   0.10940001   0.67830002]
[37m[1m [413.10148586   0.73270005   0.73830003   0.09950002   0.75389999]
[37m[1m [302.49607067   0.75819999   0.79100001   0.0247       0.78509998]]
[37m[1m[2023-06-25 04:23:31,932][129146] Max Reward on eval: 1013.1538834212813
[37m[1m[2023-06-25 04:23:31,932][129146] Min Reward on eval: 188.10640250444993
[37m[1m[2023-06-25 04:23:31,932][129146] Mean Reward across all agents: 515.8394109571332
[37m[1m[2023-06-25 04:23:31,932][129146] Average Trajectory Length: 999.7719999999999
[36m[2023-06-25 04:23:31,936][129146] mean_value=-77.81195407510215, max_value=844.4690027047997
[37m[1m[2023-06-25 04:23:31,939][129146] New mean coefficients: [[ 1.739638    0.79188573 -1.4934144  -1.4426185   0.24762583]]
[37m[1m[2023-06-25 04:23:31,940][129146] Moving the mean solution point...
[36m[2023-06-25 04:23:41,702][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 04:23:41,702][129146] FPS: 393418.74
[36m[2023-06-25 04:23:41,705][129146] itr=353, itrs=2000, Progress: 17.65%
[36m[2023-06-25 04:23:53,319][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 04:23:53,319][129146] FPS: 331132.77
[36m[2023-06-25 04:23:58,141][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:23:58,141][129146] Reward + Measures: [[498.68852728   0.57786101   0.6661067    0.124469     0.68157768]]
[37m[1m[2023-06-25 04:23:58,141][129146] Max Reward on eval: 498.6885272785911
[37m[1m[2023-06-25 04:23:58,141][129146] Min Reward on eval: 498.6885272785911
[37m[1m[2023-06-25 04:23:58,142][129146] Mean Reward across all agents: 498.6885272785911
[37m[1m[2023-06-25 04:23:58,142][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:24:03,748][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:24:03,754][129146] Reward + Measures: [[ 600.92030896    0.46070004    0.56360006    0.10120001    0.4901    ]
[37m[1m [ 767.6137722     0.45950004    0.57859999    0.1415        0.49160001]
[37m[1m [ 960.28698027    0.42729998    0.5449        0.1444        0.36550003]
[37m[1m ...
[37m[1m [ 875.11109844    0.4303        0.51910001    0.1552        0.33790001]
[37m[1m [1081.03092436    0.44070002    0.5104        0.19160001    0.34520003]
[37m[1m [1014.95043083    0.44860002    0.53100002    0.13920002    0.37510002]]
[37m[1m[2023-06-25 04:24:03,755][129146] Max Reward on eval: 1177.1036388904497
[37m[1m[2023-06-25 04:24:03,756][129146] Min Reward on eval: 269.46480792684014
[37m[1m[2023-06-25 04:24:03,757][129146] Mean Reward across all agents: 718.8892657005547
[37m[1m[2023-06-25 04:24:03,757][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:24:03,764][129146] mean_value=-100.33144444426901, max_value=794.2400329548976
[37m[1m[2023-06-25 04:24:03,768][129146] New mean coefficients: [[ 0.40517247  0.7105921  -1.6477382  -0.65903723  1.2108047 ]]
[37m[1m[2023-06-25 04:24:03,770][129146] Moving the mean solution point...
[36m[2023-06-25 04:24:13,475][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 04:24:13,475][129146] FPS: 395765.22
[36m[2023-06-25 04:24:13,477][129146] itr=354, itrs=2000, Progress: 17.70%
[36m[2023-06-25 04:24:24,929][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 04:24:24,929][129146] FPS: 335807.33
[36m[2023-06-25 04:24:29,811][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:24:29,811][129146] Reward + Measures: [[610.56593136   0.55802453   0.62872195   0.10726778   0.630292  ]]
[37m[1m[2023-06-25 04:24:29,812][129146] Max Reward on eval: 610.5659313627683
[37m[1m[2023-06-25 04:24:29,812][129146] Min Reward on eval: 610.5659313627683
[37m[1m[2023-06-25 04:24:29,812][129146] Mean Reward across all agents: 610.5659313627683
[37m[1m[2023-06-25 04:24:29,812][129146] Average Trajectory Length: 999.6556666666667
[36m[2023-06-25 04:24:35,193][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:24:35,193][129146] Reward + Measures: [[846.33564225   0.46079999   0.50220001   0.14070001   0.44029999]
[37m[1m [919.70721394   0.46539998   0.54860002   0.1561       0.42309999]
[37m[1m [405.69623051   0.64679998   0.67950004   0.1464       0.73050004]
[37m[1m ...
[37m[1m [898.07203299   0.51740003   0.59289998   0.12730001   0.48780003]
[37m[1m [945.81015706   0.4752       0.50839996   0.18050002   0.36210003]
[37m[1m [441.14643592   0.49709997   0.56110001   0.1017       0.56440002]]
[37m[1m[2023-06-25 04:24:35,193][129146] Max Reward on eval: 1207.6337212675717
[37m[1m[2023-06-25 04:24:35,194][129146] Min Reward on eval: 203.61823807675682
[37m[1m[2023-06-25 04:24:35,194][129146] Mean Reward across all agents: 853.9350982929869
[37m[1m[2023-06-25 04:24:35,194][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:24:35,198][129146] mean_value=-108.9911621338502, max_value=902.0391817013384
[37m[1m[2023-06-25 04:24:35,200][129146] New mean coefficients: [[ 0.7020677   1.0124322  -0.15863359 -0.28692502  2.4436228 ]]
[37m[1m[2023-06-25 04:24:35,201][129146] Moving the mean solution point...
[36m[2023-06-25 04:24:44,879][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 04:24:44,880][129146] FPS: 396839.28
[36m[2023-06-25 04:24:44,882][129146] itr=355, itrs=2000, Progress: 17.75%
[36m[2023-06-25 04:24:56,368][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 04:24:56,368][129146] FPS: 334829.01
[36m[2023-06-25 04:25:01,225][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:25:01,226][129146] Reward + Measures: [[526.27081087   0.59858298   0.66741204   0.09336533   0.70435798]]
[37m[1m[2023-06-25 04:25:01,226][129146] Max Reward on eval: 526.2708108725684
[37m[1m[2023-06-25 04:25:01,226][129146] Min Reward on eval: 526.2708108725684
[37m[1m[2023-06-25 04:25:01,226][129146] Mean Reward across all agents: 526.2708108725684
[37m[1m[2023-06-25 04:25:01,226][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:25:06,744][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:25:06,745][129146] Reward + Measures: [[ 648.36578332    0.49079999    0.57699996    0.1409        0.56530005]
[37m[1m [ 810.53255962    0.49830005    0.55599993    0.13689999    0.47610003]
[37m[1m [ 883.15064978    0.4522        0.412         0.27700001    0.30370003]
[37m[1m ...
[37m[1m [ 503.4638757     0.54150003    0.61669999    0.1008        0.6279    ]
[37m[1m [1057.04257657    0.44000003    0.4725        0.1909        0.3224    ]
[37m[1m [ 566.45726457    0.61100006    0.65369999    0.0429        0.69700003]]
[37m[1m[2023-06-25 04:25:06,745][129146] Max Reward on eval: 1193.6455765822902
[37m[1m[2023-06-25 04:25:06,746][129146] Min Reward on eval: 369.4195465745346
[37m[1m[2023-06-25 04:25:06,746][129146] Mean Reward across all agents: 751.9786655265307
[37m[1m[2023-06-25 04:25:06,746][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:25:06,750][129146] mean_value=-67.9555278597185, max_value=560.7311502919
[37m[1m[2023-06-25 04:25:06,752][129146] New mean coefficients: [[1.2538226  1.1030211  0.55110234 0.8783156  2.678594  ]]
[37m[1m[2023-06-25 04:25:06,753][129146] Moving the mean solution point...
[36m[2023-06-25 04:25:16,390][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 04:25:16,390][129146] FPS: 398546.66
[36m[2023-06-25 04:25:16,392][129146] itr=356, itrs=2000, Progress: 17.80%
[36m[2023-06-25 04:25:27,822][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 04:25:27,823][129146] FPS: 336429.28
[36m[2023-06-25 04:25:32,554][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:25:32,554][129146] Reward + Measures: [[463.34099946   0.643915     0.71552604   0.082119     0.7784707 ]]
[37m[1m[2023-06-25 04:25:32,554][129146] Max Reward on eval: 463.34099945934554
[37m[1m[2023-06-25 04:25:32,555][129146] Min Reward on eval: 463.34099945934554
[37m[1m[2023-06-25 04:25:32,555][129146] Mean Reward across all agents: 463.34099945934554
[37m[1m[2023-06-25 04:25:32,555][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:25:38,039][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:25:38,039][129146] Reward + Measures: [[672.87494879   0.60479999   0.6638       0.1355       0.6286    ]
[37m[1m [580.28465116   0.5478       0.66210002   0.19849999   0.64270002]
[37m[1m [705.18116702   0.61570001   0.7137       0.077        0.69600004]
[37m[1m ...
[37m[1m [631.79485991   0.51710004   0.63199997   0.11009999   0.63720006]
[37m[1m [601.43017412   0.51970005   0.63270003   0.0596       0.67450005]
[37m[1m [560.17577492   0.59549999   0.68370003   0.0264       0.71160001]]
[37m[1m[2023-06-25 04:25:38,040][129146] Max Reward on eval: 918.2480503023137
[37m[1m[2023-06-25 04:25:38,040][129146] Min Reward on eval: 177.93462747287703
[37m[1m[2023-06-25 04:25:38,040][129146] Mean Reward across all agents: 563.0716305834566
[37m[1m[2023-06-25 04:25:38,040][129146] Average Trajectory Length: 999.6793333333333
[36m[2023-06-25 04:25:38,045][129146] mean_value=101.03369122101607, max_value=869.8916847907805
[37m[1m[2023-06-25 04:25:38,048][129146] New mean coefficients: [[1.1040022  1.6825168  0.40301567 0.30186915 3.713732  ]]
[37m[1m[2023-06-25 04:25:38,049][129146] Moving the mean solution point...
[36m[2023-06-25 04:25:47,866][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 04:25:47,866][129146] FPS: 391232.61
[36m[2023-06-25 04:25:47,869][129146] itr=357, itrs=2000, Progress: 17.85%
[36m[2023-06-25 04:25:59,460][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 04:25:59,460][129146] FPS: 331777.94
[36m[2023-06-25 04:26:04,209][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:26:04,210][129146] Reward + Measures: [[482.126782     0.72635663   0.77957439   0.04364533   0.838705  ]]
[37m[1m[2023-06-25 04:26:04,210][129146] Max Reward on eval: 482.1267820001192
[37m[1m[2023-06-25 04:26:04,210][129146] Min Reward on eval: 482.1267820001192
[37m[1m[2023-06-25 04:26:04,211][129146] Mean Reward across all agents: 482.1267820001192
[37m[1m[2023-06-25 04:26:04,211][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:26:09,790][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:26:09,791][129146] Reward + Measures: [[505.21113893   0.79640001   0.81769991   0.0246       0.89249992]
[37m[1m [465.46519555   0.82980007   0.80670005   0.0217       0.86520004]
[37m[1m [593.67038213   0.72930002   0.75139999   0.0689       0.73360002]
[37m[1m ...
[37m[1m [472.40326512   0.81919998   0.80830002   0.0373       0.81999999]
[37m[1m [521.41360061   0.64440006   0.76800007   0.0352       0.82460004]
[37m[1m [432.662388     0.80220002   0.80009997   0.0315       0.81490004]]
[37m[1m[2023-06-25 04:26:09,791][129146] Max Reward on eval: 1080.7351850260632
[37m[1m[2023-06-25 04:26:09,791][129146] Min Reward on eval: 151.91507713749306
[37m[1m[2023-06-25 04:26:09,791][129146] Mean Reward across all agents: 456.0182574427042
[37m[1m[2023-06-25 04:26:09,792][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:26:09,795][129146] mean_value=9.477156328633281, max_value=1005.2111389275175
[37m[1m[2023-06-25 04:26:09,798][129146] New mean coefficients: [[ 0.26043862  0.4822297   0.27538604 -1.0030333   3.7824912 ]]
[37m[1m[2023-06-25 04:26:09,799][129146] Moving the mean solution point...
[36m[2023-06-25 04:26:19,618][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 04:26:19,619][129146] FPS: 391126.03
[36m[2023-06-25 04:26:19,621][129146] itr=358, itrs=2000, Progress: 17.90%
[36m[2023-06-25 04:26:31,295][129146] train() took 11.66 seconds to complete
[36m[2023-06-25 04:26:31,295][129146] FPS: 329393.20
[36m[2023-06-25 04:26:36,168][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:26:36,168][129146] Reward + Measures: [[498.49560279   0.81617266   0.83932805   0.029162     0.87899834]]
[37m[1m[2023-06-25 04:26:36,168][129146] Max Reward on eval: 498.4956027888778
[37m[1m[2023-06-25 04:26:36,168][129146] Min Reward on eval: 498.4956027888778
[37m[1m[2023-06-25 04:26:36,169][129146] Mean Reward across all agents: 498.4956027888778
[37m[1m[2023-06-25 04:26:36,169][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:26:41,576][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:26:41,577][129146] Reward + Measures: [[624.74165209   0.75790006   0.7665       0.0335       0.7841    ]
[37m[1m [552.25439329   0.77310002   0.77939999   0.0323       0.82390004]
[37m[1m [310.36767885   0.85659999   0.79759997   0.0273       0.89320004]
[37m[1m ...
[37m[1m [509.07774778   0.9077       0.90550005   0.0237       0.90560007]
[37m[1m [259.2890817    0.7597       0.74380004   0.0394       0.85280001]
[37m[1m [338.03117949   0.815        0.83529997   0.0541       0.85420001]]
[37m[1m[2023-06-25 04:26:41,577][129146] Max Reward on eval: 977.0574469851679
[37m[1m[2023-06-25 04:26:41,577][129146] Min Reward on eval: 224.58826823916752
[37m[1m[2023-06-25 04:26:41,577][129146] Mean Reward across all agents: 454.1497004889043
[37m[1m[2023-06-25 04:26:41,578][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:26:41,582][129146] mean_value=116.8960070780671, max_value=1020.1169224292738
[37m[1m[2023-06-25 04:26:41,585][129146] New mean coefficients: [[-1.4630947  -0.09162301 -0.02285674 -1.5132929   4.4029264 ]]
[37m[1m[2023-06-25 04:26:41,586][129146] Moving the mean solution point...
[36m[2023-06-25 04:26:51,379][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 04:26:51,380][129146] FPS: 392147.60
[36m[2023-06-25 04:26:51,382][129146] itr=359, itrs=2000, Progress: 17.95%
[36m[2023-06-25 04:27:02,932][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 04:27:02,933][129146] FPS: 332911.01
[36m[2023-06-25 04:27:07,678][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:27:07,678][129146] Reward + Measures: [[465.57153778   0.84140593   0.85821265   0.02384633   0.89743197]]
[37m[1m[2023-06-25 04:27:07,678][129146] Max Reward on eval: 465.571537780315
[37m[1m[2023-06-25 04:27:07,679][129146] Min Reward on eval: 465.571537780315
[37m[1m[2023-06-25 04:27:07,679][129146] Mean Reward across all agents: 465.571537780315
[37m[1m[2023-06-25 04:27:07,679][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:27:13,195][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:27:13,196][129146] Reward + Measures: [[518.91523823   0.89610004   0.90750009   0.0231       0.91170007]
[37m[1m [567.9484267    0.87         0.88889998   0.0261       0.89580005]
[37m[1m [539.66519655   0.87519991   0.88729995   0.0233       0.90150005]
[37m[1m ...
[37m[1m [444.82153871   0.63370001   0.73110002   0.0503       0.77780002]
[37m[1m [461.31563294   0.70769995   0.70590001   0.0269       0.68160003]
[37m[1m [491.42828956   0.79730004   0.84279996   0.0332       0.86180001]]
[37m[1m[2023-06-25 04:27:13,196][129146] Max Reward on eval: 891.2582711016294
[37m[1m[2023-06-25 04:27:13,196][129146] Min Reward on eval: 263.1148410527618
[37m[1m[2023-06-25 04:27:13,196][129146] Mean Reward across all agents: 460.10415035477604
[37m[1m[2023-06-25 04:27:13,196][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:27:13,199][129146] mean_value=-120.32758322533596, max_value=596.6287630198756
[37m[1m[2023-06-25 04:27:13,202][129146] New mean coefficients: [[-1.3953757   0.3970242  -0.00686655 -1.0350537   3.8046362 ]]
[37m[1m[2023-06-25 04:27:13,203][129146] Moving the mean solution point...
[36m[2023-06-25 04:27:22,990][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 04:27:22,991][129146] FPS: 392401.24
[36m[2023-06-25 04:27:22,993][129146] itr=360, itrs=2000, Progress: 18.00%
[37m[1m[2023-06-25 04:27:26,754][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000340
[36m[2023-06-25 04:27:38,514][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 04:27:38,515][129146] FPS: 335602.41
[36m[2023-06-25 04:27:43,366][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:27:43,366][129146] Reward + Measures: [[390.71916904   0.86453533   0.87233335   0.021615     0.9118467 ]]
[37m[1m[2023-06-25 04:27:43,366][129146] Max Reward on eval: 390.7191690427593
[37m[1m[2023-06-25 04:27:43,367][129146] Min Reward on eval: 390.7191690427593
[37m[1m[2023-06-25 04:27:43,367][129146] Mean Reward across all agents: 390.7191690427593
[37m[1m[2023-06-25 04:27:43,367][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:27:49,025][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:27:49,025][129146] Reward + Measures: [[430.25945147   0.8646       0.87190002   0.0199       0.90979999]
[37m[1m [461.47882581   0.79500002   0.82010001   0.0281       0.8854    ]
[37m[1m [398.67271686   0.84329998   0.86630005   0.0168       0.92449999]
[37m[1m ...
[37m[1m [359.79838039   0.84460002   0.85840005   0.0266       0.89910001]
[37m[1m [433.22123906   0.77609998   0.82179993   0.0194       0.89559996]
[37m[1m [435.41509942   0.7511       0.78490007   0.0178       0.86800003]]
[37m[1m[2023-06-25 04:27:49,026][129146] Max Reward on eval: 642.7048126039547
[37m[1m[2023-06-25 04:27:49,026][129146] Min Reward on eval: 228.36192869169173
[37m[1m[2023-06-25 04:27:49,026][129146] Mean Reward across all agents: 411.96126031859603
[37m[1m[2023-06-25 04:27:49,026][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:27:49,029][129146] mean_value=-66.78607933959975, max_value=922.1483277749969
[37m[1m[2023-06-25 04:27:49,032][129146] New mean coefficients: [[-0.87742364 -0.28585744 -0.5819843  -1.8056934   3.9087226 ]]
[37m[1m[2023-06-25 04:27:49,033][129146] Moving the mean solution point...
[36m[2023-06-25 04:27:58,771][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 04:27:58,771][129146] FPS: 394388.13
[36m[2023-06-25 04:27:58,774][129146] itr=361, itrs=2000, Progress: 18.05%
[36m[2023-06-25 04:28:10,193][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 04:28:10,193][129146] FPS: 336817.48
[36m[2023-06-25 04:28:14,959][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:28:14,959][129146] Reward + Measures: [[333.56102503   0.83050501   0.85866505   0.02051267   0.915003  ]]
[37m[1m[2023-06-25 04:28:14,960][129146] Max Reward on eval: 333.56102502822216
[37m[1m[2023-06-25 04:28:14,960][129146] Min Reward on eval: 333.56102502822216
[37m[1m[2023-06-25 04:28:14,960][129146] Mean Reward across all agents: 333.56102502822216
[37m[1m[2023-06-25 04:28:14,960][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:28:20,469][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:28:20,469][129146] Reward + Measures: [[294.13714114   0.82719994   0.83929998   0.0231       0.90329999]
[37m[1m [343.45552499   0.80680001   0.82200003   0.0242       0.88140005]
[37m[1m [343.77691456   0.75500005   0.82460004   0.0273       0.87279999]
[37m[1m ...
[37m[1m [300.48050466   0.83459997   0.86359996   0.0195       0.90220004]
[37m[1m [105.82453527   0.89829999   0.89170009   0.0219       0.90840006]
[37m[1m [481.66992085   0.6785       0.75780004   0.0556       0.80719995]]
[37m[1m[2023-06-25 04:28:20,469][129146] Max Reward on eval: 551.666628917132
[37m[1m[2023-06-25 04:28:20,470][129146] Min Reward on eval: 55.69558843746781
[37m[1m[2023-06-25 04:28:20,470][129146] Mean Reward across all agents: 277.6494345600284
[37m[1m[2023-06-25 04:28:20,470][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:28:20,472][129146] mean_value=-233.24846102788524, max_value=815.9041647484294
[37m[1m[2023-06-25 04:28:20,474][129146] New mean coefficients: [[ 0.44618428  0.22108382 -1.453364   -0.6291888   3.1104357 ]]
[37m[1m[2023-06-25 04:28:20,475][129146] Moving the mean solution point...
[36m[2023-06-25 04:28:30,253][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 04:28:30,259][129146] FPS: 392800.81
[36m[2023-06-25 04:28:30,261][129146] itr=362, itrs=2000, Progress: 18.10%
[36m[2023-06-25 04:28:41,676][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 04:28:41,677][129146] FPS: 336893.68
[36m[2023-06-25 04:28:46,538][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:28:46,539][129146] Reward + Measures: [[343.55872753   0.81489664   0.83874965   0.02054933   0.91463673]]
[37m[1m[2023-06-25 04:28:46,539][129146] Max Reward on eval: 343.55872753217966
[37m[1m[2023-06-25 04:28:46,539][129146] Min Reward on eval: 343.55872753217966
[37m[1m[2023-06-25 04:28:46,539][129146] Mean Reward across all agents: 343.55872753217966
[37m[1m[2023-06-25 04:28:46,540][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:28:52,005][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:28:52,006][129146] Reward + Measures: [[316.64243897   0.68180001   0.74690002   0.0231       0.90509999]
[37m[1m [382.09718572   0.79170001   0.81659997   0.042        0.85479993]
[37m[1m [348.42607371   0.75830001   0.8118       0.0212       0.89069998]
[37m[1m ...
[37m[1m [446.71601504   0.65359998   0.77349997   0.0209       0.89139998]
[37m[1m [309.51347676   0.79409999   0.84110004   0.0223       0.91239995]
[37m[1m [483.62964027   0.78370005   0.75270003   0.03         0.77899998]]
[37m[1m[2023-06-25 04:28:52,006][129146] Max Reward on eval: 591.6605776015437
[37m[1m[2023-06-25 04:28:52,006][129146] Min Reward on eval: 118.98385217483155
[37m[1m[2023-06-25 04:28:52,006][129146] Mean Reward across all agents: 377.52941225565525
[37m[1m[2023-06-25 04:28:52,006][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:28:52,010][129146] mean_value=-19.543143513099775, max_value=1059.6988265901805
[37m[1m[2023-06-25 04:28:52,012][129146] New mean coefficients: [[ 0.7610713  -0.82627505 -1.5984248  -0.7170392   2.8842654 ]]
[37m[1m[2023-06-25 04:28:52,013][129146] Moving the mean solution point...
[36m[2023-06-25 04:29:01,689][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 04:29:01,689][129146] FPS: 396948.62
[36m[2023-06-25 04:29:01,691][129146] itr=363, itrs=2000, Progress: 18.15%
[36m[2023-06-25 04:29:13,136][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 04:29:13,136][129146] FPS: 336070.73
[36m[2023-06-25 04:29:18,039][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:29:18,039][129146] Reward + Measures: [[364.93335339   0.782812     0.80914837   0.01962266   0.9146226 ]]
[37m[1m[2023-06-25 04:29:18,040][129146] Max Reward on eval: 364.93335338761455
[37m[1m[2023-06-25 04:29:18,040][129146] Min Reward on eval: 364.93335338761455
[37m[1m[2023-06-25 04:29:18,040][129146] Mean Reward across all agents: 364.93335338761455
[37m[1m[2023-06-25 04:29:18,040][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:29:23,587][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:29:23,588][129146] Reward + Measures: [[355.77143655   0.80800003   0.84630007   0.0208       0.91149998]
[37m[1m [270.55149434   0.57480001   0.70649999   0.065        0.83260006]
[37m[1m [396.14982054   0.85290003   0.86510003   0.0196       0.9176001 ]
[37m[1m ...
[37m[1m [252.4210804    0.56310004   0.72830003   0.0299       0.86320001]
[37m[1m [327.96345307   0.58269995   0.75149995   0.037        0.86350006]
[37m[1m [331.27545617   0.70859998   0.78839999   0.0204       0.90889996]]
[37m[1m[2023-06-25 04:29:23,588][129146] Max Reward on eval: 715.6345884041802
[37m[1m[2023-06-25 04:29:23,589][129146] Min Reward on eval: 168.45453866719035
[37m[1m[2023-06-25 04:29:23,589][129146] Mean Reward across all agents: 333.0791637143261
[37m[1m[2023-06-25 04:29:23,589][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:29:23,592][129146] mean_value=28.898771936319886, max_value=830.195547823573
[37m[1m[2023-06-25 04:29:23,594][129146] New mean coefficients: [[ 0.9807452  -1.6755576  -0.7761923   0.32412338  4.308452  ]]
[37m[1m[2023-06-25 04:29:23,595][129146] Moving the mean solution point...
[36m[2023-06-25 04:29:33,464][129146] train() took 9.87 seconds to complete
[36m[2023-06-25 04:29:33,464][129146] FPS: 389176.04
[36m[2023-06-25 04:29:33,467][129146] itr=364, itrs=2000, Progress: 18.20%
[36m[2023-06-25 04:29:44,918][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 04:29:44,919][129146] FPS: 335831.76
[36m[2023-06-25 04:29:49,765][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:29:49,765][129146] Reward + Measures: [[390.18003267   0.73794061   0.78434926   0.02078367   0.91009599]]
[37m[1m[2023-06-25 04:29:49,766][129146] Max Reward on eval: 390.1800326654313
[37m[1m[2023-06-25 04:29:49,766][129146] Min Reward on eval: 390.1800326654313
[37m[1m[2023-06-25 04:29:49,766][129146] Mean Reward across all agents: 390.1800326654313
[37m[1m[2023-06-25 04:29:49,766][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:29:55,330][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:29:55,331][129146] Reward + Measures: [[509.98397991   0.62300003   0.72899997   0.0341       0.81490004]
[37m[1m [368.07371791   0.80450004   0.77920002   0.0233       0.86700004]
[37m[1m [469.81728476   0.6573       0.78420001   0.0265       0.89040005]
[37m[1m ...
[37m[1m [431.66357572   0.662        0.73590004   0.0285       0.88700008]
[37m[1m [601.62370663   0.55400008   0.71370006   0.0394       0.78050005]
[37m[1m [631.62849053   0.41230002   0.58880001   0.0773       0.62099999]]
[37m[1m[2023-06-25 04:29:55,331][129146] Max Reward on eval: 1052.158265610214
[37m[1m[2023-06-25 04:29:55,331][129146] Min Reward on eval: 282.13775276599915
[37m[1m[2023-06-25 04:29:55,331][129146] Mean Reward across all agents: 473.47594082389395
[37m[1m[2023-06-25 04:29:55,332][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:29:55,337][129146] mean_value=100.03305650223602, max_value=1017.5079761742497
[37m[1m[2023-06-25 04:29:55,339][129146] New mean coefficients: [[ 2.5676332 -2.3625574 -0.9611492  0.0771468  4.7006927]]
[37m[1m[2023-06-25 04:29:55,340][129146] Moving the mean solution point...
[36m[2023-06-25 04:30:05,141][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 04:30:05,141][129146] FPS: 391870.25
[36m[2023-06-25 04:30:05,144][129146] itr=365, itrs=2000, Progress: 18.25%
[36m[2023-06-25 04:30:16,586][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 04:30:16,586][129146] FPS: 336163.43
[36m[2023-06-25 04:30:21,458][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:30:21,458][129146] Reward + Measures: [[407.99335112   0.68702066   0.75801605   0.02176433   0.90427232]]
[37m[1m[2023-06-25 04:30:21,458][129146] Max Reward on eval: 407.99335111721086
[37m[1m[2023-06-25 04:30:21,459][129146] Min Reward on eval: 407.99335111721086
[37m[1m[2023-06-25 04:30:21,459][129146] Mean Reward across all agents: 407.99335111721086
[37m[1m[2023-06-25 04:30:21,459][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:30:27,131][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:30:27,131][129146] Reward + Measures: [[670.4637312    0.63889998   0.69639999   0.0327       0.72399998]
[37m[1m [502.48329374   0.5395       0.67070001   0.0422       0.78790003]
[37m[1m [608.02503479   0.59280008   0.69489998   0.0289       0.73180002]
[37m[1m ...
[37m[1m [645.52084082   0.52459997   0.66300005   0.0413       0.74239999]
[37m[1m [497.64777516   0.6408       0.73699999   0.0224       0.87270004]
[37m[1m [520.79122282   0.65880001   0.73379993   0.0273       0.83540004]]
[37m[1m[2023-06-25 04:30:27,131][129146] Max Reward on eval: 1193.255877507737
[37m[1m[2023-06-25 04:30:27,132][129146] Min Reward on eval: 180.9177094372397
[37m[1m[2023-06-25 04:30:27,132][129146] Mean Reward across all agents: 548.7192955320075
[37m[1m[2023-06-25 04:30:27,132][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:30:27,138][129146] mean_value=33.737090374515425, max_value=890.9744750717841
[37m[1m[2023-06-25 04:30:27,140][129146] New mean coefficients: [[ 2.3213804 -0.8230789 -2.0360076  1.9359564  4.8378425]]
[37m[1m[2023-06-25 04:30:27,141][129146] Moving the mean solution point...
[36m[2023-06-25 04:30:37,004][129146] train() took 9.86 seconds to complete
[36m[2023-06-25 04:30:37,004][129146] FPS: 389406.53
[36m[2023-06-25 04:30:37,006][129146] itr=366, itrs=2000, Progress: 18.30%
[36m[2023-06-25 04:30:48,504][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 04:30:48,505][129146] FPS: 334443.73
[36m[2023-06-25 04:30:53,334][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:30:53,334][129146] Reward + Measures: [[451.53031818   0.68057698   0.73731703   0.02226333   0.89828628]]
[37m[1m[2023-06-25 04:30:53,334][129146] Max Reward on eval: 451.5303181803169
[37m[1m[2023-06-25 04:30:53,335][129146] Min Reward on eval: 451.5303181803169
[37m[1m[2023-06-25 04:30:53,335][129146] Mean Reward across all agents: 451.5303181803169
[37m[1m[2023-06-25 04:30:53,335][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:30:58,884][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:30:58,885][129146] Reward + Measures: [[432.14254264   0.62180007   0.72599995   0.042        0.81110001]
[37m[1m [496.41997393   0.86679995   0.85220003   0.0269       0.90670007]
[37m[1m [521.25953942   0.67720002   0.71070004   0.0521       0.78909999]
[37m[1m ...
[37m[1m [744.76737349   0.4594       0.58560002   0.0839       0.61170006]
[37m[1m [291.51963318   0.61160004   0.70179999   0.0251       0.86499995]
[37m[1m [450.853043     0.66949999   0.77170002   0.0308       0.86020005]]
[37m[1m[2023-06-25 04:30:58,885][129146] Max Reward on eval: 986.9488525703782
[37m[1m[2023-06-25 04:30:58,885][129146] Min Reward on eval: 288.9494773105951
[37m[1m[2023-06-25 04:30:58,885][129146] Mean Reward across all agents: 519.3458400635556
[37m[1m[2023-06-25 04:30:58,886][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:30:58,888][129146] mean_value=-36.68348032675023, max_value=598.3527434207949
[37m[1m[2023-06-25 04:30:58,891][129146] New mean coefficients: [[ 2.9140558 -0.9834362 -3.2726393  1.9721782  3.2725122]]
[37m[1m[2023-06-25 04:30:58,892][129146] Moving the mean solution point...
[36m[2023-06-25 04:31:08,667][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 04:31:08,667][129146] FPS: 392902.24
[36m[2023-06-25 04:31:08,670][129146] itr=367, itrs=2000, Progress: 18.35%
[36m[2023-06-25 04:31:20,181][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 04:31:20,181][129146] FPS: 334047.43
[36m[2023-06-25 04:31:25,008][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:31:25,009][129146] Reward + Measures: [[496.06167569   0.63733798   0.7072553    0.02357133   0.8792907 ]]
[37m[1m[2023-06-25 04:31:25,009][129146] Max Reward on eval: 496.06167568931465
[37m[1m[2023-06-25 04:31:25,009][129146] Min Reward on eval: 496.06167568931465
[37m[1m[2023-06-25 04:31:25,009][129146] Mean Reward across all agents: 496.06167568931465
[37m[1m[2023-06-25 04:31:25,010][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:31:30,569][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:31:30,576][129146] Reward + Measures: [[499.38019061   0.43899998   0.30140001   0.26410002   0.25629997]
[37m[1m [448.49534563   0.68059999   0.72600001   0.0243       0.87919998]
[37m[1m [520.79533525   0.59969997   0.65039998   0.0539       0.77220005]
[37m[1m ...
[37m[1m [655.1754408    0.60610002   0.74960005   0.0433       0.76920003]
[37m[1m [562.18832194   0.63819999   0.76070005   0.0255       0.88549995]
[37m[1m [547.44011257   0.70150006   0.70810002   0.0224       0.84420007]]
[37m[1m[2023-06-25 04:31:30,577][129146] Max Reward on eval: 1160.4106240583933
[37m[1m[2023-06-25 04:31:30,577][129146] Min Reward on eval: 188.86161061571912
[37m[1m[2023-06-25 04:31:30,578][129146] Mean Reward across all agents: 617.3709365164942
[37m[1m[2023-06-25 04:31:30,578][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:31:30,587][129146] mean_value=57.38676428685884, max_value=1154.8399766989285
[37m[1m[2023-06-25 04:31:30,591][129146] New mean coefficients: [[ 3.172932   -0.33763283 -3.4547453   2.5053382   3.2851918 ]]
[37m[1m[2023-06-25 04:31:30,593][129146] Moving the mean solution point...
[36m[2023-06-25 04:31:40,476][129146] train() took 9.88 seconds to complete
[36m[2023-06-25 04:31:40,476][129146] FPS: 388637.27
[36m[2023-06-25 04:31:40,478][129146] itr=368, itrs=2000, Progress: 18.40%
[36m[2023-06-25 04:31:51,915][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 04:31:51,915][129146] FPS: 336335.38
[36m[2023-06-25 04:31:56,638][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:31:56,638][129146] Reward + Measures: [[594.72300728   0.61090171   0.66704369   0.030462     0.83470398]]
[37m[1m[2023-06-25 04:31:56,638][129146] Max Reward on eval: 594.7230072763266
[37m[1m[2023-06-25 04:31:56,638][129146] Min Reward on eval: 594.7230072763266
[37m[1m[2023-06-25 04:31:56,639][129146] Mean Reward across all agents: 594.7230072763266
[37m[1m[2023-06-25 04:31:56,639][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:32:02,068][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:32:02,069][129146] Reward + Measures: [[732.36191438   0.49230003   0.56330007   0.0775       0.60460001]
[37m[1m [608.0164579    0.52929997   0.66949999   0.0338       0.83020002]
[37m[1m [819.36729942   0.53039998   0.6142       0.0546       0.64889997]
[37m[1m ...
[37m[1m [613.82543771   0.58429998   0.67120004   0.0384       0.78890002]
[37m[1m [571.93216173   0.53460002   0.67130005   0.07120001   0.65189999]
[37m[1m [725.25454428   0.59800005   0.69309998   0.0463       0.68230003]]
[37m[1m[2023-06-25 04:32:02,069][129146] Max Reward on eval: 1266.1477165269666
[37m[1m[2023-06-25 04:32:02,069][129146] Min Reward on eval: 346.3916950481944
[37m[1m[2023-06-25 04:32:02,069][129146] Mean Reward across all agents: 688.3068455343805
[37m[1m[2023-06-25 04:32:02,070][129146] Average Trajectory Length: 999.9106666666667
[36m[2023-06-25 04:32:02,074][129146] mean_value=59.07629832825447, max_value=682.786473151483
[37m[1m[2023-06-25 04:32:02,077][129146] New mean coefficients: [[ 2.4759555  -0.35141057 -4.0729475   1.0152655   3.252881  ]]
[37m[1m[2023-06-25 04:32:02,078][129146] Moving the mean solution point...
[36m[2023-06-25 04:32:11,907][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 04:32:11,907][129146] FPS: 390736.70
[36m[2023-06-25 04:32:11,909][129146] itr=369, itrs=2000, Progress: 18.45%
[36m[2023-06-25 04:32:23,438][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 04:32:23,438][129146] FPS: 333615.57
[36m[2023-06-25 04:32:28,212][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:32:28,212][129146] Reward + Measures: [[879.43187848   0.53164333   0.59516168   0.059643     0.68655235]]
[37m[1m[2023-06-25 04:32:28,213][129146] Max Reward on eval: 879.4318784816163
[37m[1m[2023-06-25 04:32:28,213][129146] Min Reward on eval: 879.4318784816163
[37m[1m[2023-06-25 04:32:28,213][129146] Mean Reward across all agents: 879.4318784816163
[37m[1m[2023-06-25 04:32:28,213][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:32:33,673][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:32:33,673][129146] Reward + Measures: [[ 632.23887265    0.51850003    0.62149996    0.0371        0.75809997]
[37m[1m [ 804.88218485    0.47940001    0.58310002    0.0602        0.68760002]
[37m[1m [1184.64297628    0.40979996    0.46920004    0.1391        0.42860004]
[37m[1m ...
[37m[1m [1334.6232636     0.42130002    0.43010002    0.1261        0.40630004]
[37m[1m [ 351.41766446    0.56720001    0.43889999    0.41479999    0.40000001]
[37m[1m [ 676.3278724     0.48700005    0.58829999    0.0468        0.72000003]]
[37m[1m[2023-06-25 04:32:33,674][129146] Max Reward on eval: 1499.0823143076384
[37m[1m[2023-06-25 04:32:33,674][129146] Min Reward on eval: 351.4176644631836
[37m[1m[2023-06-25 04:32:33,674][129146] Mean Reward across all agents: 934.9828375190632
[37m[1m[2023-06-25 04:32:33,674][129146] Average Trajectory Length: 999.4946666666666
[36m[2023-06-25 04:32:33,681][129146] mean_value=136.98182881943146, max_value=1256.9478007631383
[37m[1m[2023-06-25 04:32:33,684][129146] New mean coefficients: [[ 2.7777238  -0.42403787 -5.298359    0.78048456  1.9952935 ]]
[37m[1m[2023-06-25 04:32:33,685][129146] Moving the mean solution point...
[36m[2023-06-25 04:32:43,504][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 04:32:43,504][129146] FPS: 391147.23
[36m[2023-06-25 04:32:43,506][129146] itr=370, itrs=2000, Progress: 18.50%
[37m[1m[2023-06-25 04:32:47,469][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000350
[36m[2023-06-25 04:32:59,337][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 04:32:59,337][129146] FPS: 332475.65
[36m[2023-06-25 04:33:04,106][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:33:04,107][129146] Reward + Measures: [[1413.9599369     0.44338334    0.47788966    0.11935699    0.40713966]]
[37m[1m[2023-06-25 04:33:04,107][129146] Max Reward on eval: 1413.9599368954102
[37m[1m[2023-06-25 04:33:04,107][129146] Min Reward on eval: 1413.9599368954102
[37m[1m[2023-06-25 04:33:04,107][129146] Mean Reward across all agents: 1413.9599368954102
[37m[1m[2023-06-25 04:33:04,108][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:33:09,675][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:33:09,675][129146] Reward + Measures: [[1008.40999163    0.51440001    0.54329997    0.0816        0.62160003]
[37m[1m [ 835.32598949    0.40130001    0.39089999    0.1833        0.35030001]
[37m[1m [ 824.8110545     0.40490004    0.43969998    0.19880001    0.44159999]
[37m[1m ...
[37m[1m [1174.10394774    0.40900001    0.44820005    0.1265        0.41700003]
[37m[1m [1191.9951574     0.47230002    0.5334        0.14479999    0.46360001]
[37m[1m [ 837.63160204    0.47160003    0.52850002    0.15189999    0.54290003]]
[37m[1m[2023-06-25 04:33:09,675][129146] Max Reward on eval: 1551.6361411782448
[37m[1m[2023-06-25 04:33:09,676][129146] Min Reward on eval: 548.185074316425
[37m[1m[2023-06-25 04:33:09,676][129146] Mean Reward across all agents: 1082.5652353393311
[37m[1m[2023-06-25 04:33:09,676][129146] Average Trajectory Length: 994.8633333333333
[36m[2023-06-25 04:33:09,681][129146] mean_value=-65.97482377938995, max_value=1203.7898827201006
[37m[1m[2023-06-25 04:33:09,683][129146] New mean coefficients: [[ 1.9205179   0.0949949  -4.7326      0.54814315  2.963899  ]]
[37m[1m[2023-06-25 04:33:09,684][129146] Moving the mean solution point...
[36m[2023-06-25 04:33:19,319][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 04:33:19,319][129146] FPS: 398627.11
[36m[2023-06-25 04:33:19,321][129146] itr=371, itrs=2000, Progress: 18.55%
[36m[2023-06-25 04:33:30,744][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 04:33:30,744][129146] FPS: 336640.50
[36m[2023-06-25 04:33:35,597][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:33:35,597][129146] Reward + Measures: [[1672.8796167     0.41653347    0.42075399    0.14367972    0.29625678]]
[37m[1m[2023-06-25 04:33:35,597][129146] Max Reward on eval: 1672.8796166960333
[37m[1m[2023-06-25 04:33:35,598][129146] Min Reward on eval: 1672.8796166960333
[37m[1m[2023-06-25 04:33:35,598][129146] Mean Reward across all agents: 1672.8796166960333
[37m[1m[2023-06-25 04:33:35,598][129146] Average Trajectory Length: 999.206
[36m[2023-06-25 04:33:40,963][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:33:40,963][129146] Reward + Measures: [[1467.4259974     0.3917        0.3651        0.1409        0.27320001]
[37m[1m [ 509.40019896    0.4941        0.57310003    0.0499        0.62830001]
[37m[1m [ 834.3529716     0.56150001    0.58630008    0.0554        0.662     ]
[37m[1m ...
[37m[1m [1095.82302259    0.42109999    0.44369999    0.117         0.35320002]
[37m[1m [1001.0943945     0.4454        0.42630002    0.1102        0.39850003]
[37m[1m [1505.82773539    0.40079999    0.3678        0.1532        0.28010002]]
[37m[1m[2023-06-25 04:33:40,964][129146] Max Reward on eval: 1745.9262167044449
[37m[1m[2023-06-25 04:33:40,964][129146] Min Reward on eval: 348.5739616859355
[37m[1m[2023-06-25 04:33:40,964][129146] Mean Reward across all agents: 1160.0201700303408
[37m[1m[2023-06-25 04:33:40,964][129146] Average Trajectory Length: 999.8413333333333
[36m[2023-06-25 04:33:40,968][129146] mean_value=-90.3864366903273, max_value=703.4185210068352
[37m[1m[2023-06-25 04:33:40,970][129146] New mean coefficients: [[ 2.83128    0.7626868 -3.694879   0.4441678  3.3538342]]
[37m[1m[2023-06-25 04:33:40,971][129146] Moving the mean solution point...
[36m[2023-06-25 04:33:50,660][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 04:33:50,660][129146] FPS: 396408.01
[36m[2023-06-25 04:33:50,663][129146] itr=372, itrs=2000, Progress: 18.60%
[36m[2023-06-25 04:34:02,187][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 04:34:02,187][129146] FPS: 333712.90
[36m[2023-06-25 04:34:06,981][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:34:06,981][129146] Reward + Measures: [[1802.85977745    0.41217977    0.4004831     0.14841814    0.2624191 ]]
[37m[1m[2023-06-25 04:34:06,981][129146] Max Reward on eval: 1802.8597774536547
[37m[1m[2023-06-25 04:34:06,981][129146] Min Reward on eval: 1802.8597774536547
[37m[1m[2023-06-25 04:34:06,982][129146] Mean Reward across all agents: 1802.8597774536547
[37m[1m[2023-06-25 04:34:06,982][129146] Average Trajectory Length: 999.2239999999999
[36m[2023-06-25 04:34:12,721][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:34:12,721][129146] Reward + Measures: [[1426.4919824     0.47029996    0.37490001    0.183         0.27800003]
[37m[1m [1100.68970264    0.51910001    0.36840001    0.28630003    0.2624    ]
[37m[1m [1228.19745167    0.4594        0.48389998    0.10770001    0.42050001]
[37m[1m ...
[37m[1m [1576.08770563    0.41704446    0.38865       0.15390001    0.26444444]
[37m[1m [1403.27392206    0.45730001    0.3784        0.1885        0.27700004]
[37m[1m [1145.70040041    0.50490004    0.49880001    0.1012        0.47240001]]
[37m[1m[2023-06-25 04:34:12,722][129146] Max Reward on eval: 1798.6990233807592
[37m[1m[2023-06-25 04:34:12,722][129146] Min Reward on eval: 327.13388958913276
[37m[1m[2023-06-25 04:34:12,722][129146] Mean Reward across all agents: 1410.614008044965
[37m[1m[2023-06-25 04:34:12,722][129146] Average Trajectory Length: 999.4126666666666
[36m[2023-06-25 04:34:12,726][129146] mean_value=-29.507097044449583, max_value=451.3562747066287
[37m[1m[2023-06-25 04:34:12,729][129146] New mean coefficients: [[ 3.559812   1.3619092 -2.7114203  1.7353125  3.1400208]]
[37m[1m[2023-06-25 04:34:12,730][129146] Moving the mean solution point...
[36m[2023-06-25 04:34:22,536][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 04:34:22,536][129146] FPS: 391686.29
[36m[2023-06-25 04:34:22,538][129146] itr=373, itrs=2000, Progress: 18.65%
[36m[2023-06-25 04:34:34,189][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 04:34:34,189][129146] FPS: 330062.15
[36m[2023-06-25 04:34:39,023][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:34:39,023][129146] Reward + Measures: [[1918.09831174    0.40413034    0.39747456    0.14078796    0.25359812]]
[37m[1m[2023-06-25 04:34:39,024][129146] Max Reward on eval: 1918.0983117388596
[37m[1m[2023-06-25 04:34:39,024][129146] Min Reward on eval: 1918.0983117388596
[37m[1m[2023-06-25 04:34:39,024][129146] Mean Reward across all agents: 1918.0983117388596
[37m[1m[2023-06-25 04:34:39,024][129146] Average Trajectory Length: 999.0013333333333
[36m[2023-06-25 04:34:44,752][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:34:44,752][129146] Reward + Measures: [[1225.57053375    0.40110001    0.45120001    0.18889999    0.40630004]
[37m[1m [1717.84911925    0.3998        0.39170003    0.1054        0.25659999]
[37m[1m [ 724.4389303     0.4743        0.34419999    0.31780002    0.3089    ]
[37m[1m ...
[37m[1m [ 969.93026559    0.53509998    0.34240001    0.30300003    0.27579999]
[37m[1m [1504.41517445    0.4587        0.42989999    0.15640001    0.30850002]
[37m[1m [1252.66094412    0.46489999    0.36430001    0.22939999    0.292     ]]
[37m[1m[2023-06-25 04:34:44,752][129146] Max Reward on eval: 1870.3702136385953
[37m[1m[2023-06-25 04:34:44,753][129146] Min Reward on eval: -360.2765262264118
[37m[1m[2023-06-25 04:34:44,753][129146] Mean Reward across all agents: 1138.8072746590215
[37m[1m[2023-06-25 04:34:44,753][129146] Average Trajectory Length: 997.8716666666667
[36m[2023-06-25 04:34:44,757][129146] mean_value=-169.99874456725402, max_value=1025.7249626537794
[37m[1m[2023-06-25 04:34:44,759][129146] New mean coefficients: [[ 2.866683   1.2392888 -1.9265761  1.4539382  2.9471638]]
[37m[1m[2023-06-25 04:34:44,760][129146] Moving the mean solution point...
[36m[2023-06-25 04:34:54,525][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 04:34:54,526][129146] FPS: 393299.07
[36m[2023-06-25 04:34:54,528][129146] itr=374, itrs=2000, Progress: 18.70%
[36m[2023-06-25 04:35:06,114][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 04:35:06,115][129146] FPS: 331890.17
[36m[2023-06-25 04:35:10,926][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:35:10,927][129146] Reward + Measures: [[2022.43298123    0.41000614    0.39593443    0.13891864    0.24639137]]
[37m[1m[2023-06-25 04:35:10,927][129146] Max Reward on eval: 2022.4329812346555
[37m[1m[2023-06-25 04:35:10,927][129146] Min Reward on eval: 2022.4329812346555
[37m[1m[2023-06-25 04:35:10,928][129146] Mean Reward across all agents: 2022.4329812346555
[37m[1m[2023-06-25 04:35:10,928][129146] Average Trajectory Length: 999.5343333333333
[36m[2023-06-25 04:35:16,367][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:35:16,367][129146] Reward + Measures: [[ 795.53291407    0.49449998    0.59700006    0.1146        0.55190003]
[37m[1m [1258.94143484    0.47740003    0.43720004    0.16940002    0.3856    ]
[37m[1m [1965.65681073    0.41600004    0.38650003    0.14780001    0.25120002]
[37m[1m ...
[37m[1m [1225.18569599    0.43720004    0.41560003    0.1972        0.33440003]
[37m[1m [1607.1874363     0.39920002    0.41570002    0.12710001    0.30000001]
[37m[1m [1094.914164      0.4571        0.37789997    0.24410002    0.2421    ]]
[37m[1m[2023-06-25 04:35:16,368][129146] Max Reward on eval: 2068.18254987275
[37m[1m[2023-06-25 04:35:16,368][129146] Min Reward on eval: 637.3111839489313
[37m[1m[2023-06-25 04:35:16,368][129146] Mean Reward across all agents: 1584.1101063303443
[37m[1m[2023-06-25 04:35:16,368][129146] Average Trajectory Length: 999.5643333333333
[36m[2023-06-25 04:35:16,372][129146] mean_value=-16.893735989066858, max_value=1211.173185033515
[37m[1m[2023-06-25 04:35:16,375][129146] New mean coefficients: [[ 1.5996293  1.9466312 -1.4290739  1.8788893  2.5377452]]
[37m[1m[2023-06-25 04:35:16,376][129146] Moving the mean solution point...
[36m[2023-06-25 04:35:26,137][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 04:35:26,137][129146] FPS: 393476.49
[36m[2023-06-25 04:35:26,139][129146] itr=375, itrs=2000, Progress: 18.75%
[36m[2023-06-25 04:35:37,584][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 04:35:37,585][129146] FPS: 336050.47
[36m[2023-06-25 04:35:42,347][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:35:42,348][129146] Reward + Measures: [[2120.43762889    0.40043685    0.39280403    0.12850952    0.24784878]]
[37m[1m[2023-06-25 04:35:42,348][129146] Max Reward on eval: 2120.4376288893227
[37m[1m[2023-06-25 04:35:42,348][129146] Min Reward on eval: 2120.4376288893227
[37m[1m[2023-06-25 04:35:42,348][129146] Mean Reward across all agents: 2120.4376288893227
[37m[1m[2023-06-25 04:35:42,349][129146] Average Trajectory Length: 999.5776666666667
[36m[2023-06-25 04:35:47,881][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:35:47,882][129146] Reward + Measures: [[378.67900438   0.40510002   0.38830003   0.13250001   0.33600003]
[37m[1m [884.34741058   0.37         0.33190003   0.15009999   0.25369999]
[37m[1m [834.97391388   0.39750001   0.37680003   0.11849999   0.30590001]
[37m[1m ...
[37m[1m [990.0234768    0.40299997   0.33610001   0.15979999   0.25600001]
[37m[1m [874.05387374   0.32985827   0.30017194   0.1054483    0.25317192]
[37m[1m [137.83922464   0.26384169   0.23310684   0.22578005   0.18031751]]
[37m[1m[2023-06-25 04:35:47,882][129146] Max Reward on eval: 2015.8989551562584
[37m[1m[2023-06-25 04:35:47,882][129146] Min Reward on eval: -1088.785377387528
[37m[1m[2023-06-25 04:35:47,882][129146] Mean Reward across all agents: 484.21422198592006
[37m[1m[2023-06-25 04:35:47,883][129146] Average Trajectory Length: 951.754
[36m[2023-06-25 04:35:47,885][129146] mean_value=-1255.5418926940663, max_value=635.5166568151589
[37m[1m[2023-06-25 04:35:47,887][129146] New mean coefficients: [[ 2.6142607   1.2602717  -0.82674754  0.6320771   0.56099045]]
[37m[1m[2023-06-25 04:35:47,888][129146] Moving the mean solution point...
[36m[2023-06-25 04:35:57,717][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 04:35:57,718][129146] FPS: 390740.45
[36m[2023-06-25 04:35:57,720][129146] itr=376, itrs=2000, Progress: 18.80%
[36m[2023-06-25 04:36:09,217][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 04:36:09,217][129146] FPS: 334547.27
[36m[2023-06-25 04:36:14,045][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:36:14,045][129146] Reward + Measures: [[2224.88829726    0.39353487    0.39480883    0.12027789    0.24243009]]
[37m[1m[2023-06-25 04:36:14,045][129146] Max Reward on eval: 2224.888297264264
[37m[1m[2023-06-25 04:36:14,046][129146] Min Reward on eval: 2224.888297264264
[37m[1m[2023-06-25 04:36:14,046][129146] Mean Reward across all agents: 2224.888297264264
[37m[1m[2023-06-25 04:36:14,046][129146] Average Trajectory Length: 999.1453333333333
[36m[2023-06-25 04:36:19,685][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:36:19,686][129146] Reward + Measures: [[1983.51918865    0.41820002    0.37729999    0.1541        0.2552    ]
[37m[1m [1955.21972977    0.42729998    0.42550001    0.1189        0.27809998]
[37m[1m [2080.61609829    0.35429999    0.36410001    0.0891        0.2376    ]
[37m[1m ...
[37m[1m [2104.88903991    0.39840004    0.40669999    0.12879999    0.2649    ]
[37m[1m [1947.04563848    0.458         0.39739999    0.14600001    0.2491    ]
[37m[1m [1771.33764279    0.41370001    0.37340003    0.15040001    0.2256    ]]
[37m[1m[2023-06-25 04:36:19,686][129146] Max Reward on eval: 2249.2492753017227
[37m[1m[2023-06-25 04:36:19,686][129146] Min Reward on eval: 605.6711706579081
[37m[1m[2023-06-25 04:36:19,687][129146] Mean Reward across all agents: 1837.547147077086
[37m[1m[2023-06-25 04:36:19,687][129146] Average Trajectory Length: 999.8263333333333
[36m[2023-06-25 04:36:19,691][129146] mean_value=56.63695694952597, max_value=539.0667514362724
[37m[1m[2023-06-25 04:36:19,694][129146] New mean coefficients: [[ 2.0890808   0.3738318  -0.68397725 -0.12271839 -0.5213877 ]]
[37m[1m[2023-06-25 04:36:19,695][129146] Moving the mean solution point...
[36m[2023-06-25 04:36:29,399][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 04:36:29,399][129146] FPS: 395767.82
[36m[2023-06-25 04:36:29,402][129146] itr=377, itrs=2000, Progress: 18.85%
[36m[2023-06-25 04:36:40,958][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 04:36:40,958][129146] FPS: 332795.21
[36m[2023-06-25 04:36:45,741][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:36:45,742][129146] Reward + Measures: [[2321.58080254    0.37957016    0.39138761    0.1125372     0.23762743]]
[37m[1m[2023-06-25 04:36:45,742][129146] Max Reward on eval: 2321.5808025446986
[37m[1m[2023-06-25 04:36:45,742][129146] Min Reward on eval: 2321.5808025446986
[37m[1m[2023-06-25 04:36:45,743][129146] Mean Reward across all agents: 2321.5808025446986
[37m[1m[2023-06-25 04:36:45,743][129146] Average Trajectory Length: 999.3826666666666
[36m[2023-06-25 04:36:51,196][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:36:51,197][129146] Reward + Measures: [[1697.66900032    0.47280002    0.36699998    0.2027        0.2217    ]
[37m[1m [1887.6714105     0.3987        0.36919999    0.1146        0.25840002]
[37m[1m [2091.90276972    0.41770002    0.38410002    0.1473        0.2244    ]
[37m[1m ...
[37m[1m [2212.29413646    0.40040001    0.3928        0.1103        0.23910001]
[37m[1m [2033.36531704    0.44589996    0.396         0.13399999    0.23740001]
[37m[1m [1649.66466312    0.38880002    0.35460001    0.1619        0.2242    ]]
[37m[1m[2023-06-25 04:36:51,197][129146] Max Reward on eval: 2363.1899339016527
[37m[1m[2023-06-25 04:36:51,197][129146] Min Reward on eval: 450.0653552192962
[37m[1m[2023-06-25 04:36:51,197][129146] Mean Reward across all agents: 1901.9309130284455
[37m[1m[2023-06-25 04:36:51,198][129146] Average Trajectory Length: 998.3643333333333
[36m[2023-06-25 04:36:51,203][129146] mean_value=47.99402190018132, max_value=1273.5476915269364
[37m[1m[2023-06-25 04:36:51,206][129146] New mean coefficients: [[ 2.213608   0.3883742 -1.4192042  0.5992901 -0.7568548]]
[37m[1m[2023-06-25 04:36:51,207][129146] Moving the mean solution point...
[36m[2023-06-25 04:37:01,081][129146] train() took 9.87 seconds to complete
[36m[2023-06-25 04:37:01,081][129146] FPS: 388945.16
[36m[2023-06-25 04:37:01,084][129146] itr=378, itrs=2000, Progress: 18.90%
[36m[2023-06-25 04:37:12,663][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 04:37:12,664][129146] FPS: 332077.36
[36m[2023-06-25 04:37:17,504][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:37:17,504][129146] Reward + Measures: [[2406.48660169    0.38027808    0.39364445    0.10849359    0.23366846]]
[37m[1m[2023-06-25 04:37:17,504][129146] Max Reward on eval: 2406.4866016939623
[37m[1m[2023-06-25 04:37:17,505][129146] Min Reward on eval: 2406.4866016939623
[37m[1m[2023-06-25 04:37:17,505][129146] Mean Reward across all agents: 2406.4866016939623
[37m[1m[2023-06-25 04:37:17,505][129146] Average Trajectory Length: 999.8249999999999
[36m[2023-06-25 04:37:22,986][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:37:22,987][129146] Reward + Measures: [[1563.37630415    0.42869997    0.34830001    0.1304        0.22239999]
[37m[1m [ 453.77773963    0.41300002    0.37120003    0.22160001    0.1987    ]
[37m[1m [1372.12564885    0.40120003    0.40769997    0.09009999    0.39040002]
[37m[1m ...
[37m[1m [2109.66804459    0.4145        0.36180001    0.1093        0.2386    ]
[37m[1m [1046.33392009    0.42520005    0.52160001    0.0707        0.60079998]
[37m[1m [1418.30892106    0.42159995    0.4206        0.16260001    0.33559999]]
[37m[1m[2023-06-25 04:37:22,987][129146] Max Reward on eval: 2447.9195595714614
[37m[1m[2023-06-25 04:37:22,987][129146] Min Reward on eval: 349.2653802004643
[37m[1m[2023-06-25 04:37:22,988][129146] Mean Reward across all agents: 1516.244555591578
[37m[1m[2023-06-25 04:37:22,988][129146] Average Trajectory Length: 998.317
[36m[2023-06-25 04:37:22,992][129146] mean_value=-185.77756028008648, max_value=1235.2552568340961
[37m[1m[2023-06-25 04:37:22,994][129146] New mean coefficients: [[ 2.4409282   0.04880697 -1.3039258   0.42085433 -1.3583896 ]]
[37m[1m[2023-06-25 04:37:22,995][129146] Moving the mean solution point...
[36m[2023-06-25 04:37:32,721][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 04:37:32,721][129146] FPS: 394899.46
[36m[2023-06-25 04:37:32,724][129146] itr=379, itrs=2000, Progress: 18.95%
[36m[2023-06-25 04:37:44,272][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 04:37:44,273][129146] FPS: 333053.87
[36m[2023-06-25 04:37:49,062][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:37:49,068][129146] Reward + Measures: [[2510.65600649    0.37939614    0.39249048    0.10468941    0.23179752]]
[37m[1m[2023-06-25 04:37:49,068][129146] Max Reward on eval: 2510.6560064864425
[37m[1m[2023-06-25 04:37:49,068][129146] Min Reward on eval: 2510.6560064864425
[37m[1m[2023-06-25 04:37:49,068][129146] Mean Reward across all agents: 2510.6560064864425
[37m[1m[2023-06-25 04:37:49,068][129146] Average Trajectory Length: 999.7996666666667
[36m[2023-06-25 04:37:54,616][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:37:54,622][129146] Reward + Measures: [[2357.25410824    0.39649996    0.3811        0.1202        0.2321    ]
[37m[1m [1361.22035704    0.4743        0.33010003    0.26630002    0.19080001]
[37m[1m [1986.55600512    0.37369999    0.36880001    0.10769999    0.21229999]
[37m[1m ...
[37m[1m [2334.19369031    0.42719999    0.3748        0.13930002    0.2357    ]
[37m[1m [2186.97808996    0.38849998    0.4219        0.13069999    0.2368    ]
[37m[1m [2370.74889273    0.40045205    0.38439077    0.1272541     0.22607699]]
[37m[1m[2023-06-25 04:37:54,622][129146] Max Reward on eval: 2497.001386250323
[37m[1m[2023-06-25 04:37:54,623][129146] Min Reward on eval: 827.2582357743988
[37m[1m[2023-06-25 04:37:54,623][129146] Mean Reward across all agents: 2038.1152755749742
[37m[1m[2023-06-25 04:37:54,623][129146] Average Trajectory Length: 998.7436666666666
[36m[2023-06-25 04:37:54,628][129146] mean_value=53.207601137029364, max_value=1417.9224303798785
[37m[1m[2023-06-25 04:37:54,631][129146] New mean coefficients: [[ 2.2092185 -0.2673523 -1.9837532  0.4536314 -1.2675614]]
[37m[1m[2023-06-25 04:37:54,632][129146] Moving the mean solution point...
[36m[2023-06-25 04:38:04,356][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 04:38:04,356][129146] FPS: 394950.80
[36m[2023-06-25 04:38:04,358][129146] itr=380, itrs=2000, Progress: 19.00%
[37m[1m[2023-06-25 04:38:08,328][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000360
[36m[2023-06-25 04:38:20,100][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 04:38:20,100][129146] FPS: 335253.17
[36m[2023-06-25 04:38:24,739][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:38:24,739][129146] Reward + Measures: [[2611.85232893    0.37011984    0.38731617    0.09318119    0.22910199]]
[37m[1m[2023-06-25 04:38:24,739][129146] Max Reward on eval: 2611.8523289341847
[37m[1m[2023-06-25 04:38:24,740][129146] Min Reward on eval: 2611.8523289341847
[37m[1m[2023-06-25 04:38:24,740][129146] Mean Reward across all agents: 2611.8523289341847
[37m[1m[2023-06-25 04:38:24,740][129146] Average Trajectory Length: 999.2853333333333
[36m[2023-06-25 04:38:30,093][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:38:30,093][129146] Reward + Measures: [[ 568.33987559    0.4693        0.30130002    0.22920001    0.1866    ]
[37m[1m [2247.24612211    0.4003        0.4014        0.14500001    0.2455    ]
[37m[1m [  90.21287611    0.4344154     0.22933523    0.23380876    0.20984279]
[37m[1m ...
[37m[1m [1002.18334612    0.41209999    0.30040002    0.22500001    0.2024    ]
[37m[1m [ 575.14272622    0.45629999    0.43720004    0.1639        0.2579    ]
[37m[1m [ 282.01922638    0.41139999    0.39289999    0.20120001    0.31479999]]
[37m[1m[2023-06-25 04:38:30,094][129146] Max Reward on eval: 2592.902104905108
[37m[1m[2023-06-25 04:38:30,094][129146] Min Reward on eval: -2214.1545034960204
[37m[1m[2023-06-25 04:38:30,094][129146] Mean Reward across all agents: 1120.8125843185612
[37m[1m[2023-06-25 04:38:30,094][129146] Average Trajectory Length: 989.5853333333333
[36m[2023-06-25 04:38:30,097][129146] mean_value=-756.6642228186405, max_value=1293.4841894885747
[37m[1m[2023-06-25 04:38:30,099][129146] New mean coefficients: [[ 2.6683612  -0.1740607  -0.9355937  -0.76254404 -1.706962  ]]
[37m[1m[2023-06-25 04:38:30,100][129146] Moving the mean solution point...
[36m[2023-06-25 04:38:39,749][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 04:38:39,750][129146] FPS: 398026.98
[36m[2023-06-25 04:38:39,752][129146] itr=381, itrs=2000, Progress: 19.05%
[36m[2023-06-25 04:38:51,321][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 04:38:51,322][129146] FPS: 332450.11
[36m[2023-06-25 04:38:56,089][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:38:56,089][129146] Reward + Measures: [[2681.98387138    0.37041479    0.38931432    0.085208      0.22634844]]
[37m[1m[2023-06-25 04:38:56,089][129146] Max Reward on eval: 2681.9838713774866
[37m[1m[2023-06-25 04:38:56,089][129146] Min Reward on eval: 2681.9838713774866
[37m[1m[2023-06-25 04:38:56,090][129146] Mean Reward across all agents: 2681.9838713774866
[37m[1m[2023-06-25 04:38:56,090][129146] Average Trajectory Length: 999.7123333333333
[36m[2023-06-25 04:39:01,596][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:39:01,597][129146] Reward + Measures: [[ 793.44851478    0.50049996    0.28909999    0.2949        0.2441    ]
[37m[1m [ 264.75595727    0.382175      0.44569999    0.12667501    0.49977499]
[37m[1m [1941.33345352    0.39860001    0.34610003    0.19630001    0.22649999]
[37m[1m ...
[37m[1m [ 123.37181052    0.37110004    0.49810001    0.14820001    0.55269998]
[37m[1m [1915.2172832     0.47100002    0.36929998    0.204         0.25150001]
[37m[1m [1774.84115319    0.39089999    0.34209999    0.18309999    0.22520001]]
[37m[1m[2023-06-25 04:39:01,597][129146] Max Reward on eval: 2610.457408466563
[37m[1m[2023-06-25 04:39:01,597][129146] Min Reward on eval: -623.1462204744341
[37m[1m[2023-06-25 04:39:01,598][129146] Mean Reward across all agents: 1024.2516498712985
[37m[1m[2023-06-25 04:39:01,598][129146] Average Trajectory Length: 983.54
[36m[2023-06-25 04:39:01,601][129146] mean_value=-287.5521020455888, max_value=1810.7148178667017
[37m[1m[2023-06-25 04:39:01,604][129146] New mean coefficients: [[ 2.6921115  -0.24547695 -0.84623134 -0.7830491  -0.87919444]]
[37m[1m[2023-06-25 04:39:01,605][129146] Moving the mean solution point...
[36m[2023-06-25 04:39:11,442][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 04:39:11,442][129146] FPS: 390455.70
[36m[2023-06-25 04:39:11,444][129146] itr=382, itrs=2000, Progress: 19.10%
[36m[2023-06-25 04:39:23,053][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 04:39:23,053][129146] FPS: 331287.54
[36m[2023-06-25 04:39:27,821][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:39:27,821][129146] Reward + Measures: [[2779.79203269    0.36282727    0.38633841    0.07614412    0.22554202]]
[37m[1m[2023-06-25 04:39:27,821][129146] Max Reward on eval: 2779.792032691637
[37m[1m[2023-06-25 04:39:27,822][129146] Min Reward on eval: 2779.792032691637
[37m[1m[2023-06-25 04:39:27,822][129146] Mean Reward across all agents: 2779.792032691637
[37m[1m[2023-06-25 04:39:27,822][129146] Average Trajectory Length: 999.1759999999999
[36m[2023-06-25 04:39:33,287][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:39:33,288][129146] Reward + Measures: [[1416.30158522    0.29130003    0.31889999    0.1295        0.2493    ]
[37m[1m [1836.41811356    0.41960001    0.329         0.13520001    0.25960001]
[37m[1m [1015.1829986     0.27071115    0.30465555    0.09364815    0.23434813]
[37m[1m ...
[37m[1m [ 802.81356321    0.66830003    0.56560004    0.51460004    0.178     ]
[37m[1m [1217.40367163    0.48649999    0.317         0.1754        0.2967    ]
[37m[1m [  68.98030325    0.43189999    0.19330001    0.2825        0.26140001]]
[37m[1m[2023-06-25 04:39:33,288][129146] Max Reward on eval: 2255.2048445260853
[37m[1m[2023-06-25 04:39:33,288][129146] Min Reward on eval: -742.1635964055255
[37m[1m[2023-06-25 04:39:33,288][129146] Mean Reward across all agents: 693.9175647059623
[37m[1m[2023-06-25 04:39:33,289][129146] Average Trajectory Length: 984.8353333333333
[36m[2023-06-25 04:39:33,291][129146] mean_value=-720.502991594476, max_value=904.7124248879607
[37m[1m[2023-06-25 04:39:33,294][129146] New mean coefficients: [[ 3.1542869   0.01492955 -1.3794168  -1.0487653  -1.0812075 ]]
[37m[1m[2023-06-25 04:39:33,295][129146] Moving the mean solution point...
[36m[2023-06-25 04:39:43,062][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 04:39:43,063][129146] FPS: 393209.50
[36m[2023-06-25 04:39:43,065][129146] itr=383, itrs=2000, Progress: 19.15%
[36m[2023-06-25 04:39:54,629][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 04:39:54,629][129146] FPS: 332581.64
[36m[2023-06-25 04:39:59,469][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:39:59,469][129146] Reward + Measures: [[2886.27150679    0.34993675    0.37802371    0.06271903    0.22503535]]
[37m[1m[2023-06-25 04:39:59,469][129146] Max Reward on eval: 2886.271506785886
[37m[1m[2023-06-25 04:39:59,469][129146] Min Reward on eval: 2886.271506785886
[37m[1m[2023-06-25 04:39:59,470][129146] Mean Reward across all agents: 2886.271506785886
[37m[1m[2023-06-25 04:39:59,470][129146] Average Trajectory Length: 999.2436666666666
[36m[2023-06-25 04:40:05,186][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:40:05,187][129146] Reward + Measures: [[2107.74488032    0.28860003    0.34810001    0.11060001    0.24129999]
[37m[1m [1390.03704118    0.30310002    0.31230003    0.13540001    0.25060001]
[37m[1m [2707.74182038    0.38860002    0.352         0.0782        0.2376    ]
[37m[1m ...
[37m[1m [ 799.43792175    0.26024401    0.31513333    0.09613521    0.29532835]
[37m[1m [1438.01168579    0.47510001    0.43570003    0.1551        0.30300003]
[37m[1m [2339.55287489    0.46310002    0.40650001    0.09980001    0.2538    ]]
[37m[1m[2023-06-25 04:40:05,187][129146] Max Reward on eval: 2869.5259541823993
[37m[1m[2023-06-25 04:40:05,187][129146] Min Reward on eval: -130.08325839413448
[37m[1m[2023-06-25 04:40:05,188][129146] Mean Reward across all agents: 1642.3478357902932
[37m[1m[2023-06-25 04:40:05,188][129146] Average Trajectory Length: 993.0403333333333
[36m[2023-06-25 04:40:05,192][129146] mean_value=-430.14246934635304, max_value=1496.540131964307
[37m[1m[2023-06-25 04:40:05,195][129146] New mean coefficients: [[ 2.224967    0.52201176 -0.25486767 -0.7975996  -0.08411354]]
[37m[1m[2023-06-25 04:40:05,196][129146] Moving the mean solution point...
[36m[2023-06-25 04:40:15,050][129146] train() took 9.85 seconds to complete
[36m[2023-06-25 04:40:15,051][129146] FPS: 389733.04
[36m[2023-06-25 04:40:15,053][129146] itr=384, itrs=2000, Progress: 19.20%
[36m[2023-06-25 04:40:26,711][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 04:40:26,711][129146] FPS: 329884.99
[36m[2023-06-25 04:40:31,509][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:40:31,509][129146] Reward + Measures: [[2965.64228088    0.3462157     0.36563364    0.04992338    0.22305846]]
[37m[1m[2023-06-25 04:40:31,509][129146] Max Reward on eval: 2965.6422808811503
[37m[1m[2023-06-25 04:40:31,509][129146] Min Reward on eval: 2965.6422808811503
[37m[1m[2023-06-25 04:40:31,510][129146] Mean Reward across all agents: 2965.6422808811503
[37m[1m[2023-06-25 04:40:31,510][129146] Average Trajectory Length: 999.5179999999999
[36m[2023-06-25 04:40:36,978][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:40:36,984][129146] Reward + Measures: [[ 203.38288073    0.4959425     0.5972808     0.07328724    0.64922774]
[37m[1m [ 463.1034937     0.55510002    0.63860005    0.0576        0.62370002]
[37m[1m [2095.1866951     0.41850001    0.44380003    0.0411        0.25      ]
[37m[1m ...
[37m[1m [ 421.05399988    0.2375        0.2175        0.1112        0.1605    ]
[37m[1m [-164.82945839    0.29084975    0.21218391    0.17120133    0.17740448]
[37m[1m [1958.76536766    0.41499996    0.3163        0.16849999    0.2402    ]]
[37m[1m[2023-06-25 04:40:36,984][129146] Max Reward on eval: 2863.0058754283004
[37m[1m[2023-06-25 04:40:36,985][129146] Min Reward on eval: -528.4733956773532
[37m[1m[2023-06-25 04:40:36,985][129146] Mean Reward across all agents: 1492.0768755154008
[37m[1m[2023-06-25 04:40:36,985][129146] Average Trajectory Length: 976.3663333333333
[36m[2023-06-25 04:40:36,988][129146] mean_value=-599.7887664503938, max_value=1003.6524404633274
[37m[1m[2023-06-25 04:40:36,991][129146] New mean coefficients: [[1.4813476  0.9595014  0.30289882 0.5530649  0.1394329 ]]
[37m[1m[2023-06-25 04:40:36,992][129146] Moving the mean solution point...
[36m[2023-06-25 04:40:46,815][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 04:40:46,815][129146] FPS: 391010.56
[36m[2023-06-25 04:40:46,817][129146] itr=385, itrs=2000, Progress: 19.25%
[36m[2023-06-25 04:40:58,365][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 04:40:58,365][129146] FPS: 333025.87
[36m[2023-06-25 04:41:03,157][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:41:03,158][129146] Reward + Measures: [[3056.42339584    0.35606104    0.37067845    0.04955977    0.22136627]]
[37m[1m[2023-06-25 04:41:03,158][129146] Max Reward on eval: 3056.4233958406535
[37m[1m[2023-06-25 04:41:03,158][129146] Min Reward on eval: 3056.4233958406535
[37m[1m[2023-06-25 04:41:03,158][129146] Mean Reward across all agents: 3056.4233958406535
[37m[1m[2023-06-25 04:41:03,159][129146] Average Trajectory Length: 999.6243333333333
[36m[2023-06-25 04:41:08,639][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:41:08,640][129146] Reward + Measures: [[  85.23290942    0.4657        0.31189999    0.40939999    0.29850003]
[37m[1m [ -90.85667804    0.4289        0.375         0.32290003    0.33530003]
[37m[1m [2332.41701143    0.43600002    0.41599998    0.0575        0.25620002]
[37m[1m ...
[37m[1m [ 864.20444629    0.38890001    0.41339999    0.1619        0.28690001]
[37m[1m [ 356.30921737    0.51480001    0.31259999    0.38750002    0.35609999]
[37m[1m [ 426.31607795    0.62889999    0.45320001    0.45559999    0.44169998]]
[37m[1m[2023-06-25 04:41:08,640][129146] Max Reward on eval: 2655.515922025358
[37m[1m[2023-06-25 04:41:08,640][129146] Min Reward on eval: -967.1302127300994
[37m[1m[2023-06-25 04:41:08,640][129146] Mean Reward across all agents: 750.162744299614
[37m[1m[2023-06-25 04:41:08,641][129146] Average Trajectory Length: 991.5623333333333
[36m[2023-06-25 04:41:08,645][129146] mean_value=-471.7859443246036, max_value=1351.7974223776273
[37m[1m[2023-06-25 04:41:08,648][129146] New mean coefficients: [[ 1.566807    0.34646708  1.0034273  -0.07565558 -0.8322961 ]]
[37m[1m[2023-06-25 04:41:08,649][129146] Moving the mean solution point...
[36m[2023-06-25 04:41:18,430][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 04:41:18,430][129146] FPS: 392652.37
[36m[2023-06-25 04:41:18,433][129146] itr=386, itrs=2000, Progress: 19.30%
[36m[2023-06-25 04:41:29,875][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 04:41:29,875][129146] FPS: 336133.95
[36m[2023-06-25 04:41:34,691][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:41:34,691][129146] Reward + Measures: [[3128.94971191    0.35819304    0.37023693    0.03606898    0.21955696]]
[37m[1m[2023-06-25 04:41:34,691][129146] Max Reward on eval: 3128.9497119108314
[37m[1m[2023-06-25 04:41:34,692][129146] Min Reward on eval: 3128.9497119108314
[37m[1m[2023-06-25 04:41:34,692][129146] Mean Reward across all agents: 3128.9497119108314
[37m[1m[2023-06-25 04:41:34,692][129146] Average Trajectory Length: 999.8956666666667
[36m[2023-06-25 04:41:40,104][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:41:40,105][129146] Reward + Measures: [[-332.23880577    0.35289198    0.19642475    0.15339532    0.21174014]
[37m[1m [-704.03393761    0.36229098    0.21956101    0.25339827    0.22832541]
[37m[1m [ 415.11382791    0.31920001    0.3637        0.1961        0.20610002]
[37m[1m ...
[37m[1m [-305.1633351     0.35055867    0.24663492    0.22648601    0.19508849]
[37m[1m [1299.48596047    0.50380003    0.4849        0.24010001    0.2599    ]
[37m[1m [ 199.23382055    0.32660004    0.3238        0.27990001    0.23340002]]
[37m[1m[2023-06-25 04:41:40,105][129146] Max Reward on eval: 2917.1840654656526
[37m[1m[2023-06-25 04:41:40,105][129146] Min Reward on eval: -850.4407850979478
[37m[1m[2023-06-25 04:41:40,105][129146] Mean Reward across all agents: 275.0573346881943
[37m[1m[2023-06-25 04:41:40,106][129146] Average Trajectory Length: 891.567
[36m[2023-06-25 04:41:40,108][129146] mean_value=-1052.2549982167523, max_value=1443.1606225713501
[37m[1m[2023-06-25 04:41:40,111][129146] New mean coefficients: [[-0.2217853   0.5183515   0.9952906   0.40256166 -0.224567  ]]
[37m[1m[2023-06-25 04:41:40,112][129146] Moving the mean solution point...
[36m[2023-06-25 04:41:49,721][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 04:41:49,721][129146] FPS: 399690.28
[36m[2023-06-25 04:41:49,724][129146] itr=387, itrs=2000, Progress: 19.35%
[36m[2023-06-25 04:42:01,489][129146] train() took 11.75 seconds to complete
[36m[2023-06-25 04:42:01,489][129146] FPS: 326873.50
[36m[2023-06-25 04:42:06,320][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:42:06,320][129146] Reward + Measures: [[3070.13661153    0.35711518    0.36000004    0.03373209    0.22097042]]
[37m[1m[2023-06-25 04:42:06,321][129146] Max Reward on eval: 3070.1366115297515
[37m[1m[2023-06-25 04:42:06,321][129146] Min Reward on eval: 3070.1366115297515
[37m[1m[2023-06-25 04:42:06,321][129146] Mean Reward across all agents: 3070.1366115297515
[37m[1m[2023-06-25 04:42:06,321][129146] Average Trajectory Length: 999.7006666666666
[36m[2023-06-25 04:42:11,784][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:42:11,785][129146] Reward + Measures: [[1388.71240308    0.48899999    0.37410003    0.10580001    0.35819998]
[37m[1m [1908.69923211    0.49559999    0.34560001    0.14039999    0.22850001]
[37m[1m [ 885.16620963    0.42750001    0.30630001    0.09950001    0.29270002]
[37m[1m ...
[37m[1m [ -94.585724      0.3876        0.33059999    0.1655        0.2863    ]
[37m[1m [ 944.46495077    0.4844        0.3017        0.1893        0.21370001]
[37m[1m [1901.49524275    0.42160001    0.44530001    0.11440001    0.2502    ]]
[37m[1m[2023-06-25 04:42:11,785][129146] Max Reward on eval: 3034.2004728612956
[37m[1m[2023-06-25 04:42:11,785][129146] Min Reward on eval: -781.6185202023014
[37m[1m[2023-06-25 04:42:11,786][129146] Mean Reward across all agents: 1443.684725908461
[37m[1m[2023-06-25 04:42:11,786][129146] Average Trajectory Length: 993.3143333333333
[36m[2023-06-25 04:42:11,789][129146] mean_value=-365.06536729158614, max_value=2360.740337354969
[37m[1m[2023-06-25 04:42:11,792][129146] New mean coefficients: [[-0.3596413   0.55759263  1.1534628   0.5156036   0.00240017]]
[37m[1m[2023-06-25 04:42:11,793][129146] Moving the mean solution point...
[36m[2023-06-25 04:42:21,604][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 04:42:21,605][129146] FPS: 391458.46
[36m[2023-06-25 04:42:21,607][129146] itr=388, itrs=2000, Progress: 19.40%
[36m[2023-06-25 04:42:33,269][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 04:42:33,269][129146] FPS: 329809.80
[36m[2023-06-25 04:42:38,158][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:42:38,158][129146] Reward + Measures: [[2961.15904085    0.36611605    0.38807935    0.047998      0.22083268]]
[37m[1m[2023-06-25 04:42:38,158][129146] Max Reward on eval: 2961.15904085205
[37m[1m[2023-06-25 04:42:38,159][129146] Min Reward on eval: 2961.15904085205
[37m[1m[2023-06-25 04:42:38,159][129146] Mean Reward across all agents: 2961.15904085205
[37m[1m[2023-06-25 04:42:38,159][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:42:43,844][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:42:43,844][129146] Reward + Measures: [[2381.3116423     0.36160001    0.39709997    0.0805        0.22090001]
[37m[1m [ 971.69455828    0.43459997    0.3831        0.14839999    0.30570003]
[37m[1m [2157.19011436    0.39750004    0.40360004    0.19240001    0.2502    ]
[37m[1m ...
[37m[1m [2472.38920993    0.40689999    0.39579999    0.12980001    0.2287    ]
[37m[1m [1154.05248904    0.42950001    0.36050001    0.1231        0.21089999]
[37m[1m [1173.92853519    0.38060004    0.36860004    0.27900001    0.29720002]]
[37m[1m[2023-06-25 04:42:43,844][129146] Max Reward on eval: 2892.7052412346006
[37m[1m[2023-06-25 04:42:43,845][129146] Min Reward on eval: -575.04684304361
[37m[1m[2023-06-25 04:42:43,845][129146] Mean Reward across all agents: 1485.7404355892247
[37m[1m[2023-06-25 04:42:43,845][129146] Average Trajectory Length: 966.5936666666666
[36m[2023-06-25 04:42:43,848][129146] mean_value=-602.9964869126723, max_value=1640.7924900510066
[37m[1m[2023-06-25 04:42:43,851][129146] New mean coefficients: [[-1.2134881   0.44017607  0.9093509  -0.31076974 -0.12479363]]
[37m[1m[2023-06-25 04:42:43,852][129146] Moving the mean solution point...
[36m[2023-06-25 04:42:53,581][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 04:42:53,581][129146] FPS: 394762.62
[36m[2023-06-25 04:42:53,584][129146] itr=389, itrs=2000, Progress: 19.45%
[36m[2023-06-25 04:43:05,090][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 04:43:05,090][129146] FPS: 334227.10
[36m[2023-06-25 04:43:09,906][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:43:09,906][129146] Reward + Measures: [[2459.81166513    0.43396834    0.39219898    0.076212      0.20862766]]
[37m[1m[2023-06-25 04:43:09,906][129146] Max Reward on eval: 2459.811665134824
[37m[1m[2023-06-25 04:43:09,907][129146] Min Reward on eval: 2459.811665134824
[37m[1m[2023-06-25 04:43:09,907][129146] Mean Reward across all agents: 2459.811665134824
[37m[1m[2023-06-25 04:43:09,907][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:43:15,390][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:43:15,391][129146] Reward + Measures: [[ 392.00968969    0.56999999    0.44029999    0.42669997    0.26859999]
[37m[1m [ 231.99278857    0.49910003    0.37100002    0.37080002    0.33140001]
[37m[1m [  42.59686876    0.5133        0.38229999    0.37820002    0.3303    ]
[37m[1m ...
[37m[1m [1046.94322944    0.33402941    0.24137059    0.13870588    0.18397646]
[37m[1m [  78.79604968    0.53109998    0.37799999    0.38329998    0.35260001]
[37m[1m [  80.41855074    0.58990002    0.44490001    0.48530003    0.48709998]]
[37m[1m[2023-06-25 04:43:15,391][129146] Max Reward on eval: 2392.9567226769404
[37m[1m[2023-06-25 04:43:15,391][129146] Min Reward on eval: -570.1196039451752
[37m[1m[2023-06-25 04:43:15,391][129146] Mean Reward across all agents: 555.5513050724622
[37m[1m[2023-06-25 04:43:15,392][129146] Average Trajectory Length: 979.0563333333333
[36m[2023-06-25 04:43:15,395][129146] mean_value=-574.3040486058203, max_value=1428.6815585312415
[37m[1m[2023-06-25 04:43:15,398][129146] New mean coefficients: [[-1.6881105   0.13121748  1.3033541  -1.1649718  -0.3860711 ]]
[37m[1m[2023-06-25 04:43:15,399][129146] Moving the mean solution point...
[36m[2023-06-25 04:43:25,098][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 04:43:25,099][129146] FPS: 395958.80
[36m[2023-06-25 04:43:25,101][129146] itr=390, itrs=2000, Progress: 19.50%
[37m[1m[2023-06-25 04:43:28,861][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000370
[36m[2023-06-25 04:43:40,674][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 04:43:40,674][129146] FPS: 334188.14
[36m[2023-06-25 04:43:45,305][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:43:45,306][129146] Reward + Measures: [[2296.92458068    0.45416105    0.38843364    0.08452315    0.2089479 ]]
[37m[1m[2023-06-25 04:43:45,306][129146] Max Reward on eval: 2296.924580684955
[37m[1m[2023-06-25 04:43:45,306][129146] Min Reward on eval: 2296.924580684955
[37m[1m[2023-06-25 04:43:45,306][129146] Mean Reward across all agents: 2296.924580684955
[37m[1m[2023-06-25 04:43:45,306][129146] Average Trajectory Length: 999.7736666666666
[36m[2023-06-25 04:43:50,663][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:43:50,669][129146] Reward + Measures: [[-273.38132531    0.5323        0.29210001    0.4296        0.2696    ]
[37m[1m [-647.77600038    0.45339999    0.21600001    0.44849998    0.21540001]
[37m[1m [ 613.08291972    0.49489999    0.3651        0.2045        0.19670001]
[37m[1m ...
[37m[1m [1847.82406021    0.35100001    0.41339999    0.0459        0.3159    ]
[37m[1m [-421.96743255    0.3655        0.2297        0.27340001    0.16779999]
[37m[1m [-857.48563942    0.34090003    0.21970001    0.34830001    0.1921    ]]
[37m[1m[2023-06-25 04:43:50,669][129146] Max Reward on eval: 2107.10164862657
[37m[1m[2023-06-25 04:43:50,669][129146] Min Reward on eval: -1225.757896288557
[37m[1m[2023-06-25 04:43:50,670][129146] Mean Reward across all agents: 496.1294635751147
[37m[1m[2023-06-25 04:43:50,670][129146] Average Trajectory Length: 982.3969999999999
[36m[2023-06-25 04:43:50,672][129146] mean_value=-800.8366881138247, max_value=1133.3357186364753
[37m[1m[2023-06-25 04:43:50,675][129146] New mean coefficients: [[-0.7211387   0.6835917   1.1861422  -0.26592016  0.05442107]]
[37m[1m[2023-06-25 04:43:50,676][129146] Moving the mean solution point...
[36m[2023-06-25 04:44:00,487][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 04:44:00,488][129146] FPS: 391427.06
[36m[2023-06-25 04:44:00,490][129146] itr=391, itrs=2000, Progress: 19.55%
[36m[2023-06-25 04:44:12,053][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 04:44:12,054][129146] FPS: 332539.14
[36m[2023-06-25 04:44:16,853][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:44:16,854][129146] Reward + Measures: [[2143.0618479     0.47833154    0.39583102    0.10033514    0.2060768 ]]
[37m[1m[2023-06-25 04:44:16,854][129146] Max Reward on eval: 2143.061847895058
[37m[1m[2023-06-25 04:44:16,854][129146] Min Reward on eval: 2143.061847895058
[37m[1m[2023-06-25 04:44:16,854][129146] Mean Reward across all agents: 2143.061847895058
[37m[1m[2023-06-25 04:44:16,855][129146] Average Trajectory Length: 999.8633333333333
[36m[2023-06-25 04:44:22,478][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:44:22,478][129146] Reward + Measures: [[1194.51991276    0.43060002    0.36590001    0.2           0.26089999]
[37m[1m [ -30.81061105    0.5851        0.27199998    0.56829995    0.24150001]
[37m[1m [-408.07720286    0.32428944    0.28705126    0.2166709     0.22347124]
[37m[1m ...
[37m[1m [ 283.32052782    0.6146        0.4409        0.45890003    0.35260001]
[37m[1m [ -50.10204297    0.41300002    0.45190001    0.1066        0.38370004]
[37m[1m [ 545.9567717     0.53910005    0.46709999    0.28400001    0.32160002]]
[37m[1m[2023-06-25 04:44:22,479][129146] Max Reward on eval: 2116.4651780711256
[37m[1m[2023-06-25 04:44:22,479][129146] Min Reward on eval: -766.4184611211182
[37m[1m[2023-06-25 04:44:22,479][129146] Mean Reward across all agents: 703.8203968772118
[37m[1m[2023-06-25 04:44:22,479][129146] Average Trajectory Length: 982.8806666666667
[36m[2023-06-25 04:44:22,483][129146] mean_value=-690.4437036104994, max_value=1721.0296781785446
[37m[1m[2023-06-25 04:44:22,486][129146] New mean coefficients: [[ 0.13164526  1.0556893   0.8630544   0.49789578 -0.34865198]]
[37m[1m[2023-06-25 04:44:22,487][129146] Moving the mean solution point...
[36m[2023-06-25 04:44:32,264][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 04:44:32,264][129146] FPS: 392817.42
[36m[2023-06-25 04:44:32,267][129146] itr=392, itrs=2000, Progress: 19.60%
[36m[2023-06-25 04:44:43,705][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 04:44:43,705][129146] FPS: 336206.49
[36m[2023-06-25 04:44:48,559][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:44:48,559][129146] Reward + Measures: [[2136.05194588    0.4872596     0.40232512    0.10535988    0.20500077]]
[37m[1m[2023-06-25 04:44:48,559][129146] Max Reward on eval: 2136.0519458847275
[37m[1m[2023-06-25 04:44:48,560][129146] Min Reward on eval: 2136.0519458847275
[37m[1m[2023-06-25 04:44:48,560][129146] Mean Reward across all agents: 2136.0519458847275
[37m[1m[2023-06-25 04:44:48,560][129146] Average Trajectory Length: 999.9453333333333
[36m[2023-06-25 04:44:54,076][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:44:54,076][129146] Reward + Measures: [[ 739.28994487    0.32819998    0.3197        0.24089999    0.23150001]
[37m[1m [2042.59927555    0.45660001    0.38820001    0.14619999    0.22500001]
[37m[1m [2166.13515553    0.46450001    0.39220002    0.0763        0.27309999]
[37m[1m ...
[37m[1m [2075.29147658    0.44959998    0.40580001    0.15640001    0.21039999]
[37m[1m [1167.67641416    0.45099998    0.47220001    0.13380001    0.2922    ]
[37m[1m [ 588.89371175    0.47860003    0.5212        0.1925        0.58400005]]
[37m[1m[2023-06-25 04:44:54,077][129146] Max Reward on eval: 2359.8944817607526
[37m[1m[2023-06-25 04:44:54,077][129146] Min Reward on eval: -359.3873622759944
[37m[1m[2023-06-25 04:44:54,077][129146] Mean Reward across all agents: 1307.9745351712138
[37m[1m[2023-06-25 04:44:54,077][129146] Average Trajectory Length: 997.831
[36m[2023-06-25 04:44:54,081][129146] mean_value=-343.52866677653157, max_value=2217.508490806818
[37m[1m[2023-06-25 04:44:54,084][129146] New mean coefficients: [[0.05620916 0.9809048  1.1189442  1.0141542  0.31895062]]
[37m[1m[2023-06-25 04:44:54,085][129146] Moving the mean solution point...
[36m[2023-06-25 04:45:03,993][129146] train() took 9.91 seconds to complete
[36m[2023-06-25 04:45:03,993][129146] FPS: 387630.80
[36m[2023-06-25 04:45:03,996][129146] itr=393, itrs=2000, Progress: 19.65%
[36m[2023-06-25 04:45:15,457][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 04:45:15,458][129146] FPS: 335571.58
[36m[2023-06-25 04:45:20,273][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:45:20,274][129146] Reward + Measures: [[2115.14311923    0.50490797    0.41455701    0.11867733    0.20481934]]
[37m[1m[2023-06-25 04:45:20,274][129146] Max Reward on eval: 2115.1431192261634
[37m[1m[2023-06-25 04:45:20,274][129146] Min Reward on eval: 2115.1431192261634
[37m[1m[2023-06-25 04:45:20,274][129146] Mean Reward across all agents: 2115.1431192261634
[37m[1m[2023-06-25 04:45:20,275][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:45:25,699][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:45:25,705][129146] Reward + Measures: [[ 178.1038583     0.51850003    0.40710002    0.44520003    0.3594    ]
[37m[1m [ 855.64841719    0.41809997    0.35620001    0.22420001    0.1788    ]
[37m[1m [-236.1665535     0.80880004    0.62339991    0.7726        0.60470003]
[37m[1m ...
[37m[1m [2026.23764624    0.41279998    0.45559999    0.12990001    0.33489999]
[37m[1m [-752.1723664     0.90370005    0.65380001    0.89729995    0.66250002]
[37m[1m [-178.34870984    0.32947394    0.22180067    0.2263428     0.15900429]]
[37m[1m[2023-06-25 04:45:25,705][129146] Max Reward on eval: 2279.2586289679516
[37m[1m[2023-06-25 04:45:25,706][129146] Min Reward on eval: -1027.0428117852657
[37m[1m[2023-06-25 04:45:25,706][129146] Mean Reward across all agents: 530.1149377845938
[37m[1m[2023-06-25 04:45:25,706][129146] Average Trajectory Length: 947.0516666666666
[36m[2023-06-25 04:45:25,710][129146] mean_value=-496.92673782615765, max_value=706.9202214759982
[37m[1m[2023-06-25 04:45:25,712][129146] New mean coefficients: [[-0.320176    0.9029761   1.113903    1.082149    0.31645432]]
[37m[1m[2023-06-25 04:45:25,713][129146] Moving the mean solution point...
[36m[2023-06-25 04:45:35,414][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 04:45:35,415][129146] FPS: 395903.26
[36m[2023-06-25 04:45:35,417][129146] itr=394, itrs=2000, Progress: 19.70%
[36m[2023-06-25 04:45:47,110][129146] train() took 11.68 seconds to complete
[36m[2023-06-25 04:45:47,111][129146] FPS: 328843.66
[36m[2023-06-25 04:45:51,851][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:45:51,851][129146] Reward + Measures: [[1986.80865771    0.51741761    0.42199844    0.13401362    0.20360115]]
[37m[1m[2023-06-25 04:45:51,851][129146] Max Reward on eval: 1986.808657708788
[37m[1m[2023-06-25 04:45:51,852][129146] Min Reward on eval: 1986.808657708788
[37m[1m[2023-06-25 04:45:51,852][129146] Mean Reward across all agents: 1986.808657708788
[37m[1m[2023-06-25 04:45:51,852][129146] Average Trajectory Length: 999.7646666666666
[36m[2023-06-25 04:45:57,360][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:45:57,360][129146] Reward + Measures: [[1940.96942667    0.46000004    0.4289        0.1851        0.2316    ]
[37m[1m [1772.39436719    0.41210005    0.43200001    0.1428        0.27700001]
[37m[1m [1349.09306615    0.36803553    0.41537476    0.16607624    0.37776366]
[37m[1m ...
[37m[1m [1929.73041371    0.50100005    0.45730001    0.1847        0.2096    ]
[37m[1m [1068.99128404    0.5844        0.4303        0.22880001    0.2552    ]
[37m[1m [1556.23413706    0.5334        0.39339998    0.15510002    0.2084    ]]
[37m[1m[2023-06-25 04:45:57,360][129146] Max Reward on eval: 2093.993921198603
[37m[1m[2023-06-25 04:45:57,361][129146] Min Reward on eval: -80.01259590476984
[37m[1m[2023-06-25 04:45:57,361][129146] Mean Reward across all agents: 1377.156099878839
[37m[1m[2023-06-25 04:45:57,361][129146] Average Trajectory Length: 998.3923333333333
[36m[2023-06-25 04:45:57,364][129146] mean_value=-362.9906135806994, max_value=704.5276500126126
[37m[1m[2023-06-25 04:45:57,367][129146] New mean coefficients: [[ 0.5446351   1.7674012   1.8546057   2.0029569  -0.21536407]]
[37m[1m[2023-06-25 04:45:57,368][129146] Moving the mean solution point...
[36m[2023-06-25 04:46:07,128][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 04:46:07,128][129146] FPS: 393512.80
[36m[2023-06-25 04:46:07,131][129146] itr=395, itrs=2000, Progress: 19.75%
[36m[2023-06-25 04:46:19,107][129146] train() took 11.96 seconds to complete
[36m[2023-06-25 04:46:19,108][129146] FPS: 321143.74
[36m[2023-06-25 04:46:23,785][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:46:23,785][129146] Reward + Measures: [[1990.94323378    0.51737499    0.43159863    0.14068332    0.20138499]]
[37m[1m[2023-06-25 04:46:23,785][129146] Max Reward on eval: 1990.9432337826654
[37m[1m[2023-06-25 04:46:23,786][129146] Min Reward on eval: 1990.9432337826654
[37m[1m[2023-06-25 04:46:23,786][129146] Mean Reward across all agents: 1990.9432337826654
[37m[1m[2023-06-25 04:46:23,786][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:46:29,196][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:46:29,197][129146] Reward + Measures: [[-299.17964775    0.47499999    0.36289999    0.37330005    0.31110001]
[37m[1m [1704.93299426    0.54080003    0.47690001    0.17200001    0.23280001]
[37m[1m [ 687.64512942    0.47720003    0.44379997    0.2271        0.2326    ]
[37m[1m ...
[37m[1m [-310.82330043    0.55019999    0.37090001    0.38990003    0.32220003]
[37m[1m [ 790.68756207    0.4355        0.41170001    0.19240001    0.27879998]
[37m[1m [1998.8367762     0.40299997    0.40920001    0.12590002    0.21370001]]
[37m[1m[2023-06-25 04:46:29,197][129146] Max Reward on eval: 2066.0345897627994
[37m[1m[2023-06-25 04:46:29,197][129146] Min Reward on eval: -815.3450807831832
[37m[1m[2023-06-25 04:46:29,197][129146] Mean Reward across all agents: 700.3967934892381
[37m[1m[2023-06-25 04:46:29,198][129146] Average Trajectory Length: 986.9119999999999
[36m[2023-06-25 04:46:29,200][129146] mean_value=-766.6331257731928, max_value=1351.4338089930461
[37m[1m[2023-06-25 04:46:29,202][129146] New mean coefficients: [[-0.27629524  1.5616305   1.7870475   1.6201415   0.18873125]]
[37m[1m[2023-06-25 04:46:29,203][129146] Moving the mean solution point...
[36m[2023-06-25 04:46:39,094][129146] train() took 9.89 seconds to complete
[36m[2023-06-25 04:46:39,094][129146] FPS: 388318.29
[36m[2023-06-25 04:46:39,096][129146] itr=396, itrs=2000, Progress: 19.80%
[36m[2023-06-25 04:46:51,147][129146] train() took 12.03 seconds to complete
[36m[2023-06-25 04:46:51,147][129146] FPS: 319162.45
[36m[2023-06-25 04:46:55,875][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:46:55,876][129146] Reward + Measures: [[1936.70069194    0.5375827     0.43667272    0.15131332    0.2034014 ]]
[37m[1m[2023-06-25 04:46:55,876][129146] Max Reward on eval: 1936.7006919406385
[37m[1m[2023-06-25 04:46:55,876][129146] Min Reward on eval: 1936.7006919406385
[37m[1m[2023-06-25 04:46:55,877][129146] Mean Reward across all agents: 1936.7006919406385
[37m[1m[2023-06-25 04:46:55,877][129146] Average Trajectory Length: 999.8553333333333
[36m[2023-06-25 04:47:01,459][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:47:01,459][129146] Reward + Measures: [[1188.15355815    0.3705        0.32210001    0.19149999    0.21200001]
[37m[1m [1273.84810651    0.57819998    0.51179999    0.1622        0.29099998]
[37m[1m [1603.84793791    0.55989999    0.44720003    0.1885        0.2247    ]
[37m[1m ...
[37m[1m [ 306.66579305    0.54369998    0.58540004    0.36320001    0.58059996]
[37m[1m [ 144.99019075    0.25209999    0.18809998    0.16720001    0.18130001]
[37m[1m [1620.60912885    0.54869998    0.46870002    0.134         0.2098    ]]
[37m[1m[2023-06-25 04:47:01,460][129146] Max Reward on eval: 2041.9285851656691
[37m[1m[2023-06-25 04:47:01,460][129146] Min Reward on eval: -836.3645717877749
[37m[1m[2023-06-25 04:47:01,460][129146] Mean Reward across all agents: 583.9535369254316
[37m[1m[2023-06-25 04:47:01,460][129146] Average Trajectory Length: 976.1323333333333
[36m[2023-06-25 04:47:01,465][129146] mean_value=-516.2698000835652, max_value=842.4018758552397
[37m[1m[2023-06-25 04:47:01,468][129146] New mean coefficients: [[ 0.22836459  1.7513452   2.141771    1.6327659  -0.95029217]]
[37m[1m[2023-06-25 04:47:01,469][129146] Moving the mean solution point...
[36m[2023-06-25 04:47:11,141][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 04:47:11,142][129146] FPS: 397057.92
[36m[2023-06-25 04:47:11,144][129146] itr=397, itrs=2000, Progress: 19.85%
[36m[2023-06-25 04:47:22,558][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 04:47:22,559][129146] FPS: 336985.28
[36m[2023-06-25 04:47:27,356][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:47:27,357][129146] Reward + Measures: [[1963.18840284    0.55053371    0.448594      0.16149166    0.20426531]]
[37m[1m[2023-06-25 04:47:27,357][129146] Max Reward on eval: 1963.188402835968
[37m[1m[2023-06-25 04:47:27,357][129146] Min Reward on eval: 1963.188402835968
[37m[1m[2023-06-25 04:47:27,357][129146] Mean Reward across all agents: 1963.188402835968
[37m[1m[2023-06-25 04:47:27,358][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:47:32,880][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:47:32,880][129146] Reward + Measures: [[ 327.69813176    0.32290003    0.35549998    0.11790001    0.31750003]
[37m[1m [ 893.44472437    0.51440001    0.52219999    0.2309        0.32910001]
[37m[1m [1679.0747872     0.54860002    0.48549995    0.1876        0.22119999]
[37m[1m ...
[37m[1m [ 164.5356033     0.34290001    0.32549998    0.1534        0.28730002]
[37m[1m [1433.20433927    0.47279999    0.40080005    0.20630001    0.25410002]
[37m[1m [ 574.13103148    0.39439997    0.55459994    0.2184        0.4104    ]]
[37m[1m[2023-06-25 04:47:32,880][129146] Max Reward on eval: 2004.8386350660585
[37m[1m[2023-06-25 04:47:32,881][129146] Min Reward on eval: -571.4950842063874
[37m[1m[2023-06-25 04:47:32,881][129146] Mean Reward across all agents: 780.4359097723395
[37m[1m[2023-06-25 04:47:32,881][129146] Average Trajectory Length: 984.536
[36m[2023-06-25 04:47:32,884][129146] mean_value=-625.4860771661974, max_value=1304.1277044337696
[37m[1m[2023-06-25 04:47:32,887][129146] New mean coefficients: [[ 1.0261296   2.4576507   2.5861044   0.60759413 -1.1955938 ]]
[37m[1m[2023-06-25 04:47:32,888][129146] Moving the mean solution point...
[36m[2023-06-25 04:47:42,703][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 04:47:42,703][129146] FPS: 391299.76
[36m[2023-06-25 04:47:42,705][129146] itr=398, itrs=2000, Progress: 19.90%
[36m[2023-06-25 04:47:54,269][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 04:47:54,269][129146] FPS: 332578.28
[36m[2023-06-25 04:47:59,115][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:47:59,115][129146] Reward + Measures: [[2049.44642147    0.55797535    0.45677766    0.15301067    0.20481066]]
[37m[1m[2023-06-25 04:47:59,116][129146] Max Reward on eval: 2049.4464214728714
[37m[1m[2023-06-25 04:47:59,116][129146] Min Reward on eval: 2049.4464214728714
[37m[1m[2023-06-25 04:47:59,116][129146] Mean Reward across all agents: 2049.4464214728714
[37m[1m[2023-06-25 04:47:59,116][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:48:04,630][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:48:04,636][129146] Reward + Measures: [[1460.66863877    0.55660003    0.50390005    0.27859998    0.25580001]
[37m[1m [ 519.68795623    0.55470002    0.56670004    0.0368        0.64289999]
[37m[1m [1039.63107657    0.53660005    0.4269        0.22839999    0.22880001]
[37m[1m ...
[37m[1m [ 470.35338928    0.4817        0.37490001    0.26980001    0.33020002]
[37m[1m [ 521.66608608    0.50330001    0.35700002    0.31239998    0.22989999]
[37m[1m [ 744.49753186    0.45969996    0.50730002    0.23369999    0.37820002]]
[37m[1m[2023-06-25 04:48:04,636][129146] Max Reward on eval: 1967.3916958414018
[37m[1m[2023-06-25 04:48:04,637][129146] Min Reward on eval: -647.033533427371
[37m[1m[2023-06-25 04:48:04,637][129146] Mean Reward across all agents: 824.046375081224
[37m[1m[2023-06-25 04:48:04,637][129146] Average Trajectory Length: 991.7956666666666
[36m[2023-06-25 04:48:04,641][129146] mean_value=-391.9595679592896, max_value=925.8480190069425
[37m[1m[2023-06-25 04:48:04,644][129146] New mean coefficients: [[ 0.8414609  2.7760653  2.28266    0.4644742 -1.8241482]]
[37m[1m[2023-06-25 04:48:04,645][129146] Moving the mean solution point...
[36m[2023-06-25 04:48:14,330][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 04:48:14,330][129146] FPS: 396552.36
[36m[2023-06-25 04:48:14,332][129146] itr=399, itrs=2000, Progress: 19.95%
[36m[2023-06-25 04:48:25,835][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 04:48:25,835][129146] FPS: 334387.24
[36m[2023-06-25 04:48:30,572][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:48:30,578][129146] Reward + Measures: [[2163.80864427    0.56469834    0.47342467    0.14379667    0.20701033]]
[37m[1m[2023-06-25 04:48:30,578][129146] Max Reward on eval: 2163.808644266552
[37m[1m[2023-06-25 04:48:30,578][129146] Min Reward on eval: 2163.808644266552
[37m[1m[2023-06-25 04:48:30,579][129146] Mean Reward across all agents: 2163.808644266552
[37m[1m[2023-06-25 04:48:30,579][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:48:36,038][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:48:36,039][129146] Reward + Measures: [[  94.02949619    0.82410002    0.64219999    0.75169998    0.727     ]
[37m[1m [-114.99893377    0.59829998    0.37570003    0.4869        0.5086    ]
[37m[1m [ 210.68556204    0.83010006    0.62269998    0.75510001    0.68650001]
[37m[1m ...
[37m[1m [ 337.5419588     0.80839998    0.53870004    0.74369997    0.6207    ]
[37m[1m [  57.33319664    0.84680003    0.58669996    0.78610003    0.74990004]
[37m[1m [ 192.79060269    0.72389996    0.48090002    0.61109996    0.61179996]]
[37m[1m[2023-06-25 04:48:36,039][129146] Max Reward on eval: 2226.7210130533667
[37m[1m[2023-06-25 04:48:36,039][129146] Min Reward on eval: -555.4982639266061
[37m[1m[2023-06-25 04:48:36,039][129146] Mean Reward across all agents: 695.8104256713805
[37m[1m[2023-06-25 04:48:36,040][129146] Average Trajectory Length: 996.2916666666666
[36m[2023-06-25 04:48:36,050][129146] mean_value=245.19268743238092, max_value=1741.5155608962277
[37m[1m[2023-06-25 04:48:36,053][129146] New mean coefficients: [[ 1.109353    2.8151135   1.8611155   0.40131512 -1.9676139 ]]
[37m[1m[2023-06-25 04:48:36,054][129146] Moving the mean solution point...
[36m[2023-06-25 04:48:45,770][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 04:48:45,770][129146] FPS: 395300.37
[36m[2023-06-25 04:48:45,772][129146] itr=400, itrs=2000, Progress: 20.00%
[37m[1m[2023-06-25 04:48:49,758][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000380
[36m[2023-06-25 04:49:01,522][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 04:49:01,523][129146] FPS: 335592.39
[36m[2023-06-25 04:49:06,276][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:49:06,276][129146] Reward + Measures: [[2261.47128422    0.56700164    0.48146963    0.13561334    0.20831734]]
[37m[1m[2023-06-25 04:49:06,276][129146] Max Reward on eval: 2261.4712842155423
[37m[1m[2023-06-25 04:49:06,277][129146] Min Reward on eval: 2261.4712842155423
[37m[1m[2023-06-25 04:49:06,277][129146] Mean Reward across all agents: 2261.4712842155423
[37m[1m[2023-06-25 04:49:06,277][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:49:11,638][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:49:11,638][129146] Reward + Measures: [[2061.91793797    0.57419997    0.47170001    0.16080001    0.21689999]
[37m[1m [2006.13244364    0.61199999    0.46230003    0.1653        0.21190003]
[37m[1m [1947.95767522    0.60930002    0.42940003    0.17220001    0.20469999]
[37m[1m ...
[37m[1m [2166.92387296    0.61300004    0.49359998    0.14670001    0.20710002]
[37m[1m [2139.52967237    0.56440002    0.46169996    0.15350001    0.2103    ]
[37m[1m [1613.13693273    0.62830001    0.38620001    0.20209999    0.23540001]]
[37m[1m[2023-06-25 04:49:11,639][129146] Max Reward on eval: 2305.7917014091277
[37m[1m[2023-06-25 04:49:11,639][129146] Min Reward on eval: 1598.759711427847
[37m[1m[2023-06-25 04:49:11,639][129146] Mean Reward across all agents: 2005.2617135506134
[37m[1m[2023-06-25 04:49:11,639][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:49:11,642][129146] mean_value=-39.70061957791893, max_value=318.64579413194315
[37m[1m[2023-06-25 04:49:11,645][129146] New mean coefficients: [[-0.11883807  0.85219514  1.9553742   0.4336026  -1.5198945 ]]
[37m[1m[2023-06-25 04:49:11,646][129146] Moving the mean solution point...
[36m[2023-06-25 04:49:21,338][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 04:49:21,338][129146] FPS: 396246.78
[36m[2023-06-25 04:49:21,341][129146] itr=401, itrs=2000, Progress: 20.05%
[36m[2023-06-25 04:49:32,783][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 04:49:32,784][129146] FPS: 336055.30
[36m[2023-06-25 04:49:37,522][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:49:37,523][129146] Reward + Measures: [[2240.74284796    0.57755202    0.48738068    0.144867      0.20822001]]
[37m[1m[2023-06-25 04:49:37,523][129146] Max Reward on eval: 2240.7428479558457
[37m[1m[2023-06-25 04:49:37,523][129146] Min Reward on eval: 2240.7428479558457
[37m[1m[2023-06-25 04:49:37,524][129146] Mean Reward across all agents: 2240.7428479558457
[37m[1m[2023-06-25 04:49:37,524][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:49:42,921][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:49:42,922][129146] Reward + Measures: [[-245.70925157    0.30820858    0.32618365    0.17093991    0.24376522]
[37m[1m [ 952.67478948    0.60899997    0.35479999    0.37339997    0.20970002]
[37m[1m [1360.20370491    0.56350005    0.43859997    0.20829999    0.31790003]
[37m[1m ...
[37m[1m [ 829.2344868     0.39010003    0.39340001    0.17660001    0.23740001]
[37m[1m [1181.38061994    0.40570003    0.41840002    0.1849        0.20550001]
[37m[1m [1660.03309178    0.53630006    0.45549998    0.25029999    0.20180002]]
[37m[1m[2023-06-25 04:49:42,922][129146] Max Reward on eval: 2233.4145801074105
[37m[1m[2023-06-25 04:49:42,922][129146] Min Reward on eval: -554.1881793918786
[37m[1m[2023-06-25 04:49:42,923][129146] Mean Reward across all agents: 1186.165116815161
[37m[1m[2023-06-25 04:49:42,923][129146] Average Trajectory Length: 996.8693333333333
[36m[2023-06-25 04:49:42,926][129146] mean_value=-547.4398586887093, max_value=1525.5836323317853
[37m[1m[2023-06-25 04:49:42,929][129146] New mean coefficients: [[ 0.13618505  0.5489102   2.3915486  -0.21663001 -1.8595167 ]]
[37m[1m[2023-06-25 04:49:42,930][129146] Moving the mean solution point...
[36m[2023-06-25 04:49:52,613][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 04:49:52,614][129146] FPS: 396605.59
[36m[2023-06-25 04:49:52,616][129146] itr=402, itrs=2000, Progress: 20.10%
[36m[2023-06-25 04:50:04,094][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 04:50:04,094][129146] FPS: 335026.07
[36m[2023-06-25 04:50:08,946][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:50:08,946][129146] Reward + Measures: [[2268.09580229    0.56898534    0.51041931    0.14736333    0.20739998]]
[37m[1m[2023-06-25 04:50:08,947][129146] Max Reward on eval: 2268.095802294123
[37m[1m[2023-06-25 04:50:08,947][129146] Min Reward on eval: 2268.095802294123
[37m[1m[2023-06-25 04:50:08,947][129146] Mean Reward across all agents: 2268.095802294123
[37m[1m[2023-06-25 04:50:08,947][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:50:14,449][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:50:14,450][129146] Reward + Measures: [[1076.09265971    0.5219        0.5334        0.1926        0.32000002]
[37m[1m [ 146.91392619    0.55719995    0.34260002    0.45809999    0.287     ]
[37m[1m [ 768.66885191    0.41949996    0.54319996    0.14690001    0.47820002]
[37m[1m ...
[37m[1m [1964.45737688    0.55509996    0.45720002    0.14670001    0.25939998]
[37m[1m [1345.11338435    0.4219        0.35029998    0.1629        0.19700001]
[37m[1m [ 750.55982068    0.44520003    0.35090002    0.22620001    0.19749999]]
[37m[1m[2023-06-25 04:50:14,450][129146] Max Reward on eval: 2151.3542337238323
[37m[1m[2023-06-25 04:50:14,451][129146] Min Reward on eval: -472.42500759689136
[37m[1m[2023-06-25 04:50:14,451][129146] Mean Reward across all agents: 925.9345638188751
[37m[1m[2023-06-25 04:50:14,451][129146] Average Trajectory Length: 992.7126666666667
[36m[2023-06-25 04:50:14,454][129146] mean_value=-471.89467059989914, max_value=997.0893233320389
[37m[1m[2023-06-25 04:50:14,457][129146] New mean coefficients: [[ 0.24425957  0.8142039   2.7756321   0.34605    -1.3710295 ]]
[37m[1m[2023-06-25 04:50:14,458][129146] Moving the mean solution point...
[36m[2023-06-25 04:50:24,407][129146] train() took 9.95 seconds to complete
[36m[2023-06-25 04:50:24,407][129146] FPS: 386027.99
[36m[2023-06-25 04:50:24,409][129146] itr=403, itrs=2000, Progress: 20.15%
[36m[2023-06-25 04:50:36,119][129146] train() took 11.69 seconds to complete
[36m[2023-06-25 04:50:36,119][129146] FPS: 328433.40
[36m[2023-06-25 04:50:40,920][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:50:40,920][129146] Reward + Measures: [[2283.98053801    0.56071097    0.53402835    0.14861399    0.20595701]]
[37m[1m[2023-06-25 04:50:40,920][129146] Max Reward on eval: 2283.980538009019
[37m[1m[2023-06-25 04:50:40,920][129146] Min Reward on eval: 2283.980538009019
[37m[1m[2023-06-25 04:50:40,921][129146] Mean Reward across all agents: 2283.980538009019
[37m[1m[2023-06-25 04:50:40,921][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:50:46,560][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:50:46,560][129146] Reward + Measures: [[1183.5417141     0.38369998    0.35899997    0.2023        0.28040001]
[37m[1m [1924.65269616    0.54949999    0.491         0.1842        0.18800001]
[37m[1m [ 844.00165804    0.53969997    0.49370003    0.1648        0.27069998]
[37m[1m ...
[37m[1m [1252.69094234    0.59329998    0.52410001    0.22660001    0.20489998]
[37m[1m [2092.13535328    0.46260005    0.44650003    0.17389999    0.2106    ]
[37m[1m [ -14.98934508    0.61555737    0.33615327    0.34689966    0.48889905]]
[37m[1m[2023-06-25 04:50:46,560][129146] Max Reward on eval: 2122.6969800847583
[37m[1m[2023-06-25 04:50:46,561][129146] Min Reward on eval: -971.1790400009835
[37m[1m[2023-06-25 04:50:46,561][129146] Mean Reward across all agents: 1080.0253636962327
[37m[1m[2023-06-25 04:50:46,561][129146] Average Trajectory Length: 996.1093333333333
[36m[2023-06-25 04:50:46,565][129146] mean_value=-378.8795294006608, max_value=2229.73192032763
[37m[1m[2023-06-25 04:50:46,568][129146] New mean coefficients: [[ 0.7303721  1.2332807  3.227381   0.7707535 -0.8051519]]
[37m[1m[2023-06-25 04:50:46,569][129146] Moving the mean solution point...
[36m[2023-06-25 04:50:56,418][129146] train() took 9.85 seconds to complete
[36m[2023-06-25 04:50:56,419][129146] FPS: 389946.28
[36m[2023-06-25 04:50:56,421][129146] itr=404, itrs=2000, Progress: 20.20%
[36m[2023-06-25 04:51:08,003][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 04:51:08,003][129146] FPS: 332022.80
[36m[2023-06-25 04:51:12,824][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:51:12,825][129146] Reward + Measures: [[2365.42727265    0.55606633    0.54781437    0.14314267    0.20942765]]
[37m[1m[2023-06-25 04:51:12,825][129146] Max Reward on eval: 2365.4272726539853
[37m[1m[2023-06-25 04:51:12,825][129146] Min Reward on eval: 2365.4272726539853
[37m[1m[2023-06-25 04:51:12,825][129146] Mean Reward across all agents: 2365.4272726539853
[37m[1m[2023-06-25 04:51:12,825][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:51:18,258][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:51:18,258][129146] Reward + Measures: [[ 539.40924099    0.41820002    0.3845        0.16950001    0.35170004]
[37m[1m [2329.6030851     0.52069998    0.5449        0.10600001    0.22480002]
[37m[1m [ 608.80234838    0.55330002    0.27579999    0.249         0.2572    ]
[37m[1m ...
[37m[1m [2163.0458334     0.49200001    0.52500004    0.0788        0.24379997]
[37m[1m [1299.58320989    0.41690001    0.53250003    0.1201        0.24629998]
[37m[1m [1587.60471632    0.45760003    0.50650001    0.0786        0.30540001]]
[37m[1m[2023-06-25 04:51:18,259][129146] Max Reward on eval: 2329.6030850985553
[37m[1m[2023-06-25 04:51:18,259][129146] Min Reward on eval: -9.312693874537946
[37m[1m[2023-06-25 04:51:18,259][129146] Mean Reward across all agents: 1433.1445915713875
[37m[1m[2023-06-25 04:51:18,259][129146] Average Trajectory Length: 998.5836666666667
[36m[2023-06-25 04:51:18,263][129146] mean_value=-307.01903076353824, max_value=1584.699723325832
[37m[1m[2023-06-25 04:51:18,266][129146] New mean coefficients: [[ 0.56842387  0.96949     2.7980843   0.4751756  -0.5045663 ]]
[37m[1m[2023-06-25 04:51:18,267][129146] Moving the mean solution point...
[36m[2023-06-25 04:51:27,882][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 04:51:27,882][129146] FPS: 399441.88
[36m[2023-06-25 04:51:27,884][129146] itr=405, itrs=2000, Progress: 20.25%
[36m[2023-06-25 04:51:39,331][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 04:51:39,331][129146] FPS: 336054.77
[36m[2023-06-25 04:51:44,063][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:51:44,064][129146] Reward + Measures: [[2409.53553241    0.56098902    0.56269699    0.14336768    0.21169233]]
[37m[1m[2023-06-25 04:51:44,064][129146] Max Reward on eval: 2409.5355324096768
[37m[1m[2023-06-25 04:51:44,064][129146] Min Reward on eval: 2409.5355324096768
[37m[1m[2023-06-25 04:51:44,065][129146] Mean Reward across all agents: 2409.5355324096768
[37m[1m[2023-06-25 04:51:44,065][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:51:49,511][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:51:49,512][129146] Reward + Measures: [[ 397.41218838    0.69530004    0.59990001    0.59569997    0.56210005]
[37m[1m [1009.23775288    0.57319999    0.45940003    0.2263        0.34720001]
[37m[1m [ 308.46693714    0.70660001    0.1652        0.62650007    0.48070002]
[37m[1m ...
[37m[1m [ 300.9949677     0.3845        0.26939997    0.2595        0.1311    ]
[37m[1m [1925.82296205    0.56150001    0.55129999    0.20249999    0.24270001]
[37m[1m [2137.84319876    0.52759999    0.54610008    0.14790002    0.2138    ]]
[37m[1m[2023-06-25 04:51:49,512][129146] Max Reward on eval: 2476.9969788373915
[37m[1m[2023-06-25 04:51:49,512][129146] Min Reward on eval: -178.6847242039512
[37m[1m[2023-06-25 04:51:49,512][129146] Mean Reward across all agents: 1136.2880539541634
[37m[1m[2023-06-25 04:51:49,513][129146] Average Trajectory Length: 989.418
[36m[2023-06-25 04:51:49,518][129146] mean_value=-172.32177909943502, max_value=1389.988887411405
[37m[1m[2023-06-25 04:51:49,521][129146] New mean coefficients: [[ 0.33713835  0.95859843  2.9609022   0.40998966 -0.8903941 ]]
[37m[1m[2023-06-25 04:51:49,522][129146] Moving the mean solution point...
[36m[2023-06-25 04:51:59,278][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 04:51:59,278][129146] FPS: 393661.40
[36m[2023-06-25 04:51:59,281][129146] itr=406, itrs=2000, Progress: 20.30%
[36m[2023-06-25 04:52:10,720][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 04:52:10,720][129146] FPS: 336247.90
[36m[2023-06-25 04:52:15,457][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:52:15,458][129146] Reward + Measures: [[2527.07090985    0.55521995    0.58266598    0.12913565    0.217061  ]]
[37m[1m[2023-06-25 04:52:15,458][129146] Max Reward on eval: 2527.070909849856
[37m[1m[2023-06-25 04:52:15,458][129146] Min Reward on eval: 2527.070909849856
[37m[1m[2023-06-25 04:52:15,458][129146] Mean Reward across all agents: 2527.070909849856
[37m[1m[2023-06-25 04:52:15,458][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:52:20,987][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:52:20,987][129146] Reward + Measures: [[1376.97448488    0.36160001    0.49050003    0.0976        0.34889999]
[37m[1m [1281.98300072    0.4278        0.33419999    0.22430001    0.17460001]
[37m[1m [ 773.05361278    0.55470002    0.39450002    0.28420001    0.26690003]
[37m[1m ...
[37m[1m [2184.99582818    0.45030004    0.49720001    0.0619        0.2289    ]
[37m[1m [1305.89455291    0.38540003    0.43130001    0.15360001    0.2552    ]
[37m[1m [1190.6792728     0.63999999    0.46140003    0.28669998    0.24260001]]
[37m[1m[2023-06-25 04:52:20,988][129146] Max Reward on eval: 2420.2869663605697
[37m[1m[2023-06-25 04:52:20,988][129146] Min Reward on eval: -441.6099053009413
[37m[1m[2023-06-25 04:52:20,988][129146] Mean Reward across all agents: 1044.4468573157055
[37m[1m[2023-06-25 04:52:20,988][129146] Average Trajectory Length: 995.9599999999999
[36m[2023-06-25 04:52:20,991][129146] mean_value=-624.5877953428335, max_value=913.0854430839672
[37m[1m[2023-06-25 04:52:20,994][129146] New mean coefficients: [[ 0.00793159  0.38733357  2.859784    0.46082574 -0.10767323]]
[37m[1m[2023-06-25 04:52:20,995][129146] Moving the mean solution point...
[36m[2023-06-25 04:52:30,717][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 04:52:30,717][129146] FPS: 395043.26
[36m[2023-06-25 04:52:30,720][129146] itr=407, itrs=2000, Progress: 20.35%
[36m[2023-06-25 04:52:42,134][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 04:52:42,134][129146] FPS: 336973.94
[36m[2023-06-25 04:52:46,983][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:52:46,983][129146] Reward + Measures: [[2537.96020261    0.54391265    0.608504      0.1295        0.21778066]]
[37m[1m[2023-06-25 04:52:46,983][129146] Max Reward on eval: 2537.9602026134094
[37m[1m[2023-06-25 04:52:46,984][129146] Min Reward on eval: 2537.9602026134094
[37m[1m[2023-06-25 04:52:46,984][129146] Mean Reward across all agents: 2537.9602026134094
[37m[1m[2023-06-25 04:52:46,984][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:52:52,582][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:52:52,583][129146] Reward + Measures: [[-286.48209262    0.3351        0.26809999    0.17290001    0.17800002]
[37m[1m [1189.79568159    0.40670004    0.48449999    0.24270001    0.28210002]
[37m[1m [1058.4561905     0.33309999    0.42850003    0.2516        0.3251    ]
[37m[1m ...
[37m[1m [1314.09761656    0.60649997    0.57499999    0.23959999    0.1935    ]
[37m[1m [ 608.40748787    0.50440001    0.46440002    0.2483        0.2218    ]
[37m[1m [ 427.79662636    0.368         0.35769999    0.23050001    0.34219998]]
[37m[1m[2023-06-25 04:52:52,583][129146] Max Reward on eval: 2537.535065658437
[37m[1m[2023-06-25 04:52:52,583][129146] Min Reward on eval: -304.85518307645395
[37m[1m[2023-06-25 04:52:52,583][129146] Mean Reward across all agents: 1190.6919901628091
[37m[1m[2023-06-25 04:52:52,584][129146] Average Trajectory Length: 997.9486666666667
[36m[2023-06-25 04:52:52,588][129146] mean_value=-304.63003640317874, max_value=2127.07048579636
[37m[1m[2023-06-25 04:52:52,590][129146] New mean coefficients: [[0.28063017 0.5178459  2.6424592  0.22123775 0.05632854]]
[37m[1m[2023-06-25 04:52:52,591][129146] Moving the mean solution point...
[36m[2023-06-25 04:53:02,291][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 04:53:02,291][129146] FPS: 395953.22
[36m[2023-06-25 04:53:02,293][129146] itr=408, itrs=2000, Progress: 20.40%
[36m[2023-06-25 04:53:13,744][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 04:53:13,745][129146] FPS: 335815.83
[36m[2023-06-25 04:53:18,578][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:53:18,579][129146] Reward + Measures: [[2614.09507657    0.53201866    0.62319368    0.11889099    0.21714701]]
[37m[1m[2023-06-25 04:53:18,579][129146] Max Reward on eval: 2614.0950765684975
[37m[1m[2023-06-25 04:53:18,579][129146] Min Reward on eval: 2614.0950765684975
[37m[1m[2023-06-25 04:53:18,580][129146] Mean Reward across all agents: 2614.0950765684975
[37m[1m[2023-06-25 04:53:18,580][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:53:24,124][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:53:24,125][129146] Reward + Measures: [[1343.10869572    0.44000003    0.5801        0.10600001    0.4355    ]
[37m[1m [2200.61408524    0.50349998    0.51810002    0.09589999    0.22600003]
[37m[1m [1427.46562279    0.4481        0.56720001    0.1371        0.34480003]
[37m[1m ...
[37m[1m [2440.99695979    0.4666        0.54430002    0.0945        0.2234    ]
[37m[1m [1611.77185921    0.43010002    0.63280004    0.0943        0.35930002]
[37m[1m [1002.83598574    0.56260002    0.51890004    0.2366        0.1596    ]]
[37m[1m[2023-06-25 04:53:24,125][129146] Max Reward on eval: 2652.2734431484714
[37m[1m[2023-06-25 04:53:24,125][129146] Min Reward on eval: -67.94024029173889
[37m[1m[2023-06-25 04:53:24,126][129146] Mean Reward across all agents: 1203.1326521255419
[37m[1m[2023-06-25 04:53:24,126][129146] Average Trajectory Length: 993.6603333333333
[36m[2023-06-25 04:53:24,130][129146] mean_value=-329.5887019318697, max_value=2789.2477701837197
[37m[1m[2023-06-25 04:53:24,132][129146] New mean coefficients: [[0.40519112 0.23216859 2.8720915  0.5172421  0.03296427]]
[37m[1m[2023-06-25 04:53:24,133][129146] Moving the mean solution point...
[36m[2023-06-25 04:53:33,832][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 04:53:33,832][129146] FPS: 395995.16
[36m[2023-06-25 04:53:33,835][129146] itr=409, itrs=2000, Progress: 20.45%
[36m[2023-06-25 04:53:45,390][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 04:53:45,391][129146] FPS: 332771.25
[36m[2023-06-25 04:53:50,198][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:53:50,199][129146] Reward + Measures: [[2638.81592945    0.52285635    0.64439332    0.11186232    0.220273  ]]
[37m[1m[2023-06-25 04:53:50,199][129146] Max Reward on eval: 2638.8159294545208
[37m[1m[2023-06-25 04:53:50,199][129146] Min Reward on eval: 2638.8159294545208
[37m[1m[2023-06-25 04:53:50,199][129146] Mean Reward across all agents: 2638.8159294545208
[37m[1m[2023-06-25 04:53:50,200][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:53:55,666][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:53:55,667][129146] Reward + Measures: [[ 249.14738275    0.69710004    0.39200002    0.5959        0.40349999]
[37m[1m [1586.22828676    0.43520004    0.41169998    0.24519999    0.2263    ]
[37m[1m [1672.8672173     0.42010003    0.36489996    0.1698        0.25      ]
[37m[1m ...
[37m[1m [1991.51219107    0.5722        0.59640008    0.1051        0.2563    ]
[37m[1m [2180.44433455    0.48480001    0.58810002    0.18950002    0.21970001]
[37m[1m [1492.50253237    0.52580005    0.42230001    0.16860001    0.25640002]]
[37m[1m[2023-06-25 04:53:55,667][129146] Max Reward on eval: 2595.2121150184657
[37m[1m[2023-06-25 04:53:55,667][129146] Min Reward on eval: -190.06920246593072
[37m[1m[2023-06-25 04:53:55,668][129146] Mean Reward across all agents: 1335.829323359954
[37m[1m[2023-06-25 04:53:55,668][129146] Average Trajectory Length: 997.4856666666666
[36m[2023-06-25 04:53:55,673][129146] mean_value=47.07873142179761, max_value=3095.2121150184657
[37m[1m[2023-06-25 04:53:55,676][129146] New mean coefficients: [[0.11342347 0.27174842 2.9370244  0.01740891 0.27470562]]
[37m[1m[2023-06-25 04:53:55,677][129146] Moving the mean solution point...
[36m[2023-06-25 04:54:05,327][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 04:54:05,327][129146] FPS: 398000.15
[36m[2023-06-25 04:54:05,329][129146] itr=410, itrs=2000, Progress: 20.50%
[37m[1m[2023-06-25 04:54:09,256][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000390
[36m[2023-06-25 04:54:21,310][129146] train() took 11.72 seconds to complete
[36m[2023-06-25 04:54:21,310][129146] FPS: 327663.35
[36m[2023-06-25 04:54:26,138][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:54:26,138][129146] Reward + Measures: [[2650.74507964    0.5155127     0.67218101    0.11292066    0.22104968]]
[37m[1m[2023-06-25 04:54:26,139][129146] Max Reward on eval: 2650.745079641587
[37m[1m[2023-06-25 04:54:26,139][129146] Min Reward on eval: 2650.745079641587
[37m[1m[2023-06-25 04:54:26,139][129146] Mean Reward across all agents: 2650.745079641587
[37m[1m[2023-06-25 04:54:26,139][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:54:31,771][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:54:31,772][129146] Reward + Measures: [[1540.94572479    0.52469999    0.4526        0.29350001    0.25569996]
[37m[1m [1822.04659948    0.48639998    0.65039998    0.0844        0.4578    ]
[37m[1m [1988.39432328    0.46539998    0.50910002    0.20390001    0.2089    ]
[37m[1m ...
[37m[1m [1358.56452748    0.53560001    0.44070002    0.33689997    0.33540002]
[37m[1m [ 443.64212147    0.39030001    0.4842        0.19140001    0.45360002]
[37m[1m [ 743.31738101    0.4131        0.72310001    0.1236        0.77209997]]
[37m[1m[2023-06-25 04:54:31,772][129146] Max Reward on eval: 2574.8392728562467
[37m[1m[2023-06-25 04:54:31,772][129146] Min Reward on eval: -342.1089689769433
[37m[1m[2023-06-25 04:54:31,773][129146] Mean Reward across all agents: 1114.3588194207064
[37m[1m[2023-06-25 04:54:31,773][129146] Average Trajectory Length: 998.6303333333333
[36m[2023-06-25 04:54:31,781][129146] mean_value=-18.268733222144462, max_value=1904.5452878651063
[37m[1m[2023-06-25 04:54:31,784][129146] New mean coefficients: [[0.09002788 0.07044482 2.30682    0.47811607 0.5071471 ]]
[37m[1m[2023-06-25 04:54:31,785][129146] Moving the mean solution point...
[36m[2023-06-25 04:54:41,677][129146] train() took 9.89 seconds to complete
[36m[2023-06-25 04:54:41,677][129146] FPS: 388280.97
[36m[2023-06-25 04:54:41,679][129146] itr=411, itrs=2000, Progress: 20.55%
[36m[2023-06-25 04:54:53,393][129146] train() took 11.69 seconds to complete
[36m[2023-06-25 04:54:53,394][129146] FPS: 328353.34
[36m[2023-06-25 04:54:58,154][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:54:58,154][129146] Reward + Measures: [[2643.73610386    0.52719831    0.69370562    0.10792834    0.22543301]]
[37m[1m[2023-06-25 04:54:58,154][129146] Max Reward on eval: 2643.736103857191
[37m[1m[2023-06-25 04:54:58,155][129146] Min Reward on eval: 2643.736103857191
[37m[1m[2023-06-25 04:54:58,155][129146] Mean Reward across all agents: 2643.736103857191
[37m[1m[2023-06-25 04:54:58,155][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:55:03,677][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:55:03,678][129146] Reward + Measures: [[1328.66763393    0.52770007    0.62840003    0.25659999    0.25869998]
[37m[1m [1807.75100952    0.4269        0.48289999    0.13090001    0.2198    ]
[37m[1m [1818.15390197    0.50660002    0.49610001    0.18600002    0.22260001]
[37m[1m ...
[37m[1m [1749.87717776    0.28239998    0.36020002    0.11140001    0.2421    ]
[37m[1m [1266.84782016    0.31620002    0.33739999    0.14300001    0.23080002]
[37m[1m [1740.6714452     0.29850003    0.38830003    0.1266        0.24450003]]
[37m[1m[2023-06-25 04:55:03,678][129146] Max Reward on eval: 2715.4072078170952
[37m[1m[2023-06-25 04:55:03,679][129146] Min Reward on eval: -87.94807590732816
[37m[1m[2023-06-25 04:55:03,679][129146] Mean Reward across all agents: 1650.8403543425627
[37m[1m[2023-06-25 04:55:03,679][129146] Average Trajectory Length: 995.5886666666667
[36m[2023-06-25 04:55:03,684][129146] mean_value=-62.24837339221073, max_value=3138.0896051796853
[37m[1m[2023-06-25 04:55:03,686][129146] New mean coefficients: [[-0.03053521 -0.23338836  2.1799364   0.04154342  0.02187285]]
[37m[1m[2023-06-25 04:55:03,687][129146] Moving the mean solution point...
[36m[2023-06-25 04:55:13,416][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 04:55:13,416][129146] FPS: 394773.40
[36m[2023-06-25 04:55:13,419][129146] itr=412, itrs=2000, Progress: 20.60%
[36m[2023-06-25 04:55:24,966][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 04:55:24,966][129146] FPS: 333106.40
[36m[2023-06-25 04:55:29,808][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:55:29,808][129146] Reward + Measures: [[2570.85134292    0.5232107     0.70858568    0.11190999    0.22752032]]
[37m[1m[2023-06-25 04:55:29,809][129146] Max Reward on eval: 2570.851342924297
[37m[1m[2023-06-25 04:55:29,809][129146] Min Reward on eval: 2570.851342924297
[37m[1m[2023-06-25 04:55:29,809][129146] Mean Reward across all agents: 2570.851342924297
[37m[1m[2023-06-25 04:55:29,809][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:55:35,164][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:55:35,165][129146] Reward + Measures: [[2390.97630006    0.55919999    0.71740001    0.1094        0.22990003]
[37m[1m [1836.30591102    0.41149998    0.49510002    0.1919        0.21780001]
[37m[1m [1909.52063226    0.46510002    0.56020004    0.2155        0.21180001]
[37m[1m ...
[37m[1m [1420.7991513     0.53710002    0.68830001    0.08630001    0.32540002]
[37m[1m [1249.80972419    0.52059996    0.66079998    0.12409999    0.27220002]
[37m[1m [ 425.12695503    0.51520002    0.36100003    0.40089998    0.3441    ]]
[37m[1m[2023-06-25 04:55:35,165][129146] Max Reward on eval: 2603.6860151809406
[37m[1m[2023-06-25 04:55:35,165][129146] Min Reward on eval: -319.86212882627007
[37m[1m[2023-06-25 04:55:35,165][129146] Mean Reward across all agents: 1580.193212671101
[37m[1m[2023-06-25 04:55:35,166][129146] Average Trajectory Length: 993.8896666666666
[36m[2023-06-25 04:55:35,171][129146] mean_value=356.9338569963961, max_value=2890.976300056232
[37m[1m[2023-06-25 04:55:35,174][129146] New mean coefficients: [[-0.17964035 -0.04654807  1.9648176   0.07123534  0.5710753 ]]
[37m[1m[2023-06-25 04:55:35,175][129146] Moving the mean solution point...
[36m[2023-06-25 04:55:44,893][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 04:55:44,894][129146] FPS: 395218.15
[36m[2023-06-25 04:55:44,896][129146] itr=413, itrs=2000, Progress: 20.65%
[36m[2023-06-25 04:55:56,311][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 04:55:56,311][129146] FPS: 336959.33
[36m[2023-06-25 04:56:01,033][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:56:01,033][129146] Reward + Measures: [[2460.1108029     0.54068953    0.71783477    0.13376687    0.22845525]]
[37m[1m[2023-06-25 04:56:01,033][129146] Max Reward on eval: 2460.1108028970993
[37m[1m[2023-06-25 04:56:01,033][129146] Min Reward on eval: 2460.1108028970993
[37m[1m[2023-06-25 04:56:01,034][129146] Mean Reward across all agents: 2460.1108028970993
[37m[1m[2023-06-25 04:56:01,034][129146] Average Trajectory Length: 999.8886666666666
[36m[2023-06-25 04:56:06,473][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:56:06,473][129146] Reward + Measures: [[ 745.55191047    0.57710004    0.73790002    0.0689        0.83970004]
[37m[1m [ 350.02666397    0.48878953    0.34497947    0.28678772    0.30973285]
[37m[1m [1251.38853948    0.45160004    0.43070003    0.15360001    0.19939999]
[37m[1m ...
[37m[1m [1016.78568921    0.34660003    0.30679998    0.1048        0.149     ]
[37m[1m [ 388.95877945    0.47799999    0.57130003    0.0426        0.52360004]
[37m[1m [-215.22590729    0.24453108    0.22574285    0.14139588    0.1876587 ]]
[37m[1m[2023-06-25 04:56:06,473][129146] Max Reward on eval: 2475.2693645358086
[37m[1m[2023-06-25 04:56:06,474][129146] Min Reward on eval: -520.3353253418463
[37m[1m[2023-06-25 04:56:06,474][129146] Mean Reward across all agents: 993.038456984917
[37m[1m[2023-06-25 04:56:06,474][129146] Average Trajectory Length: 997.4846666666666
[36m[2023-06-25 04:56:06,481][129146] mean_value=-130.54363364797615, max_value=2268.0402194309513
[37m[1m[2023-06-25 04:56:06,483][129146] New mean coefficients: [[ 0.25010973  0.17331295  1.8799909  -0.16982263 -0.12702209]]
[37m[1m[2023-06-25 04:56:06,485][129146] Moving the mean solution point...
[36m[2023-06-25 04:56:16,194][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 04:56:16,194][129146] FPS: 395571.14
[36m[2023-06-25 04:56:16,196][129146] itr=414, itrs=2000, Progress: 20.70%
[36m[2023-06-25 04:56:27,637][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 04:56:27,638][129146] FPS: 336139.39
[36m[2023-06-25 04:56:32,535][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:56:32,536][129146] Reward + Measures: [[2533.68988791    0.5215233     0.72991568    0.12032367    0.22685766]]
[37m[1m[2023-06-25 04:56:32,536][129146] Max Reward on eval: 2533.6898879102337
[37m[1m[2023-06-25 04:56:32,536][129146] Min Reward on eval: 2533.6898879102337
[37m[1m[2023-06-25 04:56:32,536][129146] Mean Reward across all agents: 2533.6898879102337
[37m[1m[2023-06-25 04:56:32,537][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:56:38,241][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:56:38,241][129146] Reward + Measures: [[2100.49757178    0.46310002    0.62729996    0.20120001    0.18810003]
[37m[1m [2154.67593958    0.48520002    0.55470002    0.142         0.21970001]
[37m[1m [2209.52539668    0.44479999    0.64050001    0.1797        0.2086    ]
[37m[1m ...
[37m[1m [ 385.6121483     0.63900006    0.3888        0.57690001    0.3612    ]
[37m[1m [ -52.56503262    0.74290001    0.1202        0.72440004    0.65469998]
[37m[1m [ 471.62309649    0.58860004    0.5988        0.3116        0.55479997]]
[37m[1m[2023-06-25 04:56:38,242][129146] Max Reward on eval: 2636.8560007845053
[37m[1m[2023-06-25 04:56:38,242][129146] Min Reward on eval: -467.08101229603636
[37m[1m[2023-06-25 04:56:38,242][129146] Mean Reward across all agents: 1336.5887780817936
[37m[1m[2023-06-25 04:56:38,242][129146] Average Trajectory Length: 998.2916666666666
[36m[2023-06-25 04:56:38,251][129146] mean_value=185.0476551348578, max_value=2642.0405671646818
[37m[1m[2023-06-25 04:56:38,254][129146] New mean coefficients: [[ 0.16447413 -0.265545    1.6449054   0.14262    -0.08472219]]
[37m[1m[2023-06-25 04:56:38,255][129146] Moving the mean solution point...
[36m[2023-06-25 04:56:47,997][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 04:56:47,998][129146] FPS: 394220.19
[36m[2023-06-25 04:56:48,000][129146] itr=415, itrs=2000, Progress: 20.75%
[36m[2023-06-25 04:56:59,607][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 04:56:59,608][129146] FPS: 331326.26
[36m[2023-06-25 04:57:04,405][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:57:04,405][129146] Reward + Measures: [[2541.45200235    0.51180267    0.74349236    0.11735566    0.22773966]]
[37m[1m[2023-06-25 04:57:04,406][129146] Max Reward on eval: 2541.4520023481023
[37m[1m[2023-06-25 04:57:04,406][129146] Min Reward on eval: 2541.4520023481023
[37m[1m[2023-06-25 04:57:04,406][129146] Mean Reward across all agents: 2541.4520023481023
[37m[1m[2023-06-25 04:57:04,406][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:57:09,830][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:57:09,830][129146] Reward + Measures: [[1702.48354799    0.41823602    0.48959047    0.2509436     0.23502134]
[37m[1m [1887.73954326    0.44380003    0.55779999    0.2217        0.212     ]
[37m[1m [ 485.8890841     0.72929996    0.65030003    0.63889998    0.57750005]
[37m[1m ...
[37m[1m [-225.03973648    0.97939998    0.97489995    0.9799        0.97310001]
[37m[1m [1958.00073286    0.60009998    0.69950002    0.21279998    0.28099999]
[37m[1m [ 880.20292871    0.59540004    0.62009996    0.45480004    0.4777    ]]
[37m[1m[2023-06-25 04:57:09,831][129146] Max Reward on eval: 2566.4155676520545
[37m[1m[2023-06-25 04:57:09,831][129146] Min Reward on eval: -225.03973647947424
[37m[1m[2023-06-25 04:57:09,831][129146] Mean Reward across all agents: 1253.1899085362625
[37m[1m[2023-06-25 04:57:09,831][129146] Average Trajectory Length: 997.0423333333333
[36m[2023-06-25 04:57:09,840][129146] mean_value=231.42921417861095, max_value=2184.0290760146454
[37m[1m[2023-06-25 04:57:09,842][129146] New mean coefficients: [[-0.07286292 -0.41540876  1.9467424   0.08790296  0.16252717]]
[37m[1m[2023-06-25 04:57:09,844][129146] Moving the mean solution point...
[36m[2023-06-25 04:57:19,596][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 04:57:19,596][129146] FPS: 393837.90
[36m[2023-06-25 04:57:19,598][129146] itr=416, itrs=2000, Progress: 20.80%
[36m[2023-06-25 04:57:31,124][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 04:57:31,124][129146] FPS: 333726.15
[36m[2023-06-25 04:57:35,996][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:57:35,997][129146] Reward + Measures: [[2570.63282318    0.48868969    0.76028401    0.11233699    0.22534399]]
[37m[1m[2023-06-25 04:57:35,997][129146] Max Reward on eval: 2570.6328231758084
[37m[1m[2023-06-25 04:57:35,997][129146] Min Reward on eval: 2570.6328231758084
[37m[1m[2023-06-25 04:57:35,997][129146] Mean Reward across all agents: 2570.6328231758084
[37m[1m[2023-06-25 04:57:35,998][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:57:41,340][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:57:41,340][129146] Reward + Measures: [[ 593.05690358    0.78299999    0.61980003    0.68519992    0.51420003]
[37m[1m [1168.17866868    0.62299997    0.59069997    0.36860001    0.36320004]
[37m[1m [ 537.97056428    0.65869999    0.6512        0.57050008    0.49790007]
[37m[1m ...
[37m[1m [ 359.40897383    0.62590003    0.64600003    0.0409        0.68400002]
[37m[1m [ 779.82639975    0.73559994    0.66600001    0.55800003    0.56890005]
[37m[1m [1526.72267819    0.51360005    0.61760002    0.11610001    0.317     ]]
[37m[1m[2023-06-25 04:57:41,341][129146] Max Reward on eval: 2555.910822733771
[37m[1m[2023-06-25 04:57:41,341][129146] Min Reward on eval: -174.2738687740828
[37m[1m[2023-06-25 04:57:41,341][129146] Mean Reward across all agents: 1182.5172377413937
[37m[1m[2023-06-25 04:57:41,341][129146] Average Trajectory Length: 995.877
[36m[2023-06-25 04:57:41,348][129146] mean_value=198.70889518552576, max_value=2820.9322913335286
[37m[1m[2023-06-25 04:57:41,351][129146] New mean coefficients: [[ 0.08595353 -0.47896007  1.9468477   0.29303548  0.3294026 ]]
[37m[1m[2023-06-25 04:57:41,352][129146] Moving the mean solution point...
[36m[2023-06-25 04:57:50,893][129146] train() took 9.54 seconds to complete
[36m[2023-06-25 04:57:50,894][129146] FPS: 402545.95
[36m[2023-06-25 04:57:50,896][129146] itr=417, itrs=2000, Progress: 20.85%
[36m[2023-06-25 04:58:02,381][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 04:58:02,382][129146] FPS: 334895.86
[36m[2023-06-25 04:58:07,216][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:58:07,216][129146] Reward + Measures: [[2658.8695468     0.47399235    0.771882      0.10092732    0.22545198]]
[37m[1m[2023-06-25 04:58:07,217][129146] Max Reward on eval: 2658.8695468041756
[37m[1m[2023-06-25 04:58:07,217][129146] Min Reward on eval: 2658.8695468041756
[37m[1m[2023-06-25 04:58:07,217][129146] Mean Reward across all agents: 2658.8695468041756
[37m[1m[2023-06-25 04:58:07,217][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:58:12,691][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:58:12,692][129146] Reward + Measures: [[ 498.24187189    0.47229996    0.57629997    0.3901        0.44250003]
[37m[1m [2106.44586049    0.46499997    0.53759998    0.1339        0.2405    ]
[37m[1m [2182.85608247    0.3761        0.55390006    0.0633        0.21790002]
[37m[1m ...
[37m[1m [2214.64235857    0.3486        0.61560005    0.0964        0.20640002]
[37m[1m [2172.97887834    0.45029998    0.56919998    0.13519999    0.271     ]
[37m[1m [ 528.86879776    0.70930004    0.58149999    0.62309998    0.43670002]]
[37m[1m[2023-06-25 04:58:12,692][129146] Max Reward on eval: 2715.7644724315965
[37m[1m[2023-06-25 04:58:12,692][129146] Min Reward on eval: 45.501737847580806
[37m[1m[2023-06-25 04:58:12,692][129146] Mean Reward across all agents: 1616.3640060339444
[37m[1m[2023-06-25 04:58:12,693][129146] Average Trajectory Length: 999.718
[36m[2023-06-25 04:58:12,699][129146] mean_value=288.8685693715503, max_value=3196.016276199813
[37m[1m[2023-06-25 04:58:12,702][129146] New mean coefficients: [[-0.13484289 -0.15785831  1.4159987   0.291352    0.41626513]]
[37m[1m[2023-06-25 04:58:12,704][129146] Moving the mean solution point...
[36m[2023-06-25 04:58:22,443][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 04:58:22,444][129146] FPS: 394333.58
[36m[2023-06-25 04:58:22,446][129146] itr=418, itrs=2000, Progress: 20.90%
[36m[2023-06-25 04:58:34,133][129146] train() took 11.67 seconds to complete
[36m[2023-06-25 04:58:34,133][129146] FPS: 329113.91
[36m[2023-06-25 04:58:39,028][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:58:39,028][129146] Reward + Measures: [[2622.22288882    0.46909162    0.78367531    0.102814      0.22756734]]
[37m[1m[2023-06-25 04:58:39,028][129146] Max Reward on eval: 2622.222888819415
[37m[1m[2023-06-25 04:58:39,028][129146] Min Reward on eval: 2622.222888819415
[37m[1m[2023-06-25 04:58:39,028][129146] Mean Reward across all agents: 2622.222888819415
[37m[1m[2023-06-25 04:58:39,029][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:58:44,645][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:58:44,646][129146] Reward + Measures: [[1402.04933231    0.47830001    0.59180003    0.0722        0.41210005]
[37m[1m [1188.33914806    0.64289999    0.58700001    0.25939998    0.3434    ]
[37m[1m [1495.5055983     0.57709998    0.63959998    0.1964        0.31250003]
[37m[1m ...
[37m[1m [1885.34191822    0.57940006    0.71390003    0.19159999    0.24349999]
[37m[1m [ 965.5595158     0.46960002    0.69270003    0.2728        0.48800001]
[37m[1m [1433.88541179    0.62579995    0.65580004    0.23510002    0.28889999]]
[37m[1m[2023-06-25 04:58:44,646][129146] Max Reward on eval: 2566.8259538300335
[37m[1m[2023-06-25 04:58:44,646][129146] Min Reward on eval: -329.9637720636325
[37m[1m[2023-06-25 04:58:44,647][129146] Mean Reward across all agents: 1503.267996011007
[37m[1m[2023-06-25 04:58:44,647][129146] Average Trajectory Length: 995.6183333333333
[36m[2023-06-25 04:58:44,652][129146] mean_value=146.95519774323782, max_value=2224.678128042729
[37m[1m[2023-06-25 04:58:44,655][129146] New mean coefficients: [[0.1809354  0.30255798 1.0789431  0.5293908  0.31495157]]
[37m[1m[2023-06-25 04:58:44,656][129146] Moving the mean solution point...
[36m[2023-06-25 04:58:54,714][129146] train() took 10.06 seconds to complete
[36m[2023-06-25 04:58:54,714][129146] FPS: 381845.93
[36m[2023-06-25 04:58:54,717][129146] itr=419, itrs=2000, Progress: 20.95%
[36m[2023-06-25 04:59:06,282][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 04:59:06,282][129146] FPS: 332518.90
[36m[2023-06-25 04:59:11,089][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:59:11,090][129146] Reward + Measures: [[2651.8414753     0.4769803     0.79003143    0.10187968    0.23248465]]
[37m[1m[2023-06-25 04:59:11,090][129146] Max Reward on eval: 2651.841475303726
[37m[1m[2023-06-25 04:59:11,090][129146] Min Reward on eval: 2651.841475303726
[37m[1m[2023-06-25 04:59:11,090][129146] Mean Reward across all agents: 2651.841475303726
[37m[1m[2023-06-25 04:59:11,091][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:59:16,683][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:59:16,684][129146] Reward + Measures: [[1905.72093166    0.44530001    0.64910001    0.21540001    0.22140002]
[37m[1m [2091.52080364    0.53470004    0.67289996    0.2026        0.22430001]
[37m[1m [ 105.11894826    0.861         0.85869998    0.82569999    0.79879999]
[37m[1m ...
[37m[1m [1910.25769475    0.52210003    0.57000005    0.1911        0.1946    ]
[37m[1m [1472.45893623    0.55140001    0.67610008    0.3256        0.36809999]
[37m[1m [1456.45332089    0.4664        0.65310001    0.189         0.38560003]]
[37m[1m[2023-06-25 04:59:16,689][129146] Max Reward on eval: 2511.2075924051983
[37m[1m[2023-06-25 04:59:16,689][129146] Min Reward on eval: -818.3005151861114
[37m[1m[2023-06-25 04:59:16,690][129146] Mean Reward across all agents: 1297.4119736993298
[37m[1m[2023-06-25 04:59:16,690][129146] Average Trajectory Length: 999.5733333333333
[36m[2023-06-25 04:59:16,696][129146] mean_value=135.35506824511398, max_value=2910.3402989011956
[37m[1m[2023-06-25 04:59:16,699][129146] New mean coefficients: [[0.41224104 0.50123847 1.7106065  0.68168694 0.48993957]]
[37m[1m[2023-06-25 04:59:16,700][129146] Moving the mean solution point...
[36m[2023-06-25 04:59:26,349][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 04:59:26,350][129146] FPS: 398020.63
[36m[2023-06-25 04:59:26,352][129146] itr=420, itrs=2000, Progress: 21.00%
[37m[1m[2023-06-25 04:59:30,255][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000400
[36m[2023-06-25 04:59:42,198][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 04:59:42,198][129146] FPS: 330725.33
[36m[2023-06-25 04:59:47,056][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:59:47,057][129146] Reward + Measures: [[2756.10124512    0.47059265    0.80098194    0.07973333    0.23556864]]
[37m[1m[2023-06-25 04:59:47,057][129146] Max Reward on eval: 2756.1012451196357
[37m[1m[2023-06-25 04:59:47,057][129146] Min Reward on eval: 2756.1012451196357
[37m[1m[2023-06-25 04:59:47,057][129146] Mean Reward across all agents: 2756.1012451196357
[37m[1m[2023-06-25 04:59:47,057][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 04:59:52,623][129146] Finished Evaluation Step
[37m[1m[2023-06-25 04:59:52,623][129146] Reward + Measures: [[1068.00608205    0.42790005    0.52920002    0.2368        0.39070001]
[37m[1m [ 953.6972878     0.57359999    0.50369996    0.42820001    0.36750001]
[37m[1m [1805.10821272    0.45229998    0.56059998    0.1372        0.3096    ]
[37m[1m ...
[37m[1m [2208.36032358    0.46059999    0.57800001    0.18710001    0.21519999]
[37m[1m [1746.41495964    0.53179997    0.68370003    0.17389999    0.24069999]
[37m[1m [2625.26187811    0.46129999    0.78139997    0.0907        0.22019999]]
[37m[1m[2023-06-25 04:59:52,628][129146] Max Reward on eval: 2779.862672725879
[37m[1m[2023-06-25 04:59:52,629][129146] Min Reward on eval: 128.9092138825916
[37m[1m[2023-06-25 04:59:52,629][129146] Mean Reward across all agents: 1756.3632174887864
[37m[1m[2023-06-25 04:59:52,629][129146] Average Trajectory Length: 999.0006666666667
[36m[2023-06-25 04:59:52,633][129146] mean_value=-69.92015094661272, max_value=2450.87929392315
[37m[1m[2023-06-25 04:59:52,636][129146] New mean coefficients: [[0.3913291  0.4868571  1.8611853  0.48999488 0.09254447]]
[37m[1m[2023-06-25 04:59:52,637][129146] Moving the mean solution point...
[36m[2023-06-25 05:00:02,672][129146] train() took 10.03 seconds to complete
[36m[2023-06-25 05:00:02,672][129146] FPS: 382734.89
[36m[2023-06-25 05:00:02,675][129146] itr=421, itrs=2000, Progress: 21.05%
[36m[2023-06-25 05:00:14,317][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 05:00:14,317][129146] FPS: 330300.27
[36m[2023-06-25 05:00:19,208][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:00:19,209][129146] Reward + Measures: [[2780.32720974    0.48141068    0.7969777     0.083411      0.25306898]]
[37m[1m[2023-06-25 05:00:19,209][129146] Max Reward on eval: 2780.3272097439008
[37m[1m[2023-06-25 05:00:19,209][129146] Min Reward on eval: 2780.3272097439008
[37m[1m[2023-06-25 05:00:19,210][129146] Mean Reward across all agents: 2780.3272097439008
[37m[1m[2023-06-25 05:00:19,210][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:00:24,895][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:00:24,895][129146] Reward + Measures: [[1168.02435168    0.29769999    0.44180003    0.226         0.24430001]
[37m[1m [ 295.43778038    0.84680003    0.67830002    0.77489996    0.6426    ]
[37m[1m [ 723.27714434    0.634         0.56119996    0.4903        0.36829999]
[37m[1m ...
[37m[1m [1664.05896231    0.45650002    0.58680004    0.28790003    0.23899999]
[37m[1m [1047.66937849    0.28740001    0.38970003    0.1619        0.2692    ]
[37m[1m [1832.67038072    0.32680002    0.4598        0.18429999    0.22680001]]
[37m[1m[2023-06-25 05:00:24,895][129146] Max Reward on eval: 2916.874148966011
[37m[1m[2023-06-25 05:00:24,896][129146] Min Reward on eval: -50.166750933160074
[37m[1m[2023-06-25 05:00:24,896][129146] Mean Reward across all agents: 1475.592371770315
[37m[1m[2023-06-25 05:00:24,896][129146] Average Trajectory Length: 999.6716666666666
[36m[2023-06-25 05:00:24,904][129146] mean_value=271.7703883357778, max_value=2549.4995871895135
[37m[1m[2023-06-25 05:00:24,907][129146] New mean coefficients: [[0.0753513  0.29981244 1.8052262  0.23291117 0.22741774]]
[37m[1m[2023-06-25 05:00:24,908][129146] Moving the mean solution point...
[36m[2023-06-25 05:00:34,753][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 05:00:34,754][129146] FPS: 390107.96
[36m[2023-06-25 05:00:34,756][129146] itr=422, itrs=2000, Progress: 21.10%
[36m[2023-06-25 05:00:46,466][129146] train() took 11.69 seconds to complete
[36m[2023-06-25 05:00:46,466][129146] FPS: 328406.37
[36m[2023-06-25 05:00:51,354][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:00:51,355][129146] Reward + Measures: [[2590.0129088     0.52506965    0.79636097    0.09693567    0.25001967]]
[37m[1m[2023-06-25 05:00:51,355][129146] Max Reward on eval: 2590.0129088019785
[37m[1m[2023-06-25 05:00:51,355][129146] Min Reward on eval: 2590.0129088019785
[37m[1m[2023-06-25 05:00:51,355][129146] Mean Reward across all agents: 2590.0129088019785
[37m[1m[2023-06-25 05:00:51,356][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:00:56,984][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:00:56,985][129146] Reward + Measures: [[2303.67809776    0.53190005    0.72220004    0.1578        0.25030002]
[37m[1m [ 832.87857902    0.79330009    0.83589995    0.59810001    0.6347    ]
[37m[1m [2012.00664652    0.31349999    0.57160002    0.20010002    0.21860002]
[37m[1m ...
[37m[1m [ 169.38976401    0.92830002    0.89270002    0.87560004    0.85789996]
[37m[1m [1978.84311337    0.57019997    0.74580002    0.2041        0.29819998]
[37m[1m [ 146.94896307    0.91400003    0.89349997    0.87970001    0.85580009]]
[37m[1m[2023-06-25 05:00:56,985][129146] Max Reward on eval: 2707.3635976668797
[37m[1m[2023-06-25 05:00:56,985][129146] Min Reward on eval: -338.3176933209412
[37m[1m[2023-06-25 05:00:56,986][129146] Mean Reward across all agents: 1610.976923121522
[37m[1m[2023-06-25 05:00:56,986][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:00:56,992][129146] mean_value=715.4530203947572, max_value=2554.040801725269
[37m[1m[2023-06-25 05:00:56,995][129146] New mean coefficients: [[0.15149343 0.74479127 1.8841056  0.13879251 0.10579449]]
[37m[1m[2023-06-25 05:00:56,996][129146] Moving the mean solution point...
[36m[2023-06-25 05:01:06,802][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 05:01:06,802][129146] FPS: 391677.78
[36m[2023-06-25 05:01:06,804][129146] itr=423, itrs=2000, Progress: 21.15%
[36m[2023-06-25 05:01:18,372][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 05:01:18,372][129146] FPS: 332535.78
[36m[2023-06-25 05:01:23,140][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:01:23,140][129146] Reward + Measures: [[2649.78151797    0.54645133    0.80722332    0.08774966    0.25328434]]
[37m[1m[2023-06-25 05:01:23,141][129146] Max Reward on eval: 2649.781517968436
[37m[1m[2023-06-25 05:01:23,141][129146] Min Reward on eval: 2649.781517968436
[37m[1m[2023-06-25 05:01:23,141][129146] Mean Reward across all agents: 2649.781517968436
[37m[1m[2023-06-25 05:01:23,141][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:01:28,601][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:01:28,601][129146] Reward + Measures: [[2369.1499968     0.50810003    0.6476        0.065         0.2352    ]
[37m[1m [1229.20430546    0.63630003    0.72040004    0.43900004    0.39770001]
[37m[1m [1161.58162624    0.58270001    0.59199995    0.33040002    0.24659999]
[37m[1m ...
[37m[1m [1662.96042349    0.52380002    0.58890003    0.0725        0.26199999]
[37m[1m [ 959.31285224    0.63800001    0.61350006    0.45280001    0.33380002]
[37m[1m [2395.55906494    0.5302        0.76849997    0.1416        0.23110001]]
[37m[1m[2023-06-25 05:01:28,601][129146] Max Reward on eval: 2732.5309606919996
[37m[1m[2023-06-25 05:01:28,602][129146] Min Reward on eval: 524.3815859806724
[37m[1m[2023-06-25 05:01:28,602][129146] Mean Reward across all agents: 1685.2599550547097
[37m[1m[2023-06-25 05:01:28,602][129146] Average Trajectory Length: 999.827
[36m[2023-06-25 05:01:28,609][129146] mean_value=352.69565172685714, max_value=2954.2414367749357
[37m[1m[2023-06-25 05:01:28,612][129146] New mean coefficients: [[0.23372379 0.4285601  1.5064977  0.33448613 0.21231842]]
[37m[1m[2023-06-25 05:01:28,612][129146] Moving the mean solution point...
[36m[2023-06-25 05:01:38,454][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 05:01:38,454][129146] FPS: 390254.16
[36m[2023-06-25 05:01:38,457][129146] itr=424, itrs=2000, Progress: 21.20%
[36m[2023-06-25 05:01:49,928][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 05:01:49,928][129146] FPS: 335264.06
[36m[2023-06-25 05:01:54,648][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:01:54,648][129146] Reward + Measures: [[1934.20805837    0.59560001    0.78704232    0.24902499    0.36543432]]
[37m[1m[2023-06-25 05:01:54,648][129146] Max Reward on eval: 1934.2080583691013
[37m[1m[2023-06-25 05:01:54,649][129146] Min Reward on eval: 1934.2080583691013
[37m[1m[2023-06-25 05:01:54,649][129146] Mean Reward across all agents: 1934.2080583691013
[37m[1m[2023-06-25 05:01:54,649][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:01:59,621][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:01:59,622][129146] Reward + Measures: [[2448.9708502     0.49899998    0.72040004    0.092         0.24029998]
[37m[1m [1835.39263842    0.49830005    0.66230005    0.2237        0.33129999]
[37m[1m [ 936.13242096    0.71110004    0.7809        0.56540006    0.59600002]
[37m[1m ...
[37m[1m [1878.30565633    0.53649998    0.72490001    0.23629999    0.31060001]
[37m[1m [ 618.8944245     0.60210001    0.65820003    0.46890002    0.45760003]
[37m[1m [1341.39430977    0.60960001    0.72140002    0.41529998    0.43829998]]
[37m[1m[2023-06-25 05:01:59,622][129146] Max Reward on eval: 2663.7532644881167
[37m[1m[2023-06-25 05:01:59,622][129146] Min Reward on eval: 233.4780022777035
[37m[1m[2023-06-25 05:01:59,622][129146] Mean Reward across all agents: 1324.3248200203936
[37m[1m[2023-06-25 05:01:59,623][129146] Average Trajectory Length: 999.2669999999999
[36m[2023-06-25 05:01:59,628][129146] mean_value=224.80375418196118, max_value=2442.7491103538123
[37m[1m[2023-06-25 05:01:59,631][129146] New mean coefficients: [[0.5330507  0.592465   0.8118911  0.3021946  0.06716162]]
[37m[1m[2023-06-25 05:01:59,632][129146] Moving the mean solution point...
[36m[2023-06-25 05:02:08,688][129146] train() took 9.05 seconds to complete
[36m[2023-06-25 05:02:08,688][129146] FPS: 424096.63
[36m[2023-06-25 05:02:08,690][129146] itr=425, itrs=2000, Progress: 21.25%
[36m[2023-06-25 05:02:20,196][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 05:02:20,196][129146] FPS: 334229.04
[36m[2023-06-25 05:02:24,947][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:02:24,947][129146] Reward + Measures: [[2822.41009613    0.5423187     0.81635171    0.051336      0.24353968]]
[37m[1m[2023-06-25 05:02:24,947][129146] Max Reward on eval: 2822.4100961292465
[37m[1m[2023-06-25 05:02:24,947][129146] Min Reward on eval: 2822.4100961292465
[37m[1m[2023-06-25 05:02:24,947][129146] Mean Reward across all agents: 2822.4100961292465
[37m[1m[2023-06-25 05:02:24,948][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:02:30,557][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:02:30,563][129146] Reward + Measures: [[1858.41041187    0.52249998    0.65920001    0.1138        0.26620001]
[37m[1m [1216.66735569    0.36950001    0.5126        0.22150002    0.21800001]
[37m[1m [ 482.61397805    0.37556344    0.40993443    0.2636871     0.24989033]
[37m[1m ...
[37m[1m [2365.55503179    0.42210004    0.6663        0.1461        0.22490001]
[37m[1m [2476.02267798    0.49699998    0.71759999    0.1372        0.23650001]
[37m[1m [2633.54831842    0.39060003    0.71600002    0.09770001    0.2247    ]]
[37m[1m[2023-06-25 05:02:30,563][129146] Max Reward on eval: 2901.9841447226236
[37m[1m[2023-06-25 05:02:30,563][129146] Min Reward on eval: 67.57791308402957
[37m[1m[2023-06-25 05:02:30,564][129146] Mean Reward across all agents: 1736.145971418293
[37m[1m[2023-06-25 05:02:30,564][129146] Average Trajectory Length: 998.4106666666667
[36m[2023-06-25 05:02:30,567][129146] mean_value=-351.8900587105849, max_value=3194.2786664798505
[37m[1m[2023-06-25 05:02:30,569][129146] New mean coefficients: [[0.197086   0.10388514 0.9671589  0.10639627 0.12267591]]
[37m[1m[2023-06-25 05:02:30,570][129146] Moving the mean solution point...
[36m[2023-06-25 05:02:40,413][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 05:02:40,413][129146] FPS: 390200.06
[36m[2023-06-25 05:02:40,416][129146] itr=426, itrs=2000, Progress: 21.30%
[36m[2023-06-25 05:02:51,916][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 05:02:51,916][129146] FPS: 334498.44
[36m[2023-06-25 05:02:56,754][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:02:56,754][129146] Reward + Measures: [[477.23504561   0.70121896   0.59079164   0.65889633   0.58945435]]
[37m[1m[2023-06-25 05:02:56,755][129146] Max Reward on eval: 477.2350456092704
[37m[1m[2023-06-25 05:02:56,755][129146] Min Reward on eval: 477.2350456092704
[37m[1m[2023-06-25 05:02:56,755][129146] Mean Reward across all agents: 477.2350456092704
[37m[1m[2023-06-25 05:02:56,755][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:03:02,191][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:03:02,192][129146] Reward + Measures: [[872.89953192   0.38509998   0.51109999   0.26040003   0.28639999]
[37m[1m [369.75557156   0.82700008   0.54879999   0.84729999   0.59050006]
[37m[1m [385.35265558   0.81900007   0.63440001   0.78830004   0.67519999]
[37m[1m ...
[37m[1m [267.88348075   0.67570001   0.56880003   0.58299994   0.53290004]
[37m[1m [253.07745551   0.91650003   0.6796       0.91619998   0.70850003]
[37m[1m [720.6461086    0.56700003   0.6832       0.4402       0.59299999]]
[37m[1m[2023-06-25 05:03:02,192][129146] Max Reward on eval: 1211.0575338673546
[37m[1m[2023-06-25 05:03:02,192][129146] Min Reward on eval: -33.95096047356492
[37m[1m[2023-06-25 05:03:02,192][129146] Mean Reward across all agents: 539.417021900408
[37m[1m[2023-06-25 05:03:02,192][129146] Average Trajectory Length: 999.7286666666666
[36m[2023-06-25 05:03:02,201][129146] mean_value=281.5221900278597, max_value=1262.3849886781366
[37m[1m[2023-06-25 05:03:02,203][129146] New mean coefficients: [[0.56247663 0.574462   0.7603792  0.40099293 0.3500778 ]]
[37m[1m[2023-06-25 05:03:02,204][129146] Moving the mean solution point...
[36m[2023-06-25 05:03:11,920][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 05:03:11,920][129146] FPS: 395302.17
[36m[2023-06-25 05:03:11,922][129146] itr=427, itrs=2000, Progress: 21.35%
[36m[2023-06-25 05:03:23,430][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 05:03:23,430][129146] FPS: 334179.48
[36m[2023-06-25 05:03:28,189][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:03:28,190][129146] Reward + Measures: [[510.45336839   0.7050786    0.61971897   0.66892266   0.60754532]]
[37m[1m[2023-06-25 05:03:28,190][129146] Max Reward on eval: 510.4533683903694
[37m[1m[2023-06-25 05:03:28,190][129146] Min Reward on eval: 510.4533683903694
[37m[1m[2023-06-25 05:03:28,191][129146] Mean Reward across all agents: 510.4533683903694
[37m[1m[2023-06-25 05:03:28,191][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:03:33,675][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:03:33,676][129146] Reward + Measures: [[554.03419501   0.74669999   0.57210004   0.68639994   0.64039999]
[37m[1m [568.58244942   0.63910002   0.46669999   0.60760003   0.5327    ]
[37m[1m [403.35787108   0.73010004   0.51609999   0.68609995   0.60390002]
[37m[1m ...
[37m[1m [720.56836427   0.66659993   0.45679998   0.56960005   0.44059998]
[37m[1m [641.85345653   0.6164       0.40900001   0.54280001   0.39719999]
[37m[1m [862.10587475   0.64390004   0.32360002   0.47440001   0.3425    ]]
[37m[1m[2023-06-25 05:03:33,676][129146] Max Reward on eval: 1472.6291270598072
[37m[1m[2023-06-25 05:03:33,677][129146] Min Reward on eval: 100.84976546411345
[37m[1m[2023-06-25 05:03:33,677][129146] Mean Reward across all agents: 860.3890154277651
[37m[1m[2023-06-25 05:03:33,677][129146] Average Trajectory Length: 999.8003333333334
[36m[2023-06-25 05:03:33,685][129146] mean_value=238.50815329041583, max_value=1689.195250437269
[37m[1m[2023-06-25 05:03:33,688][129146] New mean coefficients: [[0.57297814 0.51886517 1.1217618  0.36132172 0.31822595]]
[37m[1m[2023-06-25 05:03:33,689][129146] Moving the mean solution point...
[36m[2023-06-25 05:03:43,573][129146] train() took 9.88 seconds to complete
[36m[2023-06-25 05:03:43,573][129146] FPS: 388578.01
[36m[2023-06-25 05:03:43,576][129146] itr=428, itrs=2000, Progress: 21.40%
[36m[2023-06-25 05:03:55,199][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 05:03:55,199][129146] FPS: 330880.44
[36m[2023-06-25 05:04:00,064][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:04:00,065][129146] Reward + Measures: [[547.46800101   0.72032565   0.70045733   0.68558937   0.64916265]]
[37m[1m[2023-06-25 05:04:00,065][129146] Max Reward on eval: 547.4680010112502
[37m[1m[2023-06-25 05:04:00,065][129146] Min Reward on eval: 547.4680010112502
[37m[1m[2023-06-25 05:04:00,066][129146] Mean Reward across all agents: 547.4680010112502
[37m[1m[2023-06-25 05:04:00,066][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:04:05,477][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:04:05,477][129146] Reward + Measures: [[927.27384464   0.53860003   0.48149997   0.39829999   0.31240001]
[37m[1m [669.30151747   0.67089999   0.5934       0.62400001   0.5484001 ]
[37m[1m [505.04948803   0.83260006   0.84240001   0.80290002   0.78420001]
[37m[1m ...
[37m[1m [493.45408901   0.80950004   0.77120006   0.78109998   0.72729999]
[37m[1m [251.01859455   0.88850003   0.7493       0.86650002   0.81820005]
[37m[1m [296.6451713    0.89930004   0.88679999   0.89209998   0.8835001 ]]
[37m[1m[2023-06-25 05:04:05,478][129146] Max Reward on eval: 1549.4083579643861
[37m[1m[2023-06-25 05:04:05,478][129146] Min Reward on eval: 242.01564024720574
[37m[1m[2023-06-25 05:04:05,478][129146] Mean Reward across all agents: 812.2119286816161
[37m[1m[2023-06-25 05:04:05,479][129146] Average Trajectory Length: 999.7776666666666
[36m[2023-06-25 05:04:05,485][129146] mean_value=149.0023406753593, max_value=1525.6529800752126
[37m[1m[2023-06-25 05:04:05,488][129146] New mean coefficients: [[0.44764853 0.35870966 0.43654412 0.49858832 0.52533615]]
[37m[1m[2023-06-25 05:04:05,489][129146] Moving the mean solution point...
[36m[2023-06-25 05:04:15,345][129146] train() took 9.85 seconds to complete
[36m[2023-06-25 05:04:15,345][129146] FPS: 389691.24
[36m[2023-06-25 05:04:15,347][129146] itr=429, itrs=2000, Progress: 21.45%
[36m[2023-06-25 05:04:26,965][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 05:04:26,965][129146] FPS: 331007.11
[36m[2023-06-25 05:04:31,659][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:04:31,660][129146] Reward + Measures: [[536.49144212   0.76664764   0.78778529   0.73651165   0.72086895]]
[37m[1m[2023-06-25 05:04:31,660][129146] Max Reward on eval: 536.4914421248212
[37m[1m[2023-06-25 05:04:31,660][129146] Min Reward on eval: 536.4914421248212
[37m[1m[2023-06-25 05:04:31,660][129146] Mean Reward across all agents: 536.4914421248212
[37m[1m[2023-06-25 05:04:31,660][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:04:37,078][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:04:37,078][129146] Reward + Measures: [[ 505.90746231    0.71270007    0.76609999    0.65940005    0.65710002]
[37m[1m [1220.23906806    0.3804        0.57310003    0.252         0.29360002]
[37m[1m [ 438.46322533    0.76599997    0.74720001    0.73360008    0.7143001 ]
[37m[1m ...
[37m[1m [ 988.43392407    0.43600002    0.72589999    0.1142        0.53610003]
[37m[1m [1070.29547572    0.46160004    0.65350002    0.2217        0.45579997]
[37m[1m [1338.38796543    0.36310002    0.60360003    0.17810002    0.26220003]]
[37m[1m[2023-06-25 05:04:37,079][129146] Max Reward on eval: 1737.8376395531463
[37m[1m[2023-06-25 05:04:37,079][129146] Min Reward on eval: 179.7080179159064
[37m[1m[2023-06-25 05:04:37,079][129146] Mean Reward across all agents: 772.8677462561411
[37m[1m[2023-06-25 05:04:37,079][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:04:37,089][129146] mean_value=298.9986477210402, max_value=1642.6998975119016
[37m[1m[2023-06-25 05:04:37,091][129146] New mean coefficients: [[0.6552023 0.3884395 0.6145748 0.5306961 0.5472504]]
[37m[1m[2023-06-25 05:04:37,092][129146] Moving the mean solution point...
[36m[2023-06-25 05:04:46,789][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 05:04:46,789][129146] FPS: 396092.60
[36m[2023-06-25 05:04:46,792][129146] itr=430, itrs=2000, Progress: 21.50%
[37m[1m[2023-06-25 05:04:50,941][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000410
[36m[2023-06-25 05:05:02,905][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 05:05:02,906][129146] FPS: 329840.72
[36m[2023-06-25 05:05:07,796][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:05:07,797][129146] Reward + Measures: [[536.59910377   0.80382323   0.83329564   0.77799332   0.76634568]]
[37m[1m[2023-06-25 05:05:07,797][129146] Max Reward on eval: 536.5991037748734
[37m[1m[2023-06-25 05:05:07,797][129146] Min Reward on eval: 536.5991037748734
[37m[1m[2023-06-25 05:05:07,798][129146] Mean Reward across all agents: 536.5991037748734
[37m[1m[2023-06-25 05:05:07,798][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:05:13,328][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:05:13,329][129146] Reward + Measures: [[551.18670165   0.70710003   0.60550004   0.69         0.60330003]
[37m[1m [759.11790542   0.64640003   0.48030001   0.4111       0.3303    ]
[37m[1m [297.21386467   0.81830007   0.56310004   0.80419999   0.59329998]
[37m[1m ...
[37m[1m [881.87788942   0.71340001   0.81020004   0.64399999   0.66260004]
[37m[1m [572.82967544   0.86399996   0.82910007   0.82930005   0.80170006]
[37m[1m [543.56126555   0.72919995   0.44709998   0.59750003   0.4501    ]]
[37m[1m[2023-06-25 05:05:13,329][129146] Max Reward on eval: 1258.5077203576104
[37m[1m[2023-06-25 05:05:13,329][129146] Min Reward on eval: 297.2138646650477
[37m[1m[2023-06-25 05:05:13,329][129146] Mean Reward across all agents: 610.063608927246
[37m[1m[2023-06-25 05:05:13,330][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:05:13,338][129146] mean_value=275.6109618080806, max_value=1494.165104327698
[37m[1m[2023-06-25 05:05:13,341][129146] New mean coefficients: [[0.6584295  0.21099643 0.47744828 0.4760096  0.58914053]]
[37m[1m[2023-06-25 05:05:13,342][129146] Moving the mean solution point...
[36m[2023-06-25 05:05:23,172][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 05:05:23,173][129146] FPS: 390695.23
[36m[2023-06-25 05:05:23,175][129146] itr=431, itrs=2000, Progress: 21.55%
[36m[2023-06-25 05:05:34,590][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 05:05:34,590][129146] FPS: 336878.70
[36m[2023-06-25 05:05:39,435][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:05:39,436][129146] Reward + Measures: [[628.35550957   0.80569595   0.85257471   0.77174836   0.76628131]]
[37m[1m[2023-06-25 05:05:39,436][129146] Max Reward on eval: 628.3555095745577
[37m[1m[2023-06-25 05:05:39,436][129146] Min Reward on eval: 628.3555095745577
[37m[1m[2023-06-25 05:05:39,436][129146] Mean Reward across all agents: 628.3555095745577
[37m[1m[2023-06-25 05:05:39,436][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:05:44,868][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:05:44,869][129146] Reward + Measures: [[329.1798987    0.91430008   0.89779997   0.8969       0.87279999]
[37m[1m [316.10346382   0.77329999   0.45700002   0.72640002   0.50690001]
[37m[1m [315.38201231   0.66440004   0.3554       0.58849996   0.32640001]
[37m[1m ...
[37m[1m [359.17413891   0.76260006   0.41370001   0.70389998   0.49000001]
[37m[1m [335.7413749    0.93190002   0.93520004   0.92340004   0.91030008]
[37m[1m [411.60383236   0.81730002   0.68180001   0.79829997   0.71170002]]
[37m[1m[2023-06-25 05:05:44,869][129146] Max Reward on eval: 1555.495310392289
[37m[1m[2023-06-25 05:05:44,869][129146] Min Reward on eval: 64.9433285458712
[37m[1m[2023-06-25 05:05:44,869][129146] Mean Reward across all agents: 491.2839951156648
[37m[1m[2023-06-25 05:05:44,870][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:05:44,878][129146] mean_value=135.1371969142834, max_value=1342.3121751573422
[37m[1m[2023-06-25 05:05:44,880][129146] New mean coefficients: [[0.633718   0.66631025 0.50752556 0.47243673 0.558843  ]]
[37m[1m[2023-06-25 05:05:44,881][129146] Moving the mean solution point...
[36m[2023-06-25 05:05:54,651][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 05:05:54,651][129146] FPS: 393113.29
[36m[2023-06-25 05:05:54,654][129146] itr=432, itrs=2000, Progress: 21.60%
[36m[2023-06-25 05:06:06,206][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 05:06:06,207][129146] FPS: 332879.43
[36m[2023-06-25 05:06:11,018][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:06:11,018][129146] Reward + Measures: [[603.17471069   0.84235865   0.88199037   0.81316966   0.80817759]]
[37m[1m[2023-06-25 05:06:11,019][129146] Max Reward on eval: 603.1747106913458
[37m[1m[2023-06-25 05:06:11,019][129146] Min Reward on eval: 603.1747106913458
[37m[1m[2023-06-25 05:06:11,019][129146] Mean Reward across all agents: 603.1747106913458
[37m[1m[2023-06-25 05:06:11,019][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:06:16,658][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:06:16,659][129146] Reward + Measures: [[1112.60922399    0.54249996    0.66900003    0.4289        0.45840007]
[37m[1m [ 545.01600555    0.94659996    0.95199996    0.94239998    0.93419999]
[37m[1m [ 418.89270926    0.9429        0.94699997    0.93980008    0.92449999]
[37m[1m ...
[37m[1m [ 531.68862243    0.88560003    0.89610004    0.86560005    0.84960002]
[37m[1m [ 492.33289649    0.89530003    0.91280001    0.8858        0.87530005]
[37m[1m [ 994.27520576    0.63999999    0.77850008    0.49580002    0.57300001]]
[37m[1m[2023-06-25 05:06:16,659][129146] Max Reward on eval: 1244.4870217386401
[37m[1m[2023-06-25 05:06:16,659][129146] Min Reward on eval: 287.5102988391882
[37m[1m[2023-06-25 05:06:16,659][129146] Mean Reward across all agents: 645.8597989271088
[37m[1m[2023-06-25 05:06:16,660][129146] Average Trajectory Length: 999.6873333333333
[36m[2023-06-25 05:06:16,666][129146] mean_value=152.59158830509435, max_value=1370.858531036873
[37m[1m[2023-06-25 05:06:16,669][129146] New mean coefficients: [[0.66530615 0.6951377  0.6376082  0.43306202 0.22178641]]
[37m[1m[2023-06-25 05:06:16,670][129146] Moving the mean solution point...
[36m[2023-06-25 05:06:26,396][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 05:06:26,396][129146] FPS: 394877.17
[36m[2023-06-25 05:06:26,398][129146] itr=433, itrs=2000, Progress: 21.65%
[36m[2023-06-25 05:06:37,954][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 05:06:37,954][129146] FPS: 332790.18
[36m[2023-06-25 05:06:42,784][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:06:42,790][129146] Reward + Measures: [[581.48008135   0.88522458   0.91399235   0.86350799   0.85932332]]
[37m[1m[2023-06-25 05:06:42,790][129146] Max Reward on eval: 581.4800813533922
[37m[1m[2023-06-25 05:06:42,790][129146] Min Reward on eval: 581.4800813533922
[37m[1m[2023-06-25 05:06:42,791][129146] Mean Reward across all agents: 581.4800813533922
[37m[1m[2023-06-25 05:06:42,791][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:06:48,285][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:06:48,286][129146] Reward + Measures: [[429.32962123   0.97500002   0.97509998   0.98099995   0.97290003]
[37m[1m [726.5290091    0.84130001   0.88660002   0.80419999   0.81899995]
[37m[1m [620.8130506    0.88859999   0.91750002   0.87830001   0.87670004]
[37m[1m ...
[37m[1m [728.6869476    0.83310002   0.88249999   0.79870003   0.80500001]
[37m[1m [649.29667923   0.82850009   0.82769996   0.80039996   0.7877    ]
[37m[1m [736.03766421   0.81690007   0.86439991   0.77360004   0.78000003]]
[37m[1m[2023-06-25 05:06:48,286][129146] Max Reward on eval: 1520.4743552353932
[37m[1m[2023-06-25 05:06:48,286][129146] Min Reward on eval: 218.04365390678868
[37m[1m[2023-06-25 05:06:48,286][129146] Mean Reward across all agents: 759.6052209653209
[37m[1m[2023-06-25 05:06:48,287][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:06:48,295][129146] mean_value=200.37248127216256, max_value=1182.3040745464832
[37m[1m[2023-06-25 05:06:48,297][129146] New mean coefficients: [[0.7842853  1.0918055  0.37004063 0.32182252 0.13877873]]
[37m[1m[2023-06-25 05:06:48,298][129146] Moving the mean solution point...
[36m[2023-06-25 05:06:58,136][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 05:06:58,136][129146] FPS: 390395.68
[36m[2023-06-25 05:06:58,139][129146] itr=434, itrs=2000, Progress: 21.70%
[36m[2023-06-25 05:07:09,697][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 05:07:09,697][129146] FPS: 332745.27
[36m[2023-06-25 05:07:14,589][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:07:14,590][129146] Reward + Measures: [[783.03178985   0.82625604   0.885746     0.77836162   0.78369832]]
[37m[1m[2023-06-25 05:07:14,590][129146] Max Reward on eval: 783.0317898480855
[37m[1m[2023-06-25 05:07:14,591][129146] Min Reward on eval: 783.0317898480855
[37m[1m[2023-06-25 05:07:14,591][129146] Mean Reward across all agents: 783.0317898480855
[37m[1m[2023-06-25 05:07:14,591][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:07:20,082][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:07:20,083][129146] Reward + Measures: [[787.52445452   0.80060005   0.8616001    0.75150001   0.745     ]
[37m[1m [429.12815518   0.96600002   0.96149999   0.95930004   0.94750005]
[37m[1m [712.97214025   0.72270006   0.78299999   0.68260002   0.67730004]
[37m[1m ...
[37m[1m [432.36259579   0.9551       0.95410007   0.94890004   0.93880004]
[37m[1m [633.51917494   0.90109998   0.92570013   0.87510008   0.86940002]
[37m[1m [571.22916946   0.92179996   0.94229996   0.90420002   0.90030003]]
[37m[1m[2023-06-25 05:07:20,083][129146] Max Reward on eval: 1179.612429270125
[37m[1m[2023-06-25 05:07:20,083][129146] Min Reward on eval: 333.92912941907997
[37m[1m[2023-06-25 05:07:20,083][129146] Mean Reward across all agents: 647.4731035885853
[37m[1m[2023-06-25 05:07:20,084][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:07:20,088][129146] mean_value=57.508480316093674, max_value=892.839511011041
[37m[1m[2023-06-25 05:07:20,091][129146] New mean coefficients: [[ 0.6020026  -0.12308121 -0.0509935   0.09007731 -0.00495717]]
[37m[1m[2023-06-25 05:07:20,092][129146] Moving the mean solution point...
[36m[2023-06-25 05:07:29,838][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 05:07:29,838][129146] FPS: 394061.50
[36m[2023-06-25 05:07:29,840][129146] itr=435, itrs=2000, Progress: 21.75%
[36m[2023-06-25 05:07:41,497][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 05:07:41,497][129146] FPS: 329936.80
[36m[2023-06-25 05:07:46,251][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:07:46,251][129146] Reward + Measures: [[948.23345431   0.7720927    0.85811532   0.70200998   0.71732062]]
[37m[1m[2023-06-25 05:07:46,251][129146] Max Reward on eval: 948.2334543105335
[37m[1m[2023-06-25 05:07:46,251][129146] Min Reward on eval: 948.2334543105335
[37m[1m[2023-06-25 05:07:46,251][129146] Mean Reward across all agents: 948.2334543105335
[37m[1m[2023-06-25 05:07:46,252][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:07:51,719][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:07:51,719][129146] Reward + Measures: [[748.47976939   0.80870003   0.87120003   0.76639998   0.76380002]
[37m[1m [794.68860714   0.81389999   0.87439996   0.75089997   0.75480002]
[37m[1m [577.77263      0.5323       0.56170005   0.34630001   0.4601    ]
[37m[1m ...
[37m[1m [953.88440333   0.77630001   0.87039995   0.71670002   0.72430003]
[37m[1m [471.50274987   0.74449998   0.59510005   0.70390004   0.52220005]
[37m[1m [548.22040979   0.89770001   0.90079993   0.87670004   0.86360008]]
[37m[1m[2023-06-25 05:07:51,719][129146] Max Reward on eval: 1462.0786088730674
[37m[1m[2023-06-25 05:07:51,720][129146] Min Reward on eval: 433.30331681098323
[37m[1m[2023-06-25 05:07:51,720][129146] Mean Reward across all agents: 912.6249202627489
[37m[1m[2023-06-25 05:07:51,720][129146] Average Trajectory Length: 999.7456666666666
[36m[2023-06-25 05:07:51,726][129146] mean_value=141.15430642039192, max_value=1344.8315414314686
[37m[1m[2023-06-25 05:07:51,729][129146] New mean coefficients: [[0.72582436 0.11366916 0.235221   0.09110084 0.05337491]]
[37m[1m[2023-06-25 05:07:51,730][129146] Moving the mean solution point...
[36m[2023-06-25 05:08:01,526][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 05:08:01,527][129146] FPS: 392041.77
[36m[2023-06-25 05:08:01,529][129146] itr=436, itrs=2000, Progress: 21.80%
[36m[2023-06-25 05:08:13,087][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 05:08:13,087][129146] FPS: 332716.56
[36m[2023-06-25 05:08:17,948][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:08:17,948][129146] Reward + Measures: [[1517.12896622    0.6085813     0.77797836    0.455946      0.50741404]]
[37m[1m[2023-06-25 05:08:17,948][129146] Max Reward on eval: 1517.1289662244496
[37m[1m[2023-06-25 05:08:17,949][129146] Min Reward on eval: 1517.1289662244496
[37m[1m[2023-06-25 05:08:17,949][129146] Mean Reward across all agents: 1517.1289662244496
[37m[1m[2023-06-25 05:08:17,949][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:08:23,449][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:08:23,450][129146] Reward + Measures: [[1499.86982327    0.30260003    0.58640003    0.20119999    0.23910001]
[37m[1m [1327.89108811    0.41949996    0.6778        0.19590001    0.30520001]
[37m[1m [1960.47218432    0.39660001    0.62729996    0.12909999    0.2167    ]
[37m[1m ...
[37m[1m [1134.78346898    0.67309999    0.74589998    0.56890005    0.58000004]
[37m[1m [1755.94273967    0.48709998    0.71900004    0.30899999    0.37400001]
[37m[1m [1116.05337999    0.53949994    0.68709999    0.43559995    0.45690003]]
[37m[1m[2023-06-25 05:08:23,450][129146] Max Reward on eval: 1960.4721843159757
[37m[1m[2023-06-25 05:08:23,450][129146] Min Reward on eval: 400.4659185990924
[37m[1m[2023-06-25 05:08:23,451][129146] Mean Reward across all agents: 1205.8473632428634
[37m[1m[2023-06-25 05:08:23,451][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:08:23,458][129146] mean_value=374.3001382796246, max_value=2320.284866474006
[37m[1m[2023-06-25 05:08:23,461][129146] New mean coefficients: [[ 0.73973143 -0.25351998  0.7535651   0.0559303  -0.03024666]]
[37m[1m[2023-06-25 05:08:23,462][129146] Moving the mean solution point...
[36m[2023-06-25 05:08:33,223][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 05:08:33,224][129146] FPS: 393437.66
[36m[2023-06-25 05:08:33,226][129146] itr=437, itrs=2000, Progress: 21.85%
[36m[2023-06-25 05:08:44,689][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 05:08:44,689][129146] FPS: 335505.14
[36m[2023-06-25 05:08:49,521][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:08:49,521][129146] Reward + Measures: [[1903.77335754    0.50661737    0.74249697    0.30964297    0.38324234]]
[37m[1m[2023-06-25 05:08:49,521][129146] Max Reward on eval: 1903.7733575364934
[37m[1m[2023-06-25 05:08:49,521][129146] Min Reward on eval: 1903.7733575364934
[37m[1m[2023-06-25 05:08:49,521][129146] Mean Reward across all agents: 1903.7733575364934
[37m[1m[2023-06-25 05:08:49,522][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:08:55,162][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:08:55,163][129146] Reward + Measures: [[1126.83619005    0.68889999    0.77789998    0.59860003    0.60780001]
[37m[1m [1005.17980633    0.77249998    0.83269995    0.71620005    0.70240009]
[37m[1m [1429.10592988    0.48990002    0.70139998    0.34999999    0.37040001]
[37m[1m ...
[37m[1m [1386.2930586     0.6433        0.80660003    0.53230006    0.56120008]
[37m[1m [1333.72255448    0.54449999    0.7202        0.44560003    0.45739999]
[37m[1m [1620.7905309     0.46340004    0.62459999    0.12100001    0.31640002]]
[37m[1m[2023-06-25 05:08:55,163][129146] Max Reward on eval: 2350.632356814132
[37m[1m[2023-06-25 05:08:55,163][129146] Min Reward on eval: 655.8187165918382
[37m[1m[2023-06-25 05:08:55,164][129146] Mean Reward across all agents: 1601.6693699523807
[37m[1m[2023-06-25 05:08:55,164][129146] Average Trajectory Length: 999.7626666666666
[36m[2023-06-25 05:08:55,170][129146] mean_value=201.62247225227583, max_value=2512.0988539823097
[37m[1m[2023-06-25 05:08:55,173][129146] New mean coefficients: [[ 0.48905107 -0.48070264  0.22843534 -0.14391117  0.01633505]]
[37m[1m[2023-06-25 05:08:55,174][129146] Moving the mean solution point...
[36m[2023-06-25 05:09:04,913][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 05:09:04,913][129146] FPS: 394345.50
[36m[2023-06-25 05:09:04,915][129146] itr=438, itrs=2000, Progress: 21.90%
[36m[2023-06-25 05:09:16,332][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 05:09:16,332][129146] FPS: 336885.58
[36m[2023-06-25 05:09:21,127][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:09:21,128][129146] Reward + Measures: [[2254.79961605    0.42529097    0.71392667    0.18326034    0.28445235]]
[37m[1m[2023-06-25 05:09:21,128][129146] Max Reward on eval: 2254.799616046185
[37m[1m[2023-06-25 05:09:21,128][129146] Min Reward on eval: 2254.799616046185
[37m[1m[2023-06-25 05:09:21,128][129146] Mean Reward across all agents: 2254.799616046185
[37m[1m[2023-06-25 05:09:21,128][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:09:26,691][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:09:26,692][129146] Reward + Measures: [[1545.35791776    0.56490004    0.70840001    0.43619999    0.4763    ]
[37m[1m [1359.31827015    0.52060002    0.68530005    0.35499999    0.36660001]
[37m[1m [1962.60217428    0.40050003    0.64700001    0.21030001    0.26730001]
[37m[1m ...
[37m[1m [1390.35109826    0.55809999    0.67229998    0.43659997    0.48409995]
[37m[1m [2207.08157674    0.36090001    0.55930001    0.12719999    0.23249999]
[37m[1m [1695.47550259    0.58470005    0.71759999    0.42019996    0.4772    ]]
[37m[1m[2023-06-25 05:09:26,692][129146] Max Reward on eval: 2390.594228592329
[37m[1m[2023-06-25 05:09:26,692][129146] Min Reward on eval: 677.0591150947497
[37m[1m[2023-06-25 05:09:26,692][129146] Mean Reward across all agents: 1653.0402585771694
[37m[1m[2023-06-25 05:09:26,693][129146] Average Trajectory Length: 999.5849999999999
[36m[2023-06-25 05:09:26,698][129146] mean_value=150.2378822983403, max_value=2890.594228592329
[37m[1m[2023-06-25 05:09:26,701][129146] New mean coefficients: [[ 0.40986004 -0.49971503  0.37119794 -0.1406723  -0.04493444]]
[37m[1m[2023-06-25 05:09:26,702][129146] Moving the mean solution point...
[36m[2023-06-25 05:09:36,554][129146] train() took 9.85 seconds to complete
[36m[2023-06-25 05:09:36,554][129146] FPS: 389853.16
[36m[2023-06-25 05:09:36,556][129146] itr=439, itrs=2000, Progress: 21.95%
[36m[2023-06-25 05:09:48,094][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 05:09:48,094][129146] FPS: 333303.92
[36m[2023-06-25 05:09:52,882][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:09:52,882][129146] Reward + Measures: [[2494.25780169    0.37664202    0.71050268    0.12811632    0.23983198]]
[37m[1m[2023-06-25 05:09:52,882][129146] Max Reward on eval: 2494.2578016865
[37m[1m[2023-06-25 05:09:52,883][129146] Min Reward on eval: 2494.2578016865
[37m[1m[2023-06-25 05:09:52,883][129146] Mean Reward across all agents: 2494.2578016865
[37m[1m[2023-06-25 05:09:52,883][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:09:58,508][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:09:58,509][129146] Reward + Measures: [[ 511.90039405    0.72750002    0.74130005    0.6868        0.67790002]
[37m[1m [1651.80793927    0.56290007    0.77640003    0.41170001    0.48370001]
[37m[1m [2486.94677243    0.34700003    0.65630001    0.0797        0.21610001]
[37m[1m ...
[37m[1m [ 949.32951429    0.62770003    0.68479997    0.54879999    0.53010005]
[37m[1m [1798.00044577    0.53049999    0.74490005    0.35300002    0.43899998]
[37m[1m [2139.95358639    0.4271        0.68920004    0.19170003    0.28849998]]
[37m[1m[2023-06-25 05:09:58,509][129146] Max Reward on eval: 2551.2842366211817
[37m[1m[2023-06-25 05:09:58,509][129146] Min Reward on eval: 445.5468657067744
[37m[1m[2023-06-25 05:09:58,510][129146] Mean Reward across all agents: 1613.518148115585
[37m[1m[2023-06-25 05:09:58,510][129146] Average Trajectory Length: 999.651
[36m[2023-06-25 05:09:58,515][129146] mean_value=84.84709455132919, max_value=2716.033487064822
[37m[1m[2023-06-25 05:09:58,517][129146] New mean coefficients: [[ 0.25418365 -0.44780532  0.5279595  -0.02544848 -0.14132047]]
[37m[1m[2023-06-25 05:09:58,518][129146] Moving the mean solution point...
[36m[2023-06-25 05:10:08,277][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 05:10:08,278][129146] FPS: 393540.38
[36m[2023-06-25 05:10:08,280][129146] itr=440, itrs=2000, Progress: 22.00%
[37m[1m[2023-06-25 05:10:12,453][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000420
[36m[2023-06-25 05:10:24,176][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 05:10:24,176][129146] FPS: 336621.19
[36m[2023-06-25 05:10:28,950][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:10:28,951][129146] Reward + Measures: [[2615.38322997    0.346861      0.72168702    0.10555866    0.22066034]]
[37m[1m[2023-06-25 05:10:28,951][129146] Max Reward on eval: 2615.3832299661976
[37m[1m[2023-06-25 05:10:28,951][129146] Min Reward on eval: 2615.3832299661976
[37m[1m[2023-06-25 05:10:28,951][129146] Mean Reward across all agents: 2615.3832299661976
[37m[1m[2023-06-25 05:10:28,952][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:10:34,445][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:10:34,446][129146] Reward + Measures: [[1863.06116452    0.48909998    0.68340009    0.2543        0.31720001]
[37m[1m [2023.61617813    0.38330001    0.62449998    0.19670001    0.25100002]
[37m[1m [2408.81671941    0.36199999    0.65819997    0.13829999    0.21040002]
[37m[1m ...
[37m[1m [2341.99388978    0.3594        0.653         0.1252        0.22989999]
[37m[1m [2198.0490431     0.34530002    0.74299997    0.11920001    0.22000001]
[37m[1m [2411.46196669    0.35880002    0.69349998    0.09499999    0.20420001]]
[37m[1m[2023-06-25 05:10:34,446][129146] Max Reward on eval: 2681.7901345325636
[37m[1m[2023-06-25 05:10:34,446][129146] Min Reward on eval: 528.6166911762906
[37m[1m[2023-06-25 05:10:34,446][129146] Mean Reward across all agents: 1969.6089096362446
[37m[1m[2023-06-25 05:10:34,447][129146] Average Trajectory Length: 999.6453333333333
[36m[2023-06-25 05:10:34,451][129146] mean_value=93.66948948823784, max_value=3142.217552278261
[37m[1m[2023-06-25 05:10:34,453][129146] New mean coefficients: [[ 0.03316431 -0.7108321   0.5391829  -0.04261196 -0.05723058]]
[37m[1m[2023-06-25 05:10:34,454][129146] Moving the mean solution point...
[36m[2023-06-25 05:10:44,234][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 05:10:44,234][129146] FPS: 392716.11
[36m[2023-06-25 05:10:44,237][129146] itr=441, itrs=2000, Progress: 22.05%
[36m[2023-06-25 05:10:55,651][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 05:10:55,651][129146] FPS: 336947.86
[36m[2023-06-25 05:11:00,451][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:11:00,451][129146] Reward + Measures: [[2584.13132548    0.32044032    0.74192965    0.11355001    0.21887031]]
[37m[1m[2023-06-25 05:11:00,451][129146] Max Reward on eval: 2584.1313254766146
[37m[1m[2023-06-25 05:11:00,452][129146] Min Reward on eval: 2584.1313254766146
[37m[1m[2023-06-25 05:11:00,452][129146] Mean Reward across all agents: 2584.1313254766146
[37m[1m[2023-06-25 05:11:00,452][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:11:05,985][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:11:06,045][129146] Reward + Measures: [[2369.69059107    0.29770002    0.74309999    0.10570001    0.20729999]
[37m[1m [2225.5997074     0.278         0.67919999    0.1105        0.1894    ]
[37m[1m [2470.34282023    0.37639999    0.75280005    0.17830001    0.28060004]
[37m[1m ...
[37m[1m [2484.6419148     0.29299998    0.74069995    0.1037        0.20080002]
[37m[1m [2391.92136186    0.37970001    0.76310009    0.1869        0.2868    ]
[37m[1m [2367.27558643    0.28980002    0.69929999    0.1066        0.1877    ]]
[37m[1m[2023-06-25 05:11:06,046][129146] Max Reward on eval: 2720.9959503255086
[37m[1m[2023-06-25 05:11:06,046][129146] Min Reward on eval: 1439.2200652644271
[37m[1m[2023-06-25 05:11:06,046][129146] Mean Reward across all agents: 2408.4981697044336
[37m[1m[2023-06-25 05:11:06,046][129146] Average Trajectory Length: 999.8706666666666
[36m[2023-06-25 05:11:06,055][129146] mean_value=2112.7807835456, max_value=3192.4566209408686
[37m[1m[2023-06-25 05:11:06,058][129146] New mean coefficients: [[-0.00315794 -0.61999255  0.481477   -0.05642814  0.07400963]]
[37m[1m[2023-06-25 05:11:06,059][129146] Moving the mean solution point...
[36m[2023-06-25 05:11:15,721][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 05:11:15,721][129146] FPS: 397484.74
[36m[2023-06-25 05:11:15,724][129146] itr=442, itrs=2000, Progress: 22.10%
[36m[2023-06-25 05:11:27,309][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 05:11:27,309][129146] FPS: 331928.94
[36m[2023-06-25 05:11:32,132][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:11:32,133][129146] Reward + Measures: [[2532.6961651     0.299216      0.75947201    0.12687099    0.21553634]]
[37m[1m[2023-06-25 05:11:32,133][129146] Max Reward on eval: 2532.6961651039737
[37m[1m[2023-06-25 05:11:32,133][129146] Min Reward on eval: 2532.6961651039737
[37m[1m[2023-06-25 05:11:32,133][129146] Mean Reward across all agents: 2532.6961651039737
[37m[1m[2023-06-25 05:11:32,134][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:11:37,565][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:11:37,566][129146] Reward + Measures: [[ 647.80535952    0.73150009    0.81630003    0.6983        0.68650001]
[37m[1m [2257.11949263    0.31100002    0.64399999    0.0958        0.21140002]
[37m[1m [1415.22495842    0.59660006    0.77150005    0.46450001    0.50139999]
[37m[1m ...
[37m[1m [2192.48941593    0.35910001    0.7475        0.21180001    0.28210002]
[37m[1m [1048.67328756    0.60320002    0.7324        0.5018        0.5205    ]
[37m[1m [1874.63114053    0.41689998    0.72419995    0.2545        0.28440002]]
[37m[1m[2023-06-25 05:11:37,566][129146] Max Reward on eval: 2631.166905001737
[37m[1m[2023-06-25 05:11:37,566][129146] Min Reward on eval: 420.39878832144893
[37m[1m[2023-06-25 05:11:37,566][129146] Mean Reward across all agents: 1909.8591207922645
[37m[1m[2023-06-25 05:11:37,567][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:11:37,571][129146] mean_value=169.82992244599743, max_value=2135.1783496070902
[37m[1m[2023-06-25 05:11:37,574][129146] New mean coefficients: [[-0.10753857 -0.88938886  0.23596567 -0.22271374 -0.06854185]]
[37m[1m[2023-06-25 05:11:37,575][129146] Moving the mean solution point...
[36m[2023-06-25 05:11:47,248][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 05:11:47,249][129146] FPS: 397024.52
[36m[2023-06-25 05:11:47,251][129146] itr=443, itrs=2000, Progress: 22.15%
[36m[2023-06-25 05:11:58,908][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 05:11:58,909][129146] FPS: 329877.34
[36m[2023-06-25 05:12:03,712][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:12:03,713][129146] Reward + Measures: [[2513.92039963    0.27649066    0.76965934    0.12171566    0.210582  ]]
[37m[1m[2023-06-25 05:12:03,713][129146] Max Reward on eval: 2513.920399631356
[37m[1m[2023-06-25 05:12:03,714][129146] Min Reward on eval: 2513.920399631356
[37m[1m[2023-06-25 05:12:03,714][129146] Mean Reward across all agents: 2513.920399631356
[37m[1m[2023-06-25 05:12:03,714][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:12:09,190][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:12:09,191][129146] Reward + Measures: [[2329.49816877    0.29899999    0.69929999    0.0959        0.19939999]
[37m[1m [1113.86469419    0.43030006    0.54139996    0.056         0.22930001]
[37m[1m [2368.74748632    0.2987        0.75199997    0.1604        0.19859999]
[37m[1m ...
[37m[1m [1561.09595021    0.32332683    0.52499628    0.08780336    0.17522344]
[37m[1m [1980.1435842     0.3788        0.7112        0.20580001    0.28029999]
[37m[1m [1568.10724184    0.442         0.68309999    0.33120003    0.35989997]]
[37m[1m[2023-06-25 05:12:09,191][129146] Max Reward on eval: 2639.2653059563368
[37m[1m[2023-06-25 05:12:09,191][129146] Min Reward on eval: 453.1048475923482
[37m[1m[2023-06-25 05:12:09,192][129146] Mean Reward across all agents: 2016.617443719015
[37m[1m[2023-06-25 05:12:09,192][129146] Average Trajectory Length: 999.0323333333333
[36m[2023-06-25 05:12:09,195][129146] mean_value=-48.48514905905979, max_value=2392.77212503067
[37m[1m[2023-06-25 05:12:09,198][129146] New mean coefficients: [[-0.09122935 -1.0023037   0.21867992 -0.1347144  -0.03672329]]
[37m[1m[2023-06-25 05:12:09,199][129146] Moving the mean solution point...
[36m[2023-06-25 05:12:18,999][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 05:12:18,999][129146] FPS: 391893.29
[36m[2023-06-25 05:12:19,002][129146] itr=444, itrs=2000, Progress: 22.20%
[36m[2023-06-25 05:12:30,587][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 05:12:30,587][129146] FPS: 331925.52
[36m[2023-06-25 05:12:35,353][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:12:35,353][129146] Reward + Measures: [[2410.91811906    0.25519466    0.78454232    0.13397133    0.20934932]]
[37m[1m[2023-06-25 05:12:35,353][129146] Max Reward on eval: 2410.9181190563613
[37m[1m[2023-06-25 05:12:35,354][129146] Min Reward on eval: 2410.9181190563613
[37m[1m[2023-06-25 05:12:35,354][129146] Mean Reward across all agents: 2410.9181190563613
[37m[1m[2023-06-25 05:12:35,354][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:12:41,003][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:12:41,008][129146] Reward + Measures: [[2120.78019327    0.26589999    0.71490002    0.1488        0.19419999]
[37m[1m [1949.56985216    0.29350001    0.71039999    0.171         0.22220002]
[37m[1m [2301.02731205    0.25540003    0.75570005    0.14770001    0.20469999]
[37m[1m ...
[37m[1m [2251.94600347    0.29770002    0.76410002    0.1772        0.2579    ]
[37m[1m [1674.05990533    0.4395        0.72820002    0.34259999    0.38070002]
[37m[1m [1418.91338946    0.44499999    0.7367        0.33039999    0.36300004]]
[37m[1m[2023-06-25 05:12:41,009][129146] Max Reward on eval: 2538.8928634914105
[37m[1m[2023-06-25 05:12:41,009][129146] Min Reward on eval: 297.5504137437907
[37m[1m[2023-06-25 05:12:41,009][129146] Mean Reward across all agents: 1918.2960189118671
[37m[1m[2023-06-25 05:12:41,010][129146] Average Trajectory Length: 999.932
[36m[2023-06-25 05:12:41,013][129146] mean_value=290.96423046345535, max_value=2839.154689140618
[37m[1m[2023-06-25 05:12:41,015][129146] New mean coefficients: [[-0.12206271 -1.1190944   0.29736787  0.01158172 -0.02681612]]
[37m[1m[2023-06-25 05:12:41,016][129146] Moving the mean solution point...
[36m[2023-06-25 05:12:50,787][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 05:12:50,787][129146] FPS: 393076.52
[36m[2023-06-25 05:12:50,790][129146] itr=445, itrs=2000, Progress: 22.25%
[36m[2023-06-25 05:13:02,335][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 05:13:02,335][129146] FPS: 333081.52
[36m[2023-06-25 05:13:07,158][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:13:07,159][129146] Reward + Measures: [[2288.41379786    0.24063666    0.78976762    0.15182233    0.21019734]]
[37m[1m[2023-06-25 05:13:07,159][129146] Max Reward on eval: 2288.4137978589965
[37m[1m[2023-06-25 05:13:07,159][129146] Min Reward on eval: 2288.4137978589965
[37m[1m[2023-06-25 05:13:07,159][129146] Mean Reward across all agents: 2288.4137978589965
[37m[1m[2023-06-25 05:13:07,159][129146] Average Trajectory Length: 999.5409999999999
[36m[2023-06-25 05:13:12,588][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:13:12,602][129146] Reward + Measures: [[1456.75674831    0.49309999    0.73560011    0.41560003    0.39489999]
[37m[1m [2116.15316927    0.249         0.75870001    0.17380001    0.20089999]
[37m[1m [1859.67232108    0.3964        0.80140001    0.31240001    0.34020001]
[37m[1m ...
[37m[1m [1174.22130208    0.5176        0.73190004    0.48210001    0.45310003]
[37m[1m [2208.18917037    0.23869999    0.77840006    0.1576        0.21600001]
[37m[1m [1694.22294313    0.34489998    0.70790005    0.2685        0.26370001]]
[37m[1m[2023-06-25 05:13:12,602][129146] Max Reward on eval: 2443.4003180189293
[37m[1m[2023-06-25 05:13:12,602][129146] Min Reward on eval: 631.9243898729328
[37m[1m[2023-06-25 05:13:12,603][129146] Mean Reward across all agents: 1976.2676878380178
[37m[1m[2023-06-25 05:13:12,603][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:13:12,608][129146] mean_value=430.6823348911259, max_value=2577.742886621761
[37m[1m[2023-06-25 05:13:12,611][129146] New mean coefficients: [[-0.24531919 -1.5292723   0.1993655  -0.019228    0.07004867]]
[37m[1m[2023-06-25 05:13:12,612][129146] Moving the mean solution point...
[36m[2023-06-25 05:13:22,318][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 05:13:22,318][129146] FPS: 395682.05
[36m[2023-06-25 05:13:22,321][129146] itr=446, itrs=2000, Progress: 22.30%
[36m[2023-06-25 05:13:33,913][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 05:13:33,913][129146] FPS: 331816.20
[36m[2023-06-25 05:13:38,723][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:13:38,724][129146] Reward + Measures: [[2134.86937324    0.23323999    0.78743732    0.17712967    0.21160734]]
[37m[1m[2023-06-25 05:13:38,724][129146] Max Reward on eval: 2134.8693732398983
[37m[1m[2023-06-25 05:13:38,724][129146] Min Reward on eval: 2134.8693732398983
[37m[1m[2023-06-25 05:13:38,724][129146] Mean Reward across all agents: 2134.8693732398983
[37m[1m[2023-06-25 05:13:38,724][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:13:44,192][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:13:44,193][129146] Reward + Measures: [[1209.7024953     0.52720004    0.78240007    0.46019998    0.47800002]
[37m[1m [1289.89309775    0.37919998    0.70919997    0.21860002    0.35120001]
[37m[1m [ 779.39523312    0.41599998    0.47410002    0.3673        0.27489999]
[37m[1m ...
[37m[1m [1441.34736803    0.43070003    0.77260011    0.3795        0.37460002]
[37m[1m [1749.471612      0.29879999    0.73450005    0.20969999    0.22200003]
[37m[1m [1526.16083571    0.43340001    0.7761001     0.37300003    0.37360001]]
[37m[1m[2023-06-25 05:13:44,193][129146] Max Reward on eval: 2181.2388777322367
[37m[1m[2023-06-25 05:13:44,193][129146] Min Reward on eval: 190.60611880499636
[37m[1m[2023-06-25 05:13:44,194][129146] Mean Reward across all agents: 1547.5138645583695
[37m[1m[2023-06-25 05:13:44,194][129146] Average Trajectory Length: 999.515
[36m[2023-06-25 05:13:44,198][129146] mean_value=90.7423718542836, max_value=2660.598781562387
[37m[1m[2023-06-25 05:13:44,201][129146] New mean coefficients: [[-0.3155236  -1.4977419   0.5177032  -0.06410395 -0.08785792]]
[37m[1m[2023-06-25 05:13:44,202][129146] Moving the mean solution point...
[36m[2023-06-25 05:13:53,980][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 05:13:53,980][129146] FPS: 392793.43
[36m[2023-06-25 05:13:53,983][129146] itr=447, itrs=2000, Progress: 22.35%
[36m[2023-06-25 05:14:05,593][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 05:14:05,593][129146] FPS: 331252.45
[36m[2023-06-25 05:14:10,441][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:14:10,442][129146] Reward + Measures: [[2021.16333508    0.22801168    0.7958374     0.191741      0.21641698]]
[37m[1m[2023-06-25 05:14:10,442][129146] Max Reward on eval: 2021.163335083044
[37m[1m[2023-06-25 05:14:10,442][129146] Min Reward on eval: 2021.163335083044
[37m[1m[2023-06-25 05:14:10,442][129146] Mean Reward across all agents: 2021.163335083044
[37m[1m[2023-06-25 05:14:10,443][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:14:15,938][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:14:15,943][129146] Reward + Measures: [[1607.03768322    0.22140001    0.58990002    0.1416        0.17220001]
[37m[1m [2100.01958099    0.23070002    0.72510004    0.16329999    0.19739999]
[37m[1m [1461.78583303    0.41640002    0.80849999    0.40580001    0.38680002]
[37m[1m ...
[37m[1m [2057.53912229    0.24389999    0.7209        0.14350002    0.1937    ]
[37m[1m [1406.85086874    0.35559997    0.75639999    0.32609999    0.32519999]
[37m[1m [1546.05219754    0.34570003    0.78080004    0.32250002    0.32030001]]
[37m[1m[2023-06-25 05:14:15,944][129146] Max Reward on eval: 2252.82260560377
[37m[1m[2023-06-25 05:14:15,944][129146] Min Reward on eval: 536.703332871967
[37m[1m[2023-06-25 05:14:15,944][129146] Mean Reward across all agents: 1620.7898629262397
[37m[1m[2023-06-25 05:14:15,944][129146] Average Trajectory Length: 998.687
[36m[2023-06-25 05:14:15,948][129146] mean_value=92.58279571374102, max_value=2405.519639343745
[37m[1m[2023-06-25 05:14:15,951][129146] New mean coefficients: [[-0.27225024 -1.7410909   0.3797943  -0.02243868 -0.05673227]]
[37m[1m[2023-06-25 05:14:15,952][129146] Moving the mean solution point...
[36m[2023-06-25 05:14:25,703][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 05:14:25,704][129146] FPS: 393860.81
[36m[2023-06-25 05:14:25,706][129146] itr=448, itrs=2000, Progress: 22.40%
[36m[2023-06-25 05:14:37,301][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 05:14:37,301][129146] FPS: 331680.22
[36m[2023-06-25 05:14:42,092][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:14:42,093][129146] Reward + Measures: [[2000.78498552    0.20806071    0.81265903    0.18924999    0.20858727]]
[37m[1m[2023-06-25 05:14:42,093][129146] Max Reward on eval: 2000.7849855155125
[37m[1m[2023-06-25 05:14:42,093][129146] Min Reward on eval: 2000.7849855155125
[37m[1m[2023-06-25 05:14:42,093][129146] Mean Reward across all agents: 2000.7849855155125
[37m[1m[2023-06-25 05:14:42,094][129146] Average Trajectory Length: 999.9119999999999
[36m[2023-06-25 05:14:47,598][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:14:47,604][129146] Reward + Measures: [[1680.79905745    0.31099999    0.74239999    0.24419999    0.23020001]
[37m[1m [1696.54032203    0.25560001    0.69139999    0.2106        0.21490002]
[37m[1m [1523.30752582    0.2994        0.74609995    0.23079999    0.22360002]
[37m[1m ...
[37m[1m [1960.78997245    0.20369999    0.72139996    0.1612        0.19649999]
[37m[1m [1560.99758474    0.27700001    0.74360007    0.24599998    0.22020002]
[37m[1m [1825.94563134    0.27410001    0.7209        0.23170002    0.26719999]]
[37m[1m[2023-06-25 05:14:47,604][129146] Max Reward on eval: 2093.570447937725
[37m[1m[2023-06-25 05:14:47,605][129146] Min Reward on eval: -90.41902338428773
[37m[1m[2023-06-25 05:14:47,605][129146] Mean Reward across all agents: 1572.1043660400435
[37m[1m[2023-06-25 05:14:47,605][129146] Average Trajectory Length: 999.4163333333333
[36m[2023-06-25 05:14:47,609][129146] mean_value=92.78882136465778, max_value=2540.1559414793855
[37m[1m[2023-06-25 05:14:47,612][129146] New mean coefficients: [[-0.22876097 -1.4343023   0.1681857   0.02068292  0.04313628]]
[37m[1m[2023-06-25 05:14:47,613][129146] Moving the mean solution point...
[36m[2023-06-25 05:14:57,439][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 05:14:57,439][129146] FPS: 390848.75
[36m[2023-06-25 05:14:57,442][129146] itr=449, itrs=2000, Progress: 22.45%
[36m[2023-06-25 05:15:08,863][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 05:15:08,863][129146] FPS: 336799.33
[36m[2023-06-25 05:15:13,609][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:15:13,610][129146] Reward + Measures: [[1912.77930833    0.19639866    0.81774032    0.199183      0.206716  ]]
[37m[1m[2023-06-25 05:15:13,610][129146] Max Reward on eval: 1912.7793083253066
[37m[1m[2023-06-25 05:15:13,610][129146] Min Reward on eval: 1912.7793083253066
[37m[1m[2023-06-25 05:15:13,611][129146] Mean Reward across all agents: 1912.7793083253066
[37m[1m[2023-06-25 05:15:13,611][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:15:19,282][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:15:19,283][129146] Reward + Measures: [[1512.2590378     0.29249999    0.78710002    0.28839999    0.26499999]
[37m[1m [1881.30572476    0.23339999    0.78680003    0.20720001    0.22409999]
[37m[1m [1775.66468185    0.20910001    0.81140006    0.21270001    0.19510001]
[37m[1m ...
[37m[1m [ 675.8812034     0.6469        0.75809997    0.62930006    0.58660001]
[37m[1m [1490.44183642    0.26440001    0.75450003    0.23889999    0.2146    ]
[37m[1m [1625.35833664    0.23840001    0.76440001    0.2138        0.19750002]]
[37m[1m[2023-06-25 05:15:19,283][129146] Max Reward on eval: 2054.090100285015
[37m[1m[2023-06-25 05:15:19,283][129146] Min Reward on eval: 378.8815265212499
[37m[1m[2023-06-25 05:15:19,284][129146] Mean Reward across all agents: 1477.7104382467573
[37m[1m[2023-06-25 05:15:19,284][129146] Average Trajectory Length: 999.87
[36m[2023-06-25 05:15:19,287][129146] mean_value=-26.714741955586238, max_value=2134.8547490039305
[37m[1m[2023-06-25 05:15:19,289][129146] New mean coefficients: [[-0.2653407  -1.496081    0.2841227   0.00988112  0.01033515]]
[37m[1m[2023-06-25 05:15:19,290][129146] Moving the mean solution point...
[36m[2023-06-25 05:15:29,018][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 05:15:29,019][129146] FPS: 394797.39
[36m[2023-06-25 05:15:29,021][129146] itr=450, itrs=2000, Progress: 22.50%
[37m[1m[2023-06-25 05:15:33,103][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000430
[36m[2023-06-25 05:15:44,945][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 05:15:44,945][129146] FPS: 333492.83
[36m[2023-06-25 05:15:49,787][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:15:49,787][129146] Reward + Measures: [[1835.92550318    0.18299399    0.83127862    0.21307068    0.20918499]]
[37m[1m[2023-06-25 05:15:49,788][129146] Max Reward on eval: 1835.9255031818095
[37m[1m[2023-06-25 05:15:49,788][129146] Min Reward on eval: 1835.9255031818095
[37m[1m[2023-06-25 05:15:49,788][129146] Mean Reward across all agents: 1835.9255031818095
[37m[1m[2023-06-25 05:15:49,788][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:15:55,491][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:15:55,492][129146] Reward + Measures: [[1764.64074244    0.1928        0.7748        0.1749        0.2076    ]
[37m[1m [1676.80512759    0.212         0.66699994    0.21100001    0.2149    ]
[37m[1m [1444.05993667    0.28059998    0.71820003    0.1674        0.2685    ]
[37m[1m ...
[37m[1m [1827.6803352     0.20640002    0.74330008    0.21110001    0.21589999]
[37m[1m [1703.58127426    0.2287        0.72169995    0.2261        0.2009    ]
[37m[1m [1778.36502928    0.21440001    0.7719        0.2237        0.21270001]]
[37m[1m[2023-06-25 05:15:55,492][129146] Max Reward on eval: 1928.404196054046
[37m[1m[2023-06-25 05:15:55,492][129146] Min Reward on eval: 812.513637070058
[37m[1m[2023-06-25 05:15:55,492][129146] Mean Reward across all agents: 1588.65540977817
[37m[1m[2023-06-25 05:15:55,492][129146] Average Trajectory Length: 999.1283333333333
[36m[2023-06-25 05:15:55,496][129146] mean_value=55.87985892489121, max_value=2210.2270573864107
[37m[1m[2023-06-25 05:15:55,499][129146] New mean coefficients: [[-0.28119782 -1.0539141   0.00401187  0.04663409  0.2836069 ]]
[37m[1m[2023-06-25 05:15:55,500][129146] Moving the mean solution point...
[36m[2023-06-25 05:16:05,461][129146] train() took 9.96 seconds to complete
[36m[2023-06-25 05:16:05,462][129146] FPS: 385554.65
[36m[2023-06-25 05:16:05,464][129146] itr=451, itrs=2000, Progress: 22.55%
[36m[2023-06-25 05:16:17,083][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 05:16:17,083][129146] FPS: 331071.65
[36m[2023-06-25 05:16:21,942][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:16:21,942][129146] Reward + Measures: [[1681.80213709    0.18864168    0.81778467    0.23019667    0.21733399]]
[37m[1m[2023-06-25 05:16:21,942][129146] Max Reward on eval: 1681.802137086542
[37m[1m[2023-06-25 05:16:21,942][129146] Min Reward on eval: 1681.802137086542
[37m[1m[2023-06-25 05:16:21,943][129146] Mean Reward across all agents: 1681.802137086542
[37m[1m[2023-06-25 05:16:21,943][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:16:27,494][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:16:27,494][129146] Reward + Measures: [[1231.11261095    0.27739999    0.59230006    0.24899998    0.20750001]
[37m[1m [1218.0711108     0.30059999    0.50609994    0.2538        0.2106    ]
[37m[1m [ 990.4199173     0.39629999    0.70620006    0.3346        0.40009999]
[37m[1m ...
[37m[1m [1224.67942153    0.33470002    0.71289998    0.32400003    0.27260002]
[37m[1m [1547.77399234    0.26610002    0.44940004    0.18240002    0.20449999]
[37m[1m [1912.38735889    0.23889999    0.61379999    0.17209999    0.211     ]]
[37m[1m[2023-06-25 05:16:27,495][129146] Max Reward on eval: 1981.436678869021
[37m[1m[2023-06-25 05:16:27,495][129146] Min Reward on eval: 297.4541977584653
[37m[1m[2023-06-25 05:16:27,495][129146] Mean Reward across all agents: 1396.807326084795
[37m[1m[2023-06-25 05:16:27,495][129146] Average Trajectory Length: 999.5983333333332
[36m[2023-06-25 05:16:27,501][129146] mean_value=19.379911997084296, max_value=2085.837500964978
[37m[1m[2023-06-25 05:16:27,503][129146] New mean coefficients: [[-0.26091778 -1.3794433  -0.16866449 -0.07360967  0.30625606]]
[37m[1m[2023-06-25 05:16:27,504][129146] Moving the mean solution point...
[36m[2023-06-25 05:16:37,261][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 05:16:37,261][129146] FPS: 393657.33
[36m[2023-06-25 05:16:37,263][129146] itr=452, itrs=2000, Progress: 22.60%
[36m[2023-06-25 05:16:48,691][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 05:16:48,692][129146] FPS: 336515.91
[36m[2023-06-25 05:16:53,442][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:16:53,443][129146] Reward + Measures: [[1561.06790367    0.18046349    0.81632715    0.23520902    0.21792966]]
[37m[1m[2023-06-25 05:16:53,443][129146] Max Reward on eval: 1561.0679036699912
[37m[1m[2023-06-25 05:16:53,443][129146] Min Reward on eval: 1561.0679036699912
[37m[1m[2023-06-25 05:16:53,443][129146] Mean Reward across all agents: 1561.0679036699912
[37m[1m[2023-06-25 05:16:53,444][129146] Average Trajectory Length: 999.7483333333333
[36m[2023-06-25 05:16:58,916][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:16:58,917][129146] Reward + Measures: [[1539.38833119    0.2027        0.81049997    0.2474        0.23540001]
[37m[1m [1523.34244766    0.1978        0.72340006    0.19000001    0.2184    ]
[37m[1m [1382.75917615    0.24020003    0.77540004    0.27610001    0.26040003]
[37m[1m ...
[37m[1m [1608.54247515    0.1813        0.82660007    0.2235        0.21799998]
[37m[1m [1640.87295394    0.21299998    0.80940002    0.24100001    0.22789998]
[37m[1m [1604.37201173    0.18240002    0.77280003    0.2237        0.21800001]]
[37m[1m[2023-06-25 05:16:58,917][129146] Max Reward on eval: 1809.3507198565173
[37m[1m[2023-06-25 05:16:58,917][129146] Min Reward on eval: 1082.7901778442842
[37m[1m[2023-06-25 05:16:58,917][129146] Mean Reward across all agents: 1534.4836742845453
[37m[1m[2023-06-25 05:16:58,917][129146] Average Trajectory Length: 999.9263333333333
[36m[2023-06-25 05:16:58,921][129146] mean_value=58.0336909567654, max_value=2220.9864108158044
[37m[1m[2023-06-25 05:16:58,923][129146] New mean coefficients: [[-0.18878463 -1.0096009  -0.33161762 -0.03306498  0.44138214]]
[37m[1m[2023-06-25 05:16:58,924][129146] Moving the mean solution point...
[36m[2023-06-25 05:17:08,634][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 05:17:08,634][129146] FPS: 395527.52
[36m[2023-06-25 05:17:08,636][129146] itr=453, itrs=2000, Progress: 22.65%
[36m[2023-06-25 05:17:20,084][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 05:17:20,085][129146] FPS: 335930.82
[36m[2023-06-25 05:17:24,897][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:17:24,898][129146] Reward + Measures: [[1457.01593282    0.17754266    0.80886936    0.23266166    0.21917468]]
[37m[1m[2023-06-25 05:17:24,898][129146] Max Reward on eval: 1457.015932820733
[37m[1m[2023-06-25 05:17:24,898][129146] Min Reward on eval: 1457.015932820733
[37m[1m[2023-06-25 05:17:24,898][129146] Mean Reward across all agents: 1457.015932820733
[37m[1m[2023-06-25 05:17:24,899][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:17:30,273][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:17:30,274][129146] Reward + Measures: [[1277.20658818    0.25170001    0.79890007    0.30739999    0.28599998]
[37m[1m [1104.89466254    0.2897        0.77750003    0.2911        0.3229    ]
[37m[1m [1541.99059343    0.2145        0.62859994    0.19329999    0.20489998]
[37m[1m ...
[37m[1m [1524.84485383    0.176         0.82709998    0.20780002    0.21010001]
[37m[1m [1557.9039716     0.17030001    0.77179998    0.18170001    0.20820001]
[37m[1m [ 898.58277635    0.36660001    0.67980003    0.36759999    0.35240003]]
[37m[1m[2023-06-25 05:17:30,274][129146] Max Reward on eval: 1829.368804482417
[37m[1m[2023-06-25 05:17:30,275][129146] Min Reward on eval: 575.2357005679165
[37m[1m[2023-06-25 05:17:30,275][129146] Mean Reward across all agents: 1215.5435011933027
[37m[1m[2023-06-25 05:17:30,275][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:17:30,279][129146] mean_value=-16.264517036942852, max_value=1855.641706016229
[37m[1m[2023-06-25 05:17:30,282][129146] New mean coefficients: [[-0.197708   -1.1416303  -0.41268933  0.10669976  0.5493947 ]]
[37m[1m[2023-06-25 05:17:30,283][129146] Moving the mean solution point...
[36m[2023-06-25 05:17:39,934][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 05:17:39,935][129146] FPS: 397931.93
[36m[2023-06-25 05:17:39,937][129146] itr=454, itrs=2000, Progress: 22.70%
[36m[2023-06-25 05:17:51,417][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 05:17:51,417][129146] FPS: 334982.82
[36m[2023-06-25 05:17:56,157][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:17:56,157][129146] Reward + Measures: [[1318.95724304    0.18442334    0.77938104    0.23698899    0.22759299]]
[37m[1m[2023-06-25 05:17:56,157][129146] Max Reward on eval: 1318.9572430389746
[37m[1m[2023-06-25 05:17:56,157][129146] Min Reward on eval: 1318.9572430389746
[37m[1m[2023-06-25 05:17:56,157][129146] Mean Reward across all agents: 1318.9572430389746
[37m[1m[2023-06-25 05:17:56,158][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:18:01,642][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:18:01,643][129146] Reward + Measures: [[1189.04870051    0.26770002    0.65350002    0.34169999    0.2913    ]
[37m[1m [1001.62481221    0.31220004    0.73450005    0.31890002    0.31909999]
[37m[1m [ 702.28756121    0.31560001    0.59990001    0.2933        0.23500001]
[37m[1m ...
[37m[1m [1382.37920845    0.23099999    0.72069997    0.30100003    0.2737    ]
[37m[1m [1170.5075921     0.30250001    0.70169997    0.36110002    0.32150003]
[37m[1m [ 930.93399492    0.30350003    0.73089999    0.31740001    0.3098    ]]
[37m[1m[2023-06-25 05:18:01,643][129146] Max Reward on eval: 1751.0755518875726
[37m[1m[2023-06-25 05:18:01,643][129146] Min Reward on eval: 243.1462041809922
[37m[1m[2023-06-25 05:18:01,644][129146] Mean Reward across all agents: 1036.5314920022724
[37m[1m[2023-06-25 05:18:01,644][129146] Average Trajectory Length: 999.9446666666666
[36m[2023-06-25 05:18:01,650][129146] mean_value=337.5427676694329, max_value=1945.442739081313
[37m[1m[2023-06-25 05:18:01,652][129146] New mean coefficients: [[-0.19139177 -1.1528614  -0.46466267  0.06468339  0.6912937 ]]
[37m[1m[2023-06-25 05:18:01,653][129146] Moving the mean solution point...
[36m[2023-06-25 05:18:11,283][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 05:18:11,283][129146] FPS: 398849.48
[36m[2023-06-25 05:18:11,285][129146] itr=455, itrs=2000, Progress: 22.75%
[36m[2023-06-25 05:18:22,819][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 05:18:22,819][129146] FPS: 333433.56
[36m[2023-06-25 05:18:27,631][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:18:27,632][129146] Reward + Measures: [[1253.94304934    0.18300043    0.75995994    0.23452848    0.23076437]]
[37m[1m[2023-06-25 05:18:27,632][129146] Max Reward on eval: 1253.9430493393645
[37m[1m[2023-06-25 05:18:27,632][129146] Min Reward on eval: 1253.9430493393645
[37m[1m[2023-06-25 05:18:27,632][129146] Mean Reward across all agents: 1253.9430493393645
[37m[1m[2023-06-25 05:18:27,632][129146] Average Trajectory Length: 999.5736666666667
[36m[2023-06-25 05:18:33,308][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:18:33,309][129146] Reward + Measures: [[1271.33341105    0.1741        0.76780003    0.2254        0.2263    ]
[37m[1m [1381.46073746    0.15260002    0.74529999    0.21870001    0.21210001]
[37m[1m [1259.629541      0.1797        0.5959        0.226         0.2457    ]
[37m[1m ...
[37m[1m [ 982.10313781    0.29120001    0.7179001     0.31220001    0.2974    ]
[37m[1m [1208.18905989    0.2105        0.59429997    0.2149        0.22090001]
[37m[1m [1161.51582327    0.20120001    0.58249998    0.24249999    0.2678    ]]
[37m[1m[2023-06-25 05:18:33,309][129146] Max Reward on eval: 1453.2665412746603
[37m[1m[2023-06-25 05:18:33,309][129146] Min Reward on eval: 416.21958250404566
[37m[1m[2023-06-25 05:18:33,310][129146] Mean Reward across all agents: 1154.2577375125818
[37m[1m[2023-06-25 05:18:33,310][129146] Average Trajectory Length: 999.7843333333333
[36m[2023-06-25 05:18:33,313][129146] mean_value=-101.39421404393569, max_value=1804.309958999569
[37m[1m[2023-06-25 05:18:33,316][129146] New mean coefficients: [[-0.13020274 -1.7137235  -0.7963501   0.01235132  0.93199337]]
[37m[1m[2023-06-25 05:18:33,317][129146] Moving the mean solution point...
[36m[2023-06-25 05:18:43,179][129146] train() took 9.86 seconds to complete
[36m[2023-06-25 05:18:43,179][129146] FPS: 389447.71
[36m[2023-06-25 05:18:43,181][129146] itr=456, itrs=2000, Progress: 22.80%
[36m[2023-06-25 05:18:54,853][129146] train() took 11.65 seconds to complete
[36m[2023-06-25 05:18:54,853][129146] FPS: 329490.14
[36m[2023-06-25 05:18:59,619][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:18:59,619][129146] Reward + Measures: [[1256.17324409    0.17637867    0.7393893     0.23322999    0.23369566]]
[37m[1m[2023-06-25 05:18:59,619][129146] Max Reward on eval: 1256.1732440940996
[37m[1m[2023-06-25 05:18:59,620][129146] Min Reward on eval: 1256.1732440940996
[37m[1m[2023-06-25 05:18:59,620][129146] Mean Reward across all agents: 1256.1732440940996
[37m[1m[2023-06-25 05:18:59,620][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:19:05,140][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:19:05,141][129146] Reward + Measures: [[ 997.83731513    0.3795        0.60900003    0.23699999    0.34450004]
[37m[1m [1264.58566179    0.29430002    0.64299995    0.20039999    0.28369999]
[37m[1m [ 635.52309359    0.49379998    0.51449996    0.33220002    0.3876    ]
[37m[1m ...
[37m[1m [1177.28965609    0.24850002    0.66609997    0.1602        0.22740002]
[37m[1m [ 541.27141388    0.52360004    0.5176        0.37470001    0.46150002]
[37m[1m [ 477.03248661    0.52590007    0.52969998    0.38330001    0.45590001]]
[37m[1m[2023-06-25 05:19:05,141][129146] Max Reward on eval: 1426.1221112460596
[37m[1m[2023-06-25 05:19:05,141][129146] Min Reward on eval: 339.401408423495
[37m[1m[2023-06-25 05:19:05,141][129146] Mean Reward across all agents: 899.9415059950376
[37m[1m[2023-06-25 05:19:05,142][129146] Average Trajectory Length: 999.3639999999999
[36m[2023-06-25 05:19:05,144][129146] mean_value=-353.14060960876157, max_value=1255.9592142634992
[37m[1m[2023-06-25 05:19:05,147][129146] New mean coefficients: [[-0.24009094 -1.5350015  -1.0993488  -0.18547016  0.75644815]]
[37m[1m[2023-06-25 05:19:05,148][129146] Moving the mean solution point...
[36m[2023-06-25 05:19:14,949][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 05:19:14,949][129146] FPS: 391875.38
[36m[2023-06-25 05:19:14,951][129146] itr=457, itrs=2000, Progress: 22.85%
[36m[2023-06-25 05:19:26,433][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 05:19:26,434][129146] FPS: 335046.91
[36m[2023-06-25 05:19:31,304][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:19:31,304][129146] Reward + Measures: [[1188.15241286    0.184637      0.70196468    0.22576399    0.24592534]]
[37m[1m[2023-06-25 05:19:31,305][129146] Max Reward on eval: 1188.152412862116
[37m[1m[2023-06-25 05:19:31,305][129146] Min Reward on eval: 1188.152412862116
[37m[1m[2023-06-25 05:19:31,305][129146] Mean Reward across all agents: 1188.152412862116
[37m[1m[2023-06-25 05:19:31,305][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:19:36,838][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:19:36,839][129146] Reward + Measures: [[ 779.77147476    0.3091        0.76970005    0.13110001    0.38100001]
[37m[1m [ 569.23974909    0.42840001    0.68690002    0.18489999    0.41330001]
[37m[1m [ 738.47316442    0.34410006    0.70650005    0.14719999    0.47560006]
[37m[1m ...
[37m[1m [ 615.31029551    0.4068        0.70979995    0.11719999    0.43830004]
[37m[1m [ 551.97615108    0.36610001    0.87709999    0.1123        0.82140011]
[37m[1m [1139.71446299    0.33570001    0.50570005    0.22679999    0.25689998]]
[37m[1m[2023-06-25 05:19:36,839][129146] Max Reward on eval: 1488.3023972175317
[37m[1m[2023-06-25 05:19:36,839][129146] Min Reward on eval: -289.06414457397767
[37m[1m[2023-06-25 05:19:36,839][129146] Mean Reward across all agents: 615.5859394971181
[37m[1m[2023-06-25 05:19:36,840][129146] Average Trajectory Length: 997.615
[36m[2023-06-25 05:19:36,846][129146] mean_value=114.92787497579043, max_value=1712.916997360997
[37m[1m[2023-06-25 05:19:36,849][129146] New mean coefficients: [[-0.30485216 -1.5777407  -1.0416644  -0.3090209   0.4750336 ]]
[37m[1m[2023-06-25 05:19:36,850][129146] Moving the mean solution point...
[36m[2023-06-25 05:19:46,639][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 05:19:46,639][129146] FPS: 392351.19
[36m[2023-06-25 05:19:46,641][129146] itr=458, itrs=2000, Progress: 22.90%
[36m[2023-06-25 05:19:58,170][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 05:19:58,170][129146] FPS: 333558.65
[36m[2023-06-25 05:20:02,982][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:20:02,983][129146] Reward + Measures: [[1133.90223372    0.18805249    0.68618101    0.21908396    0.25044984]]
[37m[1m[2023-06-25 05:20:02,983][129146] Max Reward on eval: 1133.9022337237361
[37m[1m[2023-06-25 05:20:02,983][129146] Min Reward on eval: 1133.9022337237361
[37m[1m[2023-06-25 05:20:02,983][129146] Mean Reward across all agents: 1133.9022337237361
[37m[1m[2023-06-25 05:20:02,983][129146] Average Trajectory Length: 999.699
[36m[2023-06-25 05:20:08,453][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:20:08,454][129146] Reward + Measures: [[1092.56979134    0.24790001    0.49329996    0.16850001    0.20319998]
[37m[1m [ 785.67578857    0.28387281    0.4195725     0.079831      0.20342608]
[37m[1m [1147.33540787    0.27360001    0.53980005    0.26760003    0.2397    ]
[37m[1m ...
[37m[1m [1008.13174707    0.28060001    0.47750002    0.223         0.26760003]
[37m[1m [1092.2825709     0.21659999    0.4729        0.12990001    0.20760003]
[37m[1m [1066.70162194    0.36620003    0.5783        0.25800002    0.32269999]]
[37m[1m[2023-06-25 05:20:08,454][129146] Max Reward on eval: 1523.2660447044007
[37m[1m[2023-06-25 05:20:08,454][129146] Min Reward on eval: -53.15981158952345
[37m[1m[2023-06-25 05:20:08,455][129146] Mean Reward across all agents: 957.2372672355324
[37m[1m[2023-06-25 05:20:08,455][129146] Average Trajectory Length: 996.379
[36m[2023-06-25 05:20:08,459][129146] mean_value=-160.50884863253742, max_value=1669.0070990399527
[37m[1m[2023-06-25 05:20:08,462][129146] New mean coefficients: [[-0.30725014 -1.5085285  -1.3011231  -0.3399382   0.6640928 ]]
[37m[1m[2023-06-25 05:20:08,463][129146] Moving the mean solution point...
[36m[2023-06-25 05:20:18,260][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 05:20:18,260][129146] FPS: 392018.03
[36m[2023-06-25 05:20:18,263][129146] itr=459, itrs=2000, Progress: 22.95%
[36m[2023-06-25 05:20:29,830][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 05:20:29,830][129146] FPS: 332460.49
[36m[2023-06-25 05:20:34,567][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:20:34,567][129146] Reward + Measures: [[1071.78057266    0.18587744    0.66904974    0.21272843    0.25766692]]
[37m[1m[2023-06-25 05:20:34,567][129146] Max Reward on eval: 1071.7805726632234
[37m[1m[2023-06-25 05:20:34,567][129146] Min Reward on eval: 1071.7805726632234
[37m[1m[2023-06-25 05:20:34,568][129146] Mean Reward across all agents: 1071.7805726632234
[37m[1m[2023-06-25 05:20:34,568][129146] Average Trajectory Length: 999.8539999999999
[36m[2023-06-25 05:20:40,072][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:20:40,072][129146] Reward + Measures: [[1042.79799402    0.18440001    0.66659999    0.24770002    0.29480001]
[37m[1m [ 741.80173513    0.29430002    0.75049996    0.25820002    0.42290002]
[37m[1m [ 925.35655397    0.21659999    0.75370002    0.18270001    0.29040003]
[37m[1m ...
[37m[1m [ 816.81887983    0.26499999    0.72900003    0.27589998    0.36520001]
[37m[1m [ 994.30745382    0.1934        0.69909996    0.20430003    0.26980001]
[37m[1m [ 956.39324994    0.2256        0.66860002    0.25750002    0.35409999]]
[37m[1m[2023-06-25 05:20:40,072][129146] Max Reward on eval: 1336.9169782570564
[37m[1m[2023-06-25 05:20:40,073][129146] Min Reward on eval: 328.590632499801
[37m[1m[2023-06-25 05:20:40,073][129146] Mean Reward across all agents: 792.509283661964
[37m[1m[2023-06-25 05:20:40,073][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:20:40,078][129146] mean_value=196.3184726005632, max_value=1788.7226121694548
[37m[1m[2023-06-25 05:20:40,081][129146] New mean coefficients: [[-0.28485885 -1.3942424  -0.97100127 -0.13551246  0.7778854 ]]
[37m[1m[2023-06-25 05:20:40,082][129146] Moving the mean solution point...
[36m[2023-06-25 05:20:49,742][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 05:20:49,743][129146] FPS: 397574.16
[36m[2023-06-25 05:20:49,745][129146] itr=460, itrs=2000, Progress: 23.00%
[37m[1m[2023-06-25 05:20:54,066][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000440
[36m[2023-06-25 05:21:06,014][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 05:21:06,015][129146] FPS: 330260.25
[36m[2023-06-25 05:21:10,598][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:21:10,598][129146] Reward + Measures: [[974.44025624   0.18509467   0.639328     0.20601599   0.26364997]]
[37m[1m[2023-06-25 05:21:10,599][129146] Max Reward on eval: 974.4402562404806
[37m[1m[2023-06-25 05:21:10,599][129146] Min Reward on eval: 974.4402562404806
[37m[1m[2023-06-25 05:21:10,599][129146] Mean Reward across all agents: 974.4402562404806
[37m[1m[2023-06-25 05:21:10,599][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:21:16,070][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:21:16,070][129146] Reward + Measures: [[  55.86388126    0.1824        0.38540003    0.24349999    0.29899999]
[37m[1m [ 118.82036228    0.187225      0.38183755    0.17086251    0.192975  ]
[37m[1m [ 393.82189451    0.30689999    0.31759998    0.09729999    0.24800001]
[37m[1m ...
[37m[1m [ 793.38113215    0.37560001    0.60619998    0.229         0.37099999]
[37m[1m [ 330.1019286     0.20760003    0.4039        0.1259        0.2142    ]
[37m[1m [1066.7734907     0.15890001    0.74159998    0.21360002    0.2617    ]]
[37m[1m[2023-06-25 05:21:16,070][129146] Max Reward on eval: 1180.8276223109278
[37m[1m[2023-06-25 05:21:16,071][129146] Min Reward on eval: -794.424228600564
[37m[1m[2023-06-25 05:21:16,071][129146] Mean Reward across all agents: 436.9109345277143
[37m[1m[2023-06-25 05:21:16,071][129146] Average Trajectory Length: 973.8186666666667
[36m[2023-06-25 05:21:16,076][129146] mean_value=-369.9021069081641, max_value=1446.2422650346532
[37m[1m[2023-06-25 05:21:16,079][129146] New mean coefficients: [[-0.12839435 -1.0002431  -0.8174817   0.01297943  0.8481457 ]]
[37m[1m[2023-06-25 05:21:16,080][129146] Moving the mean solution point...
[36m[2023-06-25 05:21:25,809][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 05:21:25,809][129146] FPS: 394771.31
[36m[2023-06-25 05:21:25,811][129146] itr=461, itrs=2000, Progress: 23.05%
[36m[2023-06-25 05:21:37,298][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 05:21:37,298][129146] FPS: 334804.86
[36m[2023-06-25 05:21:42,113][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:21:42,113][129146] Reward + Measures: [[903.6334887    0.18588468   0.60673702   0.20158634   0.26963967]]
[37m[1m[2023-06-25 05:21:42,113][129146] Max Reward on eval: 903.633488699642
[37m[1m[2023-06-25 05:21:42,114][129146] Min Reward on eval: 903.633488699642
[37m[1m[2023-06-25 05:21:42,114][129146] Mean Reward across all agents: 903.633488699642
[37m[1m[2023-06-25 05:21:42,114][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:21:47,562][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:21:47,563][129146] Reward + Measures: [[202.12666796   0.27062374   0.28939626   0.20660594   0.26784959]
[37m[1m [313.89606162   0.17989999   0.28279999   0.14070001   0.3053    ]
[37m[1m [888.96238469   0.19260001   0.41300002   0.13960001   0.25580001]
[37m[1m ...
[37m[1m [560.2717777    0.21980003   0.4434       0.16180001   0.28849998]
[37m[1m [365.5211489    0.28480002   0.46619996   0.14659999   0.2994    ]
[37m[1m [599.45517417   0.15629999   0.3565       0.14330001   0.2217    ]]
[37m[1m[2023-06-25 05:21:47,563][129146] Max Reward on eval: 1068.1447215759545
[37m[1m[2023-06-25 05:21:47,563][129146] Min Reward on eval: -222.06428284139838
[37m[1m[2023-06-25 05:21:47,563][129146] Mean Reward across all agents: 604.0571118020656
[37m[1m[2023-06-25 05:21:47,564][129146] Average Trajectory Length: 995.4243333333333
[36m[2023-06-25 05:21:47,567][129146] mean_value=-446.3397445929402, max_value=1252.2107463398338
[37m[1m[2023-06-25 05:21:47,570][129146] New mean coefficients: [[-0.12367198 -0.40739548 -0.3405042   0.01266291  0.60378504]]
[37m[1m[2023-06-25 05:21:47,571][129146] Moving the mean solution point...
[36m[2023-06-25 05:21:57,224][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 05:21:57,224][129146] FPS: 397871.14
[36m[2023-06-25 05:21:57,226][129146] itr=462, itrs=2000, Progress: 23.10%
[36m[2023-06-25 05:22:08,640][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 05:22:08,640][129146] FPS: 336953.07
[36m[2023-06-25 05:22:13,514][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:22:13,514][129146] Reward + Measures: [[846.50142569   0.18202627   0.57260406   0.19700813   0.27706847]]
[37m[1m[2023-06-25 05:22:13,514][129146] Max Reward on eval: 846.5014256881989
[37m[1m[2023-06-25 05:22:13,514][129146] Min Reward on eval: 846.5014256881989
[37m[1m[2023-06-25 05:22:13,515][129146] Mean Reward across all agents: 846.5014256881989
[37m[1m[2023-06-25 05:22:13,515][129146] Average Trajectory Length: 999.7576666666666
[36m[2023-06-25 05:22:19,096][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:22:19,102][129146] Reward + Measures: [[811.91626095   0.31420001   0.66060001   0.24440001   0.3486    ]
[37m[1m [800.48224831   0.32249999   0.67020005   0.27869999   0.3387    ]
[37m[1m [692.02451054   0.31380001   0.61140007   0.27650002   0.32539997]
[37m[1m ...
[37m[1m [627.55753294   0.35499999   0.72720003   0.4276       0.4461    ]
[37m[1m [712.13777829   0.44040003   0.72579998   0.50050002   0.47170001]
[37m[1m [575.67304879   0.31060001   0.71290004   0.37530002   0.40880004]]
[37m[1m[2023-06-25 05:22:19,102][129146] Max Reward on eval: 1090.739032908983
[37m[1m[2023-06-25 05:22:19,102][129146] Min Reward on eval: -19.124079165013974
[37m[1m[2023-06-25 05:22:19,103][129146] Mean Reward across all agents: 761.5209566387338
[37m[1m[2023-06-25 05:22:19,103][129146] Average Trajectory Length: 998.6936666666667
[36m[2023-06-25 05:22:19,108][129146] mean_value=154.84546900320788, max_value=1248.1967286334927
[37m[1m[2023-06-25 05:22:19,111][129146] New mean coefficients: [[-0.15659814 -1.1314812  -0.24174735  0.00478705  0.6107261 ]]
[37m[1m[2023-06-25 05:22:19,112][129146] Moving the mean solution point...
[36m[2023-06-25 05:22:28,710][129146] train() took 9.60 seconds to complete
[36m[2023-06-25 05:22:28,710][129146] FPS: 400131.01
[36m[2023-06-25 05:22:28,713][129146] itr=463, itrs=2000, Progress: 23.15%
[36m[2023-06-25 05:22:40,095][129146] train() took 11.37 seconds to complete
[36m[2023-06-25 05:22:40,095][129146] FPS: 337857.79
[36m[2023-06-25 05:22:44,891][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:22:44,891][129146] Reward + Measures: [[792.47875008   0.17486033   0.55263633   0.19594866   0.29090035]]
[37m[1m[2023-06-25 05:22:44,892][129146] Max Reward on eval: 792.4787500811917
[37m[1m[2023-06-25 05:22:44,892][129146] Min Reward on eval: 792.4787500811917
[37m[1m[2023-06-25 05:22:44,892][129146] Mean Reward across all agents: 792.4787500811917
[37m[1m[2023-06-25 05:22:44,892][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:22:50,424][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:22:50,424][129146] Reward + Measures: [[ 171.59563597    0.31810001    0.41249999    0.2316        0.368     ]
[37m[1m [ 189.44662554    0.27340001    0.60080004    0.11799999    0.37139997]
[37m[1m [ 922.41275896    0.24850002    0.62990004    0.20580001    0.30930001]
[37m[1m ...
[37m[1m [  53.29777673    0.19430001    0.3635        0.1856        0.19399999]
[37m[1m [1006.80372214    0.24879999    0.67760003    0.20490001    0.41079998]
[37m[1m [ 624.90699045    0.2323        0.48010001    0.26210001    0.27659997]]
[37m[1m[2023-06-25 05:22:50,424][129146] Max Reward on eval: 1319.0225370549829
[37m[1m[2023-06-25 05:22:50,425][129146] Min Reward on eval: -429.29999850853056
[37m[1m[2023-06-25 05:22:50,425][129146] Mean Reward across all agents: 768.6920698732356
[37m[1m[2023-06-25 05:22:50,425][129146] Average Trajectory Length: 999.5179999999999
[36m[2023-06-25 05:22:50,431][129146] mean_value=34.6358231630488, max_value=1263.1073093204386
[37m[1m[2023-06-25 05:22:50,434][129146] New mean coefficients: [[-0.19802995 -1.0548391  -0.06626715 -0.23604408  0.5259328 ]]
[37m[1m[2023-06-25 05:22:50,435][129146] Moving the mean solution point...
[36m[2023-06-25 05:23:00,079][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 05:23:00,079][129146] FPS: 398239.85
[36m[2023-06-25 05:23:00,081][129146] itr=464, itrs=2000, Progress: 23.20%
[36m[2023-06-25 05:23:11,447][129146] train() took 11.35 seconds to complete
[36m[2023-06-25 05:23:11,447][129146] FPS: 338365.32
[36m[2023-06-25 05:23:16,159][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:23:16,159][129146] Reward + Measures: [[708.07615285   0.16994266   0.53488404   0.19069666   0.30968568]]
[37m[1m[2023-06-25 05:23:16,159][129146] Max Reward on eval: 708.076152848706
[37m[1m[2023-06-25 05:23:16,160][129146] Min Reward on eval: 708.076152848706
[37m[1m[2023-06-25 05:23:16,160][129146] Mean Reward across all agents: 708.076152848706
[37m[1m[2023-06-25 05:23:16,160][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:23:21,515][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:23:21,516][129146] Reward + Measures: [[500.67526445   0.294        0.57840002   0.1672       0.42529997]
[37m[1m [585.69367613   0.3382       0.76440001   0.26030001   0.66720003]
[37m[1m [318.54298709   0.45220003   0.47340003   0.26269999   0.3457    ]
[37m[1m ...
[37m[1m [291.74164228   0.45269999   0.40340003   0.3479       0.32730004]
[37m[1m [538.30014588   0.50389999   0.7137       0.43789998   0.63319999]
[37m[1m [265.23730742   0.65979999   0.69280005   0.56510001   0.65560001]]
[37m[1m[2023-06-25 05:23:21,516][129146] Max Reward on eval: 934.7604080530466
[37m[1m[2023-06-25 05:23:21,517][129146] Min Reward on eval: 88.23351381799439
[37m[1m[2023-06-25 05:23:21,517][129146] Mean Reward across all agents: 450.6799748489742
[37m[1m[2023-06-25 05:23:21,517][129146] Average Trajectory Length: 999.3713333333333
[36m[2023-06-25 05:23:21,523][129146] mean_value=-76.85308956758463, max_value=1256.2089782543421
[37m[1m[2023-06-25 05:23:21,525][129146] New mean coefficients: [[ 0.05609785 -0.27559996  0.44646406  0.0179235   0.75646186]]
[37m[1m[2023-06-25 05:23:21,526][129146] Moving the mean solution point...
[36m[2023-06-25 05:23:31,215][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 05:23:31,215][129146] FPS: 396393.73
[36m[2023-06-25 05:23:31,218][129146] itr=465, itrs=2000, Progress: 23.25%
[36m[2023-06-25 05:23:42,613][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 05:23:42,613][129146] FPS: 337517.23
[36m[2023-06-25 05:23:47,465][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:23:47,466][129146] Reward + Measures: [[714.01602803   0.18217167   0.57943797   0.18964101   0.35163632]]
[37m[1m[2023-06-25 05:23:47,466][129146] Max Reward on eval: 714.0160280328929
[37m[1m[2023-06-25 05:23:47,466][129146] Min Reward on eval: 714.0160280328929
[37m[1m[2023-06-25 05:23:47,466][129146] Mean Reward across all agents: 714.0160280328929
[37m[1m[2023-06-25 05:23:47,467][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:23:52,948][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:23:52,949][129146] Reward + Measures: [[427.46794004   0.48670003   0.59200001   0.42480001   0.52520001]
[37m[1m [799.68591132   0.3355       0.57560009   0.23930001   0.37890002]
[37m[1m [658.84909317   0.29729998   0.67539996   0.20899999   0.43540001]
[37m[1m ...
[37m[1m [496.63269831   0.1288       0.39900002   0.1476       0.1926    ]
[37m[1m [574.77631313   0.39610001   0.59729999   0.31669998   0.44400001]
[37m[1m [535.81740736   0.38680002   0.70750004   0.28940001   0.52770001]]
[37m[1m[2023-06-25 05:23:52,949][129146] Max Reward on eval: 1242.1858698235824
[37m[1m[2023-06-25 05:23:52,949][129146] Min Reward on eval: 217.18509803366322
[37m[1m[2023-06-25 05:23:52,950][129146] Mean Reward across all agents: 736.6297429139711
[37m[1m[2023-06-25 05:23:52,950][129146] Average Trajectory Length: 998.621
[36m[2023-06-25 05:23:52,954][129146] mean_value=-51.958885992385405, max_value=1211.2508445031592
[37m[1m[2023-06-25 05:23:52,957][129146] New mean coefficients: [[-0.04790758 -0.7947305   0.84790564 -0.1200172   0.42123404]]
[37m[1m[2023-06-25 05:23:52,958][129146] Moving the mean solution point...
[36m[2023-06-25 05:24:02,661][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 05:24:02,661][129146] FPS: 395836.39
[36m[2023-06-25 05:24:02,663][129146] itr=466, itrs=2000, Progress: 23.30%
[36m[2023-06-25 05:24:14,271][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 05:24:14,271][129146] FPS: 331333.59
[36m[2023-06-25 05:24:19,138][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:24:19,138][129146] Reward + Measures: [[704.40533578   0.20509534   0.67457867   0.17857066   0.46337402]]
[37m[1m[2023-06-25 05:24:19,138][129146] Max Reward on eval: 704.4053357783705
[37m[1m[2023-06-25 05:24:19,138][129146] Min Reward on eval: 704.4053357783705
[37m[1m[2023-06-25 05:24:19,138][129146] Mean Reward across all agents: 704.4053357783705
[37m[1m[2023-06-25 05:24:19,139][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:24:24,792][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:24:24,793][129146] Reward + Measures: [[615.63459662   0.3105       0.81409997   0.24150001   0.72439998]
[37m[1m [572.93091793   0.19859998   0.78560001   0.27309999   0.57209998]
[37m[1m [602.92924868   0.37400004   0.73570007   0.24519999   0.63660002]
[37m[1m ...
[37m[1m [946.59086601   0.1693       0.66939992   0.1717       0.28130004]
[37m[1m [699.34125616   0.26230001   0.73609996   0.1991       0.47860003]
[37m[1m [834.23599077   0.16859999   0.35450003   0.0851       0.14060001]]
[37m[1m[2023-06-25 05:24:24,793][129146] Max Reward on eval: 1336.6624135199468
[37m[1m[2023-06-25 05:24:24,793][129146] Min Reward on eval: 246.0773680498416
[37m[1m[2023-06-25 05:24:24,793][129146] Mean Reward across all agents: 744.6467960840712
[37m[1m[2023-06-25 05:24:24,794][129146] Average Trajectory Length: 999.6586666666666
[36m[2023-06-25 05:24:24,800][129146] mean_value=252.84498674781412, max_value=1483.0984000372118
[37m[1m[2023-06-25 05:24:24,803][129146] New mean coefficients: [[-0.01694337 -0.5114966   0.7583292  -0.02289066  0.44172236]]
[37m[1m[2023-06-25 05:24:24,803][129146] Moving the mean solution point...
[36m[2023-06-25 05:24:34,732][129146] train() took 9.93 seconds to complete
[36m[2023-06-25 05:24:34,732][129146] FPS: 386832.37
[36m[2023-06-25 05:24:34,735][129146] itr=467, itrs=2000, Progress: 23.35%
[36m[2023-06-25 05:24:46,179][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 05:24:46,179][129146] FPS: 336122.19
[36m[2023-06-25 05:24:50,958][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:24:50,958][129146] Reward + Measures: [[704.49749618   0.22594632   0.75428468   0.16311099   0.57130933]]
[37m[1m[2023-06-25 05:24:50,958][129146] Max Reward on eval: 704.4974961798312
[37m[1m[2023-06-25 05:24:50,959][129146] Min Reward on eval: 704.4974961798312
[37m[1m[2023-06-25 05:24:50,959][129146] Mean Reward across all agents: 704.4974961798312
[37m[1m[2023-06-25 05:24:50,959][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:24:56,425][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:24:56,426][129146] Reward + Measures: [[666.12099365   0.266        0.7956       0.14049999   0.57380003]
[37m[1m [819.66209784   0.22669999   0.67030001   0.12809999   0.45000002]
[37m[1m [472.69772247   0.35489997   0.7446       0.33200002   0.6552    ]
[37m[1m ...
[37m[1m [438.09835998   0.38209999   0.63530004   0.28130004   0.53619999]
[37m[1m [645.29265526   0.20060001   0.65669996   0.15000001   0.49449998]
[37m[1m [980.06816965   0.24099998   0.63050002   0.13789999   0.3222    ]]
[37m[1m[2023-06-25 05:24:56,426][129146] Max Reward on eval: 1153.746524087782
[37m[1m[2023-06-25 05:24:56,426][129146] Min Reward on eval: 25.659057851252147
[37m[1m[2023-06-25 05:24:56,427][129146] Mean Reward across all agents: 694.6185349204726
[37m[1m[2023-06-25 05:24:56,427][129146] Average Trajectory Length: 998.1646666666667
[36m[2023-06-25 05:24:56,433][129146] mean_value=80.3065935409712, max_value=1360.9433223492815
[37m[1m[2023-06-25 05:24:56,436][129146] New mean coefficients: [[ 0.02126614 -0.27287972  0.8167805  -0.09504765  0.31718522]]
[37m[1m[2023-06-25 05:24:56,437][129146] Moving the mean solution point...
[36m[2023-06-25 05:25:06,125][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 05:25:06,126][129146] FPS: 396417.67
[36m[2023-06-25 05:25:06,128][129146] itr=468, itrs=2000, Progress: 23.40%
[36m[2023-06-25 05:25:17,528][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 05:25:17,528][129146] FPS: 337350.34
[36m[2023-06-25 05:25:22,267][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:25:22,268][129146] Reward + Measures: [[664.94499712   0.25584835   0.82915497   0.151897     0.68909162]]
[37m[1m[2023-06-25 05:25:22,268][129146] Max Reward on eval: 664.9449971150577
[37m[1m[2023-06-25 05:25:22,268][129146] Min Reward on eval: 664.9449971150577
[37m[1m[2023-06-25 05:25:22,268][129146] Mean Reward across all agents: 664.9449971150577
[37m[1m[2023-06-25 05:25:22,268][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:25:27,712][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:25:27,712][129146] Reward + Measures: [[552.53770006   0.43169999   0.77640003   0.35630003   0.74680001]
[37m[1m [694.91663913   0.23280001   0.86119998   0.15840001   0.69480002]
[37m[1m [752.82925051   0.20330003   0.77599996   0.16990001   0.5187    ]
[37m[1m ...
[37m[1m [483.43716427   0.24969999   0.6426       0.20320001   0.48220006]
[37m[1m [669.86750159   0.20030001   0.67150003   0.22620001   0.39809999]
[37m[1m [492.26928659   0.20780002   0.59420002   0.18719999   0.34560001]]
[37m[1m[2023-06-25 05:25:27,713][129146] Max Reward on eval: 972.7046433144249
[37m[1m[2023-06-25 05:25:27,713][129146] Min Reward on eval: 254.79729462176329
[37m[1m[2023-06-25 05:25:27,713][129146] Mean Reward across all agents: 677.3167496622494
[37m[1m[2023-06-25 05:25:27,713][129146] Average Trajectory Length: 997.7343333333333
[36m[2023-06-25 05:25:27,719][129146] mean_value=192.0517700672622, max_value=1182.3436510626052
[37m[1m[2023-06-25 05:25:27,722][129146] New mean coefficients: [[ 0.02133369  0.15844473  1.3366511  -0.07302617  0.00438514]]
[37m[1m[2023-06-25 05:25:27,723][129146] Moving the mean solution point...
[36m[2023-06-25 05:25:37,431][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 05:25:37,431][129146] FPS: 395627.77
[36m[2023-06-25 05:25:37,433][129146] itr=469, itrs=2000, Progress: 23.45%
[36m[2023-06-25 05:25:49,063][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 05:25:49,064][129146] FPS: 330664.44
[36m[2023-06-25 05:25:53,947][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:25:53,947][129146] Reward + Measures: [[683.2456023    0.27726632   0.87207097   0.13574333   0.74915028]]
[37m[1m[2023-06-25 05:25:53,947][129146] Max Reward on eval: 683.2456022979374
[37m[1m[2023-06-25 05:25:53,948][129146] Min Reward on eval: 683.2456022979374
[37m[1m[2023-06-25 05:25:53,948][129146] Mean Reward across all agents: 683.2456022979374
[37m[1m[2023-06-25 05:25:53,948][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:25:59,287][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:25:59,287][129146] Reward + Measures: [[481.15457409   0.37330002   0.51109999   0.3996       0.39769998]
[37m[1m [689.15004889   0.24879999   0.82159996   0.22409999   0.72439998]
[37m[1m [594.027706     0.3987       0.90480006   0.18869999   0.80890006]
[37m[1m ...
[37m[1m [728.23495046   0.29280001   0.63330001   0.1539       0.4443    ]
[37m[1m [603.25766766   0.27669999   0.68440002   0.25390002   0.56560004]
[37m[1m [554.24505085   0.38680187   0.57442451   0.12815472   0.38356793]]
[37m[1m[2023-06-25 05:25:59,288][129146] Max Reward on eval: 913.8302666140138
[37m[1m[2023-06-25 05:25:59,288][129146] Min Reward on eval: 245.01672847516164
[37m[1m[2023-06-25 05:25:59,288][129146] Mean Reward across all agents: 645.3328390379678
[37m[1m[2023-06-25 05:25:59,288][129146] Average Trajectory Length: 998.4216666666666
[36m[2023-06-25 05:25:59,298][129146] mean_value=376.24427550644117, max_value=1145.0477163979783
[37m[1m[2023-06-25 05:25:59,301][129146] New mean coefficients: [[-0.08064961 -0.03827834  1.657139   -0.16409896 -0.20812602]]
[37m[1m[2023-06-25 05:25:59,302][129146] Moving the mean solution point...
[36m[2023-06-25 05:26:09,219][129146] train() took 9.92 seconds to complete
[36m[2023-06-25 05:26:09,220][129146] FPS: 387254.63
[36m[2023-06-25 05:26:09,222][129146] itr=470, itrs=2000, Progress: 23.50%
[37m[1m[2023-06-25 05:26:13,675][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000450
[36m[2023-06-25 05:26:25,483][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 05:26:25,483][129146] FPS: 334328.51
[36m[2023-06-25 05:26:30,359][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:26:30,359][129146] Reward + Measures: [[678.88970229   0.26652995   0.90786141   0.13565826   0.80849737]]
[37m[1m[2023-06-25 05:26:30,359][129146] Max Reward on eval: 678.8897022864322
[37m[1m[2023-06-25 05:26:30,359][129146] Min Reward on eval: 678.8897022864322
[37m[1m[2023-06-25 05:26:30,360][129146] Mean Reward across all agents: 678.8897022864322
[37m[1m[2023-06-25 05:26:30,360][129146] Average Trajectory Length: 999.7099999999999
[36m[2023-06-25 05:26:35,792][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:26:35,793][129146] Reward + Measures: [[621.65167178   0.4862       0.58350003   0.44099998   0.51739997]
[37m[1m [660.77911482   0.36450002   0.60340005   0.3973       0.29079998]
[37m[1m [621.69993504   0.39489999   0.8545       0.23709999   0.69670004]
[37m[1m ...
[37m[1m [659.59659573   0.23550001   0.90979999   0.14139999   0.73070002]
[37m[1m [641.77360196   0.19319999   0.83230001   0.21970001   0.54720002]
[37m[1m [402.84163387   0.72109997   0.5952       0.76550001   0.67810005]]
[37m[1m[2023-06-25 05:26:35,793][129146] Max Reward on eval: 984.3152557309252
[37m[1m[2023-06-25 05:26:35,793][129146] Min Reward on eval: -443.786516619852
[37m[1m[2023-06-25 05:26:35,793][129146] Mean Reward across all agents: 571.2226549890834
[37m[1m[2023-06-25 05:26:35,794][129146] Average Trajectory Length: 999.3706666666666
[36m[2023-06-25 05:26:35,802][129146] mean_value=328.14297013394616, max_value=1329.498602767056
[37m[1m[2023-06-25 05:26:35,805][129146] New mean coefficients: [[-0.05434536 -0.16588211  1.7891967  -0.13142768 -0.23149453]]
[37m[1m[2023-06-25 05:26:35,806][129146] Moving the mean solution point...
[36m[2023-06-25 05:26:45,631][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 05:26:45,631][129146] FPS: 390941.10
[36m[2023-06-25 05:26:45,634][129146] itr=471, itrs=2000, Progress: 23.55%
[36m[2023-06-25 05:26:57,165][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 05:26:57,166][129146] FPS: 333512.66
[36m[2023-06-25 05:27:01,983][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:27:01,984][129146] Reward + Measures: [[680.08186593   0.26276693   0.91991991   0.12133996   0.81123906]]
[37m[1m[2023-06-25 05:27:01,984][129146] Max Reward on eval: 680.0818659336178
[37m[1m[2023-06-25 05:27:01,984][129146] Min Reward on eval: 680.0818659336178
[37m[1m[2023-06-25 05:27:01,984][129146] Mean Reward across all agents: 680.0818659336178
[37m[1m[2023-06-25 05:27:01,985][129146] Average Trajectory Length: 999.693
[36m[2023-06-25 05:27:07,515][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:27:07,515][129146] Reward + Measures: [[ 740.6077054     0.1929        0.83810008    0.2066        0.54870003]
[37m[1m [ 880.75267609    0.20470002    0.8075        0.22460003    0.46479997]
[37m[1m [1196.65639793    0.27490002    0.71449995    0.26190001    0.36199999]
[37m[1m ...
[37m[1m [ 692.81136022    0.30020002    0.87819999    0.16870002    0.71889991]
[37m[1m [ 710.39155768    0.19480002    0.88669997    0.17819999    0.6573    ]
[37m[1m [ 626.55732764    0.32440001    0.91600001    0.15100001    0.83630002]]
[37m[1m[2023-06-25 05:27:07,516][129146] Max Reward on eval: 1527.861729432654
[37m[1m[2023-06-25 05:27:07,516][129146] Min Reward on eval: 27.235003805637824
[37m[1m[2023-06-25 05:27:07,516][129146] Mean Reward across all agents: 814.3497828313725
[37m[1m[2023-06-25 05:27:07,516][129146] Average Trajectory Length: 998.765
[36m[2023-06-25 05:27:07,526][129146] mean_value=204.3382258556042, max_value=1407.7295134206156
[37m[1m[2023-06-25 05:27:07,528][129146] New mean coefficients: [[ 0.01029774 -0.13787304  1.930134    0.04590705 -0.01009591]]
[37m[1m[2023-06-25 05:27:07,529][129146] Moving the mean solution point...
[36m[2023-06-25 05:27:17,296][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 05:27:17,296][129146] FPS: 393251.86
[36m[2023-06-25 05:27:17,298][129146] itr=472, itrs=2000, Progress: 23.60%
[36m[2023-06-25 05:27:28,715][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 05:27:28,715][129146] FPS: 336864.45
[36m[2023-06-25 05:27:33,533][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:27:33,533][129146] Reward + Measures: [[679.1205099    0.25238401   0.93952668   0.11763766   0.84409767]]
[37m[1m[2023-06-25 05:27:33,533][129146] Max Reward on eval: 679.1205099045861
[37m[1m[2023-06-25 05:27:33,533][129146] Min Reward on eval: 679.1205099045861
[37m[1m[2023-06-25 05:27:33,534][129146] Mean Reward across all agents: 679.1205099045861
[37m[1m[2023-06-25 05:27:33,534][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:27:38,647][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:27:38,647][129146] Reward + Measures: [[1043.55157704    0.1974        0.57509995    0.19140001    0.24850002]
[37m[1m [1136.27170623    0.22730003    0.69679999    0.22020002    0.34810001]
[37m[1m [ 960.567279      0.19820002    0.51750004    0.19929999    0.28280002]
[37m[1m ...
[37m[1m [ 636.72832943    0.30140001    0.94670004    0.13939999    0.88549995]
[37m[1m [ 752.81859987    0.205         0.84230006    0.15900002    0.55180001]
[37m[1m [ 712.4354395     0.205         0.8646        0.15000001    0.65900004]]
[37m[1m[2023-06-25 05:27:38,647][129146] Max Reward on eval: 1177.566695954348
[37m[1m[2023-06-25 05:27:38,648][129146] Min Reward on eval: 116.72197476139408
[37m[1m[2023-06-25 05:27:38,648][129146] Mean Reward across all agents: 696.7295400315719
[37m[1m[2023-06-25 05:27:38,648][129146] Average Trajectory Length: 999.0036666666666
[36m[2023-06-25 05:27:38,655][129146] mean_value=229.11717570893651, max_value=1233.6812746663695
[37m[1m[2023-06-25 05:27:38,658][129146] New mean coefficients: [[0.04875937 0.43543017 1.5184711  0.01649537 0.10038991]]
[37m[1m[2023-06-25 05:27:38,659][129146] Moving the mean solution point...
[36m[2023-06-25 05:27:48,358][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 05:27:48,359][129146] FPS: 395984.96
[36m[2023-06-25 05:27:48,361][129146] itr=473, itrs=2000, Progress: 23.65%
[36m[2023-06-25 05:27:59,900][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 05:27:59,900][129146] FPS: 333411.13
[36m[2023-06-25 05:28:04,599][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:28:04,600][129146] Reward + Measures: [[718.19483009   0.30913064   0.94845963   0.09769767   0.84431297]]
[37m[1m[2023-06-25 05:28:04,600][129146] Max Reward on eval: 718.1948300947276
[37m[1m[2023-06-25 05:28:04,600][129146] Min Reward on eval: 718.1948300947276
[37m[1m[2023-06-25 05:28:04,600][129146] Mean Reward across all agents: 718.1948300947276
[37m[1m[2023-06-25 05:28:04,601][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:28:10,172][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:28:10,173][129146] Reward + Measures: [[715.50547979   0.34520003   0.91159993   0.2098       0.80380005]
[37m[1m [708.25201006   0.2951       0.90240002   0.18960001   0.78459996]
[37m[1m [663.89147047   0.33810002   0.91800004   0.08280001   0.76010001]
[37m[1m ...
[37m[1m [710.03379411   0.35139999   0.93349993   0.13         0.79290003]
[37m[1m [691.99707876   0.31210002   0.88080007   0.19620001   0.69690007]
[37m[1m [717.73979189   0.30320001   0.9429       0.1718       0.84390002]]
[37m[1m[2023-06-25 05:28:10,173][129146] Max Reward on eval: 1299.6375282367692
[37m[1m[2023-06-25 05:28:10,173][129146] Min Reward on eval: 514.0662084820506
[37m[1m[2023-06-25 05:28:10,174][129146] Mean Reward across all agents: 699.3597219867426
[37m[1m[2023-06-25 05:28:10,174][129146] Average Trajectory Length: 999.6773333333333
[36m[2023-06-25 05:28:10,182][129146] mean_value=251.21431236239525, max_value=1227.2137710836832
[37m[1m[2023-06-25 05:28:10,185][129146] New mean coefficients: [[0.10909662 0.3312858  1.6906312  0.01529345 0.2616153 ]]
[37m[1m[2023-06-25 05:28:10,187][129146] Moving the mean solution point...
[36m[2023-06-25 05:28:19,934][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 05:28:19,934][129146] FPS: 394038.54
[36m[2023-06-25 05:28:19,936][129146] itr=474, itrs=2000, Progress: 23.70%
[36m[2023-06-25 05:28:31,490][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 05:28:31,491][129146] FPS: 332864.53
[36m[2023-06-25 05:28:36,299][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:28:36,299][129146] Reward + Measures: [[720.73618649   0.34963664   0.95728797   0.099007     0.881468  ]]
[37m[1m[2023-06-25 05:28:36,300][129146] Max Reward on eval: 720.7361864880966
[37m[1m[2023-06-25 05:28:36,300][129146] Min Reward on eval: 720.7361864880966
[37m[1m[2023-06-25 05:28:36,300][129146] Mean Reward across all agents: 720.7361864880966
[37m[1m[2023-06-25 05:28:36,300][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:28:41,805][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:28:41,805][129146] Reward + Measures: [[608.65269173   0.55970001   0.51599997   0.53560001   0.4154    ]
[37m[1m [612.80736508   0.2474       0.88199997   0.26230001   0.74739999]
[37m[1m [756.13346517   0.39719999   0.80039996   0.3046       0.59630007]
[37m[1m ...
[37m[1m [840.70771568   0.33670002   0.8933       0.20940001   0.72980005]
[37m[1m [794.18890299   0.36300001   0.94519997   0.11700001   0.82559997]
[37m[1m [618.88217341   0.3486       0.86709994   0.25310001   0.73800004]]
[37m[1m[2023-06-25 05:28:41,805][129146] Max Reward on eval: 1284.517092854029
[37m[1m[2023-06-25 05:28:41,806][129146] Min Reward on eval: -18.16530002761865
[37m[1m[2023-06-25 05:28:41,806][129146] Mean Reward across all agents: 700.0051133169682
[37m[1m[2023-06-25 05:28:41,806][129146] Average Trajectory Length: 999.6796666666667
[36m[2023-06-25 05:28:41,815][129146] mean_value=267.6209668302608, max_value=1394.5875577962258
[37m[1m[2023-06-25 05:28:41,818][129146] New mean coefficients: [[ 0.07953273  0.25323308  1.7203838  -0.09646979  0.16011682]]
[37m[1m[2023-06-25 05:28:41,819][129146] Moving the mean solution point...
[36m[2023-06-25 05:28:51,624][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 05:28:51,625][129146] FPS: 391679.49
[36m[2023-06-25 05:28:51,627][129146] itr=475, itrs=2000, Progress: 23.75%
[36m[2023-06-25 05:29:03,028][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 05:29:03,028][129146] FPS: 337349.68
[36m[2023-06-25 05:29:07,918][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:29:07,919][129146] Reward + Measures: [[753.61532574   0.36027765   0.96407634   0.08839567   0.89372003]]
[37m[1m[2023-06-25 05:29:07,919][129146] Max Reward on eval: 753.6153257389843
[37m[1m[2023-06-25 05:29:07,919][129146] Min Reward on eval: 753.6153257389843
[37m[1m[2023-06-25 05:29:07,919][129146] Mean Reward across all agents: 753.6153257389843
[37m[1m[2023-06-25 05:29:07,920][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:29:13,455][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:29:13,455][129146] Reward + Measures: [[644.91960338   0.36770001   0.87470001   0.1141       0.76639998]
[37m[1m [756.8724181    0.24720001   0.96020001   0.0983       0.89969999]
[37m[1m [668.11028863   0.43510005   0.91890001   0.1103       0.80949992]
[37m[1m ...
[37m[1m [697.24417828   0.33610001   0.8531       0.1314       0.79070008]
[37m[1m [757.38970116   0.2242       0.94259995   0.18880001   0.85509998]
[37m[1m [702.06414687   0.42640001   0.91409999   0.30829999   0.76349998]]
[37m[1m[2023-06-25 05:29:13,456][129146] Max Reward on eval: 962.6338708505384
[37m[1m[2023-06-25 05:29:13,456][129146] Min Reward on eval: 536.5247452747193
[37m[1m[2023-06-25 05:29:13,456][129146] Mean Reward across all agents: 717.0592356616155
[37m[1m[2023-06-25 05:29:13,456][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:29:13,467][129146] mean_value=437.9569099179758, max_value=1205.014250410674
[37m[1m[2023-06-25 05:29:13,469][129146] New mean coefficients: [[ 0.05504411 -0.03745678  2.1379533  -0.160714   -0.03409353]]
[37m[1m[2023-06-25 05:29:13,471][129146] Moving the mean solution point...
[36m[2023-06-25 05:29:23,134][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 05:29:23,134][129146] FPS: 397447.70
[36m[2023-06-25 05:29:23,136][129146] itr=476, itrs=2000, Progress: 23.80%
[36m[2023-06-25 05:29:34,561][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 05:29:34,561][129146] FPS: 336614.84
[36m[2023-06-25 05:29:39,357][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:29:39,357][129146] Reward + Measures: [[750.52793958   0.35743499   0.96775067   0.07517533   0.89955437]]
[37m[1m[2023-06-25 05:29:39,357][129146] Max Reward on eval: 750.5279395829713
[37m[1m[2023-06-25 05:29:39,357][129146] Min Reward on eval: 750.5279395829713
[37m[1m[2023-06-25 05:29:39,358][129146] Mean Reward across all agents: 750.5279395829713
[37m[1m[2023-06-25 05:29:39,358][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:29:44,759][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:29:44,760][129146] Reward + Measures: [[738.48957048   0.3707       0.96050006   0.0464       0.80290002]
[37m[1m [718.70695414   0.27080002   0.94960004   0.20680001   0.86379999]
[37m[1m [688.83398696   0.39620003   0.93160003   0.1605       0.71859998]
[37m[1m ...
[37m[1m [675.13360326   0.1477       0.9752       0.1657       0.89750004]
[37m[1m [756.62131869   0.39360002   0.90039998   0.0619       0.61810005]
[37m[1m [746.52696918   0.35950002   0.94769996   0.06480001   0.7809    ]]
[37m[1m[2023-06-25 05:29:44,760][129146] Max Reward on eval: 810.037197442993
[37m[1m[2023-06-25 05:29:44,760][129146] Min Reward on eval: 403.65666643155856
[37m[1m[2023-06-25 05:29:44,760][129146] Mean Reward across all agents: 701.3205420752959
[37m[1m[2023-06-25 05:29:44,761][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:29:44,768][129146] mean_value=310.0723954766362, max_value=1282.5408175024902
[37m[1m[2023-06-25 05:29:44,771][129146] New mean coefficients: [[-0.06992614 -0.04419724  1.7887553  -0.36290067 -0.28119385]]
[37m[1m[2023-06-25 05:29:44,772][129146] Moving the mean solution point...
[36m[2023-06-25 05:29:54,519][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 05:29:54,519][129146] FPS: 394039.25
[36m[2023-06-25 05:29:54,521][129146] itr=477, itrs=2000, Progress: 23.85%
[36m[2023-06-25 05:30:06,123][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 05:30:06,124][129146] FPS: 331477.68
[36m[2023-06-25 05:30:10,856][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:30:10,857][129146] Reward + Measures: [[746.17991288   0.38855869   0.96618861   0.08403033   0.87950534]]
[37m[1m[2023-06-25 05:30:10,857][129146] Max Reward on eval: 746.1799128832823
[37m[1m[2023-06-25 05:30:10,857][129146] Min Reward on eval: 746.1799128832823
[37m[1m[2023-06-25 05:30:10,857][129146] Mean Reward across all agents: 746.1799128832823
[37m[1m[2023-06-25 05:30:10,858][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:30:16,294][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:30:16,295][129146] Reward + Measures: [[660.25660572   0.54430002   0.89379996   0.52160001   0.8071    ]
[37m[1m [713.30744092   0.44619998   0.93739998   0.16590001   0.86970007]
[37m[1m [731.85309895   0.43380004   0.95720005   0.064        0.8757    ]
[37m[1m ...
[37m[1m [645.59219657   0.57849997   0.95300007   0.41120002   0.87220001]
[37m[1m [704.2526944    0.31619999   0.91830009   0.1132       0.82419997]
[37m[1m [729.39361999   0.29460001   0.94989997   0.11360001   0.83150005]]
[37m[1m[2023-06-25 05:30:16,295][129146] Max Reward on eval: 766.8987015149789
[37m[1m[2023-06-25 05:30:16,295][129146] Min Reward on eval: 181.8819674180355
[37m[1m[2023-06-25 05:30:16,295][129146] Mean Reward across all agents: 658.9104201286606
[37m[1m[2023-06-25 05:30:16,296][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:30:16,302][129146] mean_value=253.72187986222775, max_value=1240.79310744626
[37m[1m[2023-06-25 05:30:16,305][129146] New mean coefficients: [[-0.03449557 -0.25182706  1.9728602  -0.39162147 -0.3187119 ]]
[37m[1m[2023-06-25 05:30:16,306][129146] Moving the mean solution point...
[36m[2023-06-25 05:30:26,044][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 05:30:26,044][129146] FPS: 394421.41
[36m[2023-06-25 05:30:26,046][129146] itr=478, itrs=2000, Progress: 23.90%
[36m[2023-06-25 05:30:37,631][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 05:30:37,632][129146] FPS: 332034.25
[36m[2023-06-25 05:30:42,505][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:30:42,506][129146] Reward + Measures: [[752.28657852   0.31489199   0.96972191   0.08723833   0.88012791]]
[37m[1m[2023-06-25 05:30:42,506][129146] Max Reward on eval: 752.2865785159094
[37m[1m[2023-06-25 05:30:42,506][129146] Min Reward on eval: 752.2865785159094
[37m[1m[2023-06-25 05:30:42,506][129146] Mean Reward across all agents: 752.2865785159094
[37m[1m[2023-06-25 05:30:42,507][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:30:48,326][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:30:48,326][129146] Reward + Measures: [[732.70667983   0.26750001   0.97570002   0.0621       0.866     ]
[37m[1m [584.39993148   0.20299999   0.89160007   0.14130001   0.77209997]
[37m[1m [650.30396213   0.24320002   0.93440002   0.0958       0.78389996]
[37m[1m ...
[37m[1m [691.03521306   0.24450003   0.95970005   0.0834       0.86250001]
[37m[1m [715.11640092   0.3793       0.95639992   0.18819998   0.86250001]
[37m[1m [687.53998013   0.24920002   0.94849998   0.09260001   0.81480008]]
[37m[1m[2023-06-25 05:30:48,326][129146] Max Reward on eval: 757.8014762823237
[37m[1m[2023-06-25 05:30:48,327][129146] Min Reward on eval: 474.8770052429871
[37m[1m[2023-06-25 05:30:48,327][129146] Mean Reward across all agents: 683.9355138506005
[37m[1m[2023-06-25 05:30:48,327][129146] Average Trajectory Length: 999.6856666666666
[36m[2023-06-25 05:30:48,331][129146] mean_value=57.825511068167835, max_value=1103.9448156762635
[37m[1m[2023-06-25 05:30:48,334][129146] New mean coefficients: [[ 0.07604742 -0.21990511  1.8998208  -0.20509395 -0.10675664]]
[37m[1m[2023-06-25 05:30:48,335][129146] Moving the mean solution point...
[36m[2023-06-25 05:30:58,177][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 05:30:58,177][129146] FPS: 390241.72
[36m[2023-06-25 05:30:58,179][129146] itr=479, itrs=2000, Progress: 23.95%
[36m[2023-06-25 05:31:09,650][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 05:31:09,650][129146] FPS: 335318.59
[36m[2023-06-25 05:31:14,555][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:31:14,561][129146] Reward + Measures: [[753.11840971   0.28799531   0.96867734   0.09530567   0.86971027]]
[37m[1m[2023-06-25 05:31:14,561][129146] Max Reward on eval: 753.1184097133586
[37m[1m[2023-06-25 05:31:14,561][129146] Min Reward on eval: 753.1184097133586
[37m[1m[2023-06-25 05:31:14,561][129146] Mean Reward across all agents: 753.1184097133586
[37m[1m[2023-06-25 05:31:14,562][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:31:20,110][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:31:20,111][129146] Reward + Measures: [[724.42343655   0.37080002   0.94870007   0.16590001   0.86429995]
[37m[1m [647.9459796    0.41630003   0.91720003   0.15970001   0.8634001 ]
[37m[1m [359.6313184    0.68160003   0.8854       0.70559996   0.86669999]
[37m[1m ...
[37m[1m [762.17022938   0.29969999   0.96950001   0.1142       0.84320003]
[37m[1m [668.9710832    0.52079999   0.95789999   0.24340001   0.88350004]
[37m[1m [709.29380238   0.25999999   0.9379999    0.10020001   0.85360003]]
[37m[1m[2023-06-25 05:31:20,111][129146] Max Reward on eval: 784.7242964695906
[37m[1m[2023-06-25 05:31:20,111][129146] Min Reward on eval: 18.383375425942358
[37m[1m[2023-06-25 05:31:20,112][129146] Mean Reward across all agents: 647.0008722421425
[37m[1m[2023-06-25 05:31:20,112][129146] Average Trajectory Length: 999.5369999999999
[36m[2023-06-25 05:31:20,119][129146] mean_value=225.47888763322874, max_value=1274.58983273271
[37m[1m[2023-06-25 05:31:20,122][129146] New mean coefficients: [[ 0.11585254 -0.14730318  1.9948064  -0.15881553 -0.13924651]]
[37m[1m[2023-06-25 05:31:20,123][129146] Moving the mean solution point...
[36m[2023-06-25 05:31:29,950][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 05:31:29,950][129146] FPS: 390826.31
[36m[2023-06-25 05:31:29,953][129146] itr=480, itrs=2000, Progress: 24.00%
[37m[1m[2023-06-25 05:31:34,310][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000460
[36m[2023-06-25 05:31:46,175][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 05:31:46,176][129146] FPS: 332415.75
[36m[2023-06-25 05:31:51,120][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:31:51,121][129146] Reward + Measures: [[766.01686359   0.27059335   0.97122502   0.09944367   0.84388596]]
[37m[1m[2023-06-25 05:31:51,121][129146] Max Reward on eval: 766.0168635883288
[37m[1m[2023-06-25 05:31:51,121][129146] Min Reward on eval: 766.0168635883288
[37m[1m[2023-06-25 05:31:51,121][129146] Mean Reward across all agents: 766.0168635883288
[37m[1m[2023-06-25 05:31:51,122][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:31:56,775][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:31:56,776][129146] Reward + Measures: [[676.67443362   0.48559999   0.94250005   0.4289       0.83690006]
[37m[1m [753.56252212   0.23959999   0.96079999   0.0951       0.84310001]
[37m[1m [551.40601648   0.58380002   0.88679999   0.53290004   0.74450004]
[37m[1m ...
[37m[1m [715.88618078   0.30140001   0.93360007   0.1983       0.75830001]
[37m[1m [729.93415405   0.31220004   0.97360003   0.14380001   0.82480001]
[37m[1m [739.19075864   0.29640001   0.94929999   0.17910001   0.83409995]]
[37m[1m[2023-06-25 05:31:56,781][129146] Max Reward on eval: 913.6627122556907
[37m[1m[2023-06-25 05:31:56,781][129146] Min Reward on eval: 506.9637882067007
[37m[1m[2023-06-25 05:31:56,781][129146] Mean Reward across all agents: 715.2334929149329
[37m[1m[2023-06-25 05:31:56,782][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:31:56,787][129146] mean_value=143.91440065522585, max_value=1147.5121783685522
[37m[1m[2023-06-25 05:31:56,790][129146] New mean coefficients: [[-0.04000293 -0.21040852  2.8933918  -0.42304394 -0.4511013 ]]
[37m[1m[2023-06-25 05:31:56,791][129146] Moving the mean solution point...
[36m[2023-06-25 05:32:06,598][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 05:32:06,598][129146] FPS: 391642.52
[36m[2023-06-25 05:32:06,600][129146] itr=481, itrs=2000, Progress: 24.05%
[36m[2023-06-25 05:32:18,207][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 05:32:18,207][129146] FPS: 331346.16
[36m[2023-06-25 05:32:22,525][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:32:22,526][129146] Reward + Measures: [[761.89266135   0.24443467   0.9698993    0.109013     0.81111199]]
[37m[1m[2023-06-25 05:32:22,526][129146] Max Reward on eval: 761.8926613503751
[37m[1m[2023-06-25 05:32:22,526][129146] Min Reward on eval: 761.8926613503751
[37m[1m[2023-06-25 05:32:22,526][129146] Mean Reward across all agents: 761.8926613503751
[37m[1m[2023-06-25 05:32:22,527][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:32:27,662][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:32:27,663][129146] Reward + Measures: [[753.10411363   0.22290002   0.97390002   0.0878       0.83249998]
[37m[1m [676.41539293   0.2933       0.94740003   0.20810001   0.78200001]
[37m[1m [734.40617001   0.17290001   0.95039999   0.16610001   0.77520001]
[37m[1m ...
[37m[1m [617.04292979   0.33570004   0.91449994   0.30240002   0.77579993]
[37m[1m [774.5871812    0.23440002   0.97229999   0.0957       0.80470002]
[37m[1m [734.59422337   0.23580001   0.95489997   0.12700002   0.78240007]]
[37m[1m[2023-06-25 05:32:27,663][129146] Max Reward on eval: 809.7501475071767
[37m[1m[2023-06-25 05:32:27,663][129146] Min Reward on eval: 449.40225506958086
[37m[1m[2023-06-25 05:32:27,664][129146] Mean Reward across all agents: 700.8582896779213
[37m[1m[2023-06-25 05:32:27,664][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:32:27,668][129146] mean_value=69.14416899268474, max_value=1212.3145898901857
[37m[1m[2023-06-25 05:32:27,671][129146] New mean coefficients: [[-0.23663273 -0.5440306   2.2737756  -0.5730021  -0.60211957]]
[37m[1m[2023-06-25 05:32:27,672][129146] Moving the mean solution point...
[36m[2023-06-25 05:32:37,344][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 05:32:37,344][129146] FPS: 397087.37
[36m[2023-06-25 05:32:37,346][129146] itr=482, itrs=2000, Progress: 24.10%
[36m[2023-06-25 05:32:48,802][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 05:32:48,802][129146] FPS: 335800.60
[36m[2023-06-25 05:32:53,546][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:32:53,546][129146] Reward + Measures: [[749.29689807   0.22106767   0.97164404   0.10481466   0.75327569]]
[37m[1m[2023-06-25 05:32:53,546][129146] Max Reward on eval: 749.2968980724866
[37m[1m[2023-06-25 05:32:53,546][129146] Min Reward on eval: 749.2968980724866
[37m[1m[2023-06-25 05:32:53,547][129146] Mean Reward across all agents: 749.2968980724866
[37m[1m[2023-06-25 05:32:53,547][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:32:59,031][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:32:59,032][129146] Reward + Measures: [[758.44322807   0.20940001   0.96950001   0.10400001   0.7507    ]
[37m[1m [730.41414017   0.28050002   0.9738       0.1031       0.83859998]
[37m[1m [746.9565712    0.2225       0.97399998   0.09249999   0.75240004]
[37m[1m ...
[37m[1m [770.21852918   0.21360002   0.97110003   0.094        0.72149998]
[37m[1m [762.50135153   0.21070002   0.9709       0.09790001   0.71920002]
[37m[1m [746.7377511    0.20609999   0.95110005   0.1105       0.69379997]]
[37m[1m[2023-06-25 05:32:59,032][129146] Max Reward on eval: 859.6337990198633
[37m[1m[2023-06-25 05:32:59,032][129146] Min Reward on eval: 666.5287183174165
[37m[1m[2023-06-25 05:32:59,033][129146] Mean Reward across all agents: 752.3418971550356
[37m[1m[2023-06-25 05:32:59,033][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:32:59,037][129146] mean_value=347.2304393250883, max_value=1200.524279904284
[37m[1m[2023-06-25 05:32:59,040][129146] New mean coefficients: [[-0.09267133 -0.41097504  3.3454833  -0.5880551  -0.7242428 ]]
[37m[1m[2023-06-25 05:32:59,041][129146] Moving the mean solution point...
[36m[2023-06-25 05:33:08,645][129146] train() took 9.60 seconds to complete
[36m[2023-06-25 05:33:08,646][129146] FPS: 399873.40
[36m[2023-06-25 05:33:08,648][129146] itr=483, itrs=2000, Progress: 24.15%
[36m[2023-06-25 05:33:20,067][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 05:33:20,068][129146] FPS: 336781.83
[36m[2023-06-25 05:33:24,855][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:33:24,856][129146] Reward + Measures: [[742.03268075   0.19480267   0.97247899   0.10769366   0.70828027]]
[37m[1m[2023-06-25 05:33:24,856][129146] Max Reward on eval: 742.0326807484723
[37m[1m[2023-06-25 05:33:24,856][129146] Min Reward on eval: 742.0326807484723
[37m[1m[2023-06-25 05:33:24,856][129146] Mean Reward across all agents: 742.0326807484723
[37m[1m[2023-06-25 05:33:24,857][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:33:30,298][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:33:30,299][129146] Reward + Measures: [[ 726.85201592    0.28449997    0.95430005    0.2034        0.6846    ]
[37m[1m [ 892.44231902    0.23989999    0.81129998    0.148         0.41240001]
[37m[1m [ 682.94008302    0.35170004    0.94480002    0.2685        0.75229996]
[37m[1m ...
[37m[1m [ 822.91925983    0.26950002    0.90230006    0.21370001    0.52920002]
[37m[1m [1093.13578907    0.24499999    0.7464        0.1269        0.29790002]
[37m[1m [ 701.59514329    0.33570001    0.93710005    0.29390001    0.67360002]]
[37m[1m[2023-06-25 05:33:30,303][129146] Max Reward on eval: 1120.417976490059
[37m[1m[2023-06-25 05:33:30,304][129146] Min Reward on eval: 398.8117035793606
[37m[1m[2023-06-25 05:33:30,304][129146] Mean Reward across all agents: 772.3284152510147
[37m[1m[2023-06-25 05:33:30,304][129146] Average Trajectory Length: 998.877
[36m[2023-06-25 05:33:30,313][129146] mean_value=564.3584554151313, max_value=1536.8657400148222
[37m[1m[2023-06-25 05:33:30,315][129146] New mean coefficients: [[ 0.02450438 -0.20952381  3.9783323  -0.52102137 -0.9830165 ]]
[37m[1m[2023-06-25 05:33:30,316][129146] Moving the mean solution point...
[36m[2023-06-25 05:33:40,054][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 05:33:40,054][129146] FPS: 394426.10
[36m[2023-06-25 05:33:40,056][129146] itr=484, itrs=2000, Progress: 24.20%
[36m[2023-06-25 05:33:51,594][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 05:33:51,594][129146] FPS: 333416.50
[36m[2023-06-25 05:33:56,450][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:33:56,451][129146] Reward + Measures: [[744.13603519   0.19274569   0.97308397   0.10303467   0.64137   ]]
[37m[1m[2023-06-25 05:33:56,451][129146] Max Reward on eval: 744.1360351924395
[37m[1m[2023-06-25 05:33:56,451][129146] Min Reward on eval: 744.1360351924395
[37m[1m[2023-06-25 05:33:56,451][129146] Mean Reward across all agents: 744.1360351924395
[37m[1m[2023-06-25 05:33:56,452][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:34:01,957][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:34:01,958][129146] Reward + Measures: [[576.14210532   0.47790003   0.60780007   0.13240001   0.36810002]
[37m[1m [485.00179305   0.50010008   0.56         0.1629       0.36680004]
[37m[1m [653.77290709   0.38420001   0.87050003   0.0557       0.75089997]
[37m[1m ...
[37m[1m [854.61937029   0.29980001   0.84259999   0.0922       0.49349999]
[37m[1m [318.18939182   0.43460003   0.40920001   0.32080001   0.31360003]
[37m[1m [732.39627591   0.2277       0.94420004   0.07840001   0.70620006]]
[37m[1m[2023-06-25 05:34:01,958][129146] Max Reward on eval: 952.6444593744352
[37m[1m[2023-06-25 05:34:01,959][129146] Min Reward on eval: -54.92794076869613
[37m[1m[2023-06-25 05:34:01,959][129146] Mean Reward across all agents: 558.466843360485
[37m[1m[2023-06-25 05:34:01,959][129146] Average Trajectory Length: 992.0443333333333
[36m[2023-06-25 05:34:01,963][129146] mean_value=-249.49949438593453, max_value=1452.6444593744352
[37m[1m[2023-06-25 05:34:01,966][129146] New mean coefficients: [[-0.03829908 -0.46718544  3.2321932  -0.45398492 -0.69806063]]
[37m[1m[2023-06-25 05:34:01,967][129146] Moving the mean solution point...
[36m[2023-06-25 05:34:11,811][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 05:34:11,811][129146] FPS: 390173.03
[36m[2023-06-25 05:34:11,813][129146] itr=485, itrs=2000, Progress: 24.25%
[36m[2023-06-25 05:34:23,498][129146] train() took 11.67 seconds to complete
[36m[2023-06-25 05:34:23,498][129146] FPS: 329135.79
[36m[2023-06-25 05:34:28,338][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:34:28,338][129146] Reward + Measures: [[738.86633479   0.19867533   0.97350502   0.09911667   0.56116968]]
[37m[1m[2023-06-25 05:34:28,338][129146] Max Reward on eval: 738.8663347887452
[37m[1m[2023-06-25 05:34:28,339][129146] Min Reward on eval: 738.8663347887452
[37m[1m[2023-06-25 05:34:28,339][129146] Mean Reward across all agents: 738.8663347887452
[37m[1m[2023-06-25 05:34:28,339][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:34:33,961][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:34:33,962][129146] Reward + Measures: [[671.15215139   0.29320002   0.90700001   0.20780002   0.58149999]
[37m[1m [691.78937665   0.2395       0.95739996   0.1049       0.51189995]
[37m[1m [591.47917462   0.52550006   0.84820002   0.23480001   0.4729    ]
[37m[1m ...
[37m[1m [704.90498395   0.2148       0.9016       0.12330001   0.39140001]
[37m[1m [651.95752526   0.42420003   0.88920003   0.4192       0.66530007]
[37m[1m [729.69186813   0.21440001   0.96460003   0.10830001   0.51450008]]
[37m[1m[2023-06-25 05:34:33,962][129146] Max Reward on eval: 1026.4674229743657
[37m[1m[2023-06-25 05:34:33,962][129146] Min Reward on eval: 243.7692755307653
[37m[1m[2023-06-25 05:34:33,963][129146] Mean Reward across all agents: 645.4294504493457
[37m[1m[2023-06-25 05:34:33,963][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:34:33,973][129146] mean_value=574.9884286561338, max_value=1360.7993937310298
[37m[1m[2023-06-25 05:34:33,976][129146] New mean coefficients: [[-0.05235882 -0.11459005  2.6831102  -0.47566792 -0.6645824 ]]
[37m[1m[2023-06-25 05:34:33,977][129146] Moving the mean solution point...
[36m[2023-06-25 05:34:43,782][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 05:34:43,783][129146] FPS: 391684.69
[36m[2023-06-25 05:34:43,785][129146] itr=486, itrs=2000, Progress: 24.30%
[36m[2023-06-25 05:34:55,430][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 05:34:55,430][129146] FPS: 330254.06
[36m[2023-06-25 05:35:00,299][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:35:00,299][129146] Reward + Measures: [[720.23497552   0.20160234   0.97314763   0.09265867   0.48435098]]
[37m[1m[2023-06-25 05:35:00,299][129146] Max Reward on eval: 720.2349755225138
[37m[1m[2023-06-25 05:35:00,300][129146] Min Reward on eval: 720.2349755225138
[37m[1m[2023-06-25 05:35:00,300][129146] Mean Reward across all agents: 720.2349755225138
[37m[1m[2023-06-25 05:35:00,300][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:35:05,773][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:35:05,774][129146] Reward + Measures: [[731.21326916   0.23099999   0.97039998   0.106        0.60350001]
[37m[1m [721.63417139   0.36260003   0.84009999   0.0958       0.55580002]
[37m[1m [711.16536578   0.36950001   0.89590007   0.0875       0.66000003]
[37m[1m ...
[37m[1m [723.55690672   0.23280001   0.9533       0.0927       0.58790004]
[37m[1m [761.03316638   0.24920002   0.95500004   0.1186       0.49340001]
[37m[1m [735.99198692   0.2098       0.96540004   0.0933       0.55860007]]
[37m[1m[2023-06-25 05:35:05,774][129146] Max Reward on eval: 966.9056860037963
[37m[1m[2023-06-25 05:35:05,775][129146] Min Reward on eval: 462.66554313269444
[37m[1m[2023-06-25 05:35:05,775][129146] Mean Reward across all agents: 720.6733448007681
[37m[1m[2023-06-25 05:35:05,775][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:35:05,781][129146] mean_value=248.11194830926107, max_value=1221.9820888257352
[37m[1m[2023-06-25 05:35:05,784][129146] New mean coefficients: [[-0.12746824 -0.39844152  2.1325583  -0.43151358 -0.45782632]]
[37m[1m[2023-06-25 05:35:05,785][129146] Moving the mean solution point...
[36m[2023-06-25 05:35:15,450][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 05:35:15,450][129146] FPS: 397357.49
[36m[2023-06-25 05:35:15,453][129146] itr=487, itrs=2000, Progress: 24.35%
[36m[2023-06-25 05:35:27,121][129146] train() took 11.65 seconds to complete
[36m[2023-06-25 05:35:27,121][129146] FPS: 329637.61
[36m[2023-06-25 05:35:31,993][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:35:31,999][129146] Reward + Measures: [[651.23450812   0.23217934   0.98009539   0.06873167   0.43642864]]
[37m[1m[2023-06-25 05:35:31,999][129146] Max Reward on eval: 651.2345081170491
[37m[1m[2023-06-25 05:35:32,000][129146] Min Reward on eval: 651.2345081170491
[37m[1m[2023-06-25 05:35:32,001][129146] Mean Reward across all agents: 651.2345081170491
[37m[1m[2023-06-25 05:35:32,001][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:35:37,576][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:35:37,576][129146] Reward + Measures: [[687.56736304   0.17800002   0.9526       0.1127       0.58660001]
[37m[1m [723.43992789   0.27690002   0.97410005   0.05430001   0.46529999]
[37m[1m [497.18435153   0.161        0.76689994   0.2332       0.51609999]
[37m[1m ...
[37m[1m [369.83850317   0.1389       0.76530004   0.1957       0.54640001]
[37m[1m [689.77835941   0.19149999   0.96390003   0.1076       0.58529997]
[37m[1m [605.83226143   0.2798       0.90790004   0.40089998   0.588     ]]
[37m[1m[2023-06-25 05:35:37,577][129146] Max Reward on eval: 824.45203442981
[37m[1m[2023-06-25 05:35:37,577][129146] Min Reward on eval: 366.85108420777834
[37m[1m[2023-06-25 05:35:37,577][129146] Mean Reward across all agents: 631.4665834294415
[37m[1m[2023-06-25 05:35:37,577][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:35:37,582][129146] mean_value=146.93071891676942, max_value=1187.778011552547
[37m[1m[2023-06-25 05:35:37,585][129146] New mean coefficients: [[-0.01924906  0.54842865  2.190686   -0.38579714 -0.51913756]]
[37m[1m[2023-06-25 05:35:37,586][129146] Moving the mean solution point...
[36m[2023-06-25 05:35:47,404][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 05:35:47,404][129146] FPS: 391191.64
[36m[2023-06-25 05:35:47,406][129146] itr=488, itrs=2000, Progress: 24.40%
[36m[2023-06-25 05:35:59,034][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 05:35:59,035][129146] FPS: 330736.11
[36m[2023-06-25 05:36:03,917][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:36:03,923][129146] Reward + Measures: [[622.53775705   0.29738      0.98198491   0.04620367   0.39579102]]
[37m[1m[2023-06-25 05:36:03,923][129146] Max Reward on eval: 622.5377570517691
[37m[1m[2023-06-25 05:36:03,923][129146] Min Reward on eval: 622.5377570517691
[37m[1m[2023-06-25 05:36:03,924][129146] Mean Reward across all agents: 622.5377570517691
[37m[1m[2023-06-25 05:36:03,924][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:36:09,414][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:36:09,415][129146] Reward + Measures: [[663.68300926   0.44949999   0.9763999    0.0321       0.40169999]
[37m[1m [599.23354224   0.25850001   0.98380005   0.0338       0.4619    ]
[37m[1m [687.15241448   0.33469999   0.96969998   0.14850001   0.41589999]
[37m[1m ...
[37m[1m [664.10067676   0.25489998   0.97930002   0.0548       0.39979997]
[37m[1m [651.70500627   0.2656       0.96649998   0.19         0.45539999]
[37m[1m [613.69160369   0.32589999   0.97939998   0.036        0.40240002]]
[37m[1m[2023-06-25 05:36:09,415][129146] Max Reward on eval: 1123.223499447573
[37m[1m[2023-06-25 05:36:09,415][129146] Min Reward on eval: 530.145119480195
[37m[1m[2023-06-25 05:36:09,415][129146] Mean Reward across all agents: 656.9445581424131
[37m[1m[2023-06-25 05:36:09,416][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:36:09,422][129146] mean_value=522.9455620463103, max_value=1273.795663242531
[37m[1m[2023-06-25 05:36:09,425][129146] New mean coefficients: [[ 0.18872342  1.318712    2.7248452  -0.26614183 -0.29899108]]
[37m[1m[2023-06-25 05:36:09,426][129146] Moving the mean solution point...
[36m[2023-06-25 05:36:19,174][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 05:36:19,174][129146] FPS: 393975.63
[36m[2023-06-25 05:36:19,176][129146] itr=489, itrs=2000, Progress: 24.45%
[36m[2023-06-25 05:36:30,654][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 05:36:30,655][129146] FPS: 335070.13
[36m[2023-06-25 05:36:35,336][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:36:35,336][129146] Reward + Measures: [[638.91309687   0.36113966   0.98117894   0.04229133   0.37391838]]
[37m[1m[2023-06-25 05:36:35,337][129146] Max Reward on eval: 638.913096870487
[37m[1m[2023-06-25 05:36:35,337][129146] Min Reward on eval: 638.913096870487
[37m[1m[2023-06-25 05:36:35,337][129146] Mean Reward across all agents: 638.913096870487
[37m[1m[2023-06-25 05:36:35,337][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:36:40,990][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:36:40,990][129146] Reward + Measures: [[665.60593905   0.42550001   0.95669997   0.2385       0.46339998]
[37m[1m [606.60620476   0.55400002   0.9483       0.1196       0.45860001]
[37m[1m [608.58738531   0.491        0.94200003   0.3064       0.49310002]
[37m[1m ...
[37m[1m [871.15093143   0.44600001   0.91650003   0.0376       0.31100002]
[37m[1m [723.03617153   0.43759999   0.93880004   0.2277       0.44150001]
[37m[1m [510.07274325   0.68300003   0.89439994   0.64030004   0.68040001]]
[37m[1m[2023-06-25 05:36:40,991][129146] Max Reward on eval: 1108.3851917219813
[37m[1m[2023-06-25 05:36:40,991][129146] Min Reward on eval: 84.21674401065101
[37m[1m[2023-06-25 05:36:40,991][129146] Mean Reward across all agents: 726.9057914623581
[37m[1m[2023-06-25 05:36:40,991][129146] Average Trajectory Length: 999.4096666666667
[36m[2023-06-25 05:36:40,999][129146] mean_value=352.6498519752362, max_value=1481.5535690782358
[37m[1m[2023-06-25 05:36:41,002][129146] New mean coefficients: [[ 0.094653    1.3398472   2.3141527  -0.3433711  -0.45563227]]
[37m[1m[2023-06-25 05:36:41,003][129146] Moving the mean solution point...
[36m[2023-06-25 05:36:50,581][129146] train() took 9.58 seconds to complete
[36m[2023-06-25 05:36:50,581][129146] FPS: 400991.44
[36m[2023-06-25 05:36:50,583][129146] itr=490, itrs=2000, Progress: 24.50%
[37m[1m[2023-06-25 05:36:54,995][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000470
[36m[2023-06-25 05:37:06,929][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 05:37:06,930][129146] FPS: 330256.38
[36m[2023-06-25 05:37:11,664][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:37:11,664][129146] Reward + Measures: [[656.08272915   0.47425097   0.97850764   0.03884      0.34999365]]
[37m[1m[2023-06-25 05:37:11,664][129146] Max Reward on eval: 656.0827291469857
[37m[1m[2023-06-25 05:37:11,665][129146] Min Reward on eval: 656.0827291469857
[37m[1m[2023-06-25 05:37:11,665][129146] Mean Reward across all agents: 656.0827291469857
[37m[1m[2023-06-25 05:37:11,665][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:37:17,169][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:37:17,169][129146] Reward + Measures: [[737.37508708   0.43460003   0.95510006   0.09860001   0.37010002]
[37m[1m [609.91714734   0.38910004   0.96899998   0.0388       0.41599998]
[37m[1m [612.32774245   0.38419998   0.97150004   0.0396       0.39610001]
[37m[1m ...
[37m[1m [608.53544906   0.41260001   0.97509998   0.0324       0.41470003]
[37m[1m [596.77402774   0.46890002   0.96640009   0.13329999   0.44730002]
[37m[1m [680.525982     0.4736       0.9781       0.0331       0.34130001]]
[37m[1m[2023-06-25 05:37:17,170][129146] Max Reward on eval: 985.5330481966841
[37m[1m[2023-06-25 05:37:17,170][129146] Min Reward on eval: 565.6225650503068
[37m[1m[2023-06-25 05:37:17,170][129146] Mean Reward across all agents: 624.6703131714702
[37m[1m[2023-06-25 05:37:17,170][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:37:17,173][129146] mean_value=125.93660063266255, max_value=1121.462922641046
[37m[1m[2023-06-25 05:37:17,177][129146] New mean coefficients: [[ 0.05424352  1.3283956   2.6930363  -0.3314161  -0.40301383]]
[37m[1m[2023-06-25 05:37:17,178][129146] Moving the mean solution point...
[36m[2023-06-25 05:37:27,056][129146] train() took 9.88 seconds to complete
[36m[2023-06-25 05:37:27,056][129146] FPS: 388797.55
[36m[2023-06-25 05:37:27,059][129146] itr=491, itrs=2000, Progress: 24.55%
[36m[2023-06-25 05:37:38,474][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 05:37:38,474][129146] FPS: 336944.58
[36m[2023-06-25 05:37:43,192][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:37:43,192][129146] Reward + Measures: [[699.85589278   0.53514969   0.97292233   0.04128133   0.33684766]]
[37m[1m[2023-06-25 05:37:43,192][129146] Max Reward on eval: 699.8558927818667
[37m[1m[2023-06-25 05:37:43,193][129146] Min Reward on eval: 699.8558927818667
[37m[1m[2023-06-25 05:37:43,193][129146] Mean Reward across all agents: 699.8558927818667
[37m[1m[2023-06-25 05:37:43,193][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:37:48,804][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:37:48,865][129146] Reward + Measures: [[ 874.80981353    0.46420002    0.85439998    0.1247        0.4021    ]
[37m[1m [ 724.17082289    0.43020001    0.91989994    0.15640001    0.45900002]
[37m[1m [1463.46351038    0.27420002    0.56580007    0.1495        0.21210001]
[37m[1m ...
[37m[1m [ 684.82744788    0.45460001    0.95960009    0.0294        0.3087    ]
[37m[1m [1355.59951333    0.29620001    0.71489996    0.0927        0.2498    ]
[37m[1m [ 679.70980705    0.4973        0.93330002    0.0304        0.32660004]]
[37m[1m[2023-06-25 05:37:48,865][129146] Max Reward on eval: 1676.6470307818963
[37m[1m[2023-06-25 05:37:48,866][129146] Min Reward on eval: 51.290615586796775
[37m[1m[2023-06-25 05:37:48,866][129146] Mean Reward across all agents: 977.0172409735244
[37m[1m[2023-06-25 05:37:48,866][129146] Average Trajectory Length: 997.6156666666666
[36m[2023-06-25 05:37:48,872][129146] mean_value=270.65591015409143, max_value=1770.3820850844145
[37m[1m[2023-06-25 05:37:48,875][129146] New mean coefficients: [[-0.0298725   1.1268396   2.409805   -0.36742765 -0.483499  ]]
[37m[1m[2023-06-25 05:37:48,876][129146] Moving the mean solution point...
[36m[2023-06-25 05:37:58,609][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 05:37:58,609][129146] FPS: 394597.99
[36m[2023-06-25 05:37:58,611][129146] itr=492, itrs=2000, Progress: 24.60%
[36m[2023-06-25 05:38:10,002][129146] train() took 11.37 seconds to complete
[36m[2023-06-25 05:38:10,002][129146] FPS: 337644.31
[36m[2023-06-25 05:38:14,761][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:38:14,761][129146] Reward + Measures: [[678.82277497   0.65424806   0.9778387    0.036453     0.34644595]]
[37m[1m[2023-06-25 05:38:14,761][129146] Max Reward on eval: 678.8227749667433
[37m[1m[2023-06-25 05:38:14,761][129146] Min Reward on eval: 678.8227749667433
[37m[1m[2023-06-25 05:38:14,762][129146] Mean Reward across all agents: 678.8227749667433
[37m[1m[2023-06-25 05:38:14,762][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:38:20,191][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:38:20,194][129146] Reward + Measures: [[539.54692728   0.57470006   0.59670001   0.05880001   0.36820003]
[37m[1m [509.67452294   0.56160003   0.56510001   0.16180001   0.41070005]
[37m[1m [697.97358026   0.55089998   0.76950002   0.0284       0.2861    ]
[37m[1m ...
[37m[1m [451.82048071   0.46928006   0.4244383    0.19630246   0.36924759]
[37m[1m [744.89218509   0.37180004   0.6279       0.29650003   0.46950004]
[37m[1m [754.33913502   0.45390001   0.78009999   0.21789999   0.49679995]]
[37m[1m[2023-06-25 05:38:20,195][129146] Max Reward on eval: 1159.8930462293326
[37m[1m[2023-06-25 05:38:20,195][129146] Min Reward on eval: -454.35058483540195
[37m[1m[2023-06-25 05:38:20,195][129146] Mean Reward across all agents: 634.5240654307071
[37m[1m[2023-06-25 05:38:20,196][129146] Average Trajectory Length: 988.879
[36m[2023-06-25 05:38:20,203][129146] mean_value=-10.415899143351805, max_value=1286.9909677664516
[37m[1m[2023-06-25 05:38:20,206][129146] New mean coefficients: [[-0.14078654  0.3958677   2.356973   -0.29970387 -0.4856974 ]]
[37m[1m[2023-06-25 05:38:20,207][129146] Moving the mean solution point...
[36m[2023-06-25 05:38:29,845][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 05:38:29,845][129146] FPS: 398490.78
[36m[2023-06-25 05:38:29,847][129146] itr=493, itrs=2000, Progress: 24.65%
[36m[2023-06-25 05:38:41,239][129146] train() took 11.37 seconds to complete
[36m[2023-06-25 05:38:41,239][129146] FPS: 337632.61
[36m[2023-06-25 05:38:46,034][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:38:46,034][129146] Reward + Measures: [[594.48324486   0.79361832   0.97202402   0.33847001   0.54023695]]
[37m[1m[2023-06-25 05:38:46,035][129146] Max Reward on eval: 594.4832448560364
[37m[1m[2023-06-25 05:38:46,035][129146] Min Reward on eval: 594.4832448560364
[37m[1m[2023-06-25 05:38:46,035][129146] Mean Reward across all agents: 594.4832448560364
[37m[1m[2023-06-25 05:38:46,035][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:38:51,458][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:38:51,459][129146] Reward + Measures: [[844.8436509    0.54009998   0.7912001    0.30780002   0.38930002]
[37m[1m [629.35856166   0.76200002   0.81309998   0.6476       0.6645    ]
[37m[1m [688.1928022    0.72830003   0.88249999   0.47389999   0.59120005]
[37m[1m ...
[37m[1m [574.9659597    0.7694       0.87919998   0.58920002   0.66300005]
[37m[1m [514.74160535   0.7525       0.91099995   0.31440002   0.54109997]
[37m[1m [433.03415588   0.94569999   0.84219998   0.89420003   0.84109992]]
[37m[1m[2023-06-25 05:38:51,459][129146] Max Reward on eval: 1230.509334631241
[37m[1m[2023-06-25 05:38:51,459][129146] Min Reward on eval: 316.12539963768796
[37m[1m[2023-06-25 05:38:51,459][129146] Mean Reward across all agents: 595.9388494392772
[37m[1m[2023-06-25 05:38:51,460][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:38:51,467][129146] mean_value=352.7199946124118, max_value=1179.9702726864257
[37m[1m[2023-06-25 05:38:51,470][129146] New mean coefficients: [[-0.12687883  0.43810225  1.6281015  -0.16597359 -0.24728306]]
[37m[1m[2023-06-25 05:38:51,471][129146] Moving the mean solution point...
[36m[2023-06-25 05:39:01,329][129146] train() took 9.86 seconds to complete
[36m[2023-06-25 05:39:01,329][129146] FPS: 389614.60
[36m[2023-06-25 05:39:01,331][129146] itr=494, itrs=2000, Progress: 24.70%
[36m[2023-06-25 05:39:12,903][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 05:39:12,904][129146] FPS: 332340.87
[36m[2023-06-25 05:39:17,703][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:39:17,703][129146] Reward + Measures: [[559.79146079   0.8411966    0.97639865   0.42534533   0.59169829]]
[37m[1m[2023-06-25 05:39:17,703][129146] Max Reward on eval: 559.7914607946057
[37m[1m[2023-06-25 05:39:17,704][129146] Min Reward on eval: 559.7914607946057
[37m[1m[2023-06-25 05:39:17,704][129146] Mean Reward across all agents: 559.7914607946057
[37m[1m[2023-06-25 05:39:17,704][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:39:23,161][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:39:23,161][129146] Reward + Measures: [[ 648.79400794    0.68279999    0.45930004    0.55250007    0.49470001]
[37m[1m [1393.80421697    0.45049998    0.62390006    0.22610001    0.37939999]
[37m[1m [ 456.61081494    0.74629998    0.79080003    0.69760001    0.72830003]
[37m[1m ...
[37m[1m [ 689.06479117    0.69209999    0.97180003    0.13310002    0.41070005]
[37m[1m [ 637.86268442    0.68120003    0.48199996    0.54240006    0.47480002]
[37m[1m [ 809.07442823    0.62889999    0.52980006    0.21599999    0.3671    ]]
[37m[1m[2023-06-25 05:39:23,161][129146] Max Reward on eval: 1457.5983093031216
[37m[1m[2023-06-25 05:39:23,162][129146] Min Reward on eval: 259.85932454849825
[37m[1m[2023-06-25 05:39:23,162][129146] Mean Reward across all agents: 723.124631499035
[37m[1m[2023-06-25 05:39:23,162][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:39:23,170][129146] mean_value=118.93837880184628, max_value=1367.583954502294
[37m[1m[2023-06-25 05:39:23,173][129146] New mean coefficients: [[-0.06133879  0.26577532  1.5606577  -0.07450322 -0.11478326]]
[37m[1m[2023-06-25 05:39:23,174][129146] Moving the mean solution point...
[36m[2023-06-25 05:39:32,833][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 05:39:32,833][129146] FPS: 397625.76
[36m[2023-06-25 05:39:32,835][129146] itr=495, itrs=2000, Progress: 24.75%
[36m[2023-06-25 05:39:44,257][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 05:39:44,257][129146] FPS: 336761.69
[36m[2023-06-25 05:39:49,079][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:39:49,080][129146] Reward + Measures: [[590.77285898   0.82393569   0.97838533   0.36082903   0.54017466]]
[37m[1m[2023-06-25 05:39:49,080][129146] Max Reward on eval: 590.7728589810139
[37m[1m[2023-06-25 05:39:49,080][129146] Min Reward on eval: 590.7728589810139
[37m[1m[2023-06-25 05:39:49,080][129146] Mean Reward across all agents: 590.7728589810139
[37m[1m[2023-06-25 05:39:49,081][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:39:54,562][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:39:54,563][129146] Reward + Measures: [[914.72876651   0.63099998   0.86280006   0.0709       0.35299999]
[37m[1m [440.23042374   0.98690003   0.98829997   0.98510009   0.986     ]
[37m[1m [909.13026848   0.4598       0.736        0.24590002   0.3407    ]
[37m[1m ...
[37m[1m [557.09517878   0.72969997   0.97370005   0.21639998   0.44520003]
[37m[1m [765.44101396   0.72530001   0.92600006   0.25699997   0.44840002]
[37m[1m [732.17477026   0.68370003   0.97399998   0.0229       0.30840001]]
[37m[1m[2023-06-25 05:39:54,563][129146] Max Reward on eval: 1203.1097329915967
[37m[1m[2023-06-25 05:39:54,564][129146] Min Reward on eval: -48.29295606192318
[37m[1m[2023-06-25 05:39:54,564][129146] Mean Reward across all agents: 709.21600473983
[37m[1m[2023-06-25 05:39:54,564][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:39:54,575][129146] mean_value=417.3618213717998, max_value=1410.9341306807473
[37m[1m[2023-06-25 05:39:54,578][129146] New mean coefficients: [[-0.03005328  0.38868302  1.9211848   0.04220657  0.0405461 ]]
[37m[1m[2023-06-25 05:39:54,579][129146] Moving the mean solution point...
[36m[2023-06-25 05:40:04,233][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 05:40:04,234][129146] FPS: 397800.63
[36m[2023-06-25 05:40:04,236][129146] itr=496, itrs=2000, Progress: 24.80%
[36m[2023-06-25 05:40:15,650][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 05:40:15,650][129146] FPS: 336970.88
[36m[2023-06-25 05:40:20,411][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:40:20,411][129146] Reward + Measures: [[562.40188954   0.85552132   0.98075974   0.43666935   0.59834766]]
[37m[1m[2023-06-25 05:40:20,412][129146] Max Reward on eval: 562.4018895395299
[37m[1m[2023-06-25 05:40:20,412][129146] Min Reward on eval: 562.4018895395299
[37m[1m[2023-06-25 05:40:20,412][129146] Mean Reward across all agents: 562.4018895395299
[37m[1m[2023-06-25 05:40:20,412][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:40:25,798][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:40:25,798][129146] Reward + Measures: [[515.76120046   0.80450004   0.97690004   0.51020002   0.65169996]
[37m[1m [508.99324312   0.74509996   0.87169999   0.42840001   0.55730003]
[37m[1m [525.33517944   0.82310003   0.89880002   0.65350002   0.77350003]
[37m[1m ...
[37m[1m [606.97807066   0.90860003   0.97070009   0.80260003   0.85939997]
[37m[1m [497.44442693   0.85719997   0.96460003   0.70570004   0.77740002]
[37m[1m [465.04194065   0.95809996   0.98409998   0.89650005   0.9181    ]]
[37m[1m[2023-06-25 05:40:25,799][129146] Max Reward on eval: 1068.0770344207763
[37m[1m[2023-06-25 05:40:25,799][129146] Min Reward on eval: 347.49662429469174
[37m[1m[2023-06-25 05:40:25,799][129146] Mean Reward across all agents: 600.6963024678025
[37m[1m[2023-06-25 05:40:25,799][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:40:25,806][129146] mean_value=306.7139708508013, max_value=1187.8097151000588
[37m[1m[2023-06-25 05:40:25,809][129146] New mean coefficients: [[-0.05283636  0.23451613  1.712305   -0.06109203 -0.0079898 ]]
[37m[1m[2023-06-25 05:40:25,810][129146] Moving the mean solution point...
[36m[2023-06-25 05:40:35,498][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 05:40:35,498][129146] FPS: 396428.95
[36m[2023-06-25 05:40:35,501][129146] itr=497, itrs=2000, Progress: 24.85%
[36m[2023-06-25 05:40:47,107][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 05:40:47,107][129146] FPS: 331371.27
[36m[2023-06-25 05:40:51,910][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:40:51,910][129146] Reward + Measures: [[518.9330787    0.90480727   0.98357064   0.63130766   0.73062307]]
[37m[1m[2023-06-25 05:40:51,911][129146] Max Reward on eval: 518.9330787024538
[37m[1m[2023-06-25 05:40:51,911][129146] Min Reward on eval: 518.9330787024538
[37m[1m[2023-06-25 05:40:51,911][129146] Mean Reward across all agents: 518.9330787024538
[37m[1m[2023-06-25 05:40:51,912][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:40:57,398][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:40:57,398][129146] Reward + Measures: [[344.1034246    0.57319999   0.68120003   0.0543       0.31090003]
[37m[1m [507.353771     0.68979996   0.97490007   0.8915       0.93760008]
[37m[1m [600.22988794   0.8075       0.98089999   0.6045       0.7202    ]
[37m[1m ...
[37m[1m [709.99124441   0.75929993   0.91429996   0.1115       0.3637    ]
[37m[1m [756.47638364   0.55330002   0.88929999   0.15150002   0.38820001]
[37m[1m [409.34591388   0.58100003   0.63300002   0.2278       0.45450002]]
[37m[1m[2023-06-25 05:40:57,398][129146] Max Reward on eval: 1011.9129881591304
[37m[1m[2023-06-25 05:40:57,399][129146] Min Reward on eval: 46.59870338970795
[37m[1m[2023-06-25 05:40:57,399][129146] Mean Reward across all agents: 563.0571472458429
[37m[1m[2023-06-25 05:40:57,399][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:40:57,409][129146] mean_value=245.4172496945114, max_value=1448.9975973119494
[37m[1m[2023-06-25 05:40:57,412][129146] New mean coefficients: [[-0.07378439  0.20623158  1.8345281   0.00182563 -0.04964559]]
[37m[1m[2023-06-25 05:40:57,413][129146] Moving the mean solution point...
[36m[2023-06-25 05:41:07,056][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 05:41:07,056][129146] FPS: 398272.30
[36m[2023-06-25 05:41:07,059][129146] itr=498, itrs=2000, Progress: 24.90%
[36m[2023-06-25 05:41:18,554][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 05:41:18,554][129146] FPS: 334675.97
[36m[2023-06-25 05:41:23,326][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:41:23,326][129146] Reward + Measures: [[492.62831368   0.93312174   0.98594433   0.72653842   0.79911637]]
[37m[1m[2023-06-25 05:41:23,326][129146] Max Reward on eval: 492.62831367823327
[37m[1m[2023-06-25 05:41:23,326][129146] Min Reward on eval: 492.62831367823327
[37m[1m[2023-06-25 05:41:23,327][129146] Mean Reward across all agents: 492.62831367823327
[37m[1m[2023-06-25 05:41:23,327][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:41:28,765][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:41:28,770][129146] Reward + Measures: [[601.25807271   0.75480002   0.89460003   0.54980004   0.72450006]
[37m[1m [426.54121871   0.99250001   0.99280006   0.99330008   0.99080002]
[37m[1m [730.83041114   0.47410002   0.9073       0.0306       0.42120001]
[37m[1m ...
[37m[1m [610.21092167   0.84740013   0.9795       0.62589997   0.73140001]
[37m[1m [547.01537851   0.81910002   0.93889999   0.52649999   0.69529998]
[37m[1m [595.26379898   0.7827       0.90959996   0.3267       0.62060004]]
[37m[1m[2023-06-25 05:41:28,771][129146] Max Reward on eval: 1141.898670275265
[37m[1m[2023-06-25 05:41:28,771][129146] Min Reward on eval: 328.93298288928344
[37m[1m[2023-06-25 05:41:28,771][129146] Mean Reward across all agents: 558.155013738633
[37m[1m[2023-06-25 05:41:28,771][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:41:28,778][129146] mean_value=220.26419575440747, max_value=1176.7429843878956
[37m[1m[2023-06-25 05:41:28,781][129146] New mean coefficients: [[-0.08940721  0.11476551  1.9571226  -0.17459276 -0.1085368 ]]
[37m[1m[2023-06-25 05:41:28,782][129146] Moving the mean solution point...
[36m[2023-06-25 05:41:38,441][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 05:41:38,441][129146] FPS: 397606.80
[36m[2023-06-25 05:41:38,444][129146] itr=499, itrs=2000, Progress: 24.95%
[36m[2023-06-25 05:41:49,965][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 05:41:49,966][129146] FPS: 333838.93
[36m[2023-06-25 05:41:54,627][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:41:54,627][129146] Reward + Measures: [[451.86869574   0.94384539   0.98758566   0.78995574   0.8439129 ]]
[37m[1m[2023-06-25 05:41:54,627][129146] Max Reward on eval: 451.86869573680457
[37m[1m[2023-06-25 05:41:54,628][129146] Min Reward on eval: 451.86869573680457
[37m[1m[2023-06-25 05:41:54,628][129146] Mean Reward across all agents: 451.86869573680457
[37m[1m[2023-06-25 05:41:54,628][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:42:00,055][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:42:00,061][129146] Reward + Measures: [[332.70163155   0.86929989   0.9108001    0.80830002   0.84289998]
[37m[1m [382.99530439   0.88389999   0.95190001   0.79840004   0.84470004]
[37m[1m [208.06224279   0.98800004   0.9884001    0.98890001   0.98750001]
[37m[1m ...
[37m[1m [472.3427074    0.81020004   0.8247       0.72299999   0.71530002]
[37m[1m [144.67726789   0.98640007   0.98509997   0.98640007   0.98229998]
[37m[1m [649.25898781   0.72729999   0.94890004   0.5309       0.67329997]]
[37m[1m[2023-06-25 05:42:00,061][129146] Max Reward on eval: 1104.9200572689995
[37m[1m[2023-06-25 05:42:00,061][129146] Min Reward on eval: -211.48206361496122
[37m[1m[2023-06-25 05:42:00,062][129146] Mean Reward across all agents: 451.20839394879226
[37m[1m[2023-06-25 05:42:00,062][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:42:00,067][129146] mean_value=-45.312561113303914, max_value=1107.0204670962644
[37m[1m[2023-06-25 05:42:00,070][129146] New mean coefficients: [[-0.03461353  0.4143294   1.3772349  -0.04963467  0.18181741]]
[37m[1m[2023-06-25 05:42:00,071][129146] Moving the mean solution point...
[36m[2023-06-25 05:42:09,864][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 05:42:09,864][129146] FPS: 392180.87
[36m[2023-06-25 05:42:09,866][129146] itr=500, itrs=2000, Progress: 25.00%
[37m[1m[2023-06-25 05:42:14,679][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000480
[36m[2023-06-25 05:42:26,510][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 05:42:26,511][129146] FPS: 333191.19
[36m[2023-06-25 05:42:31,187][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:42:31,187][129146] Reward + Measures: [[451.96158361   0.94914299   0.98751694   0.81480229   0.86369693]]
[37m[1m[2023-06-25 05:42:31,188][129146] Max Reward on eval: 451.96158360537686
[37m[1m[2023-06-25 05:42:31,188][129146] Min Reward on eval: 451.96158360537686
[37m[1m[2023-06-25 05:42:31,188][129146] Mean Reward across all agents: 451.96158360537686
[37m[1m[2023-06-25 05:42:31,188][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:42:36,655][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:42:36,656][129146] Reward + Measures: [[183.58762531   0.99560004   0.61760002   0.99559993   0.62019998]
[37m[1m [344.66738529   0.99239999   0.94910002   0.9932       0.97220004]
[37m[1m [671.05220835   0.82690001   0.963        0.45289999   0.62120003]
[37m[1m ...
[37m[1m [471.39448547   0.94150001   0.98530006   0.79670006   0.8537001 ]
[37m[1m [401.78212477   0.93040001   0.65410006   0.80140001   0.42019996]
[37m[1m [174.01965936   0.99449998   0.82680005   0.9939       0.88140005]]
[37m[1m[2023-06-25 05:42:36,656][129146] Max Reward on eval: 756.3448477685918
[37m[1m[2023-06-25 05:42:36,656][129146] Min Reward on eval: 35.44349462823011
[37m[1m[2023-06-25 05:42:36,657][129146] Mean Reward across all agents: 421.34795992506633
[37m[1m[2023-06-25 05:42:36,657][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:42:36,663][129146] mean_value=182.58316797632415, max_value=1052.3769678592448
[37m[1m[2023-06-25 05:42:36,666][129146] New mean coefficients: [[ 0.06541781  0.7805409  -0.1355635  -0.0319362   0.33346778]]
[37m[1m[2023-06-25 05:42:36,666][129146] Moving the mean solution point...
[36m[2023-06-25 05:42:46,464][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 05:42:46,464][129146] FPS: 392014.41
[36m[2023-06-25 05:42:46,466][129146] itr=501, itrs=2000, Progress: 25.05%
[36m[2023-06-25 05:42:58,020][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 05:42:58,020][129146] FPS: 333006.16
[36m[2023-06-25 05:43:02,786][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:43:02,787][129146] Reward + Measures: [[462.33597098   0.95132989   0.98314339   0.80413169   0.85824072]]
[37m[1m[2023-06-25 05:43:02,787][129146] Max Reward on eval: 462.33597098340476
[37m[1m[2023-06-25 05:43:02,787][129146] Min Reward on eval: 462.33597098340476
[37m[1m[2023-06-25 05:43:02,788][129146] Mean Reward across all agents: 462.33597098340476
[37m[1m[2023-06-25 05:43:02,788][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:43:08,218][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:43:08,219][129146] Reward + Measures: [[471.30457411   0.94740003   0.97560006   0.80489999   0.85769999]
[37m[1m [467.7300359    0.97059995   0.95370007   0.89899999   0.91429996]
[37m[1m [403.14183837   0.97209996   0.98559999   0.89810002   0.921     ]
[37m[1m ...
[37m[1m [429.53809201   0.99340004   0.97489995   0.99010003   0.97930002]
[37m[1m [570.66960683   0.8754999    0.9781       0.51319999   0.66250002]
[37m[1m [407.7780918    0.9677       0.97399998   0.89650005   0.92410004]]
[37m[1m[2023-06-25 05:43:08,219][129146] Max Reward on eval: 754.1188444538973
[37m[1m[2023-06-25 05:43:08,219][129146] Min Reward on eval: 232.41880054378416
[37m[1m[2023-06-25 05:43:08,220][129146] Mean Reward across all agents: 432.6101004734239
[37m[1m[2023-06-25 05:43:08,220][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:43:08,222][129146] mean_value=-39.68311178889619, max_value=1075.9992178158368
[37m[1m[2023-06-25 05:43:08,224][129146] New mean coefficients: [[ 0.0169858   0.25019413  0.28064108 -0.10253751  0.19690636]]
[37m[1m[2023-06-25 05:43:08,225][129146] Moving the mean solution point...
[36m[2023-06-25 05:43:17,933][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 05:43:17,933][129146] FPS: 395647.19
[36m[2023-06-25 05:43:17,935][129146] itr=502, itrs=2000, Progress: 25.10%
[36m[2023-06-25 05:43:29,324][129146] train() took 11.37 seconds to complete
[36m[2023-06-25 05:43:29,324][129146] FPS: 337697.73
[36m[2023-06-25 05:43:34,104][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:43:34,104][129146] Reward + Measures: [[460.64337537   0.96817428   0.97074103   0.88904864   0.91341633]]
[37m[1m[2023-06-25 05:43:34,105][129146] Max Reward on eval: 460.6433753720896
[37m[1m[2023-06-25 05:43:34,105][129146] Min Reward on eval: 460.6433753720896
[37m[1m[2023-06-25 05:43:34,105][129146] Mean Reward across all agents: 460.6433753720896
[37m[1m[2023-06-25 05:43:34,105][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:43:39,667][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:43:39,667][129146] Reward + Measures: [[551.91558489   0.97060007   0.98390001   0.89460003   0.92399997]
[37m[1m [437.07120571   0.99449998   0.97939998   0.99550003   0.98430008]
[37m[1m [374.17416814   0.98689997   0.91240007   0.99119997   0.94070005]
[37m[1m ...
[37m[1m [332.18147669   0.94220001   0.76880002   0.89680004   0.74510002]
[37m[1m [458.28199409   0.96989995   0.93729991   0.90059996   0.90150005]
[37m[1m [546.15097101   0.97170001   0.98789996   0.89910001   0.92810005]]
[37m[1m[2023-06-25 05:43:39,667][129146] Max Reward on eval: 732.7439361645141
[37m[1m[2023-06-25 05:43:39,668][129146] Min Reward on eval: 213.85298882003408
[37m[1m[2023-06-25 05:43:39,668][129146] Mean Reward across all agents: 471.47993388965483
[37m[1m[2023-06-25 05:43:39,668][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:43:39,671][129146] mean_value=-21.616293498657274, max_value=926.5467962926836
[37m[1m[2023-06-25 05:43:39,674][129146] New mean coefficients: [[-0.09145696 -0.2728533   0.77908075 -0.13002446  0.12927818]]
[37m[1m[2023-06-25 05:43:39,674][129146] Moving the mean solution point...
[36m[2023-06-25 05:43:49,329][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 05:43:49,329][129146] FPS: 397803.80
[36m[2023-06-25 05:43:49,332][129146] itr=503, itrs=2000, Progress: 25.15%
[36m[2023-06-25 05:44:00,710][129146] train() took 11.36 seconds to complete
[36m[2023-06-25 05:44:00,711][129146] FPS: 338037.33
[36m[2023-06-25 05:44:05,384][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:44:05,385][129146] Reward + Measures: [[400.49016406   0.97959763   0.98436034   0.94118261   0.95319134]]
[37m[1m[2023-06-25 05:44:05,385][129146] Max Reward on eval: 400.49016405769936
[37m[1m[2023-06-25 05:44:05,385][129146] Min Reward on eval: 400.49016405769936
[37m[1m[2023-06-25 05:44:05,385][129146] Mean Reward across all agents: 400.49016405769936
[37m[1m[2023-06-25 05:44:05,386][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:44:10,882][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:44:10,887][129146] Reward + Measures: [[385.86590751   0.99090004   0.98929995   0.99449998   0.98850006]
[37m[1m [407.42151118   0.9637       0.9762001    0.89309996   0.92010003]
[37m[1m [381.81303649   0.99610007   0.97710001   0.99640006   0.98280001]
[37m[1m ...
[37m[1m [388.78576731   0.99309999   0.9738       0.99360001   0.98110002]
[37m[1m [323.28374314   0.99410003   0.9393       0.99470007   0.95900005]
[37m[1m [338.87037156   0.99400008   0.963        0.99420005   0.97640002]]
[37m[1m[2023-06-25 05:44:10,888][129146] Max Reward on eval: 542.72881140206
[37m[1m[2023-06-25 05:44:10,888][129146] Min Reward on eval: 318.9370966484072
[37m[1m[2023-06-25 05:44:10,888][129146] Mean Reward across all agents: 403.7020141991687
[37m[1m[2023-06-25 05:44:10,889][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:44:10,890][129146] mean_value=-147.2104508090968, max_value=564.8820391783283
[37m[1m[2023-06-25 05:44:10,892][129146] New mean coefficients: [[-0.22812255 -0.997848    0.3485146  -0.2551548  -0.00920388]]
[37m[1m[2023-06-25 05:44:10,893][129146] Moving the mean solution point...
[36m[2023-06-25 05:44:20,572][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 05:44:20,572][129146] FPS: 396814.28
[36m[2023-06-25 05:44:20,574][129146] itr=504, itrs=2000, Progress: 25.20%
[36m[2023-06-25 05:44:32,188][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 05:44:32,189][129146] FPS: 331241.30
[36m[2023-06-25 05:44:36,986][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:44:36,987][129146] Reward + Measures: [[-97.85829929   0.96873963   0.97796166   0.92752528   0.94279069]]
[37m[1m[2023-06-25 05:44:36,987][129146] Max Reward on eval: -97.85829929028556
[37m[1m[2023-06-25 05:44:36,987][129146] Min Reward on eval: -97.85829929028556
[37m[1m[2023-06-25 05:44:36,988][129146] Mean Reward across all agents: -97.85829929028556
[37m[1m[2023-06-25 05:44:36,988][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:44:42,336][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:44:42,337][129146] Reward + Measures: [[-160.49863848    0.98210001    0.97750008    0.98299998    0.97770005]
[37m[1m [ -55.59430711    0.9623        0.98000002    0.8955        0.92320007]
[37m[1m [ -17.72403041    0.9162001     0.96000004    0.79030001    0.84890002]
[37m[1m ...
[37m[1m [   2.49535423    0.93260002    0.96539992    0.80050004    0.84610003]
[37m[1m [ -76.16764713    0.98719996    0.98900002    0.98810005    0.98689997]
[37m[1m [-111.97280588    0.9853        0.98400003    0.98839998    0.98439997]]
[37m[1m[2023-06-25 05:44:42,337][129146] Max Reward on eval: 937.8070853784214
[37m[1m[2023-06-25 05:44:42,337][129146] Min Reward on eval: -212.30116167049854
[37m[1m[2023-06-25 05:44:42,338][129146] Mean Reward across all agents: -2.3881705327608884
[37m[1m[2023-06-25 05:44:42,338][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:44:42,340][129146] mean_value=-550.9691295641734, max_value=968.2269141132679
[37m[1m[2023-06-25 05:44:42,342][129146] New mean coefficients: [[-0.22157437 -1.5607537   3.0521333  -0.28257102 -0.26206324]]
[37m[1m[2023-06-25 05:44:42,343][129146] Moving the mean solution point...
[36m[2023-06-25 05:44:51,880][129146] train() took 9.54 seconds to complete
[36m[2023-06-25 05:44:51,880][129146] FPS: 402708.36
[36m[2023-06-25 05:44:51,882][129146] itr=505, itrs=2000, Progress: 25.25%
[36m[2023-06-25 05:45:03,392][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 05:45:03,393][129146] FPS: 334146.16
[36m[2023-06-25 05:45:08,171][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:45:08,172][129146] Reward + Measures: [[-194.78092671    0.98898029    0.99209464    0.98436928    0.98722833]]
[37m[1m[2023-06-25 05:45:08,172][129146] Max Reward on eval: -194.78092670953336
[37m[1m[2023-06-25 05:45:08,172][129146] Min Reward on eval: -194.78092670953336
[37m[1m[2023-06-25 05:45:08,172][129146] Mean Reward across all agents: -194.78092670953336
[37m[1m[2023-06-25 05:45:08,173][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:45:13,616][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:45:13,616][129146] Reward + Measures: [[-193.41633586    0.98909998    0.9898001     0.99209994    0.99110001]
[37m[1m [ -83.20408136    0.95419997    0.97910005    0.90149993    0.92750007]
[37m[1m [-163.45254044    0.99090004    0.99290001    0.99249995    0.99240011]
[37m[1m ...
[37m[1m [-254.64738547    0.99160004    0.99279994    0.99379998    0.99329996]
[37m[1m [-212.66824053    0.98929995    0.99019998    0.99150002    0.99190009]
[37m[1m [-191.89312497    0.99180001    0.99140006    0.99379998    0.99290001]]
[37m[1m[2023-06-25 05:45:13,617][129146] Max Reward on eval: 112.95962003127207
[37m[1m[2023-06-25 05:45:13,617][129146] Min Reward on eval: -365.414453335898
[37m[1m[2023-06-25 05:45:13,617][129146] Mean Reward across all agents: -134.86611511315832
[37m[1m[2023-06-25 05:45:13,617][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:45:13,618][129146] mean_value=-699.9511149126128, max_value=-458.35546184118397
[36m[2023-06-25 05:45:13,621][129146] XNES is restarting with a new solution whose measures are [0.39250001 0.74800009 0.0913     0.82840008] and objective is 636.1701637766789
[36m[2023-06-25 05:45:13,622][129146] Emitter restarted. Changing the mean agent...
[37m[1m[2023-06-25 05:45:13,624][129146] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]]
[37m[1m[2023-06-25 05:45:13,625][129146] Moving the mean solution point...
[36m[2023-06-25 05:45:23,307][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 05:45:23,307][129146] FPS: 396677.69
[36m[2023-06-25 05:45:23,309][129146] itr=506, itrs=2000, Progress: 25.30%
[36m[2023-06-25 05:45:34,803][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 05:45:34,803][129146] FPS: 334698.84
[36m[2023-06-25 05:45:39,556][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:45:39,557][129146] Reward + Measures: [[554.7561348    0.36860812   0.7196821    0.08684924   0.78839207]]
[37m[1m[2023-06-25 05:45:39,557][129146] Max Reward on eval: 554.7561348006423
[37m[1m[2023-06-25 05:45:39,557][129146] Min Reward on eval: 554.7561348006423
[37m[1m[2023-06-25 05:45:39,557][129146] Mean Reward across all agents: 554.7561348006423
[37m[1m[2023-06-25 05:45:39,558][129146] Average Trajectory Length: 989.9913333333333
[36m[2023-06-25 05:45:45,269][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:45:45,270][129146] Reward + Measures: [[  375.39892077     0.36240003     0.50339997     0.25229999
[37m[1m      0.63770002]
[37m[1m [ -783.39732457     0.68130004     0.1875         0.61830002
[37m[1m      0.50550002]
[37m[1m [-1871.7894862      0.91890001     0.1767         0.80890006
[37m[1m      0.72140002]
[37m[1m ...
[37m[1m [  474.89951161     0.27361432     0.38013014     0.19950159
[37m[1m      0.35942385]
[37m[1m [ -999.88468844     0.33950001     0.29410002     0.31250003
[37m[1m      0.34220001]
[37m[1m [-1211.85924259     0.45250079     0.42491341     0.43452784
[37m[1m      0.54962444]]
[37m[1m[2023-06-25 05:45:45,270][129146] Max Reward on eval: 809.6307816893735
[37m[1m[2023-06-25 05:45:45,270][129146] Min Reward on eval: -2491.276261274773
[37m[1m[2023-06-25 05:45:45,271][129146] Mean Reward across all agents: -586.1210688997298
[37m[1m[2023-06-25 05:45:45,271][129146] Average Trajectory Length: 967.785
[36m[2023-06-25 05:45:45,274][129146] mean_value=-938.1386742442278, max_value=1161.3176587007242
[37m[1m[2023-06-25 05:45:45,277][129146] New mean coefficients: [[ 0.50976753 -0.43886364 -0.0643816  -0.8730854  -1.0285406 ]]
[37m[1m[2023-06-25 05:45:45,278][129146] Moving the mean solution point...
[36m[2023-06-25 05:45:55,076][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 05:45:55,076][129146] FPS: 391976.11
[36m[2023-06-25 05:45:55,078][129146] itr=507, itrs=2000, Progress: 25.35%
[36m[2023-06-25 05:46:06,631][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 05:46:06,631][129146] FPS: 332909.91
[36m[2023-06-25 05:46:11,472][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:46:11,472][129146] Reward + Measures: [[633.63255419   0.33024186   0.66836697   0.10872555   0.74045515]]
[37m[1m[2023-06-25 05:46:11,472][129146] Max Reward on eval: 633.6325541938373
[37m[1m[2023-06-25 05:46:11,473][129146] Min Reward on eval: 633.6325541938373
[37m[1m[2023-06-25 05:46:11,473][129146] Mean Reward across all agents: 633.6325541938373
[37m[1m[2023-06-25 05:46:11,473][129146] Average Trajectory Length: 989.5989999999999
[36m[2023-06-25 05:46:16,875][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:46:16,876][129146] Reward + Measures: [[-1358.02345388     0.63230002     0.38440001     0.51770002
[37m[1m      0.37290001]
[37m[1m [ -113.31608259     0.3195         0.32370001     0.20780002
[37m[1m      0.29550001]
[37m[1m [ -110.97418674     0.49159595     0.27700859     0.30581671
[37m[1m      0.19279324]
[37m[1m ...
[37m[1m [ -352.97856965     0.40430003     0.30150002     0.30920002
[37m[1m      0.2983    ]
[37m[1m [-1234.66186315     0.66169995     0.0881         0.68730003
[37m[1m      0.53489995]
[37m[1m [ -233.67128741     0.56545836     0.58911973     0.24510646
[37m[1m      0.60775232]]
[37m[1m[2023-06-25 05:46:16,876][129146] Max Reward on eval: 1123.2283083695452
[37m[1m[2023-06-25 05:46:16,876][129146] Min Reward on eval: -1800.3964405493693
[37m[1m[2023-06-25 05:46:16,876][129146] Mean Reward across all agents: -301.5054753458497
[37m[1m[2023-06-25 05:46:16,877][129146] Average Trajectory Length: 958.4866666666667
[36m[2023-06-25 05:46:16,881][129146] mean_value=-695.6876036825776, max_value=1321.3468506201525
[37m[1m[2023-06-25 05:46:16,884][129146] New mean coefficients: [[ 0.9483009  -0.37970045  0.22417513 -0.6830868  -0.5615197 ]]
[37m[1m[2023-06-25 05:46:16,885][129146] Moving the mean solution point...
[36m[2023-06-25 05:46:26,618][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 05:46:26,618][129146] FPS: 394603.99
[36m[2023-06-25 05:46:26,621][129146] itr=508, itrs=2000, Progress: 25.40%
[36m[2023-06-25 05:46:38,032][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 05:46:38,032][129146] FPS: 337090.61
[36m[2023-06-25 05:46:42,832][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:46:42,832][129146] Reward + Measures: [[751.42319131   0.33647677   0.60477352   0.13158958   0.66231912]]
[37m[1m[2023-06-25 05:46:42,833][129146] Max Reward on eval: 751.4231913110704
[37m[1m[2023-06-25 05:46:42,833][129146] Min Reward on eval: 751.4231913110704
[37m[1m[2023-06-25 05:46:42,833][129146] Mean Reward across all agents: 751.4231913110704
[37m[1m[2023-06-25 05:46:42,833][129146] Average Trajectory Length: 991.4476666666666
[36m[2023-06-25 05:46:48,380][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:46:48,381][129146] Reward + Measures: [[  499.62780871     0.42759997     0.56010002     0.10780001
[37m[1m      0.65720004]
[37m[1m [-1125.97952416     0.90879995     0.92130005     0.89169997
[37m[1m      0.9127    ]
[37m[1m [  -49.14592087     0.52010006     0.46799999     0.51999998
[37m[1m      0.56450003]
[37m[1m ...
[37m[1m [  970.24943154     0.36660001     0.47019997     0.2467
[37m[1m      0.49110004]
[37m[1m [  834.22917269     0.41190001     0.50699997     0.206
[37m[1m      0.56950003]
[37m[1m [  396.27122226     0.41480002     0.40839997     0.26030001
[37m[1m      0.53469998]]
[37m[1m[2023-06-25 05:46:48,381][129146] Max Reward on eval: 1066.7491603864007
[37m[1m[2023-06-25 05:46:48,381][129146] Min Reward on eval: -1329.616908452264
[37m[1m[2023-06-25 05:46:48,382][129146] Mean Reward across all agents: 290.78452443480717
[37m[1m[2023-06-25 05:46:48,382][129146] Average Trajectory Length: 988.5483333333333
[36m[2023-06-25 05:46:48,388][129146] mean_value=49.49272821061071, max_value=1128.0223210281226
[37m[1m[2023-06-25 05:46:48,390][129146] New mean coefficients: [[ 1.5703976  -0.02724814  0.32083863 -0.13644367 -0.5270724 ]]
[37m[1m[2023-06-25 05:46:48,391][129146] Moving the mean solution point...
[36m[2023-06-25 05:46:58,103][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 05:46:58,103][129146] FPS: 395477.06
[36m[2023-06-25 05:46:58,105][129146] itr=509, itrs=2000, Progress: 25.45%
[36m[2023-06-25 05:47:09,523][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 05:47:09,524][129146] FPS: 336894.39
[36m[2023-06-25 05:47:14,284][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:47:14,284][129146] Reward + Measures: [[998.57228032   0.3139537    0.49155834   0.18629062   0.5290584 ]]
[37m[1m[2023-06-25 05:47:14,284][129146] Max Reward on eval: 998.5722803166968
[37m[1m[2023-06-25 05:47:14,285][129146] Min Reward on eval: 998.5722803166968
[37m[1m[2023-06-25 05:47:14,285][129146] Mean Reward across all agents: 998.5722803166968
[37m[1m[2023-06-25 05:47:14,285][129146] Average Trajectory Length: 986.822
[36m[2023-06-25 05:47:19,763][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:47:19,763][129146] Reward + Measures: [[-232.28826025    0.31783587    0.36156434    0.22741094    0.25384846]
[37m[1m [ 557.43510952    0.24759999    0.25490004    0.23540001    0.30410001]
[37m[1m [-214.06564846    0.32728368    0.4599064     0.35173261    0.39039221]
[37m[1m ...
[37m[1m [ 756.41247146    0.34029999    0.30369997    0.23179999    0.27849999]
[37m[1m [ 319.20277988    0.52160001    0.34700003    0.38509998    0.42030001]
[37m[1m [ 386.69339302    0.30807814    0.40718365    0.20009728    0.40256104]]
[37m[1m[2023-06-25 05:47:19,764][129146] Max Reward on eval: 1433.1971828112728
[37m[1m[2023-06-25 05:47:19,764][129146] Min Reward on eval: -2482.7967077003095
[37m[1m[2023-06-25 05:47:19,764][129146] Mean Reward across all agents: 133.1093303482872
[37m[1m[2023-06-25 05:47:19,764][129146] Average Trajectory Length: 958.4966666666667
[36m[2023-06-25 05:47:19,770][129146] mean_value=-305.49405336273213, max_value=1217.0179556875164
[37m[1m[2023-06-25 05:47:19,772][129146] New mean coefficients: [[ 1.0529273   0.70601755 -0.02680871  0.13661712  0.5531653 ]]
[37m[1m[2023-06-25 05:47:19,773][129146] Moving the mean solution point...
[36m[2023-06-25 05:47:29,615][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 05:47:29,616][129146] FPS: 390221.33
[36m[2023-06-25 05:47:29,618][129146] itr=510, itrs=2000, Progress: 25.50%
[37m[1m[2023-06-25 05:47:34,520][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000490
[36m[2023-06-25 05:47:46,616][129146] train() took 11.68 seconds to complete
[36m[2023-06-25 05:47:46,616][129146] FPS: 328737.31
[36m[2023-06-25 05:47:51,338][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:47:51,338][129146] Reward + Measures: [[1226.13604503    0.32950968    0.43228197    0.21640617    0.44241098]]
[37m[1m[2023-06-25 05:47:51,338][129146] Max Reward on eval: 1226.136045029664
[37m[1m[2023-06-25 05:47:51,339][129146] Min Reward on eval: 1226.136045029664
[37m[1m[2023-06-25 05:47:51,339][129146] Mean Reward across all agents: 1226.136045029664
[37m[1m[2023-06-25 05:47:51,339][129146] Average Trajectory Length: 986.8723333333332
[36m[2023-06-25 05:47:56,746][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:47:56,746][129146] Reward + Measures: [[ 728.94193527    0.36399788    0.36999515    0.19717865    0.4296774 ]
[37m[1m [ 430.28431503    0.47629997    0.6063        0.18980001    0.59500003]
[37m[1m [ 189.09285846    0.36950001    0.78870004    0.1079        0.83140004]
[37m[1m ...
[37m[1m [1173.46567299    0.396         0.50489998    0.18880001    0.55720001]
[37m[1m [ 500.12125254    0.5007        0.41780001    0.2563        0.46860003]
[37m[1m [ 673.92117541    0.40219998    0.42389998    0.28330001    0.36849999]]
[37m[1m[2023-06-25 05:47:56,747][129146] Max Reward on eval: 1436.066116984596
[37m[1m[2023-06-25 05:47:56,747][129146] Min Reward on eval: -965.4349010939011
[37m[1m[2023-06-25 05:47:56,747][129146] Mean Reward across all agents: 361.8428495808227
[37m[1m[2023-06-25 05:47:56,747][129146] Average Trajectory Length: 976.23
[36m[2023-06-25 05:47:56,753][129146] mean_value=-142.82128815580583, max_value=1278.5613234565396
[37m[1m[2023-06-25 05:47:56,755][129146] New mean coefficients: [[ 1.7198689   0.54269564 -0.47072926  0.14273758  1.2874951 ]]
[37m[1m[2023-06-25 05:47:56,756][129146] Moving the mean solution point...
[36m[2023-06-25 05:48:06,490][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 05:48:06,490][129146] FPS: 394552.77
[36m[2023-06-25 05:48:06,493][129146] itr=511, itrs=2000, Progress: 25.55%
[36m[2023-06-25 05:48:18,056][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 05:48:18,056][129146] FPS: 332718.88
[36m[2023-06-25 05:48:22,783][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:48:22,784][129146] Reward + Measures: [[1406.75650672    0.34586382    0.41263691    0.22504443    0.40195474]]
[37m[1m[2023-06-25 05:48:22,784][129146] Max Reward on eval: 1406.7565067179949
[37m[1m[2023-06-25 05:48:22,784][129146] Min Reward on eval: 1406.7565067179949
[37m[1m[2023-06-25 05:48:22,784][129146] Mean Reward across all agents: 1406.7565067179949
[37m[1m[2023-06-25 05:48:22,785][129146] Average Trajectory Length: 992.2023333333333
[36m[2023-06-25 05:48:28,178][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:48:28,179][129146] Reward + Measures: [[1172.42754734    0.32440001    0.42519999    0.27120003    0.44980001]
[37m[1m [ 287.20978883    0.53060001    0.59820002    0.4526        0.63040006]
[37m[1m [-257.50452283    0.4252696     0.31352994    0.1591267     0.28209591]
[37m[1m ...
[37m[1m [ 843.75362297    0.38066003    0.29444185    0.27390811    0.30066857]
[37m[1m [ 629.98074465    0.26609999    0.611         0.1899        0.66209996]
[37m[1m [ 485.87966887    0.41230002    0.30170003    0.19840002    0.31500003]]
[37m[1m[2023-06-25 05:48:28,179][129146] Max Reward on eval: 1744.0421323595103
[37m[1m[2023-06-25 05:48:28,179][129146] Min Reward on eval: -803.5737727184256
[37m[1m[2023-06-25 05:48:28,180][129146] Mean Reward across all agents: 633.3440277050134
[37m[1m[2023-06-25 05:48:28,180][129146] Average Trajectory Length: 957.3473333333333
[36m[2023-06-25 05:48:28,186][129146] mean_value=-217.35554348840066, max_value=1298.6711590424325
[37m[1m[2023-06-25 05:48:28,189][129146] New mean coefficients: [[ 1.114196   -0.16774422 -0.42817432 -0.3379625   2.5118585 ]]
[37m[1m[2023-06-25 05:48:28,190][129146] Moving the mean solution point...
[36m[2023-06-25 05:48:37,856][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 05:48:37,856][129146] FPS: 397331.23
[36m[2023-06-25 05:48:37,858][129146] itr=512, itrs=2000, Progress: 25.60%
[36m[2023-06-25 05:48:49,264][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 05:48:49,264][129146] FPS: 337266.38
[36m[2023-06-25 05:48:54,163][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:48:54,163][129146] Reward + Measures: [[1475.78112512    0.34821326    0.42181864    0.21672811    0.41290838]]
[37m[1m[2023-06-25 05:48:54,164][129146] Max Reward on eval: 1475.7811251223509
[37m[1m[2023-06-25 05:48:54,164][129146] Min Reward on eval: 1475.7811251223509
[37m[1m[2023-06-25 05:48:54,164][129146] Mean Reward across all agents: 1475.7811251223509
[37m[1m[2023-06-25 05:48:54,164][129146] Average Trajectory Length: 990.5503333333334
[36m[2023-06-25 05:48:59,765][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:48:59,766][129146] Reward + Measures: [[1492.36908712    0.36099997    0.51380002    0.17479999    0.3651    ]
[37m[1m [1159.75659814    0.31930003    0.49700004    0.1849        0.26760003]
[37m[1m [ 825.22016408    0.28147921    0.37007546    0.18478884    0.26599583]
[37m[1m ...
[37m[1m [ 466.63661578    0.38519999    0.27490002    0.29210001    0.27850002]
[37m[1m [ 118.28530739    0.33196005    0.41077429    0.20450237    0.27818242]
[37m[1m [ 228.40369247    0.44460002    0.39840004    0.3238        0.49499997]]
[37m[1m[2023-06-25 05:48:59,766][129146] Max Reward on eval: 1633.8404158032731
[37m[1m[2023-06-25 05:48:59,766][129146] Min Reward on eval: -603.3618701556886
[37m[1m[2023-06-25 05:48:59,766][129146] Mean Reward across all agents: 410.83696023519695
[37m[1m[2023-06-25 05:48:59,767][129146] Average Trajectory Length: 986.7239999999999
[36m[2023-06-25 05:48:59,771][129146] mean_value=-396.1642389785749, max_value=1071.959436974686
[37m[1m[2023-06-25 05:48:59,774][129146] New mean coefficients: [[1.2936572  0.21916047 0.55073607 0.35214043 2.917215  ]]
[37m[1m[2023-06-25 05:48:59,775][129146] Moving the mean solution point...
[36m[2023-06-25 05:49:09,384][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 05:49:09,384][129146] FPS: 399680.27
[36m[2023-06-25 05:49:09,387][129146] itr=513, itrs=2000, Progress: 25.65%
[36m[2023-06-25 05:49:20,760][129146] train() took 11.36 seconds to complete
[36m[2023-06-25 05:49:20,760][129146] FPS: 338170.06
[36m[2023-06-25 05:49:25,489][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:49:25,489][129146] Reward + Measures: [[1534.41196037    0.35166934    0.43193108    0.21103971    0.42549521]]
[37m[1m[2023-06-25 05:49:25,489][129146] Max Reward on eval: 1534.411960368359
[37m[1m[2023-06-25 05:49:25,490][129146] Min Reward on eval: 1534.411960368359
[37m[1m[2023-06-25 05:49:25,490][129146] Mean Reward across all agents: 1534.411960368359
[37m[1m[2023-06-25 05:49:25,490][129146] Average Trajectory Length: 991.8449999999999
[36m[2023-06-25 05:49:31,039][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:49:31,040][129146] Reward + Measures: [[ -78.06980294    0.28013787    0.36156797    0.23377572    0.39701065]
[37m[1m [ 150.06880465    0.35170001    0.39849997    0.2113        0.4901    ]
[37m[1m [-394.62897141    0.25740552    0.28986582    0.20542815    0.28436586]
[37m[1m ...
[37m[1m [  87.94332428    0.46310002    0.3924        0.3409        0.51739997]
[37m[1m [ -33.26971184    0.42400274    0.48163149    0.09422055    0.40867534]
[37m[1m [-233.67864603    0.23740001    0.35730001    0.22320001    0.42480001]]
[37m[1m[2023-06-25 05:49:31,040][129146] Max Reward on eval: 1571.1698720262734
[37m[1m[2023-06-25 05:49:31,040][129146] Min Reward on eval: -1431.0098765193368
[37m[1m[2023-06-25 05:49:31,040][129146] Mean Reward across all agents: 96.09630561839889
[37m[1m[2023-06-25 05:49:31,041][129146] Average Trajectory Length: 985.906
[36m[2023-06-25 05:49:31,045][129146] mean_value=-554.2365460329539, max_value=1095.4762008305988
[37m[1m[2023-06-25 05:49:31,048][129146] New mean coefficients: [[1.1638006  0.32531068 0.9156843  0.3291547  3.3733904 ]]
[37m[1m[2023-06-25 05:49:31,049][129146] Moving the mean solution point...
[36m[2023-06-25 05:49:40,756][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 05:49:40,757][129146] FPS: 395641.11
[36m[2023-06-25 05:49:40,759][129146] itr=514, itrs=2000, Progress: 25.70%
[36m[2023-06-25 05:49:52,332][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 05:49:52,332][129146] FPS: 332434.55
[36m[2023-06-25 05:49:57,176][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:49:57,177][129146] Reward + Measures: [[1480.5964535     0.34749922    0.48823637    0.18605995    0.49943563]]
[37m[1m[2023-06-25 05:49:57,177][129146] Max Reward on eval: 1480.5964535005103
[37m[1m[2023-06-25 05:49:57,177][129146] Min Reward on eval: 1480.5964535005103
[37m[1m[2023-06-25 05:49:57,178][129146] Mean Reward across all agents: 1480.5964535005103
[37m[1m[2023-06-25 05:49:57,178][129146] Average Trajectory Length: 993.4683333333332
[36m[2023-06-25 05:50:02,719][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:50:02,720][129146] Reward + Measures: [[1122.1346509     0.33579999    0.36380002    0.2491        0.40289998]
[37m[1m [ 213.11596035    0.51217759    0.52992165    0.43189976    0.4865346 ]
[37m[1m [ 118.19863062    0.39218467    0.29835525    0.2776981     0.30521637]
[37m[1m ...
[37m[1m [ 555.82109878    0.3666271     0.35868984    0.27386779    0.3424356 ]
[37m[1m [-431.56852248    0.31900004    0.23190001    0.20200001    0.19579999]
[37m[1m [-559.71924776    0.35600361    0.22903164    0.22749956    0.20423742]]
[37m[1m[2023-06-25 05:50:02,720][129146] Max Reward on eval: 1565.0816006645678
[37m[1m[2023-06-25 05:50:02,720][129146] Min Reward on eval: -694.3476386141003
[37m[1m[2023-06-25 05:50:02,720][129146] Mean Reward across all agents: 367.85363315639114
[37m[1m[2023-06-25 05:50:02,721][129146] Average Trajectory Length: 945.5846666666666
[36m[2023-06-25 05:50:02,723][129146] mean_value=-827.37522674196, max_value=1177.1336532773682
[37m[1m[2023-06-25 05:50:02,726][129146] New mean coefficients: [[1.6706383  0.73948306 0.67671204 0.29632252 1.0577407 ]]
[37m[1m[2023-06-25 05:50:02,727][129146] Moving the mean solution point...
[36m[2023-06-25 05:50:12,491][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 05:50:12,491][129146] FPS: 393334.34
[36m[2023-06-25 05:50:12,494][129146] itr=515, itrs=2000, Progress: 25.75%
[36m[2023-06-25 05:50:23,904][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 05:50:23,904][129146] FPS: 337090.95
[36m[2023-06-25 05:50:28,637][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:50:28,637][129146] Reward + Measures: [[1745.29896283    0.37241235    0.46552971    0.19217427    0.42890462]]
[37m[1m[2023-06-25 05:50:28,637][129146] Max Reward on eval: 1745.2989628342164
[37m[1m[2023-06-25 05:50:28,638][129146] Min Reward on eval: 1745.2989628342164
[37m[1m[2023-06-25 05:50:28,638][129146] Mean Reward across all agents: 1745.2989628342164
[37m[1m[2023-06-25 05:50:28,638][129146] Average Trajectory Length: 996.536
[36m[2023-06-25 05:50:34,042][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:50:34,043][129146] Reward + Measures: [[ 268.28522668    0.22189999    0.46220002    0.35120001    0.57700002]
[37m[1m [ 161.55439068    0.42470002    0.32100001    0.26990005    0.3732    ]
[37m[1m [ 110.29923653    0.23400001    0.43150002    0.2931        0.58219999]
[37m[1m ...
[37m[1m [  20.76227862    0.25330001    0.31909999    0.21750002    0.33489999]
[37m[1m [-372.36395599    0.32258371    0.25338945    0.21863416    0.27457073]
[37m[1m [-531.04704106    0.46490002    0.2895        0.3522        0.433     ]]
[37m[1m[2023-06-25 05:50:34,043][129146] Max Reward on eval: 1854.8000732191372
[37m[1m[2023-06-25 05:50:34,043][129146] Min Reward on eval: -770.0169215552626
[37m[1m[2023-06-25 05:50:34,043][129146] Mean Reward across all agents: 335.842870819424
[37m[1m[2023-06-25 05:50:34,043][129146] Average Trajectory Length: 983.0233333333333
[36m[2023-06-25 05:50:34,049][129146] mean_value=-276.087286808105, max_value=1155.5478446345514
[37m[1m[2023-06-25 05:50:34,052][129146] New mean coefficients: [[1.4876704  1.1349133  0.32128838 0.27095765 1.8985386 ]]
[37m[1m[2023-06-25 05:50:34,052][129146] Moving the mean solution point...
[36m[2023-06-25 05:50:43,812][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 05:50:43,813][129146] FPS: 393517.13
[36m[2023-06-25 05:50:43,815][129146] itr=516, itrs=2000, Progress: 25.80%
[36m[2023-06-25 05:50:55,245][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 05:50:55,245][129146] FPS: 336524.45
[36m[2023-06-25 05:51:00,064][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:51:00,064][129146] Reward + Measures: [[1991.14430437    0.39612645    0.46203768    0.19258608    0.37318191]]
[37m[1m[2023-06-25 05:51:00,064][129146] Max Reward on eval: 1991.1443043730608
[37m[1m[2023-06-25 05:51:00,065][129146] Min Reward on eval: 1991.1443043730608
[37m[1m[2023-06-25 05:51:00,065][129146] Mean Reward across all agents: 1991.1443043730608
[37m[1m[2023-06-25 05:51:00,065][129146] Average Trajectory Length: 998.6916666666666
[36m[2023-06-25 05:51:05,480][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:51:05,481][129146] Reward + Measures: [[ 196.09528872    0.40790001    0.4901        0.16690002    0.58240002]
[37m[1m [ 177.64159942    0.54930001    0.26639998    0.44390002    0.36489999]
[37m[1m [1294.79813813    0.42570001    0.45910001    0.1513        0.31200001]
[37m[1m ...
[37m[1m [ 479.99723207    0.2352        0.44600001    0.30210003    0.58339995]
[37m[1m [ 891.70981893    0.36390001    0.41349998    0.16670001    0.36649999]
[37m[1m [  70.98532699    0.61115092    0.25989282    0.45495769    0.24962561]]
[37m[1m[2023-06-25 05:51:05,481][129146] Max Reward on eval: 2156.0237172605935
[37m[1m[2023-06-25 05:51:05,481][129146] Min Reward on eval: -1003.9646099156932
[37m[1m[2023-06-25 05:51:05,482][129146] Mean Reward across all agents: 683.6096833114592
[37m[1m[2023-06-25 05:51:05,482][129146] Average Trajectory Length: 994.6166666666667
[36m[2023-06-25 05:51:05,488][129146] mean_value=-188.85081569756778, max_value=1231.416069524648
[37m[1m[2023-06-25 05:51:05,490][129146] New mean coefficients: [[ 2.1014183  0.664236   1.209039  -0.4991073  2.8203712]]
[37m[1m[2023-06-25 05:51:05,491][129146] Moving the mean solution point...
[36m[2023-06-25 05:51:15,190][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 05:51:15,191][129146] FPS: 395978.80
[36m[2023-06-25 05:51:15,193][129146] itr=517, itrs=2000, Progress: 25.85%
[36m[2023-06-25 05:51:26,596][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 05:51:26,597][129146] FPS: 337274.12
[36m[2023-06-25 05:51:31,314][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:51:31,314][129146] Reward + Measures: [[2159.90841286    0.40160462    0.46135163    0.1839613     0.3517755 ]]
[37m[1m[2023-06-25 05:51:31,315][129146] Max Reward on eval: 2159.9084128622762
[37m[1m[2023-06-25 05:51:31,315][129146] Min Reward on eval: 2159.9084128622762
[37m[1m[2023-06-25 05:51:31,315][129146] Mean Reward across all agents: 2159.9084128622762
[37m[1m[2023-06-25 05:51:31,315][129146] Average Trajectory Length: 997.9453333333333
[36m[2023-06-25 05:51:37,010][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:51:37,011][129146] Reward + Measures: [[ 167.05012533    0.32871041    0.31259355    0.2289388     0.25778842]
[37m[1m [ 189.03543125    0.43360001    0.3224        0.28940001    0.4646    ]
[37m[1m [-180.45351683    0.25287622    0.22310565    0.15906771    0.18384087]
[37m[1m ...
[37m[1m [ 306.85577979    0.46799999    0.64300007    0.0746        0.58620006]
[37m[1m [-803.19835807    0.1701823     0.13296539    0.0959171     0.12706299]
[37m[1m [1330.11093731    0.37219998    0.46780005    0.1772        0.43249997]]
[37m[1m[2023-06-25 05:51:37,011][129146] Max Reward on eval: 2079.1345826765055
[37m[1m[2023-06-25 05:51:37,011][129146] Min Reward on eval: -1178.668541763164
[37m[1m[2023-06-25 05:51:37,011][129146] Mean Reward across all agents: 281.8143283027568
[37m[1m[2023-06-25 05:51:37,012][129146] Average Trajectory Length: 936.6946666666666
[36m[2023-06-25 05:51:37,015][129146] mean_value=-933.3445649221062, max_value=1068.8574576757412
[37m[1m[2023-06-25 05:51:37,017][129146] New mean coefficients: [[ 2.1699646   0.5488821  -0.11618555  0.02581775  1.479335  ]]
[37m[1m[2023-06-25 05:51:37,018][129146] Moving the mean solution point...
[36m[2023-06-25 05:51:46,841][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 05:51:46,841][129146] FPS: 390994.07
[36m[2023-06-25 05:51:46,843][129146] itr=518, itrs=2000, Progress: 25.90%
[36m[2023-06-25 05:51:58,282][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 05:51:58,282][129146] FPS: 336235.98
[36m[2023-06-25 05:52:02,982][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:52:02,983][129146] Reward + Measures: [[2278.21891897    0.40353492    0.46137086    0.17987306    0.33184499]]
[37m[1m[2023-06-25 05:52:02,983][129146] Max Reward on eval: 2278.2189189663422
[37m[1m[2023-06-25 05:52:02,983][129146] Min Reward on eval: 2278.2189189663422
[37m[1m[2023-06-25 05:52:02,984][129146] Mean Reward across all agents: 2278.2189189663422
[37m[1m[2023-06-25 05:52:02,984][129146] Average Trajectory Length: 998.6276666666666
[36m[2023-06-25 05:52:08,478][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:52:08,479][129146] Reward + Measures: [[ 734.84292141    0.2692        0.31740004    0.19620001    0.33759999]
[37m[1m [ 345.98439186    0.52220005    0.48930001    0.35839999    0.55330002]
[37m[1m [1554.39733969    0.40110001    0.52650005    0.13699999    0.50049996]
[37m[1m ...
[37m[1m [ 819.28085749    0.5054        0.51019996    0.29580003    0.54140002]
[37m[1m [1040.84390274    0.49810001    0.36520001    0.20679998    0.3515    ]
[37m[1m [ -13.23918625    0.62141794    0.44564477    0.5428164     0.37183133]]
[37m[1m[2023-06-25 05:52:08,479][129146] Max Reward on eval: 2367.9985990256887
[37m[1m[2023-06-25 05:52:08,480][129146] Min Reward on eval: -608.7205764684244
[37m[1m[2023-06-25 05:52:08,480][129146] Mean Reward across all agents: 861.8681166787717
[37m[1m[2023-06-25 05:52:08,480][129146] Average Trajectory Length: 970.3413333333333
[36m[2023-06-25 05:52:08,485][129146] mean_value=-300.33876647324144, max_value=1585.5321673624826
[37m[1m[2023-06-25 05:52:08,488][129146] New mean coefficients: [[ 2.3702226   1.1145723  -0.39811745  0.3570483   1.988982  ]]
[37m[1m[2023-06-25 05:52:08,489][129146] Moving the mean solution point...
[36m[2023-06-25 05:52:18,278][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 05:52:18,279][129146] FPS: 392318.66
[36m[2023-06-25 05:52:18,281][129146] itr=519, itrs=2000, Progress: 25.95%
[36m[2023-06-25 05:52:29,912][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 05:52:29,912][129146] FPS: 330726.88
[36m[2023-06-25 05:52:34,712][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:52:34,712][129146] Reward + Measures: [[2397.69066544    0.41352057    0.46254897    0.17457597    0.32169595]]
[37m[1m[2023-06-25 05:52:34,713][129146] Max Reward on eval: 2397.690665437607
[37m[1m[2023-06-25 05:52:34,713][129146] Min Reward on eval: 2397.690665437607
[37m[1m[2023-06-25 05:52:34,713][129146] Mean Reward across all agents: 2397.690665437607
[37m[1m[2023-06-25 05:52:34,713][129146] Average Trajectory Length: 999.5456666666666
[36m[2023-06-25 05:52:40,164][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:52:40,164][129146] Reward + Measures: [[ 119.06775054    0.33993563    0.29001662    0.21719471    0.26568615]
[37m[1m [-124.88553912    0.38930002    0.29099998    0.23910001    0.3759    ]
[37m[1m [ 144.84963693    0.4289        0.29949999    0.21510001    0.46669999]
[37m[1m ...
[37m[1m [ 207.05517778    0.31252736    0.27734265    0.21651585    0.28958353]
[37m[1m [-182.63154185    0.58682144    0.52165765    0.60780495    0.7861383 ]
[37m[1m [ 576.87111722    0.39500004    0.32100001    0.20490003    0.39949998]]
[37m[1m[2023-06-25 05:52:40,164][129146] Max Reward on eval: 1705.5635206703096
[37m[1m[2023-06-25 05:52:40,165][129146] Min Reward on eval: -1465.396292871097
[37m[1m[2023-06-25 05:52:40,165][129146] Mean Reward across all agents: -2.077177412482412
[37m[1m[2023-06-25 05:52:40,165][129146] Average Trajectory Length: 892.7073333333333
[36m[2023-06-25 05:52:40,168][129146] mean_value=-901.0434450159497, max_value=1044.318479181407
[37m[1m[2023-06-25 05:52:40,171][129146] New mean coefficients: [[2.5629182  0.05104244 0.27646652 0.39870298 1.1277874 ]]
[37m[1m[2023-06-25 05:52:40,172][129146] Moving the mean solution point...
[36m[2023-06-25 05:52:49,765][129146] train() took 9.59 seconds to complete
[36m[2023-06-25 05:52:49,765][129146] FPS: 400353.83
[36m[2023-06-25 05:52:49,768][129146] itr=520, itrs=2000, Progress: 26.00%
[37m[1m[2023-06-25 05:52:54,633][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000500
[36m[2023-06-25 05:53:06,643][129146] train() took 11.69 seconds to complete
[36m[2023-06-25 05:53:06,643][129146] FPS: 328412.21
[36m[2023-06-25 05:53:11,450][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:53:11,450][129146] Reward + Measures: [[2527.27183649    0.41421363    0.46944335    0.16657622    0.31030637]]
[37m[1m[2023-06-25 05:53:11,451][129146] Max Reward on eval: 2527.2718364856
[37m[1m[2023-06-25 05:53:11,451][129146] Min Reward on eval: 2527.2718364856
[37m[1m[2023-06-25 05:53:11,451][129146] Mean Reward across all agents: 2527.2718364856
[37m[1m[2023-06-25 05:53:11,451][129146] Average Trajectory Length: 999.7053333333333
[36m[2023-06-25 05:53:16,920][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:53:16,920][129146] Reward + Measures: [[203.09430264   0.42480001   0.2811       0.21040002   0.3522    ]
[37m[1m [962.56388671   0.35800001   0.47330004   0.16180001   0.50620002]
[37m[1m [145.11527247   0.58420008   0.49620005   0.43650004   0.55469996]
[37m[1m ...
[37m[1m [218.07346799   0.3759       0.36919999   0.24980001   0.29609999]
[37m[1m [229.31487335   0.58420002   0.42399999   0.37280002   0.4454    ]
[37m[1m [750.14160888   0.27396443   0.23598173   0.14213365   0.31037599]]
[37m[1m[2023-06-25 05:53:16,920][129146] Max Reward on eval: 2432.256442431826
[37m[1m[2023-06-25 05:53:16,921][129146] Min Reward on eval: -1129.8274391877117
[37m[1m[2023-06-25 05:53:16,921][129146] Mean Reward across all agents: 652.2985065626239
[37m[1m[2023-06-25 05:53:16,921][129146] Average Trajectory Length: 990.1013333333333
[36m[2023-06-25 05:53:16,925][129146] mean_value=-475.4114290148769, max_value=1538.2327370343078
[37m[1m[2023-06-25 05:53:16,927][129146] New mean coefficients: [[ 2.648399   -0.05659587  0.96198845  0.1681349   1.0592982 ]]
[37m[1m[2023-06-25 05:53:16,928][129146] Moving the mean solution point...
[36m[2023-06-25 05:53:26,751][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 05:53:26,752][129146] FPS: 390981.47
[36m[2023-06-25 05:53:26,754][129146] itr=521, itrs=2000, Progress: 26.05%
[36m[2023-06-25 05:53:38,413][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 05:53:38,413][129146] FPS: 329868.84
[36m[2023-06-25 05:53:43,277][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:53:43,278][129146] Reward + Measures: [[2639.40319831    0.40743521    0.48029533    0.15975516    0.29611942]]
[37m[1m[2023-06-25 05:53:43,278][129146] Max Reward on eval: 2639.4031983080877
[37m[1m[2023-06-25 05:53:43,278][129146] Min Reward on eval: 2639.4031983080877
[37m[1m[2023-06-25 05:53:43,279][129146] Mean Reward across all agents: 2639.4031983080877
[37m[1m[2023-06-25 05:53:43,279][129146] Average Trajectory Length: 999.408
[36m[2023-06-25 05:53:48,737][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:53:48,737][129146] Reward + Measures: [[-109.68541871    0.32647857    0.24755096    0.2683894     0.28333655]
[37m[1m [ 577.52847224    0.43779999    0.49670002    0.0631        0.54720002]
[37m[1m [1385.81511416    0.36789998    0.4686        0.1908        0.292     ]
[37m[1m ...
[37m[1m [-786.89234089    0.43600127    0.20860505    0.36687815    0.30889294]
[37m[1m [-851.99151115    0.55227441    0.14626865    0.49174672    0.4215579 ]
[37m[1m [ 763.14477713    0.48769999    0.50579995    0.20729999    0.5388    ]]
[37m[1m[2023-06-25 05:53:48,738][129146] Max Reward on eval: 2410.1084967528705
[37m[1m[2023-06-25 05:53:48,738][129146] Min Reward on eval: -870.0717205161054
[37m[1m[2023-06-25 05:53:48,738][129146] Mean Reward across all agents: 402.93860876355825
[37m[1m[2023-06-25 05:53:48,738][129146] Average Trajectory Length: 969.6753333333334
[36m[2023-06-25 05:53:48,741][129146] mean_value=-742.8865683161786, max_value=989.4601313615805
[37m[1m[2023-06-25 05:53:48,744][129146] New mean coefficients: [[2.6583917  0.17009756 1.166279   0.2329894  0.20493424]]
[37m[1m[2023-06-25 05:53:48,745][129146] Moving the mean solution point...
[36m[2023-06-25 05:53:58,453][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 05:53:58,454][129146] FPS: 395588.68
[36m[2023-06-25 05:53:58,456][129146] itr=522, itrs=2000, Progress: 26.10%
[36m[2023-06-25 05:54:09,910][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 05:54:09,910][129146] FPS: 335891.91
[36m[2023-06-25 05:54:14,625][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:54:14,625][129146] Reward + Measures: [[2761.1889719     0.40060851    0.4815608     0.15573025    0.28311145]]
[37m[1m[2023-06-25 05:54:14,625][129146] Max Reward on eval: 2761.1889718982065
[37m[1m[2023-06-25 05:54:14,625][129146] Min Reward on eval: 2761.1889718982065
[37m[1m[2023-06-25 05:54:14,626][129146] Mean Reward across all agents: 2761.1889718982065
[37m[1m[2023-06-25 05:54:14,626][129146] Average Trajectory Length: 999.661
[36m[2023-06-25 05:54:20,071][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:54:20,072][129146] Reward + Measures: [[1862.74605683    0.43509999    0.4535        0.2235        0.3612    ]
[37m[1m [1071.47980492    0.38509998    0.41529998    0.16489999    0.3811    ]
[37m[1m [1489.45349921    0.40899998    0.3827        0.20039999    0.2375    ]
[37m[1m ...
[37m[1m [ 847.57116059    0.40759999    0.51560003    0.20469999    0.4962    ]
[37m[1m [-284.86582553    0.35800001    0.40920001    0.23240001    0.36029997]
[37m[1m [1562.16604481    0.42519999    0.47489998    0.2633        0.33870003]]
[37m[1m[2023-06-25 05:54:20,072][129146] Max Reward on eval: 2692.840180246346
[37m[1m[2023-06-25 05:54:20,072][129146] Min Reward on eval: -856.623637060565
[37m[1m[2023-06-25 05:54:20,072][129146] Mean Reward across all agents: 682.7242223494394
[37m[1m[2023-06-25 05:54:20,072][129146] Average Trajectory Length: 987.6883333333333
[36m[2023-06-25 05:54:20,075][129146] mean_value=-628.2945764765605, max_value=812.5893829729947
[37m[1m[2023-06-25 05:54:20,077][129146] New mean coefficients: [[2.2399707  0.6044769  0.31479126 0.27058074 0.30745506]]
[37m[1m[2023-06-25 05:54:20,078][129146] Moving the mean solution point...
[36m[2023-06-25 05:54:29,657][129146] train() took 9.58 seconds to complete
[36m[2023-06-25 05:54:29,657][129146] FPS: 400958.07
[36m[2023-06-25 05:54:29,659][129146] itr=523, itrs=2000, Progress: 26.15%
[36m[2023-06-25 05:54:41,049][129146] train() took 11.37 seconds to complete
[36m[2023-06-25 05:54:41,050][129146] FPS: 337671.51
[36m[2023-06-25 05:54:45,820][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:54:45,820][129146] Reward + Measures: [[2849.54139809    0.40428844    0.48862094    0.15025234    0.27609807]]
[37m[1m[2023-06-25 05:54:45,820][129146] Max Reward on eval: 2849.5413980945596
[37m[1m[2023-06-25 05:54:45,821][129146] Min Reward on eval: 2849.5413980945596
[37m[1m[2023-06-25 05:54:45,821][129146] Mean Reward across all agents: 2849.5413980945596
[37m[1m[2023-06-25 05:54:45,821][129146] Average Trajectory Length: 999.7756666666667
[36m[2023-06-25 05:54:51,293][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:54:51,293][129146] Reward + Measures: [[ 304.30618244    0.52179998    0.3644        0.26770002    0.26709998]
[37m[1m [ 648.45596439    0.56990004    0.37330002    0.1664        0.31810004]
[37m[1m [ 418.85553932    0.40249997    0.36789998    0.23330002    0.36820003]
[37m[1m ...
[37m[1m [-603.40537521    0.09077442    0.53966516    0.35453025    0.58993024]
[37m[1m [-346.87381599    0.33764568    0.26438347    0.41039529    0.11043701]
[37m[1m [1533.46509629    0.4427        0.38600001    0.2402        0.31569999]]
[37m[1m[2023-06-25 05:54:51,305][129146] Max Reward on eval: 2691.5878060592977
[37m[1m[2023-06-25 05:54:51,306][129146] Min Reward on eval: -1285.6737619511725
[37m[1m[2023-06-25 05:54:51,306][129146] Mean Reward across all agents: 467.480944790845
[37m[1m[2023-06-25 05:54:51,306][129146] Average Trajectory Length: 971.0146666666666
[36m[2023-06-25 05:54:51,311][129146] mean_value=-565.7437349613282, max_value=1311.877672502061
[37m[1m[2023-06-25 05:54:51,314][129146] New mean coefficients: [[ 2.4730005   0.82071203  0.84652877  0.4199565  -0.05964619]]
[37m[1m[2023-06-25 05:54:51,315][129146] Moving the mean solution point...
[36m[2023-06-25 05:55:00,941][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 05:55:00,941][129146] FPS: 398973.69
[36m[2023-06-25 05:55:00,943][129146] itr=524, itrs=2000, Progress: 26.20%
[36m[2023-06-25 05:55:12,421][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 05:55:12,421][129146] FPS: 335096.54
[36m[2023-06-25 05:55:17,170][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:55:17,171][129146] Reward + Measures: [[2942.46215466    0.40767097    0.4924379     0.15105847    0.27361065]]
[37m[1m[2023-06-25 05:55:17,171][129146] Max Reward on eval: 2942.4621546558924
[37m[1m[2023-06-25 05:55:17,171][129146] Min Reward on eval: 2942.4621546558924
[37m[1m[2023-06-25 05:55:17,171][129146] Mean Reward across all agents: 2942.4621546558924
[37m[1m[2023-06-25 05:55:17,172][129146] Average Trajectory Length: 999.6089999999999
[36m[2023-06-25 05:55:22,869][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:55:22,869][129146] Reward + Measures: [[1061.86106479    0.23539999    0.38030002    0.19410002    0.44250003]
[37m[1m [ 703.39516789    0.33625442    0.29106098    0.16667126    0.18849939]
[37m[1m [ 498.76171511    0.33031899    0.28115392    0.15641461    0.18814769]
[37m[1m ...
[37m[1m [1737.07941505    0.33800003    0.4172        0.16970001    0.37960005]
[37m[1m [1341.03419475    0.38929999    0.49880001    0.156         0.44280002]
[37m[1m [ 853.39318882    0.45900002    0.39500004    0.2139        0.23440002]]
[37m[1m[2023-06-25 05:55:22,870][129146] Max Reward on eval: 2694.1274832515046
[37m[1m[2023-06-25 05:55:22,870][129146] Min Reward on eval: -677.9017138157039
[37m[1m[2023-06-25 05:55:22,870][129146] Mean Reward across all agents: 837.0508688321615
[37m[1m[2023-06-25 05:55:22,870][129146] Average Trajectory Length: 971.4676666666667
[36m[2023-06-25 05:55:22,874][129146] mean_value=-472.46459585330894, max_value=1618.9774046632297
[37m[1m[2023-06-25 05:55:22,877][129146] New mean coefficients: [[2.3361363  0.6584218  1.2676475  0.12460539 0.8769704 ]]
[37m[1m[2023-06-25 05:55:22,878][129146] Moving the mean solution point...
[36m[2023-06-25 05:55:32,622][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 05:55:32,623][129146] FPS: 394148.92
[36m[2023-06-25 05:55:32,625][129146] itr=525, itrs=2000, Progress: 26.25%
[36m[2023-06-25 05:55:44,175][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 05:55:44,175][129146] FPS: 333091.62
[36m[2023-06-25 05:55:48,966][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:55:48,966][129146] Reward + Measures: [[3083.33190704    0.39217684    0.48716339    0.14183664    0.26435193]]
[37m[1m[2023-06-25 05:55:48,966][129146] Max Reward on eval: 3083.3319070392954
[37m[1m[2023-06-25 05:55:48,967][129146] Min Reward on eval: 3083.3319070392954
[37m[1m[2023-06-25 05:55:48,967][129146] Mean Reward across all agents: 3083.3319070392954
[37m[1m[2023-06-25 05:55:48,967][129146] Average Trajectory Length: 999.9183333333333
[36m[2023-06-25 05:55:54,487][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:55:54,488][129146] Reward + Measures: [[1896.16866785    0.33969998    0.43080002    0.1719        0.27670002]
[37m[1m [ 888.61974931    0.42820007    0.42950001    0.32300001    0.4039    ]
[37m[1m [-409.09480846    0.27751002    0.20049258    0.17660311    0.16880606]
[37m[1m ...
[37m[1m [ 437.78791092    0.3987        0.41750002    0.35089999    0.42099997]
[37m[1m [ 392.07434056    0.44190001    0.3637        0.32640001    0.40169999]
[37m[1m [ 419.37619802    0.48709998    0.30240002    0.25299999    0.34310004]]
[37m[1m[2023-06-25 05:55:54,488][129146] Max Reward on eval: 2643.2464200373042
[37m[1m[2023-06-25 05:55:54,488][129146] Min Reward on eval: -732.9637258524133
[37m[1m[2023-06-25 05:55:54,489][129146] Mean Reward across all agents: 413.50428377794526
[37m[1m[2023-06-25 05:55:54,489][129146] Average Trajectory Length: 916.5973333333333
[36m[2023-06-25 05:55:54,496][129146] mean_value=-500.2193898115751, max_value=1116.886084797719
[37m[1m[2023-06-25 05:55:54,498][129146] New mean coefficients: [[1.8956003  0.90345216 0.5052     0.44213524 1.5350533 ]]
[37m[1m[2023-06-25 05:55:54,499][129146] Moving the mean solution point...
[36m[2023-06-25 05:56:04,187][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 05:56:04,187][129146] FPS: 396438.44
[36m[2023-06-25 05:56:04,189][129146] itr=526, itrs=2000, Progress: 26.30%
[36m[2023-06-25 05:56:15,585][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 05:56:15,586][129146] FPS: 337503.02
[36m[2023-06-25 05:56:20,436][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:56:20,437][129146] Reward + Measures: [[3198.53885241    0.39840177    0.4754003     0.14093243    0.27066255]]
[37m[1m[2023-06-25 05:56:20,437][129146] Max Reward on eval: 3198.538852410398
[37m[1m[2023-06-25 05:56:20,437][129146] Min Reward on eval: 3198.538852410398
[37m[1m[2023-06-25 05:56:20,437][129146] Mean Reward across all agents: 3198.538852410398
[37m[1m[2023-06-25 05:56:20,437][129146] Average Trajectory Length: 999.634
[36m[2023-06-25 05:56:25,839][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:56:25,839][129146] Reward + Measures: [[1389.37954945    0.47459999    0.51719999    0.20900002    0.44490001]
[37m[1m [1271.73351749    0.45090005    0.54870003    0.22020002    0.40100002]
[37m[1m [1072.32879281    0.27449998    0.2994        0.12980001    0.27879998]
[37m[1m ...
[37m[1m [1144.80325043    0.35769999    0.3035        0.24070001    0.28350002]
[37m[1m [ 682.8935403     0.36250001    0.51719999    0.1912        0.47999999]
[37m[1m [1485.99787128    0.32319999    0.32880002    0.15259999    0.24370001]]
[37m[1m[2023-06-25 05:56:25,839][129146] Max Reward on eval: 3102.922829893511
[37m[1m[2023-06-25 05:56:25,840][129146] Min Reward on eval: -436.4225832405267
[37m[1m[2023-06-25 05:56:25,840][129146] Mean Reward across all agents: 1209.3987368635817
[37m[1m[2023-06-25 05:56:25,840][129146] Average Trajectory Length: 996.9393333333333
[36m[2023-06-25 05:56:25,844][129146] mean_value=-289.95532943268233, max_value=1139.638089034341
[37m[1m[2023-06-25 05:56:25,847][129146] New mean coefficients: [[1.7533995  1.2628076  0.35045114 0.37919918 0.85895324]]
[37m[1m[2023-06-25 05:56:25,848][129146] Moving the mean solution point...
[36m[2023-06-25 05:56:35,585][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 05:56:35,585][129146] FPS: 394429.82
[36m[2023-06-25 05:56:35,588][129146] itr=527, itrs=2000, Progress: 26.35%
[36m[2023-06-25 05:56:47,001][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 05:56:47,002][129146] FPS: 337001.47
[36m[2023-06-25 05:56:51,756][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:56:51,756][129146] Reward + Measures: [[3301.56635802    0.40952262    0.45232233    0.135217      0.27707735]]
[37m[1m[2023-06-25 05:56:51,757][129146] Max Reward on eval: 3301.566358015449
[37m[1m[2023-06-25 05:56:51,757][129146] Min Reward on eval: 3301.566358015449
[37m[1m[2023-06-25 05:56:51,757][129146] Mean Reward across all agents: 3301.566358015449
[37m[1m[2023-06-25 05:56:51,757][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:56:57,185][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:56:57,186][129146] Reward + Measures: [[-421.35927101    0.47709998    0.2518        0.42870003    0.22480002]
[37m[1m [2606.28319389    0.47939998    0.48030004    0.17820001    0.3179    ]
[37m[1m [1391.5334434     0.39920002    0.465         0.2235        0.24770001]
[37m[1m ...
[37m[1m [-281.58227617    0.54100001    0.52190006    0.45469999    0.49160001]
[37m[1m [1573.33722454    0.47469997    0.51380008    0.2465        0.26830003]
[37m[1m [ 357.35232524    0.55840003    0.32960001    0.39360005    0.21259999]]
[37m[1m[2023-06-25 05:56:57,186][129146] Max Reward on eval: 2913.2929761200676
[37m[1m[2023-06-25 05:56:57,186][129146] Min Reward on eval: -1897.099080966413
[37m[1m[2023-06-25 05:56:57,187][129146] Mean Reward across all agents: 549.5887648689877
[37m[1m[2023-06-25 05:56:57,187][129146] Average Trajectory Length: 981.0103333333333
[36m[2023-06-25 05:56:57,191][129146] mean_value=-621.1447606320027, max_value=880.5363481861243
[37m[1m[2023-06-25 05:56:57,194][129146] New mean coefficients: [[ 1.8332918   1.4474143  -0.16124806  0.18652011  0.28747213]]
[37m[1m[2023-06-25 05:56:57,195][129146] Moving the mean solution point...
[36m[2023-06-25 05:57:06,955][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 05:57:06,955][129146] FPS: 393497.05
[36m[2023-06-25 05:57:06,957][129146] itr=528, itrs=2000, Progress: 26.40%
[36m[2023-06-25 05:57:18,535][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 05:57:18,536][129146] FPS: 332233.78
[36m[2023-06-25 05:57:23,326][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:57:23,326][129146] Reward + Measures: [[3415.13280755    0.40956163    0.44152132    0.128334      0.27968535]]
[37m[1m[2023-06-25 05:57:23,326][129146] Max Reward on eval: 3415.1328075544516
[37m[1m[2023-06-25 05:57:23,327][129146] Min Reward on eval: 3415.1328075544516
[37m[1m[2023-06-25 05:57:23,327][129146] Mean Reward across all agents: 3415.1328075544516
[37m[1m[2023-06-25 05:57:23,327][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 05:57:28,991][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:57:28,991][129146] Reward + Measures: [[1486.07034425    0.35720003    0.54010004    0.1876        0.33320004]
[37m[1m [1526.39052165    0.32240003    0.49770004    0.1613        0.28040001]
[37m[1m [2007.42005039    0.3748        0.36939999    0.1741        0.2246    ]
[37m[1m ...
[37m[1m [ 300.84097336    0.3831        0.30809999    0.25729999    0.19950001]
[37m[1m [ 948.50707561    0.35399997    0.29260001    0.2198        0.21180001]
[37m[1m [ -34.46120488    0.4553        0.32620001    0.21850002    0.2579    ]]
[37m[1m[2023-06-25 05:57:28,992][129146] Max Reward on eval: 2846.7261632181703
[37m[1m[2023-06-25 05:57:28,992][129146] Min Reward on eval: -563.7278179194545
[37m[1m[2023-06-25 05:57:28,992][129146] Mean Reward across all agents: 816.9711248699886
[37m[1m[2023-06-25 05:57:28,992][129146] Average Trajectory Length: 987.9196666666667
[36m[2023-06-25 05:57:28,996][129146] mean_value=-506.733798178034, max_value=964.6090588538158
[37m[1m[2023-06-25 05:57:28,999][129146] New mean coefficients: [[ 1.2753108   1.002818   -0.21522331  0.18667705  0.3966354 ]]
[37m[1m[2023-06-25 05:57:29,000][129146] Moving the mean solution point...
[36m[2023-06-25 05:57:38,699][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 05:57:38,699][129146] FPS: 395961.87
[36m[2023-06-25 05:57:38,702][129146] itr=529, itrs=2000, Progress: 26.45%
[36m[2023-06-25 05:57:50,118][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 05:57:50,119][129146] FPS: 336988.70
[36m[2023-06-25 05:57:54,994][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:57:54,995][129146] Reward + Measures: [[1969.76335033    0.30109045    0.33349669    0.14024438    0.35397398]]
[37m[1m[2023-06-25 05:57:54,995][129146] Max Reward on eval: 1969.76335033124
[37m[1m[2023-06-25 05:57:54,995][129146] Min Reward on eval: 1969.76335033124
[37m[1m[2023-06-25 05:57:54,996][129146] Mean Reward across all agents: 1969.76335033124
[37m[1m[2023-06-25 05:57:54,996][129146] Average Trajectory Length: 967.6023333333333
[36m[2023-06-25 05:58:00,464][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:58:00,465][129146] Reward + Measures: [[ 571.64438239    0.49989995    0.45879999    0.35279998    0.4862    ]
[37m[1m [1570.78767513    0.30099639    0.32666555    0.16069816    0.34246659]
[37m[1m [ 589.80601966    0.3989        0.55849999    0.09670001    0.53789997]
[37m[1m ...
[37m[1m [ 429.85765423    0.36450002    0.58690006    0.48650002    0.59840006]
[37m[1m [ -89.06667071    0.25910002    0.80250007    0.77079999    0.89560002]
[37m[1m [ 484.38182858    0.28750002    0.40510002    0.20740001    0.40549999]]
[37m[1m[2023-06-25 05:58:00,465][129146] Max Reward on eval: 2361.509102902026
[37m[1m[2023-06-25 05:58:00,465][129146] Min Reward on eval: -1655.088539024815
[37m[1m[2023-06-25 05:58:00,466][129146] Mean Reward across all agents: 269.78932112523233
[37m[1m[2023-06-25 05:58:00,466][129146] Average Trajectory Length: 976.8846666666666
[36m[2023-06-25 05:58:00,477][129146] mean_value=-78.91328033727346, max_value=1307.314538727727
[37m[1m[2023-06-25 05:58:00,480][129146] New mean coefficients: [[ 1.4422032   1.0783565  -0.31343788  0.05979115  0.75327694]]
[37m[1m[2023-06-25 05:58:00,481][129146] Moving the mean solution point...
[36m[2023-06-25 05:58:10,226][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 05:58:10,226][129146] FPS: 394117.78
[36m[2023-06-25 05:58:10,228][129146] itr=530, itrs=2000, Progress: 26.50%
[37m[1m[2023-06-25 05:58:15,116][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000510
[36m[2023-06-25 05:58:26,875][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 05:58:26,875][129146] FPS: 335236.36
[36m[2023-06-25 05:58:31,552][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:58:31,552][129146] Reward + Measures: [[2207.60751975    0.30921015    0.33186573    0.14561751    0.34493962]]
[37m[1m[2023-06-25 05:58:31,553][129146] Max Reward on eval: 2207.607519747836
[37m[1m[2023-06-25 05:58:31,553][129146] Min Reward on eval: 2207.607519747836
[37m[1m[2023-06-25 05:58:31,553][129146] Mean Reward across all agents: 2207.607519747836
[37m[1m[2023-06-25 05:58:31,553][129146] Average Trajectory Length: 962.0703333333333
[36m[2023-06-25 05:58:37,040][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:58:37,041][129146] Reward + Measures: [[1198.4997188     0.22993922    0.27606079    0.23334312    0.37996861]
[37m[1m [ 677.87373902    0.37670001    0.3836        0.30860001    0.42570001]
[37m[1m [1035.89875287    0.60299999    0.38780001    0.1893        0.40760002]
[37m[1m ...
[37m[1m [2219.24214395    0.36669999    0.40640002    0.14050001    0.39840001]
[37m[1m [1417.53845569    0.29575914    0.29269063    0.21433575    0.35484287]
[37m[1m [1617.41407783    0.36460003    0.2836        0.21089999    0.32679999]]
[37m[1m[2023-06-25 05:58:37,041][129146] Max Reward on eval: 2349.0717829264468
[37m[1m[2023-06-25 05:58:37,041][129146] Min Reward on eval: -964.9891053380212
[37m[1m[2023-06-25 05:58:37,041][129146] Mean Reward across all agents: 713.1558357003872
[37m[1m[2023-06-25 05:58:37,041][129146] Average Trajectory Length: 927.7563333333333
[36m[2023-06-25 05:58:37,045][129146] mean_value=-461.4079520067532, max_value=2069.7355752455123
[37m[1m[2023-06-25 05:58:37,048][129146] New mean coefficients: [[ 1.2415771   0.35567313 -0.12782435  0.08884609  0.7167413 ]]
[37m[1m[2023-06-25 05:58:37,049][129146] Moving the mean solution point...
[36m[2023-06-25 05:58:46,796][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 05:58:46,796][129146] FPS: 394019.37
[36m[2023-06-25 05:58:46,799][129146] itr=531, itrs=2000, Progress: 26.55%
[36m[2023-06-25 05:58:58,199][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 05:58:58,200][129146] FPS: 337430.05
[36m[2023-06-25 05:59:02,945][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:59:02,945][129146] Reward + Measures: [[2386.05306698    0.3100765     0.33077359    0.14609706    0.32942715]]
[37m[1m[2023-06-25 05:59:02,945][129146] Max Reward on eval: 2386.053066976407
[37m[1m[2023-06-25 05:59:02,945][129146] Min Reward on eval: 2386.053066976407
[37m[1m[2023-06-25 05:59:02,946][129146] Mean Reward across all agents: 2386.053066976407
[37m[1m[2023-06-25 05:59:02,946][129146] Average Trajectory Length: 960.6323333333333
[36m[2023-06-25 05:59:08,464][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:59:08,465][129146] Reward + Measures: [[  651.55935497     0.47300005     0.3946         0.23629999
[37m[1m      0.38010001]
[37m[1m [-1200.57406839     0.92290002     0.0018         0.98520005
[37m[1m      0.92140001]
[37m[1m [ 2070.06105219     0.27344304     0.35078225     0.18621393
[37m[1m      0.30397975]
[37m[1m ...
[37m[1m [ 1150.61030538     0.31930003     0.50630003     0.0921
[37m[1m      0.59540004]
[37m[1m [ 1148.96286434     0.32710001     0.3554         0.2297
[37m[1m      0.40149999]
[37m[1m [    6.85600324     0.30742532     0.24877833     0.2660518
[37m[1m      0.30782166]]
[37m[1m[2023-06-25 05:59:08,465][129146] Max Reward on eval: 2569.484461486712
[37m[1m[2023-06-25 05:59:08,465][129146] Min Reward on eval: -1200.5740683943034
[37m[1m[2023-06-25 05:59:08,465][129146] Mean Reward across all agents: 990.489149680868
[37m[1m[2023-06-25 05:59:08,466][129146] Average Trajectory Length: 979.2483333333333
[36m[2023-06-25 05:59:08,470][129146] mean_value=-324.40052962917247, max_value=1144.995932224118
[37m[1m[2023-06-25 05:59:08,473][129146] New mean coefficients: [[ 1.7257686   0.45573914  0.3590207  -0.03853106  0.5439639 ]]
[37m[1m[2023-06-25 05:59:08,474][129146] Moving the mean solution point...
[36m[2023-06-25 05:59:18,280][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 05:59:18,280][129146] FPS: 391650.78
[36m[2023-06-25 05:59:18,283][129146] itr=532, itrs=2000, Progress: 26.60%
[36m[2023-06-25 05:59:29,887][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 05:59:29,887][129146] FPS: 331555.68
[36m[2023-06-25 05:59:34,648][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:59:34,648][129146] Reward + Measures: [[2500.06988538    0.31232926    0.33092743    0.14884721    0.32216567]]
[37m[1m[2023-06-25 05:59:34,648][129146] Max Reward on eval: 2500.0698853826866
[37m[1m[2023-06-25 05:59:34,649][129146] Min Reward on eval: 2500.0698853826866
[37m[1m[2023-06-25 05:59:34,649][129146] Mean Reward across all agents: 2500.0698853826866
[37m[1m[2023-06-25 05:59:34,649][129146] Average Trajectory Length: 958.9219999999999
[36m[2023-06-25 05:59:40,144][129146] Finished Evaluation Step
[37m[1m[2023-06-25 05:59:40,145][129146] Reward + Measures: [[ 771.20709564    0.45889997    0.55930001    0.21170001    0.40540001]
[37m[1m [-421.48747738    0.62420005    0.40110001    0.32720003    0.56380004]
[37m[1m [ 648.40046225    0.22420001    0.31529999    0.14380001    0.3398    ]
[37m[1m ...
[37m[1m [1489.31548082    0.37890002    0.3899        0.22669999    0.37580001]
[37m[1m [1942.51487115    0.25481865    0.31413221    0.17385423    0.3363921 ]
[37m[1m [ 926.07785105    0.4632        0.4693        0.23480003    0.51300001]]
[37m[1m[2023-06-25 05:59:40,145][129146] Max Reward on eval: 2592.244007058046
[37m[1m[2023-06-25 05:59:40,145][129146] Min Reward on eval: -442.97524178612514
[37m[1m[2023-06-25 05:59:40,145][129146] Mean Reward across all agents: 1097.171447264415
[37m[1m[2023-06-25 05:59:40,146][129146] Average Trajectory Length: 951.764
[36m[2023-06-25 05:59:40,150][129146] mean_value=-417.93729766562103, max_value=2188.1981661906116
[37m[1m[2023-06-25 05:59:40,152][129146] New mean coefficients: [[ 2.1691115  -0.29788405  0.45773515  0.17768845  0.7964591 ]]
[37m[1m[2023-06-25 05:59:40,153][129146] Moving the mean solution point...
[36m[2023-06-25 05:59:49,863][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 05:59:49,864][129146] FPS: 395537.09
[36m[2023-06-25 05:59:49,866][129146] itr=533, itrs=2000, Progress: 26.65%
[36m[2023-06-25 06:00:01,466][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 06:00:01,466][129146] FPS: 331553.60
[36m[2023-06-25 06:00:06,318][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:00:06,318][129146] Reward + Measures: [[2641.93539582    0.31401217    0.33062878    0.14306122    0.30791742]]
[37m[1m[2023-06-25 06:00:06,319][129146] Max Reward on eval: 2641.935395815027
[37m[1m[2023-06-25 06:00:06,319][129146] Min Reward on eval: 2641.935395815027
[37m[1m[2023-06-25 06:00:06,319][129146] Mean Reward across all agents: 2641.935395815027
[37m[1m[2023-06-25 06:00:06,319][129146] Average Trajectory Length: 966.65
[36m[2023-06-25 06:00:11,820][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:00:11,820][129146] Reward + Measures: [[1114.62991303    0.37799999    0.4506        0.09680001    0.55599999]
[37m[1m [ 896.72780684    0.43099999    0.53080004    0.28709999    0.53249997]
[37m[1m [ 728.43345742    0.46739998    0.62440008    0.14230001    0.79170001]
[37m[1m ...
[37m[1m [ 183.51562656    0.30811858    0.26496431    0.19150846    0.25971827]
[37m[1m [ 142.65322541    0.29660007    0.22965646    0.15060213    0.23482807]
[37m[1m [ 757.88515474    0.26769671    0.23286453    0.12500703    0.20707779]]
[37m[1m[2023-06-25 06:00:11,820][129146] Max Reward on eval: 3008.411150615383
[37m[1m[2023-06-25 06:00:11,821][129146] Min Reward on eval: -430.84767443157614
[37m[1m[2023-06-25 06:00:11,821][129146] Mean Reward across all agents: 638.4373024960893
[37m[1m[2023-06-25 06:00:11,821][129146] Average Trajectory Length: 843.3233333333333
[36m[2023-06-25 06:00:11,825][129146] mean_value=-953.2103304531239, max_value=1194.840663832542
[37m[1m[2023-06-25 06:00:11,828][129146] New mean coefficients: [[ 2.600284   -0.6871853   0.52108747 -0.17653474 -0.37833786]]
[37m[1m[2023-06-25 06:00:11,829][129146] Moving the mean solution point...
[36m[2023-06-25 06:00:21,597][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 06:00:21,598][129146] FPS: 393164.52
[36m[2023-06-25 06:00:21,600][129146] itr=534, itrs=2000, Progress: 26.70%
[36m[2023-06-25 06:00:33,109][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 06:00:33,110][129146] FPS: 334285.14
[36m[2023-06-25 06:00:37,937][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:00:37,937][129146] Reward + Measures: [[2750.48198502    0.31753549    0.33619249    0.14454071    0.3052097 ]]
[37m[1m[2023-06-25 06:00:37,937][129146] Max Reward on eval: 2750.4819850165923
[37m[1m[2023-06-25 06:00:37,938][129146] Min Reward on eval: 2750.4819850165923
[37m[1m[2023-06-25 06:00:37,938][129146] Mean Reward across all agents: 2750.4819850165923
[37m[1m[2023-06-25 06:00:37,938][129146] Average Trajectory Length: 970.2103333333333
[36m[2023-06-25 06:00:43,454][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:00:43,454][129146] Reward + Measures: [[ 212.69968391    0.36210001    0.22340003    0.25460002    0.2297    ]
[37m[1m [ 905.87248423    0.48965454    0.31332323    0.28530201    0.38505355]
[37m[1m [ 169.76562949    0.34177873    0.29292554    0.18784043    0.19996808]
[37m[1m ...
[37m[1m [-234.9921663     0.32789999    0.25839999    0.1796        0.17960002]
[37m[1m [1609.01611645    0.29088837    0.42693207    0.19419302    0.38877571]
[37m[1m [ -53.73133657    0.34850004    0.2638        0.2289        0.23819999]]
[37m[1m[2023-06-25 06:00:43,455][129146] Max Reward on eval: 2661.070284377015
[37m[1m[2023-06-25 06:00:43,455][129146] Min Reward on eval: -723.1845186644699
[37m[1m[2023-06-25 06:00:43,455][129146] Mean Reward across all agents: 873.5213607973873
[37m[1m[2023-06-25 06:00:43,455][129146] Average Trajectory Length: 962.6466666666666
[36m[2023-06-25 06:00:43,459][129146] mean_value=-596.0607249215866, max_value=2467.1557647209247
[37m[1m[2023-06-25 06:00:43,461][129146] New mean coefficients: [[ 2.9684587  -0.51054716  0.52271205 -0.13332328  0.3165664 ]]
[37m[1m[2023-06-25 06:00:43,462][129146] Moving the mean solution point...
[36m[2023-06-25 06:00:53,187][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 06:00:53,187][129146] FPS: 394929.73
[36m[2023-06-25 06:00:53,189][129146] itr=535, itrs=2000, Progress: 26.75%
[36m[2023-06-25 06:01:04,639][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 06:01:04,639][129146] FPS: 335918.20
[36m[2023-06-25 06:01:09,412][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:01:09,413][129146] Reward + Measures: [[2880.23673722    0.31578523    0.33866963    0.14622553    0.30281556]]
[37m[1m[2023-06-25 06:01:09,413][129146] Max Reward on eval: 2880.2367372199856
[37m[1m[2023-06-25 06:01:09,413][129146] Min Reward on eval: 2880.2367372199856
[37m[1m[2023-06-25 06:01:09,413][129146] Mean Reward across all agents: 2880.2367372199856
[37m[1m[2023-06-25 06:01:09,414][129146] Average Trajectory Length: 970.2263333333333
[36m[2023-06-25 06:01:15,063][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:01:15,064][129146] Reward + Measures: [[1373.57335728    0.34059998    0.35520002    0.24530001    0.45950004]
[37m[1m [ 108.47435828    0.0831        0.85409993    0.55350006    0.93339998]
[37m[1m [1988.71792812    0.24870001    0.2617        0.15720001    0.23869999]
[37m[1m ...
[37m[1m [ 468.39334644    0.3976        0.56380004    0.074         0.5266    ]
[37m[1m [ 779.53349294    0.28796998    0.29069689    0.14170384    0.2335985 ]
[37m[1m [ 998.72302854    0.27320001    0.4677        0.13310002    0.49520001]]
[37m[1m[2023-06-25 06:01:15,064][129146] Max Reward on eval: 2834.7521696157755
[37m[1m[2023-06-25 06:01:15,064][129146] Min Reward on eval: -882.1474328768905
[37m[1m[2023-06-25 06:01:15,065][129146] Mean Reward across all agents: 958.6875954791855
[37m[1m[2023-06-25 06:01:15,065][129146] Average Trajectory Length: 971.0936666666666
[36m[2023-06-25 06:01:15,072][129146] mean_value=-86.81397565104615, max_value=2264.8031048287903
[37m[1m[2023-06-25 06:01:15,075][129146] New mean coefficients: [[ 3.5502195  -0.8481098   0.71259034 -0.19813544 -0.21118516]]
[37m[1m[2023-06-25 06:01:15,076][129146] Moving the mean solution point...
[36m[2023-06-25 06:01:24,886][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 06:01:24,886][129146] FPS: 391506.85
[36m[2023-06-25 06:01:24,888][129146] itr=536, itrs=2000, Progress: 26.80%
[36m[2023-06-25 06:01:36,542][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 06:01:36,543][129146] FPS: 330031.47
[36m[2023-06-25 06:01:41,343][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:01:41,344][129146] Reward + Measures: [[3006.70238784    0.31583294    0.34624854    0.14606959    0.29553258]]
[37m[1m[2023-06-25 06:01:41,344][129146] Max Reward on eval: 3006.7023878381897
[37m[1m[2023-06-25 06:01:41,344][129146] Min Reward on eval: 3006.7023878381897
[37m[1m[2023-06-25 06:01:41,344][129146] Mean Reward across all agents: 3006.7023878381897
[37m[1m[2023-06-25 06:01:41,345][129146] Average Trajectory Length: 978.8783333333333
[36m[2023-06-25 06:01:46,792][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:01:46,792][129146] Reward + Measures: [[2112.93730281    0.26100001    0.30840001    0.1991        0.33239999]
[37m[1m [1544.80441471    0.26818052    0.28260356    0.17636542    0.34320113]
[37m[1m [ 279.83631781    0.15730001    0.2089        0.1585        0.17819999]
[37m[1m ...
[37m[1m [ 967.62314724    0.33460003    0.29170001    0.22650002    0.4021    ]
[37m[1m [ 390.41048949    0.50950003    0.3881        0.15090001    0.39100003]
[37m[1m [1328.74151586    0.30632976    0.26050979    0.12965515    0.32475874]]
[37m[1m[2023-06-25 06:01:46,793][129146] Max Reward on eval: 2922.952983891172
[37m[1m[2023-06-25 06:01:46,793][129146] Min Reward on eval: -157.7226768357563
[37m[1m[2023-06-25 06:01:46,793][129146] Mean Reward across all agents: 1151.6093663412937
[37m[1m[2023-06-25 06:01:46,793][129146] Average Trajectory Length: 893.9466666666666
[36m[2023-06-25 06:01:46,798][129146] mean_value=-416.63038982817574, max_value=1696.0442419243877
[37m[1m[2023-06-25 06:01:46,800][129146] New mean coefficients: [[ 4.114339   -0.6003581   0.679937   -0.24525005  0.21959433]]
[37m[1m[2023-06-25 06:01:46,801][129146] Moving the mean solution point...
[36m[2023-06-25 06:01:56,607][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 06:01:56,607][129146] FPS: 391690.26
[36m[2023-06-25 06:01:56,609][129146] itr=537, itrs=2000, Progress: 26.85%
[36m[2023-06-25 06:02:08,204][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 06:02:08,204][129146] FPS: 331716.22
[36m[2023-06-25 06:02:13,083][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:02:13,083][129146] Reward + Measures: [[3175.93210245    0.31067556    0.35197809    0.13827385    0.28433812]]
[37m[1m[2023-06-25 06:02:13,083][129146] Max Reward on eval: 3175.932102449011
[37m[1m[2023-06-25 06:02:13,084][129146] Min Reward on eval: 3175.932102449011
[37m[1m[2023-06-25 06:02:13,084][129146] Mean Reward across all agents: 3175.932102449011
[37m[1m[2023-06-25 06:02:13,084][129146] Average Trajectory Length: 985.2026666666667
[36m[2023-06-25 06:02:18,290][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:02:18,291][129146] Reward + Measures: [[1169.89559349    0.36047816    0.34955046    0.05867228    0.34295261]
[37m[1m [2175.90233906    0.29970002    0.38299999    0.20020001    0.27080002]
[37m[1m [1020.74373984    0.26970002    0.37009999    0.16530001    0.25009999]
[37m[1m ...
[37m[1m [ 453.85808943    0.29340002    0.49939999    0.21410003    0.50819999]
[37m[1m [ 216.12346836    0.1839        0.30650002    0.19020002    0.22239999]
[37m[1m [ 856.82845156    0.37271282    0.30473834    0.11472332    0.28865263]]
[37m[1m[2023-06-25 06:02:18,291][129146] Max Reward on eval: 2915.2832320800517
[37m[1m[2023-06-25 06:02:18,291][129146] Min Reward on eval: -374.26125305683524
[37m[1m[2023-06-25 06:02:18,291][129146] Mean Reward across all agents: 1167.466238592329
[37m[1m[2023-06-25 06:02:18,292][129146] Average Trajectory Length: 929.2786666666666
[36m[2023-06-25 06:02:18,295][129146] mean_value=-768.2926175688114, max_value=1078.8363036471549
[37m[1m[2023-06-25 06:02:18,298][129146] New mean coefficients: [[ 4.3842654   0.16368288  0.21471822 -0.21332341  0.1371404 ]]
[37m[1m[2023-06-25 06:02:18,299][129146] Moving the mean solution point...
[36m[2023-06-25 06:02:28,115][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 06:02:28,115][129146] FPS: 391273.13
[36m[2023-06-25 06:02:28,117][129146] itr=538, itrs=2000, Progress: 26.90%
[36m[2023-06-25 06:02:39,754][129146] train() took 11.62 seconds to complete
[36m[2023-06-25 06:02:39,754][129146] FPS: 330594.07
[36m[2023-06-25 06:02:44,628][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:02:44,628][129146] Reward + Measures: [[3269.89343759    0.31006932    0.3465718     0.13692251    0.28307173]]
[37m[1m[2023-06-25 06:02:44,629][129146] Max Reward on eval: 3269.8934375897334
[37m[1m[2023-06-25 06:02:44,629][129146] Min Reward on eval: 3269.8934375897334
[37m[1m[2023-06-25 06:02:44,629][129146] Mean Reward across all agents: 3269.8934375897334
[37m[1m[2023-06-25 06:02:44,629][129146] Average Trajectory Length: 980.9186666666666
[36m[2023-06-25 06:02:50,141][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:02:50,141][129146] Reward + Measures: [[1008.2486176     0.30231884    0.32931718    0.18947239    0.24387865]
[37m[1m [ 930.4460667     0.2748588     0.28478023    0.21675466    0.27121386]
[37m[1m [ 652.73421484    0.24585095    0.27358085    0.21540098    0.23962167]
[37m[1m ...
[37m[1m [2163.54939877    0.43360001    0.39219996    0.16240001    0.26429999]
[37m[1m [ 408.9641415     0.56399995    0.634         0.42420003    0.66140002]
[37m[1m [1121.2233994     0.46520001    0.51410002    0.0641        0.49649999]]
[37m[1m[2023-06-25 06:02:50,142][129146] Max Reward on eval: 3184.6205757651014
[37m[1m[2023-06-25 06:02:50,142][129146] Min Reward on eval: -491.7000241429312
[37m[1m[2023-06-25 06:02:50,142][129146] Mean Reward across all agents: 1102.449197843771
[37m[1m[2023-06-25 06:02:50,142][129146] Average Trajectory Length: 961.9336666666667
[36m[2023-06-25 06:02:50,147][129146] mean_value=-428.6458620282893, max_value=1757.7830507844287
[37m[1m[2023-06-25 06:02:50,150][129146] New mean coefficients: [[ 4.64625     0.36582935  0.18579891 -0.56446004 -0.2251057 ]]
[37m[1m[2023-06-25 06:02:50,151][129146] Moving the mean solution point...
[36m[2023-06-25 06:02:59,948][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 06:02:59,949][129146] FPS: 391993.59
[36m[2023-06-25 06:02:59,951][129146] itr=539, itrs=2000, Progress: 26.95%
[36m[2023-06-25 06:03:11,547][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 06:03:11,547][129146] FPS: 331680.67
[36m[2023-06-25 06:03:16,439][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:03:16,440][129146] Reward + Measures: [[3371.7494705     0.30992168    0.33984536    0.13054848    0.28565478]]
[37m[1m[2023-06-25 06:03:16,440][129146] Max Reward on eval: 3371.7494705019967
[37m[1m[2023-06-25 06:03:16,440][129146] Min Reward on eval: 3371.7494705019967
[37m[1m[2023-06-25 06:03:16,440][129146] Mean Reward across all agents: 3371.7494705019967
[37m[1m[2023-06-25 06:03:16,441][129146] Average Trajectory Length: 979.6293333333333
[36m[2023-06-25 06:03:22,210][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:03:22,210][129146] Reward + Measures: [[ 707.45152325    0.3427        0.2818        0.1594        0.36690003]
[37m[1m [ 798.77695686    0.4217        0.4224        0.0605        0.3371    ]
[37m[1m [1391.06119391    0.32449743    0.30525589    0.07650383    0.35240993]
[37m[1m ...
[37m[1m [ -67.75646848    0.55150002    0.39549997    0.28280002    0.61249995]
[37m[1m [1233.34210415    0.32506606    0.31924486    0.13718598    0.2988162 ]
[37m[1m [1404.55293149    0.29356983    0.28640443    0.07539182    0.23542833]]
[37m[1m[2023-06-25 06:03:22,211][129146] Max Reward on eval: 2871.1716583021916
[37m[1m[2023-06-25 06:03:22,211][129146] Min Reward on eval: -301.8516829184897
[37m[1m[2023-06-25 06:03:22,211][129146] Mean Reward across all agents: 1114.1275472373538
[37m[1m[2023-06-25 06:03:22,211][129146] Average Trajectory Length: 949.0106666666667
[36m[2023-06-25 06:03:22,214][129146] mean_value=-671.850991354216, max_value=1500.7265530454938
[37m[1m[2023-06-25 06:03:22,217][129146] New mean coefficients: [[ 4.6997027   0.13301401  0.39041686 -0.6087727  -0.06350741]]
[37m[1m[2023-06-25 06:03:22,218][129146] Moving the mean solution point...
[36m[2023-06-25 06:03:31,827][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 06:03:31,828][129146] FPS: 399665.14
[36m[2023-06-25 06:03:31,830][129146] itr=540, itrs=2000, Progress: 27.00%
[37m[1m[2023-06-25 06:03:37,044][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000520
[36m[2023-06-25 06:03:48,768][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 06:03:48,768][129146] FPS: 336394.52
[36m[2023-06-25 06:03:53,460][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:03:53,460][129146] Reward + Measures: [[3469.35731643    0.30800045    0.34222111    0.12511535    0.28262794]]
[37m[1m[2023-06-25 06:03:53,460][129146] Max Reward on eval: 3469.3573164263257
[37m[1m[2023-06-25 06:03:53,461][129146] Min Reward on eval: 3469.3573164263257
[37m[1m[2023-06-25 06:03:53,461][129146] Mean Reward across all agents: 3469.3573164263257
[37m[1m[2023-06-25 06:03:53,461][129146] Average Trajectory Length: 982.06
[36m[2023-06-25 06:03:59,056][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:03:59,057][129146] Reward + Measures: [[ 454.58733551    0.29090002    0.31980002    0.24520002    0.30090001]
[37m[1m [ -82.73729343    0.3351        0.2881        0.2474        0.27990001]
[37m[1m [ -51.21147725    0.56420004    0.76879996    0.1877        0.78430003]
[37m[1m ...
[37m[1m [1078.82771305    0.2861172     0.29517722    0.15966296    0.20879285]
[37m[1m [1537.98610688    0.40200001    0.2987        0.2674        0.37980002]
[37m[1m [2724.5448926     0.29620001    0.38410002    0.1754        0.33510002]]
[37m[1m[2023-06-25 06:03:59,062][129146] Max Reward on eval: 3078.502233470371
[37m[1m[2023-06-25 06:03:59,062][129146] Min Reward on eval: -192.87632078102615
[37m[1m[2023-06-25 06:03:59,063][129146] Mean Reward across all agents: 1131.9277088568417
[37m[1m[2023-06-25 06:03:59,064][129146] Average Trajectory Length: 975.6593333333333
[36m[2023-06-25 06:03:59,070][129146] mean_value=-553.8451055098603, max_value=938.2254855062647
[37m[1m[2023-06-25 06:03:59,074][129146] New mean coefficients: [[ 4.108593   -0.15099938  0.7697953  -0.36861682  0.14121778]]
[37m[1m[2023-06-25 06:03:59,076][129146] Moving the mean solution point...
[36m[2023-06-25 06:04:08,739][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 06:04:08,739][129146] FPS: 397487.18
[36m[2023-06-25 06:04:08,741][129146] itr=541, itrs=2000, Progress: 27.05%
[36m[2023-06-25 06:04:20,148][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 06:04:20,148][129146] FPS: 337312.77
[36m[2023-06-25 06:04:24,856][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:04:24,856][129146] Reward + Measures: [[3604.61057548    0.30955184    0.34284106    0.11767915    0.27832258]]
[37m[1m[2023-06-25 06:04:24,856][129146] Max Reward on eval: 3604.6105754809105
[37m[1m[2023-06-25 06:04:24,856][129146] Min Reward on eval: 3604.6105754809105
[37m[1m[2023-06-25 06:04:24,856][129146] Mean Reward across all agents: 3604.6105754809105
[37m[1m[2023-06-25 06:04:24,857][129146] Average Trajectory Length: 985.6959999999999
[36m[2023-06-25 06:04:30,233][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:04:30,234][129146] Reward + Measures: [[ 876.53756016    0.41890001    0.46630001    0.22620001    0.42989999]
[37m[1m [2460.56967901    0.38620001    0.31549999    0.13280001    0.34819999]
[37m[1m [ 867.75592977    0.48280001    0.40899998    0.2379        0.3547    ]
[37m[1m ...
[37m[1m [1241.667345      0.38699999    0.48540002    0.1445        0.44309998]
[37m[1m [3225.40220623    0.33970004    0.3213        0.1305        0.30680001]
[37m[1m [1454.20346051    0.41870004    0.45110002    0.21470001    0.39429998]]
[37m[1m[2023-06-25 06:04:30,234][129146] Max Reward on eval: 3473.725596408639
[37m[1m[2023-06-25 06:04:30,234][129146] Min Reward on eval: 254.246132013062
[37m[1m[2023-06-25 06:04:30,234][129146] Mean Reward across all agents: 1736.9703313560226
[37m[1m[2023-06-25 06:04:30,234][129146] Average Trajectory Length: 985.5003333333333
[36m[2023-06-25 06:04:30,237][129146] mean_value=-440.7983401988199, max_value=1110.887157970243
[37m[1m[2023-06-25 06:04:30,240][129146] New mean coefficients: [[ 3.7699625  -0.00332661  0.73741305 -0.2519493   0.04906459]]
[37m[1m[2023-06-25 06:04:30,241][129146] Moving the mean solution point...
[36m[2023-06-25 06:04:39,928][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 06:04:39,929][129146] FPS: 396457.57
[36m[2023-06-25 06:04:39,931][129146] itr=542, itrs=2000, Progress: 27.10%
[36m[2023-06-25 06:04:51,420][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 06:04:51,421][129146] FPS: 334752.34
[36m[2023-06-25 06:04:56,129][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:04:56,130][129146] Reward + Measures: [[3718.61684448    0.30351773    0.34602416    0.11576663    0.28194532]]
[37m[1m[2023-06-25 06:04:56,130][129146] Max Reward on eval: 3718.616844483255
[37m[1m[2023-06-25 06:04:56,130][129146] Min Reward on eval: 3718.616844483255
[37m[1m[2023-06-25 06:04:56,130][129146] Mean Reward across all agents: 3718.616844483255
[37m[1m[2023-06-25 06:04:56,130][129146] Average Trajectory Length: 987.9459999999999
[36m[2023-06-25 06:05:01,540][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:05:01,541][129146] Reward + Measures: [[1397.71392802    0.32070002    0.40529999    0.18699999    0.44499999]
[37m[1m [1790.87677954    0.34089997    0.33200002    0.0781        0.37719998]
[37m[1m [2299.64694567    0.36040002    0.3811        0.1445        0.40220004]
[37m[1m ...
[37m[1m [2043.58199837    0.32266077    0.34185129    0.18218425    0.2886937 ]
[37m[1m [1717.02367301    0.33880001    0.42490003    0.20729999    0.38170001]
[37m[1m [1052.69005407    0.384         0.37780005    0.1071        0.46790001]]
[37m[1m[2023-06-25 06:05:01,541][129146] Max Reward on eval: 3522.588117316179
[37m[1m[2023-06-25 06:05:01,541][129146] Min Reward on eval: 197.4205939708976
[37m[1m[2023-06-25 06:05:01,541][129146] Mean Reward across all agents: 1695.6654022420237
[37m[1m[2023-06-25 06:05:01,542][129146] Average Trajectory Length: 983.7166666666666
[36m[2023-06-25 06:05:01,545][129146] mean_value=-467.2746671464718, max_value=1598.3637826415913
[37m[1m[2023-06-25 06:05:01,548][129146] New mean coefficients: [[ 3.4307091  -0.05118364  0.16665262 -0.2549856   0.22503473]]
[37m[1m[2023-06-25 06:05:01,548][129146] Moving the mean solution point...
[36m[2023-06-25 06:05:11,213][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 06:05:11,213][129146] FPS: 397400.78
[36m[2023-06-25 06:05:11,215][129146] itr=543, itrs=2000, Progress: 27.15%
[36m[2023-06-25 06:05:22,650][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 06:05:22,650][129146] FPS: 336363.71
[36m[2023-06-25 06:05:27,411][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:05:27,412][129146] Reward + Measures: [[3882.80189221    0.3052046     0.33551437    0.10877482    0.27872694]]
[37m[1m[2023-06-25 06:05:27,412][129146] Max Reward on eval: 3882.801892206712
[37m[1m[2023-06-25 06:05:27,412][129146] Min Reward on eval: 3882.801892206712
[37m[1m[2023-06-25 06:05:27,413][129146] Mean Reward across all agents: 3882.801892206712
[37m[1m[2023-06-25 06:05:27,413][129146] Average Trajectory Length: 990.3333333333333
[36m[2023-06-25 06:05:32,845][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:05:32,845][129146] Reward + Measures: [[ 440.069249      0.4454        0.39989999    0.20999999    0.3739    ]
[37m[1m [2025.3519146     0.37639999    0.38699999    0.19489999    0.31029999]
[37m[1m [ 908.17728357    0.25299999    0.46470004    0.12679999    0.5262    ]
[37m[1m ...
[37m[1m [3261.50343966    0.31919193    0.34089491    0.1282963     0.27069229]
[37m[1m [1163.07688669    0.28709999    0.3026        0.20200001    0.35460001]
[37m[1m [2106.5739327     0.30430004    0.32080004    0.17490001    0.27019998]]
[37m[1m[2023-06-25 06:05:32,846][129146] Max Reward on eval: 3399.616995728598
[37m[1m[2023-06-25 06:05:32,846][129146] Min Reward on eval: -389.8022261148115
[37m[1m[2023-06-25 06:05:32,846][129146] Mean Reward across all agents: 1587.021320530988
[37m[1m[2023-06-25 06:05:32,846][129146] Average Trajectory Length: 975.0896666666666
[36m[2023-06-25 06:05:32,850][129146] mean_value=-557.4116463494056, max_value=2148.534746954155
[37m[1m[2023-06-25 06:05:32,852][129146] New mean coefficients: [[ 3.3505318  -0.63630587  0.30798972 -0.20470573 -0.31363285]]
[37m[1m[2023-06-25 06:05:32,853][129146] Moving the mean solution point...
[36m[2023-06-25 06:05:42,496][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 06:05:42,497][129146] FPS: 398292.65
[36m[2023-06-25 06:05:42,499][129146] itr=544, itrs=2000, Progress: 27.20%
[36m[2023-06-25 06:05:53,860][129146] train() took 11.34 seconds to complete
[36m[2023-06-25 06:05:53,861][129146] FPS: 338581.91
[36m[2023-06-25 06:05:58,565][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:05:58,565][129146] Reward + Measures: [[4011.98892019    0.29912651    0.33138791    0.10601228    0.27539709]]
[37m[1m[2023-06-25 06:05:58,565][129146] Max Reward on eval: 4011.988920185512
[37m[1m[2023-06-25 06:05:58,566][129146] Min Reward on eval: 4011.988920185512
[37m[1m[2023-06-25 06:05:58,566][129146] Mean Reward across all agents: 4011.988920185512
[37m[1m[2023-06-25 06:05:58,566][129146] Average Trajectory Length: 992.0913333333333
[36m[2023-06-25 06:06:03,920][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:06:03,921][129146] Reward + Measures: [[1112.42006653    0.2234        0.20910001    0.1354        0.2145    ]
[37m[1m [1474.9149975     0.34600002    0.38779998    0.2692        0.43540001]
[37m[1m [2377.9734393     0.33302364    0.31529224    0.15715294    0.29430959]
[37m[1m ...
[37m[1m [1277.58473055    0.2105        0.2728        0.1429        0.32150003]
[37m[1m [1001.6031688     0.5054        0.35999998    0.1885        0.35929999]
[37m[1m [ 465.85817472    0.3626        0.53360003    0.35590002    0.46290001]]
[37m[1m[2023-06-25 06:06:03,921][129146] Max Reward on eval: 3752.4981916761026
[37m[1m[2023-06-25 06:06:03,921][129146] Min Reward on eval: -63.976306836347796
[37m[1m[2023-06-25 06:06:03,922][129146] Mean Reward across all agents: 1404.4329526294061
[37m[1m[2023-06-25 06:06:03,922][129146] Average Trajectory Length: 974.1536666666666
[36m[2023-06-25 06:06:03,927][129146] mean_value=-377.40615756789737, max_value=1891.4494276786338
[37m[1m[2023-06-25 06:06:03,929][129146] New mean coefficients: [[ 3.0613208  -0.73288935  0.31604832 -0.05363384 -0.56039274]]
[37m[1m[2023-06-25 06:06:03,930][129146] Moving the mean solution point...
[36m[2023-06-25 06:06:13,533][129146] train() took 9.60 seconds to complete
[36m[2023-06-25 06:06:13,534][129146] FPS: 399945.12
[36m[2023-06-25 06:06:13,536][129146] itr=545, itrs=2000, Progress: 27.25%
[36m[2023-06-25 06:06:24,911][129146] train() took 11.36 seconds to complete
[36m[2023-06-25 06:06:24,911][129146] FPS: 338135.74
[36m[2023-06-25 06:06:29,630][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:06:29,631][129146] Reward + Measures: [[4123.95187841    0.30077159    0.32532123    0.10649908    0.27419004]]
[37m[1m[2023-06-25 06:06:29,631][129146] Max Reward on eval: 4123.951878408589
[37m[1m[2023-06-25 06:06:29,631][129146] Min Reward on eval: 4123.951878408589
[37m[1m[2023-06-25 06:06:29,631][129146] Mean Reward across all agents: 4123.951878408589
[37m[1m[2023-06-25 06:06:29,631][129146] Average Trajectory Length: 990.5093333333333
[36m[2023-06-25 06:06:35,203][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:06:35,203][129146] Reward + Measures: [[2061.69989642    0.3619        0.34290001    0.0878        0.43360001]
[37m[1m [ 702.10878406    0.44490001    0.42449999    0.10749999    0.4941    ]
[37m[1m [2211.65437504    0.36489996    0.3955        0.13929999    0.43430001]
[37m[1m ...
[37m[1m [2190.72235897    0.28640002    0.30770001    0.15720001    0.30770001]
[37m[1m [ 638.20311596    0.44479999    0.50270003    0.14640002    0.54930001]
[37m[1m [1184.50644635    0.25034693    0.26282293    0.19506033    0.26773354]]
[37m[1m[2023-06-25 06:06:35,209][129146] Max Reward on eval: 4019.5355725967324
[37m[1m[2023-06-25 06:06:35,209][129146] Min Reward on eval: -199.1614798980765
[37m[1m[2023-06-25 06:06:35,210][129146] Mean Reward across all agents: 1814.9735654096817
[37m[1m[2023-06-25 06:06:35,210][129146] Average Trajectory Length: 966.3416666666666
[36m[2023-06-25 06:06:35,218][129146] mean_value=-454.37300579775143, max_value=1674.736400313396
[37m[1m[2023-06-25 06:06:35,222][129146] New mean coefficients: [[ 2.2871273  -0.36453384  0.00999373 -0.22492284 -0.16153589]]
[37m[1m[2023-06-25 06:06:35,224][129146] Moving the mean solution point...
[36m[2023-06-25 06:06:44,728][129146] train() took 9.50 seconds to complete
[36m[2023-06-25 06:06:44,729][129146] FPS: 404091.82
[36m[2023-06-25 06:06:44,731][129146] itr=546, itrs=2000, Progress: 27.30%
[36m[2023-06-25 06:06:56,130][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 06:06:56,131][129146] FPS: 337393.78
[36m[2023-06-25 06:07:00,936][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:07:00,937][129146] Reward + Measures: [[4242.81788244    0.3025713     0.31535381    0.1034202     0.27473727]]
[37m[1m[2023-06-25 06:07:00,937][129146] Max Reward on eval: 4242.817882435333
[37m[1m[2023-06-25 06:07:00,937][129146] Min Reward on eval: 4242.817882435333
[37m[1m[2023-06-25 06:07:00,937][129146] Mean Reward across all agents: 4242.817882435333
[37m[1m[2023-06-25 06:07:00,937][129146] Average Trajectory Length: 993.6593333333333
[36m[2023-06-25 06:07:06,398][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:07:06,403][129146] Reward + Measures: [[  89.80984465    0.2225318     0.32220516    0.32569438    0.35844621]
[37m[1m [2350.66506945    0.3256        0.3348        0.1102        0.4093    ]
[37m[1m [3791.35269641    0.303         0.3723        0.1207        0.2651    ]
[37m[1m ...
[37m[1m [1293.05897178    0.3714        0.38549998    0.20999999    0.46240002]
[37m[1m [1227.76626275    0.27959999    0.43800002    0.21960001    0.4226    ]
[37m[1m [2042.78133533    0.32723513    0.37864795    0.17694382    0.23109165]]
[37m[1m[2023-06-25 06:07:06,403][129146] Max Reward on eval: 3983.525548814575
[37m[1m[2023-06-25 06:07:06,403][129146] Min Reward on eval: -455.22030666596254
[37m[1m[2023-06-25 06:07:06,403][129146] Mean Reward across all agents: 2025.9434398489584
[37m[1m[2023-06-25 06:07:06,404][129146] Average Trajectory Length: 973.9056666666667
[36m[2023-06-25 06:07:06,409][129146] mean_value=-160.69001872840798, max_value=1655.0469065448794
[37m[1m[2023-06-25 06:07:06,412][129146] New mean coefficients: [[ 2.0935915  -0.59821904  0.12475629 -0.2570439  -0.51319236]]
[37m[1m[2023-06-25 06:07:06,412][129146] Moving the mean solution point...
[36m[2023-06-25 06:07:16,054][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 06:07:16,055][129146] FPS: 398329.90
[36m[2023-06-25 06:07:16,057][129146] itr=547, itrs=2000, Progress: 27.35%
[36m[2023-06-25 06:07:27,688][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 06:07:27,688][129146] FPS: 330784.90
[36m[2023-06-25 06:07:32,373][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:07:32,373][129146] Reward + Measures: [[4340.73535046    0.30257121    0.3178646     0.10079087    0.26558381]]
[37m[1m[2023-06-25 06:07:32,373][129146] Max Reward on eval: 4340.735350463597
[37m[1m[2023-06-25 06:07:32,374][129146] Min Reward on eval: 4340.735350463597
[37m[1m[2023-06-25 06:07:32,374][129146] Mean Reward across all agents: 4340.735350463597
[37m[1m[2023-06-25 06:07:32,374][129146] Average Trajectory Length: 993.1743333333333
[36m[2023-06-25 06:07:37,819][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:07:37,819][129146] Reward + Measures: [[ 736.21441049    0.19700001    0.47689995    0.15900001    0.54360002]
[37m[1m [3849.21240376    0.30519998    0.3186        0.0735        0.26370001]
[37m[1m [2011.44366051    0.22860001    0.33199999    0.14560001    0.2938    ]
[37m[1m ...
[37m[1m [2105.44391948    0.35949999    0.30860004    0.0537        0.36450002]
[37m[1m [2834.18600555    0.38009998    0.3928        0.21890001    0.2987    ]
[37m[1m [1389.15033714    0.22939999    0.44390002    0.09760001    0.4756    ]]
[37m[1m[2023-06-25 06:07:37,820][129146] Max Reward on eval: 4070.890127452812
[37m[1m[2023-06-25 06:07:37,820][129146] Min Reward on eval: -326.9586517224787
[37m[1m[2023-06-25 06:07:37,820][129146] Mean Reward across all agents: 1514.6818850994584
[37m[1m[2023-06-25 06:07:37,820][129146] Average Trajectory Length: 991.923
[36m[2023-06-25 06:07:37,824][129146] mean_value=-568.5781481579771, max_value=2250.3510860823094
[37m[1m[2023-06-25 06:07:37,827][129146] New mean coefficients: [[ 2.2418504  -0.65551794  0.01805037 -0.30019933 -0.28613597]]
[37m[1m[2023-06-25 06:07:37,828][129146] Moving the mean solution point...
[36m[2023-06-25 06:07:47,551][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 06:07:47,552][129146] FPS: 394972.71
[36m[2023-06-25 06:07:47,554][129146] itr=548, itrs=2000, Progress: 27.40%
[36m[2023-06-25 06:07:59,266][129146] train() took 11.69 seconds to complete
[36m[2023-06-25 06:07:59,266][129146] FPS: 328456.61
[36m[2023-06-25 06:08:04,055][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:08:04,055][129146] Reward + Measures: [[4453.03273928    0.29575878    0.30875507    0.09887036    0.26726758]]
[37m[1m[2023-06-25 06:08:04,056][129146] Max Reward on eval: 4453.032739278884
[37m[1m[2023-06-25 06:08:04,056][129146] Min Reward on eval: 4453.032739278884
[37m[1m[2023-06-25 06:08:04,056][129146] Mean Reward across all agents: 4453.032739278884
[37m[1m[2023-06-25 06:08:04,056][129146] Average Trajectory Length: 993.6823333333333
[36m[2023-06-25 06:08:09,540][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:08:09,541][129146] Reward + Measures: [[ 183.70793619    0.44220001    0.63069999    0.0403        0.72490007]
[37m[1m [1759.868992      0.28370002    0.2595        0.12099999    0.2462    ]
[37m[1m [1472.08131772    0.43459997    0.32259998    0.0309        0.39950001]
[37m[1m ...
[37m[1m [ 816.6557496     0.38640001    0.3856        0.042         0.56689996]
[37m[1m [2725.68926852    0.29050809    0.34487152    0.20439692    0.31705806]
[37m[1m [2372.86664516    0.26506603    0.31434354    0.1544594     0.27581814]]
[37m[1m[2023-06-25 06:08:09,541][129146] Max Reward on eval: 3959.199061274063
[37m[1m[2023-06-25 06:08:09,541][129146] Min Reward on eval: -205.58480562499608
[37m[1m[2023-06-25 06:08:09,541][129146] Mean Reward across all agents: 1589.9285696094853
[37m[1m[2023-06-25 06:08:09,542][129146] Average Trajectory Length: 963.9103333333333
[36m[2023-06-25 06:08:09,546][129146] mean_value=-491.57644686193146, max_value=2342.5252021225983
[37m[1m[2023-06-25 06:08:09,549][129146] New mean coefficients: [[ 2.1074488  -0.98771566  0.3233542  -0.217089   -0.57424486]]
[37m[1m[2023-06-25 06:08:09,549][129146] Moving the mean solution point...
[36m[2023-06-25 06:08:19,283][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 06:08:19,284][129146] FPS: 394565.03
[36m[2023-06-25 06:08:19,286][129146] itr=549, itrs=2000, Progress: 27.45%
[36m[2023-06-25 06:08:30,792][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 06:08:30,792][129146] FPS: 334316.74
[36m[2023-06-25 06:08:35,562][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:08:35,562][129146] Reward + Measures: [[4562.27026668    0.2913757     0.31017959    0.09524788    0.26286045]]
[37m[1m[2023-06-25 06:08:35,563][129146] Max Reward on eval: 4562.270266680809
[37m[1m[2023-06-25 06:08:35,563][129146] Min Reward on eval: 4562.270266680809
[37m[1m[2023-06-25 06:08:35,563][129146] Mean Reward across all agents: 4562.270266680809
[37m[1m[2023-06-25 06:08:35,563][129146] Average Trajectory Length: 996.4253333333334
[36m[2023-06-25 06:08:41,004][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:08:41,005][129146] Reward + Measures: [[1584.32055262    0.29965487    0.33111346    0.11908895    0.29513797]
[37m[1m [  78.38129737    0.26180002    0.3039        0.1049        0.28279999]
[37m[1m [1186.8669532     0.26313466    0.30803266    0.14989208    0.24482179]
[37m[1m ...
[37m[1m [1257.28941017    0.31025296    0.42172453    0.09726863    0.51326174]
[37m[1m [ 582.88903068    0.34506711    0.24159181    0.20502603    0.34040174]
[37m[1m [2664.07983426    0.2820636     0.29388881    0.09246657    0.31347522]]
[37m[1m[2023-06-25 06:08:41,005][129146] Max Reward on eval: 4425.662101292517
[37m[1m[2023-06-25 06:08:41,005][129146] Min Reward on eval: -646.9524124813615
[37m[1m[2023-06-25 06:08:41,006][129146] Mean Reward across all agents: 1389.1922938669322
[37m[1m[2023-06-25 06:08:41,006][129146] Average Trajectory Length: 932.674
[36m[2023-06-25 06:08:41,008][129146] mean_value=-1218.6826800174006, max_value=824.435020891462
[37m[1m[2023-06-25 06:08:41,011][129146] New mean coefficients: [[ 1.6926887  -0.45116472  0.02123106 -0.16029347 -0.5198483 ]]
[37m[1m[2023-06-25 06:08:41,011][129146] Moving the mean solution point...
[36m[2023-06-25 06:08:50,757][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 06:08:50,758][129146] FPS: 394074.94
[36m[2023-06-25 06:08:50,760][129146] itr=550, itrs=2000, Progress: 27.50%
[37m[1m[2023-06-25 06:08:55,902][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000530
[36m[2023-06-25 06:09:07,829][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 06:09:07,829][129146] FPS: 330306.06
[36m[2023-06-25 06:09:12,473][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:09:12,473][129146] Reward + Measures: [[2580.44274208    0.27901956    0.41945547    0.07829441    0.30066016]]
[37m[1m[2023-06-25 06:09:12,474][129146] Max Reward on eval: 2580.4427420814354
[37m[1m[2023-06-25 06:09:12,474][129146] Min Reward on eval: 2580.4427420814354
[37m[1m[2023-06-25 06:09:12,474][129146] Mean Reward across all agents: 2580.4427420814354
[37m[1m[2023-06-25 06:09:12,474][129146] Average Trajectory Length: 999.7936666666666
[36m[2023-06-25 06:09:17,797][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:09:17,797][129146] Reward + Measures: [[2538.38239037    0.2721        0.46289998    0.1107        0.31440002]
[37m[1m [2268.80788324    0.29620001    0.31470001    0.11979999    0.2771    ]
[37m[1m [1799.5404764     0.3114        0.3116        0.07030001    0.33270001]
[37m[1m ...
[37m[1m [ 616.98677599    0.24975608    0.25048852    0.16665368    0.17936359]
[37m[1m [1800.83652882    0.28530002    0.52959996    0.1517        0.38459998]
[37m[1m [2725.76645328    0.2676        0.39089999    0.1099        0.26950002]]
[37m[1m[2023-06-25 06:09:17,798][129146] Max Reward on eval: 2849.267626715731
[37m[1m[2023-06-25 06:09:17,798][129146] Min Reward on eval: 245.01754833824234
[37m[1m[2023-06-25 06:09:17,798][129146] Mean Reward across all agents: 1741.0873250775626
[37m[1m[2023-06-25 06:09:17,798][129146] Average Trajectory Length: 963.178
[36m[2023-06-25 06:09:17,802][129146] mean_value=-704.9793695593117, max_value=2113.1566667556704
[37m[1m[2023-06-25 06:09:17,805][129146] New mean coefficients: [[ 1.797024    0.2537918  -0.29613775 -0.09963596 -0.07754755]]
[37m[1m[2023-06-25 06:09:17,806][129146] Moving the mean solution point...
[36m[2023-06-25 06:09:27,337][129146] train() took 9.53 seconds to complete
[36m[2023-06-25 06:09:27,337][129146] FPS: 402941.59
[36m[2023-06-25 06:09:27,340][129146] itr=551, itrs=2000, Progress: 27.55%
[36m[2023-06-25 06:09:38,810][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 06:09:38,811][129146] FPS: 335410.11
[36m[2023-06-25 06:09:43,519][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:09:43,519][129146] Reward + Measures: [[2666.77540873    0.28051493    0.41574821    0.07882091    0.29546526]]
[37m[1m[2023-06-25 06:09:43,519][129146] Max Reward on eval: 2666.7754087276467
[37m[1m[2023-06-25 06:09:43,520][129146] Min Reward on eval: 2666.7754087276467
[37m[1m[2023-06-25 06:09:43,520][129146] Mean Reward across all agents: 2666.7754087276467
[37m[1m[2023-06-25 06:09:43,520][129146] Average Trajectory Length: 999.858
[36m[2023-06-25 06:09:49,131][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:09:49,131][129146] Reward + Measures: [[1688.46273649    0.35769999    0.39580002    0.19150001    0.26250002]
[37m[1m [2418.04352897    0.2471        0.40869999    0.1071        0.33329999]
[37m[1m [ 458.5471293     0.37889999    0.42269999    0.18920001    0.30720001]
[37m[1m ...
[37m[1m [2609.42168649    0.31140003    0.3678        0.06040001    0.31200001]
[37m[1m [2279.53727234    0.2651        0.39620003    0.1561        0.31710002]
[37m[1m [1514.84198547    0.21180001    0.4729        0.14320001    0.414     ]]
[37m[1m[2023-06-25 06:09:49,132][129146] Max Reward on eval: 2750.111590357684
[37m[1m[2023-06-25 06:09:49,132][129146] Min Reward on eval: -901.2527817886323
[37m[1m[2023-06-25 06:09:49,132][129146] Mean Reward across all agents: 1744.0635727901474
[37m[1m[2023-06-25 06:09:49,132][129146] Average Trajectory Length: 994.203
[36m[2023-06-25 06:09:49,135][129146] mean_value=-697.8813128672671, max_value=1931.8802376541676
[37m[1m[2023-06-25 06:09:49,138][129146] New mean coefficients: [[ 2.103399   -0.3301862   0.10511291  0.14834681 -0.2661829 ]]
[37m[1m[2023-06-25 06:09:49,139][129146] Moving the mean solution point...
[36m[2023-06-25 06:09:58,750][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 06:09:58,750][129146] FPS: 399612.07
[36m[2023-06-25 06:09:58,752][129146] itr=552, itrs=2000, Progress: 27.60%
[36m[2023-06-25 06:10:10,188][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 06:10:10,189][129146] FPS: 336312.16
[36m[2023-06-25 06:10:14,988][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:10:14,989][129146] Reward + Measures: [[2793.91567502    0.28599131    0.40474716    0.07628039    0.2914387 ]]
[37m[1m[2023-06-25 06:10:14,989][129146] Max Reward on eval: 2793.9156750243974
[37m[1m[2023-06-25 06:10:14,989][129146] Min Reward on eval: 2793.9156750243974
[37m[1m[2023-06-25 06:10:14,990][129146] Mean Reward across all agents: 2793.9156750243974
[37m[1m[2023-06-25 06:10:14,990][129146] Average Trajectory Length: 999.4746666666666
[36m[2023-06-25 06:10:20,381][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:10:20,381][129146] Reward + Measures: [[2201.42784847    0.27610001    0.4278        0.1176        0.32640001]
[37m[1m [ 994.8178696     0.22620001    0.59300005    0.23699999    0.60540003]
[37m[1m [2087.09476841    0.29130003    0.43280002    0.14820002    0.35730001]
[37m[1m ...
[37m[1m [1358.10791528    0.32960001    0.4375        0.0662        0.32960001]
[37m[1m [ 189.48349149    0.29379037    0.30484807    0.21535002    0.21931346]
[37m[1m [ 976.21309854    0.2254        0.61310005    0.2181        0.58280003]]
[37m[1m[2023-06-25 06:10:20,382][129146] Max Reward on eval: 2785.6977678821304
[37m[1m[2023-06-25 06:10:20,382][129146] Min Reward on eval: -427.38415655273127
[37m[1m[2023-06-25 06:10:20,382][129146] Mean Reward across all agents: 1691.7790291071276
[37m[1m[2023-06-25 06:10:20,383][129146] Average Trajectory Length: 996.905
[36m[2023-06-25 06:10:20,387][129146] mean_value=-388.37068265664584, max_value=1490.129984002151
[37m[1m[2023-06-25 06:10:20,389][129146] New mean coefficients: [[ 2.044771   -0.47698504  0.61325485  0.2693264  -0.7002017 ]]
[37m[1m[2023-06-25 06:10:20,390][129146] Moving the mean solution point...
[36m[2023-06-25 06:10:30,122][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 06:10:30,122][129146] FPS: 394650.55
[36m[2023-06-25 06:10:30,124][129146] itr=553, itrs=2000, Progress: 27.65%
[36m[2023-06-25 06:10:41,524][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 06:10:41,525][129146] FPS: 337413.21
[36m[2023-06-25 06:10:46,273][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:10:46,274][129146] Reward + Measures: [[2894.09905556    0.29044142    0.39680403    0.07055619    0.28681585]]
[37m[1m[2023-06-25 06:10:46,274][129146] Max Reward on eval: 2894.099055564808
[37m[1m[2023-06-25 06:10:46,274][129146] Min Reward on eval: 2894.099055564808
[37m[1m[2023-06-25 06:10:46,274][129146] Mean Reward across all agents: 2894.099055564808
[37m[1m[2023-06-25 06:10:46,275][129146] Average Trajectory Length: 999.6646666666667
[36m[2023-06-25 06:10:51,759][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:10:51,759][129146] Reward + Measures: [[ 763.67500948    0.21981643    0.30625364    0.11261628    0.21402465]
[37m[1m [ -42.28624035    0.20167165    0.25369725    0.18525106    0.20131545]
[37m[1m [ 762.88331133    0.26648408    0.27363089    0.14534722    0.22167538]
[37m[1m ...
[37m[1m [1314.10602392    0.23909998    0.23360001    0.0966        0.19309999]
[37m[1m [ 451.44562624    0.2545        0.2881        0.1674        0.30240002]
[37m[1m [1487.28504573    0.22230001    0.34650001    0.13499999    0.35640001]]
[37m[1m[2023-06-25 06:10:51,760][129146] Max Reward on eval: 2905.0332246268167
[37m[1m[2023-06-25 06:10:51,760][129146] Min Reward on eval: -706.9472688457929
[37m[1m[2023-06-25 06:10:51,760][129146] Mean Reward across all agents: 1216.3114477286829
[37m[1m[2023-06-25 06:10:51,760][129146] Average Trajectory Length: 947.584
[36m[2023-06-25 06:10:51,763][129146] mean_value=-1332.2479792092436, max_value=1580.6307253583632
[37m[1m[2023-06-25 06:10:51,765][129146] New mean coefficients: [[ 1.4511869   0.40634173  0.4375692   0.22721472 -0.35131836]]
[37m[1m[2023-06-25 06:10:51,766][129146] Moving the mean solution point...
[36m[2023-06-25 06:11:01,437][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 06:11:01,437][129146] FPS: 397137.28
[36m[2023-06-25 06:11:01,439][129146] itr=554, itrs=2000, Progress: 27.70%
[36m[2023-06-25 06:11:12,905][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 06:11:12,906][129146] FPS: 335476.25
[36m[2023-06-25 06:11:17,714][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:11:17,714][129146] Reward + Measures: [[3033.52329917    0.29325539    0.37662968    0.06308281    0.27986231]]
[37m[1m[2023-06-25 06:11:17,714][129146] Max Reward on eval: 3033.5232991684184
[37m[1m[2023-06-25 06:11:17,714][129146] Min Reward on eval: 3033.5232991684184
[37m[1m[2023-06-25 06:11:17,715][129146] Mean Reward across all agents: 3033.5232991684184
[37m[1m[2023-06-25 06:11:17,715][129146] Average Trajectory Length: 999.1709999999999
[36m[2023-06-25 06:11:23,230][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:11:23,231][129146] Reward + Measures: [[1829.80967921    0.24748318    0.37363803    0.15398407    0.33362389]
[37m[1m [1961.45609536    0.28520003    0.41100001    0.13060001    0.2538    ]
[37m[1m [2160.66557161    0.2861        0.40759999    0.10569999    0.2685    ]
[37m[1m ...
[37m[1m [2288.53044064    0.29340002    0.39579999    0.0875        0.2712    ]
[37m[1m [2211.98460682    0.27019998    0.40040001    0.1175        0.29010001]
[37m[1m [1228.71263286    0.29950002    0.43090001    0.14569999    0.24900003]]
[37m[1m[2023-06-25 06:11:23,231][129146] Max Reward on eval: 3122.837791326782
[37m[1m[2023-06-25 06:11:23,231][129146] Min Reward on eval: 240.1918822396663
[37m[1m[2023-06-25 06:11:23,231][129146] Mean Reward across all agents: 1917.872259524664
[37m[1m[2023-06-25 06:11:23,231][129146] Average Trajectory Length: 990.533
[36m[2023-06-25 06:11:23,233][129146] mean_value=-972.055751590291, max_value=880.3172058546488
[37m[1m[2023-06-25 06:11:23,236][129146] New mean coefficients: [[ 1.4952425  -0.07254118  0.5910191   0.21873538 -0.9304649 ]]
[37m[1m[2023-06-25 06:11:23,237][129146] Moving the mean solution point...
[36m[2023-06-25 06:11:32,917][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 06:11:32,918][129146] FPS: 396736.71
[36m[2023-06-25 06:11:32,920][129146] itr=555, itrs=2000, Progress: 27.75%
[36m[2023-06-25 06:11:44,282][129146] train() took 11.34 seconds to complete
[36m[2023-06-25 06:11:44,282][129146] FPS: 338552.09
[36m[2023-06-25 06:11:49,126][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:11:49,126][129146] Reward + Measures: [[3138.76992296    0.29020828    0.3713733     0.05733383    0.27294099]]
[37m[1m[2023-06-25 06:11:49,126][129146] Max Reward on eval: 3138.769922955779
[37m[1m[2023-06-25 06:11:49,127][129146] Min Reward on eval: 3138.769922955779
[37m[1m[2023-06-25 06:11:49,127][129146] Mean Reward across all agents: 3138.769922955779
[37m[1m[2023-06-25 06:11:49,127][129146] Average Trajectory Length: 997.6216666666667
[36m[2023-06-25 06:11:54,575][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:11:54,576][129146] Reward + Measures: [[-170.08626625    0.264         0.2357        0.1664        0.24560001]
[37m[1m [ 984.75876153    0.19518168    0.2515687     0.13853537    0.21286912]
[37m[1m [2583.00608886    0.23709999    0.43400002    0.1506        0.25390002]
[37m[1m ...
[37m[1m [1106.46480238    0.19076513    0.25799036    0.11092716    0.21431504]
[37m[1m [2616.244094      0.23969999    0.3874        0.14579999    0.27949998]
[37m[1m [  87.71576104    0.15530001    0.1902        0.12720001    0.1506    ]]
[37m[1m[2023-06-25 06:11:54,576][129146] Max Reward on eval: 3158.914583580755
[37m[1m[2023-06-25 06:11:54,576][129146] Min Reward on eval: -261.8790624974296
[37m[1m[2023-06-25 06:11:54,577][129146] Mean Reward across all agents: 1692.2130930972742
[37m[1m[2023-06-25 06:11:54,577][129146] Average Trajectory Length: 957.5766666666666
[36m[2023-06-25 06:11:54,579][129146] mean_value=-1123.5686065301281, max_value=840.2727792102166
[37m[1m[2023-06-25 06:11:54,581][129146] New mean coefficients: [[ 2.3723578  -0.19509965  0.38671416  0.20910786 -0.6899612 ]]
[37m[1m[2023-06-25 06:11:54,582][129146] Moving the mean solution point...
[36m[2023-06-25 06:12:04,357][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 06:12:04,357][129146] FPS: 392906.27
[36m[2023-06-25 06:12:04,360][129146] itr=556, itrs=2000, Progress: 27.80%
[36m[2023-06-25 06:12:15,800][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 06:12:15,800][129146] FPS: 336184.77
[36m[2023-06-25 06:12:20,548][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:12:20,549][129146] Reward + Measures: [[3281.7994783     0.29618967    0.34961343    0.05251963    0.26841256]]
[37m[1m[2023-06-25 06:12:20,549][129146] Max Reward on eval: 3281.799478296916
[37m[1m[2023-06-25 06:12:20,549][129146] Min Reward on eval: 3281.799478296916
[37m[1m[2023-06-25 06:12:20,549][129146] Mean Reward across all agents: 3281.799478296916
[37m[1m[2023-06-25 06:12:20,549][129146] Average Trajectory Length: 995.1709999999999
[36m[2023-06-25 06:12:26,247][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:12:26,247][129146] Reward + Measures: [[1537.23009694    0.23510002    0.39960003    0.19160001    0.46240002]
[37m[1m [ 106.354568      0.3209478     0.24923098    0.21745573    0.21303098]
[37m[1m [1223.56692041    0.27540001    0.52440006    0.19719999    0.49620005]
[37m[1m ...
[37m[1m [2555.2748094     0.29439998    0.40270001    0.19110002    0.25870001]
[37m[1m [2842.25684138    0.27659997    0.42459998    0.10129999    0.25479999]
[37m[1m [2605.63007234    0.27579999    0.44250003    0.132         0.26320001]]
[37m[1m[2023-06-25 06:12:26,248][129146] Max Reward on eval: 3110.7050699229817
[37m[1m[2023-06-25 06:12:26,248][129146] Min Reward on eval: -414.75096284379254
[37m[1m[2023-06-25 06:12:26,248][129146] Mean Reward across all agents: 1545.7874144625594
[37m[1m[2023-06-25 06:12:26,248][129146] Average Trajectory Length: 991.1413333333333
[36m[2023-06-25 06:12:26,253][129146] mean_value=-473.7137081642635, max_value=1221.0964250915954
[37m[1m[2023-06-25 06:12:26,255][129146] New mean coefficients: [[ 2.2242017  -0.3483875   0.3248591   0.23040867 -0.37178075]]
[37m[1m[2023-06-25 06:12:26,256][129146] Moving the mean solution point...
[36m[2023-06-25 06:12:36,068][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 06:12:36,068][129146] FPS: 391448.85
[36m[2023-06-25 06:12:36,070][129146] itr=557, itrs=2000, Progress: 27.85%
[36m[2023-06-25 06:12:47,672][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 06:12:47,672][129146] FPS: 331558.77
[36m[2023-06-25 06:12:52,523][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:12:52,524][129146] Reward + Measures: [[3420.60511289    0.29336193    0.3374683     0.05235984    0.26781356]]
[37m[1m[2023-06-25 06:12:52,524][129146] Max Reward on eval: 3420.605112891689
[37m[1m[2023-06-25 06:12:52,524][129146] Min Reward on eval: 3420.605112891689
[37m[1m[2023-06-25 06:12:52,524][129146] Mean Reward across all agents: 3420.605112891689
[37m[1m[2023-06-25 06:12:52,524][129146] Average Trajectory Length: 993.8876666666666
[36m[2023-06-25 06:12:58,009][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:12:58,009][129146] Reward + Measures: [[1145.02503028    0.20726608    0.2451158     0.09425297    0.17096902]
[37m[1m [1063.57645676    0.37650001    0.48569998    0.2764        0.40700004]
[37m[1m [ 769.15657468    0.42906314    0.54659075    0.04890094    0.51609194]
[37m[1m ...
[37m[1m [ 606.16484117    0.35339999    0.37120003    0.1087        0.3355    ]
[37m[1m [1200.55433778    0.41300002    0.48259997    0.1059        0.47230002]
[37m[1m [ 895.53069829    0.31160003    0.43039998    0.15650001    0.3028    ]]
[37m[1m[2023-06-25 06:12:58,010][129146] Max Reward on eval: 3529.933085485478
[37m[1m[2023-06-25 06:12:58,010][129146] Min Reward on eval: -651.9668146258512
[37m[1m[2023-06-25 06:12:58,010][129146] Mean Reward across all agents: 1425.1561453790641
[37m[1m[2023-06-25 06:12:58,010][129146] Average Trajectory Length: 934.9499999999999
[36m[2023-06-25 06:12:58,012][129146] mean_value=-1305.1559983739942, max_value=901.9832192253804
[37m[1m[2023-06-25 06:12:58,015][129146] New mean coefficients: [[ 1.8572776   0.00784919 -0.0462293   0.16399409 -0.318752  ]]
[37m[1m[2023-06-25 06:12:58,016][129146] Moving the mean solution point...
[36m[2023-06-25 06:13:07,782][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 06:13:07,783][129146] FPS: 393232.84
[36m[2023-06-25 06:13:07,785][129146] itr=558, itrs=2000, Progress: 27.90%
[36m[2023-06-25 06:13:19,203][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 06:13:19,204][129146] FPS: 336835.64
[36m[2023-06-25 06:13:23,941][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:13:23,942][129146] Reward + Measures: [[3558.57957923    0.29752377    0.33344921    0.04796334    0.26174486]]
[37m[1m[2023-06-25 06:13:23,942][129146] Max Reward on eval: 3558.579579231893
[37m[1m[2023-06-25 06:13:23,942][129146] Min Reward on eval: 3558.579579231893
[37m[1m[2023-06-25 06:13:23,942][129146] Mean Reward across all agents: 3558.579579231893
[37m[1m[2023-06-25 06:13:23,943][129146] Average Trajectory Length: 995.6833333333333
[36m[2023-06-25 06:13:29,371][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:13:29,371][129146] Reward + Measures: [[1979.09332032    0.35680002    0.42970005    0.2142        0.31500003]
[37m[1m [3073.89509533    0.28400001    0.3951        0.06190001    0.28570002]
[37m[1m [2148.90840662    0.2494        0.32640001    0.14259999    0.3391    ]
[37m[1m ...
[37m[1m [3347.59650246    0.32440001    0.34720001    0.0504        0.25250003]
[37m[1m [1804.29340102    0.26460001    0.41100001    0.1033        0.2665    ]
[37m[1m [1157.54289919    0.50590008    0.39070001    0.26230001    0.28639999]]
[37m[1m[2023-06-25 06:13:29,372][129146] Max Reward on eval: 3618.793163913861
[37m[1m[2023-06-25 06:13:29,372][129146] Min Reward on eval: -36.698704827111214
[37m[1m[2023-06-25 06:13:29,372][129146] Mean Reward across all agents: 1956.1982134374618
[37m[1m[2023-06-25 06:13:29,372][129146] Average Trajectory Length: 985.9313333333333
[36m[2023-06-25 06:13:29,375][129146] mean_value=-768.1214985328235, max_value=2198.6588687767066
[37m[1m[2023-06-25 06:13:29,378][129146] New mean coefficients: [[ 2.2433288  -0.07691274 -0.31052357  0.2175118  -0.19064818]]
[37m[1m[2023-06-25 06:13:29,379][129146] Moving the mean solution point...
[36m[2023-06-25 06:13:39,028][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 06:13:39,028][129146] FPS: 398025.31
[36m[2023-06-25 06:13:39,030][129146] itr=559, itrs=2000, Progress: 27.95%
[36m[2023-06-25 06:13:50,498][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 06:13:50,498][129146] FPS: 335508.36
[36m[2023-06-25 06:13:55,359][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:13:55,359][129146] Reward + Measures: [[3719.31560313    0.2967045     0.32955989    0.04346239    0.26094449]]
[37m[1m[2023-06-25 06:13:55,360][129146] Max Reward on eval: 3719.3156031337444
[37m[1m[2023-06-25 06:13:55,360][129146] Min Reward on eval: 3719.3156031337444
[37m[1m[2023-06-25 06:13:55,360][129146] Mean Reward across all agents: 3719.3156031337444
[37m[1m[2023-06-25 06:13:55,360][129146] Average Trajectory Length: 996.3746666666666
[36m[2023-06-25 06:14:00,865][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:14:00,866][129146] Reward + Measures: [[2026.74546299    0.33039999    0.35770002    0.06879999    0.17760001]
[37m[1m [2186.34110468    0.27152899    0.32263765    0.12053623    0.27988985]
[37m[1m [1418.26702365    0.29630002    0.37779999    0.1974        0.30180001]
[37m[1m ...
[37m[1m [1793.08555625    0.3712        0.34670001    0.1953        0.31099996]
[37m[1m [2862.51197511    0.25299999    0.33150002    0.0856        0.30000001]
[37m[1m [1801.65509096    0.35230002    0.3917        0.2181        0.30960003]]
[37m[1m[2023-06-25 06:14:00,866][129146] Max Reward on eval: 3511.6105960382147
[37m[1m[2023-06-25 06:14:00,866][129146] Min Reward on eval: -182.3088255954266
[37m[1m[2023-06-25 06:14:00,866][129146] Mean Reward across all agents: 1931.758838495478
[37m[1m[2023-06-25 06:14:00,867][129146] Average Trajectory Length: 955.0993333333333
[36m[2023-06-25 06:14:00,869][129146] mean_value=-835.7438787454095, max_value=1155.4923599890071
[37m[1m[2023-06-25 06:14:00,872][129146] New mean coefficients: [[ 1.9611317   0.13292757 -0.40465078  0.30230784  0.33978486]]
[37m[1m[2023-06-25 06:14:00,873][129146] Moving the mean solution point...
[36m[2023-06-25 06:14:10,476][129146] train() took 9.60 seconds to complete
[36m[2023-06-25 06:14:10,476][129146] FPS: 399923.87
[36m[2023-06-25 06:14:10,478][129146] itr=560, itrs=2000, Progress: 28.00%
[37m[1m[2023-06-25 06:14:15,433][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000540
[36m[2023-06-25 06:14:27,203][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 06:14:27,203][129146] FPS: 334466.41
[36m[2023-06-25 06:14:32,049][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:14:32,050][129146] Reward + Measures: [[3874.85008432    0.29581609    0.32402739    0.04008647    0.25578427]]
[37m[1m[2023-06-25 06:14:32,050][129146] Max Reward on eval: 3874.850084316621
[37m[1m[2023-06-25 06:14:32,050][129146] Min Reward on eval: 3874.850084316621
[37m[1m[2023-06-25 06:14:32,050][129146] Mean Reward across all agents: 3874.850084316621
[37m[1m[2023-06-25 06:14:32,050][129146] Average Trajectory Length: 997.8496666666666
[36m[2023-06-25 06:14:37,506][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:14:37,506][129146] Reward + Measures: [[3673.17746711    0.30539998    0.32950002    0.0409        0.26370001]
[37m[1m [2698.74272487    0.28840002    0.35879999    0.1313        0.25260001]
[37m[1m [ 261.93316217    0.1418        0.2113        0.1044        0.1323    ]
[37m[1m ...
[37m[1m [3145.48235281    0.24660002    0.34609997    0.0838        0.23190001]
[37m[1m [2442.14998817    0.2471        0.35780001    0.1436        0.27589998]
[37m[1m [ 198.91908254    0.5550549     0.27139214    0.4401314     0.60489607]]
[37m[1m[2023-06-25 06:14:37,507][129146] Max Reward on eval: 3695.1272322779987
[37m[1m[2023-06-25 06:14:37,507][129146] Min Reward on eval: -180.79944781273662
[37m[1m[2023-06-25 06:14:37,507][129146] Mean Reward across all agents: 1979.0533665145417
[37m[1m[2023-06-25 06:14:37,507][129146] Average Trajectory Length: 983.4446666666666
[36m[2023-06-25 06:14:37,510][129146] mean_value=-927.8591968846872, max_value=1067.6998074185676
[37m[1m[2023-06-25 06:14:37,512][129146] New mean coefficients: [[ 1.3465412   0.5028576  -0.35245347  0.13083264  0.3721793 ]]
[37m[1m[2023-06-25 06:14:37,513][129146] Moving the mean solution point...
[36m[2023-06-25 06:14:47,325][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 06:14:47,326][129146] FPS: 391417.98
[36m[2023-06-25 06:14:47,328][129146] itr=561, itrs=2000, Progress: 28.05%
[36m[2023-06-25 06:14:58,860][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 06:14:58,860][129146] FPS: 333529.40
[36m[2023-06-25 06:15:03,766][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:15:03,766][129146] Reward + Measures: [[3810.43699668    0.33442584    0.29442149    0.04508353    0.25195926]]
[37m[1m[2023-06-25 06:15:03,766][129146] Max Reward on eval: 3810.436996675834
[37m[1m[2023-06-25 06:15:03,767][129146] Min Reward on eval: 3810.436996675834
[37m[1m[2023-06-25 06:15:03,767][129146] Mean Reward across all agents: 3810.436996675834
[37m[1m[2023-06-25 06:15:03,767][129146] Average Trajectory Length: 997.8903333333333
[36m[2023-06-25 06:15:09,242][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:15:09,243][129146] Reward + Measures: [[1790.67557829    0.31950623    0.24623466    0.08304441    0.20990519]
[37m[1m [1328.88827823    0.28710002    0.52570003    0.1515        0.32769999]
[37m[1m [ 975.02445788    0.24630001    0.36719999    0.1066        0.25690004]
[37m[1m ...
[37m[1m [3182.30041444    0.33360001    0.32619998    0.0765        0.26000002]
[37m[1m [2035.99865225    0.30670187    0.30693334    0.08001145    0.1873184 ]
[37m[1m [1810.66194147    0.26623774    0.32569644    0.14051296    0.3303369 ]]
[37m[1m[2023-06-25 06:15:09,243][129146] Max Reward on eval: 3714.488451700285
[37m[1m[2023-06-25 06:15:09,243][129146] Min Reward on eval: 78.72603595296096
[37m[1m[2023-06-25 06:15:09,243][129146] Mean Reward across all agents: 1979.6889369432224
[37m[1m[2023-06-25 06:15:09,244][129146] Average Trajectory Length: 982.6006666666666
[36m[2023-06-25 06:15:09,247][129146] mean_value=-593.3409047006513, max_value=2963.159629755874
[37m[1m[2023-06-25 06:15:09,249][129146] New mean coefficients: [[ 1.3262008   0.19183204 -0.2593991   0.14647686  0.03489175]]
[37m[1m[2023-06-25 06:15:09,250][129146] Moving the mean solution point...
[36m[2023-06-25 06:15:18,877][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 06:15:18,877][129146] FPS: 398947.11
[36m[2023-06-25 06:15:18,880][129146] itr=562, itrs=2000, Progress: 28.10%
[36m[2023-06-25 06:15:30,397][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 06:15:30,397][129146] FPS: 334056.81
[36m[2023-06-25 06:15:35,223][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:15:35,223][129146] Reward + Measures: [[3947.60463916    0.3257679     0.30724132    0.04487449    0.25742802]]
[37m[1m[2023-06-25 06:15:35,223][129146] Max Reward on eval: 3947.6046391598497
[37m[1m[2023-06-25 06:15:35,223][129146] Min Reward on eval: 3947.6046391598497
[37m[1m[2023-06-25 06:15:35,224][129146] Mean Reward across all agents: 3947.6046391598497
[37m[1m[2023-06-25 06:15:35,224][129146] Average Trajectory Length: 997.3073333333333
[36m[2023-06-25 06:15:40,781][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:15:40,782][129146] Reward + Measures: [[3398.38408806    0.28749999    0.37380001    0.0674        0.30239999]
[37m[1m [3356.42024412    0.28959998    0.35880002    0.0672        0.29950002]
[37m[1m [ 522.55910712    0.56770003    0.35820001    0.45559999    0.46960002]
[37m[1m ...
[37m[1m [3498.69651181    0.36490002    0.31099999    0.044         0.24130002]
[37m[1m [2893.72565882    0.2915        0.41079998    0.1285        0.21830001]
[37m[1m [3370.69659752    0.26290002    0.40330002    0.1231        0.2357    ]]
[37m[1m[2023-06-25 06:15:40,782][129146] Max Reward on eval: 3955.801694513671
[37m[1m[2023-06-25 06:15:40,782][129146] Min Reward on eval: 328.1364959404338
[37m[1m[2023-06-25 06:15:40,782][129146] Mean Reward across all agents: 2403.356367662571
[37m[1m[2023-06-25 06:15:40,783][129146] Average Trajectory Length: 992.0643333333333
[36m[2023-06-25 06:15:40,787][129146] mean_value=-378.64790125721146, max_value=2891.3227146558215
[37m[1m[2023-06-25 06:15:40,790][129146] New mean coefficients: [[ 1.3477341   0.21485588 -0.36827448  0.0723241   0.33946872]]
[37m[1m[2023-06-25 06:15:40,791][129146] Moving the mean solution point...
[36m[2023-06-25 06:15:50,465][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 06:15:50,465][129146] FPS: 396999.33
[36m[2023-06-25 06:15:50,467][129146] itr=563, itrs=2000, Progress: 28.15%
[36m[2023-06-25 06:16:01,984][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 06:16:01,984][129146] FPS: 333981.29
[36m[2023-06-25 06:16:06,909][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:16:06,915][129146] Reward + Measures: [[4064.88458357    0.31938317    0.31623301    0.04992658    0.26151001]]
[37m[1m[2023-06-25 06:16:06,915][129146] Max Reward on eval: 4064.8845835654624
[37m[1m[2023-06-25 06:16:06,915][129146] Min Reward on eval: 4064.8845835654624
[37m[1m[2023-06-25 06:16:06,915][129146] Mean Reward across all agents: 4064.8845835654624
[37m[1m[2023-06-25 06:16:06,916][129146] Average Trajectory Length: 998.8856666666667
[36m[2023-06-25 06:16:12,430][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:16:12,430][129146] Reward + Measures: [[1997.20483945    0.27320623    0.36532822    0.13467942    0.21333109]
[37m[1m [1359.69228824    0.30650002    0.41510001    0.1768        0.31820002]
[37m[1m [3550.97852911    0.32430002    0.3651        0.0838        0.27110001]
[37m[1m ...
[37m[1m [1655.6041255     0.25440001    0.38150001    0.14680001    0.23220001]
[37m[1m [2506.23165413    0.29519999    0.39390001    0.111         0.2586    ]
[37m[1m [2047.38553616    0.31070003    0.33670002    0.0746        0.27169999]]
[37m[1m[2023-06-25 06:16:12,430][129146] Max Reward on eval: 4040.70822859006
[37m[1m[2023-06-25 06:16:12,431][129146] Min Reward on eval: 327.7886124555371
[37m[1m[2023-06-25 06:16:12,431][129146] Mean Reward across all agents: 2416.7632658074263
[37m[1m[2023-06-25 06:16:12,431][129146] Average Trajectory Length: 981.6883333333333
[36m[2023-06-25 06:16:12,435][129146] mean_value=-549.4439189452212, max_value=1749.9040560777307
[37m[1m[2023-06-25 06:16:12,437][129146] New mean coefficients: [[ 1.3609644  -0.14154553 -0.43774498  0.17784053  0.07229152]]
[37m[1m[2023-06-25 06:16:12,438][129146] Moving the mean solution point...
[36m[2023-06-25 06:16:22,145][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 06:16:22,145][129146] FPS: 395679.58
[36m[2023-06-25 06:16:22,147][129146] itr=564, itrs=2000, Progress: 28.20%
[36m[2023-06-25 06:16:33,681][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 06:16:33,681][129146] FPS: 333525.24
[36m[2023-06-25 06:16:38,476][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:16:38,477][129146] Reward + Measures: [[4174.03148893    0.31269249    0.32455158    0.04845846    0.2570501 ]]
[37m[1m[2023-06-25 06:16:38,477][129146] Max Reward on eval: 4174.0314889324945
[37m[1m[2023-06-25 06:16:38,477][129146] Min Reward on eval: 4174.0314889324945
[37m[1m[2023-06-25 06:16:38,478][129146] Mean Reward across all agents: 4174.0314889324945
[37m[1m[2023-06-25 06:16:38,478][129146] Average Trajectory Length: 999.6463333333332
[36m[2023-06-25 06:16:43,935][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:16:43,938][129146] Reward + Measures: [[3820.23316948    0.31189999    0.29170001    0.0635        0.29819998]
[37m[1m [2093.11095198    0.34380001    0.44260001    0.13810001    0.39270002]
[37m[1m [3800.79836475    0.30630001    0.31510001    0.0468        0.25279999]
[37m[1m ...
[37m[1m [3762.57371982    0.3017        0.28449997    0.0638        0.27669999]
[37m[1m [ 479.37525871    0.37690002    0.55650002    0.16630001    0.57260001]
[37m[1m [2274.12572269    0.2994        0.3574        0.1059        0.33090001]]
[37m[1m[2023-06-25 06:16:43,939][129146] Max Reward on eval: 4113.0917713734325
[37m[1m[2023-06-25 06:16:43,939][129146] Min Reward on eval: 43.850339630083184
[37m[1m[2023-06-25 06:16:43,939][129146] Mean Reward across all agents: 2351.8294603685613
[37m[1m[2023-06-25 06:16:43,939][129146] Average Trajectory Length: 997.1663333333333
[36m[2023-06-25 06:16:43,944][129146] mean_value=-281.67615526039305, max_value=1047.4852337672069
[37m[1m[2023-06-25 06:16:43,947][129146] New mean coefficients: [[ 1.7081769  -0.61512065 -0.5871533   0.07673161  0.12110602]]
[37m[1m[2023-06-25 06:16:43,948][129146] Moving the mean solution point...
[36m[2023-06-25 06:16:53,665][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 06:16:53,666][129146] FPS: 395250.80
[36m[2023-06-25 06:16:53,668][129146] itr=565, itrs=2000, Progress: 28.25%
[36m[2023-06-25 06:17:05,053][129146] train() took 11.36 seconds to complete
[36m[2023-06-25 06:17:05,053][129146] FPS: 337950.33
[36m[2023-06-25 06:17:09,781][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:17:09,786][129146] Reward + Measures: [[4277.4578979     0.30995059    0.31850082    0.04158254    0.25171024]]
[37m[1m[2023-06-25 06:17:09,787][129146] Max Reward on eval: 4277.4578979012485
[37m[1m[2023-06-25 06:17:09,787][129146] Min Reward on eval: 4277.4578979012485
[37m[1m[2023-06-25 06:17:09,787][129146] Mean Reward across all agents: 4277.4578979012485
[37m[1m[2023-06-25 06:17:09,787][129146] Average Trajectory Length: 997.0273333333333
[36m[2023-06-25 06:17:15,160][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:17:15,165][129146] Reward + Measures: [[1564.3409138     0.30329999    0.39910004    0.1209        0.39949998]
[37m[1m [2256.12511622    0.42290002    0.36050004    0.1552        0.35459998]
[37m[1m [1481.9801036     0.4237        0.33220002    0.09260001    0.36050001]
[37m[1m ...
[37m[1m [2600.89617107    0.37579998    0.30179998    0.0958        0.30930001]
[37m[1m [2641.18113877    0.3515        0.38010001    0.1558        0.27199998]
[37m[1m [2621.75371715    0.37780005    0.24840002    0.0565        0.22850001]]
[37m[1m[2023-06-25 06:17:15,166][129146] Max Reward on eval: 4260.901224156725
[37m[1m[2023-06-25 06:17:15,166][129146] Min Reward on eval: -960.0765114112058
[37m[1m[2023-06-25 06:17:15,166][129146] Mean Reward across all agents: 2165.6183971255145
[37m[1m[2023-06-25 06:17:15,167][129146] Average Trajectory Length: 996.8196666666666
[36m[2023-06-25 06:17:15,170][129146] mean_value=-372.8270233050384, max_value=2906.20895937577
[37m[1m[2023-06-25 06:17:15,173][129146] New mean coefficients: [[ 1.1730286  -0.6441898  -0.73520565  0.02000868  0.1601017 ]]
[37m[1m[2023-06-25 06:17:15,174][129146] Moving the mean solution point...
[36m[2023-06-25 06:17:24,920][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 06:17:24,920][129146] FPS: 394105.87
[36m[2023-06-25 06:17:24,922][129146] itr=566, itrs=2000, Progress: 28.30%
[36m[2023-06-25 06:17:36,481][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 06:17:36,481][129146] FPS: 332864.19
[36m[2023-06-25 06:17:41,273][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:17:41,274][129146] Reward + Measures: [[4345.24215456    0.29768044    0.32447955    0.04285005    0.25281796]]
[37m[1m[2023-06-25 06:17:41,274][129146] Max Reward on eval: 4345.242154559373
[37m[1m[2023-06-25 06:17:41,274][129146] Min Reward on eval: 4345.242154559373
[37m[1m[2023-06-25 06:17:41,274][129146] Mean Reward across all agents: 4345.242154559373
[37m[1m[2023-06-25 06:17:41,275][129146] Average Trajectory Length: 997.3053333333334
[36m[2023-06-25 06:17:46,929][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:17:46,930][129146] Reward + Measures: [[3367.61093234    0.33400002    0.3479        0.09670001    0.2811    ]
[37m[1m [2128.85487439    0.26092741    0.30588326    0.10792305    0.26758322]
[37m[1m [3602.9687009     0.27870002    0.3263        0.0695        0.29449999]
[37m[1m ...
[37m[1m [1247.21071898    0.25750002    0.31370002    0.1955        0.2326    ]
[37m[1m [1250.64570611    0.32430002    0.3628        0.171         0.24660002]
[37m[1m [ -67.50075401    0.1596        0.1865        0.14470001    0.14050001]]
[37m[1m[2023-06-25 06:17:46,930][129146] Max Reward on eval: 4274.135601765755
[37m[1m[2023-06-25 06:17:46,930][129146] Min Reward on eval: -67.50075400898349
[37m[1m[2023-06-25 06:17:46,930][129146] Mean Reward across all agents: 2392.7692584066463
[37m[1m[2023-06-25 06:17:46,930][129146] Average Trajectory Length: 990.7973333333333
[36m[2023-06-25 06:17:46,933][129146] mean_value=-834.2317883559211, max_value=839.4171826590987
[37m[1m[2023-06-25 06:17:46,935][129146] New mean coefficients: [[ 0.56166095 -0.420156   -0.5481704  -0.02103915  0.02772658]]
[37m[1m[2023-06-25 06:17:46,936][129146] Moving the mean solution point...
[36m[2023-06-25 06:17:56,625][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 06:17:56,625][129146] FPS: 396431.35
[36m[2023-06-25 06:17:56,627][129146] itr=567, itrs=2000, Progress: 28.35%
[36m[2023-06-25 06:18:08,028][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 06:18:08,029][129146] FPS: 337409.70
[36m[2023-06-25 06:18:12,828][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:18:12,834][129146] Reward + Measures: [[3783.19393459    0.2609131     0.37576228    0.09259602    0.27206299]]
[37m[1m[2023-06-25 06:18:12,834][129146] Max Reward on eval: 3783.1939345942146
[37m[1m[2023-06-25 06:18:12,834][129146] Min Reward on eval: 3783.1939345942146
[37m[1m[2023-06-25 06:18:12,835][129146] Mean Reward across all agents: 3783.1939345942146
[37m[1m[2023-06-25 06:18:12,835][129146] Average Trajectory Length: 997.0986666666666
[36m[2023-06-25 06:18:18,369][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:18:18,370][129146] Reward + Measures: [[ 643.97808707    0.36922127    0.5079186     0.19181846    0.35517409]
[37m[1m [1520.4767397     0.36410001    0.45059997    0.1842        0.40000001]
[37m[1m [2607.98079588    0.25101945    0.38882837    0.09990893    0.23279563]
[37m[1m ...
[37m[1m [ 915.20181119    0.31860003    0.5248        0.1947        0.44099998]
[37m[1m [2919.23903206    0.25929999    0.25560001    0.0457        0.22760001]
[37m[1m [1898.75635183    0.384         0.38500002    0.18910001    0.23509999]]
[37m[1m[2023-06-25 06:18:18,370][129146] Max Reward on eval: 3808.1870767301416
[37m[1m[2023-06-25 06:18:18,370][129146] Min Reward on eval: -181.08487121534125
[37m[1m[2023-06-25 06:18:18,371][129146] Mean Reward across all agents: 1729.2764271469518
[37m[1m[2023-06-25 06:18:18,371][129146] Average Trajectory Length: 985.0983333333332
[36m[2023-06-25 06:18:18,373][129146] mean_value=-908.2518911269293, max_value=1346.1042452746424
[37m[1m[2023-06-25 06:18:18,376][129146] New mean coefficients: [[ 0.43787664 -0.32202947 -0.35297725  0.0466832   0.1405977 ]]
[37m[1m[2023-06-25 06:18:18,377][129146] Moving the mean solution point...
[36m[2023-06-25 06:18:28,171][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 06:18:28,171][129146] FPS: 392146.14
[36m[2023-06-25 06:18:28,174][129146] itr=568, itrs=2000, Progress: 28.40%
[36m[2023-06-25 06:18:39,677][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 06:18:39,678][129146] FPS: 334379.50
[36m[2023-06-25 06:18:44,468][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:18:44,468][129146] Reward + Measures: [[4010.79731575    0.25668007    0.36498672    0.08648773    0.27347144]]
[37m[1m[2023-06-25 06:18:44,469][129146] Max Reward on eval: 4010.7973157455417
[37m[1m[2023-06-25 06:18:44,469][129146] Min Reward on eval: 4010.7973157455417
[37m[1m[2023-06-25 06:18:44,469][129146] Mean Reward across all agents: 4010.7973157455417
[37m[1m[2023-06-25 06:18:44,469][129146] Average Trajectory Length: 997.1446666666666
[36m[2023-06-25 06:18:49,818][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:18:49,818][129146] Reward + Measures: [[2303.34960456    0.28622043    0.3311086     0.16886775    0.27149352]
[37m[1m [3059.49833628    0.32529998    0.30710003    0.0505        0.2201    ]
[37m[1m [2524.65855939    0.2933        0.36039999    0.15900001    0.29660001]
[37m[1m ...
[37m[1m [2129.56109275    0.34185433    0.41709194    0.14267412    0.32265764]
[37m[1m [2534.95407248    0.31810001    0.36559999    0.18810001    0.2814    ]
[37m[1m [1304.61879555    0.36629996    0.42600003    0.22280002    0.30159998]]
[37m[1m[2023-06-25 06:18:49,819][129146] Max Reward on eval: 4056.336371340882
[37m[1m[2023-06-25 06:18:49,819][129146] Min Reward on eval: 203.8001613807457
[37m[1m[2023-06-25 06:18:49,819][129146] Mean Reward across all agents: 2360.4103984739963
[37m[1m[2023-06-25 06:18:49,819][129146] Average Trajectory Length: 986.1053333333333
[36m[2023-06-25 06:18:49,822][129146] mean_value=-837.8226333676982, max_value=1570.0122455056694
[37m[1m[2023-06-25 06:18:49,824][129146] New mean coefficients: [[ 0.19154762 -0.06735688 -0.2670283   0.00012833  0.07463713]]
[37m[1m[2023-06-25 06:18:49,825][129146] Moving the mean solution point...
[36m[2023-06-25 06:18:59,619][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 06:18:59,620][129146] FPS: 392131.87
[36m[2023-06-25 06:18:59,622][129146] itr=569, itrs=2000, Progress: 28.45%
[36m[2023-06-25 06:19:11,299][129146] train() took 11.66 seconds to complete
[36m[2023-06-25 06:19:11,299][129146] FPS: 329456.19
[36m[2023-06-25 06:19:16,133][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:19:16,133][129146] Reward + Measures: [[4221.49311798    0.26133358    0.34165141    0.07462661    0.26692641]]
[37m[1m[2023-06-25 06:19:16,133][129146] Max Reward on eval: 4221.493117982176
[37m[1m[2023-06-25 06:19:16,133][129146] Min Reward on eval: 4221.493117982176
[37m[1m[2023-06-25 06:19:16,134][129146] Mean Reward across all agents: 4221.493117982176
[37m[1m[2023-06-25 06:19:16,134][129146] Average Trajectory Length: 997.2133333333333
[36m[2023-06-25 06:19:21,587][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:19:21,640][129146] Reward + Measures: [[3656.82396204    0.27430001    0.3522        0.09180001    0.28150001]
[37m[1m [3382.15581562    0.244         0.32570001    0.1109        0.29909998]
[37m[1m [3480.45508666    0.27866814    0.39114961    0.09033843    0.22490592]
[37m[1m ...
[37m[1m [1396.99919956    0.43239999    0.50700003    0.0675        0.50570005]
[37m[1m [3261.17236145    0.29800001    0.35080001    0.08800001    0.2915    ]
[37m[1m [3643.00244458    0.30896074    0.39401704    0.08203983    0.24130097]]
[37m[1m[2023-06-25 06:19:21,640][129146] Max Reward on eval: 4121.31635153722
[37m[1m[2023-06-25 06:19:21,640][129146] Min Reward on eval: 316.4628870006185
[37m[1m[2023-06-25 06:19:21,640][129146] Mean Reward across all agents: 2723.9675218178013
[37m[1m[2023-06-25 06:19:21,641][129146] Average Trajectory Length: 976.313
[36m[2023-06-25 06:19:21,643][129146] mean_value=-677.4078967082531, max_value=962.9798505978208
[37m[1m[2023-06-25 06:19:21,646][129146] New mean coefficients: [[ 0.4804691   0.04029209 -0.15988041  0.00886279  0.12142643]]
[37m[1m[2023-06-25 06:19:21,647][129146] Moving the mean solution point...
[36m[2023-06-25 06:19:31,418][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 06:19:31,419][129146] FPS: 393043.28
[36m[2023-06-25 06:19:31,421][129146] itr=570, itrs=2000, Progress: 28.50%
[37m[1m[2023-06-25 06:19:36,583][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000550
[36m[2023-06-25 06:19:48,511][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 06:19:48,511][129146] FPS: 333397.47
[36m[2023-06-25 06:19:53,183][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:19:53,183][129146] Reward + Measures: [[4400.46718708    0.25864497    0.34280288    0.07945544    0.26964718]]
[37m[1m[2023-06-25 06:19:53,183][129146] Max Reward on eval: 4400.467187080262
[37m[1m[2023-06-25 06:19:53,184][129146] Min Reward on eval: 4400.467187080262
[37m[1m[2023-06-25 06:19:53,184][129146] Mean Reward across all agents: 4400.467187080262
[37m[1m[2023-06-25 06:19:53,184][129146] Average Trajectory Length: 997.2239999999999
[36m[2023-06-25 06:19:58,391][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:19:58,391][129146] Reward + Measures: [[1566.5539697     0.19276705    0.25261706    0.11228978    0.16705796]
[37m[1m [4337.09980458    0.27159998    0.31430003    0.0894        0.2793    ]
[37m[1m [3725.89106791    0.26525503    0.329689      0.11947942    0.27418038]
[37m[1m ...
[37m[1m [2380.39544836    0.21759926    0.33151299    0.15835725    0.22229086]
[37m[1m [1950.27043739    0.32619998    0.35440001    0.146         0.31119999]
[37m[1m [2295.49041823    0.21024685    0.23801394    0.09272658    0.20911646]]
[37m[1m[2023-06-25 06:19:58,392][129146] Max Reward on eval: 4454.801388867758
[37m[1m[2023-06-25 06:19:58,392][129146] Min Reward on eval: -400.01351072008254
[37m[1m[2023-06-25 06:19:58,392][129146] Mean Reward across all agents: 2551.610084988783
[37m[1m[2023-06-25 06:19:58,392][129146] Average Trajectory Length: 959.8186666666667
[36m[2023-06-25 06:19:58,395][129146] mean_value=-636.6770542457782, max_value=1490.7773689575722
[37m[1m[2023-06-25 06:19:58,398][129146] New mean coefficients: [[ 0.55372036 -0.12240202 -0.03182042  0.00186875 -0.00485789]]
[37m[1m[2023-06-25 06:19:58,399][129146] Moving the mean solution point...
[36m[2023-06-25 06:20:08,013][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 06:20:08,014][129146] FPS: 399446.75
[36m[2023-06-25 06:20:08,016][129146] itr=571, itrs=2000, Progress: 28.55%
[36m[2023-06-25 06:20:19,426][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 06:20:19,426][129146] FPS: 337147.40
[36m[2023-06-25 06:20:24,255][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:20:24,256][129146] Reward + Measures: [[4522.45965822    0.25967604    0.33936176    0.07507474    0.26237479]]
[37m[1m[2023-06-25 06:20:24,256][129146] Max Reward on eval: 4522.459658219153
[37m[1m[2023-06-25 06:20:24,256][129146] Min Reward on eval: 4522.459658219153
[37m[1m[2023-06-25 06:20:24,256][129146] Mean Reward across all agents: 4522.459658219153
[37m[1m[2023-06-25 06:20:24,256][129146] Average Trajectory Length: 997.1279999999999
[36m[2023-06-25 06:20:29,708][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:20:29,709][129146] Reward + Measures: [[ 828.49540355    0.21054034    0.23581873    0.13763686    0.17421697]
[37m[1m [2619.7526905     0.2881        0.3448        0.1204        0.23899999]
[37m[1m [2892.63950211    0.35479996    0.26550004    0.0787        0.24169998]
[37m[1m ...
[37m[1m [3513.03225809    0.28410003    0.28150001    0.0763        0.26550001]
[37m[1m [2983.0366285     0.317         0.35920003    0.14489999    0.26570001]
[37m[1m [3464.80031929    0.24229999    0.40050003    0.1249        0.23800002]]
[37m[1m[2023-06-25 06:20:29,709][129146] Max Reward on eval: 4570.665521860914
[37m[1m[2023-06-25 06:20:29,709][129146] Min Reward on eval: 290.13316304564944
[37m[1m[2023-06-25 06:20:29,710][129146] Mean Reward across all agents: 2563.0439343831113
[37m[1m[2023-06-25 06:20:29,710][129146] Average Trajectory Length: 971.225
[36m[2023-06-25 06:20:29,712][129146] mean_value=-746.292502960491, max_value=1052.9587798293933
[37m[1m[2023-06-25 06:20:29,715][129146] New mean coefficients: [[ 0.5315218  -0.08311234 -0.02964476 -0.05897499  0.0270343 ]]
[37m[1m[2023-06-25 06:20:29,716][129146] Moving the mean solution point...
[36m[2023-06-25 06:20:39,573][129146] train() took 9.86 seconds to complete
[36m[2023-06-25 06:20:39,573][129146] FPS: 389640.61
[36m[2023-06-25 06:20:39,575][129146] itr=572, itrs=2000, Progress: 28.60%
[36m[2023-06-25 06:20:51,187][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 06:20:51,187][129146] FPS: 331354.86
[36m[2023-06-25 06:20:56,047][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:20:56,047][129146] Reward + Measures: [[4561.71278095    0.25947046    0.3367995     0.0728942     0.25876942]]
[37m[1m[2023-06-25 06:20:56,047][129146] Max Reward on eval: 4561.712780950998
[37m[1m[2023-06-25 06:20:56,048][129146] Min Reward on eval: 4561.712780950998
[37m[1m[2023-06-25 06:20:56,048][129146] Mean Reward across all agents: 4561.712780950998
[37m[1m[2023-06-25 06:20:56,048][129146] Average Trajectory Length: 998.9816666666667
[36m[2023-06-25 06:21:01,579][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:21:01,580][129146] Reward + Measures: [[3513.16345599    0.23380001    0.27540001    0.09860001    0.25099999]
[37m[1m [1149.70717516    0.41370001    0.47270003    0.25549999    0.3788    ]
[37m[1m [1537.76926294    0.25138003    0.22947276    0.09015229    0.16051979]
[37m[1m ...
[37m[1m [2433.37766876    0.27619943    0.26073983    0.07992293    0.20976877]
[37m[1m [1172.25458976    0.24348573    0.24599424    0.13241372    0.19148174]
[37m[1m [4221.56643436    0.27989998    0.40430003    0.11930001    0.2791    ]]
[37m[1m[2023-06-25 06:21:01,580][129146] Max Reward on eval: 4432.105135606975
[37m[1m[2023-06-25 06:21:01,581][129146] Min Reward on eval: -54.916869667416904
[37m[1m[2023-06-25 06:21:01,581][129146] Mean Reward across all agents: 2306.2000493201103
[37m[1m[2023-06-25 06:21:01,581][129146] Average Trajectory Length: 967.4513333333333
[36m[2023-06-25 06:21:01,585][129146] mean_value=-660.5272316160365, max_value=1128.2539484252457
[37m[1m[2023-06-25 06:21:01,588][129146] New mean coefficients: [[ 0.5348103   0.23011473  0.20346722 -0.12493408  0.1427247 ]]
[37m[1m[2023-06-25 06:21:01,589][129146] Moving the mean solution point...
[36m[2023-06-25 06:21:11,369][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 06:21:11,369][129146] FPS: 392705.81
[36m[2023-06-25 06:21:11,371][129146] itr=573, itrs=2000, Progress: 28.65%
[36m[2023-06-25 06:21:22,946][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 06:21:22,946][129146] FPS: 332305.97
[36m[2023-06-25 06:21:27,830][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:21:27,831][129146] Reward + Measures: [[4552.16658265    0.26344553    0.33683732    0.07326248    0.25942209]]
[37m[1m[2023-06-25 06:21:27,831][129146] Max Reward on eval: 4552.166582654343
[37m[1m[2023-06-25 06:21:27,831][129146] Min Reward on eval: 4552.166582654343
[37m[1m[2023-06-25 06:21:27,831][129146] Mean Reward across all agents: 4552.166582654343
[37m[1m[2023-06-25 06:21:27,831][129146] Average Trajectory Length: 997.2769999999999
[36m[2023-06-25 06:21:33,548][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:21:33,549][129146] Reward + Measures: [[2777.59854211    0.3131828     0.35816854    0.13903566    0.29645848]
[37m[1m [2432.60197836    0.28780001    0.29609999    0.13540001    0.2455    ]
[37m[1m [3221.93644225    0.27140003    0.36560002    0.13700001    0.29189998]
[37m[1m ...
[37m[1m [1517.68607579    0.33440003    0.31440002    0.18499999    0.28309998]
[37m[1m [3861.27316704    0.25029999    0.31990001    0.0931        0.2667    ]
[37m[1m [2424.31743052    0.2647        0.29609999    0.1142        0.255     ]]
[37m[1m[2023-06-25 06:21:33,549][129146] Max Reward on eval: 4480.926408773451
[37m[1m[2023-06-25 06:21:33,549][129146] Min Reward on eval: -530.0494820172665
[37m[1m[2023-06-25 06:21:33,550][129146] Mean Reward across all agents: 2764.2252471427814
[37m[1m[2023-06-25 06:21:33,550][129146] Average Trajectory Length: 981.425
[36m[2023-06-25 06:21:33,552][129146] mean_value=-839.7117026816609, max_value=1575.2454216534686
[37m[1m[2023-06-25 06:21:33,555][129146] New mean coefficients: [[ 0.38837126 -0.02691698 -0.0537757  -0.25808144  0.19706835]]
[37m[1m[2023-06-25 06:21:33,556][129146] Moving the mean solution point...
[36m[2023-06-25 06:21:43,376][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 06:21:43,376][129146] FPS: 391113.69
[36m[2023-06-25 06:21:43,378][129146] itr=574, itrs=2000, Progress: 28.70%
[36m[2023-06-25 06:21:54,901][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 06:21:54,901][129146] FPS: 333832.71
[36m[2023-06-25 06:21:59,777][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:21:59,777][129146] Reward + Measures: [[4722.12215118    0.26686096    0.3341158     0.06511252    0.25718322]]
[37m[1m[2023-06-25 06:21:59,777][129146] Max Reward on eval: 4722.122151183028
[37m[1m[2023-06-25 06:21:59,778][129146] Min Reward on eval: 4722.122151183028
[37m[1m[2023-06-25 06:21:59,778][129146] Mean Reward across all agents: 4722.122151183028
[37m[1m[2023-06-25 06:21:59,778][129146] Average Trajectory Length: 997.8506666666666
[36m[2023-06-25 06:22:05,332][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:22:05,332][129146] Reward + Measures: [[3805.44624523    0.26200002    0.30990002    0.0427        0.23409998]
[37m[1m [1937.7806552     0.26992893    0.31564981    0.17593217    0.25227392]
[37m[1m [1922.04098367    0.32940003    0.3159        0.1538        0.27880001]
[37m[1m ...
[37m[1m [3800.43831411    0.28523093    0.35109335    0.11074954    0.30422744]
[37m[1m [3400.46686715    0.27669999    0.37280002    0.0756        0.26499999]
[37m[1m [3003.03099388    0.28771526    0.27608308    0.10646408    0.22906128]]
[37m[1m[2023-06-25 06:22:05,332][129146] Max Reward on eval: 4639.50494526308
[37m[1m[2023-06-25 06:22:05,333][129146] Min Reward on eval: 399.6227049556677
[37m[1m[2023-06-25 06:22:05,333][129146] Mean Reward across all agents: 3323.099103653191
[37m[1m[2023-06-25 06:22:05,333][129146] Average Trajectory Length: 984.6023333333333
[36m[2023-06-25 06:22:05,336][129146] mean_value=-663.9468068810194, max_value=1484.5122956064213
[37m[1m[2023-06-25 06:22:05,339][129146] New mean coefficients: [[ 0.32288253 -0.16085918  0.03425518 -0.03367515  0.07265931]]
[37m[1m[2023-06-25 06:22:05,340][129146] Moving the mean solution point...
[36m[2023-06-25 06:22:15,143][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 06:22:15,143][129146] FPS: 391760.53
[36m[2023-06-25 06:22:15,146][129146] itr=575, itrs=2000, Progress: 28.75%
[36m[2023-06-25 06:22:26,659][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 06:22:26,659][129146] FPS: 334109.29
[36m[2023-06-25 06:22:31,600][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:22:31,601][129146] Reward + Measures: [[4835.27351949    0.2702567     0.33190182    0.0600206     0.25705624]]
[37m[1m[2023-06-25 06:22:31,601][129146] Max Reward on eval: 4835.27351949154
[37m[1m[2023-06-25 06:22:31,601][129146] Min Reward on eval: 4835.27351949154
[37m[1m[2023-06-25 06:22:31,601][129146] Mean Reward across all agents: 4835.27351949154
[37m[1m[2023-06-25 06:22:31,602][129146] Average Trajectory Length: 997.9446666666666
[36m[2023-06-25 06:22:37,094][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:22:37,095][129146] Reward + Measures: [[ 537.14340684    0.5837        0.1356        0.52159995    0.54320002]
[37m[1m [ 603.220034      0.56800002    0.60439998    0.43710002    0.58450001]
[37m[1m [1836.43071338    0.53820002    0.2595        0.43099999    0.52420002]
[37m[1m ...
[37m[1m [3670.69848317    0.31259999    0.36449999    0.13070001    0.3344    ]
[37m[1m [3681.30615952    0.29679999    0.3576        0.12150001    0.37020001]
[37m[1m [1850.6743844     0.31397793    0.27635208    0.09243905    0.18510257]]
[37m[1m[2023-06-25 06:22:37,095][129146] Max Reward on eval: 4779.690656926856
[37m[1m[2023-06-25 06:22:37,095][129146] Min Reward on eval: -15.606835983961355
[37m[1m[2023-06-25 06:22:37,096][129146] Mean Reward across all agents: 2900.9677380570843
[37m[1m[2023-06-25 06:22:37,096][129146] Average Trajectory Length: 980.0566666666666
[36m[2023-06-25 06:22:37,100][129146] mean_value=-409.65649775108, max_value=2694.6715818802586
[37m[1m[2023-06-25 06:22:37,103][129146] New mean coefficients: [[ 0.5819404   0.09553175  0.32157156 -0.06049466  0.3379237 ]]
[37m[1m[2023-06-25 06:22:37,104][129146] Moving the mean solution point...
[36m[2023-06-25 06:22:46,836][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 06:22:46,836][129146] FPS: 394642.36
[36m[2023-06-25 06:22:46,839][129146] itr=576, itrs=2000, Progress: 28.80%
[36m[2023-06-25 06:22:58,358][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 06:22:58,358][129146] FPS: 333912.63
[36m[2023-06-25 06:23:03,185][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:23:03,186][129146] Reward + Measures: [[4879.54504497    0.26714757    0.3350966     0.05639055    0.2507236 ]]
[37m[1m[2023-06-25 06:23:03,186][129146] Max Reward on eval: 4879.545044970011
[37m[1m[2023-06-25 06:23:03,186][129146] Min Reward on eval: 4879.545044970011
[37m[1m[2023-06-25 06:23:03,186][129146] Mean Reward across all agents: 4879.545044970011
[37m[1m[2023-06-25 06:23:03,186][129146] Average Trajectory Length: 997.6593333333333
[36m[2023-06-25 06:23:08,625][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:23:08,626][129146] Reward + Measures: [[ 403.77534272    0.4402        0.35320002    0.30990002    0.38350001]
[37m[1m [3575.05533494    0.23450001    0.34779999    0.0846        0.25149998]
[37m[1m [ 567.81555417    0.30232808    0.22704037    0.22383685    0.18887194]
[37m[1m ...
[37m[1m [ 554.21738605    0.40310001    0.32929999    0.2392        0.24080001]
[37m[1m [3472.18970749    0.26079997    0.35490003    0.0772        0.28480002]
[37m[1m [3276.08852997    0.28141606    0.34658596    0.06903823    0.27288148]]
[37m[1m[2023-06-25 06:23:08,626][129146] Max Reward on eval: 4859.102429301385
[37m[1m[2023-06-25 06:23:08,626][129146] Min Reward on eval: 170.06313987562316
[37m[1m[2023-06-25 06:23:08,627][129146] Mean Reward across all agents: 2932.643320633226
[37m[1m[2023-06-25 06:23:08,627][129146] Average Trajectory Length: 989.6443333333333
[36m[2023-06-25 06:23:08,630][129146] mean_value=-645.8686316144733, max_value=1839.2105532318874
[37m[1m[2023-06-25 06:23:08,633][129146] New mean coefficients: [[0.1559403  0.03060518 0.56730044 0.06129304 0.07745025]]
[37m[1m[2023-06-25 06:23:08,634][129146] Moving the mean solution point...
[36m[2023-06-25 06:23:18,478][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 06:23:18,478][129146] FPS: 390156.77
[36m[2023-06-25 06:23:18,480][129146] itr=577, itrs=2000, Progress: 28.85%
[36m[2023-06-25 06:23:30,144][129146] train() took 11.65 seconds to complete
[36m[2023-06-25 06:23:30,144][129146] FPS: 329750.27
[36m[2023-06-25 06:23:34,926][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:23:34,926][129146] Reward + Measures: [[3365.07101761    0.30174771    0.39898762    0.11918754    0.29669398]]
[37m[1m[2023-06-25 06:23:34,926][129146] Max Reward on eval: 3365.0710176054886
[37m[1m[2023-06-25 06:23:34,927][129146] Min Reward on eval: 3365.0710176054886
[37m[1m[2023-06-25 06:23:34,927][129146] Mean Reward across all agents: 3365.0710176054886
[36m[2023-06-25 06:23:40,554][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:23:40,555][129146] Reward + Measures: [[2326.10372619    0.35969999    0.36520001    0.21569999    0.29350004]
[37m[1m [3099.31150616    0.32260004    0.37780002    0.12100001    0.30669999]
[37m[1m [2819.51617499    0.31360003    0.40579996    0.1366        0.2572    ]
[37m[1m ...
[37m[1m [1661.18005102    0.39340001    0.36409998    0.23080002    0.2352    ]
[37m[1m [2768.09746242    0.34470001    0.36830002    0.18450001    0.28420001]
[37m[1m [ 690.97796054    0.49400002    0.35880002    0.23940001    0.2543    ]]
[37m[1m[2023-06-25 06:23:40,555][129146] Max Reward on eval: 3548.8832646338733
[37m[1m[2023-06-25 06:23:40,555][129146] Min Reward on eval: 21.955998309561984
[37m[1m[2023-06-25 06:23:40,556][129146] Mean Reward across all agents: 1923.8313237415523
[37m[1m[2023-06-25 06:23:40,556][129146] Average Trajectory Length: 985.7023333333333
[36m[2023-06-25 06:23:40,558][129146] mean_value=-840.8271431397684, max_value=778.9619665528066
[37m[1m[2023-06-25 06:23:40,561][129146] New mean coefficients: [[ 0.20060864 -0.01817615  0.73819965  0.10902952 -0.0106656 ]]
[37m[1m[2023-06-25 06:23:40,562][129146] Moving the mean solution point...
[36m[2023-06-25 06:23:50,247][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 06:23:50,247][129146] FPS: 396548.64
[36m[2023-06-25 06:23:50,249][129146] itr=578, itrs=2000, Progress: 28.90%
[36m[2023-06-25 06:24:01,818][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 06:24:01,818][129146] FPS: 332568.42
[36m[2023-06-25 06:24:06,684][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:24:06,685][129146] Reward + Measures: [[3513.31794513    0.30394906    0.40173876    0.11578733    0.29287148]]
[37m[1m[2023-06-25 06:24:06,685][129146] Max Reward on eval: 3513.317945130581
[37m[1m[2023-06-25 06:24:06,685][129146] Min Reward on eval: 3513.317945130581
[37m[1m[2023-06-25 06:24:06,685][129146] Mean Reward across all agents: 3513.317945130581
[37m[1m[2023-06-25 06:24:06,686][129146] Average Trajectory Length: 999.4456666666666
[36m[2023-06-25 06:24:12,178][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:24:12,179][129146] Reward + Measures: [[2422.80891807    0.23206186    0.33073151    0.11513094    0.2185757 ]
[37m[1m [2319.78621682    0.26009998    0.36270002    0.147         0.25580001]
[37m[1m [3544.41193841    0.30410001    0.38870001    0.1072        0.2868    ]
[37m[1m ...
[37m[1m [ 438.80043129    0.29710004    0.3759        0.1919        0.303     ]
[37m[1m [ 843.12634269    0.28579524    0.36102363    0.15047078    0.30723813]
[37m[1m [3028.23955236    0.27649999    0.3531        0.14669999    0.28870001]]
[37m[1m[2023-06-25 06:24:12,179][129146] Max Reward on eval: 3661.839421333757
[37m[1m[2023-06-25 06:24:12,180][129146] Min Reward on eval: 40.869728734414096
[37m[1m[2023-06-25 06:24:12,180][129146] Mean Reward across all agents: 1972.0984555306165
[37m[1m[2023-06-25 06:24:12,180][129146] Average Trajectory Length: 959.1586666666666
[36m[2023-06-25 06:24:12,182][129146] mean_value=-1572.8798731874265, max_value=920.2760258757538
[37m[1m[2023-06-25 06:24:12,184][129146] New mean coefficients: [[ 0.58826935 -0.01321433  0.46939835  0.13983575  0.06193077]]
[37m[1m[2023-06-25 06:24:12,185][129146] Moving the mean solution point...
[36m[2023-06-25 06:24:21,898][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 06:24:21,899][129146] FPS: 395417.82
[36m[2023-06-25 06:24:21,901][129146] itr=579, itrs=2000, Progress: 28.95%
[36m[2023-06-25 06:24:33,435][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 06:24:33,435][129146] FPS: 333474.50
[36m[2023-06-25 06:24:38,128][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:24:38,128][129146] Reward + Measures: [[3810.83916522    0.29396248    0.36585051    0.08483662    0.30077735]]
[37m[1m[2023-06-25 06:24:38,129][129146] Max Reward on eval: 3810.839165224558
[37m[1m[2023-06-25 06:24:38,129][129146] Min Reward on eval: 3810.839165224558
[37m[1m[2023-06-25 06:24:38,129][129146] Mean Reward across all agents: 3810.839165224558
[37m[1m[2023-06-25 06:24:38,130][129146] Average Trajectory Length: 997.3676666666667
[36m[2023-06-25 06:24:43,505][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:24:43,505][129146] Reward + Measures: [[2655.11438716    0.30249998    0.40599999    0.1786        0.27480003]
[37m[1m [3114.4053933     0.3123        0.36409998    0.117         0.33080003]
[37m[1m [3329.78583603    0.31819999    0.3319        0.09600001    0.28709999]
[37m[1m ...
[37m[1m [3288.28085606    0.30636859    0.32445025    0.07770596    0.29488549]
[37m[1m [3607.86534815    0.32170001    0.3179        0.05950001    0.28599998]
[37m[1m [3517.95787827    0.31560001    0.4021        0.11720001    0.32430002]]
[37m[1m[2023-06-25 06:24:43,505][129146] Max Reward on eval: 4153.20503793722
[37m[1m[2023-06-25 06:24:43,506][129146] Min Reward on eval: -144.19238352231915
[37m[1m[2023-06-25 06:24:43,506][129146] Mean Reward across all agents: 2621.4271593927097
[37m[1m[2023-06-25 06:24:43,506][129146] Average Trajectory Length: 994.973
[36m[2023-06-25 06:24:43,508][129146] mean_value=-1064.642570918543, max_value=332.4231184135076
[37m[1m[2023-06-25 06:24:43,510][129146] New mean coefficients: [[ 0.35258645 -0.20603879  0.28326562  0.23255743  0.02880954]]
[37m[1m[2023-06-25 06:24:43,511][129146] Moving the mean solution point...
[36m[2023-06-25 06:24:53,261][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 06:24:53,261][129146] FPS: 393912.07
[36m[2023-06-25 06:24:53,264][129146] itr=580, itrs=2000, Progress: 29.00%
[37m[1m[2023-06-25 06:24:58,399][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000560
[36m[2023-06-25 06:25:10,462][129146] train() took 11.77 seconds to complete
[36m[2023-06-25 06:25:10,462][129146] FPS: 326359.58
[36m[2023-06-25 06:25:15,210][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:25:15,210][129146] Reward + Measures: [[4109.84608799    0.28178573    0.32836807    0.05841043    0.29781938]]
[37m[1m[2023-06-25 06:25:15,210][129146] Max Reward on eval: 4109.846087987511
[37m[1m[2023-06-25 06:25:15,210][129146] Min Reward on eval: 4109.846087987511
[37m[1m[2023-06-25 06:25:15,211][129146] Mean Reward across all agents: 4109.846087987511
[37m[1m[2023-06-25 06:25:15,211][129146] Average Trajectory Length: 996.837
[36m[2023-06-25 06:25:20,710][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:25:20,711][129146] Reward + Measures: [[2713.82265861    0.36000001    0.4111        0.22090001    0.3565    ]
[37m[1m [1255.18613487    0.27419999    0.30380002    0.2304        0.28560004]
[37m[1m [  37.17669084    0.36659479    0.33521169    0.21747275    0.3238247 ]
[37m[1m ...
[37m[1m [3020.89928417    0.28920001    0.25479999    0.0519        0.2386    ]
[37m[1m [1316.86251503    0.31772539    0.36527619    0.11200636    0.3793619 ]
[37m[1m [1295.59332957    0.37020001    0.45469999    0.26589999    0.45819998]]
[37m[1m[2023-06-25 06:25:20,711][129146] Max Reward on eval: 4142.868472458003
[37m[1m[2023-06-25 06:25:20,711][129146] Min Reward on eval: 37.17669083637884
[37m[1m[2023-06-25 06:25:20,711][129146] Mean Reward across all agents: 2349.377774431774
[37m[1m[2023-06-25 06:25:20,712][129146] Average Trajectory Length: 982.35
[36m[2023-06-25 06:25:20,715][129146] mean_value=-959.3662310617741, max_value=2322.1965495478985
[37m[1m[2023-06-25 06:25:20,717][129146] New mean coefficients: [[ 0.78075475 -0.21705672  0.2517639   0.21219885  0.3710149 ]]
[37m[1m[2023-06-25 06:25:20,718][129146] Moving the mean solution point...
[36m[2023-06-25 06:25:30,636][129146] train() took 9.92 seconds to complete
[36m[2023-06-25 06:25:30,637][129146] FPS: 387247.38
[36m[2023-06-25 06:25:30,639][129146] itr=581, itrs=2000, Progress: 29.05%
[36m[2023-06-25 06:25:42,165][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 06:25:42,165][129146] FPS: 333811.71
[36m[2023-06-25 06:25:46,888][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:25:46,888][129146] Reward + Measures: [[4321.72865571    0.27141798    0.32067975    0.05433074    0.2981202 ]]
[37m[1m[2023-06-25 06:25:46,888][129146] Max Reward on eval: 4321.728655714273
[37m[1m[2023-06-25 06:25:46,888][129146] Min Reward on eval: 4321.728655714273
[37m[1m[2023-06-25 06:25:46,888][129146] Mean Reward across all agents: 4321.728655714273
[37m[1m[2023-06-25 06:25:46,889][129146] Average Trajectory Length: 996.4643333333333
[36m[2023-06-25 06:25:52,225][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:25:52,226][129146] Reward + Measures: [[1012.29314462    0.2636762     0.59634286    0.09828096    0.57151908]
[37m[1m [2778.82969056    0.3547        0.40950003    0.1207        0.27680001]
[37m[1m [3576.88171621    0.2784        0.40270001    0.0916        0.27140003]
[37m[1m ...
[37m[1m [2310.51978839    0.27576795    0.368568      0.15793011    0.21105924]
[37m[1m [1014.69478914    0.30070001    0.32170001    0.34209999    0.41750002]
[37m[1m [1228.94742445    0.44320002    0.46039996    0.0556        0.51920003]]
[37m[1m[2023-06-25 06:25:52,226][129146] Max Reward on eval: 4177.6105149537325
[37m[1m[2023-06-25 06:25:52,226][129146] Min Reward on eval: 190.42422080019023
[37m[1m[2023-06-25 06:25:52,227][129146] Mean Reward across all agents: 2590.3667412829304
[37m[1m[2023-06-25 06:25:52,227][129146] Average Trajectory Length: 993.822
[36m[2023-06-25 06:25:52,229][129146] mean_value=-856.944939567801, max_value=1397.5556301027887
[37m[1m[2023-06-25 06:25:52,232][129146] New mean coefficients: [[ 0.90194905 -0.2869053   0.10557431  0.23341961  0.26136923]]
[37m[1m[2023-06-25 06:25:52,233][129146] Moving the mean solution point...
[36m[2023-06-25 06:26:01,916][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 06:26:01,916][129146] FPS: 396638.27
[36m[2023-06-25 06:26:01,919][129146] itr=582, itrs=2000, Progress: 29.10%
[36m[2023-06-25 06:26:13,442][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 06:26:13,442][129146] FPS: 333867.85
[36m[2023-06-25 06:26:18,246][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:26:18,246][129146] Reward + Measures: [[4538.52699427    0.26836923    0.30365801    0.04531408    0.29379889]]
[37m[1m[2023-06-25 06:26:18,247][129146] Max Reward on eval: 4538.526994268842
[37m[1m[2023-06-25 06:26:18,247][129146] Min Reward on eval: 4538.526994268842
[37m[1m[2023-06-25 06:26:18,247][129146] Mean Reward across all agents: 4538.526994268842
[37m[1m[2023-06-25 06:26:18,247][129146] Average Trajectory Length: 995.538
[36m[2023-06-25 06:26:23,757][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:26:23,758][129146] Reward + Measures: [[1670.62819208    0.25650001    0.2931        0.18610001    0.24050002]
[37m[1m [2471.8468431     0.2868        0.3053        0.1393        0.39580002]
[37m[1m [3976.65679929    0.27000004    0.32440001    0.0741        0.27799997]
[37m[1m ...
[37m[1m [4005.40624867    0.27379999    0.3405        0.07560001    0.28150001]
[37m[1m [1375.34322613    0.46520001    0.51960003    0.3687        0.41669998]
[37m[1m [3562.82627707    0.28380001    0.30880004    0.077         0.31440002]]
[37m[1m[2023-06-25 06:26:23,758][129146] Max Reward on eval: 4606.616642298922
[37m[1m[2023-06-25 06:26:23,758][129146] Min Reward on eval: -57.26960931097274
[37m[1m[2023-06-25 06:26:23,759][129146] Mean Reward across all agents: 2332.8978896722633
[37m[1m[2023-06-25 06:26:23,759][129146] Average Trajectory Length: 990.3763333333333
[36m[2023-06-25 06:26:23,762][129146] mean_value=-837.1407816403453, max_value=888.7363624218394
[37m[1m[2023-06-25 06:26:23,764][129146] New mean coefficients: [[ 1.3270082  -0.17875376 -0.1600565  -0.00113773  0.5198041 ]]
[37m[1m[2023-06-25 06:26:23,765][129146] Moving the mean solution point...
[36m[2023-06-25 06:26:33,512][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 06:26:33,512][129146] FPS: 394038.15
[36m[2023-06-25 06:26:33,515][129146] itr=583, itrs=2000, Progress: 29.15%
[36m[2023-06-25 06:26:44,980][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 06:26:44,980][129146] FPS: 335582.71
[36m[2023-06-25 06:26:49,870][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:26:49,870][129146] Reward + Measures: [[4709.01768323    0.26494581    0.29291081    0.03753964    0.29021561]]
[37m[1m[2023-06-25 06:26:49,871][129146] Max Reward on eval: 4709.017683230151
[37m[1m[2023-06-25 06:26:49,871][129146] Min Reward on eval: 4709.017683230151
[37m[1m[2023-06-25 06:26:49,871][129146] Mean Reward across all agents: 4709.017683230151
[37m[1m[2023-06-25 06:26:49,871][129146] Average Trajectory Length: 996.948
[36m[2023-06-25 06:26:55,312][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:26:55,312][129146] Reward + Measures: [[2876.66483839    0.24602056    0.27052671    0.0908966     0.22303306]
[37m[1m [2605.52310723    0.3382        0.42230001    0.13079999    0.30810001]
[37m[1m [2667.41068942    0.3312        0.38350001    0.1472        0.30270001]
[37m[1m ...
[37m[1m [4248.19360043    0.25920001    0.36840004    0.0841        0.28730002]
[37m[1m [1832.58646031    0.36240003    0.3829        0.2141        0.33269998]
[37m[1m [3395.52239943    0.3515        0.37470001    0.09410001    0.33450001]]
[37m[1m[2023-06-25 06:26:55,313][129146] Max Reward on eval: 4478.1975545063615
[37m[1m[2023-06-25 06:26:55,313][129146] Min Reward on eval: 13.342911944864317
[37m[1m[2023-06-25 06:26:55,313][129146] Mean Reward across all agents: 2160.0102590707447
[37m[1m[2023-06-25 06:26:55,313][129146] Average Trajectory Length: 992.3976666666666
[36m[2023-06-25 06:26:55,316][129146] mean_value=-705.8216026977942, max_value=2363.5032668074477
[37m[1m[2023-06-25 06:26:55,319][129146] New mean coefficients: [[1.5738316  0.23568138 0.06440435 0.03760201 0.8170252 ]]
[37m[1m[2023-06-25 06:26:55,320][129146] Moving the mean solution point...
[36m[2023-06-25 06:27:04,977][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 06:27:04,977][129146] FPS: 397708.05
[36m[2023-06-25 06:27:04,980][129146] itr=584, itrs=2000, Progress: 29.20%
[36m[2023-06-25 06:27:16,563][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 06:27:16,563][129146] FPS: 332050.01
[36m[2023-06-25 06:27:21,392][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:27:21,392][129146] Reward + Measures: [[4863.30624072    0.26693034    0.28317612    0.03176362    0.28408188]]
[37m[1m[2023-06-25 06:27:21,392][129146] Max Reward on eval: 4863.306240723926
[37m[1m[2023-06-25 06:27:21,393][129146] Min Reward on eval: 4863.306240723926
[37m[1m[2023-06-25 06:27:21,393][129146] Mean Reward across all agents: 4863.306240723926
[37m[1m[2023-06-25 06:27:21,393][129146] Average Trajectory Length: 997.8513333333333
[36m[2023-06-25 06:27:27,090][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:27:27,091][129146] Reward + Measures: [[2125.79890749    0.28889999    0.40839997    0.2299        0.28940001]
[37m[1m [2684.13435321    0.3224        0.38940001    0.17320001    0.317     ]
[37m[1m [2174.06547254    0.3619        0.40219998    0.22579999    0.3021    ]
[37m[1m ...
[37m[1m [2510.8908996     0.32350001    0.45839998    0.2247        0.322     ]
[37m[1m [2030.7303911     0.31349999    0.44070002    0.2642        0.41890001]
[37m[1m [2360.3113381     0.29080001    0.37779999    0.20630001    0.31599998]]
[37m[1m[2023-06-25 06:27:27,091][129146] Max Reward on eval: 4794.311108660698
[37m[1m[2023-06-25 06:27:27,091][129146] Min Reward on eval: 280.20325970487903
[37m[1m[2023-06-25 06:27:27,092][129146] Mean Reward across all agents: 2201.249151292921
[37m[1m[2023-06-25 06:27:27,092][129146] Average Trajectory Length: 970.7496666666666
[36m[2023-06-25 06:27:27,095][129146] mean_value=-707.42011473032, max_value=1149.9773027561046
[37m[1m[2023-06-25 06:27:27,098][129146] New mean coefficients: [[ 1.9007175  -0.19457918 -0.17032747  0.01017441  0.7075637 ]]
[37m[1m[2023-06-25 06:27:27,099][129146] Moving the mean solution point...
[36m[2023-06-25 06:27:36,942][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 06:27:36,943][129146] FPS: 390163.07
[36m[2023-06-25 06:27:36,945][129146] itr=585, itrs=2000, Progress: 29.25%
[36m[2023-06-25 06:27:48,569][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 06:27:48,569][129146] FPS: 330937.44
[36m[2023-06-25 06:27:53,386][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:27:53,387][129146] Reward + Measures: [[4973.61088411    0.26682284    0.2792801     0.03103304    0.28221697]]
[37m[1m[2023-06-25 06:27:53,387][129146] Max Reward on eval: 4973.610884106062
[37m[1m[2023-06-25 06:27:53,387][129146] Min Reward on eval: 4973.610884106062
[37m[1m[2023-06-25 06:27:53,388][129146] Mean Reward across all agents: 4973.610884106062
[37m[1m[2023-06-25 06:27:53,388][129146] Average Trajectory Length: 998.9236666666666
[36m[2023-06-25 06:27:58,839][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:27:58,840][129146] Reward + Measures: [[1501.92137763    0.53009999    0.45639998    0.31440002    0.31670001]
[37m[1m [1059.70461509    0.47699997    0.57470006    0.3608        0.49640003]
[37m[1m [1588.58918577    0.46620002    0.48010001    0.2746        0.31510001]
[37m[1m ...
[37m[1m [2421.16424914    0.42900005    0.39340001    0.24150001    0.2823    ]
[37m[1m [3167.54740034    0.3565        0.4391        0.211         0.27620003]
[37m[1m [ 329.30671756    0.3942        0.51249999    0.31489998    0.41079998]]
[37m[1m[2023-06-25 06:27:58,840][129146] Max Reward on eval: 4855.382833576388
[37m[1m[2023-06-25 06:27:58,840][129146] Min Reward on eval: 127.09541451497935
[37m[1m[2023-06-25 06:27:58,840][129146] Mean Reward across all agents: 1661.4420871468174
[37m[1m[2023-06-25 06:27:58,841][129146] Average Trajectory Length: 993.472
[36m[2023-06-25 06:27:58,848][129146] mean_value=-69.30598396223927, max_value=1094.3123628703179
[37m[1m[2023-06-25 06:27:58,851][129146] New mean coefficients: [[ 2.4002128  -0.21097027 -0.21747267 -0.09102783  0.91913897]]
[37m[1m[2023-06-25 06:27:58,852][129146] Moving the mean solution point...
[36m[2023-06-25 06:28:08,752][129146] train() took 9.90 seconds to complete
[36m[2023-06-25 06:28:08,753][129146] FPS: 387913.70
[36m[2023-06-25 06:28:08,755][129146] itr=586, itrs=2000, Progress: 29.30%
[36m[2023-06-25 06:28:20,285][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 06:28:20,285][129146] FPS: 333703.73
[36m[2023-06-25 06:28:25,077][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:28:25,077][129146] Reward + Measures: [[5034.05020611    0.26460412    0.27688536    0.03236913    0.277733  ]]
[37m[1m[2023-06-25 06:28:25,077][129146] Max Reward on eval: 5034.050206108964
[37m[1m[2023-06-25 06:28:25,077][129146] Min Reward on eval: 5034.050206108964
[37m[1m[2023-06-25 06:28:25,078][129146] Mean Reward across all agents: 5034.050206108964
[37m[1m[2023-06-25 06:28:25,078][129146] Average Trajectory Length: 998.894
[36m[2023-06-25 06:28:30,619][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:28:30,625][129146] Reward + Measures: [[ 617.10985301    0.27180001    0.30490002    0.16779999    0.23240001]
[37m[1m [3228.68922564    0.32389998    0.33230001    0.1409        0.32179999]
[37m[1m [1191.89537246    0.26719999    0.27380002    0.1001        0.32880002]
[37m[1m ...
[37m[1m [3246.47797703    0.25410002    0.30090004    0.1195        0.34909999]
[37m[1m [1728.76190138    0.26470003    0.25250003    0.0916        0.28930002]
[37m[1m [2158.56020533    0.35010001    0.37670001    0.19140001    0.28649998]]
[37m[1m[2023-06-25 06:28:30,625][129146] Max Reward on eval: 4715.907577433158
[37m[1m[2023-06-25 06:28:30,626][129146] Min Reward on eval: -64.53432720597485
[37m[1m[2023-06-25 06:28:30,626][129146] Mean Reward across all agents: 2120.9094747507493
[37m[1m[2023-06-25 06:28:30,626][129146] Average Trajectory Length: 993.7573333333333
[36m[2023-06-25 06:28:30,629][129146] mean_value=-930.6181734687868, max_value=2904.751553272282
[37m[1m[2023-06-25 06:28:30,631][129146] New mean coefficients: [[ 2.267973    0.09283023  0.13644713 -0.00717971  1.0354859 ]]
[37m[1m[2023-06-25 06:28:30,632][129146] Moving the mean solution point...
[36m[2023-06-25 06:28:40,394][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 06:28:40,394][129146] FPS: 393448.63
[36m[2023-06-25 06:28:40,397][129146] itr=587, itrs=2000, Progress: 29.35%
[36m[2023-06-25 06:28:51,940][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 06:28:51,940][129146] FPS: 333303.80
[36m[2023-06-25 06:28:56,773][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:28:56,773][129146] Reward + Measures: [[5209.26033062    0.25962132    0.28396559    0.03273109    0.27597255]]
[37m[1m[2023-06-25 06:28:56,773][129146] Max Reward on eval: 5209.260330619706
[37m[1m[2023-06-25 06:28:56,773][129146] Min Reward on eval: 5209.260330619706
[37m[1m[2023-06-25 06:28:56,774][129146] Mean Reward across all agents: 5209.260330619706
[37m[1m[2023-06-25 06:28:56,774][129146] Average Trajectory Length: 999.2003333333333
[36m[2023-06-25 06:29:02,051][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:29:02,052][129146] Reward + Measures: [[2383.06842245    0.29909998    0.33549997    0.13420001    0.3066    ]
[37m[1m [ 260.32259006    0.35221669    0.273875      0.30715418    0.28329167]
[37m[1m [4231.3925691     0.27700001    0.36840001    0.10399999    0.27649999]
[37m[1m ...
[37m[1m [ 550.16383194    0.37180001    0.36929998    0.3274        0.35570002]
[37m[1m [1766.1432585     0.36539999    0.29530001    0.22930001    0.37020001]
[37m[1m [ 144.84027586    0.32721624    0.31718561    0.30162463    0.30684191]]
[37m[1m[2023-06-25 06:29:02,052][129146] Max Reward on eval: 4781.189516233955
[37m[1m[2023-06-25 06:29:02,052][129146] Min Reward on eval: -36.09080726858811
[37m[1m[2023-06-25 06:29:02,052][129146] Mean Reward across all agents: 1861.5471073178373
[37m[1m[2023-06-25 06:29:02,053][129146] Average Trajectory Length: 986.569
[36m[2023-06-25 06:29:02,056][129146] mean_value=-892.1163965995922, max_value=2266.9175838612787
[37m[1m[2023-06-25 06:29:02,058][129146] New mean coefficients: [[2.0960636  0.14791057 0.16580306 0.05659719 0.8780603 ]]
[37m[1m[2023-06-25 06:29:02,059][129146] Moving the mean solution point...
[36m[2023-06-25 06:29:11,847][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 06:29:11,847][129146] FPS: 392397.58
[36m[2023-06-25 06:29:11,849][129146] itr=588, itrs=2000, Progress: 29.40%
[36m[2023-06-25 06:29:23,471][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 06:29:23,471][129146] FPS: 330966.74
[36m[2023-06-25 06:29:28,343][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:29:28,344][129146] Reward + Measures: [[5329.35944356    0.26038557    0.28836975    0.02848078    0.27692124]]
[37m[1m[2023-06-25 06:29:28,344][129146] Max Reward on eval: 5329.35944355539
[37m[1m[2023-06-25 06:29:28,344][129146] Min Reward on eval: 5329.35944355539
[37m[1m[2023-06-25 06:29:28,345][129146] Mean Reward across all agents: 5329.35944355539
[37m[1m[2023-06-25 06:29:28,345][129146] Average Trajectory Length: 999.2816666666666
[36m[2023-06-25 06:29:33,866][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:29:33,867][129146] Reward + Measures: [[1305.73238883    0.44630003    0.3087        0.20639999    0.30500001]
[37m[1m [ 878.51856039    0.57480001    0.29190001    0.39500001    0.25330001]
[37m[1m [3063.12314416    0.30461946    0.30627987    0.07762357    0.25905722]
[37m[1m ...
[37m[1m [ 811.3974446     0.38240001    0.3565        0.22749999    0.27739999]
[37m[1m [2521.51064088    0.26320001    0.36059999    0.1515        0.38510001]
[37m[1m [4422.08106695    0.30760002    0.33460003    0.1207        0.29840001]]
[37m[1m[2023-06-25 06:29:33,867][129146] Max Reward on eval: 5103.993826356903
[37m[1m[2023-06-25 06:29:33,867][129146] Min Reward on eval: -100.81148269524564
[37m[1m[2023-06-25 06:29:33,868][129146] Mean Reward across all agents: 2106.292091969886
[37m[1m[2023-06-25 06:29:33,868][129146] Average Trajectory Length: 987.5949999999999
[36m[2023-06-25 06:29:33,871][129146] mean_value=-686.8604831410291, max_value=1305.342608500438
[37m[1m[2023-06-25 06:29:33,874][129146] New mean coefficients: [[ 2.1147587   0.05053312 -0.14722319 -0.05800928  0.782246  ]]
[37m[1m[2023-06-25 06:29:33,875][129146] Moving the mean solution point...
[36m[2023-06-25 06:29:43,592][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 06:29:43,592][129146] FPS: 395262.55
[36m[2023-06-25 06:29:43,595][129146] itr=589, itrs=2000, Progress: 29.45%
[36m[2023-06-25 06:29:55,046][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 06:29:55,046][129146] FPS: 335994.28
[36m[2023-06-25 06:29:59,772][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:29:59,773][129146] Reward + Measures: [[5362.98556969    0.2564806     0.29629037    0.03246842    0.27952334]]
[37m[1m[2023-06-25 06:29:59,773][129146] Max Reward on eval: 5362.985569692738
[37m[1m[2023-06-25 06:29:59,773][129146] Min Reward on eval: 5362.985569692738
[37m[1m[2023-06-25 06:29:59,774][129146] Mean Reward across all agents: 5362.985569692738
[37m[1m[2023-06-25 06:29:59,774][129146] Average Trajectory Length: 998.9616666666666
[36m[2023-06-25 06:30:05,309][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:30:05,310][129146] Reward + Measures: [[1828.19969659    0.25546095    0.30955675    0.14089254    0.21010442]
[37m[1m [3207.86098895    0.29401559    0.33076626    0.120969      0.32942185]
[37m[1m [ 634.39900599    0.41869998    0.49180004    0.35039997    0.47510001]
[37m[1m ...
[37m[1m [3813.72320853    0.26760003    0.3364        0.1128        0.34180003]
[37m[1m [1199.7034001     0.28588614    0.23990726    0.16644083    0.21258512]
[37m[1m [ 788.82776402    0.34980002    0.34170002    0.25619999    0.25299999]]
[37m[1m[2023-06-25 06:30:05,310][129146] Max Reward on eval: 4812.807616860047
[37m[1m[2023-06-25 06:30:05,310][129146] Min Reward on eval: 208.50298441539636
[37m[1m[2023-06-25 06:30:05,310][129146] Mean Reward across all agents: 2186.5065183668066
[37m[1m[2023-06-25 06:30:05,311][129146] Average Trajectory Length: 954.0363333333333
[36m[2023-06-25 06:30:05,314][129146] mean_value=-862.5028256355191, max_value=1806.6901863665746
[37m[1m[2023-06-25 06:30:05,316][129146] New mean coefficients: [[1.6364281  0.4666362  0.4206447  0.08545436 0.82275844]]
[37m[1m[2023-06-25 06:30:05,317][129146] Moving the mean solution point...
[36m[2023-06-25 06:30:15,082][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 06:30:15,082][129146] FPS: 393313.91
[36m[2023-06-25 06:30:15,085][129146] itr=590, itrs=2000, Progress: 29.50%
[37m[1m[2023-06-25 06:30:20,137][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000570
[36m[2023-06-25 06:30:32,100][129146] train() took 11.67 seconds to complete
[36m[2023-06-25 06:30:32,101][129146] FPS: 329181.59
[36m[2023-06-25 06:30:36,736][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:30:36,736][129146] Reward + Measures: [[5459.33113799    0.25421104    0.30423573    0.03406507    0.28201219]]
[37m[1m[2023-06-25 06:30:36,737][129146] Max Reward on eval: 5459.331137988063
[37m[1m[2023-06-25 06:30:36,737][129146] Min Reward on eval: 5459.331137988063
[37m[1m[2023-06-25 06:30:36,737][129146] Mean Reward across all agents: 5459.331137988063
[37m[1m[2023-06-25 06:30:36,737][129146] Average Trajectory Length: 999.4496666666666
[36m[2023-06-25 06:30:42,077][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:30:42,078][129146] Reward + Measures: [[3370.81588277    0.30149999    0.36470002    0.10749999    0.25470003]
[37m[1m [2643.42285957    0.31570002    0.33240002    0.10110001    0.23740001]
[37m[1m [2491.56559062    0.29270002    0.3105        0.047         0.3118    ]
[37m[1m ...
[37m[1m [1462.46901216    0.3996        0.44909999    0.0404        0.4657    ]
[37m[1m [2505.89476953    0.28930002    0.36619997    0.25350001    0.33049998]
[37m[1m [ 262.96540822    0.3271755     0.26457524    0.27542827    0.30395493]]
[37m[1m[2023-06-25 06:30:42,078][129146] Max Reward on eval: 5290.034778149426
[37m[1m[2023-06-25 06:30:42,078][129146] Min Reward on eval: -90.37499706331873
[37m[1m[2023-06-25 06:30:42,079][129146] Mean Reward across all agents: 1969.6708433653232
[37m[1m[2023-06-25 06:30:42,079][129146] Average Trajectory Length: 985.1566666666666
[36m[2023-06-25 06:30:42,085][129146] mean_value=-132.67657843729597, max_value=4747.90986127271
[37m[1m[2023-06-25 06:30:42,088][129146] New mean coefficients: [[1.5082815  0.6387469  0.0678741  0.17367905 0.8405157 ]]
[37m[1m[2023-06-25 06:30:42,089][129146] Moving the mean solution point...
[36m[2023-06-25 06:30:51,809][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 06:30:51,810][129146] FPS: 395103.80
[36m[2023-06-25 06:30:51,812][129146] itr=591, itrs=2000, Progress: 29.55%
[36m[2023-06-25 06:31:03,212][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 06:31:03,212][129146] FPS: 337396.58
[36m[2023-06-25 06:31:07,962][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:31:07,962][129146] Reward + Measures: [[5615.10403814    0.26422569    0.29213554    0.02717109    0.27242512]]
[37m[1m[2023-06-25 06:31:07,962][129146] Max Reward on eval: 5615.104038144775
[37m[1m[2023-06-25 06:31:07,963][129146] Min Reward on eval: 5615.104038144775
[37m[1m[2023-06-25 06:31:07,963][129146] Mean Reward across all agents: 5615.104038144775
[37m[1m[2023-06-25 06:31:07,963][129146] Average Trajectory Length: 999.5356666666667
[36m[2023-06-25 06:31:13,393][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:31:13,393][129146] Reward + Measures: [[ 262.91765643    0.3197        0.63320005    0.55259997    0.6936    ]
[37m[1m [ 441.45323421    0.27360001    0.66090006    0.43400002    0.64749998]
[37m[1m [1375.35778403    0.24749999    0.3096        0.1754        0.32470003]
[37m[1m ...
[37m[1m [1947.28273222    0.32820001    0.35880002    0.24589999    0.45120001]
[37m[1m [2539.70001115    0.28740001    0.34750003    0.2106        0.39269999]
[37m[1m [2193.39489527    0.28450003    0.29500005    0.1689        0.27000001]]
[37m[1m[2023-06-25 06:31:13,393][129146] Max Reward on eval: 5407.075054158177
[37m[1m[2023-06-25 06:31:13,394][129146] Min Reward on eval: -74.03576008791569
[37m[1m[2023-06-25 06:31:13,394][129146] Mean Reward across all agents: 2160.7021242406554
[37m[1m[2023-06-25 06:31:13,394][129146] Average Trajectory Length: 988.644
[36m[2023-06-25 06:31:13,400][129146] mean_value=-316.0305195757999, max_value=1681.0097981341733
[37m[1m[2023-06-25 06:31:13,403][129146] New mean coefficients: [[1.5320603  0.62688386 0.1156656  0.15860325 1.2167406 ]]
[37m[1m[2023-06-25 06:31:13,404][129146] Moving the mean solution point...
[36m[2023-06-25 06:31:23,049][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 06:31:23,050][129146] FPS: 398176.42
[36m[2023-06-25 06:31:23,052][129146] itr=592, itrs=2000, Progress: 29.60%
[36m[2023-06-25 06:31:34,617][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 06:31:34,617][129146] FPS: 332600.80
[36m[2023-06-25 06:31:39,315][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:31:39,315][129146] Reward + Measures: [[5705.52033673    0.26211801    0.29824242    0.02799518    0.27080941]]
[37m[1m[2023-06-25 06:31:39,315][129146] Max Reward on eval: 5705.520336726069
[37m[1m[2023-06-25 06:31:39,316][129146] Min Reward on eval: 5705.520336726069
[37m[1m[2023-06-25 06:31:39,316][129146] Mean Reward across all agents: 5705.520336726069
[37m[1m[2023-06-25 06:31:39,316][129146] Average Trajectory Length: 999.1326666666666
[36m[2023-06-25 06:31:44,807][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:31:44,807][129146] Reward + Measures: [[ 480.61158331    0.5826        0.41260001    0.4052        0.41009998]
[37m[1m [2564.53438552    0.38330004    0.45769998    0.26859999    0.38779998]
[37m[1m [1692.34805776    0.56960005    0.46229997    0.29530001    0.38249999]
[37m[1m ...
[37m[1m [1313.18681756    0.48610002    0.4756        0.28580001    0.39660001]
[37m[1m [1417.97640053    0.52500004    0.36379999    0.22000001    0.25619999]
[37m[1m [1097.26103823    0.47270003    0.36670002    0.21619999    0.27110001]]
[37m[1m[2023-06-25 06:31:44,808][129146] Max Reward on eval: 5105.772676120978
[37m[1m[2023-06-25 06:31:44,808][129146] Min Reward on eval: 151.29974589899066
[37m[1m[2023-06-25 06:31:44,808][129146] Mean Reward across all agents: 1731.8281372881165
[37m[1m[2023-06-25 06:31:44,808][129146] Average Trajectory Length: 997.6253333333333
[36m[2023-06-25 06:31:44,813][129146] mean_value=-183.55704798633874, max_value=1243.7288976516627
[37m[1m[2023-06-25 06:31:44,816][129146] New mean coefficients: [[1.7694631  0.9377749  0.1594063  0.01568459 1.42527   ]]
[37m[1m[2023-06-25 06:31:44,817][129146] Moving the mean solution point...
[36m[2023-06-25 06:31:54,444][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 06:31:54,444][129146] FPS: 398959.57
[36m[2023-06-25 06:31:54,446][129146] itr=593, itrs=2000, Progress: 29.65%
[36m[2023-06-25 06:32:06,028][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 06:32:06,028][129146] FPS: 332184.30
[36m[2023-06-25 06:32:10,963][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:32:10,963][129146] Reward + Measures: [[5739.74148051    0.26161554    0.29330346    0.02716292    0.27193928]]
[37m[1m[2023-06-25 06:32:10,964][129146] Max Reward on eval: 5739.741480510626
[37m[1m[2023-06-25 06:32:10,964][129146] Min Reward on eval: 5739.741480510626
[37m[1m[2023-06-25 06:32:10,964][129146] Mean Reward across all agents: 5739.741480510626
[37m[1m[2023-06-25 06:32:10,964][129146] Average Trajectory Length: 999.3833333333333
[36m[2023-06-25 06:32:16,479][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:32:16,480][129146] Reward + Measures: [[ 260.17493665    0.24323192    0.20488448    0.17733315    0.2354991 ]
[37m[1m [1805.19952706    0.28644758    0.27019024    0.20822321    0.34815636]
[37m[1m [ 699.83187564    0.26210001    0.23910001    0.24870001    0.30610001]
[37m[1m ...
[37m[1m [ 885.14091655    0.29049999    0.43319997    0.27510002    0.3867    ]
[37m[1m [ 298.7086536     0.32969999    0.23720002    0.25359997    0.27219999]
[37m[1m [ 462.87809666    0.36080003    0.2545        0.30170003    0.28119999]]
[37m[1m[2023-06-25 06:32:16,480][129146] Max Reward on eval: 5516.493004132504
[37m[1m[2023-06-25 06:32:16,480][129146] Min Reward on eval: -35.1996578093851
[37m[1m[2023-06-25 06:32:16,480][129146] Mean Reward across all agents: 1110.1195122807385
[37m[1m[2023-06-25 06:32:16,481][129146] Average Trajectory Length: 945.746
[36m[2023-06-25 06:32:16,484][129146] mean_value=-648.0870618314528, max_value=1230.3750959122683
[37m[1m[2023-06-25 06:32:16,487][129146] New mean coefficients: [[1.4503438  0.9092039  0.37661764 0.16210584 1.3398124 ]]
[37m[1m[2023-06-25 06:32:16,488][129146] Moving the mean solution point...
[36m[2023-06-25 06:32:26,398][129146] train() took 9.91 seconds to complete
[36m[2023-06-25 06:32:26,398][129146] FPS: 387558.52
[36m[2023-06-25 06:32:26,401][129146] itr=594, itrs=2000, Progress: 29.70%
[36m[2023-06-25 06:32:37,965][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 06:32:37,965][129146] FPS: 332700.82
[36m[2023-06-25 06:32:42,783][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:32:42,802][129146] Reward + Measures: [[1948.732989      0.34105724    0.5158605     0.18274695    0.30613732]]
[37m[1m[2023-06-25 06:32:42,803][129146] Max Reward on eval: 1948.7329889955254
[37m[1m[2023-06-25 06:32:42,804][129146] Min Reward on eval: 1948.7329889955254
[37m[1m[2023-06-25 06:32:42,805][129146] Mean Reward across all agents: 1948.7329889955254
[37m[1m[2023-06-25 06:32:42,806][129146] Average Trajectory Length: 998.449
[36m[2023-06-25 06:32:48,355][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:32:48,356][129146] Reward + Measures: [[ 288.18540126    0.36893043    0.36802939    0.27509582    0.31805891]
[37m[1m [2858.94372789    0.33479998    0.4594        0.11259999    0.23710001]
[37m[1m [ 339.4509377     0.44440126    0.4366487     0.39737222    0.43781963]
[37m[1m ...
[37m[1m [ 367.19259391    0.39640003    0.41580001    0.19          0.55919999]
[37m[1m [ 112.61766084    0.33950001    0.33980003    0.29960003    0.3908    ]
[37m[1m [ 335.00093909    0.39264485    0.3526724     0.28234828    0.35455176]]
[37m[1m[2023-06-25 06:32:48,356][129146] Max Reward on eval: 2858.9437278947967
[37m[1m[2023-06-25 06:32:48,356][129146] Min Reward on eval: -494.5895184602123
[37m[1m[2023-06-25 06:32:48,357][129146] Mean Reward across all agents: 437.62710171227207
[37m[1m[2023-06-25 06:32:48,357][129146] Average Trajectory Length: 950.0436666666666
[36m[2023-06-25 06:32:48,358][129146] mean_value=-1526.64310384676, max_value=382.98139715360503
[37m[1m[2023-06-25 06:32:48,361][129146] New mean coefficients: [[1.2812872  0.5768185  0.27913418 0.16147496 0.817565  ]]
[37m[1m[2023-06-25 06:32:48,362][129146] Moving the mean solution point...
[36m[2023-06-25 06:32:58,229][129146] train() took 9.87 seconds to complete
[36m[2023-06-25 06:32:58,229][129146] FPS: 389252.76
[36m[2023-06-25 06:32:58,232][129146] itr=595, itrs=2000, Progress: 29.75%
[36m[2023-06-25 06:33:09,788][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 06:33:09,788][129146] FPS: 332937.13
[36m[2023-06-25 06:33:14,592][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:33:14,592][129146] Reward + Measures: [[600.22461154   0.40877521   0.57498348   0.29123893   0.61336243]]
[37m[1m[2023-06-25 06:33:14,592][129146] Max Reward on eval: 600.2246115424748
[37m[1m[2023-06-25 06:33:14,592][129146] Min Reward on eval: 600.2246115424748
[37m[1m[2023-06-25 06:33:14,593][129146] Mean Reward across all agents: 600.2246115424748
[37m[1m[2023-06-25 06:33:14,593][129146] Average Trajectory Length: 998.2653333333333
[36m[2023-06-25 06:33:20,149][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:33:20,150][129146] Reward + Measures: [[  41.54551137    0.3048        0.47600004    0.31140003    0.60190004]
[37m[1m [ 727.2208583     0.45829996    0.3547        0.3136        0.35800004]
[37m[1m [-110.61911861    0.27810001    0.38890001    0.23239999    0.43899995]
[37m[1m ...
[37m[1m [ 560.38341116    0.38750002    0.44159999    0.40009999    0.51289999]
[37m[1m [  78.57663814    0.31289998    0.456         0.31750003    0.53369999]
[37m[1m [ 594.33438006    0.40400001    0.47240001    0.34110001    0.47979999]]
[37m[1m[2023-06-25 06:33:20,150][129146] Max Reward on eval: 1396.6728828041
[37m[1m[2023-06-25 06:33:20,150][129146] Min Reward on eval: -752.9667237358459
[37m[1m[2023-06-25 06:33:20,150][129146] Mean Reward across all agents: 282.1368725694303
[37m[1m[2023-06-25 06:33:20,151][129146] Average Trajectory Length: 996.136
[36m[2023-06-25 06:33:20,155][129146] mean_value=-571.7326376872602, max_value=1062.1386656030547
[37m[1m[2023-06-25 06:33:20,158][129146] New mean coefficients: [[1.5218824  0.15960708 0.46775562 0.40109205 0.6096359 ]]
[37m[1m[2023-06-25 06:33:20,159][129146] Moving the mean solution point...
[36m[2023-06-25 06:33:29,958][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 06:33:29,959][129146] FPS: 391932.75
[36m[2023-06-25 06:33:29,961][129146] itr=596, itrs=2000, Progress: 29.80%
[36m[2023-06-25 06:33:41,375][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 06:33:41,375][129146] FPS: 336966.11
[36m[2023-06-25 06:33:46,153][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:33:46,153][129146] Reward + Measures: [[945.74183252   0.3712509    0.58904433   0.24717765   0.58451521]]
[37m[1m[2023-06-25 06:33:46,154][129146] Max Reward on eval: 945.7418325247622
[37m[1m[2023-06-25 06:33:46,154][129146] Min Reward on eval: 945.7418325247622
[37m[1m[2023-06-25 06:33:46,154][129146] Mean Reward across all agents: 945.7418325247622
[37m[1m[2023-06-25 06:33:46,154][129146] Average Trajectory Length: 999.8646666666666
[36m[2023-06-25 06:33:51,471][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:33:51,471][129146] Reward + Measures: [[1365.74890946    0.37780002    0.56340003    0.3195        0.39520001]
[37m[1m [ 487.4254911     0.42160001    0.50050002    0.4463        0.4901    ]
[37m[1m [1338.1573306     0.4224        0.53610003    0.28529999    0.35609999]
[37m[1m ...
[37m[1m [1108.64915019    0.45099998    0.56000006    0.30360004    0.4858    ]
[37m[1m [ 862.57402416    0.4824        0.48090002    0.40000001    0.40580001]
[37m[1m [1485.34052207    0.32339999    0.56240004    0.26719999    0.375     ]]
[37m[1m[2023-06-25 06:33:51,471][129146] Max Reward on eval: 1826.3742486349656
[37m[1m[2023-06-25 06:33:51,472][129146] Min Reward on eval: 333.9537632132415
[37m[1m[2023-06-25 06:33:51,472][129146] Mean Reward across all agents: 918.4650745006484
[37m[1m[2023-06-25 06:33:51,472][129146] Average Trajectory Length: 998.689
[36m[2023-06-25 06:33:51,478][129146] mean_value=-61.89433849790119, max_value=1069.720670405589
[37m[1m[2023-06-25 06:33:51,480][129146] New mean coefficients: [[2.2570987  0.14073859 0.39786386 0.39625227 0.70414174]]
[37m[1m[2023-06-25 06:33:51,482][129146] Moving the mean solution point...
[36m[2023-06-25 06:34:01,258][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 06:34:01,259][129146] FPS: 392837.81
[36m[2023-06-25 06:34:01,261][129146] itr=597, itrs=2000, Progress: 29.85%
[36m[2023-06-25 06:34:12,656][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 06:34:12,656][129146] FPS: 337543.51
[36m[2023-06-25 06:34:17,320][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:34:17,320][129146] Reward + Measures: [[1288.01777843    0.36207998    0.51107627    0.21218176    0.47679639]]
[37m[1m[2023-06-25 06:34:17,320][129146] Max Reward on eval: 1288.0177784283323
[37m[1m[2023-06-25 06:34:17,320][129146] Min Reward on eval: 1288.0177784283323
[37m[1m[2023-06-25 06:34:17,321][129146] Mean Reward across all agents: 1288.0177784283323
[37m[1m[2023-06-25 06:34:17,321][129146] Average Trajectory Length: 999.7006666666666
[36m[2023-06-25 06:34:22,822][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:34:22,823][129146] Reward + Measures: [[ 501.32411119    0.40910003    0.37620002    0.0947        0.46620002]
[37m[1m [ 541.14157673    0.33049998    0.6455        0.2103        0.74540007]
[37m[1m [2252.87971206    0.3576        0.27060002    0.07919999    0.25549999]
[37m[1m ...
[37m[1m [ 713.91604729    0.27220002    0.74190009    0.18520001    0.73660004]
[37m[1m [ 352.92867586    0.39878747    0.21268594    0.24452971    0.30000779]
[37m[1m [ 650.49568376    0.41939998    0.53060001    0.33989999    0.57059997]]
[37m[1m[2023-06-25 06:34:22,823][129146] Max Reward on eval: 2434.7583881123223
[37m[1m[2023-06-25 06:34:22,823][129146] Min Reward on eval: -121.98804509592591
[37m[1m[2023-06-25 06:34:22,823][129146] Mean Reward across all agents: 1136.3878012050452
[37m[1m[2023-06-25 06:34:22,824][129146] Average Trajectory Length: 978.8593333333333
[36m[2023-06-25 06:34:22,826][129146] mean_value=-1110.074315809054, max_value=520.7910273316313
[37m[1m[2023-06-25 06:34:22,829][129146] New mean coefficients: [[ 1.9790723  -0.3674059   0.03862104  0.20222695  0.5367696 ]]
[37m[1m[2023-06-25 06:34:22,830][129146] Moving the mean solution point...
[36m[2023-06-25 06:34:32,506][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 06:34:32,507][129146] FPS: 396905.93
[36m[2023-06-25 06:34:32,509][129146] itr=598, itrs=2000, Progress: 29.90%
[36m[2023-06-25 06:34:43,933][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 06:34:43,933][129146] FPS: 336667.45
[36m[2023-06-25 06:34:48,715][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:34:48,721][129146] Reward + Measures: [[1614.35983691    0.34715042    0.44764596    0.18157893    0.39517793]]
[37m[1m[2023-06-25 06:34:48,721][129146] Max Reward on eval: 1614.3598369123995
[37m[1m[2023-06-25 06:34:48,721][129146] Min Reward on eval: 1614.3598369123995
[37m[1m[2023-06-25 06:34:48,722][129146] Mean Reward across all agents: 1614.3598369123995
[37m[1m[2023-06-25 06:34:48,722][129146] Average Trajectory Length: 999.1136666666666
[36m[2023-06-25 06:34:54,168][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:34:54,168][129146] Reward + Measures: [[1082.0543121     0.52700001    0.50830001    0.30590001    0.52410001]
[37m[1m [1198.97453172    0.37820002    0.51859999    0.2086        0.57240003]
[37m[1m [ 693.76861921    0.2053        0.75319999    0.25190002    0.74879998]
[37m[1m ...
[37m[1m [1431.45692594    0.34820005    0.44899997    0.1751        0.43189999]
[37m[1m [1123.43510253    0.4068        0.4664        0.18520001    0.48890001]
[37m[1m [1454.75297373    0.33759999    0.43759999    0.1779        0.38299999]]
[37m[1m[2023-06-25 06:34:54,169][129146] Max Reward on eval: 2088.4809014476255
[37m[1m[2023-06-25 06:34:54,169][129146] Min Reward on eval: -440.7340147193987
[37m[1m[2023-06-25 06:34:54,169][129146] Mean Reward across all agents: 946.9287830280944
[37m[1m[2023-06-25 06:34:54,169][129146] Average Trajectory Length: 999.0243333333333
[36m[2023-06-25 06:34:54,176][129146] mean_value=-61.527141116492764, max_value=1318.3571004476748
[37m[1m[2023-06-25 06:34:54,179][129146] New mean coefficients: [[ 2.2097564  -0.92664635  0.38352326  0.41173178  0.22779861]]
[37m[1m[2023-06-25 06:34:54,180][129146] Moving the mean solution point...
[36m[2023-06-25 06:35:03,912][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 06:35:03,913][129146] FPS: 394645.59
[36m[2023-06-25 06:35:03,915][129146] itr=599, itrs=2000, Progress: 29.95%
[36m[2023-06-25 06:35:15,333][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 06:35:15,333][129146] FPS: 336889.16
[36m[2023-06-25 06:35:20,220][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:35:20,220][129146] Reward + Measures: [[1846.94069563    0.33743539    0.40320241    0.16400281    0.33982909]]
[37m[1m[2023-06-25 06:35:20,220][129146] Max Reward on eval: 1846.9406956303671
[37m[1m[2023-06-25 06:35:20,220][129146] Min Reward on eval: 1846.9406956303671
[37m[1m[2023-06-25 06:35:20,221][129146] Mean Reward across all agents: 1846.9406956303671
[37m[1m[2023-06-25 06:35:20,221][129146] Average Trajectory Length: 999.1113333333333
[36m[2023-06-25 06:35:25,837][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:35:25,837][129146] Reward + Measures: [[1750.24338966    0.2753        0.37010002    0.19760001    0.29819998]
[37m[1m [ 427.46458502    0.43450004    0.3567        0.331         0.3766    ]
[37m[1m [1446.3835395     0.38270003    0.46070001    0.22620001    0.3506    ]
[37m[1m ...
[37m[1m [ 243.19335795    0.5327        0.54370004    0.53380007    0.59490007]
[37m[1m [ 364.9369386     0.4903        0.35450003    0.30829999    0.30319998]
[37m[1m [ 217.54977127    0.60770005    0.59619999    0.61400002    0.61489993]]
[37m[1m[2023-06-25 06:35:25,837][129146] Max Reward on eval: 2226.876280654338
[37m[1m[2023-06-25 06:35:25,838][129146] Min Reward on eval: 168.79488556119614
[37m[1m[2023-06-25 06:35:25,838][129146] Mean Reward across all agents: 820.7968489035428
[37m[1m[2023-06-25 06:35:25,838][129146] Average Trajectory Length: 997.486
[36m[2023-06-25 06:35:25,841][129146] mean_value=-925.4257777998068, max_value=1826.8048053360103
[37m[1m[2023-06-25 06:35:25,843][129146] New mean coefficients: [[ 2.6886773  -1.0952388   0.5074611   0.47994822  0.45540577]]
[37m[1m[2023-06-25 06:35:25,845][129146] Moving the mean solution point...
[36m[2023-06-25 06:35:35,583][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 06:35:35,584][129146] FPS: 394372.61
[36m[2023-06-25 06:35:35,586][129146] itr=600, itrs=2000, Progress: 30.00%
[37m[1m[2023-06-25 06:35:40,393][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000580
[36m[2023-06-25 06:35:52,121][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 06:35:52,121][129146] FPS: 335718.94
[36m[2023-06-25 06:35:56,881][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:35:56,882][129146] Reward + Measures: [[2086.17331504    0.3296411     0.37397379    0.15099336    0.31248751]]
[37m[1m[2023-06-25 06:35:56,882][129146] Max Reward on eval: 2086.1733150371615
[37m[1m[2023-06-25 06:35:56,882][129146] Min Reward on eval: 2086.1733150371615
[37m[1m[2023-06-25 06:35:56,882][129146] Mean Reward across all agents: 2086.1733150371615
[37m[1m[2023-06-25 06:35:56,883][129146] Average Trajectory Length: 998.7043333333334
[36m[2023-06-25 06:36:02,471][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:36:02,472][129146] Reward + Measures: [[314.39617701   0.29755783   0.39917585   0.34009051   0.40711042]
[37m[1m [214.41580203   0.37439999   0.53400004   0.3924       0.51130003]
[37m[1m [435.34267801   0.3265       0.47100002   0.3238       0.42899999]
[37m[1m ...
[37m[1m [449.00685646   0.3096       0.38569999   0.35069999   0.38540003]
[37m[1m [479.31851456   0.36241394   0.38531628   0.36696744   0.41892791]
[37m[1m [433.27514522   0.29920003   0.44260001   0.36269999   0.46469998]]
[37m[1m[2023-06-25 06:36:02,472][129146] Max Reward on eval: 2346.295524028386
[37m[1m[2023-06-25 06:36:02,472][129146] Min Reward on eval: -40.7440598984831
[37m[1m[2023-06-25 06:36:02,472][129146] Mean Reward across all agents: 623.7869201059651
[37m[1m[2023-06-25 06:36:02,473][129146] Average Trajectory Length: 995.456
[36m[2023-06-25 06:36:02,475][129146] mean_value=-577.7425429615695, max_value=1053.0943088813992
[37m[1m[2023-06-25 06:36:02,478][129146] New mean coefficients: [[ 2.9468758  -1.61652     0.16301948  0.3971545   0.1675142 ]]
[37m[1m[2023-06-25 06:36:02,479][129146] Moving the mean solution point...
[36m[2023-06-25 06:36:12,079][129146] train() took 9.60 seconds to complete
[36m[2023-06-25 06:36:12,079][129146] FPS: 400076.22
[36m[2023-06-25 06:36:12,081][129146] itr=601, itrs=2000, Progress: 30.05%
[36m[2023-06-25 06:36:23,550][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 06:36:23,550][129146] FPS: 335369.48
[36m[2023-06-25 06:36:28,366][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:36:28,366][129146] Reward + Measures: [[2263.58849406    0.32090083    0.35280523    0.14172587    0.29986453]]
[37m[1m[2023-06-25 06:36:28,366][129146] Max Reward on eval: 2263.5884940584033
[37m[1m[2023-06-25 06:36:28,366][129146] Min Reward on eval: 2263.5884940584033
[37m[1m[2023-06-25 06:36:28,367][129146] Mean Reward across all agents: 2263.5884940584033
[37m[1m[2023-06-25 06:36:28,367][129146] Average Trajectory Length: 998.304
[36m[2023-06-25 06:36:33,834][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:36:33,835][129146] Reward + Measures: [[ 457.32627365    0.37900001    0.3364        0.329         0.38229999]
[37m[1m [ 516.14612943    0.21090002    0.30070001    0.2737        0.23720001]
[37m[1m [1670.98035845    0.26590002    0.35530004    0.18979999    0.30310002]
[37m[1m ...
[37m[1m [1226.05464134    0.28779998    0.36320001    0.22320001    0.32609999]
[37m[1m [ 842.67204001    0.32119998    0.38600001    0.25890002    0.382     ]
[37m[1m [ 496.1169025     0.5316        0.57539999    0.26100001    0.76439995]]
[37m[1m[2023-06-25 06:36:33,835][129146] Max Reward on eval: 2211.9617245675995
[37m[1m[2023-06-25 06:36:33,835][129146] Min Reward on eval: -516.4020546562504
[37m[1m[2023-06-25 06:36:33,835][129146] Mean Reward across all agents: 953.9247604004324
[37m[1m[2023-06-25 06:36:33,836][129146] Average Trajectory Length: 998.454
[36m[2023-06-25 06:36:33,840][129146] mean_value=-725.096054808516, max_value=966.9518810234609
[37m[1m[2023-06-25 06:36:33,843][129146] New mean coefficients: [[ 2.8019888  -1.6694539  -0.1782586   0.3426663   0.19165412]]
[37m[1m[2023-06-25 06:36:33,843][129146] Moving the mean solution point...
[36m[2023-06-25 06:36:43,737][129146] train() took 9.89 seconds to complete
[36m[2023-06-25 06:36:43,737][129146] FPS: 388199.91
[36m[2023-06-25 06:36:43,740][129146] itr=602, itrs=2000, Progress: 30.10%
[36m[2023-06-25 06:36:55,424][129146] train() took 11.66 seconds to complete
[36m[2023-06-25 06:36:55,424][129146] FPS: 329212.18
[36m[2023-06-25 06:37:00,247][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:37:00,248][129146] Reward + Measures: [[2444.54187364    0.31108728    0.33534953    0.13121049    0.295573  ]]
[37m[1m[2023-06-25 06:37:00,248][129146] Max Reward on eval: 2444.5418736373385
[37m[1m[2023-06-25 06:37:00,248][129146] Min Reward on eval: 2444.5418736373385
[37m[1m[2023-06-25 06:37:00,248][129146] Mean Reward across all agents: 2444.5418736373385
[37m[1m[2023-06-25 06:37:00,249][129146] Average Trajectory Length: 997.14
[36m[2023-06-25 06:37:05,686][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:37:05,692][129146] Reward + Measures: [[1914.53791187    0.22091258    0.33093584    0.09999182    0.27618363]
[37m[1m [1538.16262594    0.31990001    0.32539999    0.16949999    0.29969999]
[37m[1m [ 327.58410138    0.10219999    0.57610005    0.27469999    0.5399    ]
[37m[1m ...
[37m[1m [1470.56834105    0.36560002    0.37310001    0.1777        0.35569999]
[37m[1m [1481.72751847    0.22119999    0.36500001    0.1078        0.36179999]
[37m[1m [ -24.58517598    0.59539998    0.78859997    0.0415        0.88330001]]
[37m[1m[2023-06-25 06:37:05,692][129146] Max Reward on eval: 2498.3037773934193
[37m[1m[2023-06-25 06:37:05,692][129146] Min Reward on eval: -613.5674566663686
[37m[1m[2023-06-25 06:37:05,693][129146] Mean Reward across all agents: 815.061589986077
[37m[1m[2023-06-25 06:37:05,693][129146] Average Trajectory Length: 997.2289999999999
[36m[2023-06-25 06:37:05,696][129146] mean_value=-1066.578816172731, max_value=1224.5835985681333
[37m[1m[2023-06-25 06:37:05,699][129146] New mean coefficients: [[ 2.571805   -1.67464     0.13077307  0.30483216  0.06836283]]
[37m[1m[2023-06-25 06:37:05,700][129146] Moving the mean solution point...
[36m[2023-06-25 06:37:15,625][129146] train() took 9.92 seconds to complete
[36m[2023-06-25 06:37:15,625][129146] FPS: 386959.61
[36m[2023-06-25 06:37:15,627][129146] itr=603, itrs=2000, Progress: 30.15%
[36m[2023-06-25 06:37:27,324][129146] train() took 11.67 seconds to complete
[36m[2023-06-25 06:37:27,324][129146] FPS: 328935.84
[36m[2023-06-25 06:37:32,166][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:37:32,171][129146] Reward + Measures: [[2593.93220634    0.30515587    0.33028713    0.12687851    0.29329398]]
[37m[1m[2023-06-25 06:37:32,172][129146] Max Reward on eval: 2593.9322063375794
[37m[1m[2023-06-25 06:37:32,172][129146] Min Reward on eval: 2593.9322063375794
[37m[1m[2023-06-25 06:37:32,172][129146] Mean Reward across all agents: 2593.9322063375794
[37m[1m[2023-06-25 06:37:32,172][129146] Average Trajectory Length: 996.4366666666666
[36m[2023-06-25 06:37:37,551][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:37:37,552][129146] Reward + Measures: [[1133.67927144    0.27780002    0.39220005    0.16059999    0.26199999]
[37m[1m [1095.40417829    0.37709999    0.4262        0.2852        0.3723    ]
[37m[1m [1425.93406528    0.29239997    0.36670002    0.16040002    0.25749999]
[37m[1m ...
[37m[1m [ 851.12244469    0.2647        0.36769998    0.17709999    0.24800001]
[37m[1m [ 880.1919865     0.27010003    0.36150002    0.16199999    0.25310001]
[37m[1m [1087.99380342    0.26449999    0.36699998    0.18380001    0.2457    ]]
[37m[1m[2023-06-25 06:37:37,552][129146] Max Reward on eval: 2442.0336105510128
[37m[1m[2023-06-25 06:37:37,552][129146] Min Reward on eval: -112.76192206184787
[37m[1m[2023-06-25 06:37:37,552][129146] Mean Reward across all agents: 1060.4872308537406
[37m[1m[2023-06-25 06:37:37,553][129146] Average Trajectory Length: 995.5956666666666
[36m[2023-06-25 06:37:37,554][129146] mean_value=-2211.27782389717, max_value=563.5414267902977
[37m[1m[2023-06-25 06:37:37,557][129146] New mean coefficients: [[ 2.879167   -0.98487145  0.25087905  0.14996977  0.6790292 ]]
[37m[1m[2023-06-25 06:37:37,558][129146] Moving the mean solution point...
[36m[2023-06-25 06:37:47,401][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 06:37:47,402][129146] FPS: 390176.25
[36m[2023-06-25 06:37:47,404][129146] itr=604, itrs=2000, Progress: 30.20%
[36m[2023-06-25 06:37:59,039][129146] train() took 11.62 seconds to complete
[36m[2023-06-25 06:37:59,039][129146] FPS: 330603.17
[36m[2023-06-25 06:38:03,730][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:38:03,730][129146] Reward + Measures: [[2722.10631514    0.30200395    0.32566658    0.12217596    0.29399341]]
[37m[1m[2023-06-25 06:38:03,730][129146] Max Reward on eval: 2722.106315138146
[37m[1m[2023-06-25 06:38:03,731][129146] Min Reward on eval: 2722.106315138146
[37m[1m[2023-06-25 06:38:03,731][129146] Mean Reward across all agents: 2722.106315138146
[37m[1m[2023-06-25 06:38:03,731][129146] Average Trajectory Length: 995.1013333333333
[36m[2023-06-25 06:38:09,102][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:38:09,102][129146] Reward + Measures: [[ 884.85798313    0.23260002    0.40510002    0.2034        0.39219999]
[37m[1m [1124.69002652    0.3371        0.43770003    0.11079999    0.52640003]
[37m[1m [ 482.38931417    0.2299        0.38690001    0.2009        0.18810001]
[37m[1m ...
[37m[1m [ 653.90917828    0.28280002    0.50010002    0.17660001    0.5873    ]
[37m[1m [1372.78659673    0.26700184    0.38845459    0.15340805    0.44862786]
[37m[1m [ 767.30634516    0.32950002    0.62740004    0.16900001    0.70609999]]
[37m[1m[2023-06-25 06:38:09,102][129146] Max Reward on eval: 2484.1748382982796
[37m[1m[2023-06-25 06:38:09,103][129146] Min Reward on eval: -73.62302769664966
[37m[1m[2023-06-25 06:38:09,103][129146] Mean Reward across all agents: 893.3885717296203
[37m[1m[2023-06-25 06:38:09,103][129146] Average Trajectory Length: 947.3473333333333
[36m[2023-06-25 06:38:09,106][129146] mean_value=-1061.4349207558382, max_value=1690.4662633079897
[37m[1m[2023-06-25 06:38:09,109][129146] New mean coefficients: [[ 2.9449    -1.0334972  0.677035   0.3329172  0.6529932]]
[37m[1m[2023-06-25 06:38:09,110][129146] Moving the mean solution point...
[36m[2023-06-25 06:38:18,650][129146] train() took 9.54 seconds to complete
[36m[2023-06-25 06:38:18,651][129146] FPS: 402565.72
[36m[2023-06-25 06:38:18,653][129146] itr=605, itrs=2000, Progress: 30.25%
[36m[2023-06-25 06:38:30,048][129146] train() took 11.37 seconds to complete
[36m[2023-06-25 06:38:30,048][129146] FPS: 337583.92
[36m[2023-06-25 06:38:34,684][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:38:34,685][129146] Reward + Measures: [[2880.59388509    0.29849291    0.32360891    0.12254424    0.2968168 ]]
[37m[1m[2023-06-25 06:38:34,685][129146] Max Reward on eval: 2880.593885090715
[37m[1m[2023-06-25 06:38:34,685][129146] Min Reward on eval: 2880.593885090715
[37m[1m[2023-06-25 06:38:34,685][129146] Mean Reward across all agents: 2880.593885090715
[37m[1m[2023-06-25 06:38:34,685][129146] Average Trajectory Length: 995.4776666666667
[36m[2023-06-25 06:38:40,228][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:38:40,228][129146] Reward + Measures: [[ 456.42380213    0.7816        0.4402        0.75450003    0.40540001]
[37m[1m [ 387.82062064    0.45650002    0.39330003    0.38430002    0.38410002]
[37m[1m [1666.10728044    0.42379999    0.38410002    0.22750001    0.30169997]
[37m[1m ...
[37m[1m [ 761.06069903    0.61390001    0.36689997    0.47510001    0.30070004]
[37m[1m [ 592.32016747    0.48700005    0.57130003    0.4686        0.44600001]
[37m[1m [ 914.49249672    0.25799999    0.442         0.26430002    0.3448    ]]
[37m[1m[2023-06-25 06:38:40,229][129146] Max Reward on eval: 2942.6825324874835
[37m[1m[2023-06-25 06:38:40,229][129146] Min Reward on eval: 74.07103103338159
[37m[1m[2023-06-25 06:38:40,229][129146] Mean Reward across all agents: 855.0507802584468
[37m[1m[2023-06-25 06:38:40,229][129146] Average Trajectory Length: 998.1653333333333
[36m[2023-06-25 06:38:40,236][129146] mean_value=-169.2964578511255, max_value=1279.0485319972504
[37m[1m[2023-06-25 06:38:40,239][129146] New mean coefficients: [[ 2.7180507  -0.7659703   0.5833355   0.27243522  0.8977927 ]]
[37m[1m[2023-06-25 06:38:40,240][129146] Moving the mean solution point...
[36m[2023-06-25 06:38:49,947][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 06:38:49,947][129146] FPS: 395647.73
[36m[2023-06-25 06:38:49,949][129146] itr=606, itrs=2000, Progress: 30.30%
[36m[2023-06-25 06:39:01,524][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 06:39:01,525][129146] FPS: 332312.50
[36m[2023-06-25 06:39:06,354][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:39:06,354][129146] Reward + Measures: [[3017.06723694    0.29484284    0.31726646    0.1229234     0.30097947]]
[37m[1m[2023-06-25 06:39:06,354][129146] Max Reward on eval: 3017.067236939712
[37m[1m[2023-06-25 06:39:06,355][129146] Min Reward on eval: 3017.067236939712
[37m[1m[2023-06-25 06:39:06,355][129146] Mean Reward across all agents: 3017.067236939712
[37m[1m[2023-06-25 06:39:06,355][129146] Average Trajectory Length: 994.0073333333333
[36m[2023-06-25 06:39:11,813][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:39:11,813][129146] Reward + Measures: [[  54.37229954    0.3767651     0.41837427    0.35212606    0.36984903]
[37m[1m [ 376.85475631    0.23766418    0.2428679     0.13004568    0.20446049]
[37m[1m [2088.1905596     0.29864237    0.28071678    0.12113358    0.33586207]
[37m[1m ...
[37m[1m [ 588.83262794    0.39158055    0.40268704    0.33524546    0.35762727]
[37m[1m [ 181.92882059    0.40231749    0.34591955    0.36393005    0.377848  ]
[37m[1m [ 363.89295525    0.32378933    0.29702052    0.28715107    0.31652704]]
[37m[1m[2023-06-25 06:39:11,814][129146] Max Reward on eval: 2964.3353003278608
[37m[1m[2023-06-25 06:39:11,814][129146] Min Reward on eval: -279.25446246807115
[37m[1m[2023-06-25 06:39:11,814][129146] Mean Reward across all agents: 951.083170755835
[37m[1m[2023-06-25 06:39:11,814][129146] Average Trajectory Length: 934.3813333333333
[36m[2023-06-25 06:39:11,816][129146] mean_value=-1778.282918783758, max_value=1141.0229926980055
[37m[1m[2023-06-25 06:39:11,819][129146] New mean coefficients: [[ 2.797934   -0.50758374  0.9390269   0.39845914  0.8682391 ]]
[37m[1m[2023-06-25 06:39:11,820][129146] Moving the mean solution point...
[36m[2023-06-25 06:39:21,497][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 06:39:21,498][129146] FPS: 396854.96
[36m[2023-06-25 06:39:21,500][129146] itr=607, itrs=2000, Progress: 30.35%
[36m[2023-06-25 06:39:33,016][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 06:39:33,017][129146] FPS: 334083.63
[36m[2023-06-25 06:39:37,725][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:39:37,726][129146] Reward + Measures: [[3189.13838919    0.29196402    0.30929729    0.12114056    0.29900762]]
[37m[1m[2023-06-25 06:39:37,726][129146] Max Reward on eval: 3189.1383891914866
[37m[1m[2023-06-25 06:39:37,726][129146] Min Reward on eval: 3189.1383891914866
[37m[1m[2023-06-25 06:39:37,727][129146] Mean Reward across all agents: 3189.1383891914866
[37m[1m[2023-06-25 06:39:37,727][129146] Average Trajectory Length: 992.1516666666666
[36m[2023-06-25 06:39:43,154][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:39:43,155][129146] Reward + Measures: [[2090.66287448    0.28000924    0.31474647    0.17490566    0.32175645]
[37m[1m [1644.82435706    0.2911        0.34209999    0.24010001    0.36199999]
[37m[1m [ 694.50039118    0.47459999    0.47019997    0.43220001    0.43540001]
[37m[1m ...
[37m[1m [-323.34969158    0.0581        0.88570005    0.69169998    0.89750004]
[37m[1m [ 560.45430307    0.1222        0.5722        0.34869999    0.58269995]
[37m[1m [1520.75454497    0.38850561    0.33020103    0.19138572    0.28098398]]
[37m[1m[2023-06-25 06:39:43,155][129146] Max Reward on eval: 3138.881982773589
[37m[1m[2023-06-25 06:39:43,155][129146] Min Reward on eval: -736.8812413320877
[37m[1m[2023-06-25 06:39:43,156][129146] Mean Reward across all agents: 1373.0182244245657
[37m[1m[2023-06-25 06:39:43,156][129146] Average Trajectory Length: 989.8786666666666
[36m[2023-06-25 06:39:43,160][129146] mean_value=-1029.5381150922567, max_value=1279.824995637944
[37m[1m[2023-06-25 06:39:43,163][129146] New mean coefficients: [[ 2.5466642  -0.3516903   1.3606403   0.44509402  0.95913553]]
[37m[1m[2023-06-25 06:39:43,164][129146] Moving the mean solution point...
[36m[2023-06-25 06:39:52,883][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 06:39:52,883][129146] FPS: 395181.16
[36m[2023-06-25 06:39:52,885][129146] itr=608, itrs=2000, Progress: 30.40%
[36m[2023-06-25 06:40:04,406][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 06:40:04,406][129146] FPS: 333930.15
[36m[2023-06-25 06:40:09,231][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:40:09,231][129146] Reward + Measures: [[3279.13771604    0.28674266    0.30734381    0.12308282    0.2975933 ]]
[37m[1m[2023-06-25 06:40:09,231][129146] Max Reward on eval: 3279.137716039012
[37m[1m[2023-06-25 06:40:09,232][129146] Min Reward on eval: 3279.137716039012
[37m[1m[2023-06-25 06:40:09,232][129146] Mean Reward across all agents: 3279.137716039012
[37m[1m[2023-06-25 06:40:09,232][129146] Average Trajectory Length: 989.054
[36m[2023-06-25 06:40:14,680][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:40:14,680][129146] Reward + Measures: [[1416.48311424    0.28353521    0.31682992    0.16475378    0.26859599]
[37m[1m [1464.16428128    0.30350003    0.3188        0.18080001    0.33829999]
[37m[1m [-237.14734099    0.1949607     0.15463132    0.11045648    0.18893495]
[37m[1m ...
[37m[1m [1340.1700854     0.23860002    0.2595        0.13410001    0.24420002]
[37m[1m [2199.73421889    0.22631417    0.2592274     0.11541188    0.25378036]
[37m[1m [ 776.88782191    0.28099999    0.2613        0.20549999    0.3091    ]]
[37m[1m[2023-06-25 06:40:14,680][129146] Max Reward on eval: 3278.7037269702646
[37m[1m[2023-06-25 06:40:14,681][129146] Min Reward on eval: -532.1117869044305
[37m[1m[2023-06-25 06:40:14,681][129146] Mean Reward across all agents: 1093.8125670680158
[37m[1m[2023-06-25 06:40:14,681][129146] Average Trajectory Length: 922.5596666666667
[36m[2023-06-25 06:40:14,683][129146] mean_value=-1997.3839701674435, max_value=366.3876045689947
[37m[1m[2023-06-25 06:40:14,685][129146] New mean coefficients: [[2.7803555  0.03272489 1.1965903  0.31220108 0.8699318 ]]
[37m[1m[2023-06-25 06:40:14,686][129146] Moving the mean solution point...
[36m[2023-06-25 06:40:24,363][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 06:40:24,363][129146] FPS: 396888.49
[36m[2023-06-25 06:40:24,366][129146] itr=609, itrs=2000, Progress: 30.45%
[36m[2023-06-25 06:40:35,830][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 06:40:35,831][129146] FPS: 335531.43
[36m[2023-06-25 06:40:40,691][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:40:40,692][129146] Reward + Measures: [[3460.84224365    0.28007281    0.30270487    0.11548865    0.30120003]]
[37m[1m[2023-06-25 06:40:40,692][129146] Max Reward on eval: 3460.84224364643
[37m[1m[2023-06-25 06:40:40,692][129146] Min Reward on eval: 3460.84224364643
[37m[1m[2023-06-25 06:40:40,692][129146] Mean Reward across all agents: 3460.84224364643
[37m[1m[2023-06-25 06:40:40,692][129146] Average Trajectory Length: 988.341
[36m[2023-06-25 06:40:46,350][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:40:46,350][129146] Reward + Measures: [[2068.84987062    0.33160001    0.42060003    0.24059999    0.28330001]
[37m[1m [ 589.71875405    0.63120002    0.32580003    0.43630001    0.3425    ]
[37m[1m [2367.19852748    0.46830001    0.33310002    0.1626        0.26879999]
[37m[1m ...
[37m[1m [1346.0698516     0.40270001    0.40039998    0.2141        0.27420002]
[37m[1m [-584.6113518     0.83890003    0.74949998    0.42039999    0.79899997]
[37m[1m [1178.44262353    0.4323        0.4499        0.10089999    0.60280001]]
[37m[1m[2023-06-25 06:40:46,351][129146] Max Reward on eval: 3660.958672641637
[37m[1m[2023-06-25 06:40:46,351][129146] Min Reward on eval: -1840.2897139412119
[37m[1m[2023-06-25 06:40:46,351][129146] Mean Reward across all agents: 1376.8352556084842
[37m[1m[2023-06-25 06:40:46,351][129146] Average Trajectory Length: 995.01
[36m[2023-06-25 06:40:46,357][129146] mean_value=-490.1383762415295, max_value=1594.6936556657788
[37m[1m[2023-06-25 06:40:46,359][129146] New mean coefficients: [[2.8218     0.39355627 0.9713206  0.10544336 1.2067525 ]]
[37m[1m[2023-06-25 06:40:46,360][129146] Moving the mean solution point...
[36m[2023-06-25 06:40:56,126][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 06:40:56,127][129146] FPS: 393272.78
[36m[2023-06-25 06:40:56,129][129146] itr=610, itrs=2000, Progress: 30.50%
[37m[1m[2023-06-25 06:41:01,021][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000590
[36m[2023-06-25 06:41:12,829][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 06:41:12,829][129146] FPS: 333591.01
[36m[2023-06-25 06:41:17,635][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:41:17,635][129146] Reward + Measures: [[3624.07771731    0.27325809    0.29231346    0.10764202    0.30539501]]
[37m[1m[2023-06-25 06:41:17,635][129146] Max Reward on eval: 3624.077717314301
[37m[1m[2023-06-25 06:41:17,636][129146] Min Reward on eval: 3624.077717314301
[37m[1m[2023-06-25 06:41:17,636][129146] Mean Reward across all agents: 3624.077717314301
[37m[1m[2023-06-25 06:41:17,636][129146] Average Trajectory Length: 987.9516666666666
[36m[2023-06-25 06:41:23,088][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:41:23,089][129146] Reward + Measures: [[2386.25671891    0.29173884    0.30647531    0.12744819    0.28626925]
[37m[1m [ 474.27361255    0.32473099    0.2779499     0.14968275    0.26071578]
[37m[1m [1334.75935892    0.31767377    0.28718638    0.13054888    0.24582469]
[37m[1m ...
[37m[1m [2945.2075908     0.2915        0.31200001    0.10470001    0.2737    ]
[37m[1m [ 817.68617275    0.29017302    0.26065245    0.12180455    0.24127571]
[37m[1m [1620.73078153    0.33142868    0.33218947    0.12667874    0.27368084]]
[37m[1m[2023-06-25 06:41:23,089][129146] Max Reward on eval: 3693.95208226447
[37m[1m[2023-06-25 06:41:23,089][129146] Min Reward on eval: -331.8267630147631
[37m[1m[2023-06-25 06:41:23,089][129146] Mean Reward across all agents: 1613.5406684934235
[37m[1m[2023-06-25 06:41:23,090][129146] Average Trajectory Length: 890.6773333333333
[36m[2023-06-25 06:41:23,091][129146] mean_value=-2361.783207780299, max_value=195.94562783923504
[37m[1m[2023-06-25 06:41:23,094][129146] New mean coefficients: [[ 3.1940486   0.47212595  0.00157243 -0.10785492  1.0661612 ]]
[37m[1m[2023-06-25 06:41:23,095][129146] Moving the mean solution point...
[36m[2023-06-25 06:41:32,689][129146] train() took 9.59 seconds to complete
[36m[2023-06-25 06:41:32,689][129146] FPS: 400314.52
[36m[2023-06-25 06:41:32,691][129146] itr=611, itrs=2000, Progress: 30.55%
[36m[2023-06-25 06:41:44,237][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 06:41:44,237][129146] FPS: 333180.71
[36m[2023-06-25 06:41:49,060][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:41:49,061][129146] Reward + Measures: [[3389.13784632    0.29626748    0.2712788     0.0532299     0.27194014]]
[37m[1m[2023-06-25 06:41:49,061][129146] Max Reward on eval: 3389.1378463198903
[37m[1m[2023-06-25 06:41:49,061][129146] Min Reward on eval: 3389.1378463198903
[37m[1m[2023-06-25 06:41:49,061][129146] Mean Reward across all agents: 3389.1378463198903
[37m[1m[2023-06-25 06:41:49,062][129146] Average Trajectory Length: 998.0876666666667
[36m[2023-06-25 06:41:54,443][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:41:54,449][129146] Reward + Measures: [[2009.50202433    0.32909998    0.36740002    0.1551        0.32539999]
[37m[1m [1084.76640728    0.31169999    0.33309999    0.21080001    0.24060002]
[37m[1m [ 966.95907843    0.36770004    0.6304        0.1741        0.66370004]
[37m[1m ...
[37m[1m [1510.99615962    0.35820001    0.43760005    0.131         0.43540001]
[37m[1m [2819.09653015    0.32930002    0.30050001    0.0632        0.2746    ]
[37m[1m [ 609.15534842    0.43579999    0.46199998    0.34279999    0.3601    ]]
[37m[1m[2023-06-25 06:41:54,449][129146] Max Reward on eval: 3325.906764170155
[37m[1m[2023-06-25 06:41:54,450][129146] Min Reward on eval: -783.2439274191158
[37m[1m[2023-06-25 06:41:54,450][129146] Mean Reward across all agents: 1584.3500433714653
[37m[1m[2023-06-25 06:41:54,450][129146] Average Trajectory Length: 988.1863333333333
[36m[2023-06-25 06:41:54,453][129146] mean_value=-1441.869114922318, max_value=834.6625064996321
[37m[1m[2023-06-25 06:41:54,455][129146] New mean coefficients: [[ 3.833962    1.359427    0.89186376 -0.09259897  1.5336473 ]]
[37m[1m[2023-06-25 06:41:54,456][129146] Moving the mean solution point...
[36m[2023-06-25 06:42:04,168][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 06:42:04,168][129146] FPS: 395467.99
[36m[2023-06-25 06:42:04,170][129146] itr=612, itrs=2000, Progress: 30.60%
[36m[2023-06-25 06:42:15,804][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 06:42:15,804][129146] FPS: 330617.01
[36m[2023-06-25 06:42:20,676][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:42:20,676][129146] Reward + Measures: [[3595.14658857    0.28766951    0.26645947    0.04890729    0.27192587]]
[37m[1m[2023-06-25 06:42:20,677][129146] Max Reward on eval: 3595.1465885661814
[37m[1m[2023-06-25 06:42:20,677][129146] Min Reward on eval: 3595.1465885661814
[37m[1m[2023-06-25 06:42:20,677][129146] Mean Reward across all agents: 3595.1465885661814
[37m[1m[2023-06-25 06:42:20,678][129146] Average Trajectory Length: 997.8393333333333
[36m[2023-06-25 06:42:26,133][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:42:26,133][129146] Reward + Measures: [[-556.83731887    0.17400001    0.73089999    0.60879999    0.70139998]
[37m[1m [-598.19953245    0.23380001    0.81490004    0.77679998    0.81099999]
[37m[1m [2770.67938731    0.33320004    0.30410001    0.0609        0.25289997]
[37m[1m ...
[37m[1m [-121.68729432    0.25280002    0.89529991    0.72350001    0.88210005]
[37m[1m [-456.12506269    0.08810001    0.90930003    0.84200001    0.89580005]
[37m[1m [-490.13210949    0.08350001    0.90749997    0.84110004    0.87010002]]
[37m[1m[2023-06-25 06:42:26,134][129146] Max Reward on eval: 3223.2934420404954
[37m[1m[2023-06-25 06:42:26,134][129146] Min Reward on eval: -909.1462754066567
[37m[1m[2023-06-25 06:42:26,134][129146] Mean Reward across all agents: 456.7425617595079
[37m[1m[2023-06-25 06:42:26,134][129146] Average Trajectory Length: 995.4183333333333
[36m[2023-06-25 06:42:26,141][129146] mean_value=-499.63373743756637, max_value=866.8440234270342
[37m[1m[2023-06-25 06:42:26,144][129146] New mean coefficients: [[ 4.1006613   1.5294895   0.617211   -0.21901096  1.5090384 ]]
[37m[1m[2023-06-25 06:42:26,145][129146] Moving the mean solution point...
[36m[2023-06-25 06:42:35,873][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 06:42:35,873][129146] FPS: 394823.06
[36m[2023-06-25 06:42:35,875][129146] itr=613, itrs=2000, Progress: 30.65%
[36m[2023-06-25 06:42:47,423][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 06:42:47,424][129146] FPS: 333069.86
[36m[2023-06-25 06:42:52,282][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:42:52,283][129146] Reward + Measures: [[3817.77372031    0.27956372    0.26479843    0.04303961    0.27504718]]
[37m[1m[2023-06-25 06:42:52,283][129146] Max Reward on eval: 3817.773720309489
[37m[1m[2023-06-25 06:42:52,283][129146] Min Reward on eval: 3817.773720309489
[37m[1m[2023-06-25 06:42:52,283][129146] Mean Reward across all agents: 3817.773720309489
[37m[1m[2023-06-25 06:42:52,284][129146] Average Trajectory Length: 998.3473333333333
[36m[2023-06-25 06:42:57,758][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:42:57,772][129146] Reward + Measures: [[1759.58321198    0.35180002    0.35969999    0.1354        0.4145    ]
[37m[1m [1049.51620414    0.27100003    0.3265        0.1714        0.31869999]
[37m[1m [1215.24626337    0.38870001    0.38380003    0.13249999    0.4691    ]
[37m[1m ...
[37m[1m [2005.63292724    0.33559999    0.3364        0.08530001    0.40000001]
[37m[1m [1637.61195696    0.34930003    0.35960004    0.1276        0.40709996]
[37m[1m [1897.86480249    0.29669997    0.34400001    0.09900001    0.27199998]]
[37m[1m[2023-06-25 06:42:57,772][129146] Max Reward on eval: 3531.0103991396727
[37m[1m[2023-06-25 06:42:57,773][129146] Min Reward on eval: -543.5056957405817
[37m[1m[2023-06-25 06:42:57,773][129146] Mean Reward across all agents: 1636.0609973273806
[37m[1m[2023-06-25 06:42:57,773][129146] Average Trajectory Length: 954.342
[36m[2023-06-25 06:42:57,775][129146] mean_value=-2079.48349975008, max_value=1428.7641268074376
[37m[1m[2023-06-25 06:42:57,777][129146] New mean coefficients: [[ 3.5785513   1.0957228   1.1118518  -0.04005112  1.3655392 ]]
[37m[1m[2023-06-25 06:42:57,778][129146] Moving the mean solution point...
[36m[2023-06-25 06:43:07,498][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 06:43:07,498][129146] FPS: 395147.18
[36m[2023-06-25 06:43:07,500][129146] itr=614, itrs=2000, Progress: 30.70%
[36m[2023-06-25 06:43:19,540][129146] train() took 12.02 seconds to complete
[36m[2023-06-25 06:43:19,540][129146] FPS: 319457.71
[36m[2023-06-25 06:43:24,352][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:43:24,352][129146] Reward + Measures: [[4014.07693125    0.2658318     0.26427448    0.04373276    0.28433064]]
[37m[1m[2023-06-25 06:43:24,353][129146] Max Reward on eval: 4014.0769312490797
[37m[1m[2023-06-25 06:43:24,353][129146] Min Reward on eval: 4014.0769312490797
[37m[1m[2023-06-25 06:43:24,353][129146] Mean Reward across all agents: 4014.0769312490797
[37m[1m[2023-06-25 06:43:24,354][129146] Average Trajectory Length: 998.7073333333333
[36m[2023-06-25 06:43:29,973][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:43:29,974][129146] Reward + Measures: [[1179.1219543     0.58030003    0.32519999    0.34529999    0.32269999]
[37m[1m [3305.66373113    0.31979999    0.28060001    0.0676        0.28239998]
[37m[1m [1122.40719723    0.55800003    0.2545        0.3026        0.26760003]
[37m[1m ...
[37m[1m [ 490.27905847    0.35293695    0.31397673    0.24251781    0.25974795]
[37m[1m [1416.62394       0.45450002    0.33650002    0.23529999    0.25639999]
[37m[1m [ 490.12312951    0.27110001    0.42309999    0.23670001    0.36270002]]
[37m[1m[2023-06-25 06:43:29,974][129146] Max Reward on eval: 3644.1818168129075
[37m[1m[2023-06-25 06:43:29,974][129146] Min Reward on eval: -492.0876937664114
[37m[1m[2023-06-25 06:43:29,974][129146] Mean Reward across all agents: 1322.4223984497053
[37m[1m[2023-06-25 06:43:29,975][129146] Average Trajectory Length: 979.1943333333332
[36m[2023-06-25 06:43:29,978][129146] mean_value=-1156.1478709958978, max_value=1086.6231210702035
[37m[1m[2023-06-25 06:43:29,981][129146] New mean coefficients: [[ 3.75752    1.248358   0.3662427 -0.3493361  1.708262 ]]
[37m[1m[2023-06-25 06:43:29,982][129146] Moving the mean solution point...
[36m[2023-06-25 06:43:39,820][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 06:43:39,820][129146] FPS: 390371.30
[36m[2023-06-25 06:43:39,823][129146] itr=615, itrs=2000, Progress: 30.75%
[36m[2023-06-25 06:43:51,401][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 06:43:51,401][129146] FPS: 332263.44
[36m[2023-06-25 06:43:56,216][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:43:56,217][129146] Reward + Measures: [[4185.27889578    0.26161009    0.26311165    0.04039227    0.2888135 ]]
[37m[1m[2023-06-25 06:43:56,217][129146] Max Reward on eval: 4185.278895775892
[37m[1m[2023-06-25 06:43:56,217][129146] Min Reward on eval: 4185.278895775892
[37m[1m[2023-06-25 06:43:56,217][129146] Mean Reward across all agents: 4185.278895775892
[37m[1m[2023-06-25 06:43:56,217][129146] Average Trajectory Length: 998.6306666666667
[36m[2023-06-25 06:44:01,800][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:44:01,801][129146] Reward + Measures: [[3089.86507523    0.24870001    0.29850003    0.07540001    0.26560003]
[37m[1m [ 228.52025536    0.32432061    0.36713538    0.1054149     0.40081164]
[37m[1m [ 395.25155419    0.47540003    0.38900003    0.60220003    0.60680002]
[37m[1m ...
[37m[1m [ 185.84991876    0.3672536     0.36157799    0.37483865    0.52840936]
[37m[1m [ 292.87237526    0.27399489    0.29470912    0.09791041    0.25109601]
[37m[1m [2147.63704003    0.26470003    0.25980002    0.0511        0.25079998]]
[37m[1m[2023-06-25 06:44:01,801][129146] Max Reward on eval: 3538.0880066463724
[37m[1m[2023-06-25 06:44:01,801][129146] Min Reward on eval: -674.4146051617921
[37m[1m[2023-06-25 06:44:01,801][129146] Mean Reward across all agents: 690.4079713359497
[37m[1m[2023-06-25 06:44:01,802][129146] Average Trajectory Length: 975.9816666666667
[36m[2023-06-25 06:44:01,805][129146] mean_value=-2016.041645643494, max_value=1105.7482167855487
[37m[1m[2023-06-25 06:44:01,808][129146] New mean coefficients: [[ 3.854144    1.2382317  -0.48652357 -0.56841916  1.3554549 ]]
[37m[1m[2023-06-25 06:44:01,809][129146] Moving the mean solution point...
[36m[2023-06-25 06:44:11,621][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 06:44:11,621][129146] FPS: 391413.55
[36m[2023-06-25 06:44:11,624][129146] itr=616, itrs=2000, Progress: 30.80%
[36m[2023-06-25 06:44:23,021][129146] train() took 11.37 seconds to complete
[36m[2023-06-25 06:44:23,021][129146] FPS: 337589.39
[36m[2023-06-25 06:44:27,824][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:44:27,825][129146] Reward + Measures: [[4373.40417303    0.25664124    0.26303428    0.03464636    0.29639754]]
[37m[1m[2023-06-25 06:44:27,825][129146] Max Reward on eval: 4373.404173031756
[37m[1m[2023-06-25 06:44:27,825][129146] Min Reward on eval: 4373.404173031756
[37m[1m[2023-06-25 06:44:27,825][129146] Mean Reward across all agents: 4373.404173031756
[37m[1m[2023-06-25 06:44:27,826][129146] Average Trajectory Length: 998.0003333333333
[36m[2023-06-25 06:44:33,282][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:44:33,283][129146] Reward + Measures: [[  12.04507625    0.3044        0.62050003    0.1173        0.52870005]
[37m[1m [ 206.74965662    0.20338745    0.38197193    0.16195644    0.21917143]
[37m[1m [ 857.95571994    0.25080001    0.41480002    0.18779999    0.21960001]
[37m[1m ...
[37m[1m [ 475.33385081    0.18350001    0.5546        0.23480001    0.45379996]
[37m[1m [1426.12637173    0.27320001    0.4276        0.16870001    0.2282    ]
[37m[1m [ 717.88861091    0.26320001    0.33759999    0.17200001    0.20729999]]
[37m[1m[2023-06-25 06:44:33,283][129146] Max Reward on eval: 4078.6967099109665
[37m[1m[2023-06-25 06:44:33,283][129146] Min Reward on eval: -651.8115459362219
[37m[1m[2023-06-25 06:44:33,283][129146] Mean Reward across all agents: 1273.2989517226981
[37m[1m[2023-06-25 06:44:33,284][129146] Average Trajectory Length: 993.4353333333333
[36m[2023-06-25 06:44:33,285][129146] mean_value=-1414.313425804336, max_value=935.0466608118213
[37m[1m[2023-06-25 06:44:33,288][129146] New mean coefficients: [[ 3.6905525   0.91212326 -0.7059236  -0.6875002   1.2198032 ]]
[37m[1m[2023-06-25 06:44:33,289][129146] Moving the mean solution point...
[36m[2023-06-25 06:44:43,128][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 06:44:43,128][129146] FPS: 390337.67
[36m[2023-06-25 06:44:43,131][129146] itr=617, itrs=2000, Progress: 30.85%
[36m[2023-06-25 06:44:54,795][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 06:44:54,796][129146] FPS: 329830.98
[36m[2023-06-25 06:44:59,535][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:44:59,535][129146] Reward + Measures: [[4514.85461535    0.25442514    0.25875196    0.02962852    0.29429325]]
[37m[1m[2023-06-25 06:44:59,536][129146] Max Reward on eval: 4514.8546153466905
[37m[1m[2023-06-25 06:44:59,536][129146] Min Reward on eval: 4514.8546153466905
[37m[1m[2023-06-25 06:44:59,536][129146] Mean Reward across all agents: 4514.8546153466905
[37m[1m[2023-06-25 06:44:59,536][129146] Average Trajectory Length: 996.5796666666666
[36m[2023-06-25 06:45:04,989][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:45:04,989][129146] Reward + Measures: [[3732.8163116     0.2385        0.26460001    0.0523        0.2911    ]
[37m[1m [  93.35479862    0.31870002    0.41319999    0.26930001    0.24350002]
[37m[1m [ 782.69960611    0.35730001    0.39330003    0.2735        0.2316    ]
[37m[1m ...
[37m[1m [2664.30281667    0.3299        0.2902        0.11650001    0.27270004]
[37m[1m [ 363.51740837    0.36139998    0.4413        0.30829999    0.27540001]
[37m[1m [1639.53337002    0.3712        0.34110001    0.1672        0.3723    ]]
[37m[1m[2023-06-25 06:45:04,989][129146] Max Reward on eval: 3924.9901557967532
[37m[1m[2023-06-25 06:45:04,990][129146] Min Reward on eval: -682.9031083076028
[37m[1m[2023-06-25 06:45:04,990][129146] Mean Reward across all agents: 593.709307505125
[37m[1m[2023-06-25 06:45:04,990][129146] Average Trajectory Length: 992.9853333333333
[36m[2023-06-25 06:45:04,992][129146] mean_value=-1220.3002543492146, max_value=2718.684553737463
[37m[1m[2023-06-25 06:45:04,995][129146] New mean coefficients: [[ 3.6737368  1.0265455 -0.0438118 -0.5818554  1.0500913]]
[37m[1m[2023-06-25 06:45:04,996][129146] Moving the mean solution point...
[36m[2023-06-25 06:45:14,735][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 06:45:14,735][129146] FPS: 394359.68
[36m[2023-06-25 06:45:14,737][129146] itr=618, itrs=2000, Progress: 30.90%
[36m[2023-06-25 06:45:26,387][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 06:45:26,387][129146] FPS: 330284.64
[36m[2023-06-25 06:45:31,220][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:45:31,220][129146] Reward + Measures: [[4653.20927126    0.25879338    0.25037089    0.02377586    0.28120098]]
[37m[1m[2023-06-25 06:45:31,220][129146] Max Reward on eval: 4653.209271259537
[37m[1m[2023-06-25 06:45:31,221][129146] Min Reward on eval: 4653.209271259537
[37m[1m[2023-06-25 06:45:31,221][129146] Mean Reward across all agents: 4653.209271259537
[37m[1m[2023-06-25 06:45:31,221][129146] Average Trajectory Length: 996.6826666666666
[36m[2023-06-25 06:45:36,686][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:45:36,692][129146] Reward + Measures: [[1512.2864901     0.36289999    0.3682        0.26390001    0.25730002]
[37m[1m [ 237.51950672    0.55899996    0.47890002    0.3998        0.49400002]
[37m[1m [2791.80792301    0.34260002    0.38770002    0.1991        0.36900002]
[37m[1m ...
[37m[1m [1344.30453344    0.20669483    0.22796343    0.06235955    0.16966802]
[37m[1m [ 575.59825823    0.23902224    0.27561897    0.20532222    0.20407386]
[37m[1m [2986.77377936    0.27839762    0.32476047    0.16837116    0.28419742]]
[37m[1m[2023-06-25 06:45:36,692][129146] Max Reward on eval: 4473.620189833082
[37m[1m[2023-06-25 06:45:36,692][129146] Min Reward on eval: -432.3508731741109
[37m[1m[2023-06-25 06:45:36,693][129146] Mean Reward across all agents: 1360.1258028055831
[37m[1m[2023-06-25 06:45:36,693][129146] Average Trajectory Length: 985.889
[36m[2023-06-25 06:45:36,696][129146] mean_value=-799.3679249671846, max_value=2656.2139419136756
[37m[1m[2023-06-25 06:45:36,699][129146] New mean coefficients: [[ 3.6259768   0.69383836 -0.9029974  -0.7729748   0.78667593]]
[37m[1m[2023-06-25 06:45:36,700][129146] Moving the mean solution point...
[36m[2023-06-25 06:45:46,451][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 06:45:46,451][129146] FPS: 393862.18
[36m[2023-06-25 06:45:46,454][129146] itr=619, itrs=2000, Progress: 30.95%
[36m[2023-06-25 06:45:57,987][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 06:45:57,988][129146] FPS: 333514.50
[36m[2023-06-25 06:46:02,828][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:46:02,829][129146] Reward + Measures: [[4794.04388286    0.25825113    0.25366136    0.02483153    0.29313618]]
[37m[1m[2023-06-25 06:46:02,829][129146] Max Reward on eval: 4794.043882857933
[37m[1m[2023-06-25 06:46:02,829][129146] Min Reward on eval: 4794.043882857933
[37m[1m[2023-06-25 06:46:02,829][129146] Mean Reward across all agents: 4794.043882857933
[37m[1m[2023-06-25 06:46:02,830][129146] Average Trajectory Length: 998.2996666666667
[36m[2023-06-25 06:46:08,272][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:46:08,272][129146] Reward + Measures: [[2325.34843006    0.26789999    0.33580002    0.1435        0.31350002]
[37m[1m [ 752.42168205    0.4224        0.3831        0.31          0.43759999]
[37m[1m [2046.29002912    0.2904        0.30200002    0.17760001    0.32350001]
[37m[1m ...
[37m[1m [1358.36168597    0.30180001    0.29800001    0.1648        0.27509999]
[37m[1m [2757.876176      0.27780002    0.28290001    0.11439999    0.33540002]
[37m[1m [4122.35550685    0.27000001    0.30230004    0.0487        0.3247    ]]
[37m[1m[2023-06-25 06:46:08,272][129146] Max Reward on eval: 4556.749587571598
[37m[1m[2023-06-25 06:46:08,273][129146] Min Reward on eval: -616.6494793160527
[37m[1m[2023-06-25 06:46:08,273][129146] Mean Reward across all agents: 1682.6953111127568
[37m[1m[2023-06-25 06:46:08,273][129146] Average Trajectory Length: 993.5353333333333
[36m[2023-06-25 06:46:08,275][129146] mean_value=-1435.494207379643, max_value=1786.8125496675448
[37m[1m[2023-06-25 06:46:08,277][129146] New mean coefficients: [[ 3.5699713   0.75771815 -0.20865488 -0.53220195  0.7210833 ]]
[37m[1m[2023-06-25 06:46:08,279][129146] Moving the mean solution point...
[36m[2023-06-25 06:46:17,946][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 06:46:17,946][129146] FPS: 397294.15
[36m[2023-06-25 06:46:17,948][129146] itr=620, itrs=2000, Progress: 31.00%
[37m[1m[2023-06-25 06:46:23,025][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000600
[36m[2023-06-25 06:46:34,805][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 06:46:34,806][129146] FPS: 334427.78
[36m[2023-06-25 06:46:39,681][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:46:39,681][129146] Reward + Measures: [[3093.32858705    0.25642878    0.25221348    0.06016851    0.26523221]]
[37m[1m[2023-06-25 06:46:39,681][129146] Max Reward on eval: 3093.3285870526533
[37m[1m[2023-06-25 06:46:39,682][129146] Min Reward on eval: 3093.3285870526533
[37m[1m[2023-06-25 06:46:39,682][129146] Mean Reward across all agents: 3093.3285870526533
[37m[1m[2023-06-25 06:46:39,682][129146] Average Trajectory Length: 892.2563333333333
[36m[2023-06-25 06:46:45,151][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:46:45,201][129146] Reward + Measures: [[ 256.32902977    0.31644738    0.26312634    0.14499474    0.21588421]
[37m[1m [-198.47094485    0.299716      0.30940136    0.20838407    0.24120419]
[37m[1m [-423.25349729    0.38980001    0.34119999    0.20710002    0.22319999]
[37m[1m ...
[37m[1m [2550.64258476    0.28185198    0.26016626    0.11206686    0.24747042]
[37m[1m [ 708.34464268    0.38700002    0.48610002    0.39550003    0.36270005]
[37m[1m [1542.28503966    0.39471316    0.26576152    0.10287912    0.30552745]]
[37m[1m[2023-06-25 06:46:45,201][129146] Max Reward on eval: 3781.4564760001376
[37m[1m[2023-06-25 06:46:45,202][129146] Min Reward on eval: -1137.3969313987764
[37m[1m[2023-06-25 06:46:45,202][129146] Mean Reward across all agents: 790.7427763079529
[37m[1m[2023-06-25 06:46:45,202][129146] Average Trajectory Length: 891.2553333333333
[36m[2023-06-25 06:46:45,204][129146] mean_value=-1961.5923240468233, max_value=1127.3650585447033
[37m[1m[2023-06-25 06:46:45,206][129146] New mean coefficients: [[ 3.380021    0.19500732 -1.5225924  -0.75523853  0.26317987]]
[37m[1m[2023-06-25 06:46:45,207][129146] Moving the mean solution point...
[36m[2023-06-25 06:46:55,016][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 06:46:55,017][129146] FPS: 391535.29
[36m[2023-06-25 06:46:55,019][129146] itr=621, itrs=2000, Progress: 31.05%
[36m[2023-06-25 06:47:06,628][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 06:47:06,628][129146] FPS: 331324.44
[36m[2023-06-25 06:47:11,405][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:47:11,406][129146] Reward + Measures: [[3683.59798932    0.25707805    0.25527009    0.05053189    0.28270027]]
[37m[1m[2023-06-25 06:47:11,406][129146] Max Reward on eval: 3683.597989316723
[37m[1m[2023-06-25 06:47:11,406][129146] Min Reward on eval: 3683.597989316723
[37m[1m[2023-06-25 06:47:11,406][129146] Mean Reward across all agents: 3683.597989316723
[37m[1m[2023-06-25 06:47:11,406][129146] Average Trajectory Length: 942.7973333333333
[36m[2023-06-25 06:47:17,090][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:47:17,091][129146] Reward + Measures: [[ 778.46061614    0.493         0.38030002    0.2696        0.33220002]
[37m[1m [  20.47777801    0.28550002    0.34999999    0.22789998    0.31240001]
[37m[1m [ 993.20159265    0.45959997    0.3874        0.37870002    0.35880002]
[37m[1m ...
[37m[1m [ 586.57623733    0.50050002    0.52250004    0.47989997    0.49950004]
[37m[1m [2391.53863289    0.35880002    0.32650003    0.1099        0.29679999]
[37m[1m [2166.46875783    0.2494        0.2879        0.08310001    0.22120002]]
[37m[1m[2023-06-25 06:47:17,091][129146] Max Reward on eval: 3916.039643905219
[37m[1m[2023-06-25 06:47:17,091][129146] Min Reward on eval: -377.2596649158222
[37m[1m[2023-06-25 06:47:17,091][129146] Mean Reward across all agents: 1179.1540660490011
[37m[1m[2023-06-25 06:47:17,092][129146] Average Trajectory Length: 956.3076666666666
[36m[2023-06-25 06:47:17,094][129146] mean_value=-1953.5794772192887, max_value=1002.8629528136928
[37m[1m[2023-06-25 06:47:17,096][129146] New mean coefficients: [[ 3.6783848   0.9885294  -0.5962738  -0.60898626  0.46101373]]
[37m[1m[2023-06-25 06:47:17,097][129146] Moving the mean solution point...
[36m[2023-06-25 06:47:26,947][129146] train() took 9.85 seconds to complete
[36m[2023-06-25 06:47:26,947][129146] FPS: 389927.07
[36m[2023-06-25 06:47:26,949][129146] itr=622, itrs=2000, Progress: 31.10%
[36m[2023-06-25 06:47:38,508][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 06:47:38,508][129146] FPS: 332771.12
[36m[2023-06-25 06:47:43,290][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:47:43,290][129146] Reward + Measures: [[1701.08142598    0.19314916    0.3080374     0.14221865    0.20992351]]
[37m[1m[2023-06-25 06:47:43,290][129146] Max Reward on eval: 1701.0814259794379
[37m[1m[2023-06-25 06:47:43,291][129146] Min Reward on eval: 1701.0814259794379
[37m[1m[2023-06-25 06:47:43,291][129146] Mean Reward across all agents: 1701.0814259794379
[37m[1m[2023-06-25 06:47:43,291][129146] Average Trajectory Length: 872.5703333333333
[36m[2023-06-25 06:47:48,814][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:47:48,814][129146] Reward + Measures: [[1961.47164294    0.19502221    0.28196511    0.0894111     0.24085239]
[37m[1m [2205.24694814    0.24867988    0.30529389    0.11007258    0.21639843]
[37m[1m [1629.36910339    0.2152494     0.29181793    0.12327268    0.22284697]
[37m[1m ...
[37m[1m [1804.51419291    0.2056412     0.29970774    0.11398128    0.23360839]
[37m[1m [1926.4383833     0.24115179    0.31736133    0.15713891    0.23353937]
[37m[1m [-504.53297783    0.31590834    0.26381236    0.29588291    0.26931134]]
[37m[1m[2023-06-25 06:47:48,814][129146] Max Reward on eval: 2967.0875448734964
[37m[1m[2023-06-25 06:47:48,815][129146] Min Reward on eval: -504.5329778336687
[37m[1m[2023-06-25 06:47:48,815][129146] Mean Reward across all agents: 1621.228477804256
[37m[1m[2023-06-25 06:47:48,815][129146] Average Trajectory Length: 898.7976666666666
[36m[2023-06-25 06:47:48,817][129146] mean_value=-1444.1107210979642, max_value=766.6688653135639
[37m[1m[2023-06-25 06:47:48,819][129146] New mean coefficients: [[ 3.6830761   1.5127251  -0.2676025  -0.60820585  0.91782844]]
[37m[1m[2023-06-25 06:47:48,820][129146] Moving the mean solution point...
[36m[2023-06-25 06:47:58,536][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 06:47:58,536][129146] FPS: 395315.71
[36m[2023-06-25 06:47:58,538][129146] itr=623, itrs=2000, Progress: 31.15%
[36m[2023-06-25 06:48:10,032][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 06:48:10,032][129146] FPS: 334648.34
[36m[2023-06-25 06:48:14,872][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:48:14,872][129146] Reward + Measures: [[1947.14669816    0.20296232    0.31276944    0.14016318    0.22258791]]
[37m[1m[2023-06-25 06:48:14,872][129146] Max Reward on eval: 1947.1466981648744
[37m[1m[2023-06-25 06:48:14,872][129146] Min Reward on eval: 1947.1466981648744
[37m[1m[2023-06-25 06:48:14,873][129146] Mean Reward across all agents: 1947.1466981648744
[37m[1m[2023-06-25 06:48:14,873][129146] Average Trajectory Length: 893.2073333333333
[36m[2023-06-25 06:48:20,386][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:48:20,387][129146] Reward + Measures: [[ 650.8224181     0.25556353    0.31107041    0.2416812     0.26023126]
[37m[1m [1994.43507199    0.21308115    0.302443      0.14025412    0.22588196]
[37m[1m [1925.7427719     0.20321861    0.31446177    0.149047      0.22221529]
[37m[1m ...
[37m[1m [-332.63246531    0.34899998    0.13360001    0.25319999    0.2023    ]
[37m[1m [1176.92148329    0.22803989    0.30698824    0.19090982    0.22209965]
[37m[1m [-102.89725273    0.47815832    0.23854999    0.40422502    0.16928333]]
[37m[1m[2023-06-25 06:48:20,387][129146] Max Reward on eval: 2491.7097170346415
[37m[1m[2023-06-25 06:48:20,387][129146] Min Reward on eval: -332.6324653099029
[37m[1m[2023-06-25 06:48:20,387][129146] Mean Reward across all agents: 918.2055226100111
[37m[1m[2023-06-25 06:48:20,388][129146] Average Trajectory Length: 922.1176666666667
[36m[2023-06-25 06:48:20,389][129146] mean_value=-1327.6660162661506, max_value=499.41686855896705
[37m[1m[2023-06-25 06:48:20,392][129146] New mean coefficients: [[ 3.2985106   1.193162   -0.5048318  -0.628182    0.77530843]]
[37m[1m[2023-06-25 06:48:20,393][129146] Moving the mean solution point...
[36m[2023-06-25 06:48:30,045][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 06:48:30,045][129146] FPS: 397904.22
[36m[2023-06-25 06:48:30,047][129146] itr=624, itrs=2000, Progress: 31.20%
[36m[2023-06-25 06:48:41,484][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 06:48:41,484][129146] FPS: 336408.62
[36m[2023-06-25 06:48:46,264][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:48:46,264][129146] Reward + Measures: [[2189.70601115    0.20689483    0.31528324    0.13491105    0.23154277]]
[37m[1m[2023-06-25 06:48:46,264][129146] Max Reward on eval: 2189.7060111480423
[37m[1m[2023-06-25 06:48:46,265][129146] Min Reward on eval: 2189.7060111480423
[37m[1m[2023-06-25 06:48:46,265][129146] Mean Reward across all agents: 2189.7060111480423
[37m[1m[2023-06-25 06:48:46,265][129146] Average Trajectory Length: 925.5813333333333
[36m[2023-06-25 06:48:51,660][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:48:51,661][129146] Reward + Measures: [[1337.57557656    0.2273445     0.30271757    0.12112826    0.23217392]
[37m[1m [1080.45495137    0.21886678    0.26950535    0.13591485    0.24512951]
[37m[1m [ 803.89293311    0.23616754    0.31087607    0.17252992    0.23321453]
[37m[1m ...
[37m[1m [1836.81738471    0.22636452    0.32382581    0.13336129    0.23109356]
[37m[1m [1185.46203065    0.16414528    0.25742441    0.10977764    0.15670288]
[37m[1m [ 935.14605814    0.28890297    0.49129382    0.10509197    0.40842256]]
[37m[1m[2023-06-25 06:48:51,661][129146] Max Reward on eval: 2675.8086280873977
[37m[1m[2023-06-25 06:48:51,661][129146] Min Reward on eval: -1009.7246862329309
[37m[1m[2023-06-25 06:48:51,661][129146] Mean Reward across all agents: 1365.8696692107208
[37m[1m[2023-06-25 06:48:51,662][129146] Average Trajectory Length: 940.7646666666666
[36m[2023-06-25 06:48:51,664][129146] mean_value=-1777.5035496536161, max_value=1304.915643708157
[37m[1m[2023-06-25 06:48:51,666][129146] New mean coefficients: [[ 3.2378604   1.2192504  -0.2512755  -0.48909512  0.8079464 ]]
[37m[1m[2023-06-25 06:48:51,667][129146] Moving the mean solution point...
[36m[2023-06-25 06:49:01,414][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 06:49:01,414][129146] FPS: 394040.44
[36m[2023-06-25 06:49:01,416][129146] itr=625, itrs=2000, Progress: 31.25%
[36m[2023-06-25 06:49:12,953][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 06:49:12,954][129146] FPS: 333375.23
[36m[2023-06-25 06:49:17,796][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:49:17,796][129146] Reward + Measures: [[2378.76103748    0.21347056    0.31673145    0.12725882    0.24149288]]
[37m[1m[2023-06-25 06:49:17,797][129146] Max Reward on eval: 2378.7610374834044
[37m[1m[2023-06-25 06:49:17,797][129146] Min Reward on eval: 2378.7610374834044
[37m[1m[2023-06-25 06:49:17,797][129146] Mean Reward across all agents: 2378.7610374834044
[37m[1m[2023-06-25 06:49:17,798][129146] Average Trajectory Length: 933.097
[36m[2023-06-25 06:49:23,288][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:49:23,289][129146] Reward + Measures: [[2155.96652453    0.20469999    0.2687        0.10380001    0.2113    ]
[37m[1m [2197.98576219    0.19262326    0.27269536    0.12373023    0.22942558]
[37m[1m [1363.79534104    0.23259853    0.30147895    0.13216464    0.24284294]
[37m[1m ...
[37m[1m [2022.64848293    0.22097547    0.30106229    0.12997738    0.24998868]
[37m[1m [2643.39844815    0.2139        0.30310002    0.1279        0.25350001]
[37m[1m [1989.20693807    0.24099742    0.30381933    0.13339111    0.24414483]]
[37m[1m[2023-06-25 06:49:23,289][129146] Max Reward on eval: 2948.9757181925584
[37m[1m[2023-06-25 06:49:23,290][129146] Min Reward on eval: 749.6349585945136
[37m[1m[2023-06-25 06:49:23,290][129146] Mean Reward across all agents: 2046.2247146397735
[37m[1m[2023-06-25 06:49:23,290][129146] Average Trajectory Length: 874.7333333333333
[36m[2023-06-25 06:49:23,292][129146] mean_value=-1566.1113390821556, max_value=494.74535541615114
[37m[1m[2023-06-25 06:49:23,294][129146] New mean coefficients: [[ 2.7598069   1.0960398  -0.01880306 -0.3906785   0.7191649 ]]
[37m[1m[2023-06-25 06:49:23,295][129146] Moving the mean solution point...
[36m[2023-06-25 06:49:33,105][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 06:49:33,105][129146] FPS: 391523.26
[36m[2023-06-25 06:49:33,107][129146] itr=626, itrs=2000, Progress: 31.30%
[36m[2023-06-25 06:49:44,710][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 06:49:44,711][129146] FPS: 331555.17
[36m[2023-06-25 06:49:49,661][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:49:49,661][129146] Reward + Measures: [[2620.75801093    0.22002546    0.32018685    0.12491557    0.25298575]]
[37m[1m[2023-06-25 06:49:49,662][129146] Max Reward on eval: 2620.7580109317832
[37m[1m[2023-06-25 06:49:49,662][129146] Min Reward on eval: 2620.7580109317832
[37m[1m[2023-06-25 06:49:49,662][129146] Mean Reward across all agents: 2620.7580109317832
[37m[1m[2023-06-25 06:49:49,662][129146] Average Trajectory Length: 945.5016666666667
[36m[2023-06-25 06:49:55,191][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:49:55,191][129146] Reward + Measures: [[2545.53138868    0.20532699    0.30142027    0.12568282    0.24717057]
[37m[1m [1035.40740835    0.25411564    0.26600865    0.14499387    0.21792555]
[37m[1m [2485.91848581    0.23277245    0.32884508    0.12048992    0.28600436]
[37m[1m ...
[37m[1m [1966.22441774    0.20679508    0.2740083     0.13617431    0.23569031]
[37m[1m [2413.83773205    0.21985903    0.30921814    0.13526909    0.27246845]
[37m[1m [2637.71180158    0.23779245    0.34055296    0.12028404    0.25033027]]
[37m[1m[2023-06-25 06:49:55,191][129146] Max Reward on eval: 3165.060276033962
[37m[1m[2023-06-25 06:49:55,192][129146] Min Reward on eval: 221.89805457674083
[37m[1m[2023-06-25 06:49:55,192][129146] Mean Reward across all agents: 1667.13953976961
[37m[1m[2023-06-25 06:49:55,192][129146] Average Trajectory Length: 827.2286666666666
[36m[2023-06-25 06:49:55,194][129146] mean_value=-1448.7255782081736, max_value=833.6263062461794
[37m[1m[2023-06-25 06:49:55,196][129146] New mean coefficients: [[ 2.8418682   1.1575592   0.02612689 -0.3696207   0.5363844 ]]
[37m[1m[2023-06-25 06:49:55,197][129146] Moving the mean solution point...
[36m[2023-06-25 06:50:04,994][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 06:50:04,994][129146] FPS: 392032.76
[36m[2023-06-25 06:50:04,996][129146] itr=627, itrs=2000, Progress: 31.35%
[36m[2023-06-25 06:50:16,619][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 06:50:16,619][129146] FPS: 330922.46
[36m[2023-06-25 06:50:21,356][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:50:21,357][129146] Reward + Measures: [[2777.07303837    0.22224572    0.321206      0.11551542    0.25914961]]
[37m[1m[2023-06-25 06:50:21,357][129146] Max Reward on eval: 2777.0730383663085
[37m[1m[2023-06-25 06:50:21,357][129146] Min Reward on eval: 2777.0730383663085
[37m[1m[2023-06-25 06:50:21,357][129146] Mean Reward across all agents: 2777.0730383663085
[37m[1m[2023-06-25 06:50:21,358][129146] Average Trajectory Length: 959.39
[36m[2023-06-25 06:50:26,742][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:50:26,747][129146] Reward + Measures: [[2947.0386737     0.24590002    0.35490003    0.10399999    0.29609999]
[37m[1m [2599.15506887    0.23470907    0.31459698    0.13916667    0.26368791]
[37m[1m [1312.21299564    0.19195616    0.25899869    0.16103719    0.21790326]
[37m[1m ...
[37m[1m [2843.64698267    0.24730423    0.35172662    0.12693933    0.29406977]
[37m[1m [2015.79445221    0.22361322    0.32501322    0.15582292    0.27315986]
[37m[1m [2653.33843432    0.21970001    0.32519999    0.114         0.2529    ]]
[37m[1m[2023-06-25 06:50:26,748][129146] Max Reward on eval: 3194.745526872773
[37m[1m[2023-06-25 06:50:26,748][129146] Min Reward on eval: -25.914255490805953
[37m[1m[2023-06-25 06:50:26,748][129146] Mean Reward across all agents: 2316.1359482549365
[37m[1m[2023-06-25 06:50:26,749][129146] Average Trajectory Length: 936.3386666666667
[36m[2023-06-25 06:50:26,751][129146] mean_value=-1681.2691515256658, max_value=1581.3491859064684
[37m[1m[2023-06-25 06:50:26,753][129146] New mean coefficients: [[ 2.830395    1.371747   -0.18457414 -0.45200372  0.840758  ]]
[37m[1m[2023-06-25 06:50:26,754][129146] Moving the mean solution point...
[36m[2023-06-25 06:50:36,325][129146] train() took 9.57 seconds to complete
[36m[2023-06-25 06:50:36,325][129146] FPS: 401277.39
[36m[2023-06-25 06:50:36,328][129146] itr=628, itrs=2000, Progress: 31.40%
[36m[2023-06-25 06:50:47,843][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 06:50:47,844][129146] FPS: 333994.44
[36m[2023-06-25 06:50:52,583][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:50:52,583][129146] Reward + Measures: [[2983.21786732    0.22234385    0.32153153    0.11241915    0.2629146 ]]
[37m[1m[2023-06-25 06:50:52,583][129146] Max Reward on eval: 2983.217867324579
[37m[1m[2023-06-25 06:50:52,584][129146] Min Reward on eval: 2983.217867324579
[37m[1m[2023-06-25 06:50:52,584][129146] Mean Reward across all agents: 2983.217867324579
[37m[1m[2023-06-25 06:50:52,584][129146] Average Trajectory Length: 972.9343333333333
[36m[2023-06-25 06:50:58,022][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:50:58,027][129146] Reward + Measures: [[1862.93019883    0.17311262    0.30623892    0.14413109    0.19074701]
[37m[1m [2836.59015886    0.1974        0.33249998    0.1146        0.28400001]
[37m[1m [2197.65214699    0.2086        0.32070002    0.133         0.21009998]
[37m[1m ...
[37m[1m [2496.18922045    0.19660001    0.29420003    0.0991        0.22220002]
[37m[1m [1932.45372613    0.1652        0.28372502    0.1142        0.17765   ]
[37m[1m [2757.85023561    0.2241898     0.32543468    0.11082381    0.26429114]]
[37m[1m[2023-06-25 06:50:58,027][129146] Max Reward on eval: 3232.396380880219
[37m[1m[2023-06-25 06:50:58,028][129146] Min Reward on eval: 1118.6783399271633
[37m[1m[2023-06-25 06:50:58,028][129146] Mean Reward across all agents: 2326.9498447015694
[37m[1m[2023-06-25 06:50:58,028][129146] Average Trajectory Length: 949.0366666666666
[36m[2023-06-25 06:50:58,030][129146] mean_value=-1006.5051845329356, max_value=1132.0831390983476
[37m[1m[2023-06-25 06:50:58,032][129146] New mean coefficients: [[ 3.196447    1.3912352  -0.07326297 -0.3615932   0.82997954]]
[37m[1m[2023-06-25 06:50:58,034][129146] Moving the mean solution point...
[36m[2023-06-25 06:51:07,712][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 06:51:07,712][129146] FPS: 396817.67
[36m[2023-06-25 06:51:07,715][129146] itr=629, itrs=2000, Progress: 31.45%
[36m[2023-06-25 06:51:19,176][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 06:51:19,176][129146] FPS: 335597.06
[36m[2023-06-25 06:51:23,901][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:51:23,901][129146] Reward + Measures: [[3080.20535801    0.22250733    0.32464692    0.11688612    0.2649993 ]]
[37m[1m[2023-06-25 06:51:23,901][129146] Max Reward on eval: 3080.20535800795
[37m[1m[2023-06-25 06:51:23,901][129146] Min Reward on eval: 3080.20535800795
[37m[1m[2023-06-25 06:51:23,902][129146] Mean Reward across all agents: 3080.20535800795
[37m[1m[2023-06-25 06:51:23,902][129146] Average Trajectory Length: 971.5996666666666
[36m[2023-06-25 06:51:29,246][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:51:29,246][129146] Reward + Measures: [[2122.88896842    0.23970996    0.31314978    0.11208413    0.31327975]
[37m[1m [2627.73428956    0.22347037    0.31973335    0.11539259    0.32694814]
[37m[1m [1844.295504      0.25180003    0.35700002    0.1044        0.4014    ]
[37m[1m ...
[37m[1m [2422.83324715    0.21540001    0.32590002    0.0979        0.29589999]
[37m[1m [2139.52680163    0.3272        0.3048        0.2076        0.37110001]
[37m[1m [1754.71783564    0.27489999    0.3389        0.1426        0.39390001]]
[37m[1m[2023-06-25 06:51:29,247][129146] Max Reward on eval: 3376.8309959173666
[37m[1m[2023-06-25 06:51:29,247][129146] Min Reward on eval: 156.7996851013566
[37m[1m[2023-06-25 06:51:29,247][129146] Mean Reward across all agents: 2159.780582081812
[37m[1m[2023-06-25 06:51:29,247][129146] Average Trajectory Length: 968.2653333333333
[36m[2023-06-25 06:51:29,249][129146] mean_value=-1608.275782218187, max_value=894.4735599679907
[37m[1m[2023-06-25 06:51:29,251][129146] New mean coefficients: [[ 3.1234326   0.8488216  -0.10589793 -0.1884177   0.41651788]]
[37m[1m[2023-06-25 06:51:29,252][129146] Moving the mean solution point...
[36m[2023-06-25 06:51:38,853][129146] train() took 9.60 seconds to complete
[36m[2023-06-25 06:51:38,854][129146] FPS: 400020.43
[36m[2023-06-25 06:51:38,856][129146] itr=630, itrs=2000, Progress: 31.50%
[37m[1m[2023-06-25 06:51:43,925][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000610
[36m[2023-06-25 06:51:55,726][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 06:51:55,726][129146] FPS: 335941.59
[36m[2023-06-25 06:52:00,452][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:52:00,452][129146] Reward + Measures: [[3195.59096254    0.22303988    0.3220754     0.11414543    0.27081364]]
[37m[1m[2023-06-25 06:52:00,452][129146] Max Reward on eval: 3195.59096254395
[37m[1m[2023-06-25 06:52:00,453][129146] Min Reward on eval: 3195.59096254395
[37m[1m[2023-06-25 06:52:00,453][129146] Mean Reward across all agents: 3195.59096254395
[37m[1m[2023-06-25 06:52:00,453][129146] Average Trajectory Length: 967.593
[36m[2023-06-25 06:52:05,960][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:52:05,961][129146] Reward + Measures: [[3196.26444349    0.24336629    0.32500789    0.10808314    0.28963706]
[37m[1m [2752.11504469    0.2613875     0.2972261     0.09756574    0.27323672]
[37m[1m [3371.52786001    0.23669998    0.33380002    0.1134        0.27900001]
[37m[1m ...
[37m[1m [2910.28807427    0.22479999    0.29920003    0.106         0.24919999]
[37m[1m [3322.0728651     0.22818391    0.31585723    0.10931005    0.28339368]
[37m[1m [3340.40995609    0.23070002    0.31470001    0.1128        0.28119999]]
[37m[1m[2023-06-25 06:52:05,961][129146] Max Reward on eval: 3511.7849432277494
[37m[1m[2023-06-25 06:52:05,961][129146] Min Reward on eval: 1893.3880799284293
[37m[1m[2023-06-25 06:52:05,961][129146] Mean Reward across all agents: 3001.6688177727433
[37m[1m[2023-06-25 06:52:05,962][129146] Average Trajectory Length: 958.2456666666666
[36m[2023-06-25 06:52:05,963][129146] mean_value=-1070.6477213259648, max_value=-23.239465017183647
[36m[2023-06-25 06:52:05,965][129146] XNES is restarting with a new solution whose measures are [0.25280002 0.89529991 0.72350001 0.88210005] and objective is -121.68729432150721
[36m[2023-06-25 06:52:05,966][129146] Emitter restarted. Changing the mean agent...
[37m[1m[2023-06-25 06:52:05,969][129146] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]]
[37m[1m[2023-06-25 06:52:05,969][129146] Moving the mean solution point...
[36m[2023-06-25 06:52:15,596][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 06:52:15,596][129146] FPS: 398949.39
[36m[2023-06-25 06:52:15,599][129146] itr=631, itrs=2000, Progress: 31.55%
[36m[2023-06-25 06:52:27,355][129146] train() took 11.74 seconds to complete
[36m[2023-06-25 06:52:27,355][129146] FPS: 327146.71
[36m[2023-06-25 06:52:32,165][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:52:32,166][129146] Reward + Measures: [[-168.03012787    0.11673761    0.86030191    0.7191661     0.82826912]]
[37m[1m[2023-06-25 06:52:32,166][129146] Max Reward on eval: -168.030127865611
[37m[1m[2023-06-25 06:52:32,166][129146] Min Reward on eval: -168.030127865611
[37m[1m[2023-06-25 06:52:32,167][129146] Mean Reward across all agents: -168.030127865611
[37m[1m[2023-06-25 06:52:32,167][129146] Average Trajectory Length: 999.084
[36m[2023-06-25 06:52:37,826][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:52:37,826][129146] Reward + Measures: [[ -51.65907043    0.4283483     0.30906782    0.1767115     0.34225631]
[37m[1m [-811.1593253     0.38892061    0.1973805     0.28196535    0.14848451]
[37m[1m [-676.64795883    0.30905542    0.18613997    0.25224024    0.22957866]
[37m[1m ...
[37m[1m [-610.09606379    0.49618101    0.26646167    0.37426063    0.2412266 ]
[37m[1m [-839.44561914    0.40561628    0.19553033    0.30269501    0.17351857]
[37m[1m [-705.33357848    0.2066981     0.2083206     0.18066414    0.23711196]]
[37m[1m[2023-06-25 06:52:37,826][129146] Max Reward on eval: 117.55252684531733
[37m[1m[2023-06-25 06:52:37,827][129146] Min Reward on eval: -1954.0241475539515
[37m[1m[2023-06-25 06:52:37,827][129146] Mean Reward across all agents: -692.7787623109768
[37m[1m[2023-06-25 06:52:37,827][129146] Average Trajectory Length: 869.8006666666666
[36m[2023-06-25 06:52:37,829][129146] mean_value=-1425.9417119064074, max_value=473.205725366337
[37m[1m[2023-06-25 06:52:37,832][129146] New mean coefficients: [[-0.14894322 -0.90981656 -1.0607077  -0.23825753 -0.46897054]]
[37m[1m[2023-06-25 06:52:37,833][129146] Moving the mean solution point...
[36m[2023-06-25 06:52:47,617][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 06:52:47,618][129146] FPS: 392532.26
[36m[2023-06-25 06:52:47,620][129146] itr=632, itrs=2000, Progress: 31.60%
[36m[2023-06-25 06:52:59,315][129146] train() took 11.68 seconds to complete
[36m[2023-06-25 06:52:59,315][129146] FPS: 328891.07
[36m[2023-06-25 06:53:04,123][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:53:04,124][129146] Reward + Measures: [[-136.10617965    0.10492008    0.86051285    0.72419155    0.83141196]]
[37m[1m[2023-06-25 06:53:04,124][129146] Max Reward on eval: -136.10617965126792
[37m[1m[2023-06-25 06:53:04,124][129146] Min Reward on eval: -136.10617965126792
[37m[1m[2023-06-25 06:53:04,124][129146] Mean Reward across all agents: -136.10617965126792
[37m[1m[2023-06-25 06:53:04,124][129146] Average Trajectory Length: 999.4006666666667
[36m[2023-06-25 06:53:09,573][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:53:09,574][129146] Reward + Measures: [[ -690.46411034     0.13600001     0.7726         0.66140002
[37m[1m      0.74069995]
[37m[1m [ -662.75465224     0.39908004     0.31638724     0.31969157
[37m[1m      0.22292303]
[37m[1m [  -32.28579351     0.38439998     0.37810001     0.117
[37m[1m      0.41350004]
[37m[1m ...
[37m[1m [ -922.75352885     0.38955745     0.42907125     0.19258223
[37m[1m      0.42143998]
[37m[1m [-1384.56150207     0.4778516      0.48260942     0.47705474
[37m[1m      0.08833899]
[37m[1m [  150.46001903     0.51719999     0.66800004     0.1236
[37m[1m      0.60100001]]
[37m[1m[2023-06-25 06:53:09,574][129146] Max Reward on eval: 918.957266012812
[37m[1m[2023-06-25 06:53:09,574][129146] Min Reward on eval: -1685.5270710719983
[37m[1m[2023-06-25 06:53:09,575][129146] Mean Reward across all agents: -480.7120410695536
[37m[1m[2023-06-25 06:53:09,575][129146] Average Trajectory Length: 963.697
[36m[2023-06-25 06:53:09,578][129146] mean_value=-1382.5947532235673, max_value=555.3086185114807
[37m[1m[2023-06-25 06:53:09,581][129146] New mean coefficients: [[ 0.36520877  0.6799051  -0.97407913  0.35730463 -0.59834194]]
[37m[1m[2023-06-25 06:53:09,582][129146] Moving the mean solution point...
[36m[2023-06-25 06:53:19,444][129146] train() took 9.86 seconds to complete
[36m[2023-06-25 06:53:19,445][129146] FPS: 389425.71
[36m[2023-06-25 06:53:19,447][129146] itr=633, itrs=2000, Progress: 31.65%
[36m[2023-06-25 06:53:31,185][129146] train() took 11.72 seconds to complete
[36m[2023-06-25 06:53:31,186][129146] FPS: 327664.57
[36m[2023-06-25 06:53:35,979][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:53:35,979][129146] Reward + Measures: [[-123.50644461    0.11820349    0.83719981    0.69301385    0.80427814]]
[37m[1m[2023-06-25 06:53:35,979][129146] Max Reward on eval: -123.50644461477987
[37m[1m[2023-06-25 06:53:35,979][129146] Min Reward on eval: -123.50644461477987
[37m[1m[2023-06-25 06:53:35,980][129146] Mean Reward across all agents: -123.50644461477987
[37m[1m[2023-06-25 06:53:35,980][129146] Average Trajectory Length: 999.6986666666667
[36m[2023-06-25 06:53:41,486][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:53:41,487][129146] Reward + Measures: [[-500.16641968    0.37477803    0.36610708    0.29417801    0.30577022]
[37m[1m [-221.92565167    0.23000002    0.59960002    0.39359999    0.52780002]
[37m[1m [-354.85091663    0.34224781    0.35242173    0.25375652    0.28156957]
[37m[1m ...
[37m[1m [-667.34100983    0.21606421    0.42134115    0.38318354    0.44256791]
[37m[1m [-997.55569844    0.22847863    0.405195      0.34321293    0.39437816]
[37m[1m [ -56.82722695    0.39930001    0.49829999    0.32519999    0.4323    ]]
[37m[1m[2023-06-25 06:53:41,487][129146] Max Reward on eval: 212.6465779219754
[37m[1m[2023-06-25 06:53:41,487][129146] Min Reward on eval: -1130.6284016106395
[37m[1m[2023-06-25 06:53:41,487][129146] Mean Reward across all agents: -283.6832776081915
[37m[1m[2023-06-25 06:53:41,488][129146] Average Trajectory Length: 985.6966666666666
[36m[2023-06-25 06:53:41,493][129146] mean_value=-935.1844479090736, max_value=570.8127605713854
[37m[1m[2023-06-25 06:53:41,496][129146] New mean coefficients: [[ 0.6292715   0.5076077  -0.8498747   1.3505702  -0.35241178]]
[37m[1m[2023-06-25 06:53:41,497][129146] Moving the mean solution point...
[36m[2023-06-25 06:53:51,221][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 06:53:51,221][129146] FPS: 394980.94
[36m[2023-06-25 06:53:51,223][129146] itr=634, itrs=2000, Progress: 31.70%
[36m[2023-06-25 06:54:02,727][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 06:54:02,727][129146] FPS: 334468.55
[36m[2023-06-25 06:54:07,513][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:54:07,514][129146] Reward + Measures: [[-122.62006462    0.1014718     0.83784676    0.73073965    0.81387663]]
[37m[1m[2023-06-25 06:54:07,514][129146] Max Reward on eval: -122.62006462388881
[37m[1m[2023-06-25 06:54:07,514][129146] Min Reward on eval: -122.62006462388881
[37m[1m[2023-06-25 06:54:07,514][129146] Mean Reward across all agents: -122.62006462388881
[37m[1m[2023-06-25 06:54:07,514][129146] Average Trajectory Length: 998.5376666666666
[36m[2023-06-25 06:54:12,950][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:54:12,951][129146] Reward + Measures: [[-221.40432489    0.4152        0.88920003    0.66070002    0.8757    ]
[37m[1m [-181.99888759    0.36628982    0.42850408    0.33131024    0.45377144]
[37m[1m [-519.1686535     0.22792141    0.12581912    0.12387039    0.12685363]
[37m[1m ...
[37m[1m [  61.85943504    0.2938        0.61240005    0.40349999    0.62760001]
[37m[1m [  30.50144905    0.31986138    0.23480545    0.20248233    0.20043483]
[37m[1m [-264.75693454    0.1727        0.36460003    0.266         0.30440003]]
[37m[1m[2023-06-25 06:54:12,951][129146] Max Reward on eval: 297.7408847160579
[37m[1m[2023-06-25 06:54:12,951][129146] Min Reward on eval: -1281.1032621292863
[37m[1m[2023-06-25 06:54:12,952][129146] Mean Reward across all agents: -295.4225074178171
[37m[1m[2023-06-25 06:54:12,952][129146] Average Trajectory Length: 970.242
[36m[2023-06-25 06:54:12,956][129146] mean_value=-997.1626145531542, max_value=488.1172916698764
[37m[1m[2023-06-25 06:54:12,958][129146] New mean coefficients: [[ 0.21418983 -0.36709553 -0.7733895   1.8854408   0.14071518]]
[37m[1m[2023-06-25 06:54:12,959][129146] Moving the mean solution point...
[36m[2023-06-25 06:54:22,729][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 06:54:22,729][129146] FPS: 393129.35
[36m[2023-06-25 06:54:22,731][129146] itr=635, itrs=2000, Progress: 31.75%
[36m[2023-06-25 06:54:34,258][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 06:54:34,259][129146] FPS: 333780.82
[36m[2023-06-25 06:54:39,043][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:54:39,043][129146] Reward + Measures: [[-104.5814425     0.08754891    0.83871955    0.75664276    0.81618327]]
[37m[1m[2023-06-25 06:54:39,043][129146] Max Reward on eval: -104.58144250337047
[37m[1m[2023-06-25 06:54:39,044][129146] Min Reward on eval: -104.58144250337047
[37m[1m[2023-06-25 06:54:39,044][129146] Mean Reward across all agents: -104.58144250337047
[37m[1m[2023-06-25 06:54:39,044][129146] Average Trajectory Length: 998.5106666666667
[36m[2023-06-25 06:54:44,468][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:54:44,469][129146] Reward + Measures: [[ -97.54117848    0.29340002    0.63900006    0.5205        0.62270004]
[37m[1m [-270.80437104    0.35840002    0.229         0.2678        0.11600001]
[37m[1m [-112.02055074    0.09730887    0.80078477    0.74747723    0.79498237]
[37m[1m ...
[37m[1m [-178.91922994    0.0962        0.84770006    0.73470002    0.84279996]
[37m[1m [-197.01592686    0.30710003    0.26000002    0.23670001    0.26199999]
[37m[1m [   9.35207412    0.30930933    0.29745683    0.20970061    0.27545235]]
[37m[1m[2023-06-25 06:54:44,469][129146] Max Reward on eval: 181.65516654901438
[37m[1m[2023-06-25 06:54:44,469][129146] Min Reward on eval: -878.2887481367331
[37m[1m[2023-06-25 06:54:44,470][129146] Mean Reward across all agents: -213.2934763802713
[37m[1m[2023-06-25 06:54:44,470][129146] Average Trajectory Length: 983.958
[36m[2023-06-25 06:54:44,475][129146] mean_value=-502.1336528524085, max_value=536.7358446116393
[37m[1m[2023-06-25 06:54:44,478][129146] New mean coefficients: [[ 0.01660277 -0.8349544  -1.3422334   2.0180619   0.09063815]]
[37m[1m[2023-06-25 06:54:44,479][129146] Moving the mean solution point...
[36m[2023-06-25 06:54:54,380][129146] train() took 9.90 seconds to complete
[36m[2023-06-25 06:54:54,380][129146] FPS: 387900.57
[36m[2023-06-25 06:54:54,383][129146] itr=636, itrs=2000, Progress: 31.80%
[36m[2023-06-25 06:55:05,946][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 06:55:05,946][129146] FPS: 332791.54
[36m[2023-06-25 06:55:10,717][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:55:10,718][129146] Reward + Measures: [[-110.35190256    0.06971805    0.8358525     0.79971355    0.82325852]]
[37m[1m[2023-06-25 06:55:10,718][129146] Max Reward on eval: -110.35190255929153
[37m[1m[2023-06-25 06:55:10,718][129146] Min Reward on eval: -110.35190255929153
[37m[1m[2023-06-25 06:55:10,718][129146] Mean Reward across all agents: -110.35190255929153
[37m[1m[2023-06-25 06:55:10,719][129146] Average Trajectory Length: 999.1046666666666
[36m[2023-06-25 06:55:16,266][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:55:16,267][129146] Reward + Measures: [[-711.34666986    0.40984273    0.36610579    0.3589184     0.30698565]
[37m[1m [-510.565683      0.60458994    0.46701208    0.56380409    0.50286311]
[37m[1m [-388.03027147    0.4799467     0.33921903    0.4435626     0.21396838]
[37m[1m ...
[37m[1m [ -73.82111819    0.48090002    0.70500004    0.68289995    0.48780003]
[37m[1m [-928.71233394    0.77139997    0.1605        0.6649        0.65609998]
[37m[1m [ -77.13023663    0.44910002    0.60149997    0.65710002    0.50089997]]
[37m[1m[2023-06-25 06:55:16,267][129146] Max Reward on eval: 323.93728257195323
[37m[1m[2023-06-25 06:55:16,268][129146] Min Reward on eval: -1103.6694111027289
[37m[1m[2023-06-25 06:55:16,268][129146] Mean Reward across all agents: -239.4008830250273
[37m[1m[2023-06-25 06:55:16,268][129146] Average Trajectory Length: 976.8109999999999
[36m[2023-06-25 06:55:16,273][129146] mean_value=-770.3752252373876, max_value=555.2059139438206
[37m[1m[2023-06-25 06:55:16,276][129146] New mean coefficients: [[-0.40275353  0.11907959 -1.0813122   2.2646866  -0.14199658]]
[37m[1m[2023-06-25 06:55:16,277][129146] Moving the mean solution point...
[36m[2023-06-25 06:55:26,112][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 06:55:26,112][129146] FPS: 390503.90
[36m[2023-06-25 06:55:26,115][129146] itr=637, itrs=2000, Progress: 31.85%
[36m[2023-06-25 06:55:37,658][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 06:55:37,658][129146] FPS: 333236.88
[36m[2023-06-25 06:55:42,549][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:55:42,549][129146] Reward + Measures: [[-138.00319853    0.06870683    0.83651513    0.80876666    0.82403672]]
[37m[1m[2023-06-25 06:55:42,549][129146] Max Reward on eval: -138.0031985294881
[37m[1m[2023-06-25 06:55:42,550][129146] Min Reward on eval: -138.0031985294881
[37m[1m[2023-06-25 06:55:42,550][129146] Mean Reward across all agents: -138.0031985294881
[37m[1m[2023-06-25 06:55:42,550][129146] Average Trajectory Length: 999.6983333333333
[36m[2023-06-25 06:55:48,012][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:55:48,013][129146] Reward + Measures: [[-203.74084801    0.0832        0.93530005    0.78310007    0.90039998]
[37m[1m [-145.5192958     0.1714        0.65350002    0.5535        0.60170001]
[37m[1m [-163.73537744    0.4297505     0.63205522    0.65239936    0.59831661]
[37m[1m ...
[37m[1m [ -37.85743459    0.59110004    0.64320004    0.45910001    0.71920002]
[37m[1m [-318.37651549    0.29508018    0.19490615    0.15129533    0.18456176]
[37m[1m [ 250.68801376    0.45520002    0.33119997    0.28809997    0.27309999]]
[37m[1m[2023-06-25 06:55:48,013][129146] Max Reward on eval: 697.2038030022406
[37m[1m[2023-06-25 06:55:48,013][129146] Min Reward on eval: -1134.725406587543
[37m[1m[2023-06-25 06:55:48,014][129146] Mean Reward across all agents: -116.24908433363574
[37m[1m[2023-06-25 06:55:48,014][129146] Average Trajectory Length: 978.5713333333333
[36m[2023-06-25 06:55:48,018][129146] mean_value=-985.793751841336, max_value=404.61843465900745
[37m[1m[2023-06-25 06:55:48,020][129146] New mean coefficients: [[ 0.17759782 -0.43719226 -0.75286937  2.659317   -0.00085258]]
[37m[1m[2023-06-25 06:55:48,021][129146] Moving the mean solution point...
[36m[2023-06-25 06:55:57,702][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 06:55:57,702][129146] FPS: 396744.24
[36m[2023-06-25 06:55:57,704][129146] itr=638, itrs=2000, Progress: 31.90%
[36m[2023-06-25 06:56:09,228][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 06:56:09,229][129146] FPS: 333879.12
[36m[2023-06-25 06:56:13,946][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:56:13,946][129146] Reward + Measures: [[-139.20869963    0.06606254    0.83967978    0.81387663    0.82624072]]
[37m[1m[2023-06-25 06:56:13,946][129146] Max Reward on eval: -139.2086996300489
[37m[1m[2023-06-25 06:56:13,947][129146] Min Reward on eval: -139.2086996300489
[37m[1m[2023-06-25 06:56:13,947][129146] Mean Reward across all agents: -139.2086996300489
[37m[1m[2023-06-25 06:56:13,947][129146] Average Trajectory Length: 999.395
[36m[2023-06-25 06:56:19,343][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:56:19,344][129146] Reward + Measures: [[  83.06991661    0.55989999    0.44770002    0.57560009    0.1374    ]
[37m[1m [ 202.28628217    0.51899999    0.58230001    0.62160003    0.21859999]
[37m[1m [ 141.42611309    0.55919999    0.29780003    0.44790003    0.1904    ]
[37m[1m ...
[37m[1m [   4.2296662     0.47660002    0.48010001    0.51960003    0.1023    ]
[37m[1m [-449.7638547     0.64649999    0.71859998    0.63840002    0.73989999]
[37m[1m [  30.69424379    0.61190003    0.63900006    0.73230004    0.1066    ]]
[37m[1m[2023-06-25 06:56:19,344][129146] Max Reward on eval: 349.27964906563864
[37m[1m[2023-06-25 06:56:19,344][129146] Min Reward on eval: -1362.0215979311033
[37m[1m[2023-06-25 06:56:19,344][129146] Mean Reward across all agents: -221.51646456774236
[37m[1m[2023-06-25 06:56:19,345][129146] Average Trajectory Length: 998.185
[36m[2023-06-25 06:56:19,353][129146] mean_value=-165.0171709976546, max_value=738.3964504071628
[37m[1m[2023-06-25 06:56:19,356][129146] New mean coefficients: [[ 0.46343464 -1.0891869   0.5263413   2.983767   -0.29175013]]
[37m[1m[2023-06-25 06:56:19,357][129146] Moving the mean solution point...
[36m[2023-06-25 06:56:29,116][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 06:56:29,117][129146] FPS: 393522.55
[36m[2023-06-25 06:56:29,119][129146] itr=639, itrs=2000, Progress: 31.95%
[36m[2023-06-25 06:56:40,627][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 06:56:40,628][129146] FPS: 334221.75
[36m[2023-06-25 06:56:45,304][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:56:45,304][129146] Reward + Measures: [[-135.04409213    0.060092      0.85687232    0.83592904    0.84538865]]
[37m[1m[2023-06-25 06:56:45,305][129146] Max Reward on eval: -135.04409213225233
[37m[1m[2023-06-25 06:56:45,305][129146] Min Reward on eval: -135.04409213225233
[37m[1m[2023-06-25 06:56:45,305][129146] Mean Reward across all agents: -135.04409213225233
[37m[1m[2023-06-25 06:56:45,305][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 06:56:50,709][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:56:50,710][129146] Reward + Measures: [[ -17.89617763    0.1735        0.56409997    0.53450006    0.59810007]
[37m[1m [-114.21641907    0.2863        0.2454        0.15030001    0.23830001]
[37m[1m [-358.7845798     0.19160001    0.58850002    0.5485        0.56660002]
[37m[1m ...
[37m[1m [  60.96548905    0.64500004    0.59840006    0.64580005    0.62800002]
[37m[1m [ -68.62993665    0.32450002    0.39750001    0.32390001    0.36790004]
[37m[1m [ -77.57216629    0.08759999    0.87019998    0.84920007    0.85740006]]
[37m[1m[2023-06-25 06:56:50,710][129146] Max Reward on eval: 340.8262209588662
[37m[1m[2023-06-25 06:56:50,710][129146] Min Reward on eval: -1298.4717556472635
[37m[1m[2023-06-25 06:56:50,711][129146] Mean Reward across all agents: -167.59254306108159
[37m[1m[2023-06-25 06:56:50,711][129146] Average Trajectory Length: 988.9653333333333
[36m[2023-06-25 06:56:50,715][129146] mean_value=-922.8828937579109, max_value=471.59677804078643
[37m[1m[2023-06-25 06:56:50,718][129146] New mean coefficients: [[-0.23259205 -0.40662974 -0.02524942  2.924943    0.05503893]]
[37m[1m[2023-06-25 06:56:50,719][129146] Moving the mean solution point...
[36m[2023-06-25 06:57:00,471][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 06:57:00,471][129146] FPS: 393807.47
[36m[2023-06-25 06:57:00,474][129146] itr=640, itrs=2000, Progress: 32.00%
[37m[1m[2023-06-25 06:57:05,690][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000620
[36m[2023-06-25 06:57:17,560][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 06:57:17,561][129146] FPS: 331748.80
[36m[2023-06-25 06:57:22,409][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:57:22,409][129146] Reward + Measures: [[-159.48499357    0.05755146    0.87372369    0.85584992    0.86287677]]
[37m[1m[2023-06-25 06:57:22,409][129146] Max Reward on eval: -159.48499357004076
[37m[1m[2023-06-25 06:57:22,410][129146] Min Reward on eval: -159.48499357004076
[37m[1m[2023-06-25 06:57:22,410][129146] Mean Reward across all agents: -159.48499357004076
[37m[1m[2023-06-25 06:57:22,410][129146] Average Trajectory Length: 999.745
[36m[2023-06-25 06:57:27,899][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:57:27,899][129146] Reward + Measures: [[-284.15784595    0.0498        0.91590005    0.90350002    0.90749997]
[37m[1m [-363.88033406    0.06440001    0.91130012    0.7823        0.89600003]
[37m[1m [-201.97758779    0.38649997    0.61400002    0.3529        0.6821    ]
[37m[1m ...
[37m[1m [-255.32713765    0.0599        0.83050007    0.79479998    0.83420002]
[37m[1m [-668.15787962    0.79538405    0.70366234    0.73820418    0.71190155]
[37m[1m [ 112.81406961    0.39400002    0.60040003    0.49650002    0.5201    ]]
[37m[1m[2023-06-25 06:57:27,899][129146] Max Reward on eval: 317.6322202057054
[37m[1m[2023-06-25 06:57:27,900][129146] Min Reward on eval: -1157.4760028902208
[37m[1m[2023-06-25 06:57:27,900][129146] Mean Reward across all agents: -142.42235036110105
[37m[1m[2023-06-25 06:57:27,900][129146] Average Trajectory Length: 983.8546666666666
[36m[2023-06-25 06:57:27,906][129146] mean_value=-207.76011016595348, max_value=632.5353641345398
[37m[1m[2023-06-25 06:57:27,909][129146] New mean coefficients: [[-0.14106682  0.03168786  0.2288261   4.1723776  -0.79572546]]
[37m[1m[2023-06-25 06:57:27,910][129146] Moving the mean solution point...
[36m[2023-06-25 06:57:37,588][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 06:57:37,588][129146] FPS: 396844.48
[36m[2023-06-25 06:57:37,591][129146] itr=641, itrs=2000, Progress: 32.05%
[36m[2023-06-25 06:57:49,133][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 06:57:49,133][129146] FPS: 333356.09
[36m[2023-06-25 06:57:53,915][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:57:53,916][129146] Reward + Measures: [[-158.42377454    0.05756359    0.87451613    0.8585825     0.86465687]]
[37m[1m[2023-06-25 06:57:53,916][129146] Max Reward on eval: -158.42377454104798
[37m[1m[2023-06-25 06:57:53,916][129146] Min Reward on eval: -158.42377454104798
[37m[1m[2023-06-25 06:57:53,916][129146] Mean Reward across all agents: -158.42377454104798
[37m[1m[2023-06-25 06:57:53,916][129146] Average Trajectory Length: 999.4056666666667
[36m[2023-06-25 06:57:59,451][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:57:59,452][129146] Reward + Measures: [[  42.77441058    0.1758        0.62690002    0.46360001    0.634     ]
[37m[1m [-656.83781516    0.70710009    0.23840001    0.67510003    0.79460001]
[37m[1m [-174.29023396    0.57420003    0.80340004    0.63780004    0.81719989]
[37m[1m ...
[37m[1m [-115.78437537    0.30009243    0.50877988    0.36049664    0.42914033]
[37m[1m [-301.17053618    0.14009999    0.6433        0.6099        0.64449996]
[37m[1m [-449.8574158     0.53739995    0.39410001    0.45690003    0.69230002]]
[37m[1m[2023-06-25 06:57:59,452][129146] Max Reward on eval: 188.66027136658084
[37m[1m[2023-06-25 06:57:59,452][129146] Min Reward on eval: -1156.9256798711663
[37m[1m[2023-06-25 06:57:59,453][129146] Mean Reward across all agents: -272.7570884991861
[37m[1m[2023-06-25 06:57:59,453][129146] Average Trajectory Length: 994.6576666666666
[36m[2023-06-25 06:57:59,459][129146] mean_value=-237.9723691525473, max_value=638.4093681306974
[37m[1m[2023-06-25 06:57:59,462][129146] New mean coefficients: [[-0.44543177 -0.04620133 -1.6991467   3.7272816   0.32747495]]
[37m[1m[2023-06-25 06:57:59,463][129146] Moving the mean solution point...
[36m[2023-06-25 06:58:09,223][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 06:58:09,229][129146] FPS: 393479.89
[36m[2023-06-25 06:58:09,232][129146] itr=642, itrs=2000, Progress: 32.10%
[36m[2023-06-25 06:58:20,777][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 06:58:20,777][129146] FPS: 333272.16
[36m[2023-06-25 06:58:25,629][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:58:25,630][129146] Reward + Measures: [[-169.96825794    0.05662722    0.87975246    0.86608362    0.87061536]]
[37m[1m[2023-06-25 06:58:25,630][129146] Max Reward on eval: -169.96825794047533
[37m[1m[2023-06-25 06:58:25,630][129146] Min Reward on eval: -169.96825794047533
[37m[1m[2023-06-25 06:58:25,630][129146] Mean Reward across all agents: -169.96825794047533
[37m[1m[2023-06-25 06:58:25,631][129146] Average Trajectory Length: 999.4053333333333
[36m[2023-06-25 06:58:31,184][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:58:31,184][129146] Reward + Measures: [[ 215.12948202    0.63510001    0.22160001    0.51879996    0.34870002]
[37m[1m [-276.772751      0.26070002    0.54150003    0.58160001    0.61140001]
[37m[1m [-440.41000411    0.051         0.9127        0.87760001    0.89540005]
[37m[1m ...
[37m[1m [  44.72593418    0.82070011    0.59230006    0.83530009    0.82120001]
[37m[1m [-145.50332916    0.0435        0.92559999    0.89239997    0.91310006]
[37m[1m [-240.07669924    0.0838        0.87089998    0.85340005    0.85570002]]
[37m[1m[2023-06-25 06:58:31,185][129146] Max Reward on eval: 480.05394585680335
[37m[1m[2023-06-25 06:58:31,185][129146] Min Reward on eval: -655.3586670803022
[37m[1m[2023-06-25 06:58:31,185][129146] Mean Reward across all agents: -112.74300065005023
[37m[1m[2023-06-25 06:58:31,185][129146] Average Trajectory Length: 996.4036666666666
[36m[2023-06-25 06:58:31,192][129146] mean_value=-129.19644265642248, max_value=544.7259341842082
[37m[1m[2023-06-25 06:58:31,195][129146] New mean coefficients: [[-0.60436475  0.30130208 -1.83845     4.2739806   0.39117974]]
[37m[1m[2023-06-25 06:58:31,196][129146] Moving the mean solution point...
[36m[2023-06-25 06:58:41,016][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 06:58:41,016][129146] FPS: 391093.55
[36m[2023-06-25 06:58:41,018][129146] itr=643, itrs=2000, Progress: 32.15%
[36m[2023-06-25 06:58:52,510][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 06:58:52,511][129146] FPS: 334818.68
[36m[2023-06-25 06:58:57,349][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:58:57,349][129146] Reward + Measures: [[-199.74135963    0.06036288    0.86731565    0.85579026    0.85933137]]
[37m[1m[2023-06-25 06:58:57,349][129146] Max Reward on eval: -199.7413596267035
[37m[1m[2023-06-25 06:58:57,350][129146] Min Reward on eval: -199.7413596267035
[37m[1m[2023-06-25 06:58:57,350][129146] Mean Reward across all agents: -199.7413596267035
[37m[1m[2023-06-25 06:58:57,350][129146] Average Trajectory Length: 999.0873333333333
[36m[2023-06-25 06:59:02,821][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:59:02,822][129146] Reward + Measures: [[-246.70633849    0.30179998    0.74570006    0.74629998    0.75410002]
[37m[1m [-135.07768338    0.84600002    0.66350001    0.76069999    0.65640002]
[37m[1m [-205.34941906    0.06730001    0.82050002    0.78899997    0.81210005]
[37m[1m ...
[37m[1m [ -73.09657436    0.85430002    0.64060003    0.80950004    0.63609999]
[37m[1m [ -50.56576469    0.0624        0.91979998    0.90240002    0.90450001]
[37m[1m [-125.5285959     0.79189998    0.82660002    0.75579995    0.83039999]]
[37m[1m[2023-06-25 06:59:02,822][129146] Max Reward on eval: 224.1749511816015
[37m[1m[2023-06-25 06:59:02,822][129146] Min Reward on eval: -384.7867121472955
[37m[1m[2023-06-25 06:59:02,823][129146] Mean Reward across all agents: -135.9340241058063
[37m[1m[2023-06-25 06:59:02,823][129146] Average Trajectory Length: 999.146
[36m[2023-06-25 06:59:02,830][129146] mean_value=-39.61423702133417, max_value=500.1635568031808
[37m[1m[2023-06-25 06:59:02,832][129146] New mean coefficients: [[-0.5497289   0.54586494 -1.364914    4.9401045   0.6058854 ]]
[37m[1m[2023-06-25 06:59:02,833][129146] Moving the mean solution point...
[36m[2023-06-25 06:59:12,563][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 06:59:12,563][129146] FPS: 394759.97
[36m[2023-06-25 06:59:12,565][129146] itr=644, itrs=2000, Progress: 32.20%
[36m[2023-06-25 06:59:24,060][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 06:59:24,061][129146] FPS: 334605.47
[36m[2023-06-25 06:59:28,845][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:59:28,845][129146] Reward + Measures: [[-212.02332217    0.06040576    0.86798227    0.85891503    0.86260825]]
[37m[1m[2023-06-25 06:59:28,846][129146] Max Reward on eval: -212.02332216688063
[37m[1m[2023-06-25 06:59:28,846][129146] Min Reward on eval: -212.02332216688063
[37m[1m[2023-06-25 06:59:28,846][129146] Mean Reward across all agents: -212.02332216688063
[37m[1m[2023-06-25 06:59:28,846][129146] Average Trajectory Length: 999.4096666666667
[36m[2023-06-25 06:59:34,317][129146] Finished Evaluation Step
[37m[1m[2023-06-25 06:59:34,317][129146] Reward + Measures: [[-186.00842644    0.0478        0.85519999    0.83160001    0.83929998]
[37m[1m [-262.56410555    0.20999999    0.89749998    0.89880002    0.8860001 ]
[37m[1m [-261.97193005    0.0586        0.93059999    0.91330004    0.91530001]
[37m[1m ...
[37m[1m [-149.76573801    0.0681        0.85070002    0.81849998    0.82340002]
[37m[1m [-223.34971877    0.1098        0.92950004    0.90359992    0.91119999]
[37m[1m [-225.36702803    0.11870001    0.76719999    0.80260003    0.79449999]]
[37m[1m[2023-06-25 06:59:34,317][129146] Max Reward on eval: 13.356307599286083
[37m[1m[2023-06-25 06:59:34,318][129146] Min Reward on eval: -574.1032727485348
[37m[1m[2023-06-25 06:59:34,318][129146] Mean Reward across all agents: -239.33977502808037
[37m[1m[2023-06-25 06:59:34,318][129146] Average Trajectory Length: 998.5783333333333
[36m[2023-06-25 06:59:34,324][129146] mean_value=-55.188516353963784, max_value=480.21146474502746
[37m[1m[2023-06-25 06:59:34,326][129146] New mean coefficients: [[ 0.08890313  0.50124615 -3.136001    5.766359    1.0277456 ]]
[37m[1m[2023-06-25 06:59:34,327][129146] Moving the mean solution point...
[36m[2023-06-25 06:59:44,015][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 06:59:44,015][129146] FPS: 396458.06
[36m[2023-06-25 06:59:44,018][129146] itr=645, itrs=2000, Progress: 32.25%
[36m[2023-06-25 06:59:55,443][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 06:59:55,443][129146] FPS: 336760.66
[36m[2023-06-25 07:00:00,301][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:00:00,302][129146] Reward + Measures: [[-221.41834355    0.05388667    0.88672167    0.87665397    0.88057029]]
[37m[1m[2023-06-25 07:00:00,302][129146] Max Reward on eval: -221.41834354807568
[37m[1m[2023-06-25 07:00:00,302][129146] Min Reward on eval: -221.41834354807568
[37m[1m[2023-06-25 07:00:00,302][129146] Mean Reward across all agents: -221.41834354807568
[37m[1m[2023-06-25 07:00:00,303][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:00:05,866][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:00:05,866][129146] Reward + Measures: [[-154.07441201    0.30739999    0.45570001    0.53130001    0.52319998]
[37m[1m [ -24.32189673    0.6807        0.73359996    0.71099997    0.69910002]
[37m[1m [-371.21288719    0.0653        0.83630002    0.82019997    0.83350003]
[37m[1m ...
[37m[1m [  77.0441754     0.50010008    0.51349998    0.46050006    0.54020005]
[37m[1m [ 417.12265588    0.4804        0.308         0.37079999    0.26770002]
[37m[1m [ 656.58832572    0.42120001    0.3705        0.19780003    0.32690001]]
[37m[1m[2023-06-25 07:00:05,866][129146] Max Reward on eval: 959.655282351852
[37m[1m[2023-06-25 07:00:05,867][129146] Min Reward on eval: -544.7040326002054
[37m[1m[2023-06-25 07:00:05,867][129146] Mean Reward across all agents: -9.564589015401364
[37m[1m[2023-06-25 07:00:05,867][129146] Average Trajectory Length: 997.8153333333333
[36m[2023-06-25 07:00:05,873][129146] mean_value=-312.7911054334177, max_value=542.0205755667645
[37m[1m[2023-06-25 07:00:05,876][129146] New mean coefficients: [[-0.29517195  0.20159122 -2.513907    6.172606    2.2260656 ]]
[37m[1m[2023-06-25 07:00:05,877][129146] Moving the mean solution point...
[36m[2023-06-25 07:00:15,677][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 07:00:15,678][129146] FPS: 391880.68
[36m[2023-06-25 07:00:15,680][129146] itr=646, itrs=2000, Progress: 32.30%
[36m[2023-06-25 07:00:27,162][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 07:00:27,163][129146] FPS: 335097.04
[36m[2023-06-25 07:00:31,966][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:00:31,967][129146] Reward + Measures: [[-195.13906252    0.05183733    0.89990234    0.8913123     0.89449203]]
[37m[1m[2023-06-25 07:00:31,967][129146] Max Reward on eval: -195.1390625229055
[37m[1m[2023-06-25 07:00:31,967][129146] Min Reward on eval: -195.1390625229055
[37m[1m[2023-06-25 07:00:31,967][129146] Mean Reward across all agents: -195.1390625229055
[37m[1m[2023-06-25 07:00:31,967][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:00:37,661][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:00:37,662][129146] Reward + Measures: [[ 121.20922188    0.36579999    0.47659999    0.31659999    0.57489997]
[37m[1m [  53.71753392    0.42589998    0.76879996    0.38040003    0.6814    ]
[37m[1m [ 213.53936599    0.51780003    0.53379995    0.28549999    0.3714    ]
[37m[1m ...
[37m[1m [-217.62836855    0.0487        0.93040001    0.90060008    0.91819996]
[37m[1m [-286.65658891    0.0775        0.81809998    0.79250002    0.79910004]
[37m[1m [ -93.64139525    0.0625        0.84440005    0.82030004    0.8233    ]]
[37m[1m[2023-06-25 07:00:37,662][129146] Max Reward on eval: 810.433817526931
[37m[1m[2023-06-25 07:00:37,662][129146] Min Reward on eval: -781.0794079436339
[37m[1m[2023-06-25 07:00:37,663][129146] Mean Reward across all agents: -8.951362863820465
[37m[1m[2023-06-25 07:00:37,663][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:00:37,669][129146] mean_value=-190.58750817308896, max_value=600.6070512071398
[37m[1m[2023-06-25 07:00:37,672][129146] New mean coefficients: [[-0.34343493  0.28994375 -2.9873185   6.6363006   2.5936887 ]]
[37m[1m[2023-06-25 07:00:37,673][129146] Moving the mean solution point...
[36m[2023-06-25 07:00:47,468][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 07:00:47,468][129146] FPS: 392089.55
[36m[2023-06-25 07:00:47,471][129146] itr=647, itrs=2000, Progress: 32.35%
[36m[2023-06-25 07:00:58,997][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 07:00:58,997][129146] FPS: 333813.68
[36m[2023-06-25 07:01:03,842][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:01:03,843][129146] Reward + Measures: [[-176.76807313    0.05088333    0.90625501    0.89866859    0.90088391]]
[37m[1m[2023-06-25 07:01:03,843][129146] Max Reward on eval: -176.7680731272974
[37m[1m[2023-06-25 07:01:03,843][129146] Min Reward on eval: -176.7680731272974
[37m[1m[2023-06-25 07:01:03,844][129146] Mean Reward across all agents: -176.7680731272974
[37m[1m[2023-06-25 07:01:03,844][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:01:09,283][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:01:09,284][129146] Reward + Measures: [[-580.34658702    0.1867        0.57669997    0.59040004    0.58329999]
[37m[1m [-501.5288689     0.0339        0.93840009    0.92360002    0.92910004]
[37m[1m [-253.23832668    0.14430001    0.72229999    0.75620002    0.75920004]
[37m[1m ...
[37m[1m [ -63.07182082    0.43890005    0.32430002    0.66550004    0.58939999]
[37m[1m [-119.12563778    0.59289998    0.39260003    0.55410004    0.20140003]
[37m[1m [-113.24832679    0.67869997    0.54700005    0.72670001    0.22809999]]
[37m[1m[2023-06-25 07:01:09,284][129146] Max Reward on eval: 146.64560731045785
[37m[1m[2023-06-25 07:01:09,284][129146] Min Reward on eval: -802.815518308722
[37m[1m[2023-06-25 07:01:09,285][129146] Mean Reward across all agents: -238.1163440361295
[37m[1m[2023-06-25 07:01:09,285][129146] Average Trajectory Length: 998.0156666666667
[36m[2023-06-25 07:01:09,294][129146] mean_value=23.1180627430023, max_value=514.1248328761139
[37m[1m[2023-06-25 07:01:09,297][129146] New mean coefficients: [[-0.22161242 -0.311235   -3.8605504   7.5138884   3.371963  ]]
[37m[1m[2023-06-25 07:01:09,298][129146] Moving the mean solution point...
[36m[2023-06-25 07:01:19,047][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 07:01:19,048][129146] FPS: 393942.47
[36m[2023-06-25 07:01:19,050][129146] itr=648, itrs=2000, Progress: 32.40%
[36m[2023-06-25 07:01:30,584][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 07:01:30,584][129146] FPS: 333615.55
[36m[2023-06-25 07:01:35,406][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:01:35,407][129146] Reward + Measures: [[-193.67551346    0.05139733    0.90469337    0.90038359    0.90168369]]
[37m[1m[2023-06-25 07:01:35,407][129146] Max Reward on eval: -193.67551345553636
[37m[1m[2023-06-25 07:01:35,407][129146] Min Reward on eval: -193.67551345553636
[37m[1m[2023-06-25 07:01:35,407][129146] Mean Reward across all agents: -193.67551345553636
[37m[1m[2023-06-25 07:01:35,407][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:01:40,907][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:01:40,908][129146] Reward + Measures: [[-184.84699246    0.1337        0.6469        0.61980003    0.63410002]
[37m[1m [-202.58918732    0.30930001    0.57999998    0.61750001    0.53030002]
[37m[1m [-258.66560198    0.2861        0.5528        0.59390002    0.54310006]
[37m[1m ...
[37m[1m [-250.19166634    0.1542        0.74619997    0.80620003    0.79900002]
[37m[1m [-208.49383393    0.26750001    0.1867        0.29609999    0.26659998]
[37m[1m [-378.04613538    0.1056        0.74830002    0.71469998    0.71099997]]
[37m[1m[2023-06-25 07:01:40,908][129146] Max Reward on eval: 815.489562410349
[37m[1m[2023-06-25 07:01:40,908][129146] Min Reward on eval: -545.5677485664324
[37m[1m[2023-06-25 07:01:40,908][129146] Mean Reward across all agents: -230.25268139500298
[37m[1m[2023-06-25 07:01:40,909][129146] Average Trajectory Length: 999.7993333333333
[36m[2023-06-25 07:01:40,916][129146] mean_value=14.14625133926175, max_value=658.071181339398
[37m[1m[2023-06-25 07:01:40,919][129146] New mean coefficients: [[ 0.5528765 -0.8291775 -4.5137267  7.997195   3.4433875]]
[37m[1m[2023-06-25 07:01:40,920][129146] Moving the mean solution point...
[36m[2023-06-25 07:01:50,596][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 07:01:50,597][129146] FPS: 396913.60
[36m[2023-06-25 07:01:50,599][129146] itr=649, itrs=2000, Progress: 32.45%
[36m[2023-06-25 07:02:02,013][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 07:02:02,013][129146] FPS: 337028.81
[36m[2023-06-25 07:02:06,691][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:02:06,692][129146] Reward + Measures: [[-158.97718641    0.054067      0.89980698    0.90052003    0.90091538]]
[37m[1m[2023-06-25 07:02:06,692][129146] Max Reward on eval: -158.97718641065848
[37m[1m[2023-06-25 07:02:06,692][129146] Min Reward on eval: -158.97718641065848
[37m[1m[2023-06-25 07:02:06,692][129146] Mean Reward across all agents: -158.97718641065848
[37m[1m[2023-06-25 07:02:06,692][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:02:12,109][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:02:12,110][129146] Reward + Measures: [[ -42.21376007    0.204         0.73990005    0.83240002    0.78670001]
[37m[1m [-193.26658723    0.1586        0.7008        0.76239997    0.74230003]
[37m[1m [-147.95120885    0.20970002    0.60439998    0.65730006    0.66409999]
[37m[1m ...
[37m[1m [-371.51199372    0.09060001    0.78850001    0.78649998    0.79150003]
[37m[1m [-382.85683307    0.07919999    0.65219998    0.64690006    0.66619998]
[37m[1m [-564.85044313    0.0665        0.88479996    0.87250006    0.88209993]]
[37m[1m[2023-06-25 07:02:12,110][129146] Max Reward on eval: 177.93792941462016
[37m[1m[2023-06-25 07:02:12,110][129146] Min Reward on eval: -892.8578987865243
[37m[1m[2023-06-25 07:02:12,111][129146] Mean Reward across all agents: -289.934256564863
[37m[1m[2023-06-25 07:02:12,111][129146] Average Trajectory Length: 999.4543333333334
[36m[2023-06-25 07:02:12,116][129146] mean_value=-115.14637259370022, max_value=572.3547109004692
[37m[1m[2023-06-25 07:02:12,118][129146] New mean coefficients: [[ 0.27541107  0.4371693  -3.519008    8.639536    3.9562664 ]]
[37m[1m[2023-06-25 07:02:12,119][129146] Moving the mean solution point...
[36m[2023-06-25 07:02:21,762][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 07:02:21,762][129146] FPS: 398288.83
[36m[2023-06-25 07:02:21,765][129146] itr=650, itrs=2000, Progress: 32.50%
[37m[1m[2023-06-25 07:02:27,168][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000630
[36m[2023-06-25 07:02:38,891][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 07:02:38,892][129146] FPS: 335641.34
[36m[2023-06-25 07:02:43,567][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:02:43,568][129146] Reward + Measures: [[-150.48786328    0.05283533    0.9068076     0.90927333    0.90940404]]
[37m[1m[2023-06-25 07:02:43,568][129146] Max Reward on eval: -150.4878632769219
[37m[1m[2023-06-25 07:02:43,568][129146] Min Reward on eval: -150.4878632769219
[37m[1m[2023-06-25 07:02:43,568][129146] Mean Reward across all agents: -150.4878632769219
[37m[1m[2023-06-25 07:02:43,569][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:02:48,922][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:02:48,922][129146] Reward + Measures: [[ 156.28641452    0.5916        0.39110002    0.69569999    0.53610009]
[37m[1m [  89.36099906    0.184         0.7809        0.78610003    0.85340005]
[37m[1m [-175.2301623     0.2563        0.66990006    0.74400002    0.74860001]
[37m[1m ...
[37m[1m [ 252.60277033    0.50159997    0.31990004    0.5212        0.49110004]
[37m[1m [  17.69314843    0.26180002    0.74780005    0.81689996    0.82790005]
[37m[1m [ 100.4788943     0.55932903    0.34970871    0.49041447    0.37743625]]
[37m[1m[2023-06-25 07:02:48,923][129146] Max Reward on eval: 393.835966532008
[37m[1m[2023-06-25 07:02:48,923][129146] Min Reward on eval: -511.72071649786085
[37m[1m[2023-06-25 07:02:48,923][129146] Mean Reward across all agents: 42.68218190511302
[37m[1m[2023-06-25 07:02:48,923][129146] Average Trajectory Length: 998.6419999999999
[36m[2023-06-25 07:02:48,936][129146] mean_value=235.39324765971222, max_value=765.7203341600368
[37m[1m[2023-06-25 07:02:48,939][129146] New mean coefficients: [[ 0.84859705  0.7860801  -3.730414    9.644509    4.917307  ]]
[37m[1m[2023-06-25 07:02:48,940][129146] Moving the mean solution point...
[36m[2023-06-25 07:02:58,583][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 07:02:58,584][129146] FPS: 398273.60
[36m[2023-06-25 07:02:58,586][129146] itr=651, itrs=2000, Progress: 32.55%
[36m[2023-06-25 07:03:10,217][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 07:03:10,217][129146] FPS: 330821.94
[36m[2023-06-25 07:03:15,042][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:03:15,042][129146] Reward + Measures: [[-187.48327851    0.049418      0.91344368    0.91477764    0.91517526]]
[37m[1m[2023-06-25 07:03:15,042][129146] Max Reward on eval: -187.483278513161
[37m[1m[2023-06-25 07:03:15,043][129146] Min Reward on eval: -187.483278513161
[37m[1m[2023-06-25 07:03:15,043][129146] Mean Reward across all agents: -187.483278513161
[37m[1m[2023-06-25 07:03:15,043][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:03:20,439][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:03:20,440][129146] Reward + Measures: [[ -36.68213917    0.51960003    0.264         0.47409996    0.42680001]
[37m[1m [  45.79723225    0.53204942    0.29011235    0.47023821    0.3798472 ]
[37m[1m [ -65.39732612    0.48289999    0.55930001    0.70469999    0.64990008]
[37m[1m ...
[37m[1m [-108.9511652     0.40990001    0.44179997    0.48380002    0.4619    ]
[37m[1m [-110.67220909    0.7324        0.2194        0.8233        0.51660001]
[37m[1m [-180.34931539    0.82330006    0.41420004    0.83890003    0.82429999]]
[37m[1m[2023-06-25 07:03:20,440][129146] Max Reward on eval: 264.48387673789404
[37m[1m[2023-06-25 07:03:20,440][129146] Min Reward on eval: -425.57945115559266
[37m[1m[2023-06-25 07:03:20,440][129146] Mean Reward across all agents: -55.13008720749725
[37m[1m[2023-06-25 07:03:20,441][129146] Average Trajectory Length: 997.9503333333333
[36m[2023-06-25 07:03:20,452][129146] mean_value=80.178572740321, max_value=606.0207911931211
[37m[1m[2023-06-25 07:03:20,454][129146] New mean coefficients: [[ 1.6316164  0.6589366 -3.2373583 10.316086   5.898704 ]]
[37m[1m[2023-06-25 07:03:20,455][129146] Moving the mean solution point...
[36m[2023-06-25 07:03:30,034][129146] train() took 9.58 seconds to complete
[36m[2023-06-25 07:03:30,034][129146] FPS: 400973.69
[36m[2023-06-25 07:03:30,036][129146] itr=652, itrs=2000, Progress: 32.60%
[36m[2023-06-25 07:03:41,467][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 07:03:41,467][129146] FPS: 336640.70
[36m[2023-06-25 07:03:46,234][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:03:46,234][129146] Reward + Measures: [[-155.14781035    0.04993833    0.91129327    0.91399437    0.91442001]]
[37m[1m[2023-06-25 07:03:46,235][129146] Max Reward on eval: -155.14781035272443
[37m[1m[2023-06-25 07:03:46,235][129146] Min Reward on eval: -155.14781035272443
[37m[1m[2023-06-25 07:03:46,235][129146] Mean Reward across all agents: -155.14781035272443
[37m[1m[2023-06-25 07:03:46,236][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:03:51,805][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:03:51,805][129146] Reward + Measures: [[ 36.55193315   0.69799995   0.29809999   0.68190002   0.1973    ]
[37m[1m [ 31.86137662   0.5126       0.87779999   0.80480003   0.80410004]
[37m[1m [-65.0205944    0.67980003   0.49039999   0.60409999   0.4127    ]
[37m[1m ...
[37m[1m [ 50.79120782   0.31900001   0.66960001   0.33629999   0.71680003]
[37m[1m [ 49.8336529    0.63590002   0.79660004   0.67070001   0.78859997]
[37m[1m [  1.10934032   0.54999995   0.62690002   0.54150003   0.50940001]]
[37m[1m[2023-06-25 07:03:51,806][129146] Max Reward on eval: 400.3235586343682
[37m[1m[2023-06-25 07:03:51,806][129146] Min Reward on eval: -619.3404103950713
[37m[1m[2023-06-25 07:03:51,806][129146] Mean Reward across all agents: 3.9719245038617403
[37m[1m[2023-06-25 07:03:51,806][129146] Average Trajectory Length: 997.592
[36m[2023-06-25 07:03:51,811][129146] mean_value=-295.90220379012254, max_value=594.5603864118632
[37m[1m[2023-06-25 07:03:51,814][129146] New mean coefficients: [[ 1.1487699  0.4823658 -2.7849312  9.247206   5.4030943]]
[37m[1m[2023-06-25 07:03:51,815][129146] Moving the mean solution point...
[36m[2023-06-25 07:04:01,407][129146] train() took 9.59 seconds to complete
[36m[2023-06-25 07:04:01,408][129146] FPS: 400387.13
[36m[2023-06-25 07:04:01,410][129146] itr=653, itrs=2000, Progress: 32.65%
[36m[2023-06-25 07:04:12,926][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 07:04:12,926][129146] FPS: 334123.15
[36m[2023-06-25 07:04:17,609][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:04:17,610][129146] Reward + Measures: [[-92.75744088   0.04439466   0.92496037   0.92562264   0.92534471]]
[37m[1m[2023-06-25 07:04:17,610][129146] Max Reward on eval: -92.75744087617367
[37m[1m[2023-06-25 07:04:17,610][129146] Min Reward on eval: -92.75744087617367
[37m[1m[2023-06-25 07:04:17,610][129146] Mean Reward across all agents: -92.75744087617367
[37m[1m[2023-06-25 07:04:17,610][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:04:23,074][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:04:23,075][129146] Reward + Measures: [[-311.4746921     0.72700006    0.70929998    0.76840001    0.7137    ]
[37m[1m [-293.6188845     0.71430004    0.59920001    0.85109997    0.47779998]
[37m[1m [-145.13981278    0.50780004    0.84160006    0.89110005    0.83600008]
[37m[1m ...
[37m[1m [-147.29320937    0.65900004    0.25150001    0.63679999    0.49689999]
[37m[1m [-108.30596795    0.16760002    0.84679997    0.82530004    0.82810003]
[37m[1m [-362.94782321    0.61290002    0.73030007    0.78270006    0.68160003]]
[37m[1m[2023-06-25 07:04:23,075][129146] Max Reward on eval: 230.67487341468805
[37m[1m[2023-06-25 07:04:23,075][129146] Min Reward on eval: -632.6503681353294
[37m[1m[2023-06-25 07:04:23,076][129146] Mean Reward across all agents: -202.80811988112004
[37m[1m[2023-06-25 07:04:23,076][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:04:23,084][129146] mean_value=-29.129429610845666, max_value=653.7403214152698
[37m[1m[2023-06-25 07:04:23,087][129146] New mean coefficients: [[ 1.176588    0.36220884 -0.61526966  8.612674    5.9902644 ]]
[37m[1m[2023-06-25 07:04:23,088][129146] Moving the mean solution point...
[36m[2023-06-25 07:04:32,943][129146] train() took 9.85 seconds to complete
[36m[2023-06-25 07:04:32,944][129146] FPS: 389695.39
[36m[2023-06-25 07:04:32,946][129146] itr=654, itrs=2000, Progress: 32.70%
[36m[2023-06-25 07:04:44,544][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 07:04:44,544][129146] FPS: 331725.17
[36m[2023-06-25 07:04:49,380][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:04:49,381][129146] Reward + Measures: [[-96.86480456   0.044368     0.92856467   0.93076205   0.9298473 ]]
[37m[1m[2023-06-25 07:04:49,381][129146] Max Reward on eval: -96.8648045636937
[37m[1m[2023-06-25 07:04:49,381][129146] Min Reward on eval: -96.8648045636937
[37m[1m[2023-06-25 07:04:49,381][129146] Mean Reward across all agents: -96.8648045636937
[37m[1m[2023-06-25 07:04:49,381][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:04:54,894][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:04:54,899][129146] Reward + Measures: [[-249.36414823    0.62420005    0.89989996    0.90380001    0.8876999 ]
[37m[1m [  76.70420603    0.75960004    0.65790004    0.80980009    0.74470007]
[37m[1m [-204.08098207    0.2218        0.82890004    0.90499991    0.89230007]
[37m[1m ...
[37m[1m [-179.28989283    0.69999999    0.7863        0.90380001    0.88269997]
[37m[1m [-410.43541603    0.29430002    0.70629996    0.90320009    0.85270005]
[37m[1m [-186.64182389    0.053         0.94000006    0.92819995    0.93190002]]
[37m[1m[2023-06-25 07:04:54,899][129146] Max Reward on eval: 352.8201070531155
[37m[1m[2023-06-25 07:04:54,900][129146] Min Reward on eval: -828.0502663608553
[37m[1m[2023-06-25 07:04:54,900][129146] Mean Reward across all agents: -262.46039742417264
[37m[1m[2023-06-25 07:04:54,900][129146] Average Trajectory Length: 999.7213333333333
[36m[2023-06-25 07:04:54,907][129146] mean_value=-72.5468726491279, max_value=690.2045670365694
[37m[1m[2023-06-25 07:04:54,910][129146] New mean coefficients: [[ 1.7373291  -0.1885997  -0.24902764  8.036503    6.000286  ]]
[37m[1m[2023-06-25 07:04:54,911][129146] Moving the mean solution point...
[36m[2023-06-25 07:05:04,843][129146] train() took 9.93 seconds to complete
[36m[2023-06-25 07:05:04,844][129146] FPS: 386687.29
[36m[2023-06-25 07:05:04,846][129146] itr=655, itrs=2000, Progress: 32.75%
[36m[2023-06-25 07:05:16,497][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 07:05:16,498][129146] FPS: 330148.69
[36m[2023-06-25 07:05:21,315][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:05:21,316][129146] Reward + Measures: [[-26.73361916   0.04531267   0.92600131   0.9304443    0.92928237]]
[37m[1m[2023-06-25 07:05:21,316][129146] Max Reward on eval: -26.733619164586592
[37m[1m[2023-06-25 07:05:21,316][129146] Min Reward on eval: -26.733619164586592
[37m[1m[2023-06-25 07:05:21,316][129146] Mean Reward across all agents: -26.733619164586592
[37m[1m[2023-06-25 07:05:21,317][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:05:26,922][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:05:26,928][129146] Reward + Measures: [[-122.97134998    0.69300002    0.44860002    0.78460002    0.37380001]
[37m[1m [-182.93533817    0.63889998    0.46630001    0.70180005    0.4391    ]
[37m[1m [-145.64690774    0.76820004    0.52179998    0.9005        0.66079998]
[37m[1m ...
[37m[1m [ -78.45634117    0.69380009    0.34999999    0.70900005    0.42670003]
[37m[1m [ -26.70456949    0.29880002    0.88670009    0.84770006    0.85129994]
[37m[1m [-144.17729817    0.65329999    0.68920004    0.74669999    0.31550002]]
[37m[1m[2023-06-25 07:05:26,928][129146] Max Reward on eval: 868.136657414923
[37m[1m[2023-06-25 07:05:26,929][129146] Min Reward on eval: -325.82675950247796
[37m[1m[2023-06-25 07:05:26,929][129146] Mean Reward across all agents: -32.280743798256495
[37m[1m[2023-06-25 07:05:26,929][129146] Average Trajectory Length: 999.8879999999999
[36m[2023-06-25 07:05:26,942][129146] mean_value=186.1649317230851, max_value=804.2655705779325
[37m[1m[2023-06-25 07:05:26,945][129146] New mean coefficients: [[1.85757    0.16919246 0.11811689 8.205637   6.6792693 ]]
[37m[1m[2023-06-25 07:05:26,946][129146] Moving the mean solution point...
[36m[2023-06-25 07:05:36,733][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 07:05:36,733][129146] FPS: 392419.86
[36m[2023-06-25 07:05:36,735][129146] itr=656, itrs=2000, Progress: 32.80%
[36m[2023-06-25 07:05:48,298][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 07:05:48,298][129146] FPS: 332679.36
[36m[2023-06-25 07:05:53,180][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:05:53,181][129146] Reward + Measures: [[8.18089969 0.04569867 0.9313314  0.93780005 0.93627036]]
[37m[1m[2023-06-25 07:05:53,181][129146] Max Reward on eval: 8.180899688384356
[37m[1m[2023-06-25 07:05:53,181][129146] Min Reward on eval: 8.180899688384356
[37m[1m[2023-06-25 07:05:53,181][129146] Mean Reward across all agents: 8.180899688384356
[37m[1m[2023-06-25 07:05:53,181][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:05:58,821][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:05:58,821][129146] Reward + Measures: [[-413.85729803    0.85420001    0.0428        0.96990007    0.89750004]
[37m[1m [ 134.84811103    0.38480002    0.8872        0.77270001    0.82310003]
[37m[1m [  53.63155761    0.1566        0.7604        0.79370004    0.78499997]
[37m[1m ...
[37m[1m [ 114.0455275     0.28280002    0.66979998    0.67259997    0.685     ]
[37m[1m [-366.51575141    0.89580005    0.047         0.97549993    0.94940007]
[37m[1m [-398.95014244    0.7924        0.4921        0.80330002    0.75810003]]
[37m[1m[2023-06-25 07:05:58,821][129146] Max Reward on eval: 579.8101578776957
[37m[1m[2023-06-25 07:05:58,822][129146] Min Reward on eval: -704.0475343017606
[37m[1m[2023-06-25 07:05:58,822][129146] Mean Reward across all agents: -82.24262189733132
[37m[1m[2023-06-25 07:05:58,822][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:05:58,835][129146] mean_value=222.09222749421954, max_value=638.8740985619102
[37m[1m[2023-06-25 07:05:58,838][129146] New mean coefficients: [[2.3021898 0.196569  1.8965234 7.8255725 6.2922463]]
[37m[1m[2023-06-25 07:05:58,839][129146] Moving the mean solution point...
[36m[2023-06-25 07:06:08,588][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 07:06:08,588][129146] FPS: 393939.27
[36m[2023-06-25 07:06:08,591][129146] itr=657, itrs=2000, Progress: 32.85%
[36m[2023-06-25 07:06:20,175][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 07:06:20,176][129146] FPS: 332031.56
[36m[2023-06-25 07:06:24,995][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:06:24,995][129146] Reward + Measures: [[66.61077176  0.04563833  0.92789596  0.93509901  0.93340069]]
[37m[1m[2023-06-25 07:06:24,996][129146] Max Reward on eval: 66.61077176088384
[37m[1m[2023-06-25 07:06:24,996][129146] Min Reward on eval: 66.61077176088384
[37m[1m[2023-06-25 07:06:24,996][129146] Mean Reward across all agents: 66.61077176088384
[37m[1m[2023-06-25 07:06:24,996][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:06:30,483][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:06:30,484][129146] Reward + Measures: [[ -94.47190431    0.59040004    0.47799999    0.43590003    0.62260002]
[37m[1m [-629.71124308    0.11420001    0.68940002    0.61610001    0.72600001]
[37m[1m [-378.22373104    0.1372        0.82340002    0.81669998    0.82680005]
[37m[1m ...
[37m[1m [ -76.92093132    0.07030001    0.89169997    0.87379998    0.89899999]
[37m[1m [-365.44257775    0.122         0.87720007    0.82170004    0.91009998]
[37m[1m [ 222.44144078    0.29550001    0.5618        0.1921        0.60619998]]
[37m[1m[2023-06-25 07:06:30,484][129146] Max Reward on eval: 340.4987924761605
[37m[1m[2023-06-25 07:06:30,484][129146] Min Reward on eval: -691.585487474408
[37m[1m[2023-06-25 07:06:30,485][129146] Mean Reward across all agents: -136.4673000516204
[37m[1m[2023-06-25 07:06:30,485][129146] Average Trajectory Length: 999.405
[36m[2023-06-25 07:06:30,492][129146] mean_value=-67.07448901269007, max_value=656.5140769304148
[37m[1m[2023-06-25 07:06:30,494][129146] New mean coefficients: [[2.3073888  0.49221882 3.0885334  7.846254   6.647946  ]]
[37m[1m[2023-06-25 07:06:30,495][129146] Moving the mean solution point...
[36m[2023-06-25 07:06:40,377][129146] train() took 9.88 seconds to complete
[36m[2023-06-25 07:06:40,377][129146] FPS: 388663.84
[36m[2023-06-25 07:06:40,379][129146] itr=658, itrs=2000, Progress: 32.90%
[36m[2023-06-25 07:06:51,961][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 07:06:51,961][129146] FPS: 332137.11
[36m[2023-06-25 07:06:56,716][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:06:56,721][129146] Reward + Measures: [[106.43271445   0.04540297   0.92610466   0.9330228    0.93060285]]
[37m[1m[2023-06-25 07:06:56,721][129146] Max Reward on eval: 106.43271445141558
[37m[1m[2023-06-25 07:06:56,722][129146] Min Reward on eval: 106.43271445141558
[37m[1m[2023-06-25 07:06:56,722][129146] Mean Reward across all agents: 106.43271445141558
[37m[1m[2023-06-25 07:06:56,722][129146] Average Trajectory Length: 999.7049999999999
[36m[2023-06-25 07:07:02,226][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:07:02,226][129146] Reward + Measures: [[-247.75120721    0.19340001    0.58600003    0.63510007    0.64020002]
[37m[1m [  -0.55181394    0.87390006    0.17220001    0.829         0.903     ]
[37m[1m [ 126.04304635    0.50669998    0.4646        0.96040004    0.94510001]
[37m[1m ...
[37m[1m [-271.64355269    0.13620001    0.79790002    0.81920004    0.82730001]
[37m[1m [ -51.0765596     0.3321        0.61259997    0.88129997    0.87509996]
[37m[1m [  47.76178344    0.30960003    0.65650004    0.93010008    0.92630005]]
[37m[1m[2023-06-25 07:07:02,227][129146] Max Reward on eval: 387.072340488981
[37m[1m[2023-06-25 07:07:02,227][129146] Min Reward on eval: -551.3844213598641
[37m[1m[2023-06-25 07:07:02,227][129146] Mean Reward across all agents: -34.667095201679594
[37m[1m[2023-06-25 07:07:02,227][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:07:02,238][129146] mean_value=204.3695005992322, max_value=774.8756837696768
[37m[1m[2023-06-25 07:07:02,241][129146] New mean coefficients: [[ 2.4191432  -0.22883192  3.1853848   8.156289    6.425727  ]]
[37m[1m[2023-06-25 07:07:02,242][129146] Moving the mean solution point...
[36m[2023-06-25 07:07:11,969][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 07:07:11,969][129146] FPS: 394854.42
[36m[2023-06-25 07:07:11,971][129146] itr=659, itrs=2000, Progress: 32.95%
[36m[2023-06-25 07:07:23,539][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 07:07:23,540][129146] FPS: 332521.13
[36m[2023-06-25 07:07:28,279][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:07:28,280][129146] Reward + Measures: [[188.92883216   0.05171366   0.92611134   0.94002992   0.9366743 ]]
[37m[1m[2023-06-25 07:07:28,280][129146] Max Reward on eval: 188.9288321603656
[37m[1m[2023-06-25 07:07:28,280][129146] Min Reward on eval: 188.9288321603656
[37m[1m[2023-06-25 07:07:28,280][129146] Mean Reward across all agents: 188.9288321603656
[37m[1m[2023-06-25 07:07:28,280][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:07:33,706][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:07:33,707][129146] Reward + Measures: [[554.39336246   0.171        0.6864       0.43540001   0.66609997]
[37m[1m [415.78330417   0.27900001   0.60640001   0.65369999   0.66260004]
[37m[1m [335.37680221   0.0971       0.88370001   0.94439995   0.93670005]
[37m[1m ...
[37m[1m [473.57096085   0.13160001   0.74660003   0.58020002   0.81020004]
[37m[1m [ 72.8017441    0.68510002   0.37900001   0.78099996   0.61280006]
[37m[1m [235.09433067   0.62810004   0.25310001   0.74509996   0.60330003]]
[37m[1m[2023-06-25 07:07:33,707][129146] Max Reward on eval: 639.2590270558837
[37m[1m[2023-06-25 07:07:33,707][129146] Min Reward on eval: -188.54289206884567
[37m[1m[2023-06-25 07:07:33,707][129146] Mean Reward across all agents: 289.7389809126564
[37m[1m[2023-06-25 07:07:33,708][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:07:33,723][129146] mean_value=500.70015256809535, max_value=985.6446640873794
[37m[1m[2023-06-25 07:07:33,726][129146] New mean coefficients: [[ 2.6430266 -0.7522433  3.5216413  7.9551163  6.128539 ]]
[37m[1m[2023-06-25 07:07:33,727][129146] Moving the mean solution point...
[36m[2023-06-25 07:07:43,358][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 07:07:43,358][129146] FPS: 398799.92
[36m[2023-06-25 07:07:43,360][129146] itr=660, itrs=2000, Progress: 33.00%
[37m[1m[2023-06-25 07:07:49,020][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000640
[36m[2023-06-25 07:08:00,695][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 07:08:00,695][129146] FPS: 337137.34
[36m[2023-06-25 07:08:05,456][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:08:05,456][129146] Reward + Measures: [[266.86095023   0.04748167   0.9327513    0.94372964   0.94009507]]
[37m[1m[2023-06-25 07:08:05,456][129146] Max Reward on eval: 266.8609502309919
[37m[1m[2023-06-25 07:08:05,456][129146] Min Reward on eval: 266.8609502309919
[37m[1m[2023-06-25 07:08:05,457][129146] Mean Reward across all agents: 266.8609502309919
[37m[1m[2023-06-25 07:08:05,457][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:08:10,829][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:08:10,835][129146] Reward + Measures: [[ 64.65362373   0.70279998   0.27520001   0.90460008   0.90390009]
[37m[1m [238.08163129   0.67580003   0.91429996   0.87690002   0.90959996]
[37m[1m [113.1938801    0.83500004   0.38920003   0.8405       0.88339996]
[37m[1m ...
[37m[1m [ 72.69216209   0.67500001   0.91949999   0.87330002   0.89770001]
[37m[1m [136.37148065   0.81490004   0.27579999   0.81190008   0.66099995]
[37m[1m [ -1.93829173   0.85620004   0.22029999   0.92539996   0.88070005]]
[37m[1m[2023-06-25 07:08:10,835][129146] Max Reward on eval: 415.12057047144043
[37m[1m[2023-06-25 07:08:10,836][129146] Min Reward on eval: -256.2093956245459
[37m[1m[2023-06-25 07:08:10,836][129146] Mean Reward across all agents: 83.74484103923967
[37m[1m[2023-06-25 07:08:10,836][129146] Average Trajectory Length: 999.7126666666667
[36m[2023-06-25 07:08:10,849][129146] mean_value=384.5599907962302, max_value=837.67275904749
[37m[1m[2023-06-25 07:08:10,852][129146] New mean coefficients: [[ 3.0982058  -0.37414923  3.3708377   7.150084    5.696623  ]]
[37m[1m[2023-06-25 07:08:10,853][129146] Moving the mean solution point...
[36m[2023-06-25 07:08:20,424][129146] train() took 9.57 seconds to complete
[36m[2023-06-25 07:08:20,424][129146] FPS: 401296.24
[36m[2023-06-25 07:08:20,426][129146] itr=661, itrs=2000, Progress: 33.05%
[36m[2023-06-25 07:08:31,791][129146] train() took 11.34 seconds to complete
[36m[2023-06-25 07:08:31,791][129146] FPS: 338478.75
[36m[2023-06-25 07:08:36,523][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:08:36,523][129146] Reward + Measures: [[342.47147782   0.051324     0.92914534   0.94231963   0.93889374]]
[37m[1m[2023-06-25 07:08:36,523][129146] Max Reward on eval: 342.47147782223516
[37m[1m[2023-06-25 07:08:36,524][129146] Min Reward on eval: 342.47147782223516
[37m[1m[2023-06-25 07:08:36,524][129146] Mean Reward across all agents: 342.47147782223516
[37m[1m[2023-06-25 07:08:36,524][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:08:42,185][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:08:42,185][129146] Reward + Measures: [[-19.44171812   0.80670005   0.0206       0.79150003   0.70360005]
[37m[1m [ 73.70719436   0.95860004   0.0026       0.98050004   0.8434    ]
[37m[1m [148.83010986   0.62480003   0.27190003   0.78600001   0.78470004]
[37m[1m ...
[37m[1m [ 55.20306063   0.91399997   0.0055       0.95819998   0.79589999]
[37m[1m [ 86.42166832   0.86690009   0.0204       0.88850003   0.71559995]
[37m[1m [168.9408247    0.70719999   0.1055       0.7834       0.75760001]]
[37m[1m[2023-06-25 07:08:42,186][129146] Max Reward on eval: 323.6188011563616
[37m[1m[2023-06-25 07:08:42,186][129146] Min Reward on eval: -389.0453785274527
[37m[1m[2023-06-25 07:08:42,186][129146] Mean Reward across all agents: 44.487715347134426
[37m[1m[2023-06-25 07:08:42,186][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:08:42,194][129146] mean_value=54.6209030732089, max_value=733.7850268577849
[37m[1m[2023-06-25 07:08:42,196][129146] New mean coefficients: [[ 1.8746337  -0.28903732  2.4099557   7.0298367   4.4929695 ]]
[37m[1m[2023-06-25 07:08:42,197][129146] Moving the mean solution point...
[36m[2023-06-25 07:08:52,054][129146] train() took 9.86 seconds to complete
[36m[2023-06-25 07:08:52,055][129146] FPS: 389642.73
[36m[2023-06-25 07:08:52,057][129146] itr=662, itrs=2000, Progress: 33.10%
[36m[2023-06-25 07:09:03,579][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 07:09:03,579][129146] FPS: 333866.96
[36m[2023-06-25 07:09:08,357][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:09:08,363][129146] Reward + Measures: [[348.60818339   0.05771967   0.92072731   0.94166797   0.93830931]]
[37m[1m[2023-06-25 07:09:08,364][129146] Max Reward on eval: 348.60818338932927
[37m[1m[2023-06-25 07:09:08,365][129146] Min Reward on eval: 348.60818338932927
[37m[1m[2023-06-25 07:09:08,365][129146] Mean Reward across all agents: 348.60818338932927
[37m[1m[2023-06-25 07:09:08,366][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:09:13,845][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:09:13,845][129146] Reward + Measures: [[317.96950762   0.41029999   0.73260003   0.62950003   0.64340001]
[37m[1m [439.91495138   0.1559       0.85879993   0.77180004   0.82620001]
[37m[1m [281.60025766   0.7561       0.6807       0.85890007   0.51509994]
[37m[1m ...
[37m[1m [129.73284438   0.79500002   0.49940005   0.7274       0.46079999]
[37m[1m [462.20678482   0.4815       0.91940004   0.77120006   0.88100004]
[37m[1m [362.37744774   0.45590001   0.89960003   0.89250004   0.85389996]]
[37m[1m[2023-06-25 07:09:13,845][129146] Max Reward on eval: 525.4428450360313
[37m[1m[2023-06-25 07:09:13,846][129146] Min Reward on eval: -446.69711571942315
[37m[1m[2023-06-25 07:09:13,846][129146] Mean Reward across all agents: 320.1616519079467
[37m[1m[2023-06-25 07:09:13,846][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:09:13,858][129146] mean_value=343.37097997732013, max_value=869.3900338823557
[37m[1m[2023-06-25 07:09:13,860][129146] New mean coefficients: [[1.4502857  0.15386236 2.058683   7.4219832  5.5076118 ]]
[37m[1m[2023-06-25 07:09:13,862][129146] Moving the mean solution point...
[36m[2023-06-25 07:09:23,586][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 07:09:23,586][129146] FPS: 394966.18
[36m[2023-06-25 07:09:23,588][129146] itr=663, itrs=2000, Progress: 33.15%
[36m[2023-06-25 07:09:35,159][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 07:09:35,159][129146] FPS: 332441.90
[36m[2023-06-25 07:09:39,904][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:09:39,904][129146] Reward + Measures: [[392.58596787   0.06835      0.91219872   0.94352001   0.93990999]]
[37m[1m[2023-06-25 07:09:39,905][129146] Max Reward on eval: 392.58596786960067
[37m[1m[2023-06-25 07:09:39,905][129146] Min Reward on eval: 392.58596786960067
[37m[1m[2023-06-25 07:09:39,905][129146] Mean Reward across all agents: 392.58596786960067
[37m[1m[2023-06-25 07:09:39,905][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:09:45,269][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:09:45,275][129146] Reward + Measures: [[ 64.90397112   0.73549998   0.52160001   0.7051       0.67760003]
[37m[1m [115.21837467   0.60009998   0.66130012   0.65099996   0.77270001]
[37m[1m [166.77196496   0.68970001   0.24960001   0.92600006   0.90420008]
[37m[1m ...
[37m[1m [378.39539349   0.0988       0.87760001   0.94679993   0.93580002]
[37m[1m [350.24706904   0.0423       0.86640006   0.86289996   0.8592    ]
[37m[1m [292.76954506   0.77240002   0.46869999   0.74770004   0.54190004]]
[37m[1m[2023-06-25 07:09:45,275][129146] Max Reward on eval: 407.3656349453842
[37m[1m[2023-06-25 07:09:45,276][129146] Min Reward on eval: -85.20723281269893
[37m[1m[2023-06-25 07:09:45,276][129146] Mean Reward across all agents: 195.62597303769664
[37m[1m[2023-06-25 07:09:45,276][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:09:45,287][129146] mean_value=314.12430351882716, max_value=805.93373569994
[37m[1m[2023-06-25 07:09:45,289][129146] New mean coefficients: [[0.9604753  0.48045996 1.8485785  7.4685564  5.0839143 ]]
[37m[1m[2023-06-25 07:09:45,291][129146] Moving the mean solution point...
[36m[2023-06-25 07:09:54,969][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 07:09:54,970][129146] FPS: 396801.14
[36m[2023-06-25 07:09:54,972][129146] itr=664, itrs=2000, Progress: 33.20%
[36m[2023-06-25 07:10:06,406][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 07:10:06,407][129146] FPS: 336538.49
[36m[2023-06-25 07:10:11,218][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:10:11,218][129146] Reward + Measures: [[437.15945663   0.11024767   0.86961228   0.94588035   0.9412303 ]]
[37m[1m[2023-06-25 07:10:11,218][129146] Max Reward on eval: 437.15945663039207
[37m[1m[2023-06-25 07:10:11,219][129146] Min Reward on eval: 437.15945663039207
[37m[1m[2023-06-25 07:10:11,219][129146] Mean Reward across all agents: 437.15945663039207
[37m[1m[2023-06-25 07:10:11,219][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:10:16,691][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:10:16,697][129146] Reward + Measures: [[ -34.66083234    0.88520002    0.0107        0.89930004    0.87130004]
[37m[1m [-110.71059575    0.81049997    0.1374        0.90549994    0.92559999]
[37m[1m [ 296.18466189    0.7938        0.46070001    0.88749999    0.86580002]
[37m[1m ...
[37m[1m [-288.45504372    0.97259998    0.0158        0.9891001     0.95990008]
[37m[1m [ 229.88058092    0.69170004    0.38229999    0.75909996    0.4048    ]
[37m[1m [  12.8232719     0.83939999    0.21429999    0.9073        0.9465    ]]
[37m[1m[2023-06-25 07:10:16,697][129146] Max Reward on eval: 504.92262535919434
[37m[1m[2023-06-25 07:10:16,698][129146] Min Reward on eval: -288.4550437222468
[37m[1m[2023-06-25 07:10:16,698][129146] Mean Reward across all agents: 190.66100415742167
[37m[1m[2023-06-25 07:10:16,698][129146] Average Trajectory Length: 999.66
[36m[2023-06-25 07:10:16,713][129146] mean_value=456.99297154218095, max_value=880.5174642293016
[37m[1m[2023-06-25 07:10:16,716][129146] New mean coefficients: [[1.0152773  0.25072825 1.4158163  8.127698   4.4963045 ]]
[37m[1m[2023-06-25 07:10:16,717][129146] Moving the mean solution point...
[36m[2023-06-25 07:10:26,661][129146] train() took 9.94 seconds to complete
[36m[2023-06-25 07:10:26,661][129146] FPS: 386230.91
[36m[2023-06-25 07:10:26,664][129146] itr=665, itrs=2000, Progress: 33.25%
[36m[2023-06-25 07:10:38,217][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 07:10:38,218][129146] FPS: 333055.16
[36m[2023-06-25 07:10:43,034][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:10:43,035][129146] Reward + Measures: [[453.46319729   0.14757299   0.834916     0.95071465   0.94596225]]
[37m[1m[2023-06-25 07:10:43,035][129146] Max Reward on eval: 453.46319729040556
[37m[1m[2023-06-25 07:10:43,035][129146] Min Reward on eval: 453.46319729040556
[37m[1m[2023-06-25 07:10:43,035][129146] Mean Reward across all agents: 453.46319729040556
[37m[1m[2023-06-25 07:10:43,036][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:10:48,596][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:10:48,596][129146] Reward + Measures: [[232.55874566   0.56790006   0.48300001   0.95990002   0.95020002]
[37m[1m [188.07953441   0.64970005   0.23939998   0.62150002   0.50150007]
[37m[1m [309.9352841    0.60480005   0.20030001   0.5449       0.40219998]
[37m[1m ...
[37m[1m [207.82798694   0.70270008   0.25900003   0.93879998   0.9169001 ]
[37m[1m [ -0.14453692   0.88309997   0.08350001   0.94679993   0.92900002]
[37m[1m [190.00728595   0.94510001   0.08810001   0.9655       0.94150001]]
[37m[1m[2023-06-25 07:10:48,597][129146] Max Reward on eval: 471.64649790744295
[37m[1m[2023-06-25 07:10:48,597][129146] Min Reward on eval: -156.77708577503216
[37m[1m[2023-06-25 07:10:48,597][129146] Mean Reward across all agents: 140.38847209545963
[37m[1m[2023-06-25 07:10:48,597][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:10:48,608][129146] mean_value=325.1770515475033, max_value=836.0516785623456
[37m[1m[2023-06-25 07:10:48,611][129146] New mean coefficients: [[0.89036244 0.46514863 1.0923963  7.7062907  3.437738  ]]
[37m[1m[2023-06-25 07:10:48,612][129146] Moving the mean solution point...
[36m[2023-06-25 07:10:58,571][129146] train() took 9.96 seconds to complete
[36m[2023-06-25 07:10:58,572][129146] FPS: 385655.01
[36m[2023-06-25 07:10:58,574][129146] itr=666, itrs=2000, Progress: 33.30%
[36m[2023-06-25 07:11:10,191][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 07:11:10,192][129146] FPS: 331279.22
[36m[2023-06-25 07:11:15,139][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:11:15,139][129146] Reward + Measures: [[435.955239     0.17341435   0.81240332   0.95598567   0.95361167]]
[37m[1m[2023-06-25 07:11:15,140][129146] Max Reward on eval: 435.9552390033026
[37m[1m[2023-06-25 07:11:15,140][129146] Min Reward on eval: 435.9552390033026
[37m[1m[2023-06-25 07:11:15,140][129146] Mean Reward across all agents: 435.9552390033026
[37m[1m[2023-06-25 07:11:15,140][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:11:20,782][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:11:20,788][129146] Reward + Measures: [[-225.89443453    0.2559        0.64359999    0.56989998    0.58560002]
[37m[1m [ 117.07881889    0.85570002    0.2811        0.90920001    0.48990002]
[37m[1m [  -6.38059528    0.37510002    0.81990004    0.80410004    0.80610001]
[37m[1m ...
[37m[1m [-432.97663845    0.083         0.93260002    0.91820002    0.93260002]
[37m[1m [ 151.25227935    0.58210003    0.3662        0.94589996    0.83990002]
[37m[1m [-112.50315451    0.61414522    0.36630595    0.55866712    0.43344814]]
[37m[1m[2023-06-25 07:11:20,788][129146] Max Reward on eval: 317.71850935926193
[37m[1m[2023-06-25 07:11:20,788][129146] Min Reward on eval: -936.3808020158671
[37m[1m[2023-06-25 07:11:20,789][129146] Mean Reward across all agents: -128.73258919485957
[37m[1m[2023-06-25 07:11:20,789][129146] Average Trajectory Length: 938.4879999999999
[36m[2023-06-25 07:11:20,794][129146] mean_value=-374.51587083733165, max_value=643.2524965399236
[37m[1m[2023-06-25 07:11:20,797][129146] New mean coefficients: [[0.53094465 0.59820306 0.356461   7.6349306  1.8797449 ]]
[37m[1m[2023-06-25 07:11:20,798][129146] Moving the mean solution point...
[36m[2023-06-25 07:11:30,568][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 07:11:30,569][129146] FPS: 393108.58
[36m[2023-06-25 07:11:30,571][129146] itr=667, itrs=2000, Progress: 33.35%
[36m[2023-06-25 07:11:42,108][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 07:11:42,109][129146] FPS: 333479.86
[36m[2023-06-25 07:11:46,937][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:11:46,938][129146] Reward + Measures: [[387.16988115   0.26754168   0.7181477    0.9605667    0.95618063]]
[37m[1m[2023-06-25 07:11:46,938][129146] Max Reward on eval: 387.1698811473805
[37m[1m[2023-06-25 07:11:46,938][129146] Min Reward on eval: 387.1698811473805
[37m[1m[2023-06-25 07:11:46,938][129146] Mean Reward across all agents: 387.1698811473805
[37m[1m[2023-06-25 07:11:46,939][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:11:52,442][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:11:52,442][129146] Reward + Measures: [[343.88701193   0.0744       0.94690001   0.9386999    0.93979996]
[37m[1m [279.57283991   0.76960009   0.76089996   0.84619999   0.73650008]
[37m[1m [368.01978922   0.222        0.91110003   0.87550002   0.92160004]
[37m[1m ...
[37m[1m [409.27822339   0.0953       0.95199996   0.9443       0.94109994]
[37m[1m [389.11449266   0.0741       0.93950003   0.92970002   0.92460006]
[37m[1m [259.2718906    0.15109999   0.89849997   0.838        0.87100011]]
[37m[1m[2023-06-25 07:11:52,442][129146] Max Reward on eval: 561.59934478465
[37m[1m[2023-06-25 07:11:52,443][129146] Min Reward on eval: -307.5562470207457
[37m[1m[2023-06-25 07:11:52,443][129146] Mean Reward across all agents: 264.28667909765625
[37m[1m[2023-06-25 07:11:52,443][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:11:52,452][129146] mean_value=280.88770772944446, max_value=959.4753629355691
[37m[1m[2023-06-25 07:11:52,455][129146] New mean coefficients: [[ 0.7388861   0.7917185  -0.41933882  7.485452    1.4720973 ]]
[37m[1m[2023-06-25 07:11:52,456][129146] Moving the mean solution point...
[36m[2023-06-25 07:12:02,229][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 07:12:02,230][129146] FPS: 392993.84
[36m[2023-06-25 07:12:02,232][129146] itr=668, itrs=2000, Progress: 33.40%
[36m[2023-06-25 07:12:13,788][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 07:12:13,788][129146] FPS: 332908.76
[36m[2023-06-25 07:12:18,672][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:12:18,678][129146] Reward + Measures: [[289.86491733   0.39187565   0.59408832   0.96643263   0.96012467]]
[37m[1m[2023-06-25 07:12:18,678][129146] Max Reward on eval: 289.86491733386777
[37m[1m[2023-06-25 07:12:18,679][129146] Min Reward on eval: 289.86491733386777
[37m[1m[2023-06-25 07:12:18,679][129146] Mean Reward across all agents: 289.86491733386777
[37m[1m[2023-06-25 07:12:18,679][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:12:24,168][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:12:24,169][129146] Reward + Measures: [[ 456.04210867    0.63940001    0.3996        0.63870001    0.51360005]
[37m[1m [ 504.16193111    0.59440005    0.43800002    0.5413        0.41829997]
[37m[1m [-105.31774541    0.96870005    0.37470001    0.96859998    0.95170003]
[37m[1m ...
[37m[1m [ 485.63355159    0.62560004    0.1992        0.54689997    0.45679998]
[37m[1m [ 187.75856074    0.69819999    0.4686        0.91280001    0.94550002]
[37m[1m [ 457.33570112    0.64380002    0.40529999    0.79550004    0.72059995]]
[37m[1m[2023-06-25 07:12:24,169][129146] Max Reward on eval: 614.6754415814066
[37m[1m[2023-06-25 07:12:24,170][129146] Min Reward on eval: -372.33802877471896
[37m[1m[2023-06-25 07:12:24,170][129146] Mean Reward across all agents: 394.69571018856624
[37m[1m[2023-06-25 07:12:24,170][129146] Average Trajectory Length: 995.9686666666666
[36m[2023-06-25 07:12:24,181][129146] mean_value=298.1990575583026, max_value=906.0485289945966
[37m[1m[2023-06-25 07:12:24,184][129146] New mean coefficients: [[ 0.21108955  0.6313437  -0.39931372  7.170436    2.1460285 ]]
[37m[1m[2023-06-25 07:12:24,185][129146] Moving the mean solution point...
[36m[2023-06-25 07:12:34,025][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 07:12:34,026][129146] FPS: 390306.29
[36m[2023-06-25 07:12:34,028][129146] itr=669, itrs=2000, Progress: 33.45%
[36m[2023-06-25 07:12:45,559][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 07:12:45,559][129146] FPS: 333626.64
[36m[2023-06-25 07:12:50,447][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:12:50,447][129146] Reward + Measures: [[144.99281094   0.6771633    0.31218567   0.97973728   0.97377568]]
[37m[1m[2023-06-25 07:12:50,448][129146] Max Reward on eval: 144.99281093601545
[37m[1m[2023-06-25 07:12:50,448][129146] Min Reward on eval: 144.99281093601545
[37m[1m[2023-06-25 07:12:50,448][129146] Mean Reward across all agents: 144.99281093601545
[37m[1m[2023-06-25 07:12:50,448][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:12:55,901][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:12:55,902][129146] Reward + Measures: [[193.20169439   0.58879995   0.54230005   0.82230008   0.7342    ]
[37m[1m [  4.93968055   0.84389991   0.2323       0.88300002   0.68889999]
[37m[1m [319.0083226    0.87400001   0.22040001   0.85810006   0.3423    ]
[37m[1m ...
[37m[1m [288.81932383   0.62299997   0.8549       0.75619996   0.78830004]
[37m[1m [116.40645968   0.70600003   0.38859996   0.90570003   0.81669998]
[37m[1m [136.38078359   0.3784       0.62150002   0.8901       0.84289998]]
[37m[1m[2023-06-25 07:12:55,902][129146] Max Reward on eval: 591.2397124418465
[37m[1m[2023-06-25 07:12:55,902][129146] Min Reward on eval: -544.8345958840567
[37m[1m[2023-06-25 07:12:55,902][129146] Mean Reward across all agents: 293.11520981673567
[37m[1m[2023-06-25 07:12:55,903][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:12:55,915][129146] mean_value=346.2756046852011, max_value=899.3194376431511
[37m[1m[2023-06-25 07:12:55,918][129146] New mean coefficients: [[ 0.40383965  0.7254853  -0.7036484   7.833321    2.3171368 ]]
[37m[1m[2023-06-25 07:12:55,919][129146] Moving the mean solution point...
[36m[2023-06-25 07:13:05,724][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 07:13:05,724][129146] FPS: 391695.74
[36m[2023-06-25 07:13:05,726][129146] itr=670, itrs=2000, Progress: 33.50%
[37m[1m[2023-06-25 07:13:11,707][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000650
[36m[2023-06-25 07:13:23,443][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 07:13:23,444][129146] FPS: 334754.94
[36m[2023-06-25 07:13:28,334][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:13:28,335][129146] Reward + Measures: [[0.63695053 0.81735367 0.16839133 0.98477733 0.97563994]]
[37m[1m[2023-06-25 07:13:28,335][129146] Max Reward on eval: 0.6369505315611799
[37m[1m[2023-06-25 07:13:28,335][129146] Min Reward on eval: 0.6369505315611799
[37m[1m[2023-06-25 07:13:28,335][129146] Mean Reward across all agents: 0.6369505315611799
[37m[1m[2023-06-25 07:13:28,336][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:13:33,770][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:13:33,771][129146] Reward + Measures: [[ 226.13237161    0.82789993    0.3364        0.9551        0.91779995]
[37m[1m [ 233.85231423    0.76640004    0.76589996    0.85790008    0.85339993]
[37m[1m [-539.85322527    0.94000006    0.0029        0.96700001    0.95620006]
[37m[1m ...
[37m[1m [ 292.78971612    0.83750004    0.79789996    0.89820004    0.90409994]
[37m[1m [ 456.0022463     0.34450001    0.60849994    0.65970004    0.89750004]
[37m[1m [ 454.35429649    0.52650005    0.75159997    0.70139998    0.5244    ]]
[37m[1m[2023-06-25 07:13:33,771][129146] Max Reward on eval: 572.0250297314254
[37m[1m[2023-06-25 07:13:33,771][129146] Min Reward on eval: -862.5350072385743
[37m[1m[2023-06-25 07:13:33,772][129146] Mean Reward across all agents: 129.8563379697302
[37m[1m[2023-06-25 07:13:33,772][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:13:33,783][129146] mean_value=197.31780079112528, max_value=1072.0250297314255
[37m[1m[2023-06-25 07:13:33,786][129146] New mean coefficients: [[ 1.3308868  0.4523884 -1.5666003  7.4952745  1.6696966]]
[37m[1m[2023-06-25 07:13:33,787][129146] Moving the mean solution point...
[36m[2023-06-25 07:13:43,492][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 07:13:43,492][129146] FPS: 395767.48
[36m[2023-06-25 07:13:43,494][129146] itr=671, itrs=2000, Progress: 33.55%
[36m[2023-06-25 07:13:55,039][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 07:13:55,040][129146] FPS: 333195.56
[36m[2023-06-25 07:13:59,801][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:13:59,801][129146] Reward + Measures: [[39.21490938  0.98620367  0.00701033  0.9960646   0.98802   ]]
[37m[1m[2023-06-25 07:13:59,801][129146] Max Reward on eval: 39.21490937737417
[37m[1m[2023-06-25 07:13:59,802][129146] Min Reward on eval: 39.21490937737417
[37m[1m[2023-06-25 07:13:59,802][129146] Mean Reward across all agents: 39.21490937737417
[37m[1m[2023-06-25 07:13:59,802][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:14:05,378][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:14:05,379][129146] Reward + Measures: [[  10.15048319    0.50582188    0.31604061    0.46611562    0.38498434]
[37m[1m [-568.93055772    0.96149999    0.0028        0.98019999    0.93330002]
[37m[1m [-350.78951582    0.91870004    0.98089999    0.9938001     0.98930007]
[37m[1m ...
[37m[1m [-254.48921752    0.94469994    0.0027        0.98019999    0.9188    ]
[37m[1m [-711.8248595     0.98859996    0.0028        0.99510002    0.89289999]
[37m[1m [-967.51880791    0.88660002    0.89160007    0.94189996    0.93170005]]
[37m[1m[2023-06-25 07:14:05,379][129146] Max Reward on eval: 398.4261534672463
[37m[1m[2023-06-25 07:14:05,379][129146] Min Reward on eval: -1208.186846968904
[37m[1m[2023-06-25 07:14:05,380][129146] Mean Reward across all agents: -167.44339745784038
[37m[1m[2023-06-25 07:14:05,380][129146] Average Trajectory Length: 999.752
[36m[2023-06-25 07:14:05,386][129146] mean_value=-215.96343709068577, max_value=859.8737117685844
[37m[1m[2023-06-25 07:14:05,388][129146] New mean coefficients: [[ 1.5162264   0.41170624 -1.1393855   7.042859    0.6155944 ]]
[37m[1m[2023-06-25 07:14:05,389][129146] Moving the mean solution point...
[36m[2023-06-25 07:14:15,052][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 07:14:15,052][129146] FPS: 397481.08
[36m[2023-06-25 07:14:15,054][129146] itr=672, itrs=2000, Progress: 33.60%
[36m[2023-06-25 07:14:26,583][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 07:14:26,583][129146] FPS: 333796.06
[36m[2023-06-25 07:14:31,343][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:14:31,343][129146] Reward + Measures: [[-51.64810795   0.89743471   0.12991101   0.97899663   0.8356657 ]]
[37m[1m[2023-06-25 07:14:31,343][129146] Max Reward on eval: -51.64810795364303
[37m[1m[2023-06-25 07:14:31,344][129146] Min Reward on eval: -51.64810795364303
[37m[1m[2023-06-25 07:14:31,344][129146] Mean Reward across all agents: -51.64810795364303
[37m[1m[2023-06-25 07:14:31,344][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:14:36,769][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:14:36,775][129146] Reward + Measures: [[ 44.66716345   0.59510005   0.47070003   0.80039996   0.80059999]
[37m[1m [-46.6811202    0.75290006   0.28260002   0.83060008   0.82790005]
[37m[1m [ 55.50450683   0.6627       0.2728       0.80019999   0.76440001]
[37m[1m ...
[37m[1m [-40.18328627   0.90869999   0.0184       0.96259993   0.84580004]
[37m[1m [ 14.6319893    0.6408       0.31549999   0.80450004   0.79530001]
[37m[1m [103.57937763   0.54549998   0.42810002   0.74960005   0.76459998]]
[37m[1m[2023-06-25 07:14:36,775][129146] Max Reward on eval: 341.4389410731266
[37m[1m[2023-06-25 07:14:36,775][129146] Min Reward on eval: -132.01275892228006
[37m[1m[2023-06-25 07:14:36,775][129146] Mean Reward across all agents: 36.571351929386836
[37m[1m[2023-06-25 07:14:36,776][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:14:36,781][129146] mean_value=140.5864225026293, max_value=566.7105051156104
[37m[1m[2023-06-25 07:14:36,784][129146] New mean coefficients: [[ 2.0802958   0.2377501  -1.1289529   7.9870305   0.03264034]]
[37m[1m[2023-06-25 07:14:36,785][129146] Moving the mean solution point...
[36m[2023-06-25 07:14:46,493][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 07:14:46,493][129146] FPS: 395618.18
[36m[2023-06-25 07:14:46,496][129146] itr=673, itrs=2000, Progress: 33.65%
[36m[2023-06-25 07:14:57,931][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 07:14:57,932][129146] FPS: 336452.47
[36m[2023-06-25 07:15:02,660][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:15:02,661][129146] Reward + Measures: [[263.99190282   0.44470569   0.44889966   0.90789837   0.83610904]]
[37m[1m[2023-06-25 07:15:02,661][129146] Max Reward on eval: 263.99190282250737
[37m[1m[2023-06-25 07:15:02,661][129146] Min Reward on eval: 263.99190282250737
[37m[1m[2023-06-25 07:15:02,661][129146] Mean Reward across all agents: 263.99190282250737
[37m[1m[2023-06-25 07:15:02,662][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:15:08,079][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:15:08,079][129146] Reward + Measures: [[  253.52274212     0.5553         0.43640003     0.87819999
[37m[1m      0.87110007]
[37m[1m [ -568.50629313     0.93650001     0.0011         0.98159999
[37m[1m      0.95009995]
[37m[1m [ -420.51835176     0.89849997     0.0225         0.82679999
[37m[1m      0.83070004]
[37m[1m ...
[37m[1m [-1086.54507322     0.94470006     0.0176         0.86309999
[37m[1m      0.83220005]
[37m[1m [  185.24666135     0.72570008     0.28099999     0.73060006
[37m[1m      0.62300003]
[37m[1m [  155.79600188     0.67189997     0.35090002     0.91860002
[37m[1m      0.8448    ]]
[37m[1m[2023-06-25 07:15:08,079][129146] Max Reward on eval: 534.7950947781908
[37m[1m[2023-06-25 07:15:08,080][129146] Min Reward on eval: -1664.2823149435221
[37m[1m[2023-06-25 07:15:08,080][129146] Mean Reward across all agents: -346.028113669364
[37m[1m[2023-06-25 07:15:08,080][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:15:08,084][129146] mean_value=-518.699922323331, max_value=714.3881816960358
[37m[1m[2023-06-25 07:15:08,087][129146] New mean coefficients: [[0.65072584 0.5475506  0.15971518 8.326225   0.88814384]]
[37m[1m[2023-06-25 07:15:08,088][129146] Moving the mean solution point...
[36m[2023-06-25 07:15:17,842][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 07:15:17,842][129146] FPS: 393751.44
[36m[2023-06-25 07:15:17,845][129146] itr=674, itrs=2000, Progress: 33.70%
[36m[2023-06-25 07:15:29,364][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 07:15:29,364][129146] FPS: 333959.47
[36m[2023-06-25 07:15:34,069][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:15:34,069][129146] Reward + Measures: [[-32.45442292   0.96472597   0.026353     0.99065      0.88540429]]
[37m[1m[2023-06-25 07:15:34,069][129146] Max Reward on eval: -32.45442292405254
[37m[1m[2023-06-25 07:15:34,070][129146] Min Reward on eval: -32.45442292405254
[37m[1m[2023-06-25 07:15:34,070][129146] Mean Reward across all agents: -32.45442292405254
[37m[1m[2023-06-25 07:15:34,070][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:15:39,521][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:15:39,521][129146] Reward + Measures: [[ 456.00287268    0.63810009    0.40149999    0.66370004    0.26830003]
[37m[1m [ -40.99655683    0.99260008    0.001         0.99580002    0.97209996]
[37m[1m [-399.45191496    0.75279999    0.5837        0.66769999    0.17309999]
[37m[1m ...
[37m[1m [ 426.43831607    0.72029996    0.5503        0.64350003    0.35139999]
[37m[1m [ 105.70163694    0.98029995    0.0045        0.9945001     0.95410007]
[37m[1m [ -70.27426362    0.77789998    0.74809998    0.80699998    0.58200002]]
[37m[1m[2023-06-25 07:15:39,522][129146] Max Reward on eval: 524.6720326803159
[37m[1m[2023-06-25 07:15:39,522][129146] Min Reward on eval: -1523.379568677838
[37m[1m[2023-06-25 07:15:39,522][129146] Mean Reward across all agents: -3.2699224226526393
[37m[1m[2023-06-25 07:15:39,522][129146] Average Trajectory Length: 998.6316666666667
[36m[2023-06-25 07:15:39,531][129146] mean_value=13.886218034722308, max_value=919.0601419921306
[37m[1m[2023-06-25 07:15:39,533][129146] New mean coefficients: [[0.7706658  0.81528646 0.21274391 9.032101   0.49853715]]
[37m[1m[2023-06-25 07:15:39,534][129146] Moving the mean solution point...
[36m[2023-06-25 07:15:49,274][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 07:15:49,274][129146] FPS: 394343.42
[36m[2023-06-25 07:15:49,276][129146] itr=675, itrs=2000, Progress: 33.75%
[36m[2023-06-25 07:16:00,689][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 07:16:00,689][129146] FPS: 337060.86
[36m[2023-06-25 07:16:05,488][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:16:05,489][129146] Reward + Measures: [[53.95701064  0.99040467  0.00167633  0.99672461  0.89524871]]
[37m[1m[2023-06-25 07:16:05,489][129146] Max Reward on eval: 53.95701063509706
[37m[1m[2023-06-25 07:16:05,489][129146] Min Reward on eval: 53.95701063509706
[37m[1m[2023-06-25 07:16:05,489][129146] Mean Reward across all agents: 53.95701063509706
[37m[1m[2023-06-25 07:16:05,490][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:16:11,183][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:16:11,184][129146] Reward + Measures: [[382.8567938    0.95410007   0.0127       0.95270008   0.74169999]
[37m[1m [179.46126996   0.9903       0.0038       0.99410003   0.89280003]
[37m[1m [102.35984621   0.98949999   0.0026       0.9903       0.76910001]
[37m[1m ...
[37m[1m [390.81348468   0.92530006   0.0397       0.95029992   0.61410004]
[37m[1m [349.5414024    0.94379997   0.0378       0.90970004   0.71800005]
[37m[1m [276.80212298   0.98169994   0.0037       0.98430008   0.80790007]]
[37m[1m[2023-06-25 07:16:11,184][129146] Max Reward on eval: 611.2004521851195
[37m[1m[2023-06-25 07:16:11,184][129146] Min Reward on eval: -21.460134591546375
[37m[1m[2023-06-25 07:16:11,184][129146] Mean Reward across all agents: 276.4244379771365
[37m[1m[2023-06-25 07:16:11,185][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:16:11,192][129146] mean_value=185.98932181408327, max_value=1061.0525757846074
[37m[1m[2023-06-25 07:16:11,194][129146] New mean coefficients: [[ 1.5358334   0.72875285 -0.8999984  10.58064     1.7332762 ]]
[37m[1m[2023-06-25 07:16:11,195][129146] Moving the mean solution point...
[36m[2023-06-25 07:16:21,004][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 07:16:21,004][129146] FPS: 391566.10
[36m[2023-06-25 07:16:21,006][129146] itr=676, itrs=2000, Progress: 33.80%
[36m[2023-06-25 07:16:32,569][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 07:16:32,570][129146] FPS: 332672.87
[36m[2023-06-25 07:16:37,500][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:16:37,505][129146] Reward + Measures: [[158.02694618   0.95443141   0.00402167   0.98877263   0.93743229]]
[37m[1m[2023-06-25 07:16:37,506][129146] Max Reward on eval: 158.0269461792154
[37m[1m[2023-06-25 07:16:37,506][129146] Min Reward on eval: 158.0269461792154
[37m[1m[2023-06-25 07:16:37,506][129146] Mean Reward across all agents: 158.0269461792154
[37m[1m[2023-06-25 07:16:37,506][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:16:42,984][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:16:42,985][129146] Reward + Measures: [[ 411.16680094    0.76679999    0.039         0.79790002    0.58120006]
[37m[1m [-643.68476655    0.56160003    0.16949999    0.84700006    0.71579999]
[37m[1m [-100.03120828    0.57880002    0.47440001    0.71700001    0.84389991]
[37m[1m ...
[37m[1m [ 297.22922082    0.77960002    0.0384        0.86090004    0.88590014]
[37m[1m [ 151.32872548    0.9077        0.0035        0.98280001    0.95179999]
[37m[1m [-629.32445505    0.477         0.63970006    0.53570002    0.70819998]]
[37m[1m[2023-06-25 07:16:42,985][129146] Max Reward on eval: 448.22348612735516
[37m[1m[2023-06-25 07:16:42,985][129146] Min Reward on eval: -1031.0533894989408
[37m[1m[2023-06-25 07:16:42,985][129146] Mean Reward across all agents: -101.41080197482961
[37m[1m[2023-06-25 07:16:42,986][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:16:42,992][129146] mean_value=-42.33115907279515, max_value=730.994241719304
[37m[1m[2023-06-25 07:16:42,995][129146] New mean coefficients: [[ 0.3368355  0.8749764 -0.5554807 11.285781   2.8801847]]
[37m[1m[2023-06-25 07:16:42,996][129146] Moving the mean solution point...
[36m[2023-06-25 07:16:52,641][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 07:16:52,641][129146] FPS: 398209.35
[36m[2023-06-25 07:16:52,643][129146] itr=677, itrs=2000, Progress: 33.85%
[36m[2023-06-25 07:17:04,238][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 07:17:04,238][129146] FPS: 331865.76
[36m[2023-06-25 07:17:08,996][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:17:08,997][129146] Reward + Measures: [[-985.80743806    0.98020929    0.00258       0.99606597    0.97760463]]
[37m[1m[2023-06-25 07:17:08,997][129146] Max Reward on eval: -985.8074380635626
[37m[1m[2023-06-25 07:17:08,997][129146] Min Reward on eval: -985.8074380635626
[37m[1m[2023-06-25 07:17:08,997][129146] Mean Reward across all agents: -985.8074380635626
[37m[1m[2023-06-25 07:17:08,997][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:17:14,389][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:17:14,390][129146] Reward + Measures: [[-1365.50672165     0.34465662     0.34684154     0.34959057
[37m[1m      0.30349058]
[37m[1m [-1549.64116108     0.25113839     0.23243336     0.25072452
[37m[1m      0.18910232]
[37m[1m [-1456.73604063     0.2402         0.38480002     0.35959998
[37m[1m      0.34040001]
[37m[1m ...
[37m[1m [-1357.08718306     0.31013137     0.2829338      0.29852825
[37m[1m      0.253867  ]
[37m[1m [ -372.95451083     0.62290001     0.4152         0.70019996
[37m[1m      0.44930002]
[37m[1m [-1616.69480425     0.34274158     0.30055723     0.3171784
[37m[1m      0.24987207]]
[37m[1m[2023-06-25 07:17:14,390][129146] Max Reward on eval: -372.9545108295803
[37m[1m[2023-06-25 07:17:14,390][129146] Min Reward on eval: -1979.3452573889867
[37m[1m[2023-06-25 07:17:14,390][129146] Mean Reward across all agents: -1457.6976482979298
[37m[1m[2023-06-25 07:17:14,391][129146] Average Trajectory Length: 940.3439999999999
[36m[2023-06-25 07:17:14,392][129146] mean_value=-2310.4249181963287, max_value=8.39044460977209
[37m[1m[2023-06-25 07:17:14,395][129146] New mean coefficients: [[ 0.89519614  0.90974855 -0.5177433   9.365133    2.7600012 ]]
[37m[1m[2023-06-25 07:17:14,396][129146] Moving the mean solution point...
[36m[2023-06-25 07:17:23,978][129146] train() took 9.58 seconds to complete
[36m[2023-06-25 07:17:23,978][129146] FPS: 400810.48
[36m[2023-06-25 07:17:23,980][129146] itr=678, itrs=2000, Progress: 33.90%
[36m[2023-06-25 07:17:35,373][129146] train() took 11.37 seconds to complete
[36m[2023-06-25 07:17:35,373][129146] FPS: 337671.72
[36m[2023-06-25 07:17:40,111][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:17:40,111][129146] Reward + Measures: [[-949.30847092    0.98185432    0.00184733    0.99752969    0.978167  ]]
[37m[1m[2023-06-25 07:17:40,112][129146] Max Reward on eval: -949.308470915157
[37m[1m[2023-06-25 07:17:40,112][129146] Min Reward on eval: -949.308470915157
[37m[1m[2023-06-25 07:17:40,112][129146] Mean Reward across all agents: -949.308470915157
[37m[1m[2023-06-25 07:17:40,112][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:17:45,480][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:17:45,481][129146] Reward + Measures: [[-849.69902082    0.97579998    0.0012        0.99599999    0.96880007]
[37m[1m [ 217.23254072    0.31930003    0.65250003    0.55909997    0.6674    ]
[37m[1m [-774.85770341    0.95090002    0.0072        0.9601        0.87279999]
[37m[1m ...
[37m[1m [  65.7599664     0.2994        0.64059997    0.54109997    0.54949999]
[37m[1m [-678.49892053    0.85860008    0.0364        0.87980002    0.80680001]
[37m[1m [-748.17018226    0.92109996    0.0472        0.94840002    0.91530001]]
[37m[1m[2023-06-25 07:17:45,486][129146] Max Reward on eval: 441.82175556932344
[37m[1m[2023-06-25 07:17:45,486][129146] Min Reward on eval: -1269.3099499659147
[37m[1m[2023-06-25 07:17:45,486][129146] Mean Reward across all agents: -478.2411858667016
[37m[1m[2023-06-25 07:17:45,487][129146] Average Trajectory Length: 999.453
[36m[2023-06-25 07:17:45,492][129146] mean_value=-494.8834919926429, max_value=728.9467391903861
[37m[1m[2023-06-25 07:17:45,495][129146] New mean coefficients: [[ 2.175932    0.77106154 -2.8953853   9.829148    1.4256227 ]]
[37m[1m[2023-06-25 07:17:45,495][129146] Moving the mean solution point...
[36m[2023-06-25 07:17:55,241][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 07:17:55,241][129146] FPS: 394100.86
[36m[2023-06-25 07:17:55,243][129146] itr=679, itrs=2000, Progress: 33.95%
[36m[2023-06-25 07:18:06,738][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 07:18:06,738][129146] FPS: 334790.37
[36m[2023-06-25 07:18:11,471][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:18:11,477][129146] Reward + Measures: [[384.93061195   0.92045659   0.387694     0.94157195   0.15544233]]
[37m[1m[2023-06-25 07:18:11,477][129146] Max Reward on eval: 384.9306119528052
[37m[1m[2023-06-25 07:18:11,477][129146] Min Reward on eval: 384.9306119528052
[37m[1m[2023-06-25 07:18:11,478][129146] Mean Reward across all agents: 384.9306119528052
[37m[1m[2023-06-25 07:18:11,478][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:18:16,983][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:18:16,989][129146] Reward + Measures: [[384.25446351   0.8174001    0.64429998   0.77039999   0.35690004]
[37m[1m [332.89091161   0.75229996   0.75660002   0.63620001   0.64120001]
[37m[1m [395.79890347   0.88520002   0.45100003   0.82690001   0.1831    ]
[37m[1m ...
[37m[1m [408.43469668   0.75740004   0.68440002   0.68360007   0.60719997]
[37m[1m [372.65373389   0.54150003   0.70069999   0.45339999   0.64660001]
[37m[1m [350.1083527    0.89530003   0.49130002   0.84189999   0.10390001]]
[37m[1m[2023-06-25 07:18:16,989][129146] Max Reward on eval: 502.52770033684794
[37m[1m[2023-06-25 07:18:16,990][129146] Min Reward on eval: 159.17119423105615
[37m[1m[2023-06-25 07:18:16,990][129146] Mean Reward across all agents: 357.45515866182575
[37m[1m[2023-06-25 07:18:16,990][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:18:17,001][129146] mean_value=360.84098166856984, max_value=986.9090039604403
[37m[1m[2023-06-25 07:18:17,004][129146] New mean coefficients: [[ 2.010953   0.797566  -1.712962   8.989524   1.2753971]]
[37m[1m[2023-06-25 07:18:17,005][129146] Moving the mean solution point...
[36m[2023-06-25 07:18:26,967][129146] train() took 9.96 seconds to complete
[36m[2023-06-25 07:18:26,967][129146] FPS: 385527.66
[36m[2023-06-25 07:18:26,969][129146] itr=680, itrs=2000, Progress: 34.00%
[37m[1m[2023-06-25 07:18:33,083][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000660
[36m[2023-06-25 07:18:44,892][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 07:18:44,892][129146] FPS: 332654.59
[36m[2023-06-25 07:18:49,728][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:18:49,728][129146] Reward + Measures: [[-134.57653823    0.89718431    0.17761332    0.9138543     0.19465965]]
[37m[1m[2023-06-25 07:18:49,728][129146] Max Reward on eval: -134.5765382338205
[37m[1m[2023-06-25 07:18:49,729][129146] Min Reward on eval: -134.5765382338205
[37m[1m[2023-06-25 07:18:49,729][129146] Mean Reward across all agents: -134.5765382338205
[37m[1m[2023-06-25 07:18:49,729][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:18:55,314][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:18:55,315][129146] Reward + Measures: [[  -8.79901531    0.41679999    0.77859998    0.75220001    0.68919998]
[37m[1m [  36.42827289    0.90380001    0.46180001    0.91139996    0.0292    ]
[37m[1m [  75.01919752    0.90679997    0.3876        0.9047001     0.13770001]
[37m[1m ...
[37m[1m [-148.76933589    0.89700001    0.21610001    0.92080003    0.1621    ]
[37m[1m [ 131.08326479    0.71950001    0.47150001    0.75500005    0.33700001]
[37m[1m [ 104.90732357    0.82180005    0.47499999    0.87159997    0.0401    ]]
[37m[1m[2023-06-25 07:18:55,315][129146] Max Reward on eval: 410.46163890314057
[37m[1m[2023-06-25 07:18:55,315][129146] Min Reward on eval: -337.31523841328453
[37m[1m[2023-06-25 07:18:55,315][129146] Mean Reward across all agents: 62.48078216191088
[37m[1m[2023-06-25 07:18:55,316][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:18:55,320][129146] mean_value=71.13920160373333, max_value=592.0608194600483
[37m[1m[2023-06-25 07:18:55,323][129146] New mean coefficients: [[ 2.7491589   0.61848104 -1.8071043   9.556157    0.58468413]]
[37m[1m[2023-06-25 07:18:55,324][129146] Moving the mean solution point...
[36m[2023-06-25 07:19:05,083][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 07:19:05,083][129146] FPS: 393573.42
[36m[2023-06-25 07:19:05,085][129146] itr=681, itrs=2000, Progress: 34.05%
[36m[2023-06-25 07:19:16,462][129146] train() took 11.36 seconds to complete
[36m[2023-06-25 07:19:16,462][129146] FPS: 338157.34
[36m[2023-06-25 07:19:21,252][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:19:21,253][129146] Reward + Measures: [[-117.53930054    0.91724497    0.11660667    0.94470066    0.22636431]]
[37m[1m[2023-06-25 07:19:21,253][129146] Max Reward on eval: -117.53930054444909
[37m[1m[2023-06-25 07:19:21,253][129146] Min Reward on eval: -117.53930054444909
[37m[1m[2023-06-25 07:19:21,254][129146] Mean Reward across all agents: -117.53930054444909
[37m[1m[2023-06-25 07:19:21,254][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:19:26,611][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:19:26,612][129146] Reward + Measures: [[214.76557234   0.83080006   0.4941       0.75789994   0.0746    ]
[37m[1m [ 31.17038438   0.72329998   0.27760002   0.70090002   0.22680001]
[37m[1m [238.22734301   0.80919999   0.4677       0.76720005   0.19160001]
[37m[1m ...
[37m[1m [246.84203642   0.93949997   0.54530001   0.89390004   0.0172    ]
[37m[1m [164.54171183   0.96789998   0.5108       0.93599999   0.0219    ]
[37m[1m [106.05092352   0.93840009   0.36479998   0.92880005   0.0605    ]]
[37m[1m[2023-06-25 07:19:26,612][129146] Max Reward on eval: 374.0615649409243
[37m[1m[2023-06-25 07:19:26,612][129146] Min Reward on eval: -54.03532816802617
[37m[1m[2023-06-25 07:19:26,612][129146] Mean Reward across all agents: 178.99958680188058
[37m[1m[2023-06-25 07:19:26,613][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:19:26,618][129146] mean_value=105.92740252698385, max_value=830.5186635147577
[37m[1m[2023-06-25 07:19:26,621][129146] New mean coefficients: [[ 2.6097665   0.54387593 -0.5192995   8.811811    0.02355617]]
[37m[1m[2023-06-25 07:19:26,622][129146] Moving the mean solution point...
[36m[2023-06-25 07:19:36,436][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 07:19:36,437][129146] FPS: 391326.26
[36m[2023-06-25 07:19:36,439][129146] itr=682, itrs=2000, Progress: 34.10%
[36m[2023-06-25 07:19:47,839][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 07:19:47,840][129146] FPS: 337426.77
[36m[2023-06-25 07:19:52,671][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:19:52,672][129146] Reward + Measures: [[-54.64334476   0.92909765   0.08392633   0.96470165   0.25371432]]
[37m[1m[2023-06-25 07:19:52,672][129146] Max Reward on eval: -54.64334475794191
[37m[1m[2023-06-25 07:19:52,672][129146] Min Reward on eval: -54.64334475794191
[37m[1m[2023-06-25 07:19:52,673][129146] Mean Reward across all agents: -54.64334475794191
[37m[1m[2023-06-25 07:19:52,673][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:19:58,210][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:19:58,210][129146] Reward + Measures: [[ 69.76845317   0.92339993   0.0891       0.96289998   0.26840001]
[37m[1m [ 77.19558631   0.89950001   0.071        0.94390005   0.29910001]
[37m[1m [179.02204658   0.84560007   0.1242       0.8847       0.22309999]
[37m[1m ...
[37m[1m [150.25059474   0.88050002   0.098        0.92550004   0.2543    ]
[37m[1m [ 66.89049943   0.89180005   0.0411       0.94620001   0.36050001]
[37m[1m [ 66.00229045   0.7726       0.0789       0.86269999   0.3055    ]]
[37m[1m[2023-06-25 07:19:58,211][129146] Max Reward on eval: 388.0788796442677
[37m[1m[2023-06-25 07:19:58,211][129146] Min Reward on eval: -22.73722738774959
[37m[1m[2023-06-25 07:19:58,211][129146] Mean Reward across all agents: 115.4284864625333
[37m[1m[2023-06-25 07:19:58,211][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:19:58,217][129146] mean_value=58.21560788383305, max_value=680.4457064347225
[37m[1m[2023-06-25 07:19:58,220][129146] New mean coefficients: [[ 1.445614   0.6237944  1.5410271 10.080973   1.4986153]]
[37m[1m[2023-06-25 07:19:58,221][129146] Moving the mean solution point...
[36m[2023-06-25 07:20:07,861][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 07:20:07,862][129146] FPS: 398385.42
[36m[2023-06-25 07:20:07,864][129146] itr=683, itrs=2000, Progress: 34.15%
[36m[2023-06-25 07:20:19,402][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 07:20:19,402][129146] FPS: 333466.64
[36m[2023-06-25 07:20:24,335][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:20:24,336][129146] Reward + Measures: [[-15.40145507   0.94655132   0.10011133   0.97493434   0.23472567]]
[37m[1m[2023-06-25 07:20:24,336][129146] Max Reward on eval: -15.40145506794714
[37m[1m[2023-06-25 07:20:24,336][129146] Min Reward on eval: -15.40145506794714
[37m[1m[2023-06-25 07:20:24,336][129146] Mean Reward across all agents: -15.40145506794714
[37m[1m[2023-06-25 07:20:24,337][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:20:29,934][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:20:29,935][129146] Reward + Measures: [[149.92091065   0.88370001   0.0248       0.94690001   0.39309999]
[37m[1m [332.16577094   0.81400007   0.32479998   0.89889997   0.11490001]
[37m[1m [254.05602568   0.9533       0.28669998   0.96030009   0.12980001]
[37m[1m ...
[37m[1m [449.06236704   0.73380005   0.40100002   0.80849999   0.3901    ]
[37m[1m [590.99989645   0.44639999   0.62870002   0.62130004   0.66110003]
[37m[1m [401.76298256   0.64490002   0.414        0.77019995   0.42870003]]
[37m[1m[2023-06-25 07:20:29,935][129146] Max Reward on eval: 599.3187123631011
[37m[1m[2023-06-25 07:20:29,935][129146] Min Reward on eval: -37.91130535163684
[37m[1m[2023-06-25 07:20:29,936][129146] Mean Reward across all agents: 359.4171026570642
[37m[1m[2023-06-25 07:20:29,936][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:20:29,947][129146] mean_value=387.64423934988804, max_value=932.8132451128222
[37m[1m[2023-06-25 07:20:29,950][129146] New mean coefficients: [[ 1.8838495  0.6231064  2.0252063 11.570278   2.8193948]]
[37m[1m[2023-06-25 07:20:29,951][129146] Moving the mean solution point...
[36m[2023-06-25 07:20:39,773][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 07:20:39,773][129146] FPS: 391022.44
[36m[2023-06-25 07:20:39,776][129146] itr=684, itrs=2000, Progress: 34.20%
[36m[2023-06-25 07:20:51,317][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 07:20:51,317][129146] FPS: 333438.43
[36m[2023-06-25 07:20:55,913][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:20:55,914][129146] Reward + Measures: [[-694.0555667     0.97181374    0.82854426    0.97160596    0.74941826]]
[37m[1m[2023-06-25 07:20:55,914][129146] Max Reward on eval: -694.0555667049757
[37m[1m[2023-06-25 07:20:55,914][129146] Min Reward on eval: -694.0555667049757
[37m[1m[2023-06-25 07:20:55,914][129146] Mean Reward across all agents: -694.0555667049757
[37m[1m[2023-06-25 07:20:55,915][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:21:01,216][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:21:01,217][129146] Reward + Measures: [[ -280.37174476     0.87379998     0.87639999     0.88049996
[37m[1m      0.89300007]
[37m[1m [   23.09358872     0.8567999      0.23889999     0.866
[37m[1m      0.38490003]
[37m[1m [ -696.10801931     0.89289999     0.88859999     0.88759995
[37m[1m      0.86860001]
[37m[1m ...
[37m[1m [-1891.39108161     0.98289996     0.98190004     0.98070002
[37m[1m      0.98099995]
[37m[1m [ -220.86427676     0.9163         0.94410002     0.92210001
[37m[1m      0.954     ]
[37m[1m [ -874.42980789     0.94119996     0.94469994     0.93729991
[37m[1m      0.92910004]]
[37m[1m[2023-06-25 07:21:01,217][129146] Max Reward on eval: 154.43583901486127
[37m[1m[2023-06-25 07:21:01,218][129146] Min Reward on eval: -1927.6598445126087
[37m[1m[2023-06-25 07:21:01,218][129146] Mean Reward across all agents: -615.2282930061046
[37m[1m[2023-06-25 07:21:01,218][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:21:01,220][129146] mean_value=-1083.4002076167612, max_value=506.8742354133865
[37m[1m[2023-06-25 07:21:01,222][129146] New mean coefficients: [[-0.6458894  0.9874837  3.5602808 12.454495   4.03682  ]]
[37m[1m[2023-06-25 07:21:01,223][129146] Moving the mean solution point...
[36m[2023-06-25 07:21:10,923][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 07:21:10,923][129146] FPS: 395948.02
[36m[2023-06-25 07:21:10,926][129146] itr=685, itrs=2000, Progress: 34.25%
[36m[2023-06-25 07:21:22,465][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 07:21:22,465][129146] FPS: 333385.91
[36m[2023-06-25 07:21:27,201][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:21:27,202][129146] Reward + Measures: [[-1028.74028777     0.78595835     0.89327699     0.8137666
[37m[1m      0.92565805]]
[37m[1m[2023-06-25 07:21:27,203][129146] Max Reward on eval: -1028.7402877662928
[37m[1m[2023-06-25 07:21:27,203][129146] Min Reward on eval: -1028.7402877662928
[37m[1m[2023-06-25 07:21:27,203][129146] Mean Reward across all agents: -1028.7402877662928
[37m[1m[2023-06-25 07:21:27,203][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:21:32,733][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:21:32,739][129146] Reward + Measures: [[ 264.73861307    0.57859999    0.60320002    0.6311        0.68150008]
[37m[1m [-321.1185708     0.3976        0.86849993    0.54820001    0.88799995]
[37m[1m [ 302.78026446    0.47910005    0.64120001    0.55769998    0.66730005]
[37m[1m ...
[37m[1m [ 189.11336207    0.53299999    0.6663        0.60460001    0.7123    ]
[37m[1m [ 328.76139412    0.61040002    0.61879998    0.61280006    0.66530001]
[37m[1m [ 196.66760957    0.47530004    0.6498        0.5869        0.69279999]]
[37m[1m[2023-06-25 07:21:32,739][129146] Max Reward on eval: 469.11848440618485
[37m[1m[2023-06-25 07:21:32,739][129146] Min Reward on eval: -703.054387597437
[37m[1m[2023-06-25 07:21:32,739][129146] Mean Reward across all agents: -7.041270092277861
[37m[1m[2023-06-25 07:21:32,740][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:21:32,746][129146] mean_value=103.60322605259185, max_value=572.7858929322504
[37m[1m[2023-06-25 07:21:32,749][129146] New mean coefficients: [[ 0.2809192  0.8316171  3.695601  12.148217   4.969091 ]]
[37m[1m[2023-06-25 07:21:32,749][129146] Moving the mean solution point...
[36m[2023-06-25 07:21:42,474][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 07:21:42,474][129146] FPS: 394961.04
[36m[2023-06-25 07:21:42,476][129146] itr=686, itrs=2000, Progress: 34.30%
[36m[2023-06-25 07:21:53,880][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 07:21:53,880][129146] FPS: 337381.65
[36m[2023-06-25 07:21:58,704][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:21:58,704][129146] Reward + Measures: [[-1036.94854453     0.85343373     0.93305665     0.8847993
[37m[1m      0.95609498]]
[37m[1m[2023-06-25 07:21:58,704][129146] Max Reward on eval: -1036.9485445304879
[37m[1m[2023-06-25 07:21:58,704][129146] Min Reward on eval: -1036.9485445304879
[37m[1m[2023-06-25 07:21:58,705][129146] Mean Reward across all agents: -1036.9485445304879
[37m[1m[2023-06-25 07:21:58,705][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:22:04,077][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:22:04,078][129146] Reward + Measures: [[ -495.61016935     0.65720004     0.87309998     0.75030005
[37m[1m      0.8969    ]
[37m[1m [-1086.92941039     0.94550002     0.96989995     0.95230007
[37m[1m      0.97589999]
[37m[1m [-1286.78048706     0.94530004     0.95739996     0.9339
[37m[1m      0.94560003]
[37m[1m ...
[37m[1m [  -38.05602388     0.41910002     0.8351         0.54899997
[37m[1m      0.81210005]
[37m[1m [  370.85605176     0.4587         0.63459998     0.57820004
[37m[1m      0.69870001]
[37m[1m [-1022.32617264     0.85980004     0.91780007     0.8768
[37m[1m      0.93240005]]
[37m[1m[2023-06-25 07:22:04,078][129146] Max Reward on eval: 523.4932444464299
[37m[1m[2023-06-25 07:22:04,078][129146] Min Reward on eval: -1487.2376000009476
[37m[1m[2023-06-25 07:22:04,078][129146] Mean Reward across all agents: -653.5285235529601
[37m[1m[2023-06-25 07:22:04,079][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:22:04,083][129146] mean_value=-821.1100566060176, max_value=910.1296874378132
[37m[1m[2023-06-25 07:22:04,086][129146] New mean coefficients: [[ 2.8282833  0.5110316  1.9169115 11.403692   3.5665865]]
[37m[1m[2023-06-25 07:22:04,087][129146] Moving the mean solution point...
[36m[2023-06-25 07:22:13,753][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 07:22:13,753][129146] FPS: 397348.29
[36m[2023-06-25 07:22:13,755][129146] itr=687, itrs=2000, Progress: 34.35%
[36m[2023-06-25 07:22:25,183][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 07:22:25,183][129146] FPS: 336642.57
[36m[2023-06-25 07:22:29,997][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:22:29,997][129146] Reward + Measures: [[-508.34953337    0.98864436    0.99180931    0.98150861    0.98777133]]
[37m[1m[2023-06-25 07:22:29,997][129146] Max Reward on eval: -508.3495333689045
[37m[1m[2023-06-25 07:22:29,997][129146] Min Reward on eval: -508.3495333689045
[37m[1m[2023-06-25 07:22:29,998][129146] Mean Reward across all agents: -508.3495333689045
[37m[1m[2023-06-25 07:22:29,998][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:22:35,327][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:22:35,332][129146] Reward + Measures: [[ 220.60634711    0.34940001    0.74309999    0.51550001    0.73450005]
[37m[1m [ 342.25407365    0.33690003    0.73540002    0.49169999    0.70680004]
[37m[1m [  82.05178981    0.41009998    0.73520005    0.58130002    0.67809993]
[37m[1m ...
[37m[1m [ 368.34264043    0.34630004    0.74050003    0.4578        0.69670004]
[37m[1m [ 304.89761073    0.19350001    0.80310005    0.51459998    0.79040003]
[37m[1m [-191.66369639    0.91280001    0.84190005    0.87799996    0.77469999]]
[37m[1m[2023-06-25 07:22:35,333][129146] Max Reward on eval: 506.52992415998597
[37m[1m[2023-06-25 07:22:35,333][129146] Min Reward on eval: -559.8629772051936
[37m[1m[2023-06-25 07:22:35,333][129146] Mean Reward across all agents: 162.5222772189996
[37m[1m[2023-06-25 07:22:35,334][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:22:35,339][129146] mean_value=141.51158249036402, max_value=766.2763362345441
[37m[1m[2023-06-25 07:22:35,342][129146] New mean coefficients: [[ 1.9447548  0.6581435  3.9131584 11.470615   4.4826756]]
[37m[1m[2023-06-25 07:22:35,343][129146] Moving the mean solution point...
[36m[2023-06-25 07:22:45,029][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 07:22:45,029][129146] FPS: 396538.86
[36m[2023-06-25 07:22:45,031][129146] itr=688, itrs=2000, Progress: 34.40%
[36m[2023-06-25 07:22:56,430][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 07:22:56,430][129146] FPS: 337568.35
[36m[2023-06-25 07:23:01,155][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:23:01,156][129146] Reward + Measures: [[619.13848103   0.630234     0.56974268   0.63728368   0.67027199]]
[37m[1m[2023-06-25 07:23:01,156][129146] Max Reward on eval: 619.1384810326394
[37m[1m[2023-06-25 07:23:01,156][129146] Min Reward on eval: 619.1384810326394
[37m[1m[2023-06-25 07:23:01,157][129146] Mean Reward across all agents: 619.1384810326394
[37m[1m[2023-06-25 07:23:01,157][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:23:06,578][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:23:06,584][129146] Reward + Measures: [[366.17912076   0.66860002   0.73949999   0.51200002   0.70570004]
[37m[1m [238.24316996   0.59980005   0.73270005   0.55419999   0.61910003]
[37m[1m [145.90538743   0.69500011   0.86180001   0.4285       0.69150001]
[37m[1m ...
[37m[1m [423.07848302   0.71380001   0.78170002   0.69840002   0.69920009]
[37m[1m [373.57144228   0.71750003   0.76920003   0.65109998   0.68009996]
[37m[1m [359.70798648   0.64060003   0.77740002   0.51810002   0.75390005]]
[37m[1m[2023-06-25 07:23:06,584][129146] Max Reward on eval: 579.3386674895067
[37m[1m[2023-06-25 07:23:06,584][129146] Min Reward on eval: -61.823588961426864
[37m[1m[2023-06-25 07:23:06,585][129146] Mean Reward across all agents: 346.0253041848128
[37m[1m[2023-06-25 07:23:06,585][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:23:06,591][129146] mean_value=186.4218281544711, max_value=942.668190007814
[37m[1m[2023-06-25 07:23:06,594][129146] New mean coefficients: [[ 1.9409777   0.86634094  5.0067897  11.169133    3.8509207 ]]
[37m[1m[2023-06-25 07:23:06,595][129146] Moving the mean solution point...
[36m[2023-06-25 07:23:16,328][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 07:23:16,328][129146] FPS: 394584.08
[36m[2023-06-25 07:23:16,331][129146] itr=689, itrs=2000, Progress: 34.45%
[36m[2023-06-25 07:23:27,747][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 07:23:27,747][129146] FPS: 337070.88
[36m[2023-06-25 07:23:32,540][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:23:32,540][129146] Reward + Measures: [[564.46551928   0.56946135   0.70650631   0.6477623    0.67684799]]
[37m[1m[2023-06-25 07:23:32,541][129146] Max Reward on eval: 564.465519284673
[37m[1m[2023-06-25 07:23:32,541][129146] Min Reward on eval: 564.465519284673
[37m[1m[2023-06-25 07:23:32,541][129146] Mean Reward across all agents: 564.465519284673
[37m[1m[2023-06-25 07:23:32,541][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:23:37,917][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:23:37,918][129146] Reward + Measures: [[-528.53152228    0.60020006    0.94950002    0.0915        0.89399999]
[37m[1m [ 108.6485447     0.6523        0.7924        0.37750003    0.65259999]
[37m[1m [-878.9070872     0.73909998    0.95760006    0.06590001    0.9307    ]
[37m[1m ...
[37m[1m [-542.32781299    0.6013        0.88050002    0.1479        0.84570009]
[37m[1m [ 285.72519292    0.72600001    0.68080002    0.53780001    0.54910004]
[37m[1m [ -85.09885048    0.5273        0.82950002    0.296         0.76820004]]
[37m[1m[2023-06-25 07:23:37,918][129146] Max Reward on eval: 476.22573102304597
[37m[1m[2023-06-25 07:23:37,918][129146] Min Reward on eval: -1622.8395955004962
[37m[1m[2023-06-25 07:23:37,919][129146] Mean Reward across all agents: -379.597283181214
[37m[1m[2023-06-25 07:23:37,919][129146] Average Trajectory Length: 998.6176666666667
[36m[2023-06-25 07:23:37,922][129146] mean_value=-550.8026376915004, max_value=497.8261747764377
[37m[1m[2023-06-25 07:23:37,925][129146] New mean coefficients: [[ 1.2144942   0.88749933  5.6391     10.770725    3.780582  ]]
[37m[1m[2023-06-25 07:23:37,926][129146] Moving the mean solution point...
[36m[2023-06-25 07:23:47,534][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 07:23:47,535][129146] FPS: 399695.54
[36m[2023-06-25 07:23:47,537][129146] itr=690, itrs=2000, Progress: 34.50%
[37m[1m[2023-06-25 07:23:53,529][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000670
[36m[2023-06-25 07:24:05,342][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 07:24:05,343][129146] FPS: 332244.11
[36m[2023-06-25 07:24:09,999][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:24:10,000][129146] Reward + Measures: [[533.40413351   0.55026203   0.77831072   0.67838097   0.70757371]]
[37m[1m[2023-06-25 07:24:10,000][129146] Max Reward on eval: 533.404133506734
[37m[1m[2023-06-25 07:24:10,000][129146] Min Reward on eval: 533.404133506734
[37m[1m[2023-06-25 07:24:10,000][129146] Mean Reward across all agents: 533.404133506734
[37m[1m[2023-06-25 07:24:10,000][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:24:15,624][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:24:15,625][129146] Reward + Measures: [[106.95874454   0.89309996   0.88950008   0.80219996   0.82030004]
[37m[1m [361.29039019   0.84800005   0.89099997   0.76969999   0.79609996]
[37m[1m [178.81355202   0.88100004   0.89710009   0.78120005   0.81550008]
[37m[1m ...
[37m[1m [211.01110122   0.89330006   0.91909999   0.8106001    0.84860003]
[37m[1m [146.82625352   0.72180003   0.80489999   0.5564       0.72410005]
[37m[1m [278.24696254   0.89780009   0.92119998   0.81730002   0.8351    ]]
[37m[1m[2023-06-25 07:24:15,625][129146] Max Reward on eval: 597.9550469957758
[37m[1m[2023-06-25 07:24:15,625][129146] Min Reward on eval: -41.16055593836936
[37m[1m[2023-06-25 07:24:15,625][129146] Mean Reward across all agents: 256.48247180349404
[37m[1m[2023-06-25 07:24:15,626][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:24:15,627][129146] mean_value=-429.83327976726736, max_value=624.887795965328
[37m[1m[2023-06-25 07:24:15,630][129146] New mean coefficients: [[ 0.72358    0.9535456  4.552481  11.505328   4.734888 ]]
[37m[1m[2023-06-25 07:24:15,631][129146] Moving the mean solution point...
[36m[2023-06-25 07:24:25,462][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 07:24:25,462][129146] FPS: 390667.16
[36m[2023-06-25 07:24:25,464][129146] itr=691, itrs=2000, Progress: 34.55%
[36m[2023-06-25 07:24:36,891][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 07:24:36,891][129146] FPS: 336663.33
[36m[2023-06-25 07:24:41,641][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:24:41,641][129146] Reward + Measures: [[419.82229788   0.48917633   0.8422066    0.72253758   0.7487396 ]]
[37m[1m[2023-06-25 07:24:41,642][129146] Max Reward on eval: 419.8222978754317
[37m[1m[2023-06-25 07:24:41,642][129146] Min Reward on eval: 419.8222978754317
[37m[1m[2023-06-25 07:24:41,642][129146] Mean Reward across all agents: 419.8222978754317
[37m[1m[2023-06-25 07:24:41,642][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:24:46,943][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:24:46,949][129146] Reward + Measures: [[ 131.59820592    0.7446        0.90319997    0.66020006    0.81759995]
[37m[1m [ 171.36393341    0.79480004    0.93269998    0.72139996    0.85949993]
[37m[1m [ 316.00633906    0.74690002    0.92269993    0.57429999    0.85109997]
[37m[1m ...
[37m[1m [ -50.48679712    0.84580004    0.92410004    0.80599993    0.87790006]
[37m[1m [  40.75761769    0.81119996    0.90700001    0.77130002    0.85249996]
[37m[1m [-167.96799016    0.76630002    0.88289994    0.6972        0.81990004]]
[37m[1m[2023-06-25 07:24:46,949][129146] Max Reward on eval: 418.94597383615326
[37m[1m[2023-06-25 07:24:46,950][129146] Min Reward on eval: -394.14961072466104
[37m[1m[2023-06-25 07:24:46,950][129146] Mean Reward across all agents: 68.14553583096915
[37m[1m[2023-06-25 07:24:46,950][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:24:46,955][129146] mean_value=141.59615805155602, max_value=851.3623012866825
[37m[1m[2023-06-25 07:24:46,958][129146] New mean coefficients: [[ 1.8334467  1.0614748  5.364353  10.709516   3.4322195]]
[37m[1m[2023-06-25 07:24:46,959][129146] Moving the mean solution point...
[36m[2023-06-25 07:24:56,644][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 07:24:56,645][129146] FPS: 396523.74
[36m[2023-06-25 07:24:56,647][129146] itr=692, itrs=2000, Progress: 34.60%
[36m[2023-06-25 07:25:08,058][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 07:25:08,059][129146] FPS: 337104.21
[36m[2023-06-25 07:25:12,698][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:25:12,698][129146] Reward + Measures: [[383.25538394   0.57800436   0.87415361   0.74563462   0.78657329]]
[37m[1m[2023-06-25 07:25:12,698][129146] Max Reward on eval: 383.25538393645746
[37m[1m[2023-06-25 07:25:12,698][129146] Min Reward on eval: 383.25538393645746
[37m[1m[2023-06-25 07:25:12,698][129146] Mean Reward across all agents: 383.25538393645746
[37m[1m[2023-06-25 07:25:12,699][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:25:18,068][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:25:18,069][129146] Reward + Measures: [[  43.46995031    0.78119999    0.1876        0.76719999    0.60780001]
[37m[1m [ 100.51110193    0.64740002    0.8696        0.62080002    0.82670003]
[37m[1m [-118.98749187    0.86970007    0.40270001    0.86140007    0.82089996]
[37m[1m ...
[37m[1m [  57.00693542    0.87080002    0.56990004    0.89029998    0.84740001]
[37m[1m [  30.68662604    0.92900002    0.61039996    0.93470001    0.92270005]
[37m[1m [ 166.1291823     0.82139999    0.61350006    0.87299997    0.88390011]]
[37m[1m[2023-06-25 07:25:18,069][129146] Max Reward on eval: 404.49332126555964
[37m[1m[2023-06-25 07:25:18,069][129146] Min Reward on eval: -890.3417863430456
[37m[1m[2023-06-25 07:25:18,070][129146] Mean Reward across all agents: 113.34788487591274
[37m[1m[2023-06-25 07:25:18,070][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:25:18,077][129146] mean_value=128.91168097918444, max_value=750.086941718717
[37m[1m[2023-06-25 07:25:18,079][129146] New mean coefficients: [[ 1.5819826  1.0076292  5.4928966 10.66349    3.875292 ]]
[37m[1m[2023-06-25 07:25:18,080][129146] Moving the mean solution point...
[36m[2023-06-25 07:25:27,637][129146] train() took 9.55 seconds to complete
[36m[2023-06-25 07:25:27,637][129146] FPS: 401888.95
[36m[2023-06-25 07:25:27,639][129146] itr=693, itrs=2000, Progress: 34.65%
[36m[2023-06-25 07:25:39,028][129146] train() took 11.37 seconds to complete
[36m[2023-06-25 07:25:39,028][129146] FPS: 337809.65
[36m[2023-06-25 07:25:43,759][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:25:43,760][129146] Reward + Measures: [[272.2418892    0.7682026    0.917        0.76261067   0.82965499]]
[37m[1m[2023-06-25 07:25:43,760][129146] Max Reward on eval: 272.24188919696843
[37m[1m[2023-06-25 07:25:43,760][129146] Min Reward on eval: 272.24188919696843
[37m[1m[2023-06-25 07:25:43,760][129146] Mean Reward across all agents: 272.24188919696843
[37m[1m[2023-06-25 07:25:43,761][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:25:49,221][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:25:49,221][129146] Reward + Measures: [[ -628.89266089     0.73659998     0.74700004     0.47049999
[37m[1m      0.64300007]
[37m[1m [  491.59748134     0.62670004     0.51340002     0.56650001
[37m[1m      0.43100005]
[37m[1m [ -355.18283188     0.81140006     0.62120003     0.75180006
[37m[1m      0.41630003]
[37m[1m ...
[37m[1m [  494.39876919     0.68970001     0.51049995     0.60120004
[37m[1m      0.26290002]
[37m[1m [-1355.44064831     0.48680001     0.59920001     0.30130002
[37m[1m      0.5862    ]
[37m[1m [ -189.0753644      0.89410001     0.60480005     0.72610003
[37m[1m      0.32029998]]
[37m[1m[2023-06-25 07:25:49,221][129146] Max Reward on eval: 591.7465742695844
[37m[1m[2023-06-25 07:25:49,222][129146] Min Reward on eval: -1355.440648310294
[37m[1m[2023-06-25 07:25:49,222][129146] Mean Reward across all agents: -178.7910883272208
[37m[1m[2023-06-25 07:25:49,222][129146] Average Trajectory Length: 999.5543333333333
[36m[2023-06-25 07:25:49,228][129146] mean_value=-185.15760600805254, max_value=1031.791416161065
[37m[1m[2023-06-25 07:25:49,231][129146] New mean coefficients: [[ 0.24830163  1.1398407   7.8048944  10.625659    4.8537683 ]]
[37m[1m[2023-06-25 07:25:49,232][129146] Moving the mean solution point...
[36m[2023-06-25 07:25:58,862][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 07:25:58,862][129146] FPS: 398838.62
[36m[2023-06-25 07:25:58,864][129146] itr=694, itrs=2000, Progress: 34.70%
[36m[2023-06-25 07:26:10,454][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 07:26:10,455][129146] FPS: 332063.27
[36m[2023-06-25 07:26:15,187][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:26:15,187][129146] Reward + Measures: [[157.84833802   0.85262901   0.95523632   0.80067772   0.88675034]]
[37m[1m[2023-06-25 07:26:15,187][129146] Max Reward on eval: 157.84833801929156
[37m[1m[2023-06-25 07:26:15,187][129146] Min Reward on eval: 157.84833801929156
[37m[1m[2023-06-25 07:26:15,188][129146] Mean Reward across all agents: 157.84833801929156
[37m[1m[2023-06-25 07:26:15,188][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:26:20,705][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:26:20,711][129146] Reward + Measures: [[-331.71317042    0.98940003    0.98400003    0.98490012    0.97489995]
[37m[1m [ 189.6528295     0.58359998    0.90170002    0.71660006    0.89020008]
[37m[1m [-145.27116944    0.91040003    0.97200006    0.90680009    0.95569992]
[37m[1m ...
[37m[1m [-606.45797895    0.92740005    0.78490001    0.85680002    0.5728001 ]
[37m[1m [-219.14662721    0.97460002    0.96919996    0.95340008    0.95380002]
[37m[1m [-271.39289029    0.99049997    0.96999997    0.96880001    0.78180003]]
[37m[1m[2023-06-25 07:26:20,711][129146] Max Reward on eval: 341.2461200785707
[37m[1m[2023-06-25 07:26:20,712][129146] Min Reward on eval: -630.1006664619548
[37m[1m[2023-06-25 07:26:20,712][129146] Mean Reward across all agents: -100.01738667937389
[37m[1m[2023-06-25 07:26:20,712][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:26:20,720][129146] mean_value=-68.62455777152456, max_value=799.9304526652908
[37m[1m[2023-06-25 07:26:20,723][129146] New mean coefficients: [[-0.06401122  0.9688144   8.129022    9.829821    5.1783333 ]]
[37m[1m[2023-06-25 07:26:20,724][129146] Moving the mean solution point...
[36m[2023-06-25 07:26:30,540][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 07:26:30,541][129146] FPS: 391236.51
[36m[2023-06-25 07:26:30,543][129146] itr=695, itrs=2000, Progress: 34.75%
[36m[2023-06-25 07:26:42,066][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 07:26:42,066][129146] FPS: 333854.58
[36m[2023-06-25 07:26:46,948][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:26:46,949][129146] Reward + Measures: [[112.07358916   0.22719799   0.86279166   0.84660894   0.76938134]]
[37m[1m[2023-06-25 07:26:46,949][129146] Max Reward on eval: 112.07358916363951
[37m[1m[2023-06-25 07:26:46,949][129146] Min Reward on eval: 112.07358916363951
[37m[1m[2023-06-25 07:26:46,950][129146] Mean Reward across all agents: 112.07358916363951
[37m[1m[2023-06-25 07:26:46,950][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:26:52,545][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:26:52,545][129146] Reward + Measures: [[ 246.38514211    0.36420003    0.58590001    0.47890002    0.52850002]
[37m[1m [ 124.72941971    0.62880003    0.4677        0.56890005    0.18190001]
[37m[1m [  -1.81449022    0.33239999    0.50010002    0.46280003    0.59499997]
[37m[1m ...
[37m[1m [ 349.95805225    0.60269994    0.66119999    0.59200001    0.2775    ]
[37m[1m [   0.50704037    0.61619997    0.6512        0.63999999    0.14820001]
[37m[1m [-278.68871593    0.38869998    0.73400003    0.69699997    0.58050007]]
[37m[1m[2023-06-25 07:26:52,545][129146] Max Reward on eval: 630.8797842440778
[37m[1m[2023-06-25 07:26:52,546][129146] Min Reward on eval: -2196.0590831077193
[37m[1m[2023-06-25 07:26:52,546][129146] Mean Reward across all agents: 38.13685609855862
[37m[1m[2023-06-25 07:26:52,546][129146] Average Trajectory Length: 994.434
[36m[2023-06-25 07:26:52,554][129146] mean_value=-47.93539523100798, max_value=940.242151454424
[37m[1m[2023-06-25 07:26:52,556][129146] New mean coefficients: [[-0.6453495   0.76176953  7.929264    8.932682    5.3123684 ]]
[37m[1m[2023-06-25 07:26:52,557][129146] Moving the mean solution point...
[36m[2023-06-25 07:27:02,151][129146] train() took 9.59 seconds to complete
[36m[2023-06-25 07:27:02,151][129146] FPS: 400333.96
[36m[2023-06-25 07:27:02,153][129146] itr=696, itrs=2000, Progress: 34.80%
[36m[2023-06-25 07:27:13,678][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 07:27:13,678][129146] FPS: 333904.75
[36m[2023-06-25 07:27:18,269][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:27:18,270][129146] Reward + Measures: [[-21.55577782   0.22613366   0.88637269   0.87579393   0.77387697]]
[37m[1m[2023-06-25 07:27:18,270][129146] Max Reward on eval: -21.555777820362
[37m[1m[2023-06-25 07:27:18,270][129146] Min Reward on eval: -21.555777820362
[37m[1m[2023-06-25 07:27:18,271][129146] Mean Reward across all agents: -21.555777820362
[37m[1m[2023-06-25 07:27:18,271][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:27:23,627][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:27:23,632][129146] Reward + Measures: [[ 386.46283586    0.55790001    0.49340001    0.50460005    0.37990001]
[37m[1m [ 489.10174636    0.47440001    0.47830001    0.56029999    0.4395    ]
[37m[1m [-235.38697334    0.16860001    0.73550004    0.66180003    0.77609998]
[37m[1m ...
[37m[1m [-235.98041032    0.26030001    0.74480003    0.69230002    0.81879997]
[37m[1m [ 295.32945087    0.35160002    0.60360003    0.57660002    0.5223    ]
[37m[1m [ 703.41197513    0.52399999    0.53190005    0.50739998    0.49370003]]
[37m[1m[2023-06-25 07:27:23,633][129146] Max Reward on eval: 703.4119751274236
[37m[1m[2023-06-25 07:27:23,633][129146] Min Reward on eval: -1029.1738217993873
[37m[1m[2023-06-25 07:27:23,633][129146] Mean Reward across all agents: 163.80923029990396
[37m[1m[2023-06-25 07:27:23,634][129146] Average Trajectory Length: 999.353
[36m[2023-06-25 07:27:23,641][129146] mean_value=23.56721950533276, max_value=943.3901181303665
[37m[1m[2023-06-25 07:27:23,644][129146] New mean coefficients: [[0.01366675 0.7682196  7.0218587  8.85733    4.6139193 ]]
[37m[1m[2023-06-25 07:27:23,644][129146] Moving the mean solution point...
[36m[2023-06-25 07:27:33,211][129146] train() took 9.56 seconds to complete
[36m[2023-06-25 07:27:33,211][129146] FPS: 401475.18
[36m[2023-06-25 07:27:33,213][129146] itr=697, itrs=2000, Progress: 34.85%
[36m[2023-06-25 07:27:44,681][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 07:27:44,682][129146] FPS: 335495.09
[36m[2023-06-25 07:27:49,520][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:27:49,520][129146] Reward + Measures: [[-25.92481067   0.233301     0.89771998   0.89057404   0.77000344]]
[37m[1m[2023-06-25 07:27:49,521][129146] Max Reward on eval: -25.924810674121137
[37m[1m[2023-06-25 07:27:49,521][129146] Min Reward on eval: -25.924810674121137
[37m[1m[2023-06-25 07:27:49,521][129146] Mean Reward across all agents: -25.924810674121137
[37m[1m[2023-06-25 07:27:49,522][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:27:55,050][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:27:55,051][129146] Reward + Measures: [[431.47279784   0.99650002   0.97399998   0.97920001   0.002     ]
[37m[1m [ 73.15438691   0.75400001   0.84250003   0.78290004   0.38610002]
[37m[1m [ 33.86747203   0.87340015   0.79119998   0.90630001   0.21630001]
[37m[1m ...
[37m[1m [  1.2818632    0.96639997   0.8077001    0.93110001   0.42910004]
[37m[1m [-62.7861353    0.95880002   0.63850003   0.92539996   0.204     ]
[37m[1m [140.52068219   0.60420007   0.7978       0.75659996   0.44500002]]
[37m[1m[2023-06-25 07:27:55,051][129146] Max Reward on eval: 431.4727978350711
[37m[1m[2023-06-25 07:27:55,051][129146] Min Reward on eval: -1068.4448497212493
[37m[1m[2023-06-25 07:27:55,051][129146] Mean Reward across all agents: -80.05114242879525
[37m[1m[2023-06-25 07:27:55,052][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:27:55,062][129146] mean_value=76.76156229912223, max_value=815.7449640736334
[37m[1m[2023-06-25 07:27:55,064][129146] New mean coefficients: [[0.4806594 0.7898347 6.550702  9.112033  3.7584577]]
[37m[1m[2023-06-25 07:27:55,065][129146] Moving the mean solution point...
[36m[2023-06-25 07:28:04,955][129146] train() took 9.89 seconds to complete
[36m[2023-06-25 07:28:04,956][129146] FPS: 388337.58
[36m[2023-06-25 07:28:04,958][129146] itr=698, itrs=2000, Progress: 34.90%
[36m[2023-06-25 07:28:16,488][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 07:28:16,488][129146] FPS: 333713.81
[36m[2023-06-25 07:28:21,446][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:28:21,446][129146] Reward + Measures: [[11.55292493  0.240171    0.90526766  0.90042168  0.7670083 ]]
[37m[1m[2023-06-25 07:28:21,446][129146] Max Reward on eval: 11.552924925635143
[37m[1m[2023-06-25 07:28:21,447][129146] Min Reward on eval: 11.552924925635143
[37m[1m[2023-06-25 07:28:21,447][129146] Mean Reward across all agents: 11.552924925635143
[37m[1m[2023-06-25 07:28:21,447][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:28:26,900][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:28:26,900][129146] Reward + Measures: [[214.29863416   0.84429997   0.2933       0.7784       0.46599999]
[37m[1m [280.94932743   0.87110007   0.23889999   0.79820001   0.57919997]
[37m[1m [172.52726072   0.84300005   0.56840003   0.79700005   0.47919998]
[37m[1m ...
[37m[1m [ 48.15622485   0.88930005   0.2045       0.80950004   0.61420006]
[37m[1m [301.72434468   0.84380007   0.30960003   0.79360002   0.47790003]
[37m[1m [348.04171931   0.82700008   0.26500002   0.79579997   0.52920002]]
[37m[1m[2023-06-25 07:28:26,900][129146] Max Reward on eval: 604.0251618444396
[37m[1m[2023-06-25 07:28:26,901][129146] Min Reward on eval: -970.1850488008349
[37m[1m[2023-06-25 07:28:26,901][129146] Mean Reward across all agents: 131.3186382215799
[37m[1m[2023-06-25 07:28:26,901][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:28:26,910][129146] mean_value=219.02098970882128, max_value=833.2642337826313
[37m[1m[2023-06-25 07:28:26,913][129146] New mean coefficients: [[0.76239824 0.65883344 6.284693   8.376675   3.7034743 ]]
[37m[1m[2023-06-25 07:28:26,914][129146] Moving the mean solution point...
[36m[2023-06-25 07:28:36,668][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 07:28:36,669][129146] FPS: 393725.59
[36m[2023-06-25 07:28:36,671][129146] itr=699, itrs=2000, Progress: 34.95%
[36m[2023-06-25 07:28:48,127][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 07:28:48,127][129146] FPS: 335881.80
[36m[2023-06-25 07:28:52,936][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:28:52,936][129146] Reward + Measures: [[-28.75901601   0.38867331   0.8908273    0.92860401   0.64311099]]
[37m[1m[2023-06-25 07:28:52,936][129146] Max Reward on eval: -28.75901600914653
[37m[1m[2023-06-25 07:28:52,937][129146] Min Reward on eval: -28.75901600914653
[37m[1m[2023-06-25 07:28:52,937][129146] Mean Reward across all agents: -28.75901600914653
[37m[1m[2023-06-25 07:28:52,937][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:28:58,384][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:28:58,385][129146] Reward + Measures: [[368.99055687   0.228        0.63569999   0.58829999   0.65620005]
[37m[1m [482.7227698    0.43710002   0.65890002   0.73440003   0.597     ]
[37m[1m [ 37.90979383   0.16759999   0.78450006   0.61740005   0.76770002]
[37m[1m ...
[37m[1m [294.73032791   0.2922       0.86250001   0.60109997   0.82770008]
[37m[1m [611.24184545   0.1725       0.71540004   0.52320004   0.74239999]
[37m[1m [354.60138575   0.23029999   0.86129999   0.79689997   0.77170002]]
[37m[1m[2023-06-25 07:28:58,385][129146] Max Reward on eval: 638.9951358696446
[37m[1m[2023-06-25 07:28:58,385][129146] Min Reward on eval: -668.9194650731399
[37m[1m[2023-06-25 07:28:58,385][129146] Mean Reward across all agents: 216.97016313207348
[37m[1m[2023-06-25 07:28:58,386][129146] Average Trajectory Length: 999.723
[36m[2023-06-25 07:28:58,397][129146] mean_value=292.5877003641294, max_value=1025.7596785433939
[37m[1m[2023-06-25 07:28:58,400][129146] New mean coefficients: [[1.202875  0.8383331 5.606473  7.565898  2.101397 ]]
[37m[1m[2023-06-25 07:28:58,401][129146] Moving the mean solution point...
[36m[2023-06-25 07:29:08,138][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 07:29:08,138][129146] FPS: 394442.74
[36m[2023-06-25 07:29:08,141][129146] itr=700, itrs=2000, Progress: 35.00%
[37m[1m[2023-06-25 07:29:14,570][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000680
[36m[2023-06-25 07:29:26,425][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 07:29:26,425][129146] FPS: 330841.34
[36m[2023-06-25 07:29:31,214][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:29:31,214][129146] Reward + Measures: [[40.29376639  0.428978    0.90604705  0.93576461  0.60778338]]
[37m[1m[2023-06-25 07:29:31,214][129146] Max Reward on eval: 40.29376639026859
[37m[1m[2023-06-25 07:29:31,215][129146] Min Reward on eval: 40.29376639026859
[37m[1m[2023-06-25 07:29:31,215][129146] Mean Reward across all agents: 40.29376639026859
[37m[1m[2023-06-25 07:29:31,215][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:29:36,687][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:29:36,688][129146] Reward + Measures: [[278.37189265   0.69580001   0.54580003   0.60000002   0.54350001]
[37m[1m [242.04565415   0.96870005   0.65940005   0.94389993   0.0181    ]
[37m[1m [120.32647703   0.82490009   0.45409998   0.759        0.0909    ]
[37m[1m ...
[37m[1m [195.46822789   0.95529997   0.50279999   0.91609997   0.056     ]
[37m[1m [126.35579203   0.6063       0.3901       0.45970002   0.30420002]
[37m[1m [206.53947273   0.90280002   0.51910001   0.87989998   0.45809999]]
[37m[1m[2023-06-25 07:29:36,688][129146] Max Reward on eval: 506.7990573505638
[37m[1m[2023-06-25 07:29:36,689][129146] Min Reward on eval: -651.4665133852511
[37m[1m[2023-06-25 07:29:36,689][129146] Mean Reward across all agents: 220.12718773760193
[37m[1m[2023-06-25 07:29:36,689][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:29:36,697][129146] mean_value=213.05428285354827, max_value=789.9675146146694
[37m[1m[2023-06-25 07:29:36,700][129146] New mean coefficients: [[1.1431941 0.7138224 5.911911  6.952727  1.9811953]]
[37m[1m[2023-06-25 07:29:36,701][129146] Moving the mean solution point...
[36m[2023-06-25 07:29:46,392][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 07:29:46,392][129146] FPS: 396318.55
[36m[2023-06-25 07:29:46,394][129146] itr=701, itrs=2000, Progress: 35.05%
[36m[2023-06-25 07:29:57,928][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 07:29:57,929][129146] FPS: 333546.38
[36m[2023-06-25 07:30:02,610][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:30:02,611][129146] Reward + Measures: [[-708.69923685    0.99532264    0.96717042    0.9890576     0.085278  ]]
[37m[1m[2023-06-25 07:30:02,611][129146] Max Reward on eval: -708.6992368482609
[37m[1m[2023-06-25 07:30:02,611][129146] Min Reward on eval: -708.6992368482609
[37m[1m[2023-06-25 07:30:02,611][129146] Mean Reward across all agents: -708.6992368482609
[37m[1m[2023-06-25 07:30:02,612][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:30:08,015][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:30:08,016][129146] Reward + Measures: [[ -51.60575908    0.49810001    0.88459998    0.91310006    0.47780004]
[37m[1m [  93.47037411    0.51520002    0.60550004    0.17830001    0.61700004]
[37m[1m [-386.41920352    0.85260004    0.8143        0.87059993    0.28659999]
[37m[1m ...
[37m[1m [-497.4585249     0.8132        0.75920004    0.72390002    0.72960001]
[37m[1m [ -50.31629079    0.86349994    0.85519999    0.61979997    0.83350003]
[37m[1m [-190.71581018    0.86700004    0.83199996    0.74439996    0.80410004]]
[37m[1m[2023-06-25 07:30:08,016][129146] Max Reward on eval: 561.3385749298031
[37m[1m[2023-06-25 07:30:08,016][129146] Min Reward on eval: -1095.915705237817
[37m[1m[2023-06-25 07:30:08,016][129146] Mean Reward across all agents: -126.19601353363822
[37m[1m[2023-06-25 07:30:08,017][129146] Average Trajectory Length: 999.4756666666666
[36m[2023-06-25 07:30:08,023][129146] mean_value=-293.799084504825, max_value=898.4374125151196
[37m[1m[2023-06-25 07:30:08,026][129146] New mean coefficients: [[1.1075718  0.588743   6.2750754  6.7688766  0.76525867]]
[37m[1m[2023-06-25 07:30:08,027][129146] Moving the mean solution point...
[36m[2023-06-25 07:30:17,711][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 07:30:17,712][129146] FPS: 396596.47
[36m[2023-06-25 07:30:17,714][129146] itr=702, itrs=2000, Progress: 35.10%
[36m[2023-06-25 07:30:29,367][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 07:30:29,368][129146] FPS: 330124.88
[36m[2023-06-25 07:30:34,156][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:30:34,157][129146] Reward + Measures: [[-673.65886548    0.99560463    0.984218      0.9913066     0.06713367]]
[37m[1m[2023-06-25 07:30:34,157][129146] Max Reward on eval: -673.6588654792814
[37m[1m[2023-06-25 07:30:34,157][129146] Min Reward on eval: -673.6588654792814
[37m[1m[2023-06-25 07:30:34,158][129146] Mean Reward across all agents: -673.6588654792814
[37m[1m[2023-06-25 07:30:34,158][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:30:39,683][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:30:39,689][129146] Reward + Measures: [[-616.39325389    0.99410003    0.98570007    0.99140006    0.54439998]
[37m[1m [-525.17063724    0.98080009    0.96049994    0.97419995    0.90679997]
[37m[1m [-214.81207189    0.93190002    0.91650003    0.89720005    0.81590003]
[37m[1m ...
[37m[1m [-374.79971604    0.99270004    0.93530005    0.92910004    0.0138    ]
[37m[1m [-672.23842221    0.95499992    0.90140003    0.94920009    0.83589995]
[37m[1m [-707.56579529    0.93050003    0.83400005    0.83829993    0.0956    ]]
[37m[1m[2023-06-25 07:30:39,689][129146] Max Reward on eval: 84.46259377403767
[37m[1m[2023-06-25 07:30:39,690][129146] Min Reward on eval: -951.9890325484681
[37m[1m[2023-06-25 07:30:39,690][129146] Mean Reward across all agents: -379.92293982362725
[37m[1m[2023-06-25 07:30:39,690][129146] Average Trajectory Length: 998.968
[36m[2023-06-25 07:30:39,693][129146] mean_value=-551.6652727430563, max_value=492.0973226958537
[37m[1m[2023-06-25 07:30:39,696][129146] New mean coefficients: [[0.29611564 0.6689956  7.3059726  6.8677335  1.4879544 ]]
[37m[1m[2023-06-25 07:30:39,697][129146] Moving the mean solution point...
[36m[2023-06-25 07:30:49,528][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 07:30:49,528][129146] FPS: 390680.89
[36m[2023-06-25 07:30:49,531][129146] itr=703, itrs=2000, Progress: 35.15%
[36m[2023-06-25 07:31:01,008][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 07:31:01,009][129146] FPS: 335281.53
[36m[2023-06-25 07:31:05,782][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:31:05,782][129146] Reward + Measures: [[-1424.73309836     0.99518734     0.98329198     0.98825055
[37m[1m      0.00100467]]
[37m[1m[2023-06-25 07:31:05,782][129146] Max Reward on eval: -1424.7330983566942
[37m[1m[2023-06-25 07:31:05,782][129146] Min Reward on eval: -1424.7330983566942
[37m[1m[2023-06-25 07:31:05,783][129146] Mean Reward across all agents: -1424.7330983566942
[37m[1m[2023-06-25 07:31:05,783][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:31:11,203][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:31:11,204][129146] Reward + Measures: [[ -942.73290427     0.98879999     0.92959994     0.92269993
[37m[1m      0.0016    ]
[37m[1m [ -883.64450404     0.9582001      0.85420001     0.82980007
[37m[1m      0.0103    ]
[37m[1m [-1568.06446746     0.9891001      0.89919996     0.95419997
[37m[1m      0.0021    ]
[37m[1m ...
[37m[1m [-1005.09507881     0.97040004     0.89410001     0.87550002
[37m[1m      0.0016    ]
[37m[1m [ -863.77331676     0.92749995     0.86930001     0.90360004
[37m[1m      0.0717    ]
[37m[1m [-1313.05645987     0.99010003     0.87770003     0.95920002
[37m[1m      0.0108    ]]
[37m[1m[2023-06-25 07:31:11,204][129146] Max Reward on eval: -387.8585551381344
[37m[1m[2023-06-25 07:31:11,205][129146] Min Reward on eval: -1776.5505481043365
[37m[1m[2023-06-25 07:31:11,205][129146] Mean Reward across all agents: -1007.9893958006819
[37m[1m[2023-06-25 07:31:11,205][129146] Average Trajectory Length: 999.799
[36m[2023-06-25 07:31:11,206][129146] mean_value=-1206.2473272779216, max_value=-82.05996517431294
[36m[2023-06-25 07:31:11,209][129146] XNES is restarting with a new solution whose measures are [0.87360001 0.62849998 0.7306     0.46070001] and objective is -292.1445958149037
[36m[2023-06-25 07:31:11,210][129146] Emitter restarted. Changing the mean agent...
[37m[1m[2023-06-25 07:31:11,213][129146] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]]
[37m[1m[2023-06-25 07:31:11,214][129146] Moving the mean solution point...
[36m[2023-06-25 07:31:20,838][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 07:31:20,838][129146] FPS: 399085.85
[36m[2023-06-25 07:31:20,840][129146] itr=704, itrs=2000, Progress: 35.20%
[36m[2023-06-25 07:31:32,306][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 07:31:32,307][129146] FPS: 335636.56
[36m[2023-06-25 07:31:37,114][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:31:37,115][129146] Reward + Measures: [[-827.42481171    0.66628867    0.35472867    0.57065767    0.69652528]]
[37m[1m[2023-06-25 07:31:37,115][129146] Max Reward on eval: -827.4248117135214
[37m[1m[2023-06-25 07:31:37,115][129146] Min Reward on eval: -827.4248117135214
[37m[1m[2023-06-25 07:31:37,115][129146] Mean Reward across all agents: -827.4248117135214
[37m[1m[2023-06-25 07:31:37,115][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:31:42,603][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:31:42,604][129146] Reward + Measures: [[-1306.87957342     0.63870001     0.65319997     0.76530004
[37m[1m      0.66140002]
[37m[1m [ -928.25188366     0.19175731     0.26938286     0.1752495
[37m[1m      0.28207463]
[37m[1m [ -926.9494516      0.19165766     0.33678165     0.21681206
[37m[1m      0.31012985]
[37m[1m ...
[37m[1m [ -808.22801399     0.22190955     0.32892466     0.13642408
[37m[1m      0.3212412 ]
[37m[1m [ -923.13286584     0.20368169     0.37912816     0.13567548
[37m[1m      0.39793497]
[37m[1m [-1579.40613312     0.29655033     0.29605857     0.29705104
[37m[1m      0.32965848]]
[37m[1m[2023-06-25 07:31:42,604][129146] Max Reward on eval: -322.25114600744564
[37m[1m[2023-06-25 07:31:42,604][129146] Min Reward on eval: -1989.7035616742446
[37m[1m[2023-06-25 07:31:42,605][129146] Mean Reward across all agents: -1197.9845425943881
[37m[1m[2023-06-25 07:31:42,605][129146] Average Trajectory Length: 886.6553333333333
[36m[2023-06-25 07:31:42,606][129146] mean_value=-2504.0645365916434, max_value=-380.9818437888123
[36m[2023-06-25 07:31:42,608][129146] XNES is restarting with a new solution whose measures are [0.36770004 0.6304     0.1741     0.66370004] and objective is 966.9590784302097
[36m[2023-06-25 07:31:42,610][129146] Emitter restarted. Changing the mean agent...
[37m[1m[2023-06-25 07:31:42,612][129146] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]]
[37m[1m[2023-06-25 07:31:42,613][129146] Moving the mean solution point...
[36m[2023-06-25 07:31:52,338][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 07:31:52,339][129146] FPS: 394897.33
[36m[2023-06-25 07:31:52,341][129146] itr=705, itrs=2000, Progress: 35.25%
[36m[2023-06-25 07:32:03,883][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 07:32:03,883][129146] FPS: 333458.66
[36m[2023-06-25 07:32:08,637][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:32:08,638][129146] Reward + Measures: [[909.67671772   0.33883938   0.59060985   0.17033067   0.62080896]]
[37m[1m[2023-06-25 07:32:08,638][129146] Max Reward on eval: 909.6767177182477
[37m[1m[2023-06-25 07:32:08,638][129146] Min Reward on eval: 909.6767177182477
[37m[1m[2023-06-25 07:32:08,638][129146] Mean Reward across all agents: 909.6767177182477
[37m[1m[2023-06-25 07:32:08,639][129146] Average Trajectory Length: 996.5319999999999
[36m[2023-06-25 07:32:14,200][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:32:14,200][129146] Reward + Measures: [[    8.50199866     0.54860002     0.85500002     0.09209999
[37m[1m      0.85690004]
[37m[1m [ -789.80107145     0.34260005     0.4206         0.1552
[37m[1m      0.51770002]
[37m[1m [ -663.06546739     0.2529273      0.37528777     0.19712654
[37m[1m      0.40940639]
[37m[1m ...
[37m[1m [  482.27325962     0.56739998     0.57779998     0.45360002
[37m[1m      0.55580002]
[37m[1m [-1353.34464102     0.26440001     0.24029998     0.1849
[37m[1m      0.3134    ]
[37m[1m [ -955.73541943     0.2605955      0.3234477      0.20869374
[37m[1m      0.3140018 ]]
[37m[1m[2023-06-25 07:32:14,201][129146] Max Reward on eval: 1058.853726584895
[37m[1m[2023-06-25 07:32:14,201][129146] Min Reward on eval: -1570.9939465591917
[37m[1m[2023-06-25 07:32:14,201][129146] Mean Reward across all agents: -249.78793118816702
[37m[1m[2023-06-25 07:32:14,201][129146] Average Trajectory Length: 974.9663333333333
[36m[2023-06-25 07:32:14,205][129146] mean_value=-792.4238848143432, max_value=988.7881955486489
[37m[1m[2023-06-25 07:32:14,208][129146] New mean coefficients: [[ 1.1356986 -0.6559376 -0.8174276  0.0341574 -1.0876343]]
[37m[1m[2023-06-25 07:32:14,209][129146] Moving the mean solution point...
[36m[2023-06-25 07:32:23,942][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 07:32:23,942][129146] FPS: 394597.40
[36m[2023-06-25 07:32:23,945][129146] itr=706, itrs=2000, Progress: 35.30%
[36m[2023-06-25 07:32:35,559][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 07:32:35,559][129146] FPS: 331371.02
[36m[2023-06-25 07:32:40,304][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:32:40,305][129146] Reward + Measures: [[1134.54250938    0.31815532    0.47269344    0.18451491    0.4777801 ]]
[37m[1m[2023-06-25 07:32:40,305][129146] Max Reward on eval: 1134.5425093762526
[37m[1m[2023-06-25 07:32:40,305][129146] Min Reward on eval: 1134.5425093762526
[37m[1m[2023-06-25 07:32:40,305][129146] Mean Reward across all agents: 1134.5425093762526
[37m[1m[2023-06-25 07:32:40,305][129146] Average Trajectory Length: 996.278
[36m[2023-06-25 07:32:45,764][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:32:45,765][129146] Reward + Measures: [[ -485.76756594     0.36282596     0.37408248     0.25765368
[37m[1m      0.27715358]
[37m[1m [  222.89698525     0.50769997     0.57019997     0.39820001
[37m[1m      0.5043    ]
[37m[1m [ -702.32536652     0.20630001     0.51470006     0.40260002
[37m[1m      0.58670008]
[37m[1m ...
[37m[1m [-1242.76345279     0.2599453      0.32227889     0.21655932
[37m[1m      0.35639215]
[37m[1m [    8.58728349     0.31809998     0.47330004     0.30180001
[37m[1m      0.57690001]
[37m[1m [-1306.91002025     0.54479998     0.86700004     0.18009999
[37m[1m      0.87029999]]
[37m[1m[2023-06-25 07:32:45,765][129146] Max Reward on eval: 1737.4197873451399
[37m[1m[2023-06-25 07:32:45,765][129146] Min Reward on eval: -1982.5693735765294
[37m[1m[2023-06-25 07:32:45,766][129146] Mean Reward across all agents: -480.37479671676954
[37m[1m[2023-06-25 07:32:45,766][129146] Average Trajectory Length: 977.5843333333333
[36m[2023-06-25 07:32:45,768][129146] mean_value=-1501.8525209356815, max_value=817.79756613928
[37m[1m[2023-06-25 07:32:45,770][129146] New mean coefficients: [[ 0.4387759  -1.143917   -0.3205731   0.79142123 -1.1463368 ]]
[37m[1m[2023-06-25 07:32:45,771][129146] Moving the mean solution point...
[36m[2023-06-25 07:32:55,529][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 07:32:55,530][129146] FPS: 393591.21
[36m[2023-06-25 07:32:55,532][129146] itr=707, itrs=2000, Progress: 35.35%
[36m[2023-06-25 07:33:06,929][129146] train() took 11.37 seconds to complete
[36m[2023-06-25 07:33:06,930][129146] FPS: 337654.87
[36m[2023-06-25 07:33:11,713][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:33:11,714][129146] Reward + Measures: [[1251.9808758     0.29822755    0.43298519    0.1924939     0.40812486]]
[37m[1m[2023-06-25 07:33:11,715][129146] Max Reward on eval: 1251.9808758021015
[37m[1m[2023-06-25 07:33:11,715][129146] Min Reward on eval: 1251.9808758021015
[37m[1m[2023-06-25 07:33:11,715][129146] Mean Reward across all agents: 1251.9808758021015
[37m[1m[2023-06-25 07:33:11,715][129146] Average Trajectory Length: 995.2673333333333
[36m[2023-06-25 07:33:17,147][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:33:17,147][129146] Reward + Measures: [[  269.0644132      0.38700002     0.59079999     0.55759996
[37m[1m      0.55839998]
[37m[1m [-1868.27776214     0.46050006     0.49900004     0.50840002
[37m[1m      0.22230001]
[37m[1m [-1130.04287869     0.27190003     0.26800001     0.42899999
[37m[1m      0.31689999]
[37m[1m ...
[37m[1m [ -614.46195743     0.24380305     0.2706627      0.31072494
[37m[1m      0.27258793]
[37m[1m [-1068.54985127     0.2192         0.189          0.19170001
[37m[1m      0.19340001]
[37m[1m [ -895.02026692     0.36579999     0.3766         0.51239997
[37m[1m      0.29969999]]
[37m[1m[2023-06-25 07:33:17,148][129146] Max Reward on eval: 1574.1736351011436
[37m[1m[2023-06-25 07:33:17,148][129146] Min Reward on eval: -1868.2777621356538
[37m[1m[2023-06-25 07:33:17,148][129146] Mean Reward across all agents: -371.5461736810567
[37m[1m[2023-06-25 07:33:17,148][129146] Average Trajectory Length: 986.3553333333333
[36m[2023-06-25 07:33:17,151][129146] mean_value=-1024.7580187129386, max_value=848.1753769878609
[37m[1m[2023-06-25 07:33:17,154][129146] New mean coefficients: [[-0.37132913 -0.9810451   0.525243   -0.17162281 -0.97941947]]
[37m[1m[2023-06-25 07:33:17,155][129146] Moving the mean solution point...
[36m[2023-06-25 07:33:26,781][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 07:33:26,781][129146] FPS: 398993.52
[36m[2023-06-25 07:33:26,783][129146] itr=708, itrs=2000, Progress: 35.40%
[36m[2023-06-25 07:33:38,263][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 07:33:38,263][129146] FPS: 335172.45
[36m[2023-06-25 07:33:43,035][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:33:43,036][129146] Reward + Measures: [[1172.30380663    0.2874788     0.42542687    0.20610343    0.38625276]]
[37m[1m[2023-06-25 07:33:43,036][129146] Max Reward on eval: 1172.3038066278527
[37m[1m[2023-06-25 07:33:43,036][129146] Min Reward on eval: 1172.3038066278527
[37m[1m[2023-06-25 07:33:43,037][129146] Mean Reward across all agents: 1172.3038066278527
[37m[1m[2023-06-25 07:33:43,037][129146] Average Trajectory Length: 991.7093333333333
[36m[2023-06-25 07:33:48,549][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:33:48,550][129146] Reward + Measures: [[ -957.02145893     0.25390002     0.28999999     0.20619999
[37m[1m      0.27760002]
[37m[1m [ -132.17684178     0.38730001     0.3637         0.20610002
[37m[1m      0.32119998]
[37m[1m [-1065.2798782      0.46877217     0.46419674     0.35979733
[37m[1m      0.51067293]
[37m[1m ...
[37m[1m [ -531.60900039     0.35769999     0.35480002     0.31000003
[37m[1m      0.33800003]
[37m[1m [  949.62132227     0.33899999     0.3872         0.3177
[37m[1m      0.36320001]
[37m[1m [  167.56899844     0.32030001     0.56699997     0.40369996
[37m[1m      0.5262    ]]
[37m[1m[2023-06-25 07:33:48,550][129146] Max Reward on eval: 1799.1439576105913
[37m[1m[2023-06-25 07:33:48,550][129146] Min Reward on eval: -1372.4810592838098
[37m[1m[2023-06-25 07:33:48,550][129146] Mean Reward across all agents: -64.98789692039405
[37m[1m[2023-06-25 07:33:48,551][129146] Average Trajectory Length: 996.1523333333333
[36m[2023-06-25 07:33:48,553][129146] mean_value=-1373.0481263407632, max_value=573.9418745619955
[37m[1m[2023-06-25 07:33:48,555][129146] New mean coefficients: [[-0.675308  -1.1055595  1.1603124 -1.1993573 -0.6618881]]
[37m[1m[2023-06-25 07:33:48,557][129146] Moving the mean solution point...
[36m[2023-06-25 07:33:58,434][129146] train() took 9.88 seconds to complete
[36m[2023-06-25 07:33:58,434][129146] FPS: 388855.43
[36m[2023-06-25 07:33:58,436][129146] itr=709, itrs=2000, Progress: 35.45%
[36m[2023-06-25 07:34:09,886][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 07:34:09,886][129146] FPS: 336052.23
[36m[2023-06-25 07:34:14,742][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:34:14,742][129146] Reward + Measures: [[1084.24041355    0.28118479    0.43846267    0.21598503    0.3882989 ]]
[37m[1m[2023-06-25 07:34:14,742][129146] Max Reward on eval: 1084.2404135510603
[37m[1m[2023-06-25 07:34:14,743][129146] Min Reward on eval: 1084.2404135510603
[37m[1m[2023-06-25 07:34:14,743][129146] Mean Reward across all agents: 1084.2404135510603
[37m[1m[2023-06-25 07:34:14,743][129146] Average Trajectory Length: 994.74
[36m[2023-06-25 07:34:20,150][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:34:20,150][129146] Reward + Measures: [[  250.77024453     0.56580001     0.61740005     0.5061
[37m[1m      0.67740005]
[37m[1m [ -726.52477346     0.61590004     0.76359999     0.37900001
[37m[1m      0.82580006]
[37m[1m [  301.70427296     0.37920001     0.38069999     0.0681
[37m[1m      0.43259999]
[37m[1m ...
[37m[1m [ 1359.30806236     0.41689998     0.43140003     0.0771
[37m[1m      0.40860006]
[37m[1m [-1063.76835799     0.31080002     0.2462         0.29780003
[37m[1m      0.25630003]
[37m[1m [ -950.68669237     0.29585049     0.31493467     0.10517686
[37m[1m      0.26938051]]
[37m[1m[2023-06-25 07:34:20,151][129146] Max Reward on eval: 1359.3080623568967
[37m[1m[2023-06-25 07:34:20,151][129146] Min Reward on eval: -2276.3961684869255
[37m[1m[2023-06-25 07:34:20,151][129146] Mean Reward across all agents: -464.55317138784835
[37m[1m[2023-06-25 07:34:20,151][129146] Average Trajectory Length: 976.5033333333333
[36m[2023-06-25 07:34:20,153][129146] mean_value=-1961.178165404617, max_value=563.8654269493586
[37m[1m[2023-06-25 07:34:20,156][129146] New mean coefficients: [[-0.34986034 -0.14493382  0.71499383 -0.5786853  -0.16611922]]
[37m[1m[2023-06-25 07:34:20,157][129146] Moving the mean solution point...
[36m[2023-06-25 07:34:29,896][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 07:34:29,896][129146] FPS: 394349.34
[36m[2023-06-25 07:34:29,899][129146] itr=710, itrs=2000, Progress: 35.50%
[37m[1m[2023-06-25 07:34:36,010][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000690
[36m[2023-06-25 07:34:47,855][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 07:34:47,855][129146] FPS: 331147.56
[36m[2023-06-25 07:34:52,692][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:34:52,693][129146] Reward + Measures: [[995.68979007   0.28144425   0.451361     0.21967928   0.40043843]]
[37m[1m[2023-06-25 07:34:52,693][129146] Max Reward on eval: 995.6897900730404
[37m[1m[2023-06-25 07:34:52,693][129146] Min Reward on eval: 995.6897900730404
[37m[1m[2023-06-25 07:34:52,693][129146] Mean Reward across all agents: 995.6897900730404
[37m[1m[2023-06-25 07:34:52,694][129146] Average Trajectory Length: 993.6443333333333
[36m[2023-06-25 07:34:58,376][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:34:58,377][129146] Reward + Measures: [[ 249.69390842    0.35689998    0.5079        0.35370001    0.39269999]
[37m[1m [-240.65487879    0.41580001    0.57770008    0.39780003    0.33530003]
[37m[1m [-932.96788804    0.57629997    0.46100003    0.54440004    0.56660002]
[37m[1m ...
[37m[1m [ 162.80102025    0.1631        0.63580006    0.36660001    0.77500004]
[37m[1m [-290.76104143    0.23849192    0.35314333    0.19401099    0.46333814]
[37m[1m [  66.57761645    0.12099999    0.58520001    0.4763        0.73169994]]
[37m[1m[2023-06-25 07:34:58,377][129146] Max Reward on eval: 1217.527014176699
[37m[1m[2023-06-25 07:34:58,378][129146] Min Reward on eval: -1265.136690826388
[37m[1m[2023-06-25 07:34:58,378][129146] Mean Reward across all agents: 49.28410853171037
[37m[1m[2023-06-25 07:34:58,378][129146] Average Trajectory Length: 992.2226666666667
[36m[2023-06-25 07:34:58,383][129146] mean_value=-399.2832905316002, max_value=925.417413826802
[37m[1m[2023-06-25 07:34:58,386][129146] New mean coefficients: [[-0.47022802 -0.312011    0.59427017  0.49713552  0.19163227]]
[37m[1m[2023-06-25 07:34:58,387][129146] Moving the mean solution point...
[36m[2023-06-25 07:35:08,190][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 07:35:08,194][129146] FPS: 391778.78
[36m[2023-06-25 07:35:08,197][129146] itr=711, itrs=2000, Progress: 35.55%
[36m[2023-06-25 07:35:19,778][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 07:35:19,778][129146] FPS: 332189.00
[36m[2023-06-25 07:35:24,630][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:35:24,630][129146] Reward + Measures: [[862.49543669   0.27111751   0.48325709   0.24078853   0.4356162 ]]
[37m[1m[2023-06-25 07:35:24,630][129146] Max Reward on eval: 862.4954366883175
[37m[1m[2023-06-25 07:35:24,631][129146] Min Reward on eval: 862.4954366883175
[37m[1m[2023-06-25 07:35:24,631][129146] Mean Reward across all agents: 862.4954366883175
[37m[1m[2023-06-25 07:35:24,631][129146] Average Trajectory Length: 993.9293333333333
[36m[2023-06-25 07:35:30,117][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:35:30,118][129146] Reward + Measures: [[489.89769029   0.28535414   0.51521569   0.35090321   0.46198398]
[37m[1m [766.4704207    0.30270001   0.4086       0.211        0.35950002]
[37m[1m [941.43978433   0.27659997   0.46230003   0.2095       0.48210001]
[37m[1m ...
[37m[1m [818.78477539   0.27300003   0.54960006   0.20250002   0.54580003]
[37m[1m [ 57.08846194   0.28706703   0.35381418   0.21274255   0.32661992]
[37m[1m [764.95158073   0.28740001   0.44800001   0.2976       0.41190001]]
[37m[1m[2023-06-25 07:35:30,118][129146] Max Reward on eval: 1078.706701459596
[37m[1m[2023-06-25 07:35:30,119][129146] Min Reward on eval: -845.8485050657066
[37m[1m[2023-06-25 07:35:30,119][129146] Mean Reward across all agents: 298.7734017095295
[37m[1m[2023-06-25 07:35:30,119][129146] Average Trajectory Length: 997.8296666666666
[36m[2023-06-25 07:35:30,123][129146] mean_value=-1162.0350412516973, max_value=837.2439106963201
[37m[1m[2023-06-25 07:35:30,125][129146] New mean coefficients: [[ 0.5389288  -0.77348554  1.7270207   0.61308277  0.74143183]]
[37m[1m[2023-06-25 07:35:30,126][129146] Moving the mean solution point...
[36m[2023-06-25 07:35:39,995][129146] train() took 9.87 seconds to complete
[36m[2023-06-25 07:35:39,995][129146] FPS: 389183.71
[36m[2023-06-25 07:35:39,998][129146] itr=712, itrs=2000, Progress: 35.60%
[36m[2023-06-25 07:35:51,601][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 07:35:51,602][129146] FPS: 331642.32
[36m[2023-06-25 07:35:56,361][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:35:56,361][129146] Reward + Measures: [[847.10177836   0.23807834   0.53073615   0.27404708   0.49459764]]
[37m[1m[2023-06-25 07:35:56,361][129146] Max Reward on eval: 847.1017783606557
[37m[1m[2023-06-25 07:35:56,362][129146] Min Reward on eval: 847.1017783606557
[37m[1m[2023-06-25 07:35:56,362][129146] Mean Reward across all agents: 847.1017783606557
[37m[1m[2023-06-25 07:35:56,362][129146] Average Trajectory Length: 992.6593333333333
[36m[2023-06-25 07:36:01,858][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:36:01,858][129146] Reward + Measures: [[ 594.13538738    0.37100002    0.63850003    0.34990001    0.63370001]
[37m[1m [-170.38319732    0.40550002    0.83680004    0.73460001    0.87080002]
[37m[1m [-380.77681857    0.60699999    0.87800008    0.77220005    0.9149    ]
[37m[1m ...
[37m[1m [ 279.3074272     0.41139999    0.49440002    0.38210002    0.5334    ]
[37m[1m [-376.594026      0.46900001    0.37380001    0.53120005    0.47490001]
[37m[1m [ 442.50465575    0.36860001    0.61410004    0.3502        0.66810006]]
[37m[1m[2023-06-25 07:36:01,858][129146] Max Reward on eval: 870.1895453770412
[37m[1m[2023-06-25 07:36:01,859][129146] Min Reward on eval: -1317.9569157487945
[37m[1m[2023-06-25 07:36:01,859][129146] Mean Reward across all agents: -27.071231960763477
[37m[1m[2023-06-25 07:36:01,859][129146] Average Trajectory Length: 997.3203333333333
[36m[2023-06-25 07:36:01,863][129146] mean_value=-619.3502992583682, max_value=928.7888543764362
[37m[1m[2023-06-25 07:36:01,866][129146] New mean coefficients: [[ 0.37661317 -1.2005842   1.9359928   0.28302953  0.8619851 ]]
[37m[1m[2023-06-25 07:36:01,867][129146] Moving the mean solution point...
[36m[2023-06-25 07:36:11,546][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 07:36:11,546][129146] FPS: 396803.20
[36m[2023-06-25 07:36:11,548][129146] itr=713, itrs=2000, Progress: 35.65%
[36m[2023-06-25 07:36:23,014][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 07:36:23,014][129146] FPS: 335545.93
[36m[2023-06-25 07:36:27,865][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:36:27,866][129146] Reward + Measures: [[663.33896224   0.16254725   0.63366008   0.37750056   0.62372059]]
[37m[1m[2023-06-25 07:36:27,866][129146] Max Reward on eval: 663.3389622350105
[37m[1m[2023-06-25 07:36:27,866][129146] Min Reward on eval: 663.3389622350105
[37m[1m[2023-06-25 07:36:27,867][129146] Mean Reward across all agents: 663.3389622350105
[37m[1m[2023-06-25 07:36:27,867][129146] Average Trajectory Length: 997.3639999999999
[36m[2023-06-25 07:36:33,310][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:36:33,310][129146] Reward + Measures: [[350.87579363   0.09600001   0.66149998   0.4677       0.69569999]
[37m[1m [-54.90596348   0.20380001   0.61589998   0.3985       0.50029999]
[37m[1m [567.88139349   0.20869999   0.57779998   0.3028       0.61309999]
[37m[1m ...
[37m[1m [241.3410377    0.1602       0.74089998   0.67260003   0.73089999]
[37m[1m [333.18914353   0.16365652   0.57997215   0.50508815   0.64640301]
[37m[1m [274.19860432   0.2105       0.4756       0.51630002   0.5406    ]]
[37m[1m[2023-06-25 07:36:33,310][129146] Max Reward on eval: 1065.515018656323
[37m[1m[2023-06-25 07:36:33,311][129146] Min Reward on eval: -980.168182888953
[37m[1m[2023-06-25 07:36:33,311][129146] Mean Reward across all agents: 311.7626888311147
[37m[1m[2023-06-25 07:36:33,311][129146] Average Trajectory Length: 996.6736666666666
[36m[2023-06-25 07:36:33,318][129146] mean_value=-109.13777476905403, max_value=1215.9102000028593
[37m[1m[2023-06-25 07:36:33,321][129146] New mean coefficients: [[ 0.03801009 -1.7017424   2.0578785  -1.1832088   1.0374055 ]]
[37m[1m[2023-06-25 07:36:33,322][129146] Moving the mean solution point...
[36m[2023-06-25 07:36:43,060][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 07:36:43,061][129146] FPS: 394361.18
[36m[2023-06-25 07:36:43,063][129146] itr=714, itrs=2000, Progress: 35.70%
[36m[2023-06-25 07:36:54,536][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 07:36:54,536][129146] FPS: 335325.00
[36m[2023-06-25 07:36:59,425][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:36:59,426][129146] Reward + Measures: [[273.0834933    0.05046216   0.79634351   0.67375648   0.82054603]]
[37m[1m[2023-06-25 07:36:59,426][129146] Max Reward on eval: 273.083493295354
[37m[1m[2023-06-25 07:36:59,426][129146] Min Reward on eval: 273.083493295354
[37m[1m[2023-06-25 07:36:59,427][129146] Mean Reward across all agents: 273.083493295354
[37m[1m[2023-06-25 07:36:59,427][129146] Average Trajectory Length: 999.5326666666666
[36m[2023-06-25 07:37:04,975][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:37:04,975][129146] Reward + Measures: [[-451.76368349    0.21659999    0.55740005    0.32730001    0.48660001]
[37m[1m [-350.86998635    0.19653812    0.50410932    0.32035202    0.4323979 ]
[37m[1m [-695.68367044    0.16283306    0.46655178    0.29279271    0.40492114]
[37m[1m ...
[37m[1m [-354.4237004     0.31469721    0.67094678    0.2432103     0.56527007]
[37m[1m [-704.4361325     0.28760001    0.42570001    0.26630002    0.34      ]
[37m[1m [-992.04534028    0.32750002    0.26929998    0.36020002    0.35740003]]
[37m[1m[2023-06-25 07:37:04,975][129146] Max Reward on eval: 1038.6806958519387
[37m[1m[2023-06-25 07:37:04,976][129146] Min Reward on eval: -1813.577770314808
[37m[1m[2023-06-25 07:37:04,976][129146] Mean Reward across all agents: -165.90441097064686
[37m[1m[2023-06-25 07:37:04,976][129146] Average Trajectory Length: 972.727
[36m[2023-06-25 07:37:04,979][129146] mean_value=-1075.6546467988464, max_value=828.2414810334745
[37m[1m[2023-06-25 07:37:04,982][129146] New mean coefficients: [[ 0.4084116  -1.5340211   2.052553   -0.33740884  1.0680486 ]]
[37m[1m[2023-06-25 07:37:04,983][129146] Moving the mean solution point...
[36m[2023-06-25 07:37:14,873][129146] train() took 9.89 seconds to complete
[36m[2023-06-25 07:37:14,874][129146] FPS: 388312.06
[36m[2023-06-25 07:37:14,876][129146] itr=715, itrs=2000, Progress: 35.75%
[36m[2023-06-25 07:37:26,503][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 07:37:26,503][129146] FPS: 330885.04
[36m[2023-06-25 07:37:31,451][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:37:31,451][129146] Reward + Measures: [[9.70524912 0.05587634 0.77911246 0.58879811 0.93582046]]
[37m[1m[2023-06-25 07:37:31,452][129146] Max Reward on eval: 9.705249121888972
[37m[1m[2023-06-25 07:37:31,452][129146] Min Reward on eval: 9.705249121888972
[37m[1m[2023-06-25 07:37:31,452][129146] Mean Reward across all agents: 9.705249121888972
[37m[1m[2023-06-25 07:37:31,452][129146] Average Trajectory Length: 999.7256666666666
[36m[2023-06-25 07:37:37,185][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:37:37,186][129146] Reward + Measures: [[  -72.66939016     0.0178         0.82620001     0.49990001
[37m[1m      0.98509997]
[37m[1m [ -192.08374058     0.0303         0.82040006     0.66670007
[37m[1m      0.96310008]
[37m[1m [   87.56719636     0.0712         0.84359998     0.59369999
[37m[1m      0.90380001]
[37m[1m ...
[37m[1m [  495.19907982     0.3698         0.72049999     0.41090003
[37m[1m      0.66210002]
[37m[1m [ -107.84157038     0.0512         0.80410004     0.61800003
[37m[1m      0.96060002]
[37m[1m [-1146.20133569     0.004          0.94570011     0.92519999
[37m[1m      0.99410003]]
[37m[1m[2023-06-25 07:37:37,186][129146] Max Reward on eval: 530.255383563228
[37m[1m[2023-06-25 07:37:37,186][129146] Min Reward on eval: -1429.4391128579737
[37m[1m[2023-06-25 07:37:37,187][129146] Mean Reward across all agents: -219.17218517477514
[37m[1m[2023-06-25 07:37:37,187][129146] Average Trajectory Length: 999.0293333333333
[36m[2023-06-25 07:37:37,194][129146] mean_value=6.472153549874129, max_value=743.3767235637794
[37m[1m[2023-06-25 07:37:37,196][129146] New mean coefficients: [[ 1.4265935  -2.3245306   1.91017     0.42920828  0.70462286]]
[37m[1m[2023-06-25 07:37:37,197][129146] Moving the mean solution point...
[36m[2023-06-25 07:37:47,028][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 07:37:47,028][129146] FPS: 390699.11
[36m[2023-06-25 07:37:47,030][129146] itr=716, itrs=2000, Progress: 35.80%
[36m[2023-06-25 07:37:58,593][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 07:37:58,593][129146] FPS: 332815.42
[36m[2023-06-25 07:38:03,355][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:38:03,355][129146] Reward + Measures: [[282.68883978   0.099957     0.78287274   0.41356367   0.85484034]]
[37m[1m[2023-06-25 07:38:03,356][129146] Max Reward on eval: 282.6888397844382
[37m[1m[2023-06-25 07:38:03,356][129146] Min Reward on eval: 282.6888397844382
[37m[1m[2023-06-25 07:38:03,356][129146] Mean Reward across all agents: 282.6888397844382
[37m[1m[2023-06-25 07:38:03,356][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:38:08,748][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:38:08,749][129146] Reward + Measures: [[ 457.23251005    0.0003        0.82969999    0.69090003    0.92830002]
[37m[1m [ 265.16179036    0.19949999    0.5898        0.3423        0.67739999]
[37m[1m [ 593.91150497    0.16630001    0.74110001    0.31760001    0.60000002]
[37m[1m ...
[37m[1m [-268.95220384    0.0805        0.9526        0.75680006    0.98159999]
[37m[1m [-278.81195758    0.0002        0.92449999    0.89300007    0.97790003]
[37m[1m [ 471.33973433    0.0063        0.78190005    0.65060002    0.89400005]]
[37m[1m[2023-06-25 07:38:08,749][129146] Max Reward on eval: 1070.1524282383848
[37m[1m[2023-06-25 07:38:08,749][129146] Min Reward on eval: -1782.3914882327895
[37m[1m[2023-06-25 07:38:08,749][129146] Mean Reward across all agents: 239.7339790347617
[37m[1m[2023-06-25 07:38:08,750][129146] Average Trajectory Length: 998.1519999999999
[36m[2023-06-25 07:38:08,756][129146] mean_value=-64.16310362876072, max_value=1093.911504968995
[37m[1m[2023-06-25 07:38:08,759][129146] New mean coefficients: [[ 1.4736346 -2.2291408  2.2845752 -0.1416865  0.3563568]]
[37m[1m[2023-06-25 07:38:08,760][129146] Moving the mean solution point...
[36m[2023-06-25 07:38:18,525][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 07:38:18,525][129146] FPS: 393309.50
[36m[2023-06-25 07:38:18,527][129146] itr=717, itrs=2000, Progress: 35.85%
[36m[2023-06-25 07:38:30,106][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 07:38:30,106][129146] FPS: 332362.15
[36m[2023-06-25 07:38:34,979][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:38:34,979][129146] Reward + Measures: [[576.86333612   0.04572767   0.95799601   0.9623006    0.98162401]]
[37m[1m[2023-06-25 07:38:34,979][129146] Max Reward on eval: 576.8633361209215
[37m[1m[2023-06-25 07:38:34,979][129146] Min Reward on eval: 576.8633361209215
[37m[1m[2023-06-25 07:38:34,980][129146] Mean Reward across all agents: 576.8633361209215
[37m[1m[2023-06-25 07:38:34,980][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:38:40,470][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:38:40,470][129146] Reward + Measures: [[-1553.33034224     0.98979998     0.9903         0.98950005
[37m[1m      0.99280006]
[37m[1m [  270.74472249     0.10320001     0.53200001     0.49280006
[37m[1m      0.50760001]
[37m[1m [-1606.02489509     0.98460001     0.98859996     0.98780006
[37m[1m      0.99069995]
[37m[1m ...
[37m[1m [  577.30649326     0.74120003     0.95380002     0.97360003
[37m[1m      0.99120009]
[37m[1m [-1144.50856631     0.97039998     0.94690001     0.963
[37m[1m      0.9655    ]
[37m[1m [ -523.07335607     0.97580004     0.98070002     0.97210008
[37m[1m      0.98569995]]
[37m[1m[2023-06-25 07:38:40,471][129146] Max Reward on eval: 776.5581309846253
[37m[1m[2023-06-25 07:38:40,471][129146] Min Reward on eval: -1883.0112705314532
[37m[1m[2023-06-25 07:38:40,471][129146] Mean Reward across all agents: -581.0978456049313
[37m[1m[2023-06-25 07:38:40,471][129146] Average Trajectory Length: 999.126
[36m[2023-06-25 07:38:40,475][129146] mean_value=-934.4916637557282, max_value=1276.5581309846252
[37m[1m[2023-06-25 07:38:40,478][129146] New mean coefficients: [[ 0.5697673  0.3900311  2.1934557 -0.8221021  0.5989187]]
[37m[1m[2023-06-25 07:38:40,479][129146] Moving the mean solution point...
[36m[2023-06-25 07:38:50,188][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 07:38:50,188][129146] FPS: 395554.32
[36m[2023-06-25 07:38:50,191][129146] itr=718, itrs=2000, Progress: 35.90%
[36m[2023-06-25 07:39:01,604][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 07:39:01,604][129146] FPS: 337182.32
[36m[2023-06-25 07:39:06,450][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:39:06,451][129146] Reward + Measures: [[258.87026514   0.00288767   0.91469336   0.79846567   0.93104768]]
[37m[1m[2023-06-25 07:39:06,451][129146] Max Reward on eval: 258.8702651375206
[37m[1m[2023-06-25 07:39:06,451][129146] Min Reward on eval: 258.8702651375206
[37m[1m[2023-06-25 07:39:06,452][129146] Mean Reward across all agents: 258.8702651375206
[37m[1m[2023-06-25 07:39:06,452][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:39:11,941][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:39:11,941][129146] Reward + Measures: [[245.90678483   0.0006       0.88640004   0.77330005   0.89770001]
[37m[1m [224.18379211   0.0017       0.90390009   0.82180005   0.90200007]
[37m[1m [118.57715602   0.0001       0.86129999   0.79479998   0.83160001]
[37m[1m ...
[37m[1m [-82.41605768   0.0014       0.83890003   0.74609995   0.75209999]
[37m[1m [-40.36363538   0.0042       0.87370008   0.82380003   0.85260004]
[37m[1m [404.30319843   0.0178       0.85360003   0.6261       0.89469999]]
[37m[1m[2023-06-25 07:39:11,942][129146] Max Reward on eval: 728.6422325053602
[37m[1m[2023-06-25 07:39:11,942][129146] Min Reward on eval: -919.7368514664006
[37m[1m[2023-06-25 07:39:11,942][129146] Mean Reward across all agents: 176.622325641619
[37m[1m[2023-06-25 07:39:11,942][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:39:11,948][129146] mean_value=66.86219827689209, max_value=837.7341138913785
[37m[1m[2023-06-25 07:39:11,950][129146] New mean coefficients: [[ 0.15046102  0.5927485   3.1256537  -1.056206    0.7821422 ]]
[37m[1m[2023-06-25 07:39:11,951][129146] Moving the mean solution point...
[36m[2023-06-25 07:39:21,689][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 07:39:21,689][129146] FPS: 394423.66
[36m[2023-06-25 07:39:21,691][129146] itr=719, itrs=2000, Progress: 35.95%
[36m[2023-06-25 07:39:33,091][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 07:39:33,091][129146] FPS: 337483.53
[36m[2023-06-25 07:39:37,831][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:39:37,831][129146] Reward + Measures: [[258.35714723   0.00226233   0.94329035   0.82349741   0.9533776 ]]
[37m[1m[2023-06-25 07:39:37,832][129146] Max Reward on eval: 258.35714723257814
[37m[1m[2023-06-25 07:39:37,832][129146] Min Reward on eval: 258.35714723257814
[37m[1m[2023-06-25 07:39:37,832][129146] Mean Reward across all agents: 258.35714723257814
[37m[1m[2023-06-25 07:39:37,832][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:39:43,420][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:39:43,421][129146] Reward + Measures: [[375.39223862   0.0054       0.87790006   0.70029992   0.9149    ]
[37m[1m [189.11344132   0.1401       0.67329997   0.55720001   0.62300009]
[37m[1m [ 76.02448797   0.0066       0.82370007   0.70559996   0.82190001]
[37m[1m ...
[37m[1m [179.93943037   0.0023       0.91870004   0.78420001   0.94540006]
[37m[1m [448.91643539   0.15100001   0.63510007   0.45520002   0.71850002]
[37m[1m [543.39522054   0.0982       0.70469999   0.60120004   0.75310004]]
[37m[1m[2023-06-25 07:39:43,421][129146] Max Reward on eval: 561.3090714566002
[37m[1m[2023-06-25 07:39:43,421][129146] Min Reward on eval: -183.1715939288726
[37m[1m[2023-06-25 07:39:43,421][129146] Mean Reward across all agents: 284.95021317259585
[37m[1m[2023-06-25 07:39:43,422][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:39:43,427][129146] mean_value=99.76042394376714, max_value=905.3103320186405
[37m[1m[2023-06-25 07:39:43,430][129146] New mean coefficients: [[-0.3786041   2.0493193   4.0774665  -0.6166689   0.87720364]]
[37m[1m[2023-06-25 07:39:43,431][129146] Moving the mean solution point...
[36m[2023-06-25 07:39:53,143][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 07:39:53,143][129146] FPS: 395455.33
[36m[2023-06-25 07:39:53,145][129146] itr=720, itrs=2000, Progress: 36.00%
[37m[1m[2023-06-25 07:39:59,396][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000700
[36m[2023-06-25 07:40:11,008][129146] train() took 11.35 seconds to complete
[36m[2023-06-25 07:40:11,009][129146] FPS: 338195.79
[36m[2023-06-25 07:40:15,772][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:40:15,773][129146] Reward + Measures: [[254.88819456   0.00202767   0.95834905   0.84224135   0.96583033]]
[37m[1m[2023-06-25 07:40:15,773][129146] Max Reward on eval: 254.8881945557001
[37m[1m[2023-06-25 07:40:15,773][129146] Min Reward on eval: 254.8881945557001
[37m[1m[2023-06-25 07:40:15,774][129146] Mean Reward across all agents: 254.8881945557001
[37m[1m[2023-06-25 07:40:15,774][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:40:21,130][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:40:21,131][129146] Reward + Measures: [[ 578.8826269     0.1224        0.71950001    0.44380003    0.69710004]
[37m[1m [  70.57578594    0.0004        0.95760006    0.87789994    0.97200006]
[37m[1m [  42.71616818    0.0004        0.96350002    0.91240007    0.97550005]
[37m[1m ...
[37m[1m [-225.01047606    0.0019        0.92320007    0.90000004    0.96640009]
[37m[1m [-232.8382716     0.            0.94679993    0.90640002    0.98280001]
[37m[1m [  80.6357929     0.0008        0.94799995    0.85039997    0.98070002]]
[37m[1m[2023-06-25 07:40:21,131][129146] Max Reward on eval: 578.8826269043377
[37m[1m[2023-06-25 07:40:21,131][129146] Min Reward on eval: -232.83827160495565
[37m[1m[2023-06-25 07:40:21,132][129146] Mean Reward across all agents: 108.30779550276135
[37m[1m[2023-06-25 07:40:21,132][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:40:21,134][129146] mean_value=-214.86429321071583, max_value=623.6456607527684
[37m[1m[2023-06-25 07:40:21,136][129146] New mean coefficients: [[0.49487084 1.1059129  0.6310952  0.6798976  0.88979816]]
[37m[1m[2023-06-25 07:40:21,137][129146] Moving the mean solution point...
[36m[2023-06-25 07:40:30,833][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 07:40:30,833][129146] FPS: 396119.70
[36m[2023-06-25 07:40:30,835][129146] itr=721, itrs=2000, Progress: 36.05%
[36m[2023-06-25 07:40:42,218][129146] train() took 11.36 seconds to complete
[36m[2023-06-25 07:40:42,219][129146] FPS: 337976.51
[36m[2023-06-25 07:40:47,156][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:40:47,161][129146] Reward + Measures: [[62.05580037  0.10515133  0.82354164  0.54132628  0.83456564]]
[37m[1m[2023-06-25 07:40:47,162][129146] Max Reward on eval: 62.05580037299899
[37m[1m[2023-06-25 07:40:47,162][129146] Min Reward on eval: 62.05580037299899
[37m[1m[2023-06-25 07:40:47,162][129146] Mean Reward across all agents: 62.05580037299899
[37m[1m[2023-06-25 07:40:47,162][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:40:52,647][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:40:52,648][129146] Reward + Measures: [[-82.31563692   0.34919998   0.82609999   0.32370001   0.81949997]
[37m[1m [-81.02744657   0.29000002   0.75259995   0.41440001   0.74580002]
[37m[1m [173.31889046   0.1578       0.82300007   0.59230006   0.86710006]
[37m[1m ...
[37m[1m [ 31.74033724   0.0853       0.80919993   0.5352       0.81030005]
[37m[1m [ 59.62263863   0.09600001   0.78240007   0.43070003   0.79750001]
[37m[1m [106.43849142   0.1238       0.86969995   0.54899997   0.91539997]]
[37m[1m[2023-06-25 07:40:52,648][129146] Max Reward on eval: 282.0584362659371
[37m[1m[2023-06-25 07:40:52,648][129146] Min Reward on eval: -708.2725838219282
[37m[1m[2023-06-25 07:40:52,648][129146] Mean Reward across all agents: 47.71040866570718
[37m[1m[2023-06-25 07:40:52,649][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:40:52,652][129146] mean_value=-73.8522066582087, max_value=718.3451388411329
[37m[1m[2023-06-25 07:40:52,655][129146] New mean coefficients: [[ 0.3618145  -0.6660341   0.0720337   1.5547719   0.66587436]]
[37m[1m[2023-06-25 07:40:52,656][129146] Moving the mean solution point...
[36m[2023-06-25 07:41:02,319][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 07:41:02,319][129146] FPS: 397451.08
[36m[2023-06-25 07:41:02,322][129146] itr=722, itrs=2000, Progress: 36.10%
[36m[2023-06-25 07:41:13,710][129146] train() took 11.37 seconds to complete
[36m[2023-06-25 07:41:13,710][129146] FPS: 337842.51
[36m[2023-06-25 07:41:18,527][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:41:18,527][129146] Reward + Measures: [[-1057.15768466     0.82952034     0.98687106     0.95077968
[37m[1m      0.989802  ]]
[37m[1m[2023-06-25 07:41:18,528][129146] Max Reward on eval: -1057.1576846563537
[37m[1m[2023-06-25 07:41:18,528][129146] Min Reward on eval: -1057.1576846563537
[37m[1m[2023-06-25 07:41:18,528][129146] Mean Reward across all agents: -1057.1576846563537
[37m[1m[2023-06-25 07:41:18,528][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:41:24,085][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:41:24,086][129146] Reward + Measures: [[ -994.8579955      0.94399995     0.98950005     0.97290003
[37m[1m      0.98920006]
[37m[1m [-1233.0787077      0.91359997     0.99270004     0.97690004
[37m[1m      0.9946    ]
[37m[1m [-1184.29442583     0.91970009     0.0817         0.77350003
[37m[1m      0.85830003]
[37m[1m ...
[37m[1m [ -790.63660768     0.52879995     0.92079991     0.82810003
[37m[1m      0.92189997]
[37m[1m [ -957.15565905     0.             0.9163         0.92290002
[37m[1m      0.95570004]
[37m[1m [-1063.5713681      0.93129998     0.97889996     0.95640004
[37m[1m      0.97119999]]
[37m[1m[2023-06-25 07:41:24,086][129146] Max Reward on eval: -397.07084889084797
[37m[1m[2023-06-25 07:41:24,086][129146] Min Reward on eval: -2234.9460233730147
[37m[1m[2023-06-25 07:41:24,087][129146] Mean Reward across all agents: -1043.4725439118997
[37m[1m[2023-06-25 07:41:24,087][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:41:24,089][129146] mean_value=-1200.6719019600464, max_value=-80.9190434102668
[36m[2023-06-25 07:41:24,091][129146] XNES is restarting with a new solution whose measures are [0.67210007 0.37540001 0.08450001 0.21470001] and objective is 2008.0645217488986
[36m[2023-06-25 07:41:24,092][129146] Emitter restarted. Changing the mean agent...
[37m[1m[2023-06-25 07:41:24,095][129146] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]]
[37m[1m[2023-06-25 07:41:24,095][129146] Moving the mean solution point...
[36m[2023-06-25 07:41:33,770][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 07:41:33,771][129146] FPS: 396966.05
[36m[2023-06-25 07:41:33,773][129146] itr=723, itrs=2000, Progress: 36.15%
[36m[2023-06-25 07:41:45,166][129146] train() took 11.37 seconds to complete
[36m[2023-06-25 07:41:45,167][129146] FPS: 337769.86
[36m[2023-06-25 07:41:49,968][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:41:49,968][129146] Reward + Measures: [[1726.69905983    0.58791554    0.32412556    0.14262462    0.20034936]]
[37m[1m[2023-06-25 07:41:49,969][129146] Max Reward on eval: 1726.6990598299417
[37m[1m[2023-06-25 07:41:49,969][129146] Min Reward on eval: 1726.6990598299417
[37m[1m[2023-06-25 07:41:49,969][129146] Mean Reward across all agents: 1726.6990598299417
[37m[1m[2023-06-25 07:41:49,970][129146] Average Trajectory Length: 999.6586666666666
[36m[2023-06-25 07:41:55,392][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:41:55,392][129146] Reward + Measures: [[ 999.40335605    0.44169998    0.3161        0.1707        0.19589999]
[37m[1m [ 205.28512635    0.59179997    0.40290004    0.36069998    0.18020001]
[37m[1m [ -47.06376456    0.53122038    0.23074715    0.35593095    0.18641475]
[37m[1m ...
[37m[1m [ 148.23547533    0.41720468    0.27014682    0.21997634    0.18044816]
[37m[1m [-310.57095429    0.46025404    0.24381439    0.41482702    0.23652522]
[37m[1m [-304.6773344     0.31588838    0.20412326    0.22123094    0.19350252]]
[37m[1m[2023-06-25 07:41:55,392][129146] Max Reward on eval: 1771.8135772775627
[37m[1m[2023-06-25 07:41:55,393][129146] Min Reward on eval: -2006.5953699797392
[37m[1m[2023-06-25 07:41:55,393][129146] Mean Reward across all agents: -209.46510720090296
[37m[1m[2023-06-25 07:41:55,393][129146] Average Trajectory Length: 860.973
[36m[2023-06-25 07:41:55,395][129146] mean_value=-1331.1652351608004, max_value=435.6526651842571
[37m[1m[2023-06-25 07:41:55,397][129146] New mean coefficients: [[ 0.65700495 -0.6168554  -1.3892351  -2.3573802  -1.326505  ]]
[37m[1m[2023-06-25 07:41:55,398][129146] Moving the mean solution point...
[36m[2023-06-25 07:42:05,026][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 07:42:05,027][129146] FPS: 398892.33
[36m[2023-06-25 07:42:05,029][129146] itr=724, itrs=2000, Progress: 36.20%
[36m[2023-06-25 07:42:16,473][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 07:42:16,474][129146] FPS: 336143.15
[36m[2023-06-25 07:42:21,176][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:42:21,176][129146] Reward + Measures: [[2033.00001578    0.48507565    0.35048774    0.12851603    0.19884497]]
[37m[1m[2023-06-25 07:42:21,177][129146] Max Reward on eval: 2033.000015780192
[37m[1m[2023-06-25 07:42:21,177][129146] Min Reward on eval: 2033.000015780192
[37m[1m[2023-06-25 07:42:21,177][129146] Mean Reward across all agents: 2033.000015780192
[37m[1m[2023-06-25 07:42:21,177][129146] Average Trajectory Length: 998.6243333333333
[36m[2023-06-25 07:42:26,829][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:42:26,829][129146] Reward + Measures: [[ -901.76262599     0.3207663      0.19863361     0.17571421
[37m[1m      0.19590202]
[37m[1m [ -830.00774268     0.41589999     0.22340003     0.30089998
[37m[1m      0.14210001]
[37m[1m [ -416.54608023     0.35414729     0.2343678      0.20962337
[37m[1m      0.19105589]
[37m[1m ...
[37m[1m [-1388.05893796     0.73358947     0.11415263     0.54619473
[37m[1m      0.45498949]
[37m[1m [ -773.28446108     0.48359999     0.2105         0.37170002
[37m[1m      0.24419999]
[37m[1m [ -303.37014347     0.40079999     0.3073         0.2033
[37m[1m      0.21899998]]
[37m[1m[2023-06-25 07:42:26,830][129146] Max Reward on eval: 1024.803762211476
[37m[1m[2023-06-25 07:42:26,830][129146] Min Reward on eval: -2114.6579471079167
[37m[1m[2023-06-25 07:42:26,830][129146] Mean Reward across all agents: -582.633870743334
[37m[1m[2023-06-25 07:42:26,830][129146] Average Trajectory Length: 886.545
[36m[2023-06-25 07:42:26,832][129146] mean_value=-1999.141511646732, max_value=-419.96963522550203
[36m[2023-06-25 07:42:26,834][129146] XNES is restarting with a new solution whose measures are [0.82499999 0.44780001 0.6688     0.0542    ] and objective is 159.33754003788346
[36m[2023-06-25 07:42:26,835][129146] Emitter restarted. Changing the mean agent...
[37m[1m[2023-06-25 07:42:26,838][129146] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]]
[37m[1m[2023-06-25 07:42:26,839][129146] Moving the mean solution point...
[36m[2023-06-25 07:42:36,569][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 07:42:36,575][129146] FPS: 394718.75
[36m[2023-06-25 07:42:36,577][129146] itr=725, itrs=2000, Progress: 36.25%
[36m[2023-06-25 07:42:48,200][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 07:42:48,200][129146] FPS: 331056.47
[36m[2023-06-25 07:42:52,934][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:42:52,939][129146] Reward + Measures: [[-89.84509402   0.71213067   0.56049365   0.43517834   0.00603233]]
[37m[1m[2023-06-25 07:42:52,940][129146] Max Reward on eval: -89.84509402110781
[37m[1m[2023-06-25 07:42:52,940][129146] Min Reward on eval: -89.84509402110781
[37m[1m[2023-06-25 07:42:52,940][129146] Mean Reward across all agents: -89.84509402110781
[37m[1m[2023-06-25 07:42:52,941][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:42:58,324][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:42:58,324][129146] Reward + Measures: [[  13.79237289    0.63449997    0.5467        0.40599999    0.0075    ]
[37m[1m [ -69.53446312    0.45989999    0.2357        0.36880001    0.1517    ]
[37m[1m [-455.83726003    0.44266286    0.3752926     0.35164204    0.25644842]
[37m[1m ...
[37m[1m [-388.55832543    0.46660003    0.27440003    0.333         0.229     ]
[37m[1m [-478.71799791    0.4276        0.18100001    0.40120003    0.20910001]
[37m[1m [-714.03713865    0.27326414    0.25728324    0.21469417    0.23823003]]
[37m[1m[2023-06-25 07:42:58,325][129146] Max Reward on eval: 280.6960821532877
[37m[1m[2023-06-25 07:42:58,325][129146] Min Reward on eval: -1555.9000608310337
[37m[1m[2023-06-25 07:42:58,325][129146] Mean Reward across all agents: -473.02412307918013
[37m[1m[2023-06-25 07:42:58,325][129146] Average Trajectory Length: 975.5486666666667
[36m[2023-06-25 07:42:58,328][129146] mean_value=-1054.8309688808754, max_value=753.4615989924175
[37m[1m[2023-06-25 07:42:58,330][129146] New mean coefficients: [[-0.09338453 -0.4257135  -0.40744248 -0.9549266  -0.43851423]]
[37m[1m[2023-06-25 07:42:58,331][129146] Moving the mean solution point...
[36m[2023-06-25 07:43:07,961][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 07:43:07,961][129146] FPS: 398854.40
[36m[2023-06-25 07:43:07,963][129146] itr=726, itrs=2000, Progress: 36.30%
[36m[2023-06-25 07:43:19,451][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 07:43:19,452][129146] FPS: 334985.62
[36m[2023-06-25 07:43:24,211][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:43:24,211][129146] Reward + Measures: [[400.59152025   0.48949364   0.37500834   0.23104398   0.21198998]]
[37m[1m[2023-06-25 07:43:24,211][129146] Max Reward on eval: 400.5915202465065
[37m[1m[2023-06-25 07:43:24,211][129146] Min Reward on eval: 400.5915202465065
[37m[1m[2023-06-25 07:43:24,212][129146] Mean Reward across all agents: 400.5915202465065
[37m[1m[2023-06-25 07:43:24,212][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:43:29,618][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:43:29,619][129146] Reward + Measures: [[-554.77942476    0.28425756    0.25384101    0.30021295    0.17156331]
[37m[1m [  90.54565256    0.47240001    0.3175        0.28650004    0.2105    ]
[37m[1m [-307.40557663    0.50520003    0.40500003    0.3348        0.09150001]
[37m[1m ...
[37m[1m [-877.95347873    0.50979996    0.2931        0.42220002    0.1163    ]
[37m[1m [  31.70289088    0.58260006    0.40970001    0.3026        0.1117    ]
[37m[1m [ 259.68197512    0.4894        0.35980001    0.29519999    0.1653    ]]
[37m[1m[2023-06-25 07:43:29,619][129146] Max Reward on eval: 460.5976344321505
[37m[1m[2023-06-25 07:43:29,619][129146] Min Reward on eval: -1268.1549360163276
[37m[1m[2023-06-25 07:43:29,619][129146] Mean Reward across all agents: -128.4798660703004
[37m[1m[2023-06-25 07:43:29,620][129146] Average Trajectory Length: 969.1983333333333
[36m[2023-06-25 07:43:29,622][129146] mean_value=-996.3795684994551, max_value=579.1771326779635
[37m[1m[2023-06-25 07:43:29,625][129146] New mean coefficients: [[ 0.5888599  -0.2686879  -0.51629966 -0.61904    -2.1552725 ]]
[37m[1m[2023-06-25 07:43:29,626][129146] Moving the mean solution point...
[36m[2023-06-25 07:43:39,339][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 07:43:39,339][129146] FPS: 395400.20
[36m[2023-06-25 07:43:39,341][129146] itr=727, itrs=2000, Progress: 36.35%
[36m[2023-06-25 07:43:50,891][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 07:43:50,892][129146] FPS: 333122.22
[36m[2023-06-25 07:43:55,764][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:43:55,764][129146] Reward + Measures: [[405.10548129   0.46975964   0.36172533   0.22811434   0.20089868]]
[37m[1m[2023-06-25 07:43:55,764][129146] Max Reward on eval: 405.10548128797546
[37m[1m[2023-06-25 07:43:55,765][129146] Min Reward on eval: 405.10548128797546
[37m[1m[2023-06-25 07:43:55,765][129146] Mean Reward across all agents: 405.10548128797546
[37m[1m[2023-06-25 07:43:55,765][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:44:01,237][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:44:01,238][129146] Reward + Measures: [[  109.95533829     0.44860002     0.29179999     0.211
[37m[1m      0.1152    ]
[37m[1m [ -207.33203587     0.43929997     0.289          0.36610001
[37m[1m      0.23240001]
[37m[1m [ -261.3856624      0.61899996     0.414          0.34670001
[37m[1m      0.1305    ]
[37m[1m ...
[37m[1m [ -178.00627161     0.54879999     0.39520001     0.26560003
[37m[1m      0.1461    ]
[37m[1m [ -387.11547706     0.52160001     0.34529999     0.28760001
[37m[1m      0.1301    ]
[37m[1m [-1159.21329841     0.55870003     0.45900002     0.54259998
[37m[1m      0.06339999]]
[37m[1m[2023-06-25 07:44:01,238][129146] Max Reward on eval: 560.6014245382394
[37m[1m[2023-06-25 07:44:01,238][129146] Min Reward on eval: -1577.3850180676673
[37m[1m[2023-06-25 07:44:01,239][129146] Mean Reward across all agents: -259.7539237726385
[37m[1m[2023-06-25 07:44:01,239][129146] Average Trajectory Length: 994.4183333333333
[36m[2023-06-25 07:44:01,241][129146] mean_value=-710.2165061395209, max_value=523.4299049254129
[37m[1m[2023-06-25 07:44:01,243][129146] New mean coefficients: [[ 1.0243142  -0.0614799  -0.54419935 -1.5503237  -1.8920808 ]]
[37m[1m[2023-06-25 07:44:01,244][129146] Moving the mean solution point...
[36m[2023-06-25 07:44:11,036][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 07:44:11,036][129146] FPS: 392257.79
[36m[2023-06-25 07:44:11,038][129146] itr=728, itrs=2000, Progress: 36.40%
[36m[2023-06-25 07:44:22,712][129146] train() took 11.65 seconds to complete
[36m[2023-06-25 07:44:22,713][129146] FPS: 329536.09
[36m[2023-06-25 07:44:27,541][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:44:27,541][129146] Reward + Measures: [[17.85798416  0.53311509  0.37106648  0.37787744  0.10122052]]
[37m[1m[2023-06-25 07:44:27,542][129146] Max Reward on eval: 17.857984163597305
[37m[1m[2023-06-25 07:44:27,542][129146] Min Reward on eval: 17.857984163597305
[37m[1m[2023-06-25 07:44:27,542][129146] Mean Reward across all agents: 17.857984163597305
[37m[1m[2023-06-25 07:44:27,542][129146] Average Trajectory Length: 999.701
[36m[2023-06-25 07:44:33,057][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:44:33,063][129146] Reward + Measures: [[-217.04518969    0.4745        0.38240001    0.45460001    0.1191    ]
[37m[1m [-164.73894212    0.52310008    0.3741        0.45590001    0.1008    ]
[37m[1m [-590.04502234    0.33637673    0.23857079    0.19628401    0.16597807]
[37m[1m ...
[37m[1m [-644.64520032    0.60140002    0.26350001    0.60690004    0.13710001]
[37m[1m [  30.65265927    0.52899998    0.3662        0.38410005    0.23380001]
[37m[1m [-344.04943731    0.5104        0.33719999    0.42050001    0.2694    ]]
[37m[1m[2023-06-25 07:44:33,063][129146] Max Reward on eval: 314.6745651061472
[37m[1m[2023-06-25 07:44:33,064][129146] Min Reward on eval: -1084.9637531635876
[37m[1m[2023-06-25 07:44:33,064][129146] Mean Reward across all agents: -301.96302868156215
[37m[1m[2023-06-25 07:44:33,064][129146] Average Trajectory Length: 993.1656666666667
[36m[2023-06-25 07:44:33,066][129146] mean_value=-856.1059387421552, max_value=281.9714756410468
[37m[1m[2023-06-25 07:44:33,069][129146] New mean coefficients: [[-0.6418687  -0.79155827 -0.1289685  -0.6332614  -0.94806623]]
[37m[1m[2023-06-25 07:44:33,070][129146] Moving the mean solution point...
[36m[2023-06-25 07:44:42,890][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 07:44:42,891][129146] FPS: 391085.68
[36m[2023-06-25 07:44:42,893][129146] itr=729, itrs=2000, Progress: 36.45%
[36m[2023-06-25 07:44:54,556][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 07:44:54,556][129146] FPS: 329875.80
[36m[2023-06-25 07:44:59,251][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:44:59,251][129146] Reward + Measures: [[24.90560442  0.53088969  0.37082815  0.36656851  0.10044602]]
[37m[1m[2023-06-25 07:44:59,252][129146] Max Reward on eval: 24.905604423177035
[37m[1m[2023-06-25 07:44:59,252][129146] Min Reward on eval: 24.905604423177035
[37m[1m[2023-06-25 07:44:59,252][129146] Mean Reward across all agents: 24.905604423177035
[37m[1m[2023-06-25 07:44:59,252][129146] Average Trajectory Length: 999.7389999999999
[36m[2023-06-25 07:45:04,848][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:45:04,849][129146] Reward + Measures: [[ -54.15990331    0.55769998    0.37939999    0.4765        0.0506    ]
[37m[1m [-407.02971116    0.45350003    0.37940001    0.36220002    0.1243    ]
[37m[1m [-357.97331684    0.51789999    0.35960001    0.51249999    0.0434    ]
[37m[1m ...
[37m[1m [ -79.50645164    0.55220002    0.375         0.42969999    0.0775    ]
[37m[1m [-327.65218339    0.44350001    0.35560003    0.33919999    0.1698    ]
[37m[1m [-725.16734955    0.3917        0.33259997    0.45440003    0.09870001]]
[37m[1m[2023-06-25 07:45:04,849][129146] Max Reward on eval: 228.83021778252441
[37m[1m[2023-06-25 07:45:04,849][129146] Min Reward on eval: -946.7553459334828
[37m[1m[2023-06-25 07:45:04,850][129146] Mean Reward across all agents: -311.3191118289967
[37m[1m[2023-06-25 07:45:04,850][129146] Average Trajectory Length: 967.5783333333333
[36m[2023-06-25 07:45:04,852][129146] mean_value=-724.2802119422726, max_value=351.6559424357479
[37m[1m[2023-06-25 07:45:04,855][129146] New mean coefficients: [[-0.06435841 -0.7013725  -1.1664553   0.5249147  -2.0837827 ]]
[37m[1m[2023-06-25 07:45:04,856][129146] Moving the mean solution point...
[36m[2023-06-25 07:45:14,462][129146] train() took 9.60 seconds to complete
[36m[2023-06-25 07:45:14,462][129146] FPS: 399840.38
[36m[2023-06-25 07:45:14,464][129146] itr=730, itrs=2000, Progress: 36.50%
[37m[1m[2023-06-25 07:45:20,743][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000710
[36m[2023-06-25 07:45:32,461][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 07:45:32,461][129146] FPS: 335129.31
[36m[2023-06-25 07:45:37,116][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:45:37,116][129146] Reward + Measures: [[18.72571851  0.52681488  0.36153528  0.36774358  0.09216411]]
[37m[1m[2023-06-25 07:45:37,116][129146] Max Reward on eval: 18.725718510844615
[37m[1m[2023-06-25 07:45:37,117][129146] Min Reward on eval: 18.725718510844615
[37m[1m[2023-06-25 07:45:37,117][129146] Mean Reward across all agents: 18.725718510844615
[37m[1m[2023-06-25 07:45:37,117][129146] Average Trajectory Length: 999.9273333333333
[36m[2023-06-25 07:45:42,477][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:45:42,538][129146] Reward + Measures: [[-404.13506653    0.43450004    0.41890001    0.3691        0.17309999]
[37m[1m [-388.83431922    0.43849999    0.41820002    0.36390004    0.1425    ]
[37m[1m [-607.83688455    0.45960003    0.41809997    0.44969997    0.0475    ]
[37m[1m ...
[37m[1m [-129.7158147     0.55549997    0.43210003    0.48689994    0.0176    ]
[37m[1m [ -36.09440242    0.40500003    0.22409999    0.31280002    0.24969999]
[37m[1m [-114.23781255    0.56870002    0.45240003    0.50199997    0.0268    ]]
[37m[1m[2023-06-25 07:45:42,538][129146] Max Reward on eval: 220.3284759623697
[37m[1m[2023-06-25 07:45:42,539][129146] Min Reward on eval: -715.3717788927373
[37m[1m[2023-06-25 07:45:42,539][129146] Mean Reward across all agents: -177.90459132300188
[37m[1m[2023-06-25 07:45:42,539][129146] Average Trajectory Length: 995.9736666666666
[36m[2023-06-25 07:45:42,541][129146] mean_value=-458.83073274453955, max_value=349.2030651423696
[37m[1m[2023-06-25 07:45:42,544][129146] New mean coefficients: [[-1.0852468 -0.1918168 -1.8525312  1.4754567 -1.3348202]]
[37m[1m[2023-06-25 07:45:42,545][129146] Moving the mean solution point...
[36m[2023-06-25 07:45:52,220][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 07:45:52,221][129146] FPS: 396939.49
[36m[2023-06-25 07:45:52,223][129146] itr=731, itrs=2000, Progress: 36.55%
[36m[2023-06-25 07:46:03,688][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 07:46:03,688][129146] FPS: 335553.69
[36m[2023-06-25 07:46:08,571][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:46:08,572][129146] Reward + Measures: [[-33.92238717   0.53623867   0.3652333    0.38137737   0.09320267]]
[37m[1m[2023-06-25 07:46:08,572][129146] Max Reward on eval: -33.922387167686495
[37m[1m[2023-06-25 07:46:08,572][129146] Min Reward on eval: -33.922387167686495
[37m[1m[2023-06-25 07:46:08,572][129146] Mean Reward across all agents: -33.922387167686495
[37m[1m[2023-06-25 07:46:08,573][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:46:14,000][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:46:14,000][129146] Reward + Measures: [[-138.34989811    0.57929999    0.3757        0.38970003    0.0719    ]
[37m[1m [-162.41465365    0.54470003    0.37270001    0.36939999    0.0754    ]
[37m[1m [  64.17642533    0.48199996    0.3468        0.30200002    0.1048    ]
[37m[1m ...
[37m[1m [ -79.68576732    0.56579995    0.377         0.37469998    0.0896    ]
[37m[1m [ -97.14624438    0.55380005    0.41980001    0.38840002    0.15110001]
[37m[1m [ -29.24655791    0.55699998    0.3881        0.42519999    0.0883    ]]
[37m[1m[2023-06-25 07:46:14,000][129146] Max Reward on eval: 121.74573140743306
[37m[1m[2023-06-25 07:46:14,001][129146] Min Reward on eval: -277.26607822892254
[37m[1m[2023-06-25 07:46:14,001][129146] Mean Reward across all agents: -66.24991512910422
[37m[1m[2023-06-25 07:46:14,001][129146] Average Trajectory Length: 999.7066666666666
[36m[2023-06-25 07:46:14,003][129146] mean_value=-57.76447766470267, max_value=282.7477023203493
[37m[1m[2023-06-25 07:46:14,006][129146] New mean coefficients: [[ 0.02989614  0.62941444 -2.3674622   2.12425    -0.6564942 ]]
[37m[1m[2023-06-25 07:46:14,007][129146] Moving the mean solution point...
[36m[2023-06-25 07:46:23,778][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 07:46:23,778][129146] FPS: 393074.26
[36m[2023-06-25 07:46:23,780][129146] itr=732, itrs=2000, Progress: 36.60%
[36m[2023-06-25 07:46:35,204][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 07:46:35,204][129146] FPS: 336780.56
[36m[2023-06-25 07:46:39,889][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:46:39,894][129146] Reward + Measures: [[-60.86151367   0.54399663   0.36347866   0.40300933   0.09625667]]
[37m[1m[2023-06-25 07:46:39,895][129146] Max Reward on eval: -60.861513670072775
[37m[1m[2023-06-25 07:46:39,895][129146] Min Reward on eval: -60.861513670072775
[37m[1m[2023-06-25 07:46:39,895][129146] Mean Reward across all agents: -60.861513670072775
[37m[1m[2023-06-25 07:46:39,895][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:46:45,270][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:46:45,271][129146] Reward + Measures: [[ 59.83642375   0.54800004   0.37810001   0.37420002   0.1177    ]
[37m[1m [ 29.45698166   0.5499       0.39540002   0.38080001   0.1069    ]
[37m[1m [ 32.53303479   0.53280002   0.40050003   0.396        0.11370001]
[37m[1m ...
[37m[1m [ 11.67182571   0.5248       0.33750001   0.36479998   0.16010001]
[37m[1m [ 64.50824076   0.5521       0.39000002   0.35209998   0.19509999]
[37m[1m [-29.76037495   0.53939998   0.36139998   0.3928       0.11850001]]
[37m[1m[2023-06-25 07:46:45,271][129146] Max Reward on eval: 205.39611817707774
[37m[1m[2023-06-25 07:46:45,271][129146] Min Reward on eval: -540.0816626565996
[37m[1m[2023-06-25 07:46:45,272][129146] Mean Reward across all agents: -40.7203016634485
[37m[1m[2023-06-25 07:46:45,272][129146] Average Trajectory Length: 996.4523333333333
[36m[2023-06-25 07:46:45,274][129146] mean_value=-686.2215374469998, max_value=140.80645937494654
[37m[1m[2023-06-25 07:46:45,276][129146] New mean coefficients: [[ 0.3278566  1.142021  -1.9756789  1.9657288 -1.131741 ]]
[37m[1m[2023-06-25 07:46:45,277][129146] Moving the mean solution point...
[36m[2023-06-25 07:46:55,058][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 07:46:55,058][129146] FPS: 392661.92
[36m[2023-06-25 07:46:55,061][129146] itr=733, itrs=2000, Progress: 36.65%
[36m[2023-06-25 07:47:06,519][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 07:47:06,520][129146] FPS: 335876.75
[36m[2023-06-25 07:47:11,324][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:47:11,324][129146] Reward + Measures: [[-75.78336068   0.54646373   0.36040461   0.4111447    0.09760866]]
[37m[1m[2023-06-25 07:47:11,324][129146] Max Reward on eval: -75.78336068018434
[37m[1m[2023-06-25 07:47:11,324][129146] Min Reward on eval: -75.78336068018434
[37m[1m[2023-06-25 07:47:11,325][129146] Mean Reward across all agents: -75.78336068018434
[37m[1m[2023-06-25 07:47:11,325][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:47:16,921][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:47:16,981][129146] Reward + Measures: [[ -89.63095216    0.5636        0.36929998    0.42300001    0.0933    ]
[37m[1m [ 109.51142685    0.52759999    0.3732        0.39900002    0.0832    ]
[37m[1m [-127.78816345    0.59060001    0.36390001    0.46799999    0.076     ]
[37m[1m ...
[37m[1m [ -51.82713262    0.58759993    0.35689998    0.43899998    0.08110001]
[37m[1m [-159.23382063    0.55790001    0.36180001    0.45290002    0.094     ]
[37m[1m [-165.03334832    0.509         0.36230001    0.40830001    0.1362    ]]
[37m[1m[2023-06-25 07:47:16,982][129146] Max Reward on eval: 257.55259341221534
[37m[1m[2023-06-25 07:47:16,982][129146] Min Reward on eval: -213.46372580016615
[37m[1m[2023-06-25 07:47:16,982][129146] Mean Reward across all agents: -36.408483469301416
[37m[1m[2023-06-25 07:47:16,982][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:47:16,984][129146] mean_value=-86.82797661643232, max_value=230.65015615760598
[37m[1m[2023-06-25 07:47:16,987][129146] New mean coefficients: [[ 1.6930773  1.0097249 -2.782123   2.6035872 -2.035883 ]]
[37m[1m[2023-06-25 07:47:16,988][129146] Moving the mean solution point...
[36m[2023-06-25 07:47:26,686][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 07:47:26,686][129146] FPS: 396007.20
[36m[2023-06-25 07:47:26,688][129146] itr=734, itrs=2000, Progress: 36.70%
[36m[2023-06-25 07:47:38,162][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 07:47:38,162][129146] FPS: 335313.69
[36m[2023-06-25 07:47:42,967][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:47:42,967][129146] Reward + Measures: [[-74.30133816   0.54923129   0.35780078   0.42913663   0.09674963]]
[37m[1m[2023-06-25 07:47:42,968][129146] Max Reward on eval: -74.30133816431693
[37m[1m[2023-06-25 07:47:42,968][129146] Min Reward on eval: -74.30133816431693
[37m[1m[2023-06-25 07:47:42,968][129146] Mean Reward across all agents: -74.30133816431693
[37m[1m[2023-06-25 07:47:42,968][129146] Average Trajectory Length: 999.8166666666666
[36m[2023-06-25 07:47:48,339][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:47:48,400][129146] Reward + Measures: [[ 34.41527778   0.56290001   0.34290001   0.41880003   0.08290001]
[37m[1m [ 77.71097592   0.56020004   0.35380003   0.39500001   0.074     ]
[37m[1m [ 96.88875667   0.54310006   0.34119996   0.37680003   0.08149999]
[37m[1m ...
[37m[1m [-46.80711055   0.56220001   0.35260001   0.4197       0.08200001]
[37m[1m [-34.29604669   0.55600005   0.37320003   0.4804       0.09250001]
[37m[1m [-19.60526537   0.52380002   0.34880003   0.45860001   0.15009999]]
[37m[1m[2023-06-25 07:47:48,400][129146] Max Reward on eval: 174.29652590036858
[37m[1m[2023-06-25 07:47:48,401][129146] Min Reward on eval: -156.891547956469
[37m[1m[2023-06-25 07:47:48,401][129146] Mean Reward across all agents: -3.0775060516012966
[37m[1m[2023-06-25 07:47:48,401][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:47:48,403][129146] mean_value=-198.7034826911512, max_value=233.8635629044056
[37m[1m[2023-06-25 07:47:48,406][129146] New mean coefficients: [[ 1.4695581  0.2856626 -2.719595   2.8066163 -2.324149 ]]
[37m[1m[2023-06-25 07:47:48,407][129146] Moving the mean solution point...
[36m[2023-06-25 07:47:58,089][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 07:47:58,089][129146] FPS: 396694.04
[36m[2023-06-25 07:47:58,091][129146] itr=735, itrs=2000, Progress: 36.75%
[36m[2023-06-25 07:48:09,534][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 07:48:09,534][129146] FPS: 336211.33
[36m[2023-06-25 07:48:14,303][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:48:14,303][129146] Reward + Measures: [[-61.66987791   0.55321562   0.35363266   0.44284669   0.09436133]]
[37m[1m[2023-06-25 07:48:14,303][129146] Max Reward on eval: -61.669877909892676
[37m[1m[2023-06-25 07:48:14,304][129146] Min Reward on eval: -61.669877909892676
[37m[1m[2023-06-25 07:48:14,304][129146] Mean Reward across all agents: -61.669877909892676
[37m[1m[2023-06-25 07:48:14,304][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:48:19,756][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:48:19,762][129146] Reward + Measures: [[-193.57468273    0.53730005    0.37530002    0.49450001    0.09740001]
[37m[1m [ -50.76470148    0.52459997    0.34440002    0.40980002    0.0682    ]
[37m[1m [-125.50645236    0.54720002    0.34490001    0.3863        0.0869    ]
[37m[1m ...
[37m[1m [-106.26196588    0.4578        0.32570001    0.3062        0.1257    ]
[37m[1m [ -16.4643616     0.43450004    0.30589998    0.27329999    0.0818    ]
[37m[1m [  32.2018118     0.55159998    0.38010001    0.31810001    0.08990001]]
[37m[1m[2023-06-25 07:48:19,763][129146] Max Reward on eval: 125.02157643475803
[37m[1m[2023-06-25 07:48:19,763][129146] Min Reward on eval: -352.9627400097321
[37m[1m[2023-06-25 07:48:19,763][129146] Mean Reward across all agents: -88.17559782453436
[37m[1m[2023-06-25 07:48:19,763][129146] Average Trajectory Length: 996.5746666666666
[36m[2023-06-25 07:48:19,765][129146] mean_value=-452.1004599340327, max_value=359.25473706611956
[37m[1m[2023-06-25 07:48:19,767][129146] New mean coefficients: [[ 1.3038489  2.0334358 -1.7256784  2.6757061 -1.7891781]]
[37m[1m[2023-06-25 07:48:19,768][129146] Moving the mean solution point...
[36m[2023-06-25 07:48:29,556][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 07:48:29,557][129146] FPS: 392380.30
[36m[2023-06-25 07:48:29,559][129146] itr=736, itrs=2000, Progress: 36.80%
[36m[2023-06-25 07:48:41,104][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 07:48:41,104][129146] FPS: 333246.90
[36m[2023-06-25 07:48:45,897][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:48:45,898][129146] Reward + Measures: [[-39.43872088   0.55765235   0.35039902   0.44847998   0.09411232]]
[37m[1m[2023-06-25 07:48:45,898][129146] Max Reward on eval: -39.43872088080154
[37m[1m[2023-06-25 07:48:45,898][129146] Min Reward on eval: -39.43872088080154
[37m[1m[2023-06-25 07:48:45,898][129146] Mean Reward across all agents: -39.43872088080154
[37m[1m[2023-06-25 07:48:45,898][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:48:51,360][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:48:51,361][129146] Reward + Measures: [[-169.27540229    0.57960004    0.36619997    0.42020002    0.0599    ]
[37m[1m [ -85.63300649    0.57610005    0.38870001    0.47569999    0.0946    ]
[37m[1m [-136.80493605    0.56410003    0.35669997    0.34470001    0.11360001]
[37m[1m ...
[37m[1m [   4.32671831    0.57980001    0.36949998    0.4691        0.09410001]
[37m[1m [ -16.12818498    0.49437913    0.34873816    0.41975185    0.11345541]
[37m[1m [ -78.20114139    0.58660001    0.37290001    0.45190001    0.10600001]]
[37m[1m[2023-06-25 07:48:51,361][129146] Max Reward on eval: 193.76698646958684
[37m[1m[2023-06-25 07:48:51,361][129146] Min Reward on eval: -297.0335896391131
[37m[1m[2023-06-25 07:48:51,361][129146] Mean Reward across all agents: -66.36750439681681
[37m[1m[2023-06-25 07:48:51,362][129146] Average Trajectory Length: 998.444
[36m[2023-06-25 07:48:51,363][129146] mean_value=-338.87901278425176, max_value=389.4736598557629
[37m[1m[2023-06-25 07:48:51,366][129146] New mean coefficients: [[ 2.3275104   3.0340858  -0.847027    3.55049     0.13757074]]
[37m[1m[2023-06-25 07:48:51,367][129146] Moving the mean solution point...
[36m[2023-06-25 07:49:01,192][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 07:49:01,192][129146] FPS: 390902.62
[36m[2023-06-25 07:49:01,194][129146] itr=737, itrs=2000, Progress: 36.85%
[36m[2023-06-25 07:49:12,657][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 07:49:12,657][129146] FPS: 335635.70
[36m[2023-06-25 07:49:17,467][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:49:17,467][129146] Reward + Measures: [[-7.10533292  0.56359494  0.35167933  0.45157766  0.09947101]]
[37m[1m[2023-06-25 07:49:17,468][129146] Max Reward on eval: -7.105332919472765
[37m[1m[2023-06-25 07:49:17,468][129146] Min Reward on eval: -7.105332919472765
[37m[1m[2023-06-25 07:49:17,468][129146] Mean Reward across all agents: -7.105332919472765
[37m[1m[2023-06-25 07:49:17,468][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:49:22,892][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:49:22,944][129146] Reward + Measures: [[113.94855862   0.54519999   0.3484       0.43730003   0.13170001]
[37m[1m [-19.17641083   0.56809998   0.3396       0.45759997   0.0936    ]
[37m[1m [ 28.36818554   0.58210003   0.34240001   0.47200003   0.1049    ]
[37m[1m ...
[37m[1m [100.42217898   0.59549999   0.35120001   0.50190002   0.0991    ]
[37m[1m [267.29782426   0.55430001   0.29440001   0.5018       0.1979    ]
[37m[1m [201.02035245   0.56969994   0.3443       0.46739998   0.11739999]]
[37m[1m[2023-06-25 07:49:22,944][129146] Max Reward on eval: 319.3905895370757
[37m[1m[2023-06-25 07:49:22,944][129146] Min Reward on eval: -203.9601943665999
[37m[1m[2023-06-25 07:49:22,944][129146] Mean Reward across all agents: 86.54313265318838
[37m[1m[2023-06-25 07:49:22,945][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:49:22,948][129146] mean_value=-12.213868947194157, max_value=307.7845395943892
[37m[1m[2023-06-25 07:49:22,951][129146] New mean coefficients: [[ 4.091695    5.2189074  -1.8399029   4.0404077   0.02450235]]
[37m[1m[2023-06-25 07:49:22,952][129146] Moving the mean solution point...
[36m[2023-06-25 07:49:32,629][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 07:49:32,630][129146] FPS: 396856.94
[36m[2023-06-25 07:49:32,632][129146] itr=738, itrs=2000, Progress: 36.90%
[36m[2023-06-25 07:49:44,250][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 07:49:44,250][129146] FPS: 331126.64
[36m[2023-06-25 07:49:49,069][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:49:49,074][129146] Reward + Measures: [[26.56067752  0.56925762  0.34792066  0.45468897  0.09924133]]
[37m[1m[2023-06-25 07:49:49,074][129146] Max Reward on eval: 26.560677517961103
[37m[1m[2023-06-25 07:49:49,075][129146] Min Reward on eval: 26.560677517961103
[37m[1m[2023-06-25 07:49:49,075][129146] Mean Reward across all agents: 26.560677517961103
[37m[1m[2023-06-25 07:49:49,075][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:49:54,742][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:49:54,748][129146] Reward + Measures: [[ 97.90408723   0.57510006   0.336        0.47779998   0.1124    ]
[37m[1m [129.2856845    0.5643       0.30770001   0.50690001   0.12630001]
[37m[1m [ 60.45422655   0.56240004   0.33870003   0.50089997   0.12360001]
[37m[1m ...
[37m[1m [175.11008039   0.56109995   0.3118       0.4612       0.19090001]
[37m[1m [ 34.73350702   0.58529997   0.3175       0.52329999   0.0775    ]
[37m[1m [ 93.70799332   0.55980003   0.32749999   0.493        0.1245    ]]
[37m[1m[2023-06-25 07:49:54,748][129146] Max Reward on eval: 204.08520878291455
[37m[1m[2023-06-25 07:49:54,748][129146] Min Reward on eval: -22.226014913422112
[37m[1m[2023-06-25 07:49:54,749][129146] Mean Reward across all agents: 78.60984841685085
[37m[1m[2023-06-25 07:49:54,749][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:49:54,751][129146] mean_value=-175.21499376104353, max_value=457.02866698155145
[37m[1m[2023-06-25 07:49:54,753][129146] New mean coefficients: [[ 4.1899343   4.859767   -2.455576    4.359931   -0.12142332]]
[37m[1m[2023-06-25 07:49:54,754][129146] Moving the mean solution point...
[36m[2023-06-25 07:50:04,608][129146] train() took 9.85 seconds to complete
[36m[2023-06-25 07:50:04,608][129146] FPS: 389762.74
[36m[2023-06-25 07:50:04,611][129146] itr=739, itrs=2000, Progress: 36.95%
[36m[2023-06-25 07:50:16,167][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 07:50:16,167][129146] FPS: 332905.01
[36m[2023-06-25 07:50:20,820][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:50:20,820][129146] Reward + Measures: [[59.95102156  0.5726366   0.34930465  0.46023268  0.10543066]]
[37m[1m[2023-06-25 07:50:20,820][129146] Max Reward on eval: 59.95102155542946
[37m[1m[2023-06-25 07:50:20,821][129146] Min Reward on eval: 59.95102155542946
[37m[1m[2023-06-25 07:50:20,821][129146] Mean Reward across all agents: 59.95102155542946
[37m[1m[2023-06-25 07:50:20,821][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:50:26,274][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:50:26,275][129146] Reward + Measures: [[124.23141269   0.58660001   0.36170003   0.47049999   0.10650001]
[37m[1m [251.58976682   0.59060001   0.36789998   0.42179999   0.1499    ]
[37m[1m [145.13235075   0.55619997   0.38930002   0.52579999   0.12899999]
[37m[1m ...
[37m[1m [132.10552573   0.57700002   0.3626       0.52039999   0.1229    ]
[37m[1m [170.9311791    0.58269995   0.31920001   0.48780003   0.1076    ]
[37m[1m [162.1916627    0.5898       0.38100001   0.46360001   0.1366    ]]
[37m[1m[2023-06-25 07:50:26,275][129146] Max Reward on eval: 326.51055038927586
[37m[1m[2023-06-25 07:50:26,275][129146] Min Reward on eval: -31.03793647615239
[37m[1m[2023-06-25 07:50:26,275][129146] Mean Reward across all agents: 137.33618792819433
[37m[1m[2023-06-25 07:50:26,276][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:50:26,279][129146] mean_value=-53.25184405197917, max_value=431.5898549651497
[37m[1m[2023-06-25 07:50:26,281][129146] New mean coefficients: [[ 4.048966    5.4251127  -2.1393633   5.3174815   0.43534338]]
[37m[1m[2023-06-25 07:50:26,282][129146] Moving the mean solution point...
[36m[2023-06-25 07:50:36,023][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 07:50:36,023][129146] FPS: 394280.80
[36m[2023-06-25 07:50:36,026][129146] itr=740, itrs=2000, Progress: 37.00%
[37m[1m[2023-06-25 07:50:42,380][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000720
[36m[2023-06-25 07:50:54,132][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 07:50:54,132][129146] FPS: 333867.69
[36m[2023-06-25 07:50:58,816][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:50:58,817][129146] Reward + Measures: [[89.65165565  0.576958    0.35069233  0.46529466  0.10989366]]
[37m[1m[2023-06-25 07:50:58,817][129146] Max Reward on eval: 89.65165564576539
[37m[1m[2023-06-25 07:50:58,817][129146] Min Reward on eval: 89.65165564576539
[37m[1m[2023-06-25 07:50:58,817][129146] Mean Reward across all agents: 89.65165564576539
[37m[1m[2023-06-25 07:50:58,818][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:51:04,268][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:51:04,269][129146] Reward + Measures: [[ 35.46775495   0.54449999   0.33109999   0.45200005   0.16489999]
[37m[1m [ 68.3072962    0.57610005   0.36310002   0.48730001   0.1109    ]
[37m[1m [ 75.01497894   0.56510001   0.3881       0.50489998   0.0921    ]
[37m[1m ...
[37m[1m [184.15208743   0.55089998   0.43360001   0.40100002   0.2124    ]
[37m[1m [ -5.64529383   0.56820005   0.39730003   0.49980003   0.1028    ]
[37m[1m [ 95.90856493   0.57110006   0.40170002   0.46380001   0.1209    ]]
[37m[1m[2023-06-25 07:51:04,269][129146] Max Reward on eval: 368.58556556507244
[37m[1m[2023-06-25 07:51:04,270][129146] Min Reward on eval: -162.63678794451988
[37m[1m[2023-06-25 07:51:04,270][129146] Mean Reward across all agents: 81.87827855259974
[37m[1m[2023-06-25 07:51:04,270][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:51:04,271][129146] mean_value=-183.60394485644457, max_value=147.63078204824635
[37m[1m[2023-06-25 07:51:04,274][129146] New mean coefficients: [[ 5.042161    5.3542886  -2.0427766   5.1064134   0.27594453]]
[37m[1m[2023-06-25 07:51:04,275][129146] Moving the mean solution point...
[36m[2023-06-25 07:51:13,927][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 07:51:13,927][129146] FPS: 397916.42
[36m[2023-06-25 07:51:13,929][129146] itr=741, itrs=2000, Progress: 37.05%
[36m[2023-06-25 07:51:25,374][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 07:51:25,375][129146] FPS: 336170.98
[36m[2023-06-25 07:51:30,198][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:51:30,198][129146] Reward + Measures: [[118.94623072   0.58098471   0.350503     0.46974131   0.11467133]]
[37m[1m[2023-06-25 07:51:30,199][129146] Max Reward on eval: 118.9462307241282
[37m[1m[2023-06-25 07:51:30,199][129146] Min Reward on eval: 118.9462307241282
[37m[1m[2023-06-25 07:51:30,199][129146] Mean Reward across all agents: 118.9462307241282
[37m[1m[2023-06-25 07:51:30,199][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:51:35,713][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:51:35,714][129146] Reward + Measures: [[ -56.99136799    0.61200005    0.40990001    0.57550001    0.0785    ]
[37m[1m [  22.93318151    0.58330005    0.41659999    0.54759997    0.11440001]
[37m[1m [-119.08220247    0.56379998    0.37540004    0.58840001    0.10109999]
[37m[1m ...
[37m[1m [ -46.93061424    0.57840002    0.38730001    0.56999999    0.1027    ]
[37m[1m [  74.55974883    0.57359999    0.35209998    0.50639999    0.13460001]
[37m[1m [ -88.10829796    0.56750005    0.35479999    0.56999999    0.1109    ]]
[37m[1m[2023-06-25 07:51:35,714][129146] Max Reward on eval: 168.03473990065976
[37m[1m[2023-06-25 07:51:35,714][129146] Min Reward on eval: -152.21763381070923
[37m[1m[2023-06-25 07:51:35,715][129146] Mean Reward across all agents: -30.011489844952848
[37m[1m[2023-06-25 07:51:35,715][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:51:35,718][129146] mean_value=-83.35232026368168, max_value=397.32436299701277
[37m[1m[2023-06-25 07:51:35,720][129146] New mean coefficients: [[ 5.3408766  7.4558496 -2.0980659  5.1751337  1.0639932]]
[37m[1m[2023-06-25 07:51:35,721][129146] Moving the mean solution point...
[36m[2023-06-25 07:51:45,413][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 07:51:45,413][129146] FPS: 396302.77
[36m[2023-06-25 07:51:45,415][129146] itr=742, itrs=2000, Progress: 37.10%
[36m[2023-06-25 07:51:56,859][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 07:51:56,859][129146] FPS: 336282.37
[36m[2023-06-25 07:52:01,503][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:52:01,504][129146] Reward + Measures: [[151.30480052   0.58300465   0.35342965   0.47423497   0.12206867]]
[37m[1m[2023-06-25 07:52:01,504][129146] Max Reward on eval: 151.30480052371456
[37m[1m[2023-06-25 07:52:01,504][129146] Min Reward on eval: 151.30480052371456
[37m[1m[2023-06-25 07:52:01,504][129146] Mean Reward across all agents: 151.30480052371456
[37m[1m[2023-06-25 07:52:01,505][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:52:07,049][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:52:07,050][129146] Reward + Measures: [[275.52729922   0.53430003   0.38709998   0.31059998   0.1089    ]
[37m[1m [199.33654295   0.54339999   0.37550002   0.36129996   0.0801    ]
[37m[1m [171.47567171   0.59650004   0.37660003   0.37270001   0.05070001]
[37m[1m ...
[37m[1m [ 65.75088687   0.5618       0.36680001   0.53750002   0.0684    ]
[37m[1m [162.04797563   0.5492       0.3818       0.41809997   0.13330001]
[37m[1m [181.10577296   0.58639997   0.3987       0.398        0.069     ]]
[37m[1m[2023-06-25 07:52:07,050][129146] Max Reward on eval: 397.3650224037585
[37m[1m[2023-06-25 07:52:07,050][129146] Min Reward on eval: -130.39623602014035
[37m[1m[2023-06-25 07:52:07,051][129146] Mean Reward across all agents: 133.0041916431667
[37m[1m[2023-06-25 07:52:07,051][129146] Average Trajectory Length: 999.6596666666667
[36m[2023-06-25 07:52:07,054][129146] mean_value=-103.64468127155452, max_value=611.7979148642626
[37m[1m[2023-06-25 07:52:07,056][129146] New mean coefficients: [[ 5.993993   8.3141165 -1.9161791  3.3594916  1.6340523]]
[37m[1m[2023-06-25 07:52:07,057][129146] Moving the mean solution point...
[36m[2023-06-25 07:52:16,747][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 07:52:16,748][129146] FPS: 396337.41
[36m[2023-06-25 07:52:16,750][129146] itr=743, itrs=2000, Progress: 37.15%
[36m[2023-06-25 07:52:28,178][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 07:52:28,179][129146] FPS: 336680.13
[36m[2023-06-25 07:52:32,939][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:52:32,939][129146] Reward + Measures: [[193.44548276   0.589127     0.35307968   0.47141731   0.12402966]]
[37m[1m[2023-06-25 07:52:32,939][129146] Max Reward on eval: 193.44548275832653
[37m[1m[2023-06-25 07:52:32,940][129146] Min Reward on eval: 193.44548275832653
[37m[1m[2023-06-25 07:52:32,940][129146] Mean Reward across all agents: 193.44548275832653
[37m[1m[2023-06-25 07:52:32,940][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:52:38,434][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:52:38,434][129146] Reward + Measures: [[ 44.60683222   0.57429999   0.38869998   0.4587       0.16690001]
[37m[1m [ 73.57053986   0.60400003   0.4032       0.39570004   0.1679    ]
[37m[1m [181.94449703   0.60680002   0.40529999   0.36420003   0.2234    ]
[37m[1m ...
[37m[1m [132.18634988   0.59310001   0.40030003   0.43670002   0.15640001]
[37m[1m [104.97033931   0.60610002   0.37330002   0.4691       0.1038    ]
[37m[1m [183.23277296   0.62589997   0.3752       0.42379999   0.1274    ]]
[37m[1m[2023-06-25 07:52:38,434][129146] Max Reward on eval: 283.92524530640804
[37m[1m[2023-06-25 07:52:38,435][129146] Min Reward on eval: -74.16305985709187
[37m[1m[2023-06-25 07:52:38,435][129146] Mean Reward across all agents: 102.34743004032318
[37m[1m[2023-06-25 07:52:38,435][129146] Average Trajectory Length: 999.9286666666667
[36m[2023-06-25 07:52:38,437][129146] mean_value=-444.32448615077436, max_value=199.9031317383173
[37m[1m[2023-06-25 07:52:38,439][129146] New mean coefficients: [[ 4.381778   6.4034214 -0.5909786  3.9684658  1.5522097]]
[37m[1m[2023-06-25 07:52:38,440][129146] Moving the mean solution point...
[36m[2023-06-25 07:52:48,127][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 07:52:48,127][129146] FPS: 396471.84
[36m[2023-06-25 07:52:48,129][129146] itr=744, itrs=2000, Progress: 37.20%
[36m[2023-06-25 07:52:59,611][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 07:52:59,611][129146] FPS: 335075.21
[36m[2023-06-25 07:53:04,475][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:53:04,475][129146] Reward + Measures: [[248.92802991   0.58878368   0.35185367   0.46526766   0.137298  ]]
[37m[1m[2023-06-25 07:53:04,475][129146] Max Reward on eval: 248.92802991295164
[37m[1m[2023-06-25 07:53:04,476][129146] Min Reward on eval: 248.92802991295164
[37m[1m[2023-06-25 07:53:04,476][129146] Mean Reward across all agents: 248.92802991295164
[37m[1m[2023-06-25 07:53:04,476][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:53:09,982][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:53:09,988][129146] Reward + Measures: [[214.12275496   0.5927       0.3655       0.53529996   0.0997    ]
[37m[1m [101.85678182   0.57389998   0.35010001   0.47610003   0.1268    ]
[37m[1m [161.27243203   0.59190005   0.37819999   0.46970001   0.11919999]
[37m[1m ...
[37m[1m [ 68.4836023    0.58359998   0.39119998   0.5492       0.0693    ]
[37m[1m [157.67566474   0.59450001   0.3987       0.46230003   0.13240001]
[37m[1m [230.80410344   0.55810004   0.35090002   0.49179998   0.14360002]]
[37m[1m[2023-06-25 07:53:09,988][129146] Max Reward on eval: 320.5940110077372
[37m[1m[2023-06-25 07:53:09,988][129146] Min Reward on eval: 40.66543889252061
[37m[1m[2023-06-25 07:53:09,989][129146] Mean Reward across all agents: 173.30350543572965
[37m[1m[2023-06-25 07:53:09,989][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:53:09,992][129146] mean_value=-16.687953672980832, max_value=379.02585844601106
[37m[1m[2023-06-25 07:53:09,994][129146] New mean coefficients: [[ 3.8980727   6.064247   -1.104285    5.454211    0.26260555]]
[37m[1m[2023-06-25 07:53:09,995][129146] Moving the mean solution point...
[36m[2023-06-25 07:53:19,699][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 07:53:19,699][129146] FPS: 395801.09
[36m[2023-06-25 07:53:19,701][129146] itr=745, itrs=2000, Progress: 37.25%
[36m[2023-06-25 07:53:31,364][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 07:53:31,364][129146] FPS: 329875.89
[36m[2023-06-25 07:53:36,245][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:53:36,246][129146] Reward + Measures: [[291.92601201   0.58614439   0.34967765   0.46171033   0.14920533]]
[37m[1m[2023-06-25 07:53:36,246][129146] Max Reward on eval: 291.92601200919574
[37m[1m[2023-06-25 07:53:36,247][129146] Min Reward on eval: 291.92601200919574
[37m[1m[2023-06-25 07:53:36,247][129146] Mean Reward across all agents: 291.92601200919574
[37m[1m[2023-06-25 07:53:36,247][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:53:41,691][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:53:41,697][129146] Reward + Measures: [[336.2193224    0.5474       0.33070001   0.45860001   0.23050001]
[37m[1m [239.69699807   0.58320004   0.30600002   0.46639997   0.15710001]
[37m[1m [136.24818889   0.52160001   0.33160001   0.4271       0.1793    ]
[37m[1m ...
[37m[1m [-93.12714962   0.53150004   0.3565       0.39080003   0.20440002]
[37m[1m [237.30618971   0.588        0.2728       0.49540001   0.1463    ]
[37m[1m [321.57963034   0.55769998   0.30800003   0.41190001   0.1957    ]]
[37m[1m[2023-06-25 07:53:41,697][129146] Max Reward on eval: 409.6735631412477
[37m[1m[2023-06-25 07:53:41,698][129146] Min Reward on eval: -276.18750432888044
[37m[1m[2023-06-25 07:53:41,698][129146] Mean Reward across all agents: 169.70395908438923
[37m[1m[2023-06-25 07:53:41,698][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:53:41,700][129146] mean_value=-352.56480770920535, max_value=132.54420325669318
[37m[1m[2023-06-25 07:53:41,703][129146] New mean coefficients: [[ 4.6687627   6.8865886  -0.7249205   6.1471224  -0.43426812]]
[37m[1m[2023-06-25 07:53:41,704][129146] Moving the mean solution point...
[36m[2023-06-25 07:53:51,414][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 07:53:51,414][129146] FPS: 395524.11
[36m[2023-06-25 07:53:51,417][129146] itr=746, itrs=2000, Progress: 37.30%
[36m[2023-06-25 07:54:03,014][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 07:54:03,015][129146] FPS: 331741.83
[36m[2023-06-25 07:54:07,858][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:54:07,858][129146] Reward + Measures: [[316.35545726   0.59020501   0.35173169   0.46445134   0.15816733]]
[37m[1m[2023-06-25 07:54:07,858][129146] Max Reward on eval: 316.35545726079164
[37m[1m[2023-06-25 07:54:07,859][129146] Min Reward on eval: 316.35545726079164
[37m[1m[2023-06-25 07:54:07,859][129146] Mean Reward across all agents: 316.35545726079164
[37m[1m[2023-06-25 07:54:07,859][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:54:13,359][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:54:13,360][129146] Reward + Measures: [[ 146.63148677    0.58100003    0.3883        0.5036        0.15570001]
[37m[1m [ 272.99065458    0.588         0.32249999    0.46610004    0.22870003]
[37m[1m [ 233.45749468    0.57620001    0.33400002    0.49959999    0.2313    ]
[37m[1m ...
[37m[1m [-104.5939081     0.5298        0.30749997    0.45499998    0.23480001]
[37m[1m [ 363.58145116    0.52930003    0.28830001    0.43560001    0.25729999]
[37m[1m [ 123.89772374    0.5589        0.37220001    0.50060004    0.13789999]]
[37m[1m[2023-06-25 07:54:13,360][129146] Max Reward on eval: 456.0281628963479
[37m[1m[2023-06-25 07:54:13,360][129146] Min Reward on eval: -329.2342308925232
[37m[1m[2023-06-25 07:54:13,361][129146] Mean Reward across all agents: 196.0859179817127
[37m[1m[2023-06-25 07:54:13,361][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:54:13,363][129146] mean_value=-307.9764916299853, max_value=170.76576910747679
[37m[1m[2023-06-25 07:54:13,365][129146] New mean coefficients: [[ 4.6892037   5.317834   -0.68624836  6.7583036   0.5751703 ]]
[37m[1m[2023-06-25 07:54:13,366][129146] Moving the mean solution point...
[36m[2023-06-25 07:54:23,170][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 07:54:23,170][129146] FPS: 391734.57
[36m[2023-06-25 07:54:23,173][129146] itr=747, itrs=2000, Progress: 37.35%
[36m[2023-06-25 07:54:34,691][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 07:54:34,691][129146] FPS: 334152.44
[36m[2023-06-25 07:54:39,481][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:54:39,481][129146] Reward + Measures: [[337.90923215   0.59439063   0.35131234   0.46985865   0.16452533]]
[37m[1m[2023-06-25 07:54:39,481][129146] Max Reward on eval: 337.9092321531062
[37m[1m[2023-06-25 07:54:39,481][129146] Min Reward on eval: 337.9092321531062
[37m[1m[2023-06-25 07:54:39,482][129146] Mean Reward across all agents: 337.9092321531062
[37m[1m[2023-06-25 07:54:39,482][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:54:45,147][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:54:45,148][129146] Reward + Measures: [[175.13835384   0.56620002   0.37269998   0.50790006   0.07740001]
[37m[1m [ 85.6152823    0.55339998   0.36930004   0.50940001   0.0823    ]
[37m[1m [309.54929827   0.61050004   0.31350002   0.51160002   0.16160001]
[37m[1m ...
[37m[1m [ 76.90981368   0.62880003   0.31550002   0.498        0.11600001]
[37m[1m [334.92725742   0.63         0.31920001   0.47849998   0.1296    ]
[37m[1m [220.59079183   0.62870008   0.34209999   0.4639       0.1097    ]]
[37m[1m[2023-06-25 07:54:45,148][129146] Max Reward on eval: 448.45167055077616
[37m[1m[2023-06-25 07:54:45,149][129146] Min Reward on eval: 54.94074078835547
[37m[1m[2023-06-25 07:54:45,149][129146] Mean Reward across all agents: 229.5638450824993
[37m[1m[2023-06-25 07:54:45,149][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:54:45,151][129146] mean_value=-108.63562950000292, max_value=130.39350256372603
[37m[1m[2023-06-25 07:54:45,154][129146] New mean coefficients: [[ 5.115257    4.1637535  -0.34909132  7.167822   -0.02790153]]
[37m[1m[2023-06-25 07:54:45,155][129146] Moving the mean solution point...
[36m[2023-06-25 07:54:54,813][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 07:54:54,813][129146] FPS: 397660.40
[36m[2023-06-25 07:54:54,815][129146] itr=748, itrs=2000, Progress: 37.40%
[36m[2023-06-25 07:55:06,414][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 07:55:06,414][129146] FPS: 331687.14
[36m[2023-06-25 07:55:11,292][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:55:11,292][129146] Reward + Measures: [[359.64214717   0.60119802   0.35119066   0.47386596   0.16574733]]
[37m[1m[2023-06-25 07:55:11,292][129146] Max Reward on eval: 359.6421471686293
[37m[1m[2023-06-25 07:55:11,292][129146] Min Reward on eval: 359.6421471686293
[37m[1m[2023-06-25 07:55:11,293][129146] Mean Reward across all agents: 359.6421471686293
[37m[1m[2023-06-25 07:55:11,293][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:55:16,755][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:55:16,755][129146] Reward + Measures: [[236.17426822   0.62720001   0.39439997   0.55390006   0.10879999]
[37m[1m [133.78464691   0.58470005   0.43500003   0.55580002   0.1096    ]
[37m[1m [262.09338194   0.62540001   0.40200001   0.5456       0.11790001]
[37m[1m ...
[37m[1m [102.30834053   0.5808       0.41389999   0.57639998   0.0905    ]
[37m[1m [147.89952457   0.60729998   0.40669999   0.57470006   0.10699999]
[37m[1m [196.8537132    0.56740004   0.44229999   0.57860005   0.1147    ]]
[37m[1m[2023-06-25 07:55:16,756][129146] Max Reward on eval: 337.22791273365726
[37m[1m[2023-06-25 07:55:16,756][129146] Min Reward on eval: -36.14294137532124
[37m[1m[2023-06-25 07:55:16,756][129146] Mean Reward across all agents: 162.68306873374632
[37m[1m[2023-06-25 07:55:16,756][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:55:16,760][129146] mean_value=26.813244830097315, max_value=434.1493951482599
[37m[1m[2023-06-25 07:55:16,763][129146] New mean coefficients: [[ 6.5927467  4.299558  -1.0044225  8.828756  -0.5869744]]
[37m[1m[2023-06-25 07:55:16,764][129146] Moving the mean solution point...
[36m[2023-06-25 07:55:26,381][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 07:55:26,381][129146] FPS: 399350.55
[36m[2023-06-25 07:55:26,383][129146] itr=749, itrs=2000, Progress: 37.45%
[36m[2023-06-25 07:55:37,962][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 07:55:37,962][129146] FPS: 332293.75
[36m[2023-06-25 07:55:42,771][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:55:42,771][129146] Reward + Measures: [[389.39315541   0.60251862   0.35280168   0.47687435   0.17044733]]
[37m[1m[2023-06-25 07:55:42,771][129146] Max Reward on eval: 389.3931554116754
[37m[1m[2023-06-25 07:55:42,772][129146] Min Reward on eval: 389.3931554116754
[37m[1m[2023-06-25 07:55:42,772][129146] Mean Reward across all agents: 389.3931554116754
[37m[1m[2023-06-25 07:55:42,772][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:55:48,292][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:55:48,293][129146] Reward + Measures: [[400.60063625   0.54319996   0.31869999   0.46199998   0.19560002]
[37m[1m [328.36146384   0.56620002   0.30790001   0.53050005   0.1517    ]
[37m[1m [436.48955754   0.58109999   0.329        0.47539997   0.1846    ]
[37m[1m ...
[37m[1m [386.62001698   0.57920003   0.3213       0.50440001   0.14230001]
[37m[1m [293.26225615   0.616        0.30090001   0.53070003   0.1285    ]
[37m[1m [352.0141127    0.6358       0.27510002   0.5363       0.1401    ]]
[37m[1m[2023-06-25 07:55:48,293][129146] Max Reward on eval: 475.75910764737927
[37m[1m[2023-06-25 07:55:48,293][129146] Min Reward on eval: 147.75364616527222
[37m[1m[2023-06-25 07:55:48,293][129146] Mean Reward across all agents: 309.19621994093325
[37m[1m[2023-06-25 07:55:48,294][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:55:48,297][129146] mean_value=-52.90628583191657, max_value=596.7461790128733
[37m[1m[2023-06-25 07:55:48,300][129146] New mean coefficients: [[ 7.4417963  3.8488483 -2.1279411 10.194302  -0.5828189]]
[37m[1m[2023-06-25 07:55:48,301][129146] Moving the mean solution point...
[36m[2023-06-25 07:55:58,084][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 07:55:58,084][129146] FPS: 392608.36
[36m[2023-06-25 07:55:58,086][129146] itr=750, itrs=2000, Progress: 37.50%
[37m[1m[2023-06-25 07:56:04,485][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000730
[36m[2023-06-25 07:56:16,184][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 07:56:16,185][129146] FPS: 334704.00
[36m[2023-06-25 07:56:20,903][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:56:20,903][129146] Reward + Measures: [[406.30506134   0.607674     0.3500393    0.48987931   0.16634533]]
[37m[1m[2023-06-25 07:56:20,903][129146] Max Reward on eval: 406.30506133860627
[37m[1m[2023-06-25 07:56:20,904][129146] Min Reward on eval: 406.30506133860627
[37m[1m[2023-06-25 07:56:20,904][129146] Mean Reward across all agents: 406.30506133860627
[37m[1m[2023-06-25 07:56:20,904][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:56:26,225][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:56:26,226][129146] Reward + Measures: [[191.57726551   0.57790005   0.36590001   0.5801       0.26900002]
[37m[1m [278.01707127   0.55680007   0.35049999   0.4993       0.1517    ]
[37m[1m [245.752849     0.5546       0.36349997   0.54500002   0.2103    ]
[37m[1m ...
[37m[1m [290.30974982   0.58459997   0.36269999   0.53350002   0.2168    ]
[37m[1m [204.8685643    0.60480005   0.4233       0.59420002   0.28740001]
[37m[1m [192.3412219    0.5625       0.33239999   0.53130001   0.18240002]]
[37m[1m[2023-06-25 07:56:26,226][129146] Max Reward on eval: 382.1102234808961
[37m[1m[2023-06-25 07:56:26,226][129146] Min Reward on eval: 28.871002556884196
[37m[1m[2023-06-25 07:56:26,226][129146] Mean Reward across all agents: 244.56148907994958
[37m[1m[2023-06-25 07:56:26,227][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:56:26,230][129146] mean_value=50.37993114493531, max_value=615.748855007248
[37m[1m[2023-06-25 07:56:26,233][129146] New mean coefficients: [[ 8.342694    5.7543764  -0.9196029  11.003024   -0.19120309]]
[37m[1m[2023-06-25 07:56:26,234][129146] Moving the mean solution point...
[36m[2023-06-25 07:56:35,851][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 07:56:35,852][129146] FPS: 399344.00
[36m[2023-06-25 07:56:35,854][129146] itr=751, itrs=2000, Progress: 37.55%
[36m[2023-06-25 07:56:47,287][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 07:56:47,287][129146] FPS: 336521.26
[36m[2023-06-25 07:56:52,051][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:56:52,051][129146] Reward + Measures: [[418.25062761   0.61302465   0.34793669   0.49963599   0.16647901]]
[37m[1m[2023-06-25 07:56:52,052][129146] Max Reward on eval: 418.25062760796135
[37m[1m[2023-06-25 07:56:52,052][129146] Min Reward on eval: 418.25062760796135
[37m[1m[2023-06-25 07:56:52,052][129146] Mean Reward across all agents: 418.25062760796135
[37m[1m[2023-06-25 07:56:52,052][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:56:57,406][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:56:57,407][129146] Reward + Measures: [[173.37171222   0.65259999   0.39180002   0.57160002   0.05449999]
[37m[1m [165.65209965   0.62169999   0.37600002   0.60610002   0.0565    ]
[37m[1m [174.74176224   0.62550002   0.38570005   0.52800006   0.05      ]
[37m[1m ...
[37m[1m [205.26173079   0.63609999   0.39920002   0.56809998   0.0587    ]
[37m[1m [285.39021423   0.56409997   0.42379999   0.47410002   0.15799999]
[37m[1m [123.54685835   0.62779999   0.40170002   0.4745       0.1089    ]]
[37m[1m[2023-06-25 07:56:57,407][129146] Max Reward on eval: 421.11469073945193
[37m[1m[2023-06-25 07:56:57,407][129146] Min Reward on eval: -43.59395524583524
[37m[1m[2023-06-25 07:56:57,408][129146] Mean Reward across all agents: 208.32869686144195
[37m[1m[2023-06-25 07:56:57,408][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:56:57,410][129146] mean_value=-114.43346312105888, max_value=434.5721015860626
[37m[1m[2023-06-25 07:56:57,413][129146] New mean coefficients: [[ 7.6497974   4.3699403  -0.54411185 10.192379   -0.06632833]]
[37m[1m[2023-06-25 07:56:57,414][129146] Moving the mean solution point...
[36m[2023-06-25 07:57:07,126][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 07:57:07,126][129146] FPS: 395458.54
[36m[2023-06-25 07:57:07,128][129146] itr=752, itrs=2000, Progress: 37.60%
[36m[2023-06-25 07:57:18,559][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 07:57:18,559][129146] FPS: 336570.71
[36m[2023-06-25 07:57:23,278][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:57:23,279][129146] Reward + Measures: [[455.64435213   0.61760801   0.34416533   0.49570864   0.16564266]]
[37m[1m[2023-06-25 07:57:23,279][129146] Max Reward on eval: 455.6443521326179
[37m[1m[2023-06-25 07:57:23,279][129146] Min Reward on eval: 455.6443521326179
[37m[1m[2023-06-25 07:57:23,279][129146] Mean Reward across all agents: 455.6443521326179
[37m[1m[2023-06-25 07:57:23,280][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:57:28,873][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:57:28,874][129146] Reward + Measures: [[230.20632586   0.61750001   0.37690002   0.58410001   0.0963    ]
[37m[1m [337.28803664   0.55940002   0.31879997   0.58579999   0.1098    ]
[37m[1m [281.93950498   0.59190005   0.36210001   0.59779996   0.0914    ]
[37m[1m ...
[37m[1m [243.35883629   0.58939999   0.373        0.58029997   0.09450001]
[37m[1m [139.43957953   0.58460003   0.3795       0.55680001   0.0873    ]
[37m[1m [197.63556175   0.58639997   0.37360001   0.58450001   0.0778    ]]
[37m[1m[2023-06-25 07:57:28,874][129146] Max Reward on eval: 423.3818117200164
[37m[1m[2023-06-25 07:57:28,874][129146] Min Reward on eval: 64.96825341287767
[37m[1m[2023-06-25 07:57:28,874][129146] Mean Reward across all agents: 250.75881379755933
[37m[1m[2023-06-25 07:57:28,875][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:57:28,880][129146] mean_value=56.122156035489496, max_value=694.0592611231978
[37m[1m[2023-06-25 07:57:28,882][129146] New mean coefficients: [[ 7.0074778  2.4643486 -0.9208179 10.63008   -1.0514494]]
[37m[1m[2023-06-25 07:57:28,884][129146] Moving the mean solution point...
[36m[2023-06-25 07:57:38,655][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 07:57:38,655][129146] FPS: 393064.92
[36m[2023-06-25 07:57:38,657][129146] itr=753, itrs=2000, Progress: 37.65%
[36m[2023-06-25 07:57:50,141][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 07:57:50,142][129146] FPS: 335000.45
[36m[2023-06-25 07:57:54,768][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:57:54,769][129146] Reward + Measures: [[477.18506148   0.6229682    0.34299323   0.50288689   0.1664117 ]]
[37m[1m[2023-06-25 07:57:54,769][129146] Max Reward on eval: 477.1850614812715
[37m[1m[2023-06-25 07:57:54,769][129146] Min Reward on eval: 477.1850614812715
[37m[1m[2023-06-25 07:57:54,770][129146] Mean Reward across all agents: 477.1850614812715
[37m[1m[2023-06-25 07:57:54,770][129146] Average Trajectory Length: 999.9399999999999
[36m[2023-06-25 07:58:00,187][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:58:00,188][129146] Reward + Measures: [[350.57923063   0.68240005   0.33880001   0.68760002   0.1269    ]
[37m[1m [386.98729884   0.73650002   0.2823       0.74450004   0.15000001]
[37m[1m [232.06760373   0.61390001   0.3132       0.49590001   0.28350002]
[37m[1m ...
[37m[1m [160.08250331   0.59450001   0.26070002   0.4973       0.23440002]
[37m[1m [466.2891237    0.74580002   0.22840002   0.75780004   0.2638    ]
[37m[1m [333.61050796   0.69680005   0.22239999   0.65270007   0.23740001]]
[37m[1m[2023-06-25 07:58:00,188][129146] Max Reward on eval: 522.9812585976673
[37m[1m[2023-06-25 07:58:00,188][129146] Min Reward on eval: -135.2454686646175
[37m[1m[2023-06-25 07:58:00,189][129146] Mean Reward across all agents: 335.53637370683214
[37m[1m[2023-06-25 07:58:00,189][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:58:00,195][129146] mean_value=170.21613345602637, max_value=856.0279469771062
[37m[1m[2023-06-25 07:58:00,198][129146] New mean coefficients: [[ 7.917874    2.8515246  -0.3776623  10.620392   -0.65043646]]
[37m[1m[2023-06-25 07:58:00,200][129146] Moving the mean solution point...
[36m[2023-06-25 07:58:09,905][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 07:58:09,905][129146] FPS: 395713.70
[36m[2023-06-25 07:58:09,908][129146] itr=754, itrs=2000, Progress: 37.70%
[36m[2023-06-25 07:58:21,445][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 07:58:21,445][129146] FPS: 333555.69
[36m[2023-06-25 07:58:26,192][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:58:26,193][129146] Reward + Measures: [[498.79779454   0.62543797   0.33641034   0.50831568   0.17174067]]
[37m[1m[2023-06-25 07:58:26,193][129146] Max Reward on eval: 498.7977945363784
[37m[1m[2023-06-25 07:58:26,193][129146] Min Reward on eval: 498.7977945363784
[37m[1m[2023-06-25 07:58:26,193][129146] Mean Reward across all agents: 498.7977945363784
[37m[1m[2023-06-25 07:58:26,194][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:58:31,607][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:58:31,608][129146] Reward + Measures: [[ 15.56367207   0.53600001   0.41880003   0.61019999   0.16250001]
[37m[1m [-24.04453472   0.66769999   0.6099       0.7238       0.3777    ]
[37m[1m [150.86978085   0.52060002   0.4664       0.61590004   0.16559999]
[37m[1m ...
[37m[1m [-25.15657467   0.66740006   0.63199997   0.7335       0.41710001]
[37m[1m [210.02538473   0.50800002   0.46090004   0.56210005   0.21529999]
[37m[1m [ 75.2596385    0.54070002   0.44430003   0.62760001   0.1389    ]]
[37m[1m[2023-06-25 07:58:31,608][129146] Max Reward on eval: 377.41153700326396
[37m[1m[2023-06-25 07:58:31,608][129146] Min Reward on eval: -158.0476886783552
[37m[1m[2023-06-25 07:58:31,609][129146] Mean Reward across all agents: 102.30986451019022
[37m[1m[2023-06-25 07:58:31,609][129146] Average Trajectory Length: 999.7806666666667
[36m[2023-06-25 07:58:31,614][129146] mean_value=251.00323399917158, max_value=775.973917713035
[37m[1m[2023-06-25 07:58:31,617][129146] New mean coefficients: [[ 7.1984043   2.6033814  -1.4878523  11.702181   -0.52074814]]
[37m[1m[2023-06-25 07:58:31,618][129146] Moving the mean solution point...
[36m[2023-06-25 07:58:41,428][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 07:58:41,428][129146] FPS: 391510.54
[36m[2023-06-25 07:58:41,430][129146] itr=755, itrs=2000, Progress: 37.75%
[36m[2023-06-25 07:58:52,865][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 07:58:52,865][129146] FPS: 336450.68
[36m[2023-06-25 07:58:57,646][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:58:57,646][129146] Reward + Measures: [[503.68231988   0.63751495   0.33461201   0.53029037   0.16474833]]
[37m[1m[2023-06-25 07:58:57,647][129146] Max Reward on eval: 503.6823198773043
[37m[1m[2023-06-25 07:58:57,647][129146] Min Reward on eval: 503.6823198773043
[37m[1m[2023-06-25 07:58:57,647][129146] Mean Reward across all agents: 503.6823198773043
[37m[1m[2023-06-25 07:58:57,648][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:59:03,073][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:59:03,074][129146] Reward + Measures: [[278.18187669   0.64230007   0.28260002   0.56409997   0.11889999]
[37m[1m [246.40800599   0.68679994   0.31480002   0.59750003   0.07899999]
[37m[1m [357.19967015   0.62379998   0.28580001   0.53120005   0.1362    ]
[37m[1m ...
[37m[1m [176.91900933   0.71640003   0.35980001   0.66600001   0.0455    ]
[37m[1m [150.70746894   0.70070004   0.38569999   0.63309997   0.0398    ]
[37m[1m [ -5.52878859   0.69890004   0.48109999   0.64829999   0.0575    ]]
[37m[1m[2023-06-25 07:59:03,074][129146] Max Reward on eval: 433.5849188750493
[37m[1m[2023-06-25 07:59:03,074][129146] Min Reward on eval: -182.11759430462263
[37m[1m[2023-06-25 07:59:03,075][129146] Mean Reward across all agents: 181.68677215218034
[37m[1m[2023-06-25 07:59:03,075][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:59:03,078][129146] mean_value=-5.171689157196536, max_value=450.731936873729
[37m[1m[2023-06-25 07:59:03,081][129146] New mean coefficients: [[ 7.404303   2.5138278 -2.1868691 12.080386  -1.0495955]]
[37m[1m[2023-06-25 07:59:03,082][129146] Moving the mean solution point...
[36m[2023-06-25 07:59:12,856][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 07:59:12,857][129146] FPS: 392928.83
[36m[2023-06-25 07:59:12,859][129146] itr=756, itrs=2000, Progress: 37.80%
[36m[2023-06-25 07:59:24,318][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 07:59:24,318][129146] FPS: 335722.05
[36m[2023-06-25 07:59:28,955][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:59:28,956][129146] Reward + Measures: [[509.52448351   0.64064401   0.33909032   0.54524964   0.15777032]]
[37m[1m[2023-06-25 07:59:28,956][129146] Max Reward on eval: 509.5244835063407
[37m[1m[2023-06-25 07:59:28,956][129146] Min Reward on eval: 509.5244835063407
[37m[1m[2023-06-25 07:59:28,957][129146] Mean Reward across all agents: 509.5244835063407
[37m[1m[2023-06-25 07:59:28,957][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:59:34,412][129146] Finished Evaluation Step
[37m[1m[2023-06-25 07:59:34,413][129146] Reward + Measures: [[208.94069028   0.72710007   0.198        0.68970007   0.1962    ]
[37m[1m [140.7966983    0.63300002   0.14030001   0.63790005   0.20539999]
[37m[1m [329.80334062   0.67000002   0.42399999   0.65970004   0.1035    ]
[37m[1m ...
[37m[1m [125.06774135   0.64200002   0.32749996   0.67200005   0.1604    ]
[37m[1m [238.03507731   0.60510004   0.24880002   0.52460003   0.13700001]
[37m[1m [349.76764589   0.66420007   0.35390002   0.61669999   0.1014    ]]
[37m[1m[2023-06-25 07:59:34,413][129146] Max Reward on eval: 378.90022864222993
[37m[1m[2023-06-25 07:59:34,413][129146] Min Reward on eval: -148.31101531999883
[37m[1m[2023-06-25 07:59:34,413][129146] Mean Reward across all agents: 188.03343388862046
[37m[1m[2023-06-25 07:59:34,414][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 07:59:34,418][129146] mean_value=-6.976648638123782, max_value=680.421133879692
[37m[1m[2023-06-25 07:59:34,420][129146] New mean coefficients: [[ 7.4998636  2.1612773 -2.8923655 12.278298  -1.9986879]]
[37m[1m[2023-06-25 07:59:34,421][129146] Moving the mean solution point...
[36m[2023-06-25 07:59:44,150][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 07:59:44,151][129146] FPS: 394754.64
[36m[2023-06-25 07:59:44,153][129146] itr=757, itrs=2000, Progress: 37.85%
[36m[2023-06-25 07:59:55,589][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 07:59:55,589][129146] FPS: 336518.24
[36m[2023-06-25 08:00:00,327][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:00:00,327][129146] Reward + Measures: [[522.0757507    0.64426303   0.34159002   0.56165236   0.15055233]]
[37m[1m[2023-06-25 08:00:00,328][129146] Max Reward on eval: 522.0757507001895
[37m[1m[2023-06-25 08:00:00,328][129146] Min Reward on eval: 522.0757507001895
[37m[1m[2023-06-25 08:00:00,328][129146] Mean Reward across all agents: 522.0757507001895
[37m[1m[2023-06-25 08:00:00,328][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:00:05,983][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:00:05,984][129146] Reward + Measures: [[  66.10126411    0.6451        0.28800002    0.56919998    0.1239    ]
[37m[1m [  22.62349869    0.68660003    0.45790002    0.58639997    0.0094    ]
[37m[1m [ 284.69472076    0.65210003    0.2579        0.616         0.14120001]
[37m[1m ...
[37m[1m [-108.35233477    0.38320002    0.2086        0.28959998    0.0959    ]
[37m[1m [ 334.01032517    0.56949997    0.26820001    0.45690003    0.15620001]
[37m[1m [ -45.09744045    0.47119999    0.20289998    0.36069998    0.1189    ]]
[37m[1m[2023-06-25 08:00:05,984][129146] Min Reward on eval: -340.9857180411462
[37m[1m[2023-06-25 08:00:05,984][129146] Mean Reward across all agents: 209.051116501637
[37m[1m[2023-06-25 08:00:05,985][129146] Average Trajectory Length: 999.6179999999999
[36m[2023-06-25 08:00:05,987][129146] mean_value=-156.88183537906693, max_value=447.435996162612
[37m[1m[2023-06-25 08:00:05,990][129146] New mean coefficients: [[ 7.4780965  2.1189222 -3.1369162 11.634682  -1.8162088]]
[37m[1m[2023-06-25 08:00:05,991][129146] Moving the mean solution point...
[36m[2023-06-25 08:00:15,726][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 08:00:15,727][129146] FPS: 394494.31
[36m[2023-06-25 08:00:15,729][129146] itr=758, itrs=2000, Progress: 37.90%
[36m[2023-06-25 08:00:27,310][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 08:00:27,310][129146] FPS: 332198.58
[36m[2023-06-25 08:00:32,157][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:00:32,158][129146] Reward + Measures: [[536.76200067   0.64887929   0.33938065   0.58248901   0.14055499]]
[37m[1m[2023-06-25 08:00:32,158][129146] Max Reward on eval: 536.7620006718714
[37m[1m[2023-06-25 08:00:32,158][129146] Min Reward on eval: 536.7620006718714
[37m[1m[2023-06-25 08:00:32,159][129146] Mean Reward across all agents: 536.7620006718714
[37m[1m[2023-06-25 08:00:32,159][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:00:37,644][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:00:37,645][129146] Reward + Measures: [[364.00499917   0.65040004   0.36520001   0.63980001   0.09320001]
[37m[1m [271.73202556   0.65679997   0.36500001   0.64089996   0.08      ]
[37m[1m [256.40802869   0.58490008   0.38960001   0.59960002   0.0838    ]
[37m[1m ...
[37m[1m [283.80920699   0.58689994   0.38690001   0.60060006   0.08060001]
[37m[1m [289.13150503   0.56159997   0.43080002   0.6002       0.0997    ]
[37m[1m [223.29599165   0.59860003   0.38699996   0.62169999   0.07539999]]
[37m[1m[2023-06-25 08:00:37,645][129146] Max Reward on eval: 391.8977306167129
[37m[1m[2023-06-25 08:00:37,645][129146] Min Reward on eval: 192.6689100105199
[37m[1m[2023-06-25 08:00:37,646][129146] Mean Reward across all agents: 289.09287073831996
[37m[1m[2023-06-25 08:00:37,646][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:00:37,650][129146] mean_value=37.925650303392125, max_value=302.35894165540105
[37m[1m[2023-06-25 08:00:37,653][129146] New mean coefficients: [[ 7.0643845  5.232862  -3.5441585 12.890258  -2.0158072]]
[37m[1m[2023-06-25 08:00:37,654][129146] Moving the mean solution point...
[36m[2023-06-25 08:00:47,499][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 08:00:47,499][129146] FPS: 390106.29
[36m[2023-06-25 08:00:47,501][129146] itr=759, itrs=2000, Progress: 37.95%
[36m[2023-06-25 08:00:59,082][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 08:00:59,082][129146] FPS: 332251.51
[36m[2023-06-25 08:01:03,910][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:01:03,910][129146] Reward + Measures: [[524.77482292   0.6582123    0.33505765   0.60821664   0.12917566]]
[37m[1m[2023-06-25 08:01:03,910][129146] Max Reward on eval: 524.7748229213405
[37m[1m[2023-06-25 08:01:03,911][129146] Min Reward on eval: 524.7748229213405
[37m[1m[2023-06-25 08:01:03,911][129146] Mean Reward across all agents: 524.7748229213405
[37m[1m[2023-06-25 08:01:03,911][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:01:09,393][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:01:09,394][129146] Reward + Measures: [[224.27453346   0.59440005   0.4605       0.68399996   0.1168    ]
[37m[1m [174.06107724   0.61390001   0.3741       0.69710004   0.035     ]
[37m[1m [133.04418608   0.62720007   0.33319998   0.65510005   0.0323    ]
[37m[1m ...
[37m[1m [171.46859335   0.61739999   0.38929999   0.67590004   0.1008    ]
[37m[1m [255.36749949   0.58330005   0.43619996   0.67470002   0.1432    ]
[37m[1m [191.8928998    0.64040005   0.42460003   0.69920003   0.1331    ]]
[37m[1m[2023-06-25 08:01:09,394][129146] Max Reward on eval: 283.11143982990177
[37m[1m[2023-06-25 08:01:09,394][129146] Min Reward on eval: 37.60296747823086
[37m[1m[2023-06-25 08:01:09,395][129146] Mean Reward across all agents: 193.11780888701318
[37m[1m[2023-06-25 08:01:09,395][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:01:09,398][129146] mean_value=-20.70671128587561, max_value=633.0441860793275
[37m[1m[2023-06-25 08:01:09,400][129146] New mean coefficients: [[ 7.2991505  5.8674617 -2.292728  12.787266  -2.142253 ]]
[37m[1m[2023-06-25 08:01:09,401][129146] Moving the mean solution point...
[36m[2023-06-25 08:01:19,243][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 08:01:19,243][129146] FPS: 390234.09
[36m[2023-06-25 08:01:19,245][129146] itr=760, itrs=2000, Progress: 38.00%
[37m[1m[2023-06-25 08:01:25,689][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000740
[36m[2023-06-25 08:01:37,425][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 08:01:37,425][129146] FPS: 334085.40
[36m[2023-06-25 08:01:42,145][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:01:42,145][129146] Reward + Measures: [[514.17431865   0.66029668   0.33631632   0.62939203   0.12075967]]
[37m[1m[2023-06-25 08:01:42,145][129146] Max Reward on eval: 514.1743186509088
[37m[1m[2023-06-25 08:01:42,146][129146] Min Reward on eval: 514.1743186509088
[37m[1m[2023-06-25 08:01:42,146][129146] Mean Reward across all agents: 514.1743186509088
[37m[1m[2023-06-25 08:01:42,146][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:01:47,595][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:01:47,601][129146] Reward + Measures: [[281.11194102   0.58400005   0.44580004   0.61260003   0.20279999]
[37m[1m [270.00013452   0.61260003   0.3892       0.6886       0.0543    ]
[37m[1m [268.22481898   0.56240004   0.41870004   0.66799998   0.13700001]
[37m[1m ...
[37m[1m [282.35927863   0.62530005   0.42309999   0.625        0.22029999]
[37m[1m [205.09398709   0.62480003   0.3633       0.72909993   0.07390001]
[37m[1m [354.44565211   0.58680004   0.42630002   0.7141       0.1239    ]]
[37m[1m[2023-06-25 08:01:47,601][129146] Max Reward on eval: 404.6098235320416
[37m[1m[2023-06-25 08:01:47,601][129146] Min Reward on eval: -113.34526177820517
[37m[1m[2023-06-25 08:01:47,602][129146] Mean Reward across all agents: 287.2050154590668
[37m[1m[2023-06-25 08:01:47,602][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:01:47,609][129146] mean_value=249.35638398807097, max_value=880.2508243504271
[37m[1m[2023-06-25 08:01:47,612][129146] New mean coefficients: [[ 7.775843    5.816749   -1.9619443  13.067007   -0.83939576]]
[37m[1m[2023-06-25 08:01:47,613][129146] Moving the mean solution point...
[36m[2023-06-25 08:01:57,372][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 08:01:57,372][129146] FPS: 393537.76
[36m[2023-06-25 08:01:57,375][129146] itr=761, itrs=2000, Progress: 38.05%
[36m[2023-06-25 08:02:08,997][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 08:02:08,997][129146] FPS: 331112.01
[36m[2023-06-25 08:02:13,799][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:02:13,799][129146] Reward + Measures: [[518.0122026    0.66536969   0.33504501   0.64356804   0.10930867]]
[37m[1m[2023-06-25 08:02:13,800][129146] Max Reward on eval: 518.0122026022228
[37m[1m[2023-06-25 08:02:13,800][129146] Min Reward on eval: 518.0122026022228
[37m[1m[2023-06-25 08:02:13,800][129146] Mean Reward across all agents: 518.0122026022228
[37m[1m[2023-06-25 08:02:13,800][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:02:19,374][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:02:19,374][129146] Reward + Measures: [[390.21674121   0.67640001   0.3608       0.59009999   0.2129    ]
[37m[1m [239.10003125   0.61700004   0.35590002   0.6182       0.14579999]
[37m[1m [372.85939098   0.63530004   0.36349997   0.6232       0.13100001]
[37m[1m ...
[37m[1m [285.27235094   0.57489997   0.43959999   0.55509996   0.19560002]
[37m[1m [348.62752085   0.6469       0.42190003   0.55409998   0.2077    ]
[37m[1m [301.38920338   0.62459993   0.34520003   0.6401       0.0857    ]]
[37m[1m[2023-06-25 08:02:19,374][129146] Max Reward on eval: 454.3069779234356
[37m[1m[2023-06-25 08:02:19,375][129146] Min Reward on eval: 32.6647442194866
[37m[1m[2023-06-25 08:02:19,375][129146] Mean Reward across all agents: 315.3286190445344
[37m[1m[2023-06-25 08:02:19,375][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:02:19,379][129146] mean_value=-32.46591341217634, max_value=294.36216487180644
[37m[1m[2023-06-25 08:02:19,381][129146] New mean coefficients: [[ 8.75337    4.403772  -1.3650451 12.003742  -0.951116 ]]
[37m[1m[2023-06-25 08:02:19,382][129146] Moving the mean solution point...
[36m[2023-06-25 08:02:29,029][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 08:02:29,029][129146] FPS: 398137.39
[36m[2023-06-25 08:02:29,031][129146] itr=762, itrs=2000, Progress: 38.10%
[36m[2023-06-25 08:02:40,594][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 08:02:40,595][129146] FPS: 332814.46
[36m[2023-06-25 08:02:45,423][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:02:45,423][129146] Reward + Measures: [[548.74926977   0.66989368   0.34088996   0.65698934   0.10247133]]
[37m[1m[2023-06-25 08:02:45,423][129146] Max Reward on eval: 548.7492697689329
[37m[1m[2023-06-25 08:02:45,423][129146] Min Reward on eval: 548.7492697689329
[37m[1m[2023-06-25 08:02:45,424][129146] Mean Reward across all agents: 548.7492697689329
[37m[1m[2023-06-25 08:02:45,424][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:02:50,860][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:02:50,861][129146] Reward + Measures: [[327.59384245   0.71939999   0.4059       0.75599998   0.1822    ]
[37m[1m [130.91964575   0.60110003   0.26300001   0.64469999   0.13600002]
[37m[1m [439.45904985   0.66620004   0.48880002   0.70670003   0.23020001]
[37m[1m ...
[37m[1m [174.88416513   0.76379997   0.27970001   0.77879995   0.1988    ]
[37m[1m [343.49179918   0.70950001   0.43519998   0.6785       0.13780001]
[37m[1m [347.98106049   0.74580002   0.4804       0.70380002   0.1982    ]]
[37m[1m[2023-06-25 08:02:50,861][129146] Max Reward on eval: 439.45904984982917
[37m[1m[2023-06-25 08:02:50,861][129146] Min Reward on eval: -19.68052199644735
[37m[1m[2023-06-25 08:02:50,861][129146] Mean Reward across all agents: 230.87937207644964
[37m[1m[2023-06-25 08:02:50,862][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:02:50,866][129146] mean_value=52.474838700151665, max_value=850.5272650035098
[37m[1m[2023-06-25 08:02:50,869][129146] New mean coefficients: [[ 9.203327   2.1974006 -0.8196756 11.496463  -1.8314955]]
[37m[1m[2023-06-25 08:02:50,870][129146] Moving the mean solution point...
[36m[2023-06-25 08:03:00,555][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 08:03:00,555][129146] FPS: 396574.98
[36m[2023-06-25 08:03:00,557][129146] itr=763, itrs=2000, Progress: 38.15%
[36m[2023-06-25 08:03:12,046][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 08:03:12,046][129146] FPS: 334894.00
[36m[2023-06-25 08:03:16,798][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:03:16,799][129146] Reward + Measures: [[562.49762173   0.670371     0.34494001   0.66265398   0.09328066]]
[37m[1m[2023-06-25 08:03:16,799][129146] Max Reward on eval: 562.4976217340111
[37m[1m[2023-06-25 08:03:16,799][129146] Min Reward on eval: 562.4976217340111
[37m[1m[2023-06-25 08:03:16,800][129146] Mean Reward across all agents: 562.4976217340111
[37m[1m[2023-06-25 08:03:16,800][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:03:22,313][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:03:22,314][129146] Reward + Measures: [[393.81107225   0.65109998   0.4147       0.60140002   0.2192    ]
[37m[1m [ 60.48285261   0.64180005   0.20969999   0.67510003   0.24250002]
[37m[1m [ 70.68976274   0.57300007   0.21510001   0.61300004   0.19059999]
[37m[1m ...
[37m[1m [277.45764404   0.60030001   0.29920003   0.62220001   0.1285    ]
[37m[1m [417.45517261   0.66169995   0.38049999   0.65920007   0.1505    ]
[37m[1m [329.92793767   0.62770003   0.3398       0.67460001   0.14420001]]
[37m[1m[2023-06-25 08:03:22,314][129146] Max Reward on eval: 518.4492591252958
[37m[1m[2023-06-25 08:03:22,314][129146] Min Reward on eval: -219.65423550683772
[37m[1m[2023-06-25 08:03:22,315][129146] Mean Reward across all agents: 219.45947097559366
[37m[1m[2023-06-25 08:03:22,315][129146] Average Trajectory Length: 999.3743333333333
[36m[2023-06-25 08:03:22,318][129146] mean_value=-39.05151974487391, max_value=555.7909183225827
[37m[1m[2023-06-25 08:03:22,321][129146] New mean coefficients: [[ 9.12104    1.6506145 -1.9221635 12.083795  -2.2306767]]
[37m[1m[2023-06-25 08:03:22,322][129146] Moving the mean solution point...
[36m[2023-06-25 08:03:32,149][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 08:03:32,149][129146] FPS: 390816.02
[36m[2023-06-25 08:03:32,152][129146] itr=764, itrs=2000, Progress: 38.20%
[36m[2023-06-25 08:03:43,694][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 08:03:43,694][129146] FPS: 333345.60
[36m[2023-06-25 08:03:48,535][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:03:48,536][129146] Reward + Measures: [[578.43180622   0.67360002   0.34688133   0.66695863   0.09750099]]
[37m[1m[2023-06-25 08:03:48,536][129146] Max Reward on eval: 578.4318062160197
[37m[1m[2023-06-25 08:03:48,536][129146] Min Reward on eval: 578.4318062160197
[37m[1m[2023-06-25 08:03:48,537][129146] Mean Reward across all agents: 578.4318062160197
[37m[1m[2023-06-25 08:03:48,537][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:03:54,052][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:03:54,052][129146] Reward + Measures: [[391.82088903   0.6038       0.4003       0.61339992   0.0824    ]
[37m[1m [343.62027613   0.56699997   0.35789999   0.5848       0.12030001]
[37m[1m [443.1426716    0.68040001   0.31690001   0.62260002   0.1227    ]
[37m[1m ...
[37m[1m [394.47324759   0.59709996   0.34489998   0.63970006   0.0726    ]
[37m[1m [450.94346183   0.5948       0.32500002   0.58700001   0.0896    ]
[37m[1m [412.95691467   0.61530006   0.29080001   0.5927       0.1523    ]]
[37m[1m[2023-06-25 08:03:54,052][129146] Max Reward on eval: 488.12389321515803
[37m[1m[2023-06-25 08:03:54,053][129146] Min Reward on eval: 106.95346451426158
[37m[1m[2023-06-25 08:03:54,053][129146] Mean Reward across all agents: 368.3031924847037
[37m[1m[2023-06-25 08:03:54,053][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:03:54,057][129146] mean_value=9.91261000221882, max_value=311.9565469075427
[37m[1m[2023-06-25 08:03:54,060][129146] New mean coefficients: [[ 9.265685  -1.1457143 -0.9256499 11.072195  -1.6706228]]
[37m[1m[2023-06-25 08:03:54,061][129146] Moving the mean solution point...
[36m[2023-06-25 08:04:03,794][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 08:04:03,794][129146] FPS: 394606.67
[36m[2023-06-25 08:04:03,796][129146] itr=765, itrs=2000, Progress: 38.25%
[36m[2023-06-25 08:04:15,298][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 08:04:15,299][129146] FPS: 334489.06
[36m[2023-06-25 08:04:20,055][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:04:20,055][129146] Reward + Measures: [[611.47448599   0.67904294   0.33799464   0.66749871   0.09351266]]
[37m[1m[2023-06-25 08:04:20,055][129146] Max Reward on eval: 611.4744859874197
[37m[1m[2023-06-25 08:04:20,056][129146] Min Reward on eval: 611.4744859874197
[37m[1m[2023-06-25 08:04:20,056][129146] Mean Reward across all agents: 611.4744859874197
[37m[1m[2023-06-25 08:04:20,056][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:04:25,492][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:04:25,498][129146] Reward + Measures: [[151.90929721   0.70310003   0.52950001   0.66589999   0.0289    ]
[37m[1m [284.2193639    0.64680004   0.46149999   0.71709996   0.0618    ]
[37m[1m [235.99591662   0.62099999   0.43120003   0.68529999   0.0977    ]
[37m[1m ...
[37m[1m [233.90622449   0.67180008   0.4736       0.69050002   0.0567    ]
[37m[1m [303.00946428   0.63850003   0.34610003   0.56800002   0.1513    ]
[37m[1m [301.86960275   0.67720002   0.35010001   0.65979999   0.1194    ]]
[37m[1m[2023-06-25 08:04:25,498][129146] Max Reward on eval: 461.3068943383114
[37m[1m[2023-06-25 08:04:25,499][129146] Min Reward on eval: 5.517828595405445
[37m[1m[2023-06-25 08:04:25,499][129146] Mean Reward across all agents: 247.12328822058183
[37m[1m[2023-06-25 08:04:25,499][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:04:25,502][129146] mean_value=112.01563807280642, max_value=728.9852253984776
[37m[1m[2023-06-25 08:04:25,505][129146] New mean coefficients: [[ 9.605685  -2.3869872 -1.6651503 10.072021  -2.4503114]]
[37m[1m[2023-06-25 08:04:25,506][129146] Moving the mean solution point...
[36m[2023-06-25 08:04:35,322][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 08:04:35,323][129146] FPS: 391246.34
[36m[2023-06-25 08:04:35,325][129146] itr=766, itrs=2000, Progress: 38.30%
[36m[2023-06-25 08:04:46,917][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 08:04:46,917][129146] FPS: 331871.92
[36m[2023-06-25 08:04:51,720][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:04:51,720][129146] Reward + Measures: [[670.77797701   0.69029701   0.34066498   0.69053501   0.09253666]]
[37m[1m[2023-06-25 08:04:51,721][129146] Max Reward on eval: 670.7779770105669
[37m[1m[2023-06-25 08:04:51,721][129146] Min Reward on eval: 670.7779770105669
[37m[1m[2023-06-25 08:04:51,721][129146] Mean Reward across all agents: 670.7779770105669
[37m[1m[2023-06-25 08:04:51,721][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:04:57,247][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:04:57,247][129146] Reward + Measures: [[326.69555796   0.65959996   0.36390001   0.6171       0.0783    ]
[37m[1m [307.96997125   0.6627       0.39750001   0.69580001   0.0488    ]
[37m[1m [141.78729161   0.667        0.35840002   0.61949998   0.0506    ]
[37m[1m ...
[37m[1m [336.85182142   0.68080008   0.38849998   0.68340003   0.0674    ]
[37m[1m [234.69495743   0.6498       0.41700003   0.73550004   0.025     ]
[37m[1m [306.76298644   0.61860007   0.33950001   0.57170004   0.15650001]]
[37m[1m[2023-06-25 08:04:57,247][129146] Max Reward on eval: 461.4689582208637
[37m[1m[2023-06-25 08:04:57,248][129146] Min Reward on eval: -370.51279777514867
[37m[1m[2023-06-25 08:04:57,248][129146] Mean Reward across all agents: 231.9323775940762
[37m[1m[2023-06-25 08:04:57,248][129146] Average Trajectory Length: 999.4359999999999
[36m[2023-06-25 08:04:57,250][129146] mean_value=-223.77708234931595, max_value=736.6680334533361
[37m[1m[2023-06-25 08:04:57,253][129146] New mean coefficients: [[ 9.900872  -1.5685978 -1.246578   8.8137665 -1.2192562]]
[37m[1m[2023-06-25 08:04:57,254][129146] Moving the mean solution point...
[36m[2023-06-25 08:05:06,939][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 08:05:06,940][129146] FPS: 396522.32
[36m[2023-06-25 08:05:06,942][129146] itr=767, itrs=2000, Progress: 38.35%
[36m[2023-06-25 08:05:18,380][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 08:05:18,380][129146] FPS: 336465.54
[36m[2023-06-25 08:05:23,163][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:05:23,163][129146] Reward + Measures: [[692.10782557   0.68534368   0.32562232   0.69527161   0.08937933]]
[37m[1m[2023-06-25 08:05:23,163][129146] Max Reward on eval: 692.1078255719848
[37m[1m[2023-06-25 08:05:23,164][129146] Min Reward on eval: 692.1078255719848
[37m[1m[2023-06-25 08:05:23,164][129146] Mean Reward across all agents: 692.1078255719848
[37m[1m[2023-06-25 08:05:23,164][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:05:28,687][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:05:28,687][129146] Reward + Measures: [[163.3534267    0.63169998   0.38350001   0.80260003   0.0587    ]
[37m[1m [265.42732169   0.61869997   0.30020002   0.67819995   0.12750001]
[37m[1m [155.39829675   0.59219998   0.27129999   0.67690009   0.1628    ]
[37m[1m ...
[37m[1m [314.4043331    0.61449999   0.30739999   0.7058       0.1478    ]
[37m[1m [362.67781628   0.63330001   0.4535       0.72259998   0.0926    ]
[37m[1m [425.3460736    0.62970001   0.36600003   0.75459999   0.0767    ]]
[37m[1m[2023-06-25 08:05:28,687][129146] Max Reward on eval: 526.0062756753119
[37m[1m[2023-06-25 08:05:28,688][129146] Min Reward on eval: -101.75146221064497
[37m[1m[2023-06-25 08:05:28,688][129146] Mean Reward across all agents: 314.20721277968426
[37m[1m[2023-06-25 08:05:28,688][129146] Average Trajectory Length: 999.4399999999999
[36m[2023-06-25 08:05:28,693][129146] mean_value=136.6256496522306, max_value=772.6690994542092
[37m[1m[2023-06-25 08:05:28,695][129146] New mean coefficients: [[ 9.892612  -3.6100197 -1.2540613  8.000739  -1.5569513]]
[37m[1m[2023-06-25 08:05:28,696][129146] Moving the mean solution point...
[36m[2023-06-25 08:05:38,436][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 08:05:38,436][129146] FPS: 394353.40
[36m[2023-06-25 08:05:38,438][129146] itr=768, itrs=2000, Progress: 38.40%
[36m[2023-06-25 08:05:49,956][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 08:05:49,956][129146] FPS: 334115.41
[36m[2023-06-25 08:05:54,639][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:05:54,639][129146] Reward + Measures: [[716.03730137   0.68267173   0.32386699   0.69563901   0.08762333]]
[37m[1m[2023-06-25 08:05:54,640][129146] Max Reward on eval: 716.0373013737138
[37m[1m[2023-06-25 08:05:54,640][129146] Min Reward on eval: 716.0373013737138
[37m[1m[2023-06-25 08:05:54,640][129146] Mean Reward across all agents: 716.0373013737138
[37m[1m[2023-06-25 08:05:54,640][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:06:00,157][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:06:00,158][129146] Reward + Measures: [[490.72527152   0.70319998   0.32080004   0.77780002   0.0619    ]
[37m[1m [524.51317481   0.68089998   0.3337       0.7396       0.0674    ]
[37m[1m [420.76711637   0.69100004   0.37310001   0.7802       0.027     ]
[37m[1m ...
[37m[1m [434.94201979   0.68279999   0.4032       0.74169999   0.0088    ]
[37m[1m [449.81332106   0.70210004   0.3391       0.73310006   0.0432    ]
[37m[1m [585.02079326   0.71470004   0.33680004   0.74480009   0.0369    ]]
[37m[1m[2023-06-25 08:06:00,158][129146] Max Reward on eval: 692.3519847550313
[37m[1m[2023-06-25 08:06:00,158][129146] Min Reward on eval: 329.9796070034266
[37m[1m[2023-06-25 08:06:00,159][129146] Mean Reward across all agents: 497.09545024415553
[37m[1m[2023-06-25 08:06:00,159][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:06:00,166][129146] mean_value=376.2263432551971, max_value=887.2064431650098
[37m[1m[2023-06-25 08:06:00,169][129146] New mean coefficients: [[ 9.206209   -3.5939994  -0.83884084  7.8665137  -1.4126052 ]]
[37m[1m[2023-06-25 08:06:00,170][129146] Moving the mean solution point...
[36m[2023-06-25 08:06:09,878][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 08:06:09,878][129146] FPS: 395627.63
[36m[2023-06-25 08:06:09,880][129146] itr=769, itrs=2000, Progress: 38.45%
[36m[2023-06-25 08:06:21,460][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 08:06:21,460][129146] FPS: 332235.28
[36m[2023-06-25 08:06:26,304][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:06:26,304][129146] Reward + Measures: [[758.7348416    0.67597032   0.31655833   0.68595862   0.09109399]]
[37m[1m[2023-06-25 08:06:26,304][129146] Max Reward on eval: 758.7348416031494
[37m[1m[2023-06-25 08:06:26,305][129146] Min Reward on eval: 758.7348416031494
[37m[1m[2023-06-25 08:06:26,305][129146] Mean Reward across all agents: 758.7348416031494
[37m[1m[2023-06-25 08:06:26,305][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:06:31,791][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:06:31,792][129146] Reward + Measures: [[494.7549974    0.67690003   0.41560003   0.75260001   0.064     ]
[37m[1m [362.44211915   0.56669998   0.42360002   0.70520008   0.19499998]
[37m[1m [210.47107636   0.76560003   0.51379997   0.78109998   0.032     ]
[37m[1m ...
[37m[1m [100.28661512   0.50640005   0.34590003   0.57650006   0.1452    ]
[37m[1m [513.05388711   0.70910001   0.3793       0.76169997   0.07210001]
[37m[1m [467.23627666   0.70319998   0.43710002   0.741        0.125     ]]
[37m[1m[2023-06-25 08:06:31,792][129146] Max Reward on eval: 608.042376350332
[37m[1m[2023-06-25 08:06:31,792][129146] Min Reward on eval: -506.3205773866677
[37m[1m[2023-06-25 08:06:31,792][129146] Mean Reward across all agents: 277.79176928332356
[37m[1m[2023-06-25 08:06:31,793][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:06:31,799][129146] mean_value=135.06318764408672, max_value=871.0488627882926
[37m[1m[2023-06-25 08:06:31,801][129146] New mean coefficients: [[ 9.422625  -2.864968  -1.0571609  7.256191  -1.286911 ]]
[37m[1m[2023-06-25 08:06:31,802][129146] Moving the mean solution point...
[36m[2023-06-25 08:06:41,589][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 08:06:41,589][129146] FPS: 392448.62
[36m[2023-06-25 08:06:41,591][129146] itr=770, itrs=2000, Progress: 38.50%
[37m[1m[2023-06-25 08:06:47,810][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000750
[36m[2023-06-25 08:06:59,423][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 08:06:59,424][129146] FPS: 337195.96
[36m[2023-06-25 08:07:04,106][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:07:04,106][129146] Reward + Measures: [[788.75861528   0.67252129   0.31768832   0.68616164   0.100819  ]]
[37m[1m[2023-06-25 08:07:04,106][129146] Max Reward on eval: 788.7586152822915
[37m[1m[2023-06-25 08:07:04,106][129146] Min Reward on eval: 788.7586152822915
[37m[1m[2023-06-25 08:07:04,107][129146] Mean Reward across all agents: 788.7586152822915
[37m[1m[2023-06-25 08:07:04,107][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:07:09,595][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:07:09,600][129146] Reward + Measures: [[632.5497842    0.67229998   0.4233       0.73079997   0.05950001]
[37m[1m [476.77599582   0.62480003   0.3127       0.63959998   0.18499999]
[37m[1m [636.50205965   0.68230003   0.396        0.70069999   0.0692    ]
[37m[1m ...
[37m[1m [527.05595926   0.64469999   0.39850003   0.71429998   0.1132    ]
[37m[1m [602.81566646   0.66810006   0.43070003   0.72930002   0.0582    ]
[37m[1m [487.74362984   0.61449999   0.38550001   0.63949996   0.1269    ]]
[37m[1m[2023-06-25 08:07:09,601][129146] Max Reward on eval: 686.0290261526941
[37m[1m[2023-06-25 08:07:09,601][129146] Min Reward on eval: 350.7678196977475
[37m[1m[2023-06-25 08:07:09,601][129146] Mean Reward across all agents: 534.7182114233607
[37m[1m[2023-06-25 08:07:09,601][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:07:09,606][129146] mean_value=33.77717447764059, max_value=701.2198144604151
[37m[1m[2023-06-25 08:07:09,609][129146] New mean coefficients: [[ 8.853689   -1.1344672  -0.9059448   6.6596217  -0.41630244]]
[37m[1m[2023-06-25 08:07:09,610][129146] Moving the mean solution point...
[36m[2023-06-25 08:07:19,222][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 08:07:19,222][129146] FPS: 399586.94
[36m[2023-06-25 08:07:19,224][129146] itr=771, itrs=2000, Progress: 38.55%
[36m[2023-06-25 08:07:30,591][129146] train() took 11.35 seconds to complete
[36m[2023-06-25 08:07:30,591][129146] FPS: 338454.36
[36m[2023-06-25 08:07:35,538][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:07:35,538][129146] Reward + Measures: [[831.11864248   0.67100269   0.31154001   0.68482268   0.10086367]]
[37m[1m[2023-06-25 08:07:35,538][129146] Max Reward on eval: 831.1186424814093
[37m[1m[2023-06-25 08:07:35,539][129146] Min Reward on eval: 831.1186424814093
[37m[1m[2023-06-25 08:07:35,539][129146] Mean Reward across all agents: 831.1186424814093
[37m[1m[2023-06-25 08:07:35,539][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:07:40,931][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:07:40,932][129146] Reward + Measures: [[469.3348725    0.6814       0.47750002   0.76069999   0.017     ]
[37m[1m [462.29715258   0.6415       0.46870002   0.78460002   0.06940001]
[37m[1m [557.23877573   0.70320004   0.42130002   0.75710005   0.0391    ]
[37m[1m ...
[37m[1m [246.84680138   0.66790003   0.60210007   0.81840003   0.0525    ]
[37m[1m [284.75028287   0.67220002   0.47960001   0.75999993   0.0144    ]
[37m[1m [443.33019838   0.69230002   0.4993       0.8075       0.0922    ]]
[37m[1m[2023-06-25 08:07:40,932][129146] Max Reward on eval: 635.4663080304978
[37m[1m[2023-06-25 08:07:40,932][129146] Min Reward on eval: -225.63445290498203
[37m[1m[2023-06-25 08:07:40,933][129146] Mean Reward across all agents: 379.00857814458027
[37m[1m[2023-06-25 08:07:40,933][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:07:40,938][129146] mean_value=149.7053792610409, max_value=935.7263648586347
[37m[1m[2023-06-25 08:07:40,940][129146] New mean coefficients: [[ 8.634624   -2.8761642  -1.1202385   6.800868    0.42744803]]
[37m[1m[2023-06-25 08:07:40,941][129146] Moving the mean solution point...
[36m[2023-06-25 08:07:50,741][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 08:07:50,741][129146] FPS: 391915.11
[36m[2023-06-25 08:07:50,743][129146] itr=772, itrs=2000, Progress: 38.60%
[36m[2023-06-25 08:08:02,202][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 08:08:02,203][129146] FPS: 335824.70
[36m[2023-06-25 08:08:06,959][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:08:06,959][129146] Reward + Measures: [[876.26119137   0.67272532   0.30626598   0.67500532   0.10178866]]
[37m[1m[2023-06-25 08:08:06,960][129146] Max Reward on eval: 876.2611913654213
[37m[1m[2023-06-25 08:08:06,960][129146] Min Reward on eval: 876.2611913654213
[37m[1m[2023-06-25 08:08:06,960][129146] Mean Reward across all agents: 876.2611913654213
[37m[1m[2023-06-25 08:08:06,960][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:08:12,340][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:08:12,341][129146] Reward + Measures: [[517.10015867   0.62709999   0.40760002   0.74239999   0.0762    ]
[37m[1m [271.29729351   0.61570007   0.36849999   0.68900001   0.0931    ]
[37m[1m [387.05928665   0.60909998   0.42940003   0.76200002   0.117     ]
[37m[1m ...
[37m[1m [209.61649164   0.67019999   0.36570001   0.76940006   0.1201    ]
[37m[1m [415.52097299   0.66710001   0.46599999   0.75840002   0.0918    ]
[37m[1m [540.36415563   0.62530005   0.37759998   0.71860003   0.06059999]]
[37m[1m[2023-06-25 08:08:12,341][129146] Max Reward on eval: 660.6753122549969
[37m[1m[2023-06-25 08:08:12,341][129146] Min Reward on eval: 136.87831048387451
[37m[1m[2023-06-25 08:08:12,341][129146] Mean Reward across all agents: 397.03978698276603
[37m[1m[2023-06-25 08:08:12,342][129146] Average Trajectory Length: 999.793
[36m[2023-06-25 08:08:12,345][129146] mean_value=-31.829781366571407, max_value=890.9485431193258
[37m[1m[2023-06-25 08:08:12,348][129146] New mean coefficients: [[ 9.048321   -4.7334433   0.05771661  5.184707    0.8205619 ]]
[37m[1m[2023-06-25 08:08:12,349][129146] Moving the mean solution point...
[36m[2023-06-25 08:08:21,953][129146] train() took 9.60 seconds to complete
[36m[2023-06-25 08:08:21,954][129146] FPS: 399867.07
[36m[2023-06-25 08:08:21,956][129146] itr=773, itrs=2000, Progress: 38.65%
[36m[2023-06-25 08:08:33,385][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 08:08:33,386][129146] FPS: 336702.75
[36m[2023-06-25 08:08:38,205][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:08:38,206][129146] Reward + Measures: [[923.18497325   0.67178166   0.30001932   0.65779102   0.11290733]]
[37m[1m[2023-06-25 08:08:38,206][129146] Max Reward on eval: 923.1849732509606
[37m[1m[2023-06-25 08:08:38,206][129146] Min Reward on eval: 923.1849732509606
[37m[1m[2023-06-25 08:08:38,206][129146] Mean Reward across all agents: 923.1849732509606
[37m[1m[2023-06-25 08:08:38,207][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:08:43,715][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:08:43,716][129146] Reward + Measures: [[325.43181632   0.68180001   0.43010002   0.63790005   0.0774    ]
[37m[1m [615.52613021   0.70359999   0.37610003   0.6943       0.0807    ]
[37m[1m [660.51251574   0.71509999   0.26509997   0.72360003   0.24010001]
[37m[1m ...
[37m[1m [648.79283518   0.73150003   0.22540002   0.74250001   0.24790001]
[37m[1m [473.83072243   0.67720002   0.32729998   0.60430002   0.0789    ]
[37m[1m [575.62978521   0.64580005   0.2375       0.6972       0.21580003]]
[37m[1m[2023-06-25 08:08:43,716][129146] Max Reward on eval: 794.9591502009076
[37m[1m[2023-06-25 08:08:43,716][129146] Min Reward on eval: -951.9691070233006
[37m[1m[2023-06-25 08:08:43,717][129146] Mean Reward across all agents: 419.8047026820837
[37m[1m[2023-06-25 08:08:43,717][129146] Average Trajectory Length: 999.6203333333333
[36m[2023-06-25 08:08:43,724][129146] mean_value=142.23869120474532, max_value=1054.6545538774255
[37m[1m[2023-06-25 08:08:43,727][129146] New mean coefficients: [[ 9.215899  -4.1791983 -1.56319    5.066878   1.1774055]]
[37m[1m[2023-06-25 08:08:43,728][129146] Moving the mean solution point...
[36m[2023-06-25 08:08:53,349][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 08:08:53,350][129146] FPS: 399156.34
[36m[2023-06-25 08:08:53,352][129146] itr=774, itrs=2000, Progress: 38.70%
[36m[2023-06-25 08:09:04,949][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 08:09:04,949][129146] FPS: 331782.69
[36m[2023-06-25 08:09:09,729][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:09:09,738][129146] Reward + Measures: [[975.88333854   0.66003895   0.29657599   0.63968569   0.12513199]]
[37m[1m[2023-06-25 08:09:09,738][129146] Max Reward on eval: 975.8833385394134
[37m[1m[2023-06-25 08:09:09,738][129146] Min Reward on eval: 975.8833385394134
[37m[1m[2023-06-25 08:09:09,739][129146] Mean Reward across all agents: 975.8833385394134
[37m[1m[2023-06-25 08:09:09,739][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:09:15,216][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:09:15,216][129146] Reward + Measures: [[258.77649265   0.75749999   0.47690001   0.71290004   0.0124    ]
[37m[1m [650.95535341   0.70200008   0.27770001   0.65570003   0.1549    ]
[37m[1m [  3.0014155    0.5309       0.1983       0.40830001   0.3527    ]
[37m[1m ...
[37m[1m [555.73317649   0.7119       0.2077       0.58020002   0.38480002]
[37m[1m [506.93231954   0.6893       0.18859999   0.58249998   0.35440001]
[37m[1m [330.06842053   0.54400003   0.42430001   0.71270001   0.12900001]]
[37m[1m[2023-06-25 08:09:15,216][129146] Max Reward on eval: 805.0493579633069
[37m[1m[2023-06-25 08:09:15,217][129146] Min Reward on eval: -176.2785122400266
[37m[1m[2023-06-25 08:09:15,217][129146] Mean Reward across all agents: 480.8696510901036
[37m[1m[2023-06-25 08:09:15,217][129146] Average Trajectory Length: 999.6543333333333
[36m[2023-06-25 08:09:15,222][129146] mean_value=-6.730829656015546, max_value=865.2121679628634
[37m[1m[2023-06-25 08:09:15,225][129146] New mean coefficients: [[ 9.142685  -5.122321  -1.0084515  4.0933843  0.6247472]]
[37m[1m[2023-06-25 08:09:15,226][129146] Moving the mean solution point...
[36m[2023-06-25 08:09:24,849][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 08:09:24,849][129146] FPS: 399103.86
[36m[2023-06-25 08:09:24,851][129146] itr=775, itrs=2000, Progress: 38.75%
[36m[2023-06-25 08:09:36,215][129146] train() took 11.34 seconds to complete
[36m[2023-06-25 08:09:36,216][129146] FPS: 338563.86
[36m[2023-06-25 08:09:40,923][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:09:40,923][129146] Reward + Measures: [[1020.05265456    0.663957      0.28902465    0.6214903     0.13187499]]
[37m[1m[2023-06-25 08:09:40,923][129146] Max Reward on eval: 1020.0526545618005
[37m[1m[2023-06-25 08:09:40,923][129146] Min Reward on eval: 1020.0526545618005
[37m[1m[2023-06-25 08:09:40,924][129146] Mean Reward across all agents: 1020.0526545618005
[37m[1m[2023-06-25 08:09:40,924][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:09:46,544][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:09:46,545][129146] Reward + Measures: [[410.31087193   0.65370005   0.4413       0.65059996   0.1495    ]
[37m[1m [420.66190485   0.68480009   0.3662       0.66860002   0.1107    ]
[37m[1m [306.50628576   0.56080002   0.47620001   0.71560001   0.14570001]
[37m[1m ...
[37m[1m [450.31611113   0.70050001   0.3626       0.69029999   0.1437    ]
[37m[1m [400.93326712   0.64709997   0.4303       0.64130002   0.1243    ]
[37m[1m [325.63710062   0.72130007   0.26950002   0.66820002   0.18269998]]
[37m[1m[2023-06-25 08:09:46,545][129146] Max Reward on eval: 881.9965997621184
[37m[1m[2023-06-25 08:09:46,545][129146] Min Reward on eval: -285.5279562427313
[37m[1m[2023-06-25 08:09:46,545][129146] Mean Reward across all agents: 401.2399578786013
[37m[1m[2023-06-25 08:09:46,546][129146] Average Trajectory Length: 997.8436666666666
[36m[2023-06-25 08:09:46,550][129146] mean_value=-49.817780216562845, max_value=939.4549122647499
[37m[1m[2023-06-25 08:09:46,553][129146] New mean coefficients: [[ 8.585086   -7.0055656   0.76165867  3.4002128   0.5338725 ]]
[37m[1m[2023-06-25 08:09:46,554][129146] Moving the mean solution point...
[36m[2023-06-25 08:09:56,274][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 08:09:56,274][129146] FPS: 395124.06
[36m[2023-06-25 08:09:56,276][129146] itr=776, itrs=2000, Progress: 38.80%
[36m[2023-06-25 08:10:07,665][129146] train() took 11.37 seconds to complete
[36m[2023-06-25 08:10:07,666][129146] FPS: 337805.51
[36m[2023-06-25 08:10:12,449][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:10:12,450][129146] Reward + Measures: [[1069.93107805    0.65298301    0.28093567    0.60516167    0.14118032]]
[37m[1m[2023-06-25 08:10:12,450][129146] Max Reward on eval: 1069.9310780540914
[37m[1m[2023-06-25 08:10:12,450][129146] Min Reward on eval: 1069.9310780540914
[37m[1m[2023-06-25 08:10:12,450][129146] Mean Reward across all agents: 1069.9310780540914
[37m[1m[2023-06-25 08:10:12,451][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:10:17,816][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:10:17,817][129146] Reward + Measures: [[436.04149288   0.4605       0.36540005   0.53840005   0.222     ]
[37m[1m [655.29469549   0.53450006   0.31869999   0.62350005   0.1551    ]
[37m[1m [118.87707239   0.56829995   0.30820003   0.65460002   0.13680001]
[37m[1m ...
[37m[1m [504.34257804   0.60459995   0.39000002   0.69410002   0.1199    ]
[37m[1m [423.39829047   0.53909999   0.35119998   0.5941       0.18630002]
[37m[1m [ 21.00542059   0.4666       0.24429999   0.51800007   0.2014    ]]
[37m[1m[2023-06-25 08:10:17,817][129146] Max Reward on eval: 882.4469487232388
[37m[1m[2023-06-25 08:10:17,817][129146] Min Reward on eval: -347.2689872014395
[37m[1m[2023-06-25 08:10:17,818][129146] Mean Reward across all agents: 437.15354256759485
[37m[1m[2023-06-25 08:10:17,818][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:10:17,821][129146] mean_value=-80.33539914021486, max_value=978.0100321683451
[37m[1m[2023-06-25 08:10:17,824][129146] New mean coefficients: [[ 9.154883   -6.8195567   1.7804008   1.6290665   0.44639042]]
[37m[1m[2023-06-25 08:10:17,825][129146] Moving the mean solution point...
[36m[2023-06-25 08:10:27,637][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 08:10:27,638][129146] FPS: 391393.16
[36m[2023-06-25 08:10:27,640][129146] itr=777, itrs=2000, Progress: 38.85%
[36m[2023-06-25 08:10:39,370][129146] train() took 11.71 seconds to complete
[36m[2023-06-25 08:10:39,370][129146] FPS: 327953.86
[36m[2023-06-25 08:10:44,238][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:10:44,238][129146] Reward + Measures: [[1114.3291343     0.64680368    0.27614498    0.59073532    0.145767  ]]
[37m[1m[2023-06-25 08:10:44,239][129146] Max Reward on eval: 1114.3291343005817
[37m[1m[2023-06-25 08:10:44,239][129146] Min Reward on eval: 1114.3291343005817
[37m[1m[2023-06-25 08:10:44,239][129146] Mean Reward across all agents: 1114.3291343005817
[37m[1m[2023-06-25 08:10:44,240][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:10:49,764][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:10:49,764][129146] Reward + Measures: [[702.02485421   0.60780001   0.32640001   0.57209998   0.12990001]
[37m[1m [601.98589002   0.58780003   0.37660003   0.52200001   0.27959999]
[37m[1m [959.62802682   0.62200004   0.27309999   0.57069999   0.1543    ]
[37m[1m ...
[37m[1m [491.8216757    0.54229999   0.41060001   0.51809996   0.32240003]
[37m[1m [ 84.29066901   0.43930003   0.4032       0.48530003   0.27059999]
[37m[1m [514.64418347   0.49780002   0.35570002   0.5887       0.1865    ]]
[37m[1m[2023-06-25 08:10:49,764][129146] Max Reward on eval: 959.6280268199043
[37m[1m[2023-06-25 08:10:49,765][129146] Min Reward on eval: -663.9961501288577
[37m[1m[2023-06-25 08:10:49,765][129146] Mean Reward across all agents: 417.807208762754
[37m[1m[2023-06-25 08:10:49,765][129146] Average Trajectory Length: 999.7663333333333
[36m[2023-06-25 08:10:49,771][129146] mean_value=0.5471152989508361, max_value=1095.9504086354113
[37m[1m[2023-06-25 08:10:49,774][129146] New mean coefficients: [[ 8.42671   -4.756501  -0.6786592  2.3579931  0.2116352]]
[37m[1m[2023-06-25 08:10:49,775][129146] Moving the mean solution point...
[36m[2023-06-25 08:10:59,640][129146] train() took 9.86 seconds to complete
[36m[2023-06-25 08:10:59,640][129146] FPS: 389312.66
[36m[2023-06-25 08:10:59,643][129146] itr=778, itrs=2000, Progress: 38.90%
[36m[2023-06-25 08:11:11,209][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 08:11:11,209][129146] FPS: 332632.50
[36m[2023-06-25 08:11:16,010][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:11:16,011][129146] Reward + Measures: [[803.53779852   0.61702335   0.32489333   0.56516701   0.18266332]]
[37m[1m[2023-06-25 08:11:16,011][129146] Max Reward on eval: 803.5377985226758
[37m[1m[2023-06-25 08:11:16,011][129146] Min Reward on eval: 803.5377985226758
[37m[1m[2023-06-25 08:11:16,011][129146] Mean Reward across all agents: 803.5377985226758
[37m[1m[2023-06-25 08:11:16,011][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:11:21,534][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:11:21,535][129146] Reward + Measures: [[540.7271956    0.68279994   0.3732       0.6354       0.0777    ]
[37m[1m [558.22229243   0.50410002   0.35579997   0.55500001   0.18910001]
[37m[1m [319.10558392   0.3867       0.22840002   0.48059997   0.20039999]
[37m[1m ...
[37m[1m [329.21561772   0.48330003   0.31809998   0.4901       0.22649999]
[37m[1m [340.26322871   0.48280001   0.27880001   0.44949999   0.22880001]
[37m[1m [325.69467724   0.44780001   0.31620002   0.47680002   0.16129999]]
[37m[1m[2023-06-25 08:11:21,535][129146] Max Reward on eval: 797.2590799458383
[37m[1m[2023-06-25 08:11:21,535][129146] Min Reward on eval: -305.39768069387355
[37m[1m[2023-06-25 08:11:21,536][129146] Mean Reward across all agents: 420.0216016387562
[37m[1m[2023-06-25 08:11:21,536][129146] Average Trajectory Length: 997.2819999999999
[36m[2023-06-25 08:11:21,540][129146] mean_value=-7.842538973664535, max_value=893.0545736093868
[37m[1m[2023-06-25 08:11:21,543][129146] New mean coefficients: [[ 8.499621  -6.346653  -1.4979501  2.7930884  1.1371233]]
[37m[1m[2023-06-25 08:11:21,544][129146] Moving the mean solution point...
[36m[2023-06-25 08:11:31,298][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 08:11:31,298][129146] FPS: 393743.25
[36m[2023-06-25 08:11:31,300][129146] itr=779, itrs=2000, Progress: 38.95%
[36m[2023-06-25 08:11:42,905][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 08:11:42,905][129146] FPS: 331572.33
[36m[2023-06-25 08:11:47,798][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:11:47,798][129146] Reward + Measures: [[864.19836386   0.60998136   0.30705366   0.55858064   0.18205066]]
[37m[1m[2023-06-25 08:11:47,799][129146] Max Reward on eval: 864.1983638628968
[37m[1m[2023-06-25 08:11:47,799][129146] Min Reward on eval: 864.1983638628968
[37m[1m[2023-06-25 08:11:47,799][129146] Mean Reward across all agents: 864.1983638628968
[37m[1m[2023-06-25 08:11:47,799][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:11:53,260][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:11:53,260][129146] Reward + Measures: [[ 607.5341034     0.6552        0.39309999    0.61110002    0.11870001]
[37m[1m [ -19.49889823    0.61760002    0.32259998    0.65009999    0.18840002]
[37m[1m [-115.28277648    0.52814811    0.1575541     0.49350491    0.26352242]
[37m[1m ...
[37m[1m [ 352.53918002    0.67939997    0.43809995    0.65929997    0.11920001]
[37m[1m [  97.1058226     0.65020001    0.41300002    0.671         0.16680001]
[37m[1m [ -64.84869639    0.58359998    0.25060001    0.54570001    0.21490002]]
[37m[1m[2023-06-25 08:11:53,261][129146] Max Reward on eval: 710.7207142154454
[37m[1m[2023-06-25 08:11:53,261][129146] Min Reward on eval: -481.93330921767046
[37m[1m[2023-06-25 08:11:53,261][129146] Mean Reward across all agents: 285.49558997874726
[37m[1m[2023-06-25 08:11:53,261][129146] Average Trajectory Length: 998.6736666666666
[36m[2023-06-25 08:11:53,264][129146] mean_value=-329.2424112656453, max_value=284.19044963306845
[37m[1m[2023-06-25 08:11:53,266][129146] New mean coefficients: [[ 8.773889   -6.80376     0.34785056  1.8015702   1.0087065 ]]
[37m[1m[2023-06-25 08:11:53,267][129146] Moving the mean solution point...
[36m[2023-06-25 08:12:03,124][129146] train() took 9.86 seconds to complete
[36m[2023-06-25 08:12:03,124][129146] FPS: 389642.86
[36m[2023-06-25 08:12:03,126][129146] itr=780, itrs=2000, Progress: 39.00%
[37m[1m[2023-06-25 08:12:09,456][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000760
[36m[2023-06-25 08:12:21,276][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 08:12:21,276][129146] FPS: 331562.32
[36m[2023-06-25 08:12:26,144][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:12:26,145][129146] Reward + Measures: [[946.52806432   0.60365134   0.29546365   0.54281837   0.189889  ]]
[37m[1m[2023-06-25 08:12:26,145][129146] Max Reward on eval: 946.5280643170619
[37m[1m[2023-06-25 08:12:26,145][129146] Min Reward on eval: 946.5280643170619
[37m[1m[2023-06-25 08:12:26,145][129146] Mean Reward across all agents: 946.5280643170619
[37m[1m[2023-06-25 08:12:26,146][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:12:31,763][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:12:31,764][129146] Reward + Measures: [[ 370.64481516    0.6135        0.3994        0.53930002    0.28839999]
[37m[1m [ 254.85406436    0.57040006    0.33470002    0.55910003    0.20019999]
[37m[1m [ 314.36877882    0.60089999    0.3141        0.60210001    0.1858    ]
[37m[1m ...
[37m[1m [ 281.94995057    0.60030001    0.34450004    0.56230003    0.2141    ]
[37m[1m [ 501.09496468    0.62959999    0.38870001    0.58470005    0.1441    ]
[37m[1m [-115.51943007    0.53430003    0.32300001    0.4698        0.22570001]]
[37m[1m[2023-06-25 08:12:31,764][129146] Max Reward on eval: 786.5663444713573
[37m[1m[2023-06-25 08:12:31,764][129146] Min Reward on eval: -437.1172560945968
[37m[1m[2023-06-25 08:12:31,764][129146] Mean Reward across all agents: 328.0493470006581
[37m[1m[2023-06-25 08:12:31,765][129146] Average Trajectory Length: 992.6959999999999
[36m[2023-06-25 08:12:31,768][129146] mean_value=-197.61266120704323, max_value=586.8274528074139
[37m[1m[2023-06-25 08:12:31,770][129146] New mean coefficients: [[ 8.762311   -5.493843   -1.0971282   2.5083852   0.49363744]]
[37m[1m[2023-06-25 08:12:31,771][129146] Moving the mean solution point...
[36m[2023-06-25 08:12:41,478][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 08:12:41,478][129146] FPS: 395656.36
[36m[2023-06-25 08:12:41,480][129146] itr=781, itrs=2000, Progress: 39.05%
[36m[2023-06-25 08:12:53,067][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 08:12:53,068][129146] FPS: 332032.43
[36m[2023-06-25 08:12:57,783][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:12:57,783][129146] Reward + Measures: [[760.00641137   0.56071568   0.28069133   0.46155033   0.19280933]]
[37m[1m[2023-06-25 08:12:57,783][129146] Max Reward on eval: 760.006411374974
[37m[1m[2023-06-25 08:12:57,784][129146] Min Reward on eval: 760.006411374974
[37m[1m[2023-06-25 08:12:57,784][129146] Mean Reward across all agents: 760.006411374974
[37m[1m[2023-06-25 08:12:57,784][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:13:03,330][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:13:03,331][129146] Reward + Measures: [[640.2832048    0.63269997   0.35750005   0.53350002   0.1312    ]
[37m[1m [703.00997706   0.60540003   0.3382       0.51529998   0.14670001]
[37m[1m [581.40834365   0.6419       0.35650003   0.57530004   0.12520002]
[37m[1m ...
[37m[1m [541.12294112   0.63780004   0.38580003   0.59700006   0.1251    ]
[37m[1m [448.87348061   0.65290004   0.40739998   0.62390006   0.10750001]
[37m[1m [488.76108125   0.63330001   0.36740002   0.58459997   0.1195    ]]
[37m[1m[2023-06-25 08:13:03,331][129146] Max Reward on eval: 737.5535659836023
[37m[1m[2023-06-25 08:13:03,331][129146] Min Reward on eval: -1.7416015320224687
[37m[1m[2023-06-25 08:13:03,331][129146] Mean Reward across all agents: 509.68674085585496
[37m[1m[2023-06-25 08:13:03,332][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:13:03,334][129146] mean_value=-117.46146802049799, max_value=264.3838227447302
[37m[1m[2023-06-25 08:13:03,337][129146] New mean coefficients: [[ 8.772481   -3.4942522  -1.3938347   3.315547    0.73827225]]
[37m[1m[2023-06-25 08:13:03,338][129146] Moving the mean solution point...
[36m[2023-06-25 08:13:12,938][129146] train() took 9.60 seconds to complete
[36m[2023-06-25 08:13:12,939][129146] FPS: 400037.75
[36m[2023-06-25 08:13:12,941][129146] itr=782, itrs=2000, Progress: 39.10%
[36m[2023-06-25 08:13:24,627][129146] train() took 11.66 seconds to complete
[36m[2023-06-25 08:13:24,628][129146] FPS: 329297.63
[36m[2023-06-25 08:13:29,374][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:13:29,375][129146] Reward + Measures: [[827.11347822   0.54924178   0.27868482   0.44453278   0.20657299]]
[37m[1m[2023-06-25 08:13:29,375][129146] Max Reward on eval: 827.113478215376
[37m[1m[2023-06-25 08:13:29,375][129146] Min Reward on eval: 827.113478215376
[37m[1m[2023-06-25 08:13:29,375][129146] Mean Reward across all agents: 827.113478215376
[37m[1m[2023-06-25 08:13:29,376][129146] Average Trajectory Length: 999.7156666666666
[36m[2023-06-25 08:13:34,852][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:13:34,852][129146] Reward + Measures: [[441.40328381   0.48990002   0.28910002   0.41490003   0.18880001]
[37m[1m [555.00198538   0.56290001   0.34380004   0.51270002   0.1909    ]
[37m[1m [455.63806908   0.44569999   0.26620001   0.40370002   0.2314    ]
[37m[1m ...
[37m[1m [529.15450375   0.51520008   0.2881       0.43090001   0.20320001]
[37m[1m [564.12934478   0.55096304   0.29061514   0.48565206   0.17940503]
[37m[1m [569.33923324   0.51660001   0.30360001   0.46480003   0.149     ]]
[37m[1m[2023-06-25 08:13:34,853][129146] Max Reward on eval: 694.9829406287987
[37m[1m[2023-06-25 08:13:34,853][129146] Min Reward on eval: 189.79868806274607
[37m[1m[2023-06-25 08:13:34,853][129146] Mean Reward across all agents: 452.81483046713265
[37m[1m[2023-06-25 08:13:34,853][129146] Average Trajectory Length: 999.577
[36m[2023-06-25 08:13:34,856][129146] mean_value=-128.35949743970704, max_value=411.6507891981751
[37m[1m[2023-06-25 08:13:34,858][129146] New mean coefficients: [[ 7.4704685  -3.5862849  -1.2132587   3.4122558   0.27712548]]
[37m[1m[2023-06-25 08:13:34,859][129146] Moving the mean solution point...
[36m[2023-06-25 08:13:44,669][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 08:13:44,670][129146] FPS: 391476.25
[36m[2023-06-25 08:13:44,672][129146] itr=783, itrs=2000, Progress: 39.15%
[36m[2023-06-25 08:13:56,272][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 08:13:56,272][129146] FPS: 331668.61
[36m[2023-06-25 08:14:01,042][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:14:01,043][129146] Reward + Measures: [[880.28328481   0.54945034   0.27750599   0.43715197   0.21573132]]
[37m[1m[2023-06-25 08:14:01,043][129146] Max Reward on eval: 880.283284805814
[37m[1m[2023-06-25 08:14:01,043][129146] Min Reward on eval: 880.283284805814
[37m[1m[2023-06-25 08:14:01,044][129146] Mean Reward across all agents: 880.283284805814
[37m[1m[2023-06-25 08:14:01,044][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:14:06,490][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:14:06,491][129146] Reward + Measures: [[348.00264641   0.65979999   0.3651       0.51270002   0.1158    ]
[37m[1m [461.8953035    0.63079995   0.3378       0.5255       0.14210001]
[37m[1m [502.67297387   0.69580001   0.33540002   0.52670002   0.12620001]
[37m[1m ...
[37m[1m [352.47364618   0.65149999   0.37280002   0.50120002   0.1242    ]
[37m[1m [314.77540551   0.56020004   0.33989999   0.46090004   0.2149    ]
[37m[1m [267.22652315   0.60000002   0.38960001   0.49759999   0.13670002]]
[37m[1m[2023-06-25 08:14:06,491][129146] Max Reward on eval: 792.6030665466096
[37m[1m[2023-06-25 08:14:06,491][129146] Min Reward on eval: -44.64641037127003
[37m[1m[2023-06-25 08:14:06,491][129146] Mean Reward across all agents: 376.74856884529436
[37m[1m[2023-06-25 08:14:06,492][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:14:06,493][129146] mean_value=-199.18739399877273, max_value=152.89061967346413
[37m[1m[2023-06-25 08:14:06,496][129146] New mean coefficients: [[ 6.558156   -2.2342913  -0.6715028   3.0512812   0.06668112]]
[37m[1m[2023-06-25 08:14:06,496][129146] Moving the mean solution point...
[36m[2023-06-25 08:14:16,307][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 08:14:16,307][129146] FPS: 391479.35
[36m[2023-06-25 08:14:16,310][129146] itr=784, itrs=2000, Progress: 39.20%
[36m[2023-06-25 08:14:27,927][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 08:14:27,928][129146] FPS: 331246.47
[36m[2023-06-25 08:14:32,596][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:14:32,596][129146] Reward + Measures: [[925.48003082   0.54334897   0.27758133   0.42880604   0.21988466]]
[37m[1m[2023-06-25 08:14:32,596][129146] Max Reward on eval: 925.4800308188741
[37m[1m[2023-06-25 08:14:32,597][129146] Min Reward on eval: 925.4800308188741
[37m[1m[2023-06-25 08:14:32,597][129146] Mean Reward across all agents: 925.4800308188741
[37m[1m[2023-06-25 08:14:32,597][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:14:38,309][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:14:38,310][129146] Reward + Measures: [[406.90715401   0.47720003   0.37020001   0.4628       0.2339    ]
[37m[1m [465.63098719   0.42309999   0.30700001   0.41210005   0.17560001]
[37m[1m [197.77364697   0.54580003   0.37189999   0.51590002   0.19860001]
[37m[1m ...
[37m[1m [505.52723274   0.5575       0.33379999   0.49439999   0.1908    ]
[37m[1m [465.3255985    0.55049998   0.34250003   0.52360004   0.1613    ]
[37m[1m [604.64929872   0.58230001   0.30509999   0.51139998   0.1793    ]]
[37m[1m[2023-06-25 08:14:38,310][129146] Max Reward on eval: 883.8264351383434
[37m[1m[2023-06-25 08:14:38,310][129146] Min Reward on eval: 88.87788560497138
[37m[1m[2023-06-25 08:14:38,311][129146] Mean Reward across all agents: 512.8596677439905
[37m[1m[2023-06-25 08:14:38,311][129146] Average Trajectory Length: 999.4653333333333
[36m[2023-06-25 08:14:38,313][129146] mean_value=-156.04110302629203, max_value=327.4263800377868
[37m[1m[2023-06-25 08:14:38,315][129146] New mean coefficients: [[ 5.5228224 -0.828447  -0.6283755  2.355     -0.4863744]]
[37m[1m[2023-06-25 08:14:38,316][129146] Moving the mean solution point...
[36m[2023-06-25 08:14:48,107][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 08:14:48,107][129146] FPS: 392287.71
[36m[2023-06-25 08:14:48,109][129146] itr=785, itrs=2000, Progress: 39.25%
[36m[2023-06-25 08:14:59,777][129146] train() took 11.65 seconds to complete
[36m[2023-06-25 08:14:59,777][129146] FPS: 329748.28
[36m[2023-06-25 08:15:04,545][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:15:04,545][129146] Reward + Measures: [[982.39187622   0.53106534   0.27953601   0.41188404   0.227293  ]]
[37m[1m[2023-06-25 08:15:04,545][129146] Max Reward on eval: 982.3918762231747
[37m[1m[2023-06-25 08:15:04,546][129146] Min Reward on eval: 982.3918762231747
[37m[1m[2023-06-25 08:15:04,546][129146] Mean Reward across all agents: 982.3918762231747
[37m[1m[2023-06-25 08:15:04,546][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:15:10,024][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:15:10,025][129146] Reward + Measures: [[966.49856055   0.54629999   0.2775       0.433        0.24610002]
[37m[1m [863.9264867    0.53530002   0.33179998   0.44090006   0.28580001]
[37m[1m [695.00129741   0.54220003   0.36450002   0.45510003   0.27409998]
[37m[1m ...
[37m[1m [819.90309831   0.50430006   0.26159999   0.41429996   0.27379999]
[37m[1m [701.03164101   0.47100002   0.24439998   0.40260002   0.257     ]
[37m[1m [618.29750736   0.49489999   0.25460002   0.45409998   0.23220001]]
[37m[1m[2023-06-25 08:15:10,025][129146] Max Reward on eval: 1036.8603219896672
[37m[1m[2023-06-25 08:15:10,025][129146] Min Reward on eval: 387.56100247662545
[37m[1m[2023-06-25 08:15:10,026][129146] Mean Reward across all agents: 793.2933786923866
[37m[1m[2023-06-25 08:15:10,026][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:15:10,030][129146] mean_value=103.3851630191192, max_value=434.33400052517027
[37m[1m[2023-06-25 08:15:10,033][129146] New mean coefficients: [[ 4.521982    0.4034946  -0.80152977  2.33357    -0.34402734]]
[37m[1m[2023-06-25 08:15:10,034][129146] Moving the mean solution point...
[36m[2023-06-25 08:15:19,779][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 08:15:19,780][129146] FPS: 394100.99
[36m[2023-06-25 08:15:19,782][129146] itr=786, itrs=2000, Progress: 39.30%
[36m[2023-06-25 08:15:31,352][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 08:15:31,352][129146] FPS: 332542.12
[36m[2023-06-25 08:15:36,121][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:15:36,121][129146] Reward + Measures: [[1023.05069474    0.53081715    0.28068009    0.40681934    0.2291052 ]]
[37m[1m[2023-06-25 08:15:36,121][129146] Max Reward on eval: 1023.0506947441005
[37m[1m[2023-06-25 08:15:36,122][129146] Min Reward on eval: 1023.0506947441005
[37m[1m[2023-06-25 08:15:36,122][129146] Mean Reward across all agents: 1023.0506947441005
[37m[1m[2023-06-25 08:15:36,122][129146] Average Trajectory Length: 999.5723333333333
[36m[2023-06-25 08:15:41,541][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:15:41,542][129146] Reward + Measures: [[828.90294672   0.53260005   0.28260002   0.4815       0.1893    ]
[37m[1m [611.14911484   0.56690001   0.287        0.4772       0.19680001]
[37m[1m [613.58720872   0.54390001   0.3154       0.53899997   0.185     ]
[37m[1m ...
[37m[1m [542.63985341   0.5528       0.30969998   0.53350002   0.1904    ]
[37m[1m [722.3426014    0.53350002   0.29340002   0.52180004   0.22669999]
[37m[1m [643.71358174   0.47         0.3558       0.4549       0.2868    ]]
[37m[1m[2023-06-25 08:15:41,542][129146] Max Reward on eval: 1054.6946023646974
[37m[1m[2023-06-25 08:15:41,542][129146] Min Reward on eval: 368.9882110447972
[37m[1m[2023-06-25 08:15:41,543][129146] Mean Reward across all agents: 750.6817232720196
[37m[1m[2023-06-25 08:15:41,543][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:15:41,547][129146] mean_value=3.419704917937294, max_value=829.1226164278138
[37m[1m[2023-06-25 08:15:41,550][129146] New mean coefficients: [[ 4.280841   -1.3108503  -0.31584907  2.037171   -0.1153193 ]]
[37m[1m[2023-06-25 08:15:41,551][129146] Moving the mean solution point...
[36m[2023-06-25 08:15:51,284][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 08:15:51,285][129146] FPS: 394574.07
[36m[2023-06-25 08:15:51,287][129146] itr=787, itrs=2000, Progress: 39.35%
[36m[2023-06-25 08:16:02,831][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 08:16:02,831][129146] FPS: 333274.77
[36m[2023-06-25 08:16:07,645][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:16:07,645][129146] Reward + Measures: [[1059.17596096    0.5358333     0.27977279    0.41043261    0.22644682]]
[37m[1m[2023-06-25 08:16:07,646][129146] Max Reward on eval: 1059.17596095641
[37m[1m[2023-06-25 08:16:07,646][129146] Min Reward on eval: 1059.17596095641
[37m[1m[2023-06-25 08:16:07,646][129146] Mean Reward across all agents: 1059.17596095641
[37m[1m[2023-06-25 08:16:07,646][129146] Average Trajectory Length: 999.7316666666667
[36m[2023-06-25 08:16:13,192][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:16:13,193][129146] Reward + Measures: [[ 794.14598925    0.5316        0.25139999    0.48020002    0.19060001]
[37m[1m [1008.83264982    0.57140005    0.28729999    0.44009995    0.2112    ]
[37m[1m [ 863.05056664    0.52079999    0.27220002    0.46139994    0.21690002]
[37m[1m ...
[37m[1m [ 784.0289683     0.56830001    0.29749998    0.48389998    0.17400001]
[37m[1m [ 680.88256372    0.55660003    0.27950001    0.50050002    0.1866    ]
[37m[1m [ 870.02255237    0.51990002    0.28040001    0.4718        0.1683    ]]
[37m[1m[2023-06-25 08:16:13,193][129146] Max Reward on eval: 1051.854897974641
[37m[1m[2023-06-25 08:16:13,193][129146] Min Reward on eval: 397.06553684513784
[37m[1m[2023-06-25 08:16:13,193][129146] Mean Reward across all agents: 831.9070540999977
[37m[1m[2023-06-25 08:16:13,194][129146] Average Trajectory Length: 999.7496666666666
[36m[2023-06-25 08:16:13,197][129146] mean_value=-2.8952033183259, max_value=393.27027867445815
[37m[1m[2023-06-25 08:16:13,199][129146] New mean coefficients: [[ 3.8346353  -1.690428   -0.5974425   2.3390126  -0.38338733]]
[37m[1m[2023-06-25 08:16:13,201][129146] Moving the mean solution point...
[36m[2023-06-25 08:16:23,045][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 08:16:23,045][129146] FPS: 390130.24
[36m[2023-06-25 08:16:23,048][129146] itr=788, itrs=2000, Progress: 39.40%
[36m[2023-06-25 08:16:34,653][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 08:16:34,653][129146] FPS: 331527.57
[36m[2023-06-25 08:16:39,413][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:16:39,414][129146] Reward + Measures: [[1111.95121934    0.53032196    0.27958798    0.40336633    0.23213966]]
[37m[1m[2023-06-25 08:16:39,414][129146] Max Reward on eval: 1111.951219340114
[37m[1m[2023-06-25 08:16:39,414][129146] Min Reward on eval: 1111.951219340114
[37m[1m[2023-06-25 08:16:39,414][129146] Mean Reward across all agents: 1111.951219340114
[37m[1m[2023-06-25 08:16:39,415][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:16:44,968][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:16:44,969][129146] Reward + Measures: [[1081.59767998    0.59259999    0.28530002    0.46310002    0.19159999]
[37m[1m [ 968.13617513    0.50920004    0.32460001    0.44140002    0.24109998]
[37m[1m [ 884.16661021    0.60499996    0.29859999    0.53780001    0.1837    ]
[37m[1m ...
[37m[1m [ 853.34724192    0.61479998    0.30430004    0.55610007    0.1742    ]
[37m[1m [1009.9821196     0.55319995    0.29450002    0.49390003    0.22540002]
[37m[1m [ 852.15411426    0.61759996    0.3055        0.55270004    0.17200001]]
[37m[1m[2023-06-25 08:16:44,969][129146] Max Reward on eval: 1182.4543613118701
[37m[1m[2023-06-25 08:16:44,970][129146] Min Reward on eval: 291.47735070325433
[37m[1m[2023-06-25 08:16:44,970][129146] Mean Reward across all agents: 848.4025382984888
[37m[1m[2023-06-25 08:16:44,970][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:16:44,975][129146] mean_value=14.894055403206185, max_value=358.18729641117784
[37m[1m[2023-06-25 08:16:44,977][129146] New mean coefficients: [[ 2.433617    0.20980012 -0.29659417  1.8235956  -0.6794776 ]]
[37m[1m[2023-06-25 08:16:44,978][129146] Moving the mean solution point...
[36m[2023-06-25 08:16:54,779][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 08:16:54,779][129146] FPS: 391862.13
[36m[2023-06-25 08:16:54,781][129146] itr=789, itrs=2000, Progress: 39.45%
[36m[2023-06-25 08:17:06,273][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 08:17:06,273][129146] FPS: 334798.19
[36m[2023-06-25 08:17:11,116][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:17:11,116][129146] Reward + Measures: [[1156.02267296    0.53522599    0.277738      0.406982      0.22917867]]
[37m[1m[2023-06-25 08:17:11,116][129146] Max Reward on eval: 1156.0226729570352
[37m[1m[2023-06-25 08:17:11,116][129146] Min Reward on eval: 1156.0226729570352
[37m[1m[2023-06-25 08:17:11,117][129146] Mean Reward across all agents: 1156.0226729570352
[37m[1m[2023-06-25 08:17:11,117][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:17:16,745][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:17:16,746][129146] Reward + Measures: [[1066.36060314    0.52249998    0.30399999    0.44440004    0.19660001]
[37m[1m [ 868.35512407    0.52349997    0.29210001    0.48050004    0.1551    ]
[37m[1m [ 808.48331443    0.53560001    0.2895        0.49849996    0.16759999]
[37m[1m ...
[37m[1m [ 767.84490184    0.45760003    0.27690002    0.4206        0.20580001]
[37m[1m [ 531.41101434    0.57950002    0.28400001    0.44350001    0.11750001]
[37m[1m [ 717.1129553     0.52570003    0.29710004    0.46830001    0.17380001]]
[37m[1m[2023-06-25 08:17:16,746][129146] Max Reward on eval: 1186.43743980258
[37m[1m[2023-06-25 08:17:16,746][129146] Min Reward on eval: 464.32661629625363
[37m[1m[2023-06-25 08:17:16,746][129146] Mean Reward across all agents: 923.2690328231561
[37m[1m[2023-06-25 08:17:16,747][129146] Average Trajectory Length: 999.4309999999999
[36m[2023-06-25 08:17:16,750][129146] mean_value=-50.054178086179675, max_value=442.97573559187106
[37m[1m[2023-06-25 08:17:16,753][129146] New mean coefficients: [[ 3.0581412  -2.1385221   0.36938003  1.022832   -0.13850737]]
[37m[1m[2023-06-25 08:17:16,754][129146] Moving the mean solution point...
[36m[2023-06-25 08:17:26,664][129146] train() took 9.91 seconds to complete
[36m[2023-06-25 08:17:26,664][129146] FPS: 387542.00
[36m[2023-06-25 08:17:26,667][129146] itr=790, itrs=2000, Progress: 39.50%
[37m[1m[2023-06-25 08:17:33,152][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000770
[36m[2023-06-25 08:17:44,950][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 08:17:44,951][129146] FPS: 332695.46
[36m[2023-06-25 08:17:49,809][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:17:49,809][129146] Reward + Measures: [[1210.26489471    0.53372598    0.27533698    0.39758301    0.22667833]]
[37m[1m[2023-06-25 08:17:49,809][129146] Max Reward on eval: 1210.2648947133614
[37m[1m[2023-06-25 08:17:49,810][129146] Min Reward on eval: 1210.2648947133614
[37m[1m[2023-06-25 08:17:49,810][129146] Mean Reward across all agents: 1210.2648947133614
[37m[1m[2023-06-25 08:17:49,810][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:17:55,203][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:17:55,204][129146] Reward + Measures: [[ 835.39626792    0.43430001    0.27620003    0.37800002    0.26929998]
[37m[1m [ 650.24066077    0.4138        0.2753        0.3739        0.21870001]
[37m[1m [ 729.11019897    0.40850002    0.2902        0.35400003    0.13509999]
[37m[1m ...
[37m[1m [1111.99878453    0.53099996    0.28529999    0.4172        0.21269999]
[37m[1m [ 962.80690824    0.45590001    0.27900001    0.37280002    0.25789997]
[37m[1m [1109.16606072    0.52570003    0.28099999    0.41910002    0.2024    ]]
[37m[1m[2023-06-25 08:17:55,204][129146] Max Reward on eval: 1251.724178912508
[37m[1m[2023-06-25 08:17:55,204][129146] Min Reward on eval: 302.5048033379135
[37m[1m[2023-06-25 08:17:55,204][129146] Mean Reward across all agents: 903.9251175870128
[37m[1m[2023-06-25 08:17:55,205][129146] Average Trajectory Length: 999.3296666666666
[36m[2023-06-25 08:17:55,208][129146] mean_value=-31.159596562072924, max_value=1144.2177169684367
[37m[1m[2023-06-25 08:17:55,210][129146] New mean coefficients: [[ 2.7992702  -1.7533985   0.6464463   0.42352855 -0.8233712 ]]
[37m[1m[2023-06-25 08:17:55,211][129146] Moving the mean solution point...
[36m[2023-06-25 08:18:04,938][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 08:18:04,938][129146] FPS: 394862.11
[36m[2023-06-25 08:18:04,941][129146] itr=791, itrs=2000, Progress: 39.55%
[36m[2023-06-25 08:18:16,416][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 08:18:16,416][129146] FPS: 335278.00
[36m[2023-06-25 08:18:21,069][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:18:21,070][129146] Reward + Measures: [[1253.72373916    0.52587533    0.27235898    0.38461736    0.22685298]]
[37m[1m[2023-06-25 08:18:21,070][129146] Max Reward on eval: 1253.7237391608849
[37m[1m[2023-06-25 08:18:21,070][129146] Min Reward on eval: 1253.7237391608849
[37m[1m[2023-06-25 08:18:21,070][129146] Mean Reward across all agents: 1253.7237391608849
[37m[1m[2023-06-25 08:18:21,071][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:18:26,495][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:18:26,495][129146] Reward + Measures: [[1260.74107054    0.55730003    0.2753        0.38250002    0.2246    ]
[37m[1m [1132.84599239    0.53380007    0.27620003    0.43420002    0.23280001]
[37m[1m [ 792.1680099     0.45280001    0.2527        0.39030001    0.25040004]
[37m[1m ...
[37m[1m [1122.09015857    0.48969999    0.24889998    0.36779997    0.26180002]
[37m[1m [ 950.51989703    0.44790003    0.25970003    0.36610004    0.2572    ]
[37m[1m [1211.89960396    0.5097        0.27280003    0.4041        0.24530001]]
[37m[1m[2023-06-25 08:18:26,495][129146] Max Reward on eval: 1312.8032972563524
[37m[1m[2023-06-25 08:18:26,496][129146] Min Reward on eval: 551.7613090004015
[37m[1m[2023-06-25 08:18:26,496][129146] Mean Reward across all agents: 1060.417054244462
[37m[1m[2023-06-25 08:18:26,496][129146] Average Trajectory Length: 999.8299999999999
[36m[2023-06-25 08:18:26,500][129146] mean_value=29.504275722959648, max_value=602.3492384244691
[37m[1m[2023-06-25 08:18:26,502][129146] New mean coefficients: [[ 2.9686997  -3.4731145  -0.05648541  0.5451318  -0.3339241 ]]
[37m[1m[2023-06-25 08:18:26,503][129146] Moving the mean solution point...
[36m[2023-06-25 08:18:36,134][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 08:18:36,135][129146] FPS: 398752.68
[36m[2023-06-25 08:18:36,137][129146] itr=792, itrs=2000, Progress: 39.60%
[36m[2023-06-25 08:18:47,557][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 08:18:47,557][129146] FPS: 336856.26
[36m[2023-06-25 08:18:52,271][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:18:52,272][129146] Reward + Measures: [[1290.43926701    0.51319402    0.26963899    0.37757164    0.23910166]]
[37m[1m[2023-06-25 08:18:52,272][129146] Max Reward on eval: 1290.4392670119307
[37m[1m[2023-06-25 08:18:52,272][129146] Min Reward on eval: 1290.4392670119307
[37m[1m[2023-06-25 08:18:52,273][129146] Mean Reward across all agents: 1290.4392670119307
[37m[1m[2023-06-25 08:18:52,273][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:18:57,675][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:18:57,675][129146] Reward + Measures: [[1097.30448647    0.57209998    0.29369998    0.44030005    0.23340002]
[37m[1m [ 934.30832999    0.53979999    0.27160001    0.44029999    0.21570002]
[37m[1m [ 918.80155161    0.52239996    0.3378        0.42290002    0.29480001]
[37m[1m ...
[37m[1m [ 961.4139168     0.51999998    0.28979999    0.45480004    0.23280001]
[37m[1m [1084.95410064    0.56100005    0.31390002    0.4355        0.2467    ]
[37m[1m [ 776.62067587    0.57619995    0.34620002    0.4707        0.21140002]]
[37m[1m[2023-06-25 08:18:57,676][129146] Max Reward on eval: 1333.6159948772518
[37m[1m[2023-06-25 08:18:57,676][129146] Min Reward on eval: 423.8443934486539
[37m[1m[2023-06-25 08:18:57,676][129146] Mean Reward across all agents: 993.7205992089104
[37m[1m[2023-06-25 08:18:57,676][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:18:57,680][129146] mean_value=-29.772003643631784, max_value=405.34161748810493
[37m[1m[2023-06-25 08:18:57,682][129146] New mean coefficients: [[ 3.790148   -3.6420972  -0.74529773  0.32378876  0.24871516]]
[37m[1m[2023-06-25 08:18:57,683][129146] Moving the mean solution point...
[36m[2023-06-25 08:19:07,295][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 08:19:07,296][129146] FPS: 399577.05
[36m[2023-06-25 08:19:07,298][129146] itr=793, itrs=2000, Progress: 39.65%
[36m[2023-06-25 08:19:18,749][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 08:19:18,749][129146] FPS: 336075.93
[36m[2023-06-25 08:19:23,605][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:19:23,605][129146] Reward + Measures: [[1335.97977598    0.50898427    0.26705834    0.36698127    0.24608496]]
[37m[1m[2023-06-25 08:19:23,606][129146] Max Reward on eval: 1335.9797759826133
[37m[1m[2023-06-25 08:19:23,606][129146] Min Reward on eval: 1335.9797759826133
[37m[1m[2023-06-25 08:19:23,606][129146] Mean Reward across all agents: 1335.9797759826133
[37m[1m[2023-06-25 08:19:23,606][129146] Average Trajectory Length: 999.4976666666666
[36m[2023-06-25 08:19:29,173][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:19:29,173][129146] Reward + Measures: [[ 733.58175563    0.42379999    0.24910001    0.33320001    0.17309999]
[37m[1m [ 683.39745036    0.34689999    0.33900002    0.34630004    0.33830005]
[37m[1m [ 461.157619      0.39850003    0.26290002    0.35819998    0.26869997]
[37m[1m ...
[37m[1m [1140.03637354    0.4619        0.28240001    0.40080005    0.2581    ]
[37m[1m [ 771.67602707    0.43930003    0.29729998    0.3795        0.2782    ]
[37m[1m [ 845.40500266    0.40299997    0.2626        0.3608        0.29800001]]
[37m[1m[2023-06-25 08:19:29,174][129146] Max Reward on eval: 1365.2295668760664
[37m[1m[2023-06-25 08:19:29,174][129146] Min Reward on eval: 66.70316590060538
[37m[1m[2023-06-25 08:19:29,174][129146] Mean Reward across all agents: 824.4063178520215
[37m[1m[2023-06-25 08:19:29,174][129146] Average Trajectory Length: 999.896
[36m[2023-06-25 08:19:29,178][129146] mean_value=-100.43716688519696, max_value=693.556856952296
[37m[1m[2023-06-25 08:19:29,180][129146] New mean coefficients: [[ 3.3353791  -3.246388   -0.23565245 -0.2505284   0.46163037]]
[37m[1m[2023-06-25 08:19:29,181][129146] Moving the mean solution point...
[36m[2023-06-25 08:19:38,876][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 08:19:38,877][129146] FPS: 396146.13
[36m[2023-06-25 08:19:38,879][129146] itr=794, itrs=2000, Progress: 39.70%
[36m[2023-06-25 08:19:50,410][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 08:19:50,410][129146] FPS: 333738.26
[36m[2023-06-25 08:19:55,385][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:19:55,386][129146] Reward + Measures: [[1382.50311154    0.49537835    0.26885501    0.35348967    0.24819568]]
[37m[1m[2023-06-25 08:19:55,386][129146] Max Reward on eval: 1382.503111535407
[37m[1m[2023-06-25 08:19:55,386][129146] Min Reward on eval: 1382.503111535407
[37m[1m[2023-06-25 08:19:55,386][129146] Mean Reward across all agents: 1382.503111535407
[37m[1m[2023-06-25 08:19:55,386][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:20:01,096][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:20:01,097][129146] Reward + Measures: [[ 807.5485993     0.45070001    0.33250001    0.40230003    0.26719999]
[37m[1m [1275.83150706    0.45430002    0.26519999    0.34200001    0.25620005]
[37m[1m [1288.93862349    0.52790004    0.2746        0.40550002    0.23729999]
[37m[1m ...
[37m[1m [1360.18273556    0.51260006    0.27199998    0.37689996    0.24460001]
[37m[1m [1088.29441268    0.5108        0.29159999    0.39260003    0.21940003]
[37m[1m [1316.97441242    0.50550002    0.2802        0.35870001    0.24349999]]
[37m[1m[2023-06-25 08:20:01,097][129146] Max Reward on eval: 1437.7311045214417
[37m[1m[2023-06-25 08:20:01,097][129146] Min Reward on eval: 718.7670355971902
[37m[1m[2023-06-25 08:20:01,097][129146] Mean Reward across all agents: 1167.1034522210348
[37m[1m[2023-06-25 08:20:01,098][129146] Average Trajectory Length: 999.4676666666667
[36m[2023-06-25 08:20:01,101][129146] mean_value=-23.693157214236276, max_value=406.6914882007868
[37m[1m[2023-06-25 08:20:01,103][129146] New mean coefficients: [[ 2.5756824  -1.7450505  -0.2707107  -0.7959538   0.29680842]]
[37m[1m[2023-06-25 08:20:01,104][129146] Moving the mean solution point...
[36m[2023-06-25 08:20:10,902][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 08:20:10,902][129146] FPS: 392007.92
[36m[2023-06-25 08:20:10,904][129146] itr=795, itrs=2000, Progress: 39.75%
[36m[2023-06-25 08:20:22,373][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 08:20:22,373][129146] FPS: 335496.00
[36m[2023-06-25 08:20:27,152][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:20:27,152][129146] Reward + Measures: [[1416.3141114     0.49080545    0.26649782    0.34063578    0.24866574]]
[37m[1m[2023-06-25 08:20:27,153][129146] Max Reward on eval: 1416.3141113988552
[37m[1m[2023-06-25 08:20:27,153][129146] Min Reward on eval: 1416.3141113988552
[37m[1m[2023-06-25 08:20:27,153][129146] Mean Reward across all agents: 1416.3141113988552
[37m[1m[2023-06-25 08:20:27,154][129146] Average Trajectory Length: 999.9883333333333
[36m[2023-06-25 08:20:32,663][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:20:32,664][129146] Reward + Measures: [[1020.12188033    0.6559        0.26640001    0.42859998    0.2184    ]
[37m[1m [1220.37714807    0.40900001    0.27309999    0.33150002    0.28929999]
[37m[1m [ 833.48833367    0.30090001    0.23410001    0.2458        0.1059    ]
[37m[1m ...
[37m[1m [1007.70723984    0.48540002    0.22220002    0.33059999    0.29569998]
[37m[1m [1194.83936398    0.35429999    0.26370001    0.29699999    0.2597    ]
[37m[1m [1434.23995279    0.43800002    0.26069999    0.31180003    0.30950001]]
[37m[1m[2023-06-25 08:20:32,664][129146] Max Reward on eval: 1513.3515560223138
[37m[1m[2023-06-25 08:20:32,665][129146] Min Reward on eval: 833.488333666214
[37m[1m[2023-06-25 08:20:32,665][129146] Mean Reward across all agents: 1275.080945637075
[37m[1m[2023-06-25 08:20:32,665][129146] Average Trajectory Length: 999.9706666666666
[36m[2023-06-25 08:20:32,670][129146] mean_value=56.07300475441907, max_value=766.4707333776687
[37m[1m[2023-06-25 08:20:32,673][129146] New mean coefficients: [[ 1.7708459  -1.6089383   0.4751163  -0.1738745  -0.12048388]]
[37m[1m[2023-06-25 08:20:32,674][129146] Moving the mean solution point...
[36m[2023-06-25 08:20:42,565][129146] train() took 9.89 seconds to complete
[36m[2023-06-25 08:20:42,566][129146] FPS: 388293.52
[36m[2023-06-25 08:20:42,568][129146] itr=796, itrs=2000, Progress: 39.80%
[36m[2023-06-25 08:20:54,239][129146] train() took 11.65 seconds to complete
[36m[2023-06-25 08:20:54,239][129146] FPS: 329631.16
[36m[2023-06-25 08:20:59,079][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:20:59,079][129146] Reward + Measures: [[1463.84228791    0.48726651    0.26526177    0.32912621    0.25653782]]
[37m[1m[2023-06-25 08:20:59,079][129146] Max Reward on eval: 1463.842287910612
[37m[1m[2023-06-25 08:20:59,080][129146] Min Reward on eval: 1463.842287910612
[37m[1m[2023-06-25 08:20:59,080][129146] Mean Reward across all agents: 1463.842287910612
[37m[1m[2023-06-25 08:20:59,080][129146] Average Trajectory Length: 999.9723333333333
[36m[2023-06-25 08:21:04,566][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:21:04,566][129146] Reward + Measures: [[1219.32255258    0.51389998    0.27870002    0.36840004    0.2798    ]
[37m[1m [1023.95494147    0.54549998    0.26760003    0.3827        0.27760002]
[37m[1m [1414.49742551    0.51620001    0.26730001    0.36090001    0.28220001]
[37m[1m ...
[37m[1m [1063.36189075    0.46650001    0.2947        0.35569999    0.2395    ]
[37m[1m [ 804.08851697    0.55129999    0.26980001    0.39850003    0.27220002]
[37m[1m [ 890.50820316    0.47869998    0.34010002    0.38600001    0.27239999]]
[37m[1m[2023-06-25 08:21:04,566][129146] Max Reward on eval: 1534.700737345242
[37m[1m[2023-06-25 08:21:04,567][129146] Min Reward on eval: 389.0183595207636
[37m[1m[2023-06-25 08:21:04,567][129146] Mean Reward across all agents: 1152.0571569630263
[37m[1m[2023-06-25 08:21:04,567][129146] Average Trajectory Length: 999.3726666666666
[36m[2023-06-25 08:21:04,570][129146] mean_value=-164.07789704321073, max_value=397.87450763385925
[37m[1m[2023-06-25 08:21:04,572][129146] New mean coefficients: [[ 2.345356   -1.2172109   0.7598623  -0.56631964  0.07129118]]
[37m[1m[2023-06-25 08:21:04,573][129146] Moving the mean solution point...
[36m[2023-06-25 08:21:14,371][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 08:21:14,371][129146] FPS: 392002.05
[36m[2023-06-25 08:21:14,373][129146] itr=797, itrs=2000, Progress: 39.85%
[36m[2023-06-25 08:21:26,020][129146] train() took 11.62 seconds to complete
[36m[2023-06-25 08:21:26,020][129146] FPS: 330369.98
[36m[2023-06-25 08:21:30,842][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:21:30,842][129146] Reward + Measures: [[1476.32514403    0.47731599    0.26755431    0.32769901    0.24733634]]
[37m[1m[2023-06-25 08:21:30,842][129146] Max Reward on eval: 1476.3251440319164
[37m[1m[2023-06-25 08:21:30,843][129146] Min Reward on eval: 1476.3251440319164
[37m[1m[2023-06-25 08:21:30,843][129146] Mean Reward across all agents: 1476.3251440319164
[37m[1m[2023-06-25 08:21:30,843][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:21:36,279][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:21:36,279][129146] Reward + Measures: [[1427.32482108    0.4858        0.27340001    0.3326        0.24190001]
[37m[1m [1351.21624402    0.51140004    0.29920003    0.34850001    0.32099998]
[37m[1m [1397.24442287    0.46569997    0.249         0.30889997    0.26029998]
[37m[1m ...
[37m[1m [1418.72549883    0.48010001    0.30410001    0.3522        0.3105    ]
[37m[1m [1478.85897665    0.491         0.28200001    0.32910004    0.27730003]
[37m[1m [1234.40424066    0.4461        0.25240001    0.3256        0.2597    ]]
[37m[1m[2023-06-25 08:21:36,280][129146] Max Reward on eval: 1548.351790913497
[37m[1m[2023-06-25 08:21:36,280][129146] Min Reward on eval: 557.9440541292715
[37m[1m[2023-06-25 08:21:36,280][129146] Mean Reward across all agents: 1279.456361969748
[37m[1m[2023-06-25 08:21:36,280][129146] Average Trajectory Length: 999.7523333333334
[36m[2023-06-25 08:21:36,283][129146] mean_value=-70.58296010666622, max_value=636.8736905679436
[37m[1m[2023-06-25 08:21:36,286][129146] New mean coefficients: [[ 1.9903715  -0.18673313  1.047975   -1.0011108  -0.4334843 ]]
[37m[1m[2023-06-25 08:21:36,287][129146] Moving the mean solution point...
[36m[2023-06-25 08:21:46,066][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 08:21:46,066][129146] FPS: 392726.12
[36m[2023-06-25 08:21:46,069][129146] itr=798, itrs=2000, Progress: 39.90%
[36m[2023-06-25 08:21:57,494][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 08:21:57,495][129146] FPS: 336753.43
[36m[2023-06-25 08:22:02,221][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:22:02,221][129146] Reward + Measures: [[1537.0580905     0.48393425    0.26644093    0.32140276    0.24761657]]
[37m[1m[2023-06-25 08:22:02,222][129146] Max Reward on eval: 1537.0580905033862
[37m[1m[2023-06-25 08:22:02,222][129146] Min Reward on eval: 1537.0580905033862
[37m[1m[2023-06-25 08:22:02,222][129146] Mean Reward across all agents: 1537.0580905033862
[37m[1m[2023-06-25 08:22:02,222][129146] Average Trajectory Length: 999.7636666666666
[36m[2023-06-25 08:22:07,557][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:22:07,563][129146] Reward + Measures: [[1459.98179169    0.53610003    0.26640001    0.31160003    0.2534    ]
[37m[1m [1444.02341292    0.4993        0.26869997    0.34260002    0.2563    ]
[37m[1m [1186.79799319    0.53149998    0.30890003    0.3759        0.23810001]
[37m[1m ...
[37m[1m [1446.99721492    0.48459998    0.25409999    0.29180002    0.25530002]
[37m[1m [1538.55432384    0.51140004    0.2615        0.30829999    0.2656    ]
[37m[1m [1365.82449836    0.48629999    0.26809999    0.34539998    0.24150001]]
[37m[1m[2023-06-25 08:22:07,563][129146] Max Reward on eval: 1621.807254314964
[37m[1m[2023-06-25 08:22:07,564][129146] Min Reward on eval: 282.9575532964547
[37m[1m[2023-06-25 08:22:07,564][129146] Mean Reward across all agents: 1181.0860182505753
[37m[1m[2023-06-25 08:22:07,564][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:22:07,567][129146] mean_value=-196.75284663716866, max_value=418.41567067883966
[37m[1m[2023-06-25 08:22:07,569][129146] New mean coefficients: [[ 1.7050369   0.86447155  0.61123854 -1.0744689  -0.02540019]]
[37m[1m[2023-06-25 08:22:07,570][129146] Moving the mean solution point...
[36m[2023-06-25 08:22:17,184][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 08:22:17,184][129146] FPS: 399489.10
[36m[2023-06-25 08:22:17,187][129146] itr=799, itrs=2000, Progress: 39.95%
[36m[2023-06-25 08:22:28,596][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 08:22:28,596][129146] FPS: 337309.53
[36m[2023-06-25 08:22:33,380][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:22:33,381][129146] Reward + Measures: [[1594.00863403    0.49271053    0.2665576     0.31811982    0.24410471]]
[37m[1m[2023-06-25 08:22:33,381][129146] Max Reward on eval: 1594.0086340311273
[37m[1m[2023-06-25 08:22:33,381][129146] Min Reward on eval: 1594.0086340311273
[37m[1m[2023-06-25 08:22:33,381][129146] Mean Reward across all agents: 1594.0086340311273
[37m[1m[2023-06-25 08:22:33,381][129146] Average Trajectory Length: 999.9486666666667
[36m[2023-06-25 08:22:39,034][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:22:39,035][129146] Reward + Measures: [[1520.158757      0.49890003    0.27320001    0.34460002    0.2454    ]
[37m[1m [1391.02684293    0.4948        0.27960002    0.34110001    0.23450001]
[37m[1m [1424.71783644    0.43730003    0.26719999    0.31570002    0.25350001]
[37m[1m ...
[37m[1m [1395.12783574    0.51980001    0.26370001    0.31720003    0.2309    ]
[37m[1m [1347.64052699    0.48129997    0.26770002    0.3089        0.23889999]
[37m[1m [1285.62744401    0.41          0.2481        0.28390002    0.2102    ]]
[37m[1m[2023-06-25 08:22:39,035][129146] Max Reward on eval: 1697.098009075236
[37m[1m[2023-06-25 08:22:39,035][129146] Min Reward on eval: 1046.2609401339082
[37m[1m[2023-06-25 08:22:39,035][129146] Mean Reward across all agents: 1425.7102823131988
[37m[1m[2023-06-25 08:22:39,036][129146] Average Trajectory Length: 999.7246666666666
[36m[2023-06-25 08:22:39,039][129146] mean_value=-14.918085473684059, max_value=539.2004546829743
[37m[1m[2023-06-25 08:22:39,042][129146] New mean coefficients: [[ 1.1357735   1.1166449   0.49954376 -0.90149367 -0.10410857]]
[37m[1m[2023-06-25 08:22:39,043][129146] Moving the mean solution point...
[36m[2023-06-25 08:22:48,621][129146] train() took 9.58 seconds to complete
[36m[2023-06-25 08:22:48,621][129146] FPS: 401003.82
[36m[2023-06-25 08:22:48,623][129146] itr=800, itrs=2000, Progress: 40.00%
[37m[1m[2023-06-25 08:22:54,873][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000780
[36m[2023-06-25 08:23:06,601][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 08:23:06,601][129146] FPS: 334468.00
[36m[2023-06-25 08:23:11,464][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:23:11,464][129146] Reward + Measures: [[1679.00155857    0.50715667    0.26411632    0.31018034    0.24568634]]
[37m[1m[2023-06-25 08:23:11,464][129146] Max Reward on eval: 1679.0015585719755
[37m[1m[2023-06-25 08:23:11,465][129146] Min Reward on eval: 1679.0015585719755
[37m[1m[2023-06-25 08:23:11,465][129146] Mean Reward across all agents: 1679.0015585719755
[37m[1m[2023-06-25 08:23:11,465][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:23:16,941][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:23:16,942][129146] Reward + Measures: [[1445.03328358    0.5783        0.27719998    0.3436        0.2465    ]
[37m[1m [1646.27011468    0.48930001    0.2728        0.31479999    0.24940002]
[37m[1m [1648.65650296    0.54049999    0.2714        0.33660004    0.2392    ]
[37m[1m ...
[37m[1m [1332.53290501    0.46469998    0.2791        0.30399999    0.2225    ]
[37m[1m [1196.94746629    0.46809998    0.2832        0.35560003    0.23110001]
[37m[1m [1010.07249149    0.60720003    0.2617        0.36190006    0.3055    ]]
[37m[1m[2023-06-25 08:23:16,942][129146] Max Reward on eval: 1714.477962494432
[37m[1m[2023-06-25 08:23:16,942][129146] Min Reward on eval: 778.4910888473853
[37m[1m[2023-06-25 08:23:16,942][129146] Mean Reward across all agents: 1455.6974855728563
[37m[1m[2023-06-25 08:23:16,943][129146] Average Trajectory Length: 999.8386666666667
[36m[2023-06-25 08:23:16,947][129146] mean_value=94.83613176645977, max_value=827.2589581722688
[37m[1m[2023-06-25 08:23:16,949][129146] New mean coefficients: [[ 0.41756463  2.2595708   0.02497038 -1.617971   -0.16137794]]
[37m[1m[2023-06-25 08:23:16,950][129146] Moving the mean solution point...
[36m[2023-06-25 08:23:26,684][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 08:23:26,684][129146] FPS: 394580.43
[36m[2023-06-25 08:23:26,687][129146] itr=801, itrs=2000, Progress: 40.05%
[36m[2023-06-25 08:23:38,209][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 08:23:38,209][129146] FPS: 333919.83
[36m[2023-06-25 08:23:42,995][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:23:42,995][129146] Reward + Measures: [[1731.28916982    0.53766602    0.26233733    0.311088      0.24804732]]
[37m[1m[2023-06-25 08:23:42,995][129146] Max Reward on eval: 1731.2891698237838
[37m[1m[2023-06-25 08:23:42,996][129146] Min Reward on eval: 1731.2891698237838
[37m[1m[2023-06-25 08:23:42,996][129146] Mean Reward across all agents: 1731.2891698237838
[37m[1m[2023-06-25 08:23:42,996][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:23:48,387][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:23:48,387][129146] Reward + Measures: [[ 611.82792314    0.55220002    0.34090003    0.40149999    0.21510001]
[37m[1m [1650.7000981     0.57450002    0.26070002    0.30700001    0.229     ]
[37m[1m [1488.23003108    0.57420003    0.27540001    0.33080003    0.21170001]
[37m[1m ...
[37m[1m [1112.99420899    0.57402879    0.28350794    0.34726712    0.18592086]
[37m[1m [1101.48633825    0.57419997    0.28569999    0.377         0.24370001]
[37m[1m [1497.28007974    0.54700005    0.26069999    0.317         0.23410001]]
[37m[1m[2023-06-25 08:23:48,387][129146] Max Reward on eval: 1757.8376878645504
[37m[1m[2023-06-25 08:23:48,388][129146] Min Reward on eval: 63.263143281982046
[37m[1m[2023-06-25 08:23:48,388][129146] Mean Reward across all agents: 1159.2515087598515
[37m[1m[2023-06-25 08:23:48,388][129146] Average Trajectory Length: 999.4133333333333
[36m[2023-06-25 08:23:48,391][129146] mean_value=-173.4820696683439, max_value=573.3316141781697
[37m[1m[2023-06-25 08:23:48,394][129146] New mean coefficients: [[ 0.08473668  2.476175   -0.02862577 -1.3197279  -0.1644682 ]]
[37m[1m[2023-06-25 08:23:48,395][129146] Moving the mean solution point...
[36m[2023-06-25 08:23:58,150][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 08:23:58,150][129146] FPS: 393717.50
[36m[2023-06-25 08:23:58,153][129146] itr=802, itrs=2000, Progress: 40.10%
[36m[2023-06-25 08:24:09,658][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 08:24:09,659][129146] FPS: 334506.22
[36m[2023-06-25 08:24:14,428][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:24:14,428][129146] Reward + Measures: [[1785.34626544    0.55697435    0.25620764    0.29710233    0.24655966]]
[37m[1m[2023-06-25 08:24:14,428][129146] Max Reward on eval: 1785.3462654448822
[37m[1m[2023-06-25 08:24:14,428][129146] Min Reward on eval: 1785.3462654448822
[37m[1m[2023-06-25 08:24:14,429][129146] Mean Reward across all agents: 1785.3462654448822
[37m[1m[2023-06-25 08:24:14,429][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:24:19,907][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:24:19,907][129146] Reward + Measures: [[1415.24611314    0.5923        0.27680001    0.3267        0.24920002]
[37m[1m [1516.76994499    0.5848        0.25889999    0.31          0.28290001]
[37m[1m [1567.96026994    0.58399999    0.26770002    0.32600001    0.23380001]
[37m[1m ...
[37m[1m [1502.66949447    0.59120005    0.26199999    0.32140002    0.25060001]
[37m[1m [1613.15817303    0.58109999    0.26800001    0.31969997    0.25620002]
[37m[1m [1361.09080539    0.59549999    0.2638        0.31960002    0.27260002]]
[37m[1m[2023-06-25 08:24:19,908][129146] Max Reward on eval: 1852.6036393161398
[37m[1m[2023-06-25 08:24:19,908][129146] Min Reward on eval: 1018.7064838664606
[37m[1m[2023-06-25 08:24:19,908][129146] Mean Reward across all agents: 1561.4698593253113
[37m[1m[2023-06-25 08:24:19,908][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:24:19,912][129146] mean_value=18.361504328006717, max_value=1404.8894880327855
[37m[1m[2023-06-25 08:24:19,915][129146] New mean coefficients: [[ 0.30770242  0.9174684   0.19034776 -0.8958676   0.1956605 ]]
[37m[1m[2023-06-25 08:24:19,916][129146] Moving the mean solution point...
[36m[2023-06-25 08:24:29,632][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 08:24:29,632][129146] FPS: 395301.75
[36m[2023-06-25 08:24:29,634][129146] itr=803, itrs=2000, Progress: 40.15%
[36m[2023-06-25 08:24:41,095][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 08:24:41,096][129146] FPS: 335666.95
[36m[2023-06-25 08:24:45,832][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:24:45,832][129146] Reward + Measures: [[1855.74106108    0.56826568    0.25446764    0.29047367    0.24935699]]
[37m[1m[2023-06-25 08:24:45,832][129146] Max Reward on eval: 1855.7410610829663
[37m[1m[2023-06-25 08:24:45,832][129146] Min Reward on eval: 1855.7410610829663
[37m[1m[2023-06-25 08:24:45,833][129146] Mean Reward across all agents: 1855.7410610829663
[37m[1m[2023-06-25 08:24:45,833][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:24:51,483][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:24:51,484][129146] Reward + Measures: [[1815.76617077    0.55500001    0.24340001    0.29359999    0.27360001]
[37m[1m [1555.51151145    0.56269997    0.2827        0.33920002    0.25850001]
[37m[1m [1611.84604595    0.5535        0.26719999    0.32870001    0.22879998]
[37m[1m ...
[37m[1m [1814.19098303    0.54180002    0.25509998    0.28590003    0.26770002]
[37m[1m [1875.80812053    0.56050003    0.25460002    0.29539999    0.2471    ]
[37m[1m [1833.28609248    0.5722        0.25150001    0.2983        0.2617    ]]
[37m[1m[2023-06-25 08:24:51,484][129146] Max Reward on eval: 1933.0509831885574
[37m[1m[2023-06-25 08:24:51,484][129146] Min Reward on eval: 1242.848506123689
[37m[1m[2023-06-25 08:24:51,485][129146] Mean Reward across all agents: 1705.232786376856
[37m[1m[2023-06-25 08:24:51,485][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:24:51,489][129146] mean_value=87.01110485434806, max_value=1527.706026524845
[37m[1m[2023-06-25 08:24:51,492][129146] New mean coefficients: [[ 0.02511483  0.8316605  -0.23938093 -0.65059036  0.57420033]]
[37m[1m[2023-06-25 08:24:51,493][129146] Moving the mean solution point...
[36m[2023-06-25 08:25:01,274][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 08:25:01,274][129146] FPS: 392680.37
[36m[2023-06-25 08:25:01,276][129146] itr=804, itrs=2000, Progress: 40.20%
[36m[2023-06-25 08:25:12,861][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 08:25:12,861][129146] FPS: 332198.84
[36m[2023-06-25 08:25:17,611][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:25:17,611][129146] Reward + Measures: [[1912.59700018    0.58152699    0.25144035    0.28240633    0.25558397]]
[37m[1m[2023-06-25 08:25:17,611][129146] Max Reward on eval: 1912.5970001787919
[37m[1m[2023-06-25 08:25:17,611][129146] Min Reward on eval: 1912.5970001787919
[37m[1m[2023-06-25 08:25:17,612][129146] Mean Reward across all agents: 1912.5970001787919
[37m[1m[2023-06-25 08:25:17,612][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:25:23,058][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:25:23,059][129146] Reward + Measures: [[1766.92120528    0.55949998    0.25929999    0.30990002    0.24839997]
[37m[1m [1611.65988995    0.56510001    0.25139999    0.28940001    0.28270003]
[37m[1m [1687.60578688    0.57250005    0.23809998    0.28000003    0.27870002]
[37m[1m ...
[37m[1m [1808.58826696    0.56580001    0.24980001    0.28819999    0.26160002]
[37m[1m [1857.22581075    0.56510001    0.25910002    0.30860001    0.2457    ]
[37m[1m [1794.1161169     0.5783        0.2599        0.30420002    0.2392    ]]
[37m[1m[2023-06-25 08:25:23,059][129146] Max Reward on eval: 1946.748834894388
[37m[1m[2023-06-25 08:25:23,060][129146] Min Reward on eval: 1285.6410870436114
[37m[1m[2023-06-25 08:25:23,060][129146] Mean Reward across all agents: 1750.5103626922378
[37m[1m[2023-06-25 08:25:23,060][129146] Average Trajectory Length: 999.838
[36m[2023-06-25 08:25:23,063][129146] mean_value=37.41531894725642, max_value=1081.4627840013507
[37m[1m[2023-06-25 08:25:23,066][129146] New mean coefficients: [[ 0.1538251   0.16550231 -0.41027203 -0.29556566  0.32503432]]
[37m[1m[2023-06-25 08:25:23,067][129146] Moving the mean solution point...
[36m[2023-06-25 08:25:32,785][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 08:25:32,785][129146] FPS: 395224.36
[36m[2023-06-25 08:25:32,787][129146] itr=805, itrs=2000, Progress: 40.25%
[36m[2023-06-25 08:25:44,484][129146] train() took 11.67 seconds to complete
[36m[2023-06-25 08:25:44,485][129146] FPS: 328944.53
[36m[2023-06-25 08:25:49,239][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:25:49,239][129146] Reward + Measures: [[1947.7834246     0.59129864    0.24552466    0.27508467    0.25950333]]
[37m[1m[2023-06-25 08:25:49,239][129146] Max Reward on eval: 1947.7834245988704
[37m[1m[2023-06-25 08:25:49,239][129146] Min Reward on eval: 1947.7834245988704
[37m[1m[2023-06-25 08:25:49,240][129146] Mean Reward across all agents: 1947.7834245988704
[37m[1m[2023-06-25 08:25:49,240][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:25:54,685][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:25:54,686][129146] Reward + Measures: [[1130.83245008    0.48269996    0.235         0.31830001    0.184     ]
[37m[1m [1870.99602554    0.5765        0.2509        0.27600002    0.26120001]
[37m[1m [1668.67478599    0.55470002    0.2807        0.31529999    0.2852    ]
[37m[1m ...
[37m[1m [1884.46862063    0.58840001    0.25220001    0.2843        0.24520002]
[37m[1m [1751.05449123    0.56479996    0.27930003    0.29370001    0.28490001]
[37m[1m [1597.74772608    0.57569999    0.24609999    0.29140002    0.2445    ]]
[37m[1m[2023-06-25 08:25:54,686][129146] Max Reward on eval: 1985.5120144266402
[37m[1m[2023-06-25 08:25:54,686][129146] Min Reward on eval: 386.1856070122682
[37m[1m[2023-06-25 08:25:54,686][129146] Mean Reward across all agents: 1540.9769566743735
[37m[1m[2023-06-25 08:25:54,687][129146] Average Trajectory Length: 999.8343333333333
[36m[2023-06-25 08:25:54,690][129146] mean_value=-81.11464632901442, max_value=918.4697297293314
[37m[1m[2023-06-25 08:25:54,693][129146] New mean coefficients: [[ 0.6417713  -0.40937477 -0.07678214 -0.04983044 -0.07704607]]
[37m[1m[2023-06-25 08:25:54,694][129146] Moving the mean solution point...
[36m[2023-06-25 08:26:04,522][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 08:26:04,522][129146] FPS: 390776.66
[36m[2023-06-25 08:26:04,524][129146] itr=806, itrs=2000, Progress: 40.30%
[36m[2023-06-25 08:26:16,209][129146] train() took 11.66 seconds to complete
[36m[2023-06-25 08:26:16,209][129146] FPS: 329256.43
[36m[2023-06-25 08:26:21,059][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:26:21,059][129146] Reward + Measures: [[2014.36822502    0.58371502    0.24171999    0.26834899    0.26127031]]
[37m[1m[2023-06-25 08:26:21,060][129146] Max Reward on eval: 2014.3682250231996
[37m[1m[2023-06-25 08:26:21,060][129146] Min Reward on eval: 2014.3682250231996
[37m[1m[2023-06-25 08:26:21,060][129146] Mean Reward across all agents: 2014.3682250231996
[37m[1m[2023-06-25 08:26:21,060][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:26:26,482][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:26:26,483][129146] Reward + Measures: [[1490.67016669    0.49329996    0.25030002    0.28049999    0.3114    ]
[37m[1m [1309.99153715    0.61879998    0.2789        0.32710001    0.32140002]
[37m[1m [1568.57925482    0.48920003    0.26250002    0.30300003    0.22210002]
[37m[1m ...
[37m[1m [1953.23973506    0.56220001    0.2502        0.27710003    0.23910001]
[37m[1m [1563.36584845    0.55649996    0.2678        0.2949        0.3272    ]
[37m[1m [1924.96596677    0.59119999    0.2525        0.2832        0.25059998]]
[37m[1m[2023-06-25 08:26:26,483][129146] Max Reward on eval: 2050.0264957719482
[37m[1m[2023-06-25 08:26:26,484][129146] Min Reward on eval: 415.49674869731825
[37m[1m[2023-06-25 08:26:26,484][129146] Mean Reward across all agents: 1648.9889013017532
[37m[1m[2023-06-25 08:26:26,484][129146] Average Trajectory Length: 999.5226666666666
[36m[2023-06-25 08:26:26,488][129146] mean_value=-30.537515541980746, max_value=2440.6577360599067
[37m[1m[2023-06-25 08:26:26,491][129146] New mean coefficients: [[ 0.67899483 -0.3439838  -0.5432055  -0.08295508 -0.02963354]]
[37m[1m[2023-06-25 08:26:26,492][129146] Moving the mean solution point...
[36m[2023-06-25 08:26:36,275][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 08:26:36,275][129146] FPS: 392565.32
[36m[2023-06-25 08:26:36,277][129146] itr=807, itrs=2000, Progress: 40.35%
[36m[2023-06-25 08:26:47,933][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 08:26:47,934][129146] FPS: 330066.51
[36m[2023-06-25 08:26:52,805][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:26:52,805][129146] Reward + Measures: [[2128.70316192    0.58081532    0.23846565    0.264752      0.26021096]]
[37m[1m[2023-06-25 08:26:52,805][129146] Max Reward on eval: 2128.703161916573
[37m[1m[2023-06-25 08:26:52,805][129146] Min Reward on eval: 2128.703161916573
[37m[1m[2023-06-25 08:26:52,806][129146] Mean Reward across all agents: 2128.703161916573
[37m[1m[2023-06-25 08:26:52,806][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:26:58,424][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:26:58,425][129146] Reward + Measures: [[2000.10298043    0.60000002    0.2368        0.27379999    0.287     ]
[37m[1m [1927.121768      0.59249997    0.2397        0.28220001    0.25700003]
[37m[1m [1947.24447603    0.53459996    0.22880001    0.25850004    0.2615    ]
[37m[1m ...
[37m[1m [1907.6494074     0.58570004    0.234         0.27669999    0.27060002]
[37m[1m [1943.15616969    0.56040001    0.2309        0.27700004    0.26470003]
[37m[1m [2116.8281119     0.57069999    0.23670001    0.27470002    0.2494    ]]
[37m[1m[2023-06-25 08:26:58,425][129146] Max Reward on eval: 2224.6411400644806
[37m[1m[2023-06-25 08:26:58,425][129146] Min Reward on eval: 1404.7584542532627
[37m[1m[2023-06-25 08:26:58,426][129146] Mean Reward across all agents: 1938.392196667265
[37m[1m[2023-06-25 08:26:58,426][129146] Average Trajectory Length: 999.9173333333333
[36m[2023-06-25 08:26:58,430][129146] mean_value=74.5065411754544, max_value=2092.78245934794
[37m[1m[2023-06-25 08:26:58,433][129146] New mean coefficients: [[ 0.5421123  -0.32999766 -0.23943591  0.2397097  -0.08532108]]
[37m[1m[2023-06-25 08:26:58,434][129146] Moving the mean solution point...
[36m[2023-06-25 08:27:08,260][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 08:27:08,260][129146] FPS: 390853.58
[36m[2023-06-25 08:27:08,263][129146] itr=808, itrs=2000, Progress: 40.40%
[36m[2023-06-25 08:27:19,724][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 08:27:19,724][129146] FPS: 335713.94
[36m[2023-06-25 08:27:24,591][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:27:24,592][129146] Reward + Measures: [[2253.74806711    0.57672071    0.23251595    0.25950623    0.26355329]]
[37m[1m[2023-06-25 08:27:24,592][129146] Max Reward on eval: 2253.7480671075145
[37m[1m[2023-06-25 08:27:24,592][129146] Min Reward on eval: 2253.7480671075145
[37m[1m[2023-06-25 08:27:24,593][129146] Mean Reward across all agents: 2253.7480671075145
[37m[1m[2023-06-25 08:27:24,593][129146] Average Trajectory Length: 999.6683333333333
[36m[2023-06-25 08:27:30,180][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:27:30,181][129146] Reward + Measures: [[1721.42351945    0.5302        0.24330001    0.28080001    0.22710001]
[37m[1m [2168.76911557    0.59200001    0.24039999    0.2728        0.24560001]
[37m[1m [1108.81372601    0.58780003    0.2976        0.4364        0.30340001]
[37m[1m ...
[37m[1m [1727.5576918     0.53730005    0.2383        0.26270005    0.2041    ]
[37m[1m [1234.46994087    0.55470002    0.2832        0.29150003    0.17580001]
[37m[1m [1334.17796755    0.54290003    0.29449999    0.3651        0.34379998]]
[37m[1m[2023-06-25 08:27:30,181][129146] Max Reward on eval: 2321.4729532482916
[37m[1m[2023-06-25 08:27:30,181][129146] Min Reward on eval: 780.9312812036602
[37m[1m[2023-06-25 08:27:30,182][129146] Mean Reward across all agents: 1828.6952034567398
[37m[1m[2023-06-25 08:27:30,182][129146] Average Trajectory Length: 999.6246666666666
[36m[2023-06-25 08:27:30,186][129146] mean_value=37.35504278451095, max_value=2318.738816736778
[37m[1m[2023-06-25 08:27:30,189][129146] New mean coefficients: [[ 0.47242567 -0.03989586 -0.35851204 -0.23787776 -0.03274363]]
[37m[1m[2023-06-25 08:27:30,190][129146] Moving the mean solution point...
[36m[2023-06-25 08:27:39,947][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 08:27:39,948][129146] FPS: 393609.09
[36m[2023-06-25 08:27:39,950][129146] itr=809, itrs=2000, Progress: 40.45%
[36m[2023-06-25 08:27:51,383][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 08:27:51,383][129146] FPS: 336510.20
[36m[2023-06-25 08:27:56,251][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:27:56,251][129146] Reward + Measures: [[2369.25049087    0.57701933    0.23113267    0.25720567    0.26435533]]
[37m[1m[2023-06-25 08:27:56,251][129146] Max Reward on eval: 2369.2504908685646
[37m[1m[2023-06-25 08:27:56,252][129146] Min Reward on eval: 2369.2504908685646
[37m[1m[2023-06-25 08:27:56,252][129146] Mean Reward across all agents: 2369.2504908685646
[37m[1m[2023-06-25 08:27:56,252][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:28:01,730][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:28:01,731][129146] Reward + Measures: [[1482.67998496    0.5966        0.28000003    0.37240002    0.23870002]
[37m[1m [ 735.10130513    0.67070001    0.2624        0.44029999    0.1998    ]
[37m[1m [ 482.80125293    0.5607        0.20739999    0.29249999    0.2694    ]
[37m[1m ...
[37m[1m [ 194.0784561     0.52430004    0.1895        0.26670003    0.28070003]
[37m[1m [1822.74138982    0.52399999    0.2674        0.30880001    0.2527    ]
[37m[1m [ 895.66326268    0.57839996    0.30949998    0.43000004    0.2174    ]]
[37m[1m[2023-06-25 08:28:01,731][129146] Max Reward on eval: 2328.2778250978563
[37m[1m[2023-06-25 08:28:01,732][129146] Min Reward on eval: -486.9727766696713
[37m[1m[2023-06-25 08:28:01,732][129146] Mean Reward across all agents: 1101.6871491747702
[37m[1m[2023-06-25 08:28:01,732][129146] Average Trajectory Length: 995.4169999999999
[36m[2023-06-25 08:28:01,737][129146] mean_value=-156.2079519257114, max_value=1876.873707900758
[37m[1m[2023-06-25 08:28:01,740][129146] New mean coefficients: [[ 0.07857528  0.38661322 -0.50990564  0.1254367  -0.04301079]]
[37m[1m[2023-06-25 08:28:01,740][129146] Moving the mean solution point...
[36m[2023-06-25 08:28:11,583][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 08:28:11,583][129146] FPS: 390226.87
[36m[2023-06-25 08:28:11,585][129146] itr=810, itrs=2000, Progress: 40.50%
[37m[1m[2023-06-25 08:28:17,940][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000790
[36m[2023-06-25 08:28:29,614][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 08:28:29,614][129146] FPS: 335787.16
[36m[2023-06-25 08:28:34,341][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:28:34,341][129146] Reward + Measures: [[2486.70147368    0.6024487     0.22776997    0.25430134    0.25901303]]
[37m[1m[2023-06-25 08:28:34,342][129146] Max Reward on eval: 2486.7014736790675
[37m[1m[2023-06-25 08:28:34,342][129146] Min Reward on eval: 2486.7014736790675
[37m[1m[2023-06-25 08:28:34,342][129146] Mean Reward across all agents: 2486.7014736790675
[37m[1m[2023-06-25 08:28:34,342][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:28:39,700][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:28:39,706][129146] Reward + Measures: [[1531.94698374    0.54249996    0.24039999    0.31030002    0.3407    ]
[37m[1m [1437.07872738    0.57050008    0.22800003    0.33880001    0.27449998]
[37m[1m [1585.42313097    0.53479999    0.23840001    0.29340002    0.24090002]
[37m[1m ...
[37m[1m [1666.96412885    0.53140002    0.21640001    0.2746        0.30090004]
[37m[1m [1632.14029317    0.51850003    0.23110001    0.28590003    0.31850001]
[37m[1m [  83.16587664    0.48000002    0.2595        0.3601        0.18460001]]
[37m[1m[2023-06-25 08:28:39,706][129146] Max Reward on eval: 2500.558960321988
[37m[1m[2023-06-25 08:28:39,706][129146] Min Reward on eval: 83.16587663823739
[37m[1m[2023-06-25 08:28:39,706][129146] Mean Reward across all agents: 1641.3898589821931
[37m[1m[2023-06-25 08:28:39,707][129146] Average Trajectory Length: 999.0616666666666
[36m[2023-06-25 08:28:39,710][129146] mean_value=-187.90703989223624, max_value=2179.357785143956
[37m[1m[2023-06-25 08:28:39,712][129146] New mean coefficients: [[ 0.02696576  0.4783072  -0.7148312  -0.2805421  -0.10885221]]
[37m[1m[2023-06-25 08:28:39,713][129146] Moving the mean solution point...
[36m[2023-06-25 08:28:49,253][129146] train() took 9.54 seconds to complete
[36m[2023-06-25 08:28:49,253][129146] FPS: 402599.57
[36m[2023-06-25 08:28:49,255][129146] itr=811, itrs=2000, Progress: 40.55%
[36m[2023-06-25 08:29:00,802][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 08:29:00,803][129146] FPS: 333206.77
[36m[2023-06-25 08:29:05,647][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:29:05,647][129146] Reward + Measures: [[2545.79564064    0.63482732    0.22166248    0.24781066    0.26525903]]
[37m[1m[2023-06-25 08:29:05,648][129146] Max Reward on eval: 2545.795640635414
[37m[1m[2023-06-25 08:29:05,648][129146] Min Reward on eval: 2545.795640635414
[37m[1m[2023-06-25 08:29:05,648][129146] Mean Reward across all agents: 2545.795640635414
[37m[1m[2023-06-25 08:29:05,648][129146] Average Trajectory Length: 999.4236666666666
[36m[2023-06-25 08:29:11,012][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:29:11,012][129146] Reward + Measures: [[1309.56690313    0.70760006    0.3055        0.39320001    0.18620001]
[37m[1m [2077.42379125    0.60720003    0.2572        0.27090001    0.24299999]
[37m[1m [2207.79546984    0.65679997    0.25010002    0.26860002    0.22850001]
[37m[1m ...
[37m[1m [2471.49079636    0.65339994    0.23140001    0.25410002    0.24829999]
[37m[1m [1766.82239205    0.67550009    0.27849999    0.3231        0.20630001]
[37m[1m [1411.08107311    0.6832        0.29000002    0.37810001    0.17569999]]
[37m[1m[2023-06-25 08:29:11,012][129146] Max Reward on eval: 2566.060790588241
[37m[1m[2023-06-25 08:29:11,013][129146] Min Reward on eval: 245.0229054241383
[37m[1m[2023-06-25 08:29:11,013][129146] Mean Reward across all agents: 1697.0308669470471
[37m[1m[2023-06-25 08:29:11,013][129146] Average Trajectory Length: 999.3866666666667
[36m[2023-06-25 08:29:11,018][129146] mean_value=348.7175858122509, max_value=2899.5875933098373
[37m[1m[2023-06-25 08:29:11,021][129146] New mean coefficients: [[-0.326859    1.2518094  -0.7802793  -0.2222193   0.02373378]]
[37m[1m[2023-06-25 08:29:11,022][129146] Moving the mean solution point...
[36m[2023-06-25 08:29:20,779][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 08:29:20,779][129146] FPS: 393644.12
[36m[2023-06-25 08:29:20,781][129146] itr=812, itrs=2000, Progress: 40.60%
[36m[2023-06-25 08:29:32,331][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 08:29:32,332][129146] FPS: 333096.53
[36m[2023-06-25 08:29:37,132][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:29:37,133][129146] Reward + Measures: [[2278.38121636    0.67896003    0.22225501    0.24993266    0.28002965]]
[37m[1m[2023-06-25 08:29:37,133][129146] Max Reward on eval: 2278.3812163604084
[37m[1m[2023-06-25 08:29:37,133][129146] Min Reward on eval: 2278.3812163604084
[37m[1m[2023-06-25 08:29:37,133][129146] Mean Reward across all agents: 2278.3812163604084
[37m[1m[2023-06-25 08:29:37,134][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:29:42,570][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:29:42,571][129146] Reward + Measures: [[1818.79077136    0.708         0.22819999    0.27939999    0.29589999]
[37m[1m [2244.00269032    0.64679998    0.22070001    0.24419999    0.28      ]
[37m[1m [2154.93528127    0.65029997    0.22120002    0.24330001    0.31909999]
[37m[1m ...
[37m[1m [ 896.1207108     0.75780004    0.28169999    0.42050001    0.17839999]
[37m[1m [1894.12941724    0.58939999    0.21040002    0.234         0.35670003]
[37m[1m [1974.02380689    0.59119999    0.21630001    0.24100001    0.3461    ]]
[37m[1m[2023-06-25 08:29:42,571][129146] Max Reward on eval: 2473.6113238504854
[37m[1m[2023-06-25 08:29:42,571][129146] Min Reward on eval: 895.2770660677227
[37m[1m[2023-06-25 08:29:42,572][129146] Mean Reward across all agents: 1872.3675318095577
[37m[1m[2023-06-25 08:29:42,572][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:29:42,576][129146] mean_value=697.5448655979195, max_value=2555.0476959847474
[37m[1m[2023-06-25 08:29:42,579][129146] New mean coefficients: [[ 0.00426179  1.7378082  -0.84017575 -0.31211507  0.21275084]]
[37m[1m[2023-06-25 08:29:42,580][129146] Moving the mean solution point...
[36m[2023-06-25 08:29:52,380][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 08:29:52,380][129146] FPS: 391912.10
[36m[2023-06-25 08:29:52,382][129146] itr=813, itrs=2000, Progress: 40.65%
[36m[2023-06-25 08:30:03,949][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 08:30:03,949][129146] FPS: 332621.52
[36m[2023-06-25 08:30:08,775][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:30:08,775][129146] Reward + Measures: [[2273.24309863    0.71249104    0.22088166    0.25174233    0.26848099]]
[37m[1m[2023-06-25 08:30:08,775][129146] Max Reward on eval: 2273.2430986257873
[37m[1m[2023-06-25 08:30:08,776][129146] Min Reward on eval: 2273.2430986257873
[37m[1m[2023-06-25 08:30:08,776][129146] Mean Reward across all agents: 2273.2430986257873
[37m[1m[2023-06-25 08:30:08,776][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:30:14,502][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:30:14,502][129146] Reward + Measures: [[1755.56331423    0.70539999    0.2527        0.28289998    0.28120002]
[37m[1m [2222.89145067    0.63700002    0.2177        0.24990001    0.27710003]
[37m[1m [1858.7006236     0.70629996    0.2379        0.26890001    0.27790001]
[37m[1m ...
[37m[1m [2008.44250637    0.70360005    0.22330001    0.25119999    0.28730002]
[37m[1m [1495.99844075    0.70880002    0.28299999    0.31740004    0.27040002]
[37m[1m [1552.17999441    0.70609999    0.2782        0.31689999    0.24780002]]
[37m[1m[2023-06-25 08:30:14,503][129146] Max Reward on eval: 2328.46421632485
[37m[1m[2023-06-25 08:30:14,503][129146] Min Reward on eval: 722.3240997919966
[37m[1m[2023-06-25 08:30:14,503][129146] Mean Reward across all agents: 1740.7723780839683
[37m[1m[2023-06-25 08:30:14,503][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:30:14,507][129146] mean_value=-115.14459037429833, max_value=1349.310313472972
[37m[1m[2023-06-25 08:30:14,510][129146] New mean coefficients: [[ 0.03927217  1.396942   -0.4003149  -0.3431912   0.4287907 ]]
[37m[1m[2023-06-25 08:30:14,511][129146] Moving the mean solution point...
[36m[2023-06-25 08:30:24,405][129146] train() took 9.89 seconds to complete
[36m[2023-06-25 08:30:24,406][129146] FPS: 388158.08
[36m[2023-06-25 08:30:24,408][129146] itr=814, itrs=2000, Progress: 40.70%
[36m[2023-06-25 08:30:35,940][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 08:30:35,941][129146] FPS: 333604.99
[36m[2023-06-25 08:30:40,777][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:30:40,777][129146] Reward + Measures: [[2372.35810413    0.73685658    0.21057935    0.24215433    0.28241467]]
[37m[1m[2023-06-25 08:30:40,777][129146] Max Reward on eval: 2372.3581041269435
[37m[1m[2023-06-25 08:30:40,778][129146] Min Reward on eval: 2372.3581041269435
[37m[1m[2023-06-25 08:30:40,778][129146] Mean Reward across all agents: 2372.3581041269435
[37m[1m[2023-06-25 08:30:40,778][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:30:46,306][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:30:46,307][129146] Reward + Measures: [[2125.27116145    0.66120005    0.24389999    0.2863        0.2158    ]
[37m[1m [2186.8046001     0.69499999    0.22409999    0.25890002    0.2723    ]
[37m[1m [1485.94679036    0.59790003    0.2665        0.3348        0.19039999]
[37m[1m ...
[37m[1m [2363.54543825    0.69310004    0.21780001    0.26860002    0.23      ]
[37m[1m [2186.96967536    0.67830002    0.2349        0.27350003    0.2192    ]
[37m[1m [2305.97587868    0.6868        0.22119999    0.2674        0.23530002]]
[37m[1m[2023-06-25 08:30:46,307][129146] Max Reward on eval: 2571.9996804194757
[37m[1m[2023-06-25 08:30:46,307][129146] Min Reward on eval: 870.278719681676
[37m[1m[2023-06-25 08:30:46,308][129146] Mean Reward across all agents: 1942.2634976432594
[37m[1m[2023-06-25 08:30:46,308][129146] Average Trajectory Length: 999.92
[36m[2023-06-25 08:30:46,311][129146] mean_value=-114.47866009421831, max_value=1112.6778868386943
[37m[1m[2023-06-25 08:30:46,314][129146] New mean coefficients: [[ 0.2541827   0.8965103  -0.38115096 -0.6585108   0.2997655 ]]
[37m[1m[2023-06-25 08:30:46,315][129146] Moving the mean solution point...
[36m[2023-06-25 08:30:56,012][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 08:30:56,012][129146] FPS: 396061.71
[36m[2023-06-25 08:30:56,014][129146] itr=815, itrs=2000, Progress: 40.75%
[36m[2023-06-25 08:31:07,639][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 08:31:07,639][129146] FPS: 330949.66
[36m[2023-06-25 08:31:12,494][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:31:12,494][129146] Reward + Measures: [[2433.31999553    0.75332439    0.20654966    0.236159      0.29367766]]
[37m[1m[2023-06-25 08:31:12,495][129146] Max Reward on eval: 2433.319995526133
[37m[1m[2023-06-25 08:31:12,495][129146] Min Reward on eval: 2433.319995526133
[37m[1m[2023-06-25 08:31:12,495][129146] Mean Reward across all agents: 2433.319995526133
[37m[1m[2023-06-25 08:31:12,495][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:31:17,994][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:31:17,995][129146] Reward + Measures: [[2239.44128431    0.7403        0.23910001    0.27669999    0.2277    ]
[37m[1m [2214.83199249    0.70180005    0.20460001    0.24170001    0.28800002]
[37m[1m [1965.1973634     0.72209996    0.25049996    0.28760001    0.27130002]
[37m[1m ...
[37m[1m [2346.90004965    0.74259996    0.21340001    0.24609999    0.24609999]
[37m[1m [2074.37551635    0.73940003    0.25850001    0.2816        0.2863    ]
[37m[1m [2326.22282811    0.76850003    0.21110001    0.24519999    0.27530003]]
[37m[1m[2023-06-25 08:31:17,995][129146] Max Reward on eval: 2530.1501579933333
[37m[1m[2023-06-25 08:31:17,995][129146] Min Reward on eval: 1145.9762568563572
[37m[1m[2023-06-25 08:31:17,996][129146] Mean Reward across all agents: 2004.370529449843
[37m[1m[2023-06-25 08:31:17,996][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:31:17,999][129146] mean_value=55.35870682550395, max_value=2899.1146962379453
[37m[1m[2023-06-25 08:31:18,002][129146] New mean coefficients: [[-0.12074834  0.7039243  -0.32655463 -0.15586066  0.19659394]]
[37m[1m[2023-06-25 08:31:18,003][129146] Moving the mean solution point...
[36m[2023-06-25 08:31:27,781][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 08:31:27,781][129146] FPS: 392789.30
[36m[2023-06-25 08:31:27,783][129146] itr=816, itrs=2000, Progress: 40.80%
[36m[2023-06-25 08:31:39,353][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 08:31:39,353][129146] FPS: 332525.76
[36m[2023-06-25 08:31:44,148][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:31:44,148][129146] Reward + Measures: [[2329.20942811    0.77599299    0.20455666    0.23706266    0.307567  ]]
[37m[1m[2023-06-25 08:31:44,148][129146] Max Reward on eval: 2329.2094281136915
[37m[1m[2023-06-25 08:31:44,149][129146] Min Reward on eval: 2329.2094281136915
[37m[1m[2023-06-25 08:31:44,149][129146] Mean Reward across all agents: 2329.2094281136915
[37m[1m[2023-06-25 08:31:44,149][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:31:49,613][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:31:49,613][129146] Reward + Measures: [[2360.05884285    0.66320008    0.22130001    0.24860001    0.22680001]
[37m[1m [1755.84577079    0.71890002    0.19670001    0.24309997    0.4032    ]
[37m[1m [2450.65013144    0.71999997    0.22080003    0.2472        0.22939999]
[37m[1m ...
[37m[1m [2211.87455803    0.65189999    0.2033        0.23109999    0.31329998]
[37m[1m [2291.64139517    0.68020004    0.21790002    0.23559999    0.2095    ]
[37m[1m [1893.30736943    0.57849997    0.25070003    0.26400003    0.22080003]]
[37m[1m[2023-06-25 08:31:49,614][129146] Max Reward on eval: 2598.1212304798073
[37m[1m[2023-06-25 08:31:49,614][129146] Min Reward on eval: 461.74006813718586
[37m[1m[2023-06-25 08:31:49,614][129146] Mean Reward across all agents: 1969.7724382562326
[37m[1m[2023-06-25 08:31:49,614][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:31:49,618][129146] mean_value=7.743061599840706, max_value=2575.1739323703678
[37m[1m[2023-06-25 08:31:49,621][129146] New mean coefficients: [[ 0.07059135  0.79381114 -0.1829156  -0.45970127  0.13370001]]
[37m[1m[2023-06-25 08:31:49,622][129146] Moving the mean solution point...
[36m[2023-06-25 08:31:59,370][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 08:31:59,371][129146] FPS: 393995.03
[36m[2023-06-25 08:31:59,373][129146] itr=817, itrs=2000, Progress: 40.85%
[36m[2023-06-25 08:32:10,936][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 08:32:10,936][129146] FPS: 332717.78
[36m[2023-06-25 08:32:15,745][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:32:15,746][129146] Reward + Measures: [[2449.67510448    0.80170768    0.20246366    0.23463431    0.29116198]]
[37m[1m[2023-06-25 08:32:15,746][129146] Max Reward on eval: 2449.6751044847033
[37m[1m[2023-06-25 08:32:15,746][129146] Min Reward on eval: 2449.6751044847033
[37m[1m[2023-06-25 08:32:15,746][129146] Mean Reward across all agents: 2449.6751044847033
[37m[1m[2023-06-25 08:32:15,747][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:32:21,080][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:32:21,087][129146] Reward + Measures: [[1430.53592485    0.61059999    0.21360002    0.2949        0.42859998]
[37m[1m [ 929.71694558    0.52539998    0.21159999    0.2739        0.2753    ]
[37m[1m [1534.74697578    0.62559998    0.22360002    0.25479999    0.2529    ]
[37m[1m ...
[37m[1m [1907.85108222    0.72839999    0.2052        0.2411        0.271     ]
[37m[1m [1685.56726029    0.73500001    0.18440001    0.23810001    0.35789999]
[37m[1m [2015.54241043    0.72860003    0.21789999    0.25639999    0.3109    ]]
[37m[1m[2023-06-25 08:32:21,087][129146] Max Reward on eval: 2541.605077884439
[37m[1m[2023-06-25 08:32:21,088][129146] Min Reward on eval: 206.483282449306
[37m[1m[2023-06-25 08:32:21,088][129146] Mean Reward across all agents: 1561.9178871330562
[37m[1m[2023-06-25 08:32:21,089][129146] Average Trajectory Length: 999.6089999999999
[36m[2023-06-25 08:32:21,095][129146] mean_value=-383.40055183686417, max_value=2942.478211447201
[37m[1m[2023-06-25 08:32:21,100][129146] New mean coefficients: [[-0.36041284  0.621843    0.05592957 -0.16113546  0.1327375 ]]
[37m[1m[2023-06-25 08:32:21,101][129146] Moving the mean solution point...
[36m[2023-06-25 08:32:30,499][129146] train() took 9.40 seconds to complete
[36m[2023-06-25 08:32:30,499][129146] FPS: 408715.81
[36m[2023-06-25 08:32:30,501][129146] itr=818, itrs=2000, Progress: 40.90%
[36m[2023-06-25 08:32:41,982][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 08:32:41,982][129146] FPS: 335144.06
[36m[2023-06-25 08:32:46,654][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:32:46,655][129146] Reward + Measures: [[2002.89913907    0.801902      0.20211367    0.24385233    0.34657934]]
[37m[1m[2023-06-25 08:32:46,655][129146] Max Reward on eval: 2002.8991390667297
[37m[1m[2023-06-25 08:32:46,655][129146] Min Reward on eval: 2002.8991390667297
[37m[1m[2023-06-25 08:32:46,656][129146] Mean Reward across all agents: 2002.8991390667297
[37m[1m[2023-06-25 08:32:46,656][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:32:52,129][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:32:52,130][129146] Reward + Measures: [[1623.27202064    0.65020007    0.24249999    0.259         0.21329999]
[37m[1m [1771.00030791    0.76640004    0.2141        0.26100001    0.32710001]
[37m[1m [1201.333117      0.61669999    0.30130002    0.35820004    0.14530002]
[37m[1m ...
[37m[1m [1365.92330941    0.64960003    0.2023        0.27470002    0.50219995]
[37m[1m [2160.41165435    0.79030001    0.2142        0.2454        0.31829998]
[37m[1m [ 968.54439211    0.6103        0.2194        0.31010002    0.39060003]]
[37m[1m[2023-06-25 08:32:52,130][129146] Max Reward on eval: 2603.728164388216
[37m[1m[2023-06-25 08:32:52,130][129146] Min Reward on eval: 968.5443921110011
[37m[1m[2023-06-25 08:32:52,131][129146] Mean Reward across all agents: 1761.725031136435
[37m[1m[2023-06-25 08:32:52,131][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:32:52,135][129146] mean_value=339.6271376736048, max_value=2645.2003524662855
[37m[1m[2023-06-25 08:32:52,138][129146] New mean coefficients: [[-0.68579584  0.52689755 -0.31322882  0.16966787  0.56872046]]
[37m[1m[2023-06-25 08:32:52,139][129146] Moving the mean solution point...
[36m[2023-06-25 08:33:01,936][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 08:33:01,936][129146] FPS: 392043.07
[36m[2023-06-25 08:33:01,938][129146] itr=819, itrs=2000, Progress: 40.95%
[36m[2023-06-25 08:33:13,336][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 08:33:13,336][129146] FPS: 337569.70
[36m[2023-06-25 08:33:18,154][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:33:18,155][129146] Reward + Measures: [[770.50443524   0.61555564   0.19483398   0.30190599   0.46064997]]
[37m[1m[2023-06-25 08:33:18,155][129146] Max Reward on eval: 770.5044352415703
[37m[1m[2023-06-25 08:33:18,155][129146] Min Reward on eval: 770.5044352415703
[37m[1m[2023-06-25 08:33:18,155][129146] Mean Reward across all agents: 770.5044352415703
[37m[1m[2023-06-25 08:33:18,156][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:33:23,525][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:33:23,526][129146] Reward + Measures: [[825.56319528   0.52960008   0.24990001   0.39050001   0.2543    ]
[37m[1m [804.44212041   0.59610003   0.2464       0.43610001   0.25819999]
[37m[1m [814.39981871   0.56870002   0.18260001   0.27719998   0.41050002]
[37m[1m ...
[37m[1m [718.39273753   0.66549999   0.22190002   0.36180001   0.36219999]
[37m[1m [821.24940372   0.64320004   0.24249999   0.38720003   0.31879997]
[37m[1m [807.23517939   0.61370003   0.23709999   0.38510001   0.31940004]]
[37m[1m[2023-06-25 08:33:23,526][129146] Max Reward on eval: 904.1036761844182
[37m[1m[2023-06-25 08:33:23,526][129146] Min Reward on eval: 594.7814127806032
[37m[1m[2023-06-25 08:33:23,526][129146] Mean Reward across all agents: 751.1762855032761
[37m[1m[2023-06-25 08:33:23,526][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:33:23,529][129146] mean_value=-115.53556275715799, max_value=1351.9692386489128
[37m[1m[2023-06-25 08:33:23,532][129146] New mean coefficients: [[-0.01654989  0.07431704 -0.02031645 -0.15933317  0.20996904]]
[37m[1m[2023-06-25 08:33:23,533][129146] Moving the mean solution point...
[36m[2023-06-25 08:33:33,371][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 08:33:33,371][129146] FPS: 390389.59
[36m[2023-06-25 08:33:33,374][129146] itr=820, itrs=2000, Progress: 41.00%
[37m[1m[2023-06-25 08:33:39,778][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000800
[36m[2023-06-25 08:33:51,529][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 08:33:51,529][129146] FPS: 333730.79
[36m[2023-06-25 08:33:56,261][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:33:56,262][129146] Reward + Measures: [[806.16138844   0.62463832   0.19174634   0.292476     0.53266066]]
[37m[1m[2023-06-25 08:33:56,262][129146] Max Reward on eval: 806.1613884427602
[37m[1m[2023-06-25 08:33:56,263][129146] Min Reward on eval: 806.1613884427602
[37m[1m[2023-06-25 08:33:56,263][129146] Mean Reward across all agents: 806.1613884427602
[37m[1m[2023-06-25 08:33:56,263][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:34:01,679][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:34:01,679][129146] Reward + Measures: [[1013.47550508    0.57310003    0.19340001    0.27880001    0.5693    ]
[37m[1m [ 718.29247585    0.68430001    0.22579999    0.37349999    0.32370001]
[37m[1m [ 842.85881059    0.625         0.20200001    0.3012        0.47840005]
[37m[1m ...
[37m[1m [ 724.01384014    0.6243        0.18850002    0.296         0.50380003]
[37m[1m [ 792.85488434    0.60170001    0.19419999    0.31060001    0.52170008]
[37m[1m [ 652.39777779    0.52210003    0.26820001    0.40159997    0.3928    ]]
[37m[1m[2023-06-25 08:34:01,679][129146] Max Reward on eval: 1582.5542816797854
[37m[1m[2023-06-25 08:34:01,680][129146] Min Reward on eval: 281.7733748259605
[37m[1m[2023-06-25 08:34:01,680][129146] Mean Reward across all agents: 857.4573691535609
[37m[1m[2023-06-25 08:34:01,680][129146] Average Trajectory Length: 999.7439999999999
[36m[2023-06-25 08:34:01,685][129146] mean_value=203.04648466701926, max_value=1759.8686795647257
[37m[1m[2023-06-25 08:34:01,688][129146] New mean coefficients: [[ 0.01687974  0.03596599 -0.28745708 -0.5613563   0.2662034 ]]
[37m[1m[2023-06-25 08:34:01,689][129146] Moving the mean solution point...
[36m[2023-06-25 08:34:11,341][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 08:34:11,341][129146] FPS: 397912.18
[36m[2023-06-25 08:34:11,343][129146] itr=821, itrs=2000, Progress: 41.05%
[36m[2023-06-25 08:34:22,770][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 08:34:22,770][129146] FPS: 336696.81
[36m[2023-06-25 08:34:27,518][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:34:27,518][129146] Reward + Measures: [[872.4823534    0.62117267   0.185421     0.27006766   0.58171636]]
[37m[1m[2023-06-25 08:34:27,518][129146] Max Reward on eval: 872.4823534032023
[37m[1m[2023-06-25 08:34:27,518][129146] Min Reward on eval: 872.4823534032023
[37m[1m[2023-06-25 08:34:27,519][129146] Mean Reward across all agents: 872.4823534032023
[37m[1m[2023-06-25 08:34:27,519][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:34:32,980][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:34:32,980][129146] Reward + Measures: [[985.00441654   0.69769996   0.19670001   0.27090001   0.4217    ]
[37m[1m [562.7267983    0.63779998   0.21170001   0.3026       0.34650001]
[37m[1m [940.02151924   0.69020003   0.18520001   0.26069999   0.479     ]
[37m[1m ...
[37m[1m [662.28067533   0.63870001   0.22989999   0.40559998   0.28490001]
[37m[1m [944.01536184   0.65640002   0.1919       0.2694       0.49889994]
[37m[1m [780.40577209   0.52899998   0.2026       0.33339998   0.5381    ]]
[37m[1m[2023-06-25 08:34:32,981][129146] Max Reward on eval: 1083.5931889433764
[37m[1m[2023-06-25 08:34:32,981][129146] Min Reward on eval: 441.62826744499614
[37m[1m[2023-06-25 08:34:32,981][129146] Mean Reward across all agents: 754.6921419092009
[37m[1m[2023-06-25 08:34:32,981][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:34:32,984][129146] mean_value=-179.7486748839979, max_value=1506.0610143156955
[37m[1m[2023-06-25 08:34:32,987][129146] New mean coefficients: [[ 0.29045343 -0.16052467  0.31994376 -0.4376694   0.12408434]]
[37m[1m[2023-06-25 08:34:32,988][129146] Moving the mean solution point...
[36m[2023-06-25 08:34:42,832][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 08:34:42,833][129146] FPS: 390128.15
[36m[2023-06-25 08:34:42,835][129146] itr=822, itrs=2000, Progress: 41.10%
[36m[2023-06-25 08:34:54,334][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 08:34:54,334][129146] FPS: 334557.76
[36m[2023-06-25 08:34:59,134][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:34:59,135][129146] Reward + Measures: [[1136.90850967    0.60879296    0.191618      0.24625733    0.58438694]]
[37m[1m[2023-06-25 08:34:59,135][129146] Max Reward on eval: 1136.9085096669025
[37m[1m[2023-06-25 08:34:59,135][129146] Min Reward on eval: 1136.9085096669025
[37m[1m[2023-06-25 08:34:59,135][129146] Mean Reward across all agents: 1136.9085096669025
[37m[1m[2023-06-25 08:34:59,135][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:35:04,712][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:35:04,713][129146] Reward + Measures: [[1090.63688095    0.59310001    0.1901        0.25439999    0.57330006]
[37m[1m [1232.6525931     0.65710002    0.20180002    0.2613        0.54000002]
[37m[1m [1181.50147639    0.60190004    0.19209999    0.24919999    0.58249998]
[37m[1m ...
[37m[1m [ 922.4959388     0.52609998    0.1885        0.25489998    0.5244    ]
[37m[1m [ 557.19634681    0.50810003    0.17799999    0.27620003    0.35460001]
[37m[1m [1049.38432162    0.5819        0.19060002    0.25170001    0.54660004]]
[37m[1m[2023-06-25 08:35:04,713][129146] Max Reward on eval: 1482.0851893448155
[37m[1m[2023-06-25 08:35:04,713][129146] Min Reward on eval: 406.4847973446216
[37m[1m[2023-06-25 08:35:04,714][129146] Mean Reward across all agents: 942.3809835590513
[37m[1m[2023-06-25 08:35:04,714][129146] Average Trajectory Length: 999.5849999999999
[36m[2023-06-25 08:35:04,717][129146] mean_value=-133.77995478404458, max_value=1648.026703972579
[37m[1m[2023-06-25 08:35:04,720][129146] New mean coefficients: [[ 0.10534136 -0.09046955 -0.31422076 -0.520435    0.3120526 ]]
[37m[1m[2023-06-25 08:35:04,721][129146] Moving the mean solution point...
[36m[2023-06-25 08:35:14,412][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 08:35:14,413][129146] FPS: 396280.44
[36m[2023-06-25 08:35:14,415][129146] itr=823, itrs=2000, Progress: 41.15%
[36m[2023-06-25 08:35:25,853][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 08:35:25,853][129146] FPS: 336369.37
[36m[2023-06-25 08:35:30,721][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:35:30,722][129146] Reward + Measures: [[1333.76046147    0.60424131    0.18963932    0.22848       0.62738097]]
[37m[1m[2023-06-25 08:35:30,722][129146] Max Reward on eval: 1333.76046147448
[37m[1m[2023-06-25 08:35:30,722][129146] Min Reward on eval: 1333.76046147448
[37m[1m[2023-06-25 08:35:30,722][129146] Mean Reward across all agents: 1333.76046147448
[37m[1m[2023-06-25 08:35:30,723][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:35:36,317][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:35:36,317][129146] Reward + Measures: [[ 463.12549644    0.50229996    0.20840001    0.33039999    0.28299999]
[37m[1m [ 393.95448868    0.44930002    0.17160001    0.2978        0.27340004]
[37m[1m [1515.5421169     0.65130007    0.20120001    0.23410001    0.43470001]
[37m[1m ...
[37m[1m [ 465.98536259    0.50730002    0.18269999    0.2983        0.4666    ]
[37m[1m [1221.73923246    0.61020005    0.20969999    0.2696        0.60120004]
[37m[1m [ 537.53855276    0.52699995    0.18890001    0.29200003    0.53280002]]
[37m[1m[2023-06-25 08:35:36,317][129146] Max Reward on eval: 1712.7521216631053
[37m[1m[2023-06-25 08:35:36,318][129146] Min Reward on eval: 94.55006151567213
[37m[1m[2023-06-25 08:35:36,318][129146] Mean Reward across all agents: 790.4961594955104
[37m[1m[2023-06-25 08:35:36,318][129146] Average Trajectory Length: 999.0446666666667
[36m[2023-06-25 08:35:36,322][129146] mean_value=-201.95486502667777, max_value=2002.493020024011
[37m[1m[2023-06-25 08:35:36,325][129146] New mean coefficients: [[-0.38679624 -0.60312617 -0.20384814  0.11321056  0.26789758]]
[37m[1m[2023-06-25 08:35:36,326][129146] Moving the mean solution point...
[36m[2023-06-25 08:35:46,285][129146] train() took 9.96 seconds to complete
[36m[2023-06-25 08:35:46,286][129146] FPS: 385635.50
[36m[2023-06-25 08:35:46,288][129146] itr=824, itrs=2000, Progress: 41.20%
[36m[2023-06-25 08:35:57,935][129146] train() took 11.62 seconds to complete
[36m[2023-06-25 08:35:57,935][129146] FPS: 330427.89
[36m[2023-06-25 08:36:02,719][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:36:02,719][129146] Reward + Measures: [[1030.56776449    0.5359143     0.19357198    0.2485        0.65245497]]
[37m[1m[2023-06-25 08:36:02,720][129146] Max Reward on eval: 1030.5677644875366
[37m[1m[2023-06-25 08:36:02,720][129146] Min Reward on eval: 1030.5677644875366
[37m[1m[2023-06-25 08:36:02,720][129146] Mean Reward across all agents: 1030.5677644875366
[37m[1m[2023-06-25 08:36:02,720][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:36:08,127][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:36:08,127][129146] Reward + Measures: [[1000.9063964     0.51630002    0.19489999    0.30969998    0.56059998]
[37m[1m [ 571.0678394     0.3962        0.20739999    0.2933        0.41700003]
[37m[1m [ 716.35165057    0.63519996    0.21460001    0.31720001    0.46540004]
[37m[1m ...
[37m[1m [ 573.77627154    0.46680003    0.19200002    0.31029996    0.29550001]
[37m[1m [ 959.76064868    0.48319998    0.2395        0.31490001    0.48370001]
[37m[1m [ 958.11392093    0.48109999    0.25360003    0.2933        0.46240002]]
[37m[1m[2023-06-25 08:36:08,127][129146] Max Reward on eval: 1411.8259333842784
[37m[1m[2023-06-25 08:36:08,128][129146] Min Reward on eval: 237.7264346524491
[37m[1m[2023-06-25 08:36:08,128][129146] Mean Reward across all agents: 843.1053229986209
[37m[1m[2023-06-25 08:36:08,128][129146] Average Trajectory Length: 999.8343333333333
[36m[2023-06-25 08:36:08,132][129146] mean_value=18.14897040361092, max_value=1510.747020188467
[37m[1m[2023-06-25 08:36:08,135][129146] New mean coefficients: [[-0.18457295  0.05054206 -0.73158467 -0.04506116  0.12059711]]
[37m[1m[2023-06-25 08:36:08,136][129146] Moving the mean solution point...
[36m[2023-06-25 08:36:17,754][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 08:36:17,755][129146] FPS: 399290.26
[36m[2023-06-25 08:36:17,757][129146] itr=825, itrs=2000, Progress: 41.25%
[36m[2023-06-25 08:36:29,195][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 08:36:29,196][129146] FPS: 336451.32
[36m[2023-06-25 08:36:33,944][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:36:33,944][129146] Reward + Measures: [[701.67149285   0.49989364   0.185984     0.26535633   0.624336  ]]
[37m[1m[2023-06-25 08:36:33,944][129146] Max Reward on eval: 701.6714928541031
[37m[1m[2023-06-25 08:36:33,944][129146] Min Reward on eval: 701.6714928541031
[37m[1m[2023-06-25 08:36:33,945][129146] Mean Reward across all agents: 701.6714928541031
[37m[1m[2023-06-25 08:36:33,945][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:36:39,508][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:36:39,508][129146] Reward + Measures: [[ 862.78268707    0.61050004    0.19920002    0.27860004    0.57579994]
[37m[1m [ 694.04960875    0.66260004    0.22790001    0.34630004    0.47210002]
[37m[1m [1229.22798488    0.5248        0.1971        0.24699998    0.60219997]
[37m[1m ...
[37m[1m [ 980.72034428    0.56050003    0.2155        0.28440002    0.57800001]
[37m[1m [ 526.55128178    0.4962        0.20299999    0.31440002    0.48839998]
[37m[1m [ 652.19888346    0.54360002    0.19820002    0.2899        0.58270001]]
[37m[1m[2023-06-25 08:36:39,509][129146] Max Reward on eval: 1412.420420530904
[37m[1m[2023-06-25 08:36:39,509][129146] Min Reward on eval: 397.03958172982675
[37m[1m[2023-06-25 08:36:39,509][129146] Mean Reward across all agents: 810.1624300019315
[37m[1m[2023-06-25 08:36:39,509][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:36:39,512][129146] mean_value=-198.25348108957544, max_value=1368.1691600466497
[37m[1m[2023-06-25 08:36:39,514][129146] New mean coefficients: [[ 0.11107285 -0.06461814 -1.1140114   0.00244969 -0.01402952]]
[37m[1m[2023-06-25 08:36:39,515][129146] Moving the mean solution point...
[36m[2023-06-25 08:36:49,237][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 08:36:49,237][129146] FPS: 395059.89
[36m[2023-06-25 08:36:49,239][129146] itr=826, itrs=2000, Progress: 41.30%
[36m[2023-06-25 08:37:00,855][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 08:37:00,855][129146] FPS: 331304.80
[36m[2023-06-25 08:37:05,667][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:37:05,668][129146] Reward + Measures: [[720.50208753   0.52794367   0.17878734   0.26574767   0.63175666]]
[37m[1m[2023-06-25 08:37:05,668][129146] Max Reward on eval: 720.5020875300256
[37m[1m[2023-06-25 08:37:05,668][129146] Min Reward on eval: 720.5020875300256
[37m[1m[2023-06-25 08:37:05,669][129146] Mean Reward across all agents: 720.5020875300256
[37m[1m[2023-06-25 08:37:05,669][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:37:11,169][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:37:11,169][129146] Reward + Measures: [[ 438.01714689    0.50150001    0.15090001    0.31010002    0.3299    ]
[37m[1m [ 461.13290766    0.51899999    0.15000001    0.32029998    0.34689999]
[37m[1m [1528.54472271    0.62030005    0.2325        0.28029999    0.3308    ]
[37m[1m ...
[37m[1m [1361.83340868    0.63000005    0.1815        0.2278        0.50400001]
[37m[1m [ 593.42259297    0.54860002    0.18069999    0.28280002    0.56779999]
[37m[1m [ 350.96093362    0.54770005    0.15939999    0.41499996    0.31340003]]
[37m[1m[2023-06-25 08:37:11,170][129146] Max Reward on eval: 1595.9498124342413
[37m[1m[2023-06-25 08:37:11,170][129146] Min Reward on eval: 207.8611304267717
[37m[1m[2023-06-25 08:37:11,170][129146] Mean Reward across all agents: 544.0246007515641
[37m[1m[2023-06-25 08:37:11,170][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:37:11,174][129146] mean_value=-191.74722217895635, max_value=1449.209706059602
[37m[1m[2023-06-25 08:37:11,176][129146] New mean coefficients: [[ 0.09771514 -0.48084977 -1.0173148   0.02552498 -0.15079418]]
[37m[1m[2023-06-25 08:37:11,177][129146] Moving the mean solution point...
[36m[2023-06-25 08:37:20,881][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 08:37:20,881][129146] FPS: 395777.92
[36m[2023-06-25 08:37:20,884][129146] itr=827, itrs=2000, Progress: 41.35%
[36m[2023-06-25 08:37:32,300][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 08:37:32,300][129146] FPS: 337018.54
[36m[2023-06-25 08:37:36,813][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:37:36,813][129146] Reward + Measures: [[726.60312684   0.46369132   0.17561133   0.26363599   0.61174566]]
[37m[1m[2023-06-25 08:37:36,813][129146] Max Reward on eval: 726.6031268389324
[37m[1m[2023-06-25 08:37:36,813][129146] Min Reward on eval: 726.6031268389324
[37m[1m[2023-06-25 08:37:36,813][129146] Mean Reward across all agents: 726.6031268389324
[37m[1m[2023-06-25 08:37:36,814][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:37:42,033][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:37:42,034][129146] Reward + Measures: [[ 505.74312504    0.50910008    0.2254        0.34740001    0.3319    ]
[37m[1m [ 677.42976012    0.54710001    0.1847        0.30219999    0.56300008]
[37m[1m [ 720.40396466    0.51539999    0.22949998    0.35319999    0.46829996]
[37m[1m ...
[37m[1m [ 480.26201334    0.4384        0.20809999    0.31019998    0.39070001]
[37m[1m [ 429.71062594    0.59250003    0.3001        0.35370001    0.27110001]
[37m[1m [1342.09769541    0.61400002    0.1876        0.23109999    0.4921    ]]
[37m[1m[2023-06-25 08:37:42,034][129146] Max Reward on eval: 1572.4502429966815
[37m[1m[2023-06-25 08:37:42,034][129146] Min Reward on eval: -60.24979657059303
[37m[1m[2023-06-25 08:37:42,035][129146] Mean Reward across all agents: 589.118232275912
[37m[1m[2023-06-25 08:37:42,035][129146] Average Trajectory Length: 999.804
[36m[2023-06-25 08:37:42,037][129146] mean_value=-640.535220426452, max_value=1875.4276718572908
[37m[1m[2023-06-25 08:37:42,040][129146] New mean coefficients: [[ 0.4271062  -0.8119825  -0.2459827  -0.03293895 -0.26467705]]
[37m[1m[2023-06-25 08:37:42,041][129146] Moving the mean solution point...
[36m[2023-06-25 08:37:51,312][129146] train() took 9.27 seconds to complete
[36m[2023-06-25 08:37:51,313][129146] FPS: 414228.66
[36m[2023-06-25 08:37:51,315][129146] itr=828, itrs=2000, Progress: 41.40%
[36m[2023-06-25 08:38:02,899][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 08:38:02,899][129146] FPS: 332117.88
[36m[2023-06-25 08:38:07,800][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:38:07,801][129146] Reward + Measures: [[856.89138265   0.42836335   0.17606466   0.25615567   0.59272164]]
[37m[1m[2023-06-25 08:38:07,801][129146] Max Reward on eval: 856.8913826516662
[37m[1m[2023-06-25 08:38:07,801][129146] Min Reward on eval: 856.8913826516662
[37m[1m[2023-06-25 08:38:07,801][129146] Mean Reward across all agents: 856.8913826516662
[37m[1m[2023-06-25 08:38:07,801][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:38:13,292][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:38:13,293][129146] Reward + Measures: [[1222.69062749    0.49509999    0.2079        0.26040003    0.42799997]
[37m[1m [1086.27440625    0.44959998    0.17040001    0.2335        0.57240003]
[37m[1m [ 645.9919942     0.38719997    0.1804        0.27500001    0.53680009]
[37m[1m ...
[37m[1m [1161.76205863    0.49139997    0.176         0.23959999    0.56840003]
[37m[1m [ 795.2234368     0.42829999    0.1767        0.27269998    0.57010001]
[37m[1m [ 782.62809494    0.3989        0.183         0.27330002    0.55129999]]
[37m[1m[2023-06-25 08:38:13,293][129146] Max Reward on eval: 1433.3905067896935
[37m[1m[2023-06-25 08:38:13,294][129146] Min Reward on eval: 337.9612417665776
[37m[1m[2023-06-25 08:38:13,294][129146] Mean Reward across all agents: 874.9219848016359
[37m[1m[2023-06-25 08:38:13,294][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:38:13,297][129146] mean_value=-70.07528335342174, max_value=1800.782926423836
[37m[1m[2023-06-25 08:38:13,300][129146] New mean coefficients: [[ 0.45533416 -0.5386529  -0.08705273 -0.2762918  -0.61014134]]
[37m[1m[2023-06-25 08:38:13,301][129146] Moving the mean solution point...
[36m[2023-06-25 08:38:23,202][129146] train() took 9.90 seconds to complete
[36m[2023-06-25 08:38:23,202][129146] FPS: 387914.95
[36m[2023-06-25 08:38:23,204][129146] itr=829, itrs=2000, Progress: 41.45%
[36m[2023-06-25 08:38:34,640][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 08:38:34,641][129146] FPS: 336531.66
[36m[2023-06-25 08:38:39,461][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:38:39,462][129146] Reward + Measures: [[1104.08781875    0.42332697    0.17740799    0.24068999    0.55489832]]
[37m[1m[2023-06-25 08:38:39,462][129146] Max Reward on eval: 1104.0878187525664
[37m[1m[2023-06-25 08:38:39,462][129146] Min Reward on eval: 1104.0878187525664
[37m[1m[2023-06-25 08:38:39,462][129146] Mean Reward across all agents: 1104.0878187525664
[37m[1m[2023-06-25 08:38:39,463][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:38:44,926][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:38:44,927][129146] Reward + Measures: [[ 831.90580462    0.38869998    0.19219999    0.27379999    0.46029997]
[37m[1m [ 575.14791758    0.42959997    0.1908        0.30050001    0.46580002]
[37m[1m [ 733.19022227    0.35530001    0.18450001    0.2599        0.47809997]
[37m[1m ...
[37m[1m [ 663.21882973    0.47800002    0.2386        0.35540003    0.37220001]
[37m[1m [1131.10579783    0.42409998    0.1737        0.24080001    0.55950004]
[37m[1m [ 871.8954782     0.4745        0.19750001    0.30250001    0.52929997]]
[37m[1m[2023-06-25 08:38:44,927][129146] Max Reward on eval: 1379.4048959733336
[37m[1m[2023-06-25 08:38:44,927][129146] Min Reward on eval: 201.97705041687004
[37m[1m[2023-06-25 08:38:44,928][129146] Mean Reward across all agents: 648.9623182061777
[37m[1m[2023-06-25 08:38:44,928][129146] Average Trajectory Length: 999.4876666666667
[36m[2023-06-25 08:38:44,930][129146] mean_value=-247.6859572869508, max_value=1371.8942195967072
[37m[1m[2023-06-25 08:38:44,933][129146] New mean coefficients: [[ 0.56875384 -1.0210843  -0.10331289 -0.25602862 -0.29086548]]
[37m[1m[2023-06-25 08:38:44,934][129146] Moving the mean solution point...
[36m[2023-06-25 08:38:54,693][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 08:38:54,694][129146] FPS: 393532.48
[36m[2023-06-25 08:38:54,696][129146] itr=830, itrs=2000, Progress: 41.50%
[37m[1m[2023-06-25 08:39:01,074][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000810
[36m[2023-06-25 08:39:12,816][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 08:39:12,816][129146] FPS: 333765.68
[36m[2023-06-25 08:39:17,452][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:39:17,453][129146] Reward + Measures: [[1503.98777743    0.43860233    0.18052299    0.22354634    0.48759001]]
[37m[1m[2023-06-25 08:39:17,453][129146] Max Reward on eval: 1503.987777428202
[37m[1m[2023-06-25 08:39:17,453][129146] Min Reward on eval: 1503.987777428202
[37m[1m[2023-06-25 08:39:17,453][129146] Mean Reward across all agents: 1503.987777428202
[37m[1m[2023-06-25 08:39:17,454][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:39:22,804][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:39:22,805][129146] Reward + Measures: [[1561.88575711    0.43619999    0.18300001    0.2234        0.4756    ]
[37m[1m [1507.4894958     0.44770002    0.17569999    0.22019999    0.48120004]
[37m[1m [1529.85132038    0.3845        0.1912        0.23029999    0.479     ]
[37m[1m ...
[37m[1m [1630.01005832    0.45460001    0.1829        0.22849999    0.44549999]
[37m[1m [1371.34156698    0.38540003    0.1969        0.2455        0.48289999]
[37m[1m [1344.58134148    0.43059999    0.17779998    0.2251        0.52880001]]
[37m[1m[2023-06-25 08:39:22,805][129146] Max Reward on eval: 1807.1810444283183
[37m[1m[2023-06-25 08:39:22,805][129146] Min Reward on eval: 1037.410378943733
[37m[1m[2023-06-25 08:39:22,806][129146] Mean Reward across all agents: 1481.319783342519
[37m[1m[2023-06-25 08:39:22,806][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:39:22,813][129146] mean_value=910.6458506778197, max_value=2173.9652044300224
[37m[1m[2023-06-25 08:39:22,816][129146] New mean coefficients: [[ 0.26231766 -0.9349909  -0.515845   -0.7419512  -0.04751533]]
[37m[1m[2023-06-25 08:39:22,817][129146] Moving the mean solution point...
[36m[2023-06-25 08:39:32,384][129146] train() took 9.57 seconds to complete
[36m[2023-06-25 08:39:32,385][129146] FPS: 401437.89
[36m[2023-06-25 08:39:32,387][129146] itr=831, itrs=2000, Progress: 41.55%
[36m[2023-06-25 08:39:43,780][129146] train() took 11.37 seconds to complete
[36m[2023-06-25 08:39:43,780][129146] FPS: 337693.47
[36m[2023-06-25 08:39:48,573][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:39:48,573][129146] Reward + Measures: [[1641.5043342     0.41129166    0.18199499    0.21897933    0.45296365]]
[37m[1m[2023-06-25 08:39:48,574][129146] Max Reward on eval: 1641.5043342044453
[37m[1m[2023-06-25 08:39:48,574][129146] Min Reward on eval: 1641.5043342044453
[37m[1m[2023-06-25 08:39:48,574][129146] Mean Reward across all agents: 1641.5043342044453
[37m[1m[2023-06-25 08:39:48,574][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:39:54,064][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:39:54,064][129146] Reward + Measures: [[1720.63234678    0.41090003    0.1892        0.22049999    0.4357    ]
[37m[1m [1890.32858964    0.43980002    0.19679999    0.24320002    0.34299999]
[37m[1m [1522.37224736    0.38450003    0.1832        0.23510002    0.44250003]
[37m[1m ...
[37m[1m [1738.18934845    0.44615301    0.19242267    0.24141248    0.37476835]
[37m[1m [1727.74960113    0.412         0.1894        0.2287        0.44970003]
[37m[1m [1681.81998612    0.40920001    0.18540001    0.21399999    0.44390002]]
[37m[1m[2023-06-25 08:39:54,064][129146] Max Reward on eval: 1972.8341067154192
[37m[1m[2023-06-25 08:39:54,065][129146] Min Reward on eval: 1203.2470203744597
[37m[1m[2023-06-25 08:39:54,065][129146] Mean Reward across all agents: 1636.4111421724538
[37m[1m[2023-06-25 08:39:54,065][129146] Average Trajectory Length: 999.5233333333333
[36m[2023-06-25 08:39:54,072][129146] mean_value=255.04730752836394, max_value=2123.2751453068895
[37m[1m[2023-06-25 08:39:54,075][129146] New mean coefficients: [[ 0.8304666  -0.7439866  -0.9113046  -0.8692241   0.06980763]]
[37m[1m[2023-06-25 08:39:54,075][129146] Moving the mean solution point...
[36m[2023-06-25 08:40:03,900][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 08:40:03,901][129146] FPS: 390911.35
[36m[2023-06-25 08:40:03,903][129146] itr=832, itrs=2000, Progress: 41.60%
[36m[2023-06-25 08:40:15,383][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 08:40:15,384][129146] FPS: 335162.89
[36m[2023-06-25 08:40:20,251][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:40:20,252][129146] Reward + Measures: [[1931.98890081    0.41779548    0.1868455     0.21438763    0.40822265]]
[37m[1m[2023-06-25 08:40:20,252][129146] Max Reward on eval: 1931.9889008147077
[37m[1m[2023-06-25 08:40:20,252][129146] Min Reward on eval: 1931.9889008147077
[37m[1m[2023-06-25 08:40:20,252][129146] Mean Reward across all agents: 1931.9889008147077
[37m[1m[2023-06-25 08:40:20,253][129146] Average Trajectory Length: 999.6753333333334
[36m[2023-06-25 08:40:25,946][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:40:25,947][129146] Reward + Measures: [[1536.98933818    0.37010002    0.18980001    0.2297        0.45619997]
[37m[1m [1849.67715562    0.49020001    0.19219999    0.2165        0.49100003]
[37m[1m [1639.10203063    0.46140003    0.1983        0.23099999    0.48460004]
[37m[1m ...
[37m[1m [1776.64341734    0.4138        0.1796        0.2146        0.44180003]
[37m[1m [1824.57719713    0.41929999    0.18820003    0.2184        0.43899998]
[37m[1m [1847.59834596    0.40500003    0.19310001    0.22049999    0.447     ]]
[37m[1m[2023-06-25 08:40:25,947][129146] Max Reward on eval: 2072.763995983056
[37m[1m[2023-06-25 08:40:25,947][129146] Min Reward on eval: 542.3459915350192
[37m[1m[2023-06-25 08:40:25,947][129146] Mean Reward across all agents: 1541.1975560158812
[37m[1m[2023-06-25 08:40:25,948][129146] Average Trajectory Length: 999.9839999999999
[36m[2023-06-25 08:40:25,953][129146] mean_value=126.71335967190048, max_value=1382.9075526462855
[37m[1m[2023-06-25 08:40:25,955][129146] New mean coefficients: [[ 1.0038513   0.33991623 -1.0674635  -1.1097735   0.04819369]]
[37m[1m[2023-06-25 08:40:25,956][129146] Moving the mean solution point...
[36m[2023-06-25 08:40:35,747][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 08:40:35,747][129146] FPS: 392267.19
[36m[2023-06-25 08:40:35,750][129146] itr=833, itrs=2000, Progress: 41.65%
[36m[2023-06-25 08:40:47,307][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 08:40:47,307][129146] FPS: 332878.71
[36m[2023-06-25 08:40:52,159][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:40:52,160][129146] Reward + Measures: [[2252.43413202    0.45688054    0.19557701    0.21636197    0.36423343]]
[37m[1m[2023-06-25 08:40:52,160][129146] Max Reward on eval: 2252.4341320214444
[37m[1m[2023-06-25 08:40:52,160][129146] Min Reward on eval: 2252.4341320214444
[37m[1m[2023-06-25 08:40:52,161][129146] Mean Reward across all agents: 2252.4341320214444
[37m[1m[2023-06-25 08:40:52,161][129146] Average Trajectory Length: 999.5893333333333
[36m[2023-06-25 08:40:57,618][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:40:57,619][129146] Reward + Measures: [[1534.24269351    0.4271        0.1824        0.23910001    0.37579998]
[37m[1m [1114.29318316    0.37799999    0.18080001    0.2581        0.39540002]
[37m[1m [1576.58074475    0.44589996    0.1895        0.23380001    0.37000003]
[37m[1m ...
[37m[1m [2187.25515409    0.46829996    0.20780002    0.21950002    0.3612    ]
[37m[1m [1376.68691989    0.40489998    0.18270001    0.24259999    0.39540002]
[37m[1m [2010.85060078    0.49569997    0.19100001    0.21739998    0.37680003]]
[37m[1m[2023-06-25 08:40:57,619][129146] Max Reward on eval: 2281.275816351711
[37m[1m[2023-06-25 08:40:57,619][129146] Min Reward on eval: 493.3127399503195
[37m[1m[2023-06-25 08:40:57,620][129146] Mean Reward across all agents: 1528.3154207359714
[37m[1m[2023-06-25 08:40:57,620][129146] Average Trajectory Length: 999.7823333333333
[36m[2023-06-25 08:40:57,623][129146] mean_value=-41.40864403185223, max_value=2003.4368663895034
[37m[1m[2023-06-25 08:40:57,626][129146] New mean coefficients: [[ 0.7484621  -0.12638274 -0.6240932  -1.3071012   0.05403837]]
[37m[1m[2023-06-25 08:40:57,627][129146] Moving the mean solution point...
[36m[2023-06-25 08:41:07,419][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 08:41:07,420][129146] FPS: 392201.75
[36m[2023-06-25 08:41:07,422][129146] itr=834, itrs=2000, Progress: 41.70%
[36m[2023-06-25 08:41:19,075][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 08:41:19,075][129146] FPS: 330152.21
[36m[2023-06-25 08:41:23,823][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:41:23,824][129146] Reward + Measures: [[2512.06955269    0.46623251    0.19721435    0.21822447    0.34576294]]
[37m[1m[2023-06-25 08:41:23,824][129146] Max Reward on eval: 2512.0695526876466
[37m[1m[2023-06-25 08:41:23,824][129146] Min Reward on eval: 2512.0695526876466
[37m[1m[2023-06-25 08:41:23,824][129146] Mean Reward across all agents: 2512.0695526876466
[37m[1m[2023-06-25 08:41:23,825][129146] Average Trajectory Length: 999.819
[36m[2023-06-25 08:41:29,305][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:41:29,306][129146] Reward + Measures: [[2256.42530356    0.41280004    0.19070001    0.20920001    0.36939999]
[37m[1m [2352.35804918    0.4474        0.19930001    0.2165        0.3531    ]
[37m[1m [2302.34725045    0.50740004    0.1955        0.21400002    0.34739998]
[37m[1m ...
[37m[1m [2195.85659046    0.48270002    0.19840001    0.22220002    0.38659999]
[37m[1m [2266.28545448    0.4219        0.1981        0.21700001    0.38890001]
[37m[1m [2484.83510074    0.45790005    0.19230001    0.21070002    0.36490002]]
[37m[1m[2023-06-25 08:41:29,306][129146] Max Reward on eval: 2646.2846488384994
[37m[1m[2023-06-25 08:41:29,306][129146] Min Reward on eval: 1989.4238485746434
[37m[1m[2023-06-25 08:41:29,306][129146] Mean Reward across all agents: 2346.9466757044233
[37m[1m[2023-06-25 08:41:29,307][129146] Average Trajectory Length: 999.8009999999999
[36m[2023-06-25 08:41:29,312][129146] mean_value=357.64719639748876, max_value=1933.363984189957
[37m[1m[2023-06-25 08:41:29,315][129146] New mean coefficients: [[ 0.78439105 -0.21447848 -0.181364   -1.2526497  -0.26686895]]
[37m[1m[2023-06-25 08:41:29,316][129146] Moving the mean solution point...
[36m[2023-06-25 08:41:39,133][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 08:41:39,133][129146] FPS: 391226.40
[36m[2023-06-25 08:41:39,135][129146] itr=835, itrs=2000, Progress: 41.75%
[36m[2023-06-25 08:41:50,680][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 08:41:50,680][129146] FPS: 333268.84
[36m[2023-06-25 08:41:55,516][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:41:55,516][129146] Reward + Measures: [[2747.78306585    0.45858356    0.19960915    0.21806407    0.3389869 ]]
[37m[1m[2023-06-25 08:41:55,517][129146] Max Reward on eval: 2747.783065845934
[37m[1m[2023-06-25 08:41:55,517][129146] Min Reward on eval: 2747.783065845934
[37m[1m[2023-06-25 08:41:55,517][129146] Mean Reward across all agents: 2747.783065845934
[37m[1m[2023-06-25 08:41:55,517][129146] Average Trajectory Length: 999.822
[36m[2023-06-25 08:42:00,990][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:42:00,991][129146] Reward + Measures: [[2904.48537273    0.4725        0.20159999    0.21600001    0.34800002]
[37m[1m [2620.80360821    0.44679999    0.20650001    0.222         0.33090001]
[37m[1m [2710.71199675    0.48400003    0.1946        0.22000001    0.33679998]
[37m[1m ...
[37m[1m [2548.40281929    0.46130005    0.1972        0.22760001    0.32339999]
[37m[1m [2418.90114456    0.458         0.22400001    0.24200001    0.3416    ]
[37m[1m [2386.4976083     0.44670001    0.19580001    0.21070002    0.32300001]]
[37m[1m[2023-06-25 08:42:00,991][129146] Max Reward on eval: 2904.4853727336276
[37m[1m[2023-06-25 08:42:00,992][129146] Min Reward on eval: 2179.344874101924
[37m[1m[2023-06-25 08:42:00,992][129146] Mean Reward across all agents: 2597.5631487571586
[37m[1m[2023-06-25 08:42:00,992][129146] Average Trajectory Length: 998.7149999999999
[36m[2023-06-25 08:42:00,999][129146] mean_value=341.35009242956227, max_value=2891.478140786884
[37m[1m[2023-06-25 08:42:01,002][129146] New mean coefficients: [[ 0.09314644  0.33066815 -0.11226509 -1.0917856  -0.19188538]]
[37m[1m[2023-06-25 08:42:01,003][129146] Moving the mean solution point...
[36m[2023-06-25 08:42:10,810][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 08:42:10,811][129146] FPS: 391616.91
[36m[2023-06-25 08:42:10,813][129146] itr=836, itrs=2000, Progress: 41.80%
[36m[2023-06-25 08:42:22,266][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 08:42:22,266][129146] FPS: 335918.81
[36m[2023-06-25 08:42:27,110][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:42:27,110][129146] Reward + Measures: [[2797.99833536    0.46050671    0.195682      0.2139792     0.33658069]]
[37m[1m[2023-06-25 08:42:27,111][129146] Max Reward on eval: 2797.998335358855
[37m[1m[2023-06-25 08:42:27,111][129146] Min Reward on eval: 2797.998335358855
[37m[1m[2023-06-25 08:42:27,111][129146] Mean Reward across all agents: 2797.998335358855
[37m[1m[2023-06-25 08:42:27,111][129146] Average Trajectory Length: 999.7466666666667
[36m[2023-06-25 08:42:32,648][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:42:32,649][129146] Reward + Measures: [[1882.97447232    0.52990001    0.22580002    0.2586        0.2395    ]
[37m[1m [ 545.236918      0.34678832    0.30898318    0.30633363    0.14301351]
[37m[1m [1615.23286246    0.51490003    0.222         0.2414        0.1805    ]
[37m[1m ...
[37m[1m [1503.0408266     0.31605008    0.21786718    0.24614821    0.42583251]
[37m[1m [1474.60836425    0.44050002    0.2316        0.23819999    0.19700001]
[37m[1m [1746.09901695    0.32810003    0.19020002    0.23140001    0.4305    ]]
[37m[1m[2023-06-25 08:42:32,649][129146] Max Reward on eval: 2892.797802835773
[37m[1m[2023-06-25 08:42:32,649][129146] Min Reward on eval: 380.69287608698943
[37m[1m[2023-06-25 08:42:32,650][129146] Mean Reward across all agents: 1973.3790694377572
[37m[1m[2023-06-25 08:42:32,650][129146] Average Trajectory Length: 995.9453333333333
[36m[2023-06-25 08:42:32,654][129146] mean_value=-30.624601419527583, max_value=2300.0313120427045
[37m[1m[2023-06-25 08:42:32,657][129146] New mean coefficients: [[-0.2234664  0.662446  -0.2825501 -0.7346029 -0.1937424]]
[37m[1m[2023-06-25 08:42:32,658][129146] Moving the mean solution point...
[36m[2023-06-25 08:42:42,374][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 08:42:42,374][129146] FPS: 395292.57
[36m[2023-06-25 08:42:42,376][129146] itr=837, itrs=2000, Progress: 41.85%
[36m[2023-06-25 08:42:53,788][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 08:42:53,788][129146] FPS: 337187.96
[36m[2023-06-25 08:42:58,768][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:42:58,769][129146] Reward + Measures: [[2690.1000476     0.50340098    0.19353531    0.21518567    0.32603732]]
[37m[1m[2023-06-25 08:42:58,769][129146] Max Reward on eval: 2690.1000476036256
[37m[1m[2023-06-25 08:42:58,769][129146] Min Reward on eval: 2690.1000476036256
[37m[1m[2023-06-25 08:42:58,770][129146] Mean Reward across all agents: 2690.1000476036256
[37m[1m[2023-06-25 08:42:58,770][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:43:04,362][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:43:04,363][129146] Reward + Measures: [[2658.79683392    0.51230001    0.2062        0.23009999    0.31810001]
[37m[1m [2323.90925687    0.48740003    0.22210002    0.23099999    0.35919997]
[37m[1m [2450.76029329    0.49550006    0.1952        0.2138        0.33530003]
[37m[1m ...
[37m[1m [2528.60488951    0.49700004    0.20030001    0.2247        0.3457    ]
[37m[1m [2217.73001044    0.53380007    0.21280001    0.2527        0.31790003]
[37m[1m [2612.75263266    0.47589999    0.1955        0.21300001    0.33860001]]
[37m[1m[2023-06-25 08:43:04,363][129146] Max Reward on eval: 2797.0351291094676
[37m[1m[2023-06-25 08:43:04,363][129146] Min Reward on eval: 661.0866703196195
[37m[1m[2023-06-25 08:43:04,363][129146] Mean Reward across all agents: 2175.9081448419342
[37m[1m[2023-06-25 08:43:04,364][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:43:04,365][129146] mean_value=-348.1335408826877, max_value=517.4740528494247
[37m[1m[2023-06-25 08:43:04,368][129146] New mean coefficients: [[-0.46517855  0.18770593 -0.15767005 -0.30547848 -0.1383285 ]]
[37m[1m[2023-06-25 08:43:04,369][129146] Moving the mean solution point...
[36m[2023-06-25 08:43:13,983][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 08:43:13,983][129146] FPS: 399480.74
[36m[2023-06-25 08:43:13,986][129146] itr=838, itrs=2000, Progress: 41.90%
[36m[2023-06-25 08:43:25,402][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 08:43:25,402][129146] FPS: 336991.69
[36m[2023-06-25 08:43:30,171][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:43:30,177][129146] Reward + Measures: [[2403.79051025    0.52317899    0.18547502    0.209204      0.33412665]]
[37m[1m[2023-06-25 08:43:30,177][129146] Max Reward on eval: 2403.7905102482064
[37m[1m[2023-06-25 08:43:30,177][129146] Min Reward on eval: 2403.7905102482064
[37m[1m[2023-06-25 08:43:30,177][129146] Mean Reward across all agents: 2403.7905102482064
[37m[1m[2023-06-25 08:43:30,178][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:43:35,612][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:43:35,618][129146] Reward + Measures: [[1790.98746       0.39980003    0.17510001    0.20670001    0.44169998]
[37m[1m [1129.94062906    0.4797        0.28520003    0.31369999    0.15629999]
[37m[1m [2518.30502697    0.59610003    0.22610001    0.24609999    0.2658    ]
[37m[1m ...
[37m[1m [1594.00155764    0.49449998    0.23410001    0.2379        0.18970001]
[37m[1m [1981.77174505    0.5456        0.25939998    0.2872        0.2177    ]
[37m[1m [1906.25774392    0.40559998    0.17090002    0.206         0.39910001]]
[37m[1m[2023-06-25 08:43:35,618][129146] Max Reward on eval: 2735.1979622934946
[37m[1m[2023-06-25 08:43:35,618][129146] Min Reward on eval: 461.38584115557603
[37m[1m[2023-06-25 08:43:35,619][129146] Mean Reward across all agents: 1842.1516947378952
[37m[1m[2023-06-25 08:43:35,619][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:43:35,621][129146] mean_value=-311.73745927147024, max_value=615.0749954036626
[37m[1m[2023-06-25 08:43:35,624][129146] New mean coefficients: [[-0.39791858 -0.0457727  -0.13583057 -0.22765738 -0.03369883]]
[37m[1m[2023-06-25 08:43:35,625][129146] Moving the mean solution point...
[36m[2023-06-25 08:43:45,289][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 08:43:45,290][129146] FPS: 397417.82
[36m[2023-06-25 08:43:45,292][129146] itr=839, itrs=2000, Progress: 41.95%
[36m[2023-06-25 08:43:56,760][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 08:43:56,760][129146] FPS: 335467.55
[36m[2023-06-25 08:44:01,517][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:44:01,517][129146] Reward + Measures: [[1876.63148853    0.4963493     0.17166466    0.19851168    0.38165399]]
[37m[1m[2023-06-25 08:44:01,517][129146] Max Reward on eval: 1876.631488529509
[37m[1m[2023-06-25 08:44:01,517][129146] Min Reward on eval: 1876.631488529509
[37m[1m[2023-06-25 08:44:01,517][129146] Mean Reward across all agents: 1876.631488529509
[37m[1m[2023-06-25 08:44:01,518][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:44:06,951][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:44:06,952][129146] Reward + Measures: [[1272.67623694    0.50990003    0.17580001    0.23379998    0.38460001]
[37m[1m [1578.09083862    0.49759999    0.1954        0.2208        0.46809998]
[37m[1m [1760.85573254    0.47750002    0.184         0.20439999    0.39669999]
[37m[1m ...
[37m[1m [1837.44177091    0.48629999    0.17120001    0.19719999    0.40459999]
[37m[1m [1853.33043058    0.46549997    0.19660001    0.2165        0.46289998]
[37m[1m [1184.17412427    0.4271        0.1858        0.2388        0.40279999]]
[37m[1m[2023-06-25 08:44:06,952][129146] Max Reward on eval: 2017.193797126552
[37m[1m[2023-06-25 08:44:06,952][129146] Min Reward on eval: 861.2515494223801
[37m[1m[2023-06-25 08:44:06,953][129146] Mean Reward across all agents: 1758.1539493338084
[37m[1m[2023-06-25 08:44:06,953][129146] Average Trajectory Length: 999.8366666666666
[36m[2023-06-25 08:44:06,955][129146] mean_value=-284.75729887411717, max_value=2325.6934231812
[37m[1m[2023-06-25 08:44:06,957][129146] New mean coefficients: [[-1.4920651   0.13694707 -0.46263713  0.18863702  0.1597932 ]]
[37m[1m[2023-06-25 08:44:06,958][129146] Moving the mean solution point...
[36m[2023-06-25 08:44:16,739][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 08:44:16,740][129146] FPS: 392664.76
[36m[2023-06-25 08:44:16,742][129146] itr=840, itrs=2000, Progress: 42.00%
[37m[1m[2023-06-25 08:44:23,074][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000820
[36m[2023-06-25 08:44:34,721][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 08:44:34,721][129146] FPS: 336354.47
[36m[2023-06-25 08:44:39,462][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:44:39,463][129146] Reward + Measures: [[1295.91402844    0.45020968    0.169761      0.20907332    0.42606035]]
[37m[1m[2023-06-25 08:44:39,463][129146] Max Reward on eval: 1295.9140284353514
[37m[1m[2023-06-25 08:44:39,463][129146] Min Reward on eval: 1295.9140284353514
[37m[1m[2023-06-25 08:44:39,463][129146] Mean Reward across all agents: 1295.9140284353514
[37m[1m[2023-06-25 08:44:39,464][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:44:44,957][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:44:44,957][129146] Reward + Measures: [[ 703.8943933     0.49810001    0.21180001    0.30090001    0.39060003]
[37m[1m [ 525.64923417    0.59560007    0.30080003    0.352         0.22040001]
[37m[1m [1462.39349583    0.5158        0.1815        0.2509        0.49499997]
[37m[1m ...
[37m[1m [1416.75682022    0.47139999    0.16869999    0.2131        0.45970002]
[37m[1m [1283.66805895    0.4443        0.16830002    0.21440001    0.46149999]
[37m[1m [1175.74090889    0.43919998    0.1688        0.21200001    0.45520002]]
[37m[1m[2023-06-25 08:44:44,957][129146] Max Reward on eval: 1617.350741444528
[37m[1m[2023-06-25 08:44:44,958][129146] Min Reward on eval: 414.92023826811237
[37m[1m[2023-06-25 08:44:44,958][129146] Mean Reward across all agents: 1099.8667267322412
[37m[1m[2023-06-25 08:44:44,958][129146] Average Trajectory Length: 999.241
[36m[2023-06-25 08:44:44,959][129146] mean_value=-1083.985243250781, max_value=153.85743732670085
[37m[1m[2023-06-25 08:44:44,962][129146] New mean coefficients: [[-1.5278245   0.4749551   0.27765292  0.05340676  0.01619397]]
[37m[1m[2023-06-25 08:44:44,963][129146] Moving the mean solution point...
[36m[2023-06-25 08:44:54,774][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 08:44:54,774][129146] FPS: 391462.92
[36m[2023-06-25 08:44:54,776][129146] itr=841, itrs=2000, Progress: 42.05%
[36m[2023-06-25 08:45:06,440][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 08:45:06,441][129146] FPS: 329934.56
[36m[2023-06-25 08:45:11,129][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:45:11,129][129146] Reward + Measures: [[477.94232108   0.46714315   0.20407638   0.30246505   0.35811457]]
[37m[1m[2023-06-25 08:45:11,129][129146] Max Reward on eval: 477.9423210762615
[37m[1m[2023-06-25 08:45:11,129][129146] Min Reward on eval: 477.9423210762615
[37m[1m[2023-06-25 08:45:11,130][129146] Mean Reward across all agents: 477.9423210762615
[37m[1m[2023-06-25 08:45:11,130][129146] Average Trajectory Length: 999.0283333333333
[36m[2023-06-25 08:45:16,814][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:45:16,815][129146] Reward + Measures: [[376.54127467   0.48210001   0.2088       0.2895       0.37400001]
[37m[1m [451.36426971   0.49120003   0.21759999   0.33430001   0.34079999]
[37m[1m [445.14960809   0.51449519   0.21532822   0.33664629   0.34629878]
[37m[1m ...
[37m[1m [482.72841233   0.48359999   0.23280001   0.331        0.32950002]
[37m[1m [446.94581503   0.47930002   0.21180001   0.32769999   0.34960005]
[37m[1m [463.32979802   0.47930002   0.2105       0.33750004   0.3215    ]]
[37m[1m[2023-06-25 08:45:16,815][129146] Max Reward on eval: 592.4923493664246
[37m[1m[2023-06-25 08:45:16,815][129146] Min Reward on eval: 350.32812553031255
[37m[1m[2023-06-25 08:45:16,816][129146] Mean Reward across all agents: 464.9055481323212
[37m[1m[2023-06-25 08:45:16,816][129146] Average Trajectory Length: 998.1216666666667
[36m[2023-06-25 08:45:16,817][129146] mean_value=-1349.9281000294768, max_value=-273.3355192673554
[36m[2023-06-25 08:45:16,819][129146] XNES is restarting with a new solution whose measures are [0.38589999 0.40470001 0.49460003 0.6753    ] and objective is 36.388523807935414
[36m[2023-06-25 08:45:16,820][129146] Emitter restarted. Changing the mean agent...
[37m[1m[2023-06-25 08:45:16,822][129146] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]]
[37m[1m[2023-06-25 08:45:16,823][129146] Moving the mean solution point...
[36m[2023-06-25 08:45:26,516][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 08:45:26,516][129146] FPS: 396240.64
[36m[2023-06-25 08:45:26,518][129146] itr=842, itrs=2000, Progress: 42.10%
[36m[2023-06-25 08:45:37,952][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 08:45:37,953][129146] FPS: 336512.26
[36m[2023-06-25 08:45:42,758][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:45:42,758][129146] Reward + Measures: [[-31.66859894   0.40281379   0.30400831   0.34628052   0.40633607]]
[37m[1m[2023-06-25 08:45:42,759][129146] Max Reward on eval: -31.66859894425355
[37m[1m[2023-06-25 08:45:42,759][129146] Min Reward on eval: -31.66859894425355
[37m[1m[2023-06-25 08:45:42,759][129146] Mean Reward across all agents: -31.66859894425355
[37m[1m[2023-06-25 08:45:42,759][129146] Average Trajectory Length: 990.1469999999999
[36m[2023-06-25 08:45:48,165][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:45:48,165][129146] Reward + Measures: [[ -454.46199232     0.41459998     0.28190002     0.35900003
[37m[1m      0.33160001]
[37m[1m [ -472.41486613     0.24629998     0.2145         0.23200002
[37m[1m      0.1841    ]
[37m[1m [ -908.69811598     0.31905633     0.42264619     0.27019697
[37m[1m      0.4524467 ]
[37m[1m ...
[37m[1m [ -709.15905081     0.33846489     0.17781495     0.25908256
[37m[1m      0.11308958]
[37m[1m [ -531.12450436     0.26363033     0.31304848     0.30581513
[37m[1m      0.27903637]
[37m[1m [-1051.15770381     0.22781228     0.2075392      0.21963365
[37m[1m      0.19206136]]
[37m[1m[2023-06-25 08:45:48,166][129146] Max Reward on eval: 270.52187194876603
[37m[1m[2023-06-25 08:45:48,166][129146] Min Reward on eval: -1603.4244781162822
[37m[1m[2023-06-25 08:45:48,166][129146] Mean Reward across all agents: -631.0748813339492
[37m[1m[2023-06-25 08:45:48,166][129146] Average Trajectory Length: 894.6759999999999
[36m[2023-06-25 08:45:48,168][129146] mean_value=-2151.988819703649, max_value=75.20370262219924
[37m[1m[2023-06-25 08:45:48,170][129146] New mean coefficients: [[ 0.9865949  -0.00430274 -0.45581174 -2.572176   -0.19729674]]
[37m[1m[2023-06-25 08:45:48,171][129146] Moving the mean solution point...
[36m[2023-06-25 08:45:57,787][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 08:45:57,787][129146] FPS: 399419.67
[36m[2023-06-25 08:45:57,789][129146] itr=843, itrs=2000, Progress: 42.15%
[36m[2023-06-25 08:46:09,280][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 08:46:09,281][129146] FPS: 334802.49
[36m[2023-06-25 08:46:14,040][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:46:14,041][129146] Reward + Measures: [[-89.18876013   0.1840407    0.74290413   0.2754761    0.80397409]]
[37m[1m[2023-06-25 08:46:14,041][129146] Max Reward on eval: -89.18876013175782
[37m[1m[2023-06-25 08:46:14,041][129146] Min Reward on eval: -89.18876013175782
[37m[1m[2023-06-25 08:46:14,041][129146] Mean Reward across all agents: -89.18876013175782
[37m[1m[2023-06-25 08:46:14,041][129146] Average Trajectory Length: 996.4793333333333
[36m[2023-06-25 08:46:19,482][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:46:19,483][129146] Reward + Measures: [[ -241.46635203     0.34216166     0.20187011     0.33995628
[37m[1m      0.28282037]
[37m[1m [ -238.60223712     0.43645191     0.2609596      0.33388844
[37m[1m      0.29798269]
[37m[1m [-1051.70413369     0.3748         0.37540001     0.35189998
[37m[1m      0.43449998]
[37m[1m ...
[37m[1m [-1087.80946914     0.33714801     0.27530089     0.30363485
[37m[1m      0.26693594]
[37m[1m [ -986.20720258     0.44930002     0.26970002     0.50889999
[37m[1m      0.19840001]
[37m[1m [ -722.67468377     0.34986156     0.22783275     0.34524989
[37m[1m      0.20304707]]
[37m[1m[2023-06-25 08:46:19,483][129146] Max Reward on eval: 630.6543124754564
[37m[1m[2023-06-25 08:46:19,483][129146] Min Reward on eval: -1399.6239651888143
[37m[1m[2023-06-25 08:46:19,483][129146] Mean Reward across all agents: -399.9914550185745
[37m[1m[2023-06-25 08:46:19,484][129146] Average Trajectory Length: 939.4526666666667
[36m[2023-06-25 08:46:19,486][129146] mean_value=-1624.5469944056258, max_value=490.54673390384875
[37m[1m[2023-06-25 08:46:19,488][129146] New mean coefficients: [[ 0.86886597  0.00315481 -0.7390879  -1.5458156  -0.32381392]]
[37m[1m[2023-06-25 08:46:19,489][129146] Moving the mean solution point...
[36m[2023-06-25 08:46:29,256][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 08:46:29,256][129146] FPS: 393240.38
[36m[2023-06-25 08:46:29,258][129146] itr=844, itrs=2000, Progress: 42.20%
[36m[2023-06-25 08:46:41,084][129146] train() took 11.80 seconds to complete
[36m[2023-06-25 08:46:41,084][129146] FPS: 325419.16
[36m[2023-06-25 08:46:45,903][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:46:45,904][129146] Reward + Measures: [[46.83573033  0.39630085  0.2926555   0.35467064  0.40565568]]
[37m[1m[2023-06-25 08:46:45,904][129146] Max Reward on eval: 46.83573033136303
[37m[1m[2023-06-25 08:46:45,904][129146] Min Reward on eval: 46.83573033136303
[37m[1m[2023-06-25 08:46:45,904][129146] Mean Reward across all agents: 46.83573033136303
[37m[1m[2023-06-25 08:46:45,905][129146] Average Trajectory Length: 985.5136666666666
[36m[2023-06-25 08:46:51,308][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:46:51,309][129146] Reward + Measures: [[ -287.98491093     0.25885147     0.34055659     0.21377006
[37m[1m      0.33213705]
[37m[1m [ -486.68025256     0.2778562      0.29947951     0.19077671
[37m[1m      0.25081575]
[37m[1m [ -418.45494922     0.22801642     0.24126907     0.16240942
[37m[1m      0.2444648 ]
[37m[1m ...
[37m[1m [ -847.79435596     0.38579997     0.27599999     0.32390001
[37m[1m      0.2536    ]
[37m[1m [-1232.67326543     0.322          0.19669999     0.30250001
[37m[1m      0.20709999]
[37m[1m [ -606.34167912     0.2509895      0.27173686     0.25713682
[37m[1m      0.33809474]]
[37m[1m[2023-06-25 08:46:51,309][129146] Max Reward on eval: 150.29616882301633
[37m[1m[2023-06-25 08:46:51,309][129146] Min Reward on eval: -1472.3368123541354
[37m[1m[2023-06-25 08:46:51,310][129146] Mean Reward across all agents: -502.45830206139266
[37m[1m[2023-06-25 08:46:51,310][129146] Average Trajectory Length: 908.5963333333333
[36m[2023-06-25 08:46:51,311][129146] mean_value=-2105.0026924517656, max_value=155.83597050201215
[37m[1m[2023-06-25 08:46:51,314][129146] New mean coefficients: [[ 1.195967    0.07213701 -1.2903509  -0.38228822 -1.5721499 ]]
[37m[1m[2023-06-25 08:46:51,315][129146] Moving the mean solution point...
[36m[2023-06-25 08:47:00,937][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 08:47:00,937][129146] FPS: 399130.78
[36m[2023-06-25 08:47:00,940][129146] itr=845, itrs=2000, Progress: 42.25%
[36m[2023-06-25 08:47:12,461][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 08:47:12,462][129146] FPS: 334009.82
[36m[2023-06-25 08:47:17,205][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:47:17,206][129146] Reward + Measures: [[-61.40833353   0.33086538   0.29090753   0.26170796   0.30161026]]
[37m[1m[2023-06-25 08:47:17,206][129146] Max Reward on eval: -61.40833352569279
[37m[1m[2023-06-25 08:47:17,206][129146] Min Reward on eval: -61.40833352569279
[37m[1m[2023-06-25 08:47:17,206][129146] Mean Reward across all agents: -61.40833352569279
[37m[1m[2023-06-25 08:47:17,207][129146] Average Trajectory Length: 950.5266666666666
[36m[2023-06-25 08:47:22,673][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:47:22,679][129146] Reward + Measures: [[-238.88449816    0.38890418    0.3548336     0.3387017     0.44543535]
[37m[1m [-131.63858393    0.29779869    0.24318504    0.25014532    0.22287372]
[37m[1m [-805.37733668    0.3682        0.40958825    0.38568237    0.4885059 ]
[37m[1m ...
[37m[1m [-211.45071529    0.32409671    0.27568424    0.26188672    0.23542118]
[37m[1m [-126.19507914    0.35180002    0.30590001    0.2552        0.28990003]
[37m[1m [-416.40066465    0.33780739    0.32725558    0.26525083    0.32569632]]
[37m[1m[2023-06-25 08:47:22,679][129146] Max Reward on eval: 19.823999147559515
[37m[1m[2023-06-25 08:47:22,679][129146] Min Reward on eval: -1241.6958361959346
[37m[1m[2023-06-25 08:47:22,680][129146] Mean Reward across all agents: -342.5953836293833
[37m[1m[2023-06-25 08:47:22,680][129146] Average Trajectory Length: 923.7376666666667
[36m[2023-06-25 08:47:22,681][129146] mean_value=-2460.2378063961705, max_value=-107.10624526617534
[36m[2023-06-25 08:47:22,683][129146] XNES is restarting with a new solution whose measures are [0.35819998 0.28040001 0.21870001 0.0654    ] and objective is -20.030021095622214
[36m[2023-06-25 08:47:22,684][129146] Emitter restarted. Changing the mean agent...
[37m[1m[2023-06-25 08:47:22,687][129146] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]]
[37m[1m[2023-06-25 08:47:22,687][129146] Moving the mean solution point...
[36m[2023-06-25 08:47:32,401][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 08:47:32,402][129146] FPS: 395365.53
[36m[2023-06-25 08:47:32,404][129146] itr=846, itrs=2000, Progress: 42.30%
[36m[2023-06-25 08:47:43,853][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 08:47:43,853][129146] FPS: 336031.76
[36m[2023-06-25 08:47:48,589][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:47:48,590][129146] Reward + Measures: [[19.38043765  0.41619685  0.30078804  0.25036946  0.10991593]]
[37m[1m[2023-06-25 08:47:48,590][129146] Max Reward on eval: 19.380437646245138
[37m[1m[2023-06-25 08:47:48,590][129146] Min Reward on eval: 19.380437646245138
[37m[1m[2023-06-25 08:47:48,590][129146] Mean Reward across all agents: 19.380437646245138
[37m[1m[2023-06-25 08:47:48,591][129146] Average Trajectory Length: 994.8303333333333
[36m[2023-06-25 08:47:54,291][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:47:54,292][129146] Reward + Measures: [[  -98.36375099     0.37245348     0.30110791     0.25989243
[37m[1m      0.20602706]
[37m[1m [ -787.60084641     0.30004877     0.25461578     0.34078309
[37m[1m      0.309513  ]
[37m[1m [ -684.48129596     0.24353807     0.13058579     0.30873552
[37m[1m      0.28282365]
[37m[1m ...
[37m[1m [  -44.06821345     0.4070017      0.30162829     0.3160823
[37m[1m      0.24629079]
[37m[1m [-1048.1507809      0.28554446     0.21416076     0.35323995
[37m[1m      0.34098569]
[37m[1m [ -800.79568715     0.1708325      0.1403995      0.14405011
[37m[1m      0.15304607]]
[37m[1m[2023-06-25 08:47:54,292][129146] Max Reward on eval: 239.98359255066026
[37m[1m[2023-06-25 08:47:54,292][129146] Min Reward on eval: -1235.9506546104094
[37m[1m[2023-06-25 08:47:54,293][129146] Mean Reward across all agents: -331.71960970881446
[37m[1m[2023-06-25 08:47:54,293][129146] Average Trajectory Length: 883.7393333333333
[36m[2023-06-25 08:47:54,294][129146] mean_value=-1135.5779475651223, max_value=138.2160140043097
[37m[1m[2023-06-25 08:47:54,297][129146] New mean coefficients: [[ 1.282209  -0.8630495 -0.7902276 -2.6315558 -1.5487713]]
[37m[1m[2023-06-25 08:47:54,298][129146] Moving the mean solution point...
[36m[2023-06-25 08:48:04,155][129146] train() took 9.86 seconds to complete
[36m[2023-06-25 08:48:04,155][129146] FPS: 389631.07
[36m[2023-06-25 08:48:04,157][129146] itr=847, itrs=2000, Progress: 42.35%
[36m[2023-06-25 08:48:15,747][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 08:48:15,747][129146] FPS: 331958.46
[36m[2023-06-25 08:48:20,604][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:48:20,605][129146] Reward + Measures: [[54.77843357  0.39618564  0.28941849  0.23579273  0.10751008]]
[37m[1m[2023-06-25 08:48:20,605][129146] Max Reward on eval: 54.77843356743428
[37m[1m[2023-06-25 08:48:20,605][129146] Min Reward on eval: 54.77843356743428
[37m[1m[2023-06-25 08:48:20,605][129146] Mean Reward across all agents: 54.77843356743428
[37m[1m[2023-06-25 08:48:20,606][129146] Average Trajectory Length: 994.4483333333333
[36m[2023-06-25 08:48:26,004][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:48:26,005][129146] Reward + Measures: [[-279.1432654     0.36080003    0.23239999    0.2191        0.09519999]
[37m[1m [ 125.98821231    0.3213        0.2832        0.1901        0.1165    ]
[37m[1m [-205.66956146    0.39130002    0.25549999    0.3019        0.184     ]
[37m[1m ...
[37m[1m [-732.87641903    0.30244547    0.18487792    0.19803117    0.17231819]
[37m[1m [-223.17525123    0.47090003    0.31080002    0.33520001    0.14219999]
[37m[1m [-257.91291347    0.35849997    0.22810002    0.25560004    0.18620001]]
[37m[1m[2023-06-25 08:48:26,005][129146] Max Reward on eval: 249.11639837428228
[37m[1m[2023-06-25 08:48:26,005][129146] Min Reward on eval: -732.8764190345944
[37m[1m[2023-06-25 08:48:26,006][129146] Mean Reward across all agents: -87.79193035698476
[37m[1m[2023-06-25 08:48:26,006][129146] Average Trajectory Length: 996.473
[36m[2023-06-25 08:48:26,008][129146] mean_value=-1107.6180053267665, max_value=385.6577898457733
[37m[1m[2023-06-25 08:48:26,010][129146] New mean coefficients: [[-0.14088428 -0.56058586 -1.1374866  -2.2995796  -1.9771456 ]]
[37m[1m[2023-06-25 08:48:26,011][129146] Moving the mean solution point...
[36m[2023-06-25 08:48:35,788][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 08:48:35,788][129146] FPS: 392842.63
[36m[2023-06-25 08:48:35,790][129146] itr=848, itrs=2000, Progress: 42.40%
[36m[2023-06-25 08:48:47,249][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 08:48:47,249][129146] FPS: 335784.81
[36m[2023-06-25 08:48:52,031][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:48:52,031][129146] Reward + Measures: [[71.40987874  0.38247329  0.2780939   0.22619651  0.10252117]]
[37m[1m[2023-06-25 08:48:52,031][129146] Max Reward on eval: 71.4098787388425
[37m[1m[2023-06-25 08:48:52,031][129146] Min Reward on eval: 71.4098787388425
[37m[1m[2023-06-25 08:48:52,031][129146] Mean Reward across all agents: 71.4098787388425
[37m[1m[2023-06-25 08:48:52,032][129146] Average Trajectory Length: 992.9323333333333
[36m[2023-06-25 08:48:57,553][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:48:57,554][129146] Reward + Measures: [[ -97.42424235    0.44086638    0.35354125    0.24522901    0.16017556]
[37m[1m [-224.22973145    0.47300002    0.35639998    0.29000002    0.1234    ]
[37m[1m [-587.60174489    0.3486        0.29779997    0.20850001    0.1481    ]
[37m[1m ...
[37m[1m [ -10.74628157    0.36590001    0.22319999    0.37010002    0.24750002]
[37m[1m [-165.29680001    0.25460002    0.15750001    0.28910002    0.2043    ]
[37m[1m [-828.19507598    0.34290004    0.25710002    0.2089        0.1584    ]]
[37m[1m[2023-06-25 08:48:57,554][129146] Max Reward on eval: 279.1124601642281
[37m[1m[2023-06-25 08:48:57,554][129146] Min Reward on eval: -1122.9314509047313
[37m[1m[2023-06-25 08:48:57,555][129146] Mean Reward across all agents: -205.3934756542235
[37m[1m[2023-06-25 08:48:57,555][129146] Average Trajectory Length: 975.851
[36m[2023-06-25 08:48:57,557][129146] mean_value=-1322.6679306306155, max_value=193.37296541342516
[37m[1m[2023-06-25 08:48:57,559][129146] New mean coefficients: [[ 0.2855567  -0.51657104 -0.71420646 -2.1164227  -0.961851  ]]
[37m[1m[2023-06-25 08:48:57,560][129146] Moving the mean solution point...
[36m[2023-06-25 08:49:07,370][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 08:49:07,370][129146] FPS: 391512.78
[36m[2023-06-25 08:49:07,373][129146] itr=849, itrs=2000, Progress: 42.45%
[36m[2023-06-25 08:49:18,896][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 08:49:18,896][129146] FPS: 333931.85
[36m[2023-06-25 08:49:23,757][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:49:23,758][129146] Reward + Measures: [[103.97417226   0.36359733   0.26713353   0.21421669   0.09935416]]
[37m[1m[2023-06-25 08:49:23,758][129146] Max Reward on eval: 103.97417225911931
[37m[1m[2023-06-25 08:49:23,758][129146] Min Reward on eval: 103.97417225911931
[37m[1m[2023-06-25 08:49:23,758][129146] Mean Reward across all agents: 103.97417225911931
[37m[1m[2023-06-25 08:49:23,759][129146] Average Trajectory Length: 993.236
[36m[2023-06-25 08:49:29,195][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:49:29,195][129146] Reward + Measures: [[-332.45317634    0.3527132     0.2916064     0.28229791    0.16865097]
[37m[1m [ 144.14926635    0.27450001    0.2282        0.17430001    0.0859    ]
[37m[1m [-142.21084407    0.29379275    0.27072027    0.24600868    0.17108117]
[37m[1m ...
[37m[1m [ 172.2677263     0.31850001    0.24260001    0.18599999    0.07220001]
[37m[1m [ 117.75763433    0.33699998    0.3664        0.1772        0.29500002]
[37m[1m [ 179.58931346    0.30450001    0.34850001    0.27520001    0.33909997]]
[37m[1m[2023-06-25 08:49:29,196][129146] Max Reward on eval: 260.22293576314695
[37m[1m[2023-06-25 08:49:29,196][129146] Min Reward on eval: -1065.5252608772832
[37m[1m[2023-06-25 08:49:29,196][129146] Mean Reward across all agents: -84.05263001447062
[37m[1m[2023-06-25 08:49:29,196][129146] Average Trajectory Length: 985.2003333333333
[36m[2023-06-25 08:49:29,198][129146] mean_value=-2384.670377589063, max_value=223.3031879848864
[37m[1m[2023-06-25 08:49:29,200][129146] New mean coefficients: [[ 0.16187264 -1.1443992   0.09691668  0.32064724 -1.235402  ]]
[37m[1m[2023-06-25 08:49:29,201][129146] Moving the mean solution point...
[36m[2023-06-25 08:49:38,879][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 08:49:38,879][129146] FPS: 396847.92
[36m[2023-06-25 08:49:38,882][129146] itr=850, itrs=2000, Progress: 42.50%
[37m[1m[2023-06-25 08:49:45,286][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000830
[36m[2023-06-25 08:49:56,969][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 08:49:56,969][129146] FPS: 335716.06
[36m[2023-06-25 08:50:01,700][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:50:01,700][129146] Reward + Measures: [[120.2399196    0.34426928   0.25934544   0.20598963   0.09376094]]
[37m[1m[2023-06-25 08:50:01,700][129146] Max Reward on eval: 120.23991960408198
[37m[1m[2023-06-25 08:50:01,700][129146] Min Reward on eval: 120.23991960408198
[37m[1m[2023-06-25 08:50:01,700][129146] Mean Reward across all agents: 120.23991960408198
[37m[1m[2023-06-25 08:50:01,701][129146] Average Trajectory Length: 990.3523333333333
[36m[2023-06-25 08:50:07,230][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:50:07,230][129146] Reward + Measures: [[  39.11389756    0.39983794    0.28078282    0.24179964    0.09981693]
[37m[1m [-173.06603062    0.2           0.204         0.1587        0.10789999]
[37m[1m [  62.87032048    0.3547        0.27149999    0.1913        0.125     ]
[37m[1m ...
[37m[1m [ 110.8220161     0.53080004    0.3691        0.38499999    0.15460001]
[37m[1m [ 110.85237456    0.41360003    0.32230002    0.25480002    0.1252    ]
[37m[1m [   4.76456307    0.26236925    0.25272417    0.18892637    0.14837913]]
[37m[1m[2023-06-25 08:50:07,230][129146] Max Reward on eval: 258.7069904161064
[37m[1m[2023-06-25 08:50:07,231][129146] Min Reward on eval: -644.1573348515085
[37m[1m[2023-06-25 08:50:07,231][129146] Mean Reward across all agents: -21.39316765256039
[37m[1m[2023-06-25 08:50:07,231][129146] Average Trajectory Length: 960.5546666666667
[36m[2023-06-25 08:50:07,233][129146] mean_value=-814.9166070387262, max_value=439.57717944592844
[37m[1m[2023-06-25 08:50:07,236][129146] New mean coefficients: [[ 0.21326242 -0.748368   -0.14856336  0.80048585 -0.8306437 ]]
[37m[1m[2023-06-25 08:50:07,237][129146] Moving the mean solution point...
[36m[2023-06-25 08:50:16,831][129146] train() took 9.59 seconds to complete
[36m[2023-06-25 08:50:16,831][129146] FPS: 400307.24
[36m[2023-06-25 08:50:16,833][129146] itr=851, itrs=2000, Progress: 42.55%
[36m[2023-06-25 08:50:28,313][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 08:50:28,314][129146] FPS: 335170.29
[36m[2023-06-25 08:50:33,119][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:50:33,120][129146] Reward + Measures: [[121.32527666   0.34155673   0.26217154   0.20853633   0.09518845]]
[37m[1m[2023-06-25 08:50:33,120][129146] Max Reward on eval: 121.32527665775461
[37m[1m[2023-06-25 08:50:33,120][129146] Min Reward on eval: 121.32527665775461
[37m[1m[2023-06-25 08:50:33,121][129146] Mean Reward across all agents: 121.32527665775461
[37m[1m[2023-06-25 08:50:33,121][129146] Average Trajectory Length: 989.2883333333333
[36m[2023-06-25 08:50:38,565][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:50:38,565][129146] Reward + Measures: [[-476.98144537    0.46630001    0.34080002    0.32020003    0.15140001]
[37m[1m [-790.76698204    0.3849        0.25970003    0.27380002    0.1112    ]
[37m[1m [-800.50486647    0.30684754    0.28932953    0.33322626    0.1800328 ]
[37m[1m ...
[37m[1m [  72.4196529     0.42669567    0.26314518    0.25391212    0.14733174]
[37m[1m [-367.47797046    0.26472831    0.20840752    0.24421827    0.20181827]
[37m[1m [  79.88944586    0.38166887    0.26956084    0.25730413    0.17779976]]
[37m[1m[2023-06-25 08:50:38,565][129146] Max Reward on eval: 339.5637113214354
[37m[1m[2023-06-25 08:50:38,566][129146] Min Reward on eval: -1119.3692825991195
[37m[1m[2023-06-25 08:50:38,566][129146] Mean Reward across all agents: -217.34379362255265
[37m[1m[2023-06-25 08:50:38,566][129146] Average Trajectory Length: 923.5559999999999
[36m[2023-06-25 08:50:38,568][129146] mean_value=-1255.3338005058802, max_value=143.2073392711963
[37m[1m[2023-06-25 08:50:38,570][129146] New mean coefficients: [[ 0.51896    -0.59800684 -0.6828737   1.5138094  -0.97627485]]
[37m[1m[2023-06-25 08:50:38,571][129146] Moving the mean solution point...
[36m[2023-06-25 08:50:48,197][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 08:50:48,197][129146] FPS: 398991.58
[36m[2023-06-25 08:50:48,199][129146] itr=852, itrs=2000, Progress: 42.60%
[36m[2023-06-25 08:50:59,790][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 08:50:59,790][129146] FPS: 331920.34
[36m[2023-06-25 08:51:04,696][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:51:04,697][129146] Reward + Measures: [[142.94372377   0.35047665   0.26972842   0.2127513    0.0964894 ]]
[37m[1m[2023-06-25 08:51:04,697][129146] Max Reward on eval: 142.9437237667262
[37m[1m[2023-06-25 08:51:04,697][129146] Min Reward on eval: 142.9437237667262
[37m[1m[2023-06-25 08:51:04,697][129146] Mean Reward across all agents: 142.9437237667262
[37m[1m[2023-06-25 08:51:04,698][129146] Average Trajectory Length: 987.4399999999999
[36m[2023-06-25 08:51:10,216][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:51:10,217][129146] Reward + Measures: [[-210.49784805    0.35769773    0.29586348    0.25498682    0.14078534]
[37m[1m [  11.81973875    0.37764612    0.35088798    0.23521666    0.13353483]
[37m[1m [-122.59623799    0.50940001    0.27900001    0.32010004    0.15770002]
[37m[1m ...
[37m[1m [ 138.36570348    0.491         0.36890003    0.28009999    0.1592    ]
[37m[1m [ -97.63801552    0.252         0.23660003    0.198         0.1243    ]
[37m[1m [-446.48778986    0.45539999    0.2809        0.30990002    0.1148    ]]
[37m[1m[2023-06-25 08:51:10,217][129146] Max Reward on eval: 312.1856599255407
[37m[1m[2023-06-25 08:51:10,217][129146] Min Reward on eval: -778.0228875948117
[37m[1m[2023-06-25 08:51:10,217][129146] Mean Reward across all agents: -107.29152919543974
[37m[1m[2023-06-25 08:51:10,218][129146] Average Trajectory Length: 956.3143333333333
[36m[2023-06-25 08:51:10,219][129146] mean_value=-1210.7472335975995, max_value=192.23665020429857
[37m[1m[2023-06-25 08:51:10,222][129146] New mean coefficients: [[-0.7523712  -0.3631919  -0.48292363  1.7561508  -0.63257873]]
[37m[1m[2023-06-25 08:51:10,223][129146] Moving the mean solution point...
[36m[2023-06-25 08:51:19,862][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 08:51:19,863][129146] FPS: 398418.38
[36m[2023-06-25 08:51:19,865][129146] itr=853, itrs=2000, Progress: 42.65%
[36m[2023-06-25 08:51:31,451][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 08:51:31,451][129146] FPS: 332172.94
[36m[2023-06-25 08:51:36,258][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:51:36,259][129146] Reward + Measures: [[101.26293363   0.37312898   0.2806139    0.22562341   0.09762941]]
[37m[1m[2023-06-25 08:51:36,259][129146] Max Reward on eval: 101.26293363401123
[37m[1m[2023-06-25 08:51:36,259][129146] Min Reward on eval: 101.26293363401123
[37m[1m[2023-06-25 08:51:36,259][129146] Mean Reward across all agents: 101.26293363401123
[37m[1m[2023-06-25 08:51:36,260][129146] Average Trajectory Length: 986.939
[36m[2023-06-25 08:51:41,741][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:51:41,742][129146] Reward + Measures: [[  28.8777745     0.36360002    0.3096        0.2379        0.1276    ]
[37m[1m [  20.58931507    0.46620002    0.36929998    0.317         0.1574    ]
[37m[1m [  22.0883993     0.40900001    0.33559999    0.273         0.1745    ]
[37m[1m ...
[37m[1m [-111.41712479    0.45890003    0.29980001    0.2985        0.13430001]
[37m[1m [ 116.6031773     0.39019999    0.30949998    0.2307        0.1602    ]
[37m[1m [ 134.86419928    0.42880002    0.32210001    0.24519999    0.11360001]]
[37m[1m[2023-06-25 08:51:41,742][129146] Max Reward on eval: 299.3007235855563
[37m[1m[2023-06-25 08:51:41,742][129146] Min Reward on eval: -651.9056810709648
[37m[1m[2023-06-25 08:51:41,743][129146] Mean Reward across all agents: -57.889362948737364
[37m[1m[2023-06-25 08:51:41,743][129146] Average Trajectory Length: 986.7136666666667
[36m[2023-06-25 08:51:41,745][129146] mean_value=-728.0830170953138, max_value=291.74340032650224
[37m[1m[2023-06-25 08:51:41,747][129146] New mean coefficients: [[ 0.21567184  0.36102647 -0.37153423  1.3676102  -0.64383185]]
[37m[1m[2023-06-25 08:51:41,748][129146] Moving the mean solution point...
[36m[2023-06-25 08:51:51,521][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 08:51:51,522][129146] FPS: 392978.08
[36m[2023-06-25 08:51:51,524][129146] itr=854, itrs=2000, Progress: 42.70%
[36m[2023-06-25 08:52:03,094][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 08:52:03,094][129146] FPS: 332519.22
[36m[2023-06-25 08:52:07,948][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:52:07,949][129146] Reward + Measures: [[104.07159905   0.39381477   0.29115447   0.24077135   0.09906992]]
[37m[1m[2023-06-25 08:52:07,949][129146] Max Reward on eval: 104.07159904993777
[37m[1m[2023-06-25 08:52:07,949][129146] Min Reward on eval: 104.07159904993777
[37m[1m[2023-06-25 08:52:07,949][129146] Mean Reward across all agents: 104.07159904993777
[37m[1m[2023-06-25 08:52:07,949][129146] Average Trajectory Length: 991.1543333333333
[36m[2023-06-25 08:52:13,429][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:52:13,429][129146] Reward + Measures: [[   4.79140209    0.40669999    0.28889999    0.25320002    0.1166    ]
[37m[1m [-335.97619864    0.20580001    0.26019999    0.19700001    0.2027    ]
[37m[1m [ -32.77279774    0.45889997    0.3443        0.28550002    0.1416    ]
[37m[1m ...
[37m[1m [ 118.40769854    0.40120003    0.27920002    0.2313        0.1117    ]
[37m[1m [ -99.20933019    0.4657        0.32640001    0.28800002    0.1283    ]
[37m[1m [-141.02264348    0.38410002    0.2994        0.26449999    0.1415    ]]
[37m[1m[2023-06-25 08:52:13,430][129146] Max Reward on eval: 297.4402301711176
[37m[1m[2023-06-25 08:52:13,430][129146] Min Reward on eval: -639.882630448445
[37m[1m[2023-06-25 08:52:13,430][129146] Mean Reward across all agents: -27.304911486556584
[37m[1m[2023-06-25 08:52:13,430][129146] Average Trajectory Length: 974.5616666666666
[36m[2023-06-25 08:52:13,432][129146] mean_value=-1027.3271433145576, max_value=349.6917362462403
[37m[1m[2023-06-25 08:52:13,435][129146] New mean coefficients: [[-0.75943524  0.69010127 -0.6973856   1.4126456  -0.90523165]]
[37m[1m[2023-06-25 08:52:13,435][129146] Moving the mean solution point...
[36m[2023-06-25 08:52:23,177][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 08:52:23,177][129146] FPS: 394263.29
[36m[2023-06-25 08:52:23,179][129146] itr=855, itrs=2000, Progress: 42.75%
[36m[2023-06-25 08:52:34,843][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 08:52:34,843][129146] FPS: 329954.60
[36m[2023-06-25 08:52:39,550][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:52:39,551][129146] Reward + Measures: [[57.30977556  0.39699587  0.2915501   0.24374112  0.0976119 ]]
[37m[1m[2023-06-25 08:52:39,551][129146] Max Reward on eval: 57.30977555828171
[37m[1m[2023-06-25 08:52:39,551][129146] Min Reward on eval: 57.30977555828171
[37m[1m[2023-06-25 08:52:39,551][129146] Mean Reward across all agents: 57.30977555828171
[37m[1m[2023-06-25 08:52:39,551][129146] Average Trajectory Length: 991.35
[36m[2023-06-25 08:52:45,168][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:52:45,169][129146] Reward + Measures: [[  52.71262884    0.39950004    0.29990003    0.24540003    0.1177    ]
[37m[1m [  42.56679853    0.42899999    0.3186        0.27219999    0.12750001]
[37m[1m [  47.79565531    0.41170001    0.2685        0.24879999    0.0944    ]
[37m[1m ...
[37m[1m [  76.2203599     0.37939999    0.27349997    0.22990003    0.0798    ]
[37m[1m [ -22.46858061    0.31697038    0.25491112    0.19052222    0.08513703]
[37m[1m [-115.90743354    0.41069999    0.28800002    0.25999999    0.09760001]]
[37m[1m[2023-06-25 08:52:45,169][129146] Max Reward on eval: 188.5405418022652
[37m[1m[2023-06-25 08:52:45,169][129146] Min Reward on eval: -159.66410114528261
[37m[1m[2023-06-25 08:52:45,169][129146] Mean Reward across all agents: 20.155085747632864
[37m[1m[2023-06-25 08:52:45,170][129146] Average Trajectory Length: 993.6963333333333
[36m[2023-06-25 08:52:45,171][129146] mean_value=-245.01607386359277, max_value=104.23863290047994
[37m[1m[2023-06-25 08:52:45,174][129146] New mean coefficients: [[-0.50546473  0.33709288 -0.69285774  1.883926    0.26322943]]
[37m[1m[2023-06-25 08:52:45,175][129146] Moving the mean solution point...
[36m[2023-06-25 08:52:54,832][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 08:52:54,832][129146] FPS: 397698.18
[36m[2023-06-25 08:52:54,834][129146] itr=856, itrs=2000, Progress: 42.80%
[36m[2023-06-25 08:53:06,375][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 08:53:06,375][129146] FPS: 333405.48
[36m[2023-06-25 08:53:11,124][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:53:11,124][129146] Reward + Measures: [[20.45384675  0.41896445  0.30037332  0.25755295  0.10530612]]
[37m[1m[2023-06-25 08:53:11,125][129146] Max Reward on eval: 20.45384675082235
[37m[1m[2023-06-25 08:53:11,125][129146] Min Reward on eval: 20.45384675082235
[37m[1m[2023-06-25 08:53:11,125][129146] Mean Reward across all agents: 20.45384675082235
[37m[1m[2023-06-25 08:53:11,125][129146] Average Trajectory Length: 992.1006666666666
[36m[2023-06-25 08:53:16,508][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:53:16,509][129146] Reward + Measures: [[ 32.672626     0.49079999   0.30379999   0.2834       0.097     ]
[37m[1m [-67.62299917   0.38900003   0.26619998   0.24559999   0.1267    ]
[37m[1m [-63.33846798   0.3558       0.2457       0.22750001   0.0988    ]
[37m[1m ...
[37m[1m [ 72.73153772   0.43389997   0.3423       0.29130003   0.17990001]
[37m[1m [-19.40125874   0.43400002   0.35770002   0.29170001   0.15360001]
[37m[1m [119.63703065   0.4305       0.31070003   0.25050002   0.1566    ]]
[37m[1m[2023-06-25 08:53:16,509][129146] Max Reward on eval: 216.94280179852794
[37m[1m[2023-06-25 08:53:16,509][129146] Min Reward on eval: -185.22808989607728
[37m[1m[2023-06-25 08:53:16,509][129146] Mean Reward across all agents: 5.186516866528489
[37m[1m[2023-06-25 08:53:16,510][129146] Average Trajectory Length: 993.7676666666666
[36m[2023-06-25 08:53:16,511][129146] mean_value=-422.3439089737792, max_value=467.81383862191143
[37m[1m[2023-06-25 08:53:16,514][129146] New mean coefficients: [[-1.9403305  -0.2757004  -1.6054267   0.8639252  -0.36540252]]
[37m[1m[2023-06-25 08:53:16,515][129146] Moving the mean solution point...
[36m[2023-06-25 08:53:26,276][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 08:53:26,276][129146] FPS: 393469.50
[36m[2023-06-25 08:53:26,278][129146] itr=857, itrs=2000, Progress: 42.85%
[36m[2023-06-25 08:53:37,868][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 08:53:37,868][129146] FPS: 332100.69
[36m[2023-06-25 08:53:42,684][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:53:42,685][129146] Reward + Measures: [[-31.40337948   0.4203715    0.30007571   0.26064777   0.10653812]]
[37m[1m[2023-06-25 08:53:42,685][129146] Max Reward on eval: -31.403379475670725
[37m[1m[2023-06-25 08:53:42,685][129146] Min Reward on eval: -31.403379475670725
[37m[1m[2023-06-25 08:53:42,685][129146] Mean Reward across all agents: -31.403379475670725
[37m[1m[2023-06-25 08:53:42,685][129146] Average Trajectory Length: 992.6603333333333
[36m[2023-06-25 08:53:48,177][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:53:48,177][129146] Reward + Measures: [[ -23.43969863    0.34580001    0.2471        0.22760001    0.09339999]
[37m[1m [ -44.41041025    0.4508        0.28080001    0.2685        0.07520001]
[37m[1m [ -99.11654555    0.41879883    0.31531388    0.27909192    0.14964858]
[37m[1m ...
[37m[1m [-106.60744781    0.42300001    0.28400001    0.29879999    0.1165    ]
[37m[1m [ -79.45743289    0.421         0.3249        0.27550003    0.13690001]
[37m[1m [  16.29445369    0.38209999    0.2809        0.26370001    0.11630001]]
[37m[1m[2023-06-25 08:53:48,178][129146] Max Reward on eval: 151.1675913114159
[37m[1m[2023-06-25 08:53:48,178][129146] Min Reward on eval: -184.52085595272948
[37m[1m[2023-06-25 08:53:48,178][129146] Mean Reward across all agents: -42.37007481669266
[37m[1m[2023-06-25 08:53:48,178][129146] Average Trajectory Length: 992.7146666666666
[36m[2023-06-25 08:53:48,180][129146] mean_value=-535.7500111384979, max_value=65.13596663462096
[37m[1m[2023-06-25 08:53:48,182][129146] New mean coefficients: [[-0.40408778 -0.05587891 -3.4959998   1.1248219   0.98076254]]
[37m[1m[2023-06-25 08:53:48,183][129146] Moving the mean solution point...
[36m[2023-06-25 08:53:57,931][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 08:53:57,931][129146] FPS: 394003.77
[36m[2023-06-25 08:53:57,934][129146] itr=858, itrs=2000, Progress: 42.90%
[36m[2023-06-25 08:54:09,378][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 08:54:09,378][129146] FPS: 336203.73
[36m[2023-06-25 08:54:14,174][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:54:14,175][129146] Reward + Measures: [[-54.0547961    0.42021638   0.2949338    0.26513115   0.11286507]]
[37m[1m[2023-06-25 08:54:14,175][129146] Max Reward on eval: -54.054796103538976
[37m[1m[2023-06-25 08:54:14,175][129146] Min Reward on eval: -54.054796103538976
[37m[1m[2023-06-25 08:54:14,175][129146] Mean Reward across all agents: -54.054796103538976
[37m[1m[2023-06-25 08:54:14,176][129146] Average Trajectory Length: 993.9593333333333
[36m[2023-06-25 08:54:19,670][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:54:19,676][129146] Reward + Measures: [[-123.82782774    0.45455623    0.28623986    0.27351734    0.08712544]
[37m[1m [-180.60831004    0.45960003    0.29060003    0.2685        0.09320001]
[37m[1m [-150.25239032    0.40490004    0.26640001    0.24960001    0.0855    ]
[37m[1m ...
[37m[1m [ -32.23680505    0.4436        0.29069999    0.27079999    0.1234    ]
[37m[1m [-103.96039405    0.45590001    0.30850002    0.29370001    0.1684    ]
[37m[1m [ -58.45115822    0.46809998    0.28009999    0.29660001    0.1109    ]]
[37m[1m[2023-06-25 08:54:19,676][129146] Max Reward on eval: 55.60728435885103
[37m[1m[2023-06-25 08:54:19,677][129146] Min Reward on eval: -294.841937057137
[37m[1m[2023-06-25 08:54:19,677][129146] Mean Reward across all agents: -88.21528576063899
[37m[1m[2023-06-25 08:54:19,677][129146] Average Trajectory Length: 989.319
[36m[2023-06-25 08:54:19,678][129146] mean_value=-603.663773784806, max_value=9.463998008640743
[37m[1m[2023-06-25 08:54:19,681][129146] New mean coefficients: [[-0.37583715 -0.5714526  -3.5154917   0.2398969  -0.19819039]]
[37m[1m[2023-06-25 08:54:19,682][129146] Moving the mean solution point...
[36m[2023-06-25 08:54:29,443][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 08:54:29,443][129146] FPS: 393471.14
[36m[2023-06-25 08:54:29,445][129146] itr=859, itrs=2000, Progress: 42.95%
[36m[2023-06-25 08:54:41,187][129146] train() took 11.72 seconds to complete
[36m[2023-06-25 08:54:41,187][129146] FPS: 327756.62
[36m[2023-06-25 08:54:46,021][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:54:46,022][129146] Reward + Measures: [[-78.68620977   0.42477286   0.28922978   0.26615152   0.1064091 ]]
[37m[1m[2023-06-25 08:54:46,022][129146] Max Reward on eval: -78.68620976869894
[37m[1m[2023-06-25 08:54:46,022][129146] Min Reward on eval: -78.68620976869894
[37m[1m[2023-06-25 08:54:46,023][129146] Mean Reward across all agents: -78.68620976869894
[37m[1m[2023-06-25 08:54:46,023][129146] Average Trajectory Length: 992.3206666666666
[36m[2023-06-25 08:54:51,460][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:54:51,461][129146] Reward + Measures: [[ 21.52928548   0.35870001   0.22979999   0.21510001   0.0858    ]
[37m[1m [-21.45270586   0.36610004   0.24370001   0.25690001   0.14570001]
[37m[1m [-44.60737004   0.40770003   0.2775       0.25819999   0.13329999]
[37m[1m ...
[37m[1m [-62.59313559   0.41880003   0.27920005   0.28240004   0.1322    ]
[37m[1m [ -5.73162181   0.35244018   0.23212755   0.22821891   0.08416142]
[37m[1m [-13.00413491   0.39810002   0.26280001   0.26110002   0.1321    ]]
[37m[1m[2023-06-25 08:54:51,461][129146] Max Reward on eval: 139.3917009612087
[37m[1m[2023-06-25 08:54:51,461][129146] Min Reward on eval: -252.11404125440168
[37m[1m[2023-06-25 08:54:51,461][129146] Mean Reward across all agents: -44.72098787154409
[37m[1m[2023-06-25 08:54:51,462][129146] Average Trajectory Length: 991.059
[36m[2023-06-25 08:54:51,463][129146] mean_value=-518.6495910919677, max_value=-7.342272594673574
[36m[2023-06-25 08:54:51,465][129146] XNES is restarting with a new solution whose measures are [0.91800004 0.90640002 0.0171     0.92239994] and objective is 319.1023485855083
[36m[2023-06-25 08:54:51,466][129146] Emitter restarted. Changing the mean agent...
[37m[1m[2023-06-25 08:54:51,469][129146] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]]
[37m[1m[2023-06-25 08:54:51,469][129146] Moving the mean solution point...
[36m[2023-06-25 08:55:01,132][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 08:55:01,132][129146] FPS: 397480.31
[36m[2023-06-25 08:55:01,134][129146] itr=860, itrs=2000, Progress: 43.00%
[37m[1m[2023-06-25 08:55:07,757][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000840
[36m[2023-06-25 08:55:19,609][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 08:55:19,610][129146] FPS: 330739.23
[36m[2023-06-25 08:55:24,424][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:55:24,424][129146] Reward + Measures: [[521.04146904   0.73960817   0.75401497   0.02498553   0.77891958]]
[37m[1m[2023-06-25 08:55:24,425][129146] Max Reward on eval: 521.0414690440015
[37m[1m[2023-06-25 08:55:24,425][129146] Min Reward on eval: 521.0414690440015
[37m[1m[2023-06-25 08:55:24,425][129146] Mean Reward across all agents: 521.0414690440015
[37m[1m[2023-06-25 08:55:24,425][129146] Average Trajectory Length: 999.415
[36m[2023-06-25 08:55:29,919][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:55:29,919][129146] Reward + Measures: [[ 424.24267322    0.79980004    0.81160003    0.022         0.8472001 ]
[37m[1m [ 738.84172327    0.45319995    0.50419998    0.0564        0.4756    ]
[37m[1m [ 313.389052      0.67669994    0.69760007    0.0279        0.66790003]
[37m[1m ...
[37m[1m [1143.35944124    0.38119999    0.39569998    0.06          0.30780002]
[37m[1m [ 720.98802012    0.46090004    0.54530001    0.0556        0.49390003]
[37m[1m [ 611.83145745    0.47959995    0.49900004    0.0603        0.40349999]]
[37m[1m[2023-06-25 08:55:29,919][129146] Max Reward on eval: 1221.7640299895545
[37m[1m[2023-06-25 08:55:29,920][129146] Min Reward on eval: -15.804267849965253
[37m[1m[2023-06-25 08:55:29,920][129146] Mean Reward across all agents: 589.5662555377099
[37m[1m[2023-06-25 08:55:29,920][129146] Average Trajectory Length: 999.0106666666667
[36m[2023-06-25 08:55:29,921][129146] mean_value=-945.3682948510683, max_value=605.4117598757514
[37m[1m[2023-06-25 08:55:29,924][129146] New mean coefficients: [[ 0.31372568 -0.2863573  -0.60187745 -0.58617425  0.29362726]]
[37m[1m[2023-06-25 08:55:29,925][129146] Moving the mean solution point...
[36m[2023-06-25 08:55:39,578][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 08:55:39,578][129146] FPS: 397873.64
[36m[2023-06-25 08:55:39,580][129146] itr=861, itrs=2000, Progress: 43.05%
[36m[2023-06-25 08:55:51,069][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 08:55:51,070][129146] FPS: 334851.64
[36m[2023-06-25 08:55:55,861][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:55:55,862][129146] Reward + Measures: [[860.22391152   0.550776     0.57844138   0.04205738   0.56113631]]
[37m[1m[2023-06-25 08:55:55,862][129146] Max Reward on eval: 860.223911522286
[37m[1m[2023-06-25 08:55:55,862][129146] Min Reward on eval: 860.223911522286
[37m[1m[2023-06-25 08:55:55,862][129146] Mean Reward across all agents: 860.223911522286
[37m[1m[2023-06-25 08:55:55,863][129146] Average Trajectory Length: 999.586
[36m[2023-06-25 08:56:01,482][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:56:01,482][129146] Reward + Measures: [[ 985.51821337    0.47610003    0.50400001    0.0542        0.40459999]
[37m[1m [ 808.04210116    0.49460003    0.50559998    0.0475        0.53539997]
[37m[1m [ 577.9179384     0.70200002    0.72600001    0.0371        0.73780006]
[37m[1m ...
[37m[1m [1126.76735048    0.3775        0.39770001    0.09320001    0.32790002]
[37m[1m [ 542.81294729    0.76440001    0.70670003    0.0742        0.67379999]
[37m[1m [ 581.66792844    0.71090001    0.71240002    0.0238        0.72250003]]
[37m[1m[2023-06-25 08:56:01,482][129146] Max Reward on eval: 1352.6909109545581
[37m[1m[2023-06-25 08:56:01,483][129146] Min Reward on eval: 376.68126299047145
[37m[1m[2023-06-25 08:56:01,483][129146] Mean Reward across all agents: 866.7342050577217
[37m[1m[2023-06-25 08:56:01,483][129146] Average Trajectory Length: 996.3603333333333
[36m[2023-06-25 08:56:01,485][129146] mean_value=-948.7085177527067, max_value=577.91110444765
[37m[1m[2023-06-25 08:56:01,487][129146] New mean coefficients: [[ 1.1977915   1.3262051   1.133476   -0.38414246  0.7664933 ]]
[37m[1m[2023-06-25 08:56:01,489][129146] Moving the mean solution point...
[36m[2023-06-25 08:56:11,298][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 08:56:11,298][129146] FPS: 391541.50
[36m[2023-06-25 08:56:11,300][129146] itr=862, itrs=2000, Progress: 43.10%
[36m[2023-06-25 08:56:22,924][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 08:56:22,924][129146] FPS: 331097.29
[36m[2023-06-25 08:56:27,745][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:56:27,746][129146] Reward + Measures: [[1118.1128063     0.48835742    0.51557285    0.05831652    0.45185953]]
[37m[1m[2023-06-25 08:56:27,746][129146] Max Reward on eval: 1118.1128063019166
[37m[1m[2023-06-25 08:56:27,746][129146] Min Reward on eval: 1118.1128063019166
[37m[1m[2023-06-25 08:56:27,746][129146] Mean Reward across all agents: 1118.1128063019166
[37m[1m[2023-06-25 08:56:27,746][129146] Average Trajectory Length: 999.6089999999999
[36m[2023-06-25 08:56:33,231][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:56:33,232][129146] Reward + Measures: [[ 726.46422309    0.56890005    0.59790003    0.063         0.57790005]
[37m[1m [ 748.41554813    0.57859999    0.57190001    0.0591        0.57080001]
[37m[1m [ 586.47560878    0.69870001    0.61190003    0.1372        0.65450001]
[37m[1m ...
[37m[1m [1076.31114291    0.43070003    0.4413        0.1772        0.2737    ]
[37m[1m [1357.52130321    0.3831        0.38680002    0.1072        0.24399999]
[37m[1m [ 857.631377      0.54579997    0.57809997    0.0781        0.52210003]]
[37m[1m[2023-06-25 08:56:33,232][129146] Max Reward on eval: 1517.281447566906
[37m[1m[2023-06-25 08:56:33,232][129146] Min Reward on eval: 249.7672025993059
[37m[1m[2023-06-25 08:56:33,232][129146] Mean Reward across all agents: 976.9970857297346
[37m[1m[2023-06-25 08:56:33,233][129146] Average Trajectory Length: 999.1826666666666
[36m[2023-06-25 08:56:33,235][129146] mean_value=-578.1016179779601, max_value=720.8286346243842
[37m[1m[2023-06-25 08:56:33,237][129146] New mean coefficients: [[ 0.19421303  1.0667163   0.749804   -0.22020562  1.7814345 ]]
[37m[1m[2023-06-25 08:56:33,238][129146] Moving the mean solution point...
[36m[2023-06-25 08:56:42,958][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 08:56:42,959][129146] FPS: 395119.06
[36m[2023-06-25 08:56:42,961][129146] itr=863, itrs=2000, Progress: 43.15%
[36m[2023-06-25 08:56:54,516][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 08:56:54,516][129146] FPS: 332984.93
[36m[2023-06-25 08:56:59,390][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:56:59,390][129146] Reward + Measures: [[1005.74149317    0.55289537    0.57944       0.05085767    0.53995669]]
[37m[1m[2023-06-25 08:56:59,391][129146] Max Reward on eval: 1005.7414931650452
[37m[1m[2023-06-25 08:56:59,391][129146] Min Reward on eval: 1005.7414931650452
[37m[1m[2023-06-25 08:56:59,391][129146] Mean Reward across all agents: 1005.7414931650452
[37m[1m[2023-06-25 08:56:59,391][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:57:04,924][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:57:04,925][129146] Reward + Measures: [[ 422.06479733    0.84090006    0.84359998    0.032         0.84160006]
[37m[1m [1100.95322453    0.50570005    0.54409999    0.0716        0.47659999]
[37m[1m [ 992.03768496    0.49400005    0.54150003    0.0892        0.47100002]
[37m[1m ...
[37m[1m [ 662.91131309    0.66900003    0.68239999    0.039         0.6846    ]
[37m[1m [1502.7444851     0.36878404    0.3459087     0.0856087     0.2547493 ]
[37m[1m [ 980.63565761    0.54640001    0.61000007    0.0975        0.45490003]]
[37m[1m[2023-06-25 08:57:04,925][129146] Max Reward on eval: 1559.9794723024359
[37m[1m[2023-06-25 08:57:04,925][129146] Min Reward on eval: 422.0647973314859
[37m[1m[2023-06-25 08:57:04,926][129146] Mean Reward across all agents: 1021.0155484344042
[37m[1m[2023-06-25 08:57:04,926][129146] Average Trajectory Length: 999.324
[36m[2023-06-25 08:57:04,929][129146] mean_value=-556.4583250001455, max_value=1152.4121803428955
[37m[1m[2023-06-25 08:57:04,931][129146] New mean coefficients: [[-0.10758883  1.3563339   2.1732485   0.86679775  1.0173235 ]]
[37m[1m[2023-06-25 08:57:04,932][129146] Moving the mean solution point...
[36m[2023-06-25 08:57:14,792][129146] train() took 9.86 seconds to complete
[36m[2023-06-25 08:57:14,792][129146] FPS: 389548.67
[36m[2023-06-25 08:57:14,794][129146] itr=864, itrs=2000, Progress: 43.20%
[36m[2023-06-25 08:57:26,296][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 08:57:26,296][129146] FPS: 334594.28
[36m[2023-06-25 08:57:31,179][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:57:31,180][129146] Reward + Measures: [[632.64304107   0.72970897   0.75778162   0.029783     0.762743  ]]
[37m[1m[2023-06-25 08:57:31,180][129146] Max Reward on eval: 632.6430410734348
[37m[1m[2023-06-25 08:57:31,180][129146] Min Reward on eval: 632.6430410734348
[37m[1m[2023-06-25 08:57:31,180][129146] Mean Reward across all agents: 632.6430410734348
[37m[1m[2023-06-25 08:57:31,181][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:57:36,649][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:57:36,650][129146] Reward + Measures: [[755.11592595   0.67590004   0.70029998   0.048        0.67720002]
[37m[1m [496.39074375   0.662        0.70900005   0.0301       0.78149998]
[37m[1m [734.34790825   0.60290003   0.64700001   0.0265       0.6924001 ]
[37m[1m ...
[37m[1m [636.34311566   0.77340001   0.80830002   0.0276       0.80600005]
[37m[1m [610.5926926    0.66420001   0.69069999   0.0376       0.70990002]
[37m[1m [779.49699299   0.52920002   0.56350005   0.0282       0.55870003]]
[37m[1m[2023-06-25 08:57:36,650][129146] Max Reward on eval: 1468.2765841073356
[37m[1m[2023-06-25 08:57:36,650][129146] Min Reward on eval: 301.55756665328516
[37m[1m[2023-06-25 08:57:36,650][129146] Mean Reward across all agents: 653.9952665765762
[37m[1m[2023-06-25 08:57:36,651][129146] Average Trajectory Length: 999.4593333333333
[36m[2023-06-25 08:57:36,652][129146] mean_value=-154.4740933458411, max_value=326.7370835986515
[37m[1m[2023-06-25 08:57:36,655][129146] New mean coefficients: [[0.93879336 1.2708169  1.0905544  1.4919193  1.1920334 ]]
[37m[1m[2023-06-25 08:57:36,656][129146] Moving the mean solution point...
[36m[2023-06-25 08:57:46,304][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 08:57:46,304][129146] FPS: 398068.63
[36m[2023-06-25 08:57:46,306][129146] itr=865, itrs=2000, Progress: 43.25%
[36m[2023-06-25 08:57:57,724][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 08:57:57,725][129146] FPS: 337060.24
[36m[2023-06-25 08:58:02,526][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:58:02,527][129146] Reward + Measures: [[582.63288      0.8210327    0.83602262   0.02495533   0.82850301]]
[37m[1m[2023-06-25 08:58:02,527][129146] Max Reward on eval: 582.6328800039711
[37m[1m[2023-06-25 08:58:02,527][129146] Min Reward on eval: 582.6328800039711
[37m[1m[2023-06-25 08:58:02,528][129146] Mean Reward across all agents: 582.6328800039711
[37m[1m[2023-06-25 08:58:02,528][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:58:07,979][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:58:07,979][129146] Reward + Measures: [[366.77907656   0.69300002   0.74669999   0.0447       0.74970007]
[37m[1m [365.86622487   0.89740002   0.87630004   0.0167       0.88899994]
[37m[1m [657.01107991   0.68000001   0.73629999   0.0506       0.65079999]
[37m[1m ...
[37m[1m [738.86894615   0.64589995   0.66930002   0.0441       0.63149995]
[37m[1m [341.64800038   0.71430004   0.78879994   0.0273       0.83190006]
[37m[1m [560.94592542   0.71940005   0.73420006   0.0326       0.70559996]]
[37m[1m[2023-06-25 08:58:07,980][129146] Max Reward on eval: 1330.2374170640483
[37m[1m[2023-06-25 08:58:07,980][129146] Min Reward on eval: 127.25153583948267
[37m[1m[2023-06-25 08:58:07,980][129146] Mean Reward across all agents: 650.4307847972011
[37m[1m[2023-06-25 08:58:07,980][129146] Average Trajectory Length: 999.011
[36m[2023-06-25 08:58:07,982][129146] mean_value=-240.2647271531428, max_value=612.0179287344234
[37m[1m[2023-06-25 08:58:07,985][129146] New mean coefficients: [[1.643463   1.9706042  0.7592106  0.9086444  0.09196126]]
[37m[1m[2023-06-25 08:58:07,986][129146] Moving the mean solution point...
[36m[2023-06-25 08:58:17,691][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 08:58:17,692][129146] FPS: 395723.76
[36m[2023-06-25 08:58:17,694][129146] itr=866, itrs=2000, Progress: 43.30%
[36m[2023-06-25 08:58:29,309][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 08:58:29,310][129146] FPS: 331308.42
[36m[2023-06-25 08:58:33,991][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:58:33,992][129146] Reward + Measures: [[704.42378998   0.8189252    0.82818586   0.02861995   0.80019206]]
[37m[1m[2023-06-25 08:58:33,992][129146] Max Reward on eval: 704.4237899836997
[37m[1m[2023-06-25 08:58:33,992][129146] Min Reward on eval: 704.4237899836997
[37m[1m[2023-06-25 08:58:33,992][129146] Mean Reward across all agents: 704.4237899836997
[37m[1m[2023-06-25 08:58:33,993][129146] Average Trajectory Length: 999.4883333333333
[36m[2023-06-25 08:58:39,379][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:58:39,379][129146] Reward + Measures: [[869.96689616   0.6164       0.63440001   0.0989       0.52539998]
[37m[1m [606.47469995   0.84200001   0.84359998   0.0233       0.838     ]
[37m[1m [547.33795195   0.88129997   0.86930001   0.0246       0.86090004]
[37m[1m ...
[37m[1m [658.70184064   0.73809999   0.74790001   0.0516       0.6843999 ]
[37m[1m [435.28586723   0.86300004   0.85659999   0.0205       0.86420006]
[37m[1m [702.94544257   0.70529997   0.73050004   0.057        0.72760004]]
[37m[1m[2023-06-25 08:58:39,380][129146] Max Reward on eval: 1339.528151858563
[37m[1m[2023-06-25 08:58:39,380][129146] Min Reward on eval: 261.2901493044104
[37m[1m[2023-06-25 08:58:39,380][129146] Mean Reward across all agents: 721.9008594200563
[37m[1m[2023-06-25 08:58:39,380][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:58:39,384][129146] mean_value=-54.000930240287865, max_value=574.8560597237278
[37m[1m[2023-06-25 08:58:39,386][129146] New mean coefficients: [[ 0.76082045  2.770792    1.4984393   0.31960863 -0.6625052 ]]
[37m[1m[2023-06-25 08:58:39,387][129146] Moving the mean solution point...
[36m[2023-06-25 08:58:49,175][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 08:58:49,175][129146] FPS: 392405.34
[36m[2023-06-25 08:58:49,177][129146] itr=867, itrs=2000, Progress: 43.35%
[36m[2023-06-25 08:59:00,797][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 08:59:00,797][129146] FPS: 331212.77
[36m[2023-06-25 08:59:05,699][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:59:05,699][129146] Reward + Measures: [[836.05427131   0.78863299   0.79879475   0.03780833   0.74877065]]
[37m[1m[2023-06-25 08:59:05,700][129146] Max Reward on eval: 836.0542713134174
[37m[1m[2023-06-25 08:59:05,700][129146] Min Reward on eval: 836.0542713134174
[37m[1m[2023-06-25 08:59:05,700][129146] Mean Reward across all agents: 836.0542713134174
[37m[1m[2023-06-25 08:59:05,700][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:59:11,231][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:59:11,231][129146] Reward + Measures: [[1076.23561364    0.66680002    0.70490009    0.0543        0.61340004]
[37m[1m [ 831.39981517    0.7317        0.74830002    0.0576        0.67909998]
[37m[1m [1500.67226748    0.3976        0.45759997    0.10109999    0.30340001]
[37m[1m ...
[37m[1m [ 620.78997651    0.88510001    0.89480001    0.0222        0.87840003]
[37m[1m [ 958.20482291    0.5262        0.54329997    0.13250001    0.36349997]
[37m[1m [ 636.76110738    0.86510003    0.87269992    0.0323        0.84149998]]
[37m[1m[2023-06-25 08:59:11,231][129146] Max Reward on eval: 1500.6722674800083
[37m[1m[2023-06-25 08:59:11,232][129146] Min Reward on eval: 291.62253882553193
[37m[1m[2023-06-25 08:59:11,232][129146] Mean Reward across all agents: 808.2792136743086
[37m[1m[2023-06-25 08:59:11,232][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:59:11,238][129146] mean_value=39.960627705134115, max_value=1034.502056301618
[37m[1m[2023-06-25 08:59:11,240][129146] New mean coefficients: [[ 0.3871587   3.6651824   1.3722758   0.874886   -0.57924736]]
[37m[1m[2023-06-25 08:59:11,241][129146] Moving the mean solution point...
[36m[2023-06-25 08:59:21,068][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 08:59:21,069][129146] FPS: 390819.49
[36m[2023-06-25 08:59:21,071][129146] itr=868, itrs=2000, Progress: 43.40%
[36m[2023-06-25 08:59:32,680][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 08:59:32,680][129146] FPS: 331405.21
[36m[2023-06-25 08:59:37,483][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:59:37,484][129146] Reward + Measures: [[855.63874682   0.79841268   0.80266196   0.041103     0.74755931]]
[37m[1m[2023-06-25 08:59:37,484][129146] Max Reward on eval: 855.6387468244912
[37m[1m[2023-06-25 08:59:37,484][129146] Min Reward on eval: 855.6387468244912
[37m[1m[2023-06-25 08:59:37,484][129146] Mean Reward across all agents: 855.6387468244912
[37m[1m[2023-06-25 08:59:37,484][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 08:59:42,983][129146] Finished Evaluation Step
[37m[1m[2023-06-25 08:59:42,984][129146] Reward + Measures: [[ 881.62426097    0.73080003    0.73330003    0.05800001    0.64799994]
[37m[1m [ 979.78775863    0.72659999    0.73710006    0.0568        0.64309996]
[37m[1m [ 755.72338315    0.86180001    0.86310005    0.0402        0.82359999]
[37m[1m ...
[37m[1m [1007.74655702    0.65309995    0.65599996    0.08880001    0.52389997]
[37m[1m [ 680.28084247    0.85799998    0.85890001    0.0366        0.82639998]
[37m[1m [ 669.10196555    0.8071        0.82069999    0.0633        0.7198    ]]
[37m[1m[2023-06-25 08:59:42,984][129146] Max Reward on eval: 1408.1403464984674
[37m[1m[2023-06-25 08:59:42,984][129146] Min Reward on eval: 333.52895035663386
[37m[1m[2023-06-25 08:59:42,984][129146] Mean Reward across all agents: 855.5094810278221
[37m[1m[2023-06-25 08:59:42,985][129146] Average Trajectory Length: 999.7389999999999
[36m[2023-06-25 08:59:42,989][129146] mean_value=2.09501957363483, max_value=1213.6169328012736
[37m[1m[2023-06-25 08:59:42,992][129146] New mean coefficients: [[0.87529176 3.0483563  1.6848581  0.5856798  0.338207  ]]
[37m[1m[2023-06-25 08:59:42,993][129146] Moving the mean solution point...
[36m[2023-06-25 08:59:52,883][129146] train() took 9.89 seconds to complete
[36m[2023-06-25 08:59:52,883][129146] FPS: 388331.76
[36m[2023-06-25 08:59:52,885][129146] itr=869, itrs=2000, Progress: 43.45%
[36m[2023-06-25 09:00:04,328][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 09:00:04,328][129146] FPS: 336275.33
[36m[2023-06-25 09:00:09,127][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:00:09,127][129146] Reward + Measures: [[836.09214182   0.82883334   0.82541966   0.03838066   0.7750026 ]]
[37m[1m[2023-06-25 09:00:09,127][129146] Max Reward on eval: 836.09214182068
[37m[1m[2023-06-25 09:00:09,127][129146] Min Reward on eval: 836.09214182068
[37m[1m[2023-06-25 09:00:09,128][129146] Mean Reward across all agents: 836.09214182068
[37m[1m[2023-06-25 09:00:09,128][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:00:14,610][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:00:14,610][129146] Reward + Measures: [[709.44837803   0.90799999   0.89860004   0.0274       0.87789994]
[37m[1m [668.48227249   0.89779997   0.87280005   0.0273       0.85519999]
[37m[1m [588.23446407   0.90370005   0.87419999   0.0423       0.8441    ]
[37m[1m ...
[37m[1m [628.56437486   0.89130002   0.89480001   0.0241       0.875     ]
[37m[1m [832.41372022   0.75290006   0.78490001   0.0454       0.72839999]
[37m[1m [383.72749015   0.75629997   0.35990003   0.58199996   0.21859999]]
[37m[1m[2023-06-25 09:00:14,610][129146] Max Reward on eval: 1199.1216534542618
[37m[1m[2023-06-25 09:00:14,611][129146] Min Reward on eval: 190.68420289198403
[37m[1m[2023-06-25 09:00:14,611][129146] Mean Reward across all agents: 674.3709859506267
[37m[1m[2023-06-25 09:00:14,611][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:00:14,617][129146] mean_value=107.96545295219646, max_value=1136.2407506608056
[37m[1m[2023-06-25 09:00:14,620][129146] New mean coefficients: [[1.589514   3.7432168  1.7127934  1.8103259  0.02719298]]
[37m[1m[2023-06-25 09:00:14,621][129146] Moving the mean solution point...
[36m[2023-06-25 09:00:24,358][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 09:00:24,359][129146] FPS: 394410.41
[36m[2023-06-25 09:00:24,361][129146] itr=870, itrs=2000, Progress: 43.50%
[37m[1m[2023-06-25 09:00:30,920][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000850
[36m[2023-06-25 09:00:42,797][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 09:00:42,797][129146] FPS: 329900.15
[36m[2023-06-25 09:00:47,523][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:00:47,523][129146] Reward + Measures: [[916.86162242   0.80656761   0.79640657   0.04901425   0.73088986]]
[37m[1m[2023-06-25 09:00:47,523][129146] Max Reward on eval: 916.8616224206295
[37m[1m[2023-06-25 09:00:47,523][129146] Min Reward on eval: 916.8616224206295
[37m[1m[2023-06-25 09:00:47,524][129146] Mean Reward across all agents: 916.8616224206295
[37m[1m[2023-06-25 09:00:47,524][129146] Average Trajectory Length: 999.9676666666667
[36m[2023-06-25 09:00:52,967][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:00:52,967][129146] Reward + Measures: [[724.68687592   0.86920005   0.74970001   0.1364       0.80290002]
[37m[1m [653.36095287   0.76530004   0.73340005   0.0971       0.71379995]
[37m[1m [543.44832086   0.9005       0.87179995   0.13780001   0.76460004]
[37m[1m ...
[37m[1m [699.16735749   0.9113       0.90200007   0.0277       0.88029999]
[37m[1m [859.58254217   0.77489996   0.76169997   0.0737       0.71410006]
[37m[1m [884.49984766   0.60900003   0.63730001   0.0769       0.57960004]]
[37m[1m[2023-06-25 09:00:52,968][129146] Max Reward on eval: 1349.328216313757
[37m[1m[2023-06-25 09:00:52,968][129146] Min Reward on eval: -138.7274590339395
[37m[1m[2023-06-25 09:00:52,968][129146] Mean Reward across all agents: 720.4007681377748
[37m[1m[2023-06-25 09:00:52,968][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:00:52,972][129146] mean_value=22.386092746004227, max_value=1019.1889904997952
[37m[1m[2023-06-25 09:00:52,975][129146] New mean coefficients: [[1.3532276  3.0332248  0.595389   3.7964888  0.86024475]]
[37m[1m[2023-06-25 09:00:52,976][129146] Moving the mean solution point...
[36m[2023-06-25 09:01:02,660][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 09:01:02,660][129146] FPS: 396602.25
[36m[2023-06-25 09:01:02,663][129146] itr=871, itrs=2000, Progress: 43.55%
[36m[2023-06-25 09:01:14,222][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 09:01:14,223][129146] FPS: 332895.35
[36m[2023-06-25 09:01:18,874][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:01:18,875][129146] Reward + Measures: [[1076.25708574    0.75678742    0.732153      0.06942268    0.6372695 ]]
[37m[1m[2023-06-25 09:01:18,875][129146] Max Reward on eval: 1076.2570857408903
[37m[1m[2023-06-25 09:01:18,875][129146] Min Reward on eval: 1076.2570857408903
[37m[1m[2023-06-25 09:01:18,876][129146] Mean Reward across all agents: 1076.2570857408903
[37m[1m[2023-06-25 09:01:18,876][129146] Average Trajectory Length: 999.7216666666666
[36m[2023-06-25 09:01:24,209][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:01:24,209][129146] Reward + Measures: [[784.53414894   0.84180003   0.80810004   0.0536       0.74870008]
[37m[1m [715.33704188   0.77010006   0.76259995   0.0706       0.67970002]
[37m[1m [891.42843525   0.82479995   0.80769998   0.0492       0.74650002]
[37m[1m ...
[37m[1m [366.91177652   0.74620003   0.62169999   0.21100001   0.69349998]
[37m[1m [695.28036097   0.85120004   0.83160001   0.037        0.80730003]
[37m[1m [724.08028803   0.85760003   0.84829998   0.0425       0.79000002]]
[37m[1m[2023-06-25 09:01:24,209][129146] Max Reward on eval: 1284.5482779341285
[37m[1m[2023-06-25 09:01:24,210][129146] Min Reward on eval: 64.04989187379833
[37m[1m[2023-06-25 09:01:24,210][129146] Mean Reward across all agents: 711.7556932662749
[37m[1m[2023-06-25 09:01:24,210][129146] Average Trajectory Length: 999.7213333333333
[36m[2023-06-25 09:01:24,215][129146] mean_value=63.37596495497168, max_value=1300.3332821384538
[37m[1m[2023-06-25 09:01:24,218][129146] New mean coefficients: [[0.5620372  3.2340817  0.4446938  4.873114   0.90832967]]
[37m[1m[2023-06-25 09:01:24,219][129146] Moving the mean solution point...
[36m[2023-06-25 09:01:33,899][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 09:01:33,899][129146] FPS: 396759.76
[36m[2023-06-25 09:01:33,901][129146] itr=872, itrs=2000, Progress: 43.60%
[36m[2023-06-25 09:01:45,467][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 09:01:45,468][129146] FPS: 332672.85
[36m[2023-06-25 09:01:50,284][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:01:50,284][129146] Reward + Measures: [[1202.20566903    0.70993882    0.66027784    0.10364929    0.5313434 ]]
[37m[1m[2023-06-25 09:01:50,284][129146] Max Reward on eval: 1202.205669027372
[37m[1m[2023-06-25 09:01:50,285][129146] Min Reward on eval: 1202.205669027372
[37m[1m[2023-06-25 09:01:50,285][129146] Mean Reward across all agents: 1202.205669027372
[37m[1m[2023-06-25 09:01:50,285][129146] Average Trajectory Length: 999.7943333333333
[36m[2023-06-25 09:01:55,748][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:01:55,748][129146] Reward + Measures: [[ 644.24179091    0.81199998    0.57600003    0.3188        0.5377    ]
[37m[1m [  13.00750561    0.95819998    0.37869999    0.69410002    0.73660004]
[37m[1m [1072.40497915    0.6965        0.60540003    0.18550001    0.509     ]
[37m[1m ...
[37m[1m [ 541.96600733    0.89519995    0.67469996    0.32690001    0.73049992]
[37m[1m [1282.91532607    0.59759998    0.57450002    0.1833        0.34669998]
[37m[1m [ 992.89743371    0.75720006    0.74349993    0.0598        0.6803    ]]
[37m[1m[2023-06-25 09:01:55,748][129146] Max Reward on eval: 1399.9997572059742
[37m[1m[2023-06-25 09:01:55,749][129146] Min Reward on eval: -127.49103248607135
[37m[1m[2023-06-25 09:01:55,749][129146] Mean Reward across all agents: 698.8655952214483
[37m[1m[2023-06-25 09:01:55,749][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:01:55,763][129146] mean_value=565.0446736400048, max_value=1739.7095443548867
[37m[1m[2023-06-25 09:01:55,766][129146] New mean coefficients: [[1.0125796  2.8639784  0.32260078 4.654509   0.6020175 ]]
[37m[1m[2023-06-25 09:01:55,767][129146] Moving the mean solution point...
[36m[2023-06-25 09:02:05,491][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 09:02:05,492][129146] FPS: 394951.70
[36m[2023-06-25 09:02:05,494][129146] itr=873, itrs=2000, Progress: 43.65%
[36m[2023-06-25 09:02:16,976][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 09:02:16,977][129146] FPS: 335074.87
[36m[2023-06-25 09:02:21,762][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:02:21,763][129146] Reward + Measures: [[1240.54892877    0.71004766    0.639808      0.12737334    0.50325871]]
[37m[1m[2023-06-25 09:02:21,763][129146] Max Reward on eval: 1240.548928765483
[37m[1m[2023-06-25 09:02:21,763][129146] Min Reward on eval: 1240.548928765483
[37m[1m[2023-06-25 09:02:21,764][129146] Mean Reward across all agents: 1240.548928765483
[37m[1m[2023-06-25 09:02:21,764][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:02:27,286][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:02:27,286][129146] Reward + Measures: [[ 175.17158679    0.94129992    0.7238        0.88310003    0.0928    ]
[37m[1m [-888.59340031    0.41859999    0.1639        0.39739999    0.23290001]
[37m[1m [-392.04498646    0.48429999    0.30889997    0.31809998    0.2753    ]
[37m[1m ...
[37m[1m [-937.43747765    0.39610079    0.18206529    0.31976432    0.2497073 ]
[37m[1m [-459.63093667    0.4639        0.2142        0.39470002    0.24660002]
[37m[1m [-287.35514366    0.4835        0.31100002    0.30300003    0.2814    ]]
[37m[1m[2023-06-25 09:02:27,287][129146] Max Reward on eval: 1489.4952918761294
[37m[1m[2023-06-25 09:02:27,287][129146] Min Reward on eval: -1212.074933651602
[37m[1m[2023-06-25 09:02:27,287][129146] Mean Reward across all agents: -90.48098305800588
[37m[1m[2023-06-25 09:02:27,287][129146] Average Trajectory Length: 991.7389999999999
[36m[2023-06-25 09:02:27,290][129146] mean_value=-1154.0545743568975, max_value=900.0573576168158
[37m[1m[2023-06-25 09:02:27,293][129146] New mean coefficients: [[1.3642874  2.749693   0.5702453  1.9686964  0.49148825]]
[37m[1m[2023-06-25 09:02:27,294][129146] Moving the mean solution point...
[36m[2023-06-25 09:02:37,024][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 09:02:37,025][129146] FPS: 394686.83
[36m[2023-06-25 09:02:37,027][129146] itr=874, itrs=2000, Progress: 43.70%
[36m[2023-06-25 09:02:48,436][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 09:02:48,436][129146] FPS: 337265.06
[36m[2023-06-25 09:02:53,313][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:02:53,314][129146] Reward + Measures: [[1410.29074494    0.67367828    0.58333868    0.156692      0.41628101]]
[37m[1m[2023-06-25 09:02:53,314][129146] Max Reward on eval: 1410.2907449356298
[37m[1m[2023-06-25 09:02:53,314][129146] Min Reward on eval: 1410.2907449356298
[37m[1m[2023-06-25 09:02:53,314][129146] Mean Reward across all agents: 1410.2907449356298
[37m[1m[2023-06-25 09:02:53,314][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:02:58,824][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:02:58,830][129146] Reward + Measures: [[856.38810586   0.73729998   0.72580004   0.0715       0.55450004]
[37m[1m [984.52537234   0.67229998   0.60489994   0.1945       0.42080003]
[37m[1m [548.55017949   0.78850001   0.7841       0.0534       0.67990005]
[37m[1m ...
[37m[1m [263.06980222   0.87419999   0.52030003   0.64900005   0.50010002]
[37m[1m [413.51516947   0.74060005   0.53980005   0.33870003   0.45559999]
[37m[1m [698.07073536   0.7726       0.58840007   0.2525       0.6214    ]]
[37m[1m[2023-06-25 09:02:58,830][129146] Max Reward on eval: 1832.3952920487616
[37m[1m[2023-06-25 09:02:58,830][129146] Min Reward on eval: -241.57994140151422
[37m[1m[2023-06-25 09:02:58,831][129146] Mean Reward across all agents: 679.8280809985423
[37m[1m[2023-06-25 09:02:58,831][129146] Average Trajectory Length: 999.7013333333333
[36m[2023-06-25 09:02:58,839][129146] mean_value=282.41875653120894, max_value=1861.2844396373257
[37m[1m[2023-06-25 09:02:58,842][129146] New mean coefficients: [[1.245015   2.5485888  0.2877894  0.9862774  0.40340978]]
[37m[1m[2023-06-25 09:02:58,843][129146] Moving the mean solution point...
[36m[2023-06-25 09:03:08,461][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 09:03:08,462][129146] FPS: 399287.18
[36m[2023-06-25 09:03:08,464][129146] itr=875, itrs=2000, Progress: 43.75%
[36m[2023-06-25 09:03:19,838][129146] train() took 11.35 seconds to complete
[36m[2023-06-25 09:03:19,838][129146] FPS: 338252.18
[36m[2023-06-25 09:03:24,517][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:03:24,517][129146] Reward + Measures: [[1515.814956      0.65839303    0.55519068    0.170386      0.37314567]]
[37m[1m[2023-06-25 09:03:24,517][129146] Max Reward on eval: 1515.814955997205
[37m[1m[2023-06-25 09:03:24,518][129146] Min Reward on eval: 1515.814955997205
[37m[1m[2023-06-25 09:03:24,518][129146] Mean Reward across all agents: 1515.814955997205
[37m[1m[2023-06-25 09:03:24,518][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:03:30,112][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:03:30,117][129146] Reward + Measures: [[-137.41935936    0.35910001    0.1674        0.38950002    0.2579    ]
[37m[1m [ 749.27317236    0.8603        0.70480001    0.2658        0.60839999]
[37m[1m [  82.69781313    0.97349995    0.78420001    0.99050009    0.10450001]
[37m[1m ...
[37m[1m [ 125.68274028    0.49830699    0.4672772     0.24952805    0.42652455]
[37m[1m [ 723.44248382    0.54300004    0.54020005    0.1276        0.42729998]
[37m[1m [  66.70791708    0.81540006    0.23409998    0.71710002    0.1841    ]]
[37m[1m[2023-06-25 09:03:30,118][129146] Max Reward on eval: 1656.1453325486043
[37m[1m[2023-06-25 09:03:30,118][129146] Min Reward on eval: -582.3082454663424
[37m[1m[2023-06-25 09:03:30,118][129146] Mean Reward across all agents: 331.1075621269527
[37m[1m[2023-06-25 09:03:30,118][129146] Average Trajectory Length: 996.4766666666667
[36m[2023-06-25 09:03:30,124][129146] mean_value=-181.277469435109, max_value=1581.945959681999
[37m[1m[2023-06-25 09:03:30,127][129146] New mean coefficients: [[0.54614955 3.1838307  0.00721395 0.2709713  0.2581633 ]]
[37m[1m[2023-06-25 09:03:30,128][129146] Moving the mean solution point...
[36m[2023-06-25 09:03:39,812][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 09:03:39,812][129146] FPS: 396598.12
[36m[2023-06-25 09:03:39,814][129146] itr=876, itrs=2000, Progress: 43.80%
[36m[2023-06-25 09:03:51,564][129146] train() took 11.73 seconds to complete
[36m[2023-06-25 09:03:51,564][129146] FPS: 327450.62
[36m[2023-06-25 09:03:56,421][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:03:56,422][129146] Reward + Measures: [[1532.73614055    0.67383635    0.53943902    0.19499701    0.3495703 ]]
[37m[1m[2023-06-25 09:03:56,422][129146] Max Reward on eval: 1532.7361405520774
[37m[1m[2023-06-25 09:03:56,422][129146] Min Reward on eval: 1532.7361405520774
[37m[1m[2023-06-25 09:03:56,422][129146] Mean Reward across all agents: 1532.7361405520774
[37m[1m[2023-06-25 09:03:56,422][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:04:01,838][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:04:01,838][129146] Reward + Measures: [[ 880.67084848    0.82089996    0.6868        0.18670002    0.57840008]
[37m[1m [1383.81984796    0.67430001    0.56259996    0.24419999    0.30300003]
[37m[1m [1029.14821469    0.75989997    0.51529998    0.39489999    0.3488    ]
[37m[1m ...
[37m[1m [1474.28148225    0.70109999    0.59619999    0.14580001    0.42200002]
[37m[1m [1362.01033701    0.64410001    0.6354        0.12990001    0.48230001]
[37m[1m [1414.71530564    0.66720003    0.4756        0.22140001    0.38519999]]
[37m[1m[2023-06-25 09:04:01,838][129146] Max Reward on eval: 1859.4188527030171
[37m[1m[2023-06-25 09:04:01,839][129146] Min Reward on eval: 112.9494217395666
[37m[1m[2023-06-25 09:04:01,839][129146] Mean Reward across all agents: 1161.395524226915
[37m[1m[2023-06-25 09:04:01,839][129146] Average Trajectory Length: 999.783
[36m[2023-06-25 09:04:01,850][129146] mean_value=614.2836271534206, max_value=1881.8048911382095
[37m[1m[2023-06-25 09:04:01,853][129146] New mean coefficients: [[ 0.41714406  3.1612196  -0.6186006  -0.01791683  0.87039304]]
[37m[1m[2023-06-25 09:04:01,854][129146] Moving the mean solution point...
[36m[2023-06-25 09:04:11,711][129146] train() took 9.85 seconds to complete
[36m[2023-06-25 09:04:11,711][129146] FPS: 389656.57
[36m[2023-06-25 09:04:11,713][129146] itr=877, itrs=2000, Progress: 43.85%
[36m[2023-06-25 09:04:23,291][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 09:04:23,291][129146] FPS: 332317.43
[36m[2023-06-25 09:04:28,123][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:04:28,123][129146] Reward + Measures: [[1557.39971371    0.69334179    0.52760977    0.191525      0.35156655]]
[37m[1m[2023-06-25 09:04:28,124][129146] Max Reward on eval: 1557.3997137124004
[37m[1m[2023-06-25 09:04:28,124][129146] Min Reward on eval: 1557.3997137124004
[37m[1m[2023-06-25 09:04:28,124][129146] Mean Reward across all agents: 1557.3997137124004
[37m[1m[2023-06-25 09:04:28,124][129146] Average Trajectory Length: 999.894
[36m[2023-06-25 09:04:33,642][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:04:33,642][129146] Reward + Measures: [[ -685.58904439     0.21347201     0.2088749      0.10779466
[37m[1m      0.23166338]
[37m[1m [  141.37214675     0.65920001     0.704          0.019
[37m[1m      0.77069998]
[37m[1m [  282.95557323     0.41999999     0.55000001     0.16400002
[37m[1m      0.52820009]
[37m[1m ...
[37m[1m [  -80.88548087     0.81450003     0.3477         0.72060007
[37m[1m      0.17920001]
[37m[1m [-1160.19887455     0.68440008     0.65880007     0.1576
[37m[1m      0.63140005]
[37m[1m [  101.32083108     0.73849994     0.74250001     0.08200001
[37m[1m      0.736     ]]
[37m[1m[2023-06-25 09:04:33,642][129146] Max Reward on eval: 986.6107066402444
[37m[1m[2023-06-25 09:04:33,643][129146] Min Reward on eval: -1256.9289335146545
[37m[1m[2023-06-25 09:04:33,643][129146] Mean Reward across all agents: -106.90671601328974
[37m[1m[2023-06-25 09:04:33,643][129146] Average Trajectory Length: 985.5993333333333
[36m[2023-06-25 09:04:33,646][129146] mean_value=-1341.2101531560174, max_value=1109.5247653521667
[37m[1m[2023-06-25 09:04:33,648][129146] New mean coefficients: [[ 0.7859196   3.907164   -0.45996442  0.7062671   0.24746835]]
[37m[1m[2023-06-25 09:04:33,649][129146] Moving the mean solution point...
[36m[2023-06-25 09:04:43,480][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 09:04:43,480][129146] FPS: 390663.87
[36m[2023-06-25 09:04:43,483][129146] itr=878, itrs=2000, Progress: 43.90%
[36m[2023-06-25 09:04:55,093][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 09:04:55,093][129146] FPS: 331471.80
[36m[2023-06-25 09:04:59,936][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:04:59,942][129146] Reward + Measures: [[1444.54119918    0.73589694    0.53289932    0.24110201    0.37704629]]
[37m[1m[2023-06-25 09:04:59,943][129146] Max Reward on eval: 1444.5411991778483
[37m[1m[2023-06-25 09:04:59,943][129146] Min Reward on eval: 1444.5411991778483
[37m[1m[2023-06-25 09:04:59,944][129146] Mean Reward across all agents: 1444.5411991778483
[37m[1m[2023-06-25 09:04:59,945][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:05:05,286][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:05:05,291][129146] Reward + Measures: [[ 889.8674701     0.8901        0.84980005    0.0421        0.80910009]
[37m[1m [ 713.87962199    0.88090003    0.5873        0.38960004    0.662     ]
[37m[1m [ 390.16978518    0.84219998    0.4217        0.53890002    0.65850002]
[37m[1m ...
[37m[1m [ 930.45677508    0.84930003    0.66090006    0.3646        0.48409995]
[37m[1m [1373.22630686    0.72389996    0.57819998    0.2431        0.30630001]
[37m[1m [ 865.16922073    0.81170005    0.3466        0.46900001    0.62010002]]
[37m[1m[2023-06-25 09:05:05,292][129146] Max Reward on eval: 1886.1119990211446
[37m[1m[2023-06-25 09:05:05,292][129146] Min Reward on eval: -402.48122433973475
[37m[1m[2023-06-25 09:05:05,292][129146] Mean Reward across all agents: 895.2451419292956
[37m[1m[2023-06-25 09:05:05,292][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:05:05,305][129146] mean_value=634.7238744471074, max_value=1983.6563577406807
[37m[1m[2023-06-25 09:05:05,308][129146] New mean coefficients: [[ 0.6341237   3.3387043  -0.45994574  0.6747803  -0.06926548]]
[37m[1m[2023-06-25 09:05:05,309][129146] Moving the mean solution point...
[36m[2023-06-25 09:05:15,066][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 09:05:15,066][129146] FPS: 393639.92
[36m[2023-06-25 09:05:15,069][129146] itr=879, itrs=2000, Progress: 43.95%
[36m[2023-06-25 09:05:26,517][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 09:05:26,517][129146] FPS: 336163.40
[36m[2023-06-25 09:05:31,383][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:05:31,388][129146] Reward + Measures: [[1427.73320687    0.75007534    0.47767732    0.31891936    0.33223668]]
[37m[1m[2023-06-25 09:05:31,389][129146] Max Reward on eval: 1427.733206868476
[37m[1m[2023-06-25 09:05:31,390][129146] Min Reward on eval: 1427.733206868476
[37m[1m[2023-06-25 09:05:31,391][129146] Mean Reward across all agents: 1427.733206868476
[37m[1m[2023-06-25 09:05:31,391][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:05:36,977][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:05:36,977][129146] Reward + Measures: [[ 835.33029611    0.85389996    0.52230006    0.65369999    0.28130001]
[37m[1m [-211.69363359    0.30820858    0.23008648    0.25276679    0.22284865]
[37m[1m [ 745.32232847    0.72590005    0.69430006    0.0942        0.6056    ]
[37m[1m ...
[37m[1m [  52.37631839    0.56400007    0.20920001    0.50489998    0.39630002]
[37m[1m [ 617.18916483    0.76640004    0.76380008    0.08879999    0.61540002]
[37m[1m [ 104.49276826    0.95899993    0.18279999    0.78429997    0.83630002]]
[37m[1m[2023-06-25 09:05:36,978][129146] Max Reward on eval: 1723.4687579806894
[37m[1m[2023-06-25 09:05:36,978][129146] Min Reward on eval: -211.6936335915787
[37m[1m[2023-06-25 09:05:36,978][129146] Mean Reward across all agents: 598.92969733163
[37m[1m[2023-06-25 09:05:36,978][129146] Average Trajectory Length: 997.7806666666667
[36m[2023-06-25 09:05:36,987][129146] mean_value=47.26424625315718, max_value=1874.4013529723045
[37m[1m[2023-06-25 09:05:36,990][129146] New mean coefficients: [[ 0.22681841  2.9104176  -0.28538734  0.07669717 -0.3677379 ]]
[37m[1m[2023-06-25 09:05:36,991][129146] Moving the mean solution point...
[36m[2023-06-25 09:05:46,814][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 09:05:46,814][129146] FPS: 390998.52
[36m[2023-06-25 09:05:46,817][129146] itr=880, itrs=2000, Progress: 44.00%
[37m[1m[2023-06-25 09:05:53,525][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000860
[36m[2023-06-25 09:06:05,171][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 09:06:05,171][129146] FPS: 337054.82
[36m[2023-06-25 09:06:10,008][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:06:10,009][129146] Reward + Measures: [[1266.10307226    0.79352635    0.46384597    0.40932864    0.31477866]]
[37m[1m[2023-06-25 09:06:10,009][129146] Max Reward on eval: 1266.1030722596852
[37m[1m[2023-06-25 09:06:10,009][129146] Min Reward on eval: 1266.1030722596852
[37m[1m[2023-06-25 09:06:10,009][129146] Mean Reward across all agents: 1266.1030722596852
[37m[1m[2023-06-25 09:06:10,010][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:06:15,485][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:06:15,486][129146] Reward + Measures: [[ 685.8991468     0.82740003    0.22500001    0.59490007    0.57380003]
[37m[1m [1204.35988067    0.61630005    0.61519998    0.23889999    0.33580002]
[37m[1m [ 112.86160992    0.5503        0.5018        0.1568        0.51029998]
[37m[1m ...
[37m[1m [-586.05912222    0.39050001    0.23440002    0.20729999    0.1674    ]
[37m[1m [ 895.19148844    0.56100005    0.63670003    0.14910001    0.49270001]
[37m[1m [-272.63703681    0.82310003    0.1122        0.77740002    0.32539999]]
[37m[1m[2023-06-25 09:06:15,486][129146] Max Reward on eval: 1742.8463485576444
[37m[1m[2023-06-25 09:06:15,487][129146] Min Reward on eval: -870.5485318486695
[37m[1m[2023-06-25 09:06:15,487][129146] Mean Reward across all agents: 469.1315492704337
[37m[1m[2023-06-25 09:06:15,487][129146] Average Trajectory Length: 995.1463333333332
[36m[2023-06-25 09:06:15,493][129146] mean_value=-335.3884963899829, max_value=1513.3792025741132
[37m[1m[2023-06-25 09:06:15,496][129146] New mean coefficients: [[ 0.32237053  2.748883   -0.35762215  0.4567993  -0.29130805]]
[37m[1m[2023-06-25 09:06:15,497][129146] Moving the mean solution point...
[36m[2023-06-25 09:06:25,271][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 09:06:25,271][129146] FPS: 392940.40
[36m[2023-06-25 09:06:25,273][129146] itr=881, itrs=2000, Progress: 44.05%
[36m[2023-06-25 09:06:36,760][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 09:06:36,761][129146] FPS: 334922.69
[36m[2023-06-25 09:06:41,539][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:06:41,539][129146] Reward + Measures: [[104.24470075   0.98032826   0.49844369   0.9611237    0.29773965]]
[37m[1m[2023-06-25 09:06:41,539][129146] Max Reward on eval: 104.24470075066469
[37m[1m[2023-06-25 09:06:41,539][129146] Min Reward on eval: 104.24470075066469
[37m[1m[2023-06-25 09:06:41,540][129146] Mean Reward across all agents: 104.24470075066469
[37m[1m[2023-06-25 09:06:41,540][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:06:47,102][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:06:47,103][129146] Reward + Measures: [[834.93718846   0.81919998   0.51399994   0.6268       0.1257    ]
[37m[1m [ 59.90425019   0.97369999   0.1696       0.94950002   0.56980002]
[37m[1m [-80.96962103   0.28620002   0.58700001   0.19949999   0.50780004]
[37m[1m ...
[37m[1m [-98.45213436   0.98569995   0.0252       0.97640002   0.64379996]
[37m[1m [718.77516927   0.61869997   0.59400004   0.45900002   0.23309998]
[37m[1m [157.65743206   0.98820001   0.81440002   0.98579997   0.0064    ]]
[37m[1m[2023-06-25 09:06:47,103][129146] Max Reward on eval: 1189.2637294300716
[37m[1m[2023-06-25 09:06:47,103][129146] Min Reward on eval: -679.1529171907808
[37m[1m[2023-06-25 09:06:47,104][129146] Mean Reward across all agents: 108.30771840206653
[37m[1m[2023-06-25 09:06:47,104][129146] Average Trajectory Length: 998.7446666666666
[36m[2023-06-25 09:06:47,108][129146] mean_value=-409.4738361845264, max_value=808.3339172966866
[37m[1m[2023-06-25 09:06:47,111][129146] New mean coefficients: [[ 0.40547356  2.3808203  -0.4035308   0.6516231   0.02038074]]
[37m[1m[2023-06-25 09:06:47,112][129146] Moving the mean solution point...
[36m[2023-06-25 09:06:56,868][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 09:06:56,868][129146] FPS: 393663.90
[36m[2023-06-25 09:06:56,870][129146] itr=882, itrs=2000, Progress: 44.10%
[36m[2023-06-25 09:07:08,281][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 09:07:08,281][129146] FPS: 337205.65
[36m[2023-06-25 09:07:12,931][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:07:12,931][129146] Reward + Measures: [[82.00875234  0.980299    0.32236066  0.96184194  0.42951831]]
[37m[1m[2023-06-25 09:07:12,931][129146] Max Reward on eval: 82.0087523445192
[37m[1m[2023-06-25 09:07:12,932][129146] Min Reward on eval: 82.0087523445192
[37m[1m[2023-06-25 09:07:12,932][129146] Mean Reward across all agents: 82.0087523445192
[37m[1m[2023-06-25 09:07:12,932][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:07:18,391][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:07:18,391][129146] Reward + Measures: [[ 611.31884064    0.7482        0.30050001    0.52869999    0.2062    ]
[37m[1m [ 168.55116133    0.93350011    0.15699999    0.86680001    0.20390001]
[37m[1m [-654.40736454    0.97290003    0.0043        0.99110001    0.88029999]
[37m[1m ...
[37m[1m [  88.93851673    0.96780008    0.0318        0.95830005    0.45520002]
[37m[1m [-474.14743406    0.93970007    0.0051        0.98949999    0.78599995]
[37m[1m [-962.0903419     0.91530001    0.0053        0.98759997    0.8143    ]]
[37m[1m[2023-06-25 09:07:18,392][129146] Max Reward on eval: 1234.1745400139946
[37m[1m[2023-06-25 09:07:18,392][129146] Min Reward on eval: -1477.6295178797561
[37m[1m[2023-06-25 09:07:18,392][129146] Mean Reward across all agents: 6.353683171919697
[37m[1m[2023-06-25 09:07:18,392][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:07:18,397][129146] mean_value=-339.5787116931466, max_value=1053.9716364257847
[37m[1m[2023-06-25 09:07:18,400][129146] New mean coefficients: [[0.4854858  2.1994095  0.36804047 0.549316   0.90526694]]
[37m[1m[2023-06-25 09:07:18,401][129146] Moving the mean solution point...
[36m[2023-06-25 09:07:28,181][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 09:07:28,182][129146] FPS: 392685.10
[36m[2023-06-25 09:07:28,184][129146] itr=883, itrs=2000, Progress: 44.15%
[36m[2023-06-25 09:07:39,607][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 09:07:39,607][129146] FPS: 336910.03
[36m[2023-06-25 09:07:44,290][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:07:44,290][129146] Reward + Measures: [[161.89202511   0.98536736   0.34210664   0.96435702   0.29651099]]
[37m[1m[2023-06-25 09:07:44,290][129146] Max Reward on eval: 161.89202511091085
[37m[1m[2023-06-25 09:07:44,291][129146] Min Reward on eval: 161.89202511091085
[37m[1m[2023-06-25 09:07:44,291][129146] Mean Reward across all agents: 161.89202511091085
[37m[1m[2023-06-25 09:07:44,291][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:07:49,548][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:07:49,548][129146] Reward + Measures: [[215.09301554   0.98260003   0.34449998   0.95220006   0.17880002]
[37m[1m [245.12346425   0.97080004   0.37779999   0.93889999   0.13100001]
[37m[1m [193.41869168   0.98990005   0.48009998   0.9738       0.2014    ]
[37m[1m ...
[37m[1m [209.04092686   0.97390002   0.46299997   0.96030009   0.156     ]
[37m[1m [211.02121891   0.98210001   0.55339998   0.97920007   0.1261    ]
[37m[1m [182.31228378   0.96960002   0.39750001   0.96420002   0.1885    ]]
[37m[1m[2023-06-25 09:07:49,548][129146] Max Reward on eval: 350.65801643368906
[37m[1m[2023-06-25 09:07:49,549][129146] Min Reward on eval: 31.75262341069756
[37m[1m[2023-06-25 09:07:49,549][129146] Mean Reward across all agents: 205.53193763709157
[37m[1m[2023-06-25 09:07:49,549][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:07:49,554][129146] mean_value=158.54210735119216, max_value=531.026362556017
[37m[1m[2023-06-25 09:07:49,557][129146] New mean coefficients: [[-0.09296942  1.1432629   0.46893618 -0.10695106  0.56492126]]
[37m[1m[2023-06-25 09:07:49,558][129146] Moving the mean solution point...
[36m[2023-06-25 09:07:59,226][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 09:07:59,226][129146] FPS: 397268.23
[36m[2023-06-25 09:07:59,228][129146] itr=884, itrs=2000, Progress: 44.20%
[36m[2023-06-25 09:08:10,697][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 09:08:10,697][129146] FPS: 335614.44
[36m[2023-06-25 09:08:15,465][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:08:15,466][129146] Reward + Measures: [[47.45464724  0.98903292  0.178252    0.97611898  0.56362134]]
[37m[1m[2023-06-25 09:08:15,466][129146] Max Reward on eval: 47.454647241017994
[37m[1m[2023-06-25 09:08:15,466][129146] Min Reward on eval: 47.454647241017994
[37m[1m[2023-06-25 09:08:15,466][129146] Mean Reward across all agents: 47.454647241017994
[37m[1m[2023-06-25 09:08:15,467][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:08:20,838][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:08:20,839][129146] Reward + Measures: [[ 92.99583915   0.99010003   0.12920001   0.97799999   0.54290003]
[37m[1m [ 97.63597793   0.98620003   0.1969       0.98330003   0.65199995]
[37m[1m [ 66.29995713   0.97710001   0.12630001   0.95740002   0.5679    ]
[37m[1m ...
[37m[1m [ 27.0239119    0.98830003   0.16500001   0.97589999   0.56549996]
[37m[1m [148.70567368   0.98850006   0.39410001   0.97650003   0.36490002]
[37m[1m [ -2.17371833   0.98559999   0.0972       0.98130006   0.68900001]]
[37m[1m[2023-06-25 09:08:20,839][129146] Max Reward on eval: 365.7152604929521
[37m[1m[2023-06-25 09:08:20,839][129146] Min Reward on eval: -103.37978019347648
[37m[1m[2023-06-25 09:08:20,840][129146] Mean Reward across all agents: 107.62168194303894
[37m[1m[2023-06-25 09:08:20,840][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:08:20,845][129146] mean_value=71.54426175127993, max_value=789.9367959703318
[37m[1m[2023-06-25 09:08:20,847][129146] New mean coefficients: [[0.22898999 1.4461001  1.0799284  0.13420027 1.4083464 ]]
[37m[1m[2023-06-25 09:08:20,848][129146] Moving the mean solution point...
[36m[2023-06-25 09:08:30,447][129146] train() took 9.60 seconds to complete
[36m[2023-06-25 09:08:30,453][129146] FPS: 400106.47
[36m[2023-06-25 09:08:30,456][129146] itr=885, itrs=2000, Progress: 44.25%
[36m[2023-06-25 09:08:42,028][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 09:08:42,029][129146] FPS: 332435.54
[36m[2023-06-25 09:08:46,830][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:08:46,830][129146] Reward + Measures: [[-91.00302433   0.98563457   0.01280067   0.98003465   0.941405  ]]
[37m[1m[2023-06-25 09:08:46,830][129146] Max Reward on eval: -91.00302433101643
[37m[1m[2023-06-25 09:08:46,831][129146] Min Reward on eval: -91.00302433101643
[37m[1m[2023-06-25 09:08:46,831][129146] Mean Reward across all agents: -91.00302433101643
[37m[1m[2023-06-25 09:08:46,831][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:08:52,436][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:08:52,436][129146] Reward + Measures: [[-143.98921348    0.97250003    0.0506        0.98210001    0.77710003]
[37m[1m [ 262.98086748    0.90830004    0.13080001    0.80529994    0.84680003]
[37m[1m [ 151.21294891    0.96079999    0.24600001    0.97040004    0.53439999]
[37m[1m ...
[37m[1m [ 121.24949132    0.96530002    0.0837        0.97349995    0.77389997]
[37m[1m [-134.9663649     0.98979998    0.0018        0.9896        0.95370007]
[37m[1m [  87.47702919    0.88319999    0.0308        0.89709997    0.67919999]]
[37m[1m[2023-06-25 09:08:52,436][129146] Max Reward on eval: 1089.0174767189426
[37m[1m[2023-06-25 09:08:52,437][129146] Min Reward on eval: -250.35216009062424
[37m[1m[2023-06-25 09:08:52,437][129146] Mean Reward across all agents: 113.33039944390514
[37m[1m[2023-06-25 09:08:52,437][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:08:52,442][129146] mean_value=-153.60866858668908, max_value=1143.6423163104337
[37m[1m[2023-06-25 09:08:52,444][129146] New mean coefficients: [[ 0.40609473  1.2359891   2.1793852  -0.7470671   1.3013546 ]]
[37m[1m[2023-06-25 09:08:52,445][129146] Moving the mean solution point...
[36m[2023-06-25 09:09:02,221][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 09:09:02,221][129146] FPS: 392879.30
[36m[2023-06-25 09:09:02,224][129146] itr=886, itrs=2000, Progress: 44.30%
[36m[2023-06-25 09:09:13,748][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 09:09:13,748][129146] FPS: 333848.03
[36m[2023-06-25 09:09:18,550][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:09:18,550][129146] Reward + Measures: [[-53.94916515   0.98453259   0.01511833   0.97713262   0.94217861]]
[37m[1m[2023-06-25 09:09:18,550][129146] Max Reward on eval: -53.94916515025012
[37m[1m[2023-06-25 09:09:18,551][129146] Min Reward on eval: -53.94916515025012
[37m[1m[2023-06-25 09:09:18,551][129146] Mean Reward across all agents: -53.94916515025012
[37m[1m[2023-06-25 09:09:18,551][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:09:23,908][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:09:23,909][129146] Reward + Measures: [[247.64879397   0.73550004   0.22759998   0.60540003   0.43080002]
[37m[1m [539.62997649   0.71180004   0.3057       0.52109998   0.4436    ]
[37m[1m [-20.92051698   0.98640007   0.0051       0.9830001    0.93729991]
[37m[1m ...
[37m[1m [161.7661866    0.94039994   0.0517       0.9249       0.80159998]
[37m[1m [143.70490475   0.95200008   0.16110002   0.7931       0.90250009]
[37m[1m [133.40083726   0.9393       0.22839999   0.80909997   0.75579995]]
[37m[1m[2023-06-25 09:09:23,909][129146] Max Reward on eval: 1044.3613114674692
[37m[1m[2023-06-25 09:09:23,909][129146] Min Reward on eval: -146.14369953370186
[37m[1m[2023-06-25 09:09:23,910][129146] Mean Reward across all agents: 150.09773179123633
[37m[1m[2023-06-25 09:09:23,910][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:09:23,915][129146] mean_value=-41.049065273381615, max_value=945.4254714508647
[37m[1m[2023-06-25 09:09:23,917][129146] New mean coefficients: [[ 0.7655026  0.9574057  3.8784788 -1.7491543  1.1692204]]
[37m[1m[2023-06-25 09:09:23,918][129146] Moving the mean solution point...
[36m[2023-06-25 09:09:33,551][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 09:09:33,551][129146] FPS: 398713.03
[36m[2023-06-25 09:09:33,553][129146] itr=887, itrs=2000, Progress: 44.35%
[36m[2023-06-25 09:09:45,066][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 09:09:45,066][129146] FPS: 334296.86
[36m[2023-06-25 09:09:49,780][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:09:49,785][129146] Reward + Measures: [[-63.26751692   0.97837502   0.02565633   0.96742868   0.93054295]]
[37m[1m[2023-06-25 09:09:49,786][129146] Max Reward on eval: -63.2675169200805
[37m[1m[2023-06-25 09:09:49,786][129146] Min Reward on eval: -63.2675169200805
[37m[1m[2023-06-25 09:09:49,786][129146] Mean Reward across all agents: -63.2675169200805
[37m[1m[2023-06-25 09:09:49,786][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:09:55,270][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:09:55,270][129146] Reward + Measures: [[365.75194761   0.4921       0.50470001   0.26480004   0.41240001]
[37m[1m [720.86616082   0.74020004   0.3206       0.49559999   0.51450002]
[37m[1m [297.20027132   0.54190004   0.38030002   0.40340003   0.4127    ]
[37m[1m ...
[37m[1m [315.0977097    0.86420006   0.26060003   0.634        0.7626    ]
[37m[1m [209.37700475   0.74100006   0.24769998   0.64200002   0.69010001]
[37m[1m [485.73997652   0.75620002   0.4039       0.46040002   0.67480004]]
[37m[1m[2023-06-25 09:09:55,270][129146] Max Reward on eval: 957.9935198789696
[37m[1m[2023-06-25 09:09:55,271][129146] Min Reward on eval: -390.92335252494087
[37m[1m[2023-06-25 09:09:55,271][129146] Mean Reward across all agents: 318.2072908174968
[37m[1m[2023-06-25 09:09:55,271][129146] Average Trajectory Length: 999.5583333333333
[36m[2023-06-25 09:09:55,278][129146] mean_value=96.52571685962543, max_value=1099.5227683156536
[37m[1m[2023-06-25 09:09:55,281][129146] New mean coefficients: [[ 0.68424624  0.7817861   4.086567   -1.7393178   1.4829528 ]]
[37m[1m[2023-06-25 09:09:55,282][129146] Moving the mean solution point...
[36m[2023-06-25 09:10:04,936][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 09:10:04,936][129146] FPS: 397827.03
[36m[2023-06-25 09:10:04,938][129146] itr=888, itrs=2000, Progress: 44.40%
[36m[2023-06-25 09:10:16,445][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 09:10:16,445][129146] FPS: 334371.12
[36m[2023-06-25 09:10:21,186][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:10:21,186][129146] Reward + Measures: [[992.90044913   0.74906564   0.57378298   0.17432466   0.49798104]]
[37m[1m[2023-06-25 09:10:21,187][129146] Max Reward on eval: 992.9004491250444
[37m[1m[2023-06-25 09:10:21,187][129146] Min Reward on eval: 992.9004491250444
[37m[1m[2023-06-25 09:10:21,187][129146] Mean Reward across all agents: 992.9004491250444
[37m[1m[2023-06-25 09:10:21,187][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:10:26,627][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:10:26,633][129146] Reward + Measures: [[1189.80613786    0.62940001    0.53179997    0.2023        0.26989999]
[37m[1m [1252.27351888    0.64050001    0.53400004    0.18780001    0.26220003]
[37m[1m [1034.27471163    0.64589995    0.51179999    0.22139998    0.25920004]
[37m[1m ...
[37m[1m [1187.05623916    0.6505        0.52800006    0.1736        0.27989998]
[37m[1m [ 956.64298173    0.74910003    0.65250003    0.1344        0.54689997]
[37m[1m [1256.99553917    0.64140004    0.51930004    0.17770001    0.28260002]]
[37m[1m[2023-06-25 09:10:26,633][129146] Max Reward on eval: 1303.0384565831744
[37m[1m[2023-06-25 09:10:26,634][129146] Min Reward on eval: 575.1002521601156
[37m[1m[2023-06-25 09:10:26,634][129146] Mean Reward across all agents: 1091.588164031651
[37m[1m[2023-06-25 09:10:26,634][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:10:26,637][129146] mean_value=-89.30953836753531, max_value=1336.896932200847
[37m[1m[2023-06-25 09:10:26,640][129146] New mean coefficients: [[ 1.0628392  0.9570279  3.1867862 -1.0491421  1.200431 ]]
[37m[1m[2023-06-25 09:10:26,641][129146] Moving the mean solution point...
[36m[2023-06-25 09:10:36,309][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 09:10:36,309][129146] FPS: 397236.67
[36m[2023-06-25 09:10:36,312][129146] itr=889, itrs=2000, Progress: 44.45%
[36m[2023-06-25 09:10:47,817][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 09:10:47,817][129146] FPS: 334451.58
[36m[2023-06-25 09:10:52,609][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:10:52,609][129146] Reward + Measures: [[918.55403813   0.81678164   0.72150099   0.09365834   0.65520269]]
[37m[1m[2023-06-25 09:10:52,610][129146] Max Reward on eval: 918.554038130419
[37m[1m[2023-06-25 09:10:52,610][129146] Min Reward on eval: 918.554038130419
[37m[1m[2023-06-25 09:10:52,610][129146] Mean Reward across all agents: 918.554038130419
[37m[1m[2023-06-25 09:10:52,610][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:10:58,035][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:10:58,035][129146] Reward + Measures: [[965.01643087   0.8186       0.74230003   0.07920001   0.66210002]
[37m[1m [951.99739928   0.6742       0.56439996   0.1366       0.34870002]
[37m[1m [967.30778358   0.77130002   0.66970003   0.10219999   0.54970002]
[37m[1m ...
[37m[1m [856.6401273    0.63         0.57260007   0.13510001   0.3558    ]
[37m[1m [756.62394011   0.80010003   0.72260004   0.07810001   0.65189999]
[37m[1m [944.29851149   0.80510008   0.74119997   0.074        0.65640002]]
[37m[1m[2023-06-25 09:10:58,035][129146] Max Reward on eval: 1138.1045411993284
[37m[1m[2023-06-25 09:10:58,036][129146] Min Reward on eval: 461.16617058982956
[37m[1m[2023-06-25 09:10:58,036][129146] Mean Reward across all agents: 824.7710404189988
[37m[1m[2023-06-25 09:10:58,036][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:10:58,039][129146] mean_value=-151.86422865257177, max_value=1359.0124328066604
[37m[1m[2023-06-25 09:10:58,042][129146] New mean coefficients: [[ 1.1487117   0.9175147   2.5033114  -0.84372365  0.8832437 ]]
[37m[1m[2023-06-25 09:10:58,042][129146] Moving the mean solution point...
[36m[2023-06-25 09:11:07,781][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 09:11:07,781][129146] FPS: 394379.98
[36m[2023-06-25 09:11:07,783][129146] itr=890, itrs=2000, Progress: 44.50%
[37m[1m[2023-06-25 09:11:14,624][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000870
[36m[2023-06-25 09:11:26,496][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 09:11:26,496][129146] FPS: 330074.98
[36m[2023-06-25 09:11:31,290][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:11:31,290][129146] Reward + Measures: [[868.63322498   0.87684065   0.81883967   0.058531     0.77138036]]
[37m[1m[2023-06-25 09:11:31,291][129146] Max Reward on eval: 868.6332249821937
[37m[1m[2023-06-25 09:11:31,291][129146] Min Reward on eval: 868.6332249821937
[37m[1m[2023-06-25 09:11:31,291][129146] Mean Reward across all agents: 868.6332249821937
[37m[1m[2023-06-25 09:11:31,291][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:11:36,704][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:11:36,764][129146] Reward + Measures: [[802.74283941   0.89860004   0.86870003   0.0362       0.83339995]
[37m[1m [809.66262528   0.90700001   0.87320006   0.0394       0.83750004]
[37m[1m [757.10985731   0.87770003   0.84619999   0.0527       0.78689998]
[37m[1m ...
[37m[1m [853.36731863   0.85710001   0.81949997   0.0421       0.7802    ]
[37m[1m [758.66824511   0.88599998   0.79460001   0.0923       0.8294    ]
[37m[1m [840.77412935   0.87629998   0.82490009   0.0471       0.76929998]]
[37m[1m[2023-06-25 09:11:36,764][129146] Max Reward on eval: 1176.6371714147506
[37m[1m[2023-06-25 09:11:36,764][129146] Min Reward on eval: 539.9518417543034
[37m[1m[2023-06-25 09:11:36,765][129146] Mean Reward across all agents: 823.976218174694
[37m[1m[2023-06-25 09:11:36,765][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:11:36,768][129146] mean_value=16.275005119988432, max_value=1462.9101608687547
[37m[1m[2023-06-25 09:11:36,770][129146] New mean coefficients: [[ 1.3173575   0.81293726  2.5782952  -0.5398077   0.66661537]]
[37m[1m[2023-06-25 09:11:36,771][129146] Moving the mean solution point...
[36m[2023-06-25 09:11:46,515][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 09:11:46,515][129146] FPS: 394154.85
[36m[2023-06-25 09:11:46,518][129146] itr=891, itrs=2000, Progress: 44.55%
[36m[2023-06-25 09:11:57,939][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 09:11:57,939][129146] FPS: 336955.05
[36m[2023-06-25 09:12:02,725][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:12:02,725][129146] Reward + Measures: [[844.92543924   0.90670264   0.86776465   0.04197      0.82428205]]
[37m[1m[2023-06-25 09:12:02,725][129146] Max Reward on eval: 844.925439237213
[37m[1m[2023-06-25 09:12:02,725][129146] Min Reward on eval: 844.925439237213
[37m[1m[2023-06-25 09:12:02,726][129146] Mean Reward across all agents: 844.925439237213
[37m[1m[2023-06-25 09:12:02,726][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:12:08,098][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:12:08,099][129146] Reward + Measures: [[648.32133469   0.9217       0.90319997   0.0139       0.91759998]
[37m[1m [769.23489305   0.93170005   0.91400003   0.0176       0.90799999]
[37m[1m [715.04035713   0.85150003   0.82270002   0.0621       0.7439    ]
[37m[1m ...
[37m[1m [562.0945957    0.83479995   0.84130001   0.0228       0.88110012]
[37m[1m [487.77501245   0.82959998   0.84200001   0.032        0.82740003]
[37m[1m [687.96838137   0.80370009   0.78249997   0.0865       0.66420001]]
[37m[1m[2023-06-25 09:12:08,099][129146] Max Reward on eval: 977.584309624054
[37m[1m[2023-06-25 09:12:08,099][129146] Min Reward on eval: -314.47215182960497
[37m[1m[2023-06-25 09:12:08,099][129146] Mean Reward across all agents: 647.3270866692486
[37m[1m[2023-06-25 09:12:08,100][129146] Average Trajectory Length: 999.8783333333333
[36m[2023-06-25 09:12:08,104][129146] mean_value=-78.71486018829945, max_value=1081.7083262541564
[37m[1m[2023-06-25 09:12:08,106][129146] New mean coefficients: [[ 1.2972958   0.99280447  2.4219172  -0.7287197   0.32404676]]
[37m[1m[2023-06-25 09:12:08,107][129146] Moving the mean solution point...
[36m[2023-06-25 09:12:17,830][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 09:12:17,830][129146] FPS: 395032.15
[36m[2023-06-25 09:12:17,832][129146] itr=892, itrs=2000, Progress: 44.60%
[36m[2023-06-25 09:12:29,329][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 09:12:29,329][129146] FPS: 334688.99
[36m[2023-06-25 09:12:34,168][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:12:34,169][129146] Reward + Measures: [[841.31097725   0.92233127   0.89201528   0.03416067   0.85379034]]
[37m[1m[2023-06-25 09:12:34,169][129146] Max Reward on eval: 841.3109772464101
[37m[1m[2023-06-25 09:12:34,169][129146] Min Reward on eval: 841.3109772464101
[37m[1m[2023-06-25 09:12:34,169][129146] Mean Reward across all agents: 841.3109772464101
[37m[1m[2023-06-25 09:12:34,169][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:12:39,673][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:12:39,674][129146] Reward + Measures: [[1030.50469897    0.56260002    0.62520003    0.1015        0.56099999]
[37m[1m [ 714.27031309    0.77160001    0.65979999    0.1982        0.66610003]
[37m[1m [ 875.16832164    0.89650005    0.87179995    0.029         0.84079999]
[37m[1m ...
[37m[1m [ 984.06130371    0.71169996    0.66119999    0.0771        0.46350002]
[37m[1m [ 992.32198711    0.64440006    0.62819999    0.0514        0.51620001]
[37m[1m [ 904.46166163    0.67650002    0.71490002    0.0491        0.62910002]]
[37m[1m[2023-06-25 09:12:39,674][129146] Max Reward on eval: 1344.8431634392823
[37m[1m[2023-06-25 09:12:39,674][129146] Min Reward on eval: 477.8352773458115
[37m[1m[2023-06-25 09:12:39,675][129146] Mean Reward across all agents: 946.160164056255
[37m[1m[2023-06-25 09:12:39,675][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:12:39,679][129146] mean_value=133.30149239266729, max_value=1464.170257856639
[37m[1m[2023-06-25 09:12:39,682][129146] New mean coefficients: [[ 0.9169954   1.9052033   2.6029088  -0.7749191   0.15691544]]
[37m[1m[2023-06-25 09:12:39,683][129146] Moving the mean solution point...
[36m[2023-06-25 09:12:49,440][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 09:12:49,440][129146] FPS: 393638.36
[36m[2023-06-25 09:12:49,443][129146] itr=893, itrs=2000, Progress: 44.65%
[36m[2023-06-25 09:13:00,970][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 09:13:00,971][129146] FPS: 333746.02
[36m[2023-06-25 09:13:05,774][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:13:05,774][129146] Reward + Measures: [[848.12708166   0.92411065   0.89671701   0.033804     0.85577768]]
[37m[1m[2023-06-25 09:13:05,775][129146] Max Reward on eval: 848.1270816579871
[37m[1m[2023-06-25 09:13:05,775][129146] Min Reward on eval: 848.1270816579871
[37m[1m[2023-06-25 09:13:05,775][129146] Mean Reward across all agents: 848.1270816579871
[37m[1m[2023-06-25 09:13:05,776][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:13:11,176][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:13:11,177][129146] Reward + Measures: [[710.54058877   0.87239999   0.86729997   0.0208       0.81879997]
[37m[1m [943.29596433   0.75229996   0.77060002   0.0578       0.63199997]
[37m[1m [770.3167838    0.92620003   0.90049994   0.028        0.87239999]
[37m[1m ...
[37m[1m [711.86174823   0.9011001    0.88429993   0.0227       0.87029994]
[37m[1m [715.06443437   0.90220004   0.88440001   0.0259       0.87830001]
[37m[1m [778.8878668    0.92679995   0.90920001   0.0214       0.88550007]]
[37m[1m[2023-06-25 09:13:11,177][129146] Max Reward on eval: 1210.199374209356
[37m[1m[2023-06-25 09:13:11,177][129146] Min Reward on eval: 639.181631060713
[37m[1m[2023-06-25 09:13:11,177][129146] Mean Reward across all agents: 786.9719756762154
[37m[1m[2023-06-25 09:13:11,178][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:13:11,181][129146] mean_value=42.657267327024755, max_value=1205.9066443800778
[37m[1m[2023-06-25 09:13:11,184][129146] New mean coefficients: [[ 0.9145393   0.91017514  2.4016092  -1.0016211   0.74642575]]
[37m[1m[2023-06-25 09:13:11,185][129146] Moving the mean solution point...
[36m[2023-06-25 09:13:20,892][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 09:13:20,892][129146] FPS: 395653.84
[36m[2023-06-25 09:13:20,894][129146] itr=894, itrs=2000, Progress: 44.70%
[36m[2023-06-25 09:13:32,496][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 09:13:32,496][129146] FPS: 331618.65
[36m[2023-06-25 09:13:37,291][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:13:37,292][129146] Reward + Measures: [[841.05863697   0.93553531   0.91001725   0.03031467   0.87531036]]
[37m[1m[2023-06-25 09:13:37,292][129146] Max Reward on eval: 841.0586369706917
[37m[1m[2023-06-25 09:13:37,292][129146] Min Reward on eval: 841.0586369706917
[37m[1m[2023-06-25 09:13:37,293][129146] Mean Reward across all agents: 841.0586369706917
[37m[1m[2023-06-25 09:13:37,293][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:13:42,915][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:13:42,920][129146] Reward + Measures: [[ 611.33656836    0.47640005    0.65689999    0.17500001    0.58060002]
[37m[1m [-116.76940978    0.414         0.40699998    0.4165        0.33490002]
[37m[1m [ 837.95164007    0.75430006    0.83530009    0.0424        0.7773    ]
[37m[1m ...
[37m[1m [ 781.36198041    0.93759996    0.91280001    0.0248        0.88940001]
[37m[1m [ 870.29483177    0.89589995    0.87159997    0.0449        0.78960001]
[37m[1m [ 791.4699893     0.52820003    0.61120003    0.11719999    0.51119995]]
[37m[1m[2023-06-25 09:13:42,921][129146] Max Reward on eval: 1119.20572046726
[37m[1m[2023-06-25 09:13:42,921][129146] Min Reward on eval: -116.76940978372586
[37m[1m[2023-06-25 09:13:42,921][129146] Mean Reward across all agents: 635.2974042045329
[37m[1m[2023-06-25 09:13:42,922][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:13:42,926][129146] mean_value=-148.70432217137096, max_value=1081.441282105732
[37m[1m[2023-06-25 09:13:42,929][129146] New mean coefficients: [[ 0.75355065  1.5749834   2.3314497  -0.77933276  1.1335859 ]]
[37m[1m[2023-06-25 09:13:42,930][129146] Moving the mean solution point...
[36m[2023-06-25 09:13:52,755][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 09:13:52,756][129146] FPS: 390888.85
[36m[2023-06-25 09:13:52,758][129146] itr=895, itrs=2000, Progress: 44.75%
[36m[2023-06-25 09:14:04,261][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 09:14:04,261][129146] FPS: 334584.11
[36m[2023-06-25 09:14:09,062][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:14:09,062][129146] Reward + Measures: [[836.33885152   0.94442064   0.92469841   0.024256     0.89415866]]
[37m[1m[2023-06-25 09:14:09,062][129146] Max Reward on eval: 836.3388515223445
[37m[1m[2023-06-25 09:14:09,063][129146] Min Reward on eval: 836.3388515223445
[37m[1m[2023-06-25 09:14:09,063][129146] Mean Reward across all agents: 836.3388515223445
[37m[1m[2023-06-25 09:14:09,063][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:14:14,525][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:14:14,526][129146] Reward + Measures: [[726.31312377   0.92740005   0.91680002   0.0258       0.88240004]
[37m[1m [735.10333801   0.9436       0.9386       0.0169       0.90350008]
[37m[1m [808.00709959   0.88850003   0.88379997   0.035        0.82779998]
[37m[1m ...
[37m[1m [723.88078185   0.92110008   0.9005       0.0263       0.84720004]
[37m[1m [764.76250359   0.95360005   0.94069999   0.0179       0.91320002]
[37m[1m [633.81864342   0.92989999   0.9278       0.0197       0.89319992]]
[37m[1m[2023-06-25 09:14:14,526][129146] Max Reward on eval: 1011.4762595222797
[37m[1m[2023-06-25 09:14:14,526][129146] Min Reward on eval: 522.931845193659
[37m[1m[2023-06-25 09:14:14,526][129146] Mean Reward across all agents: 778.3675884644317
[37m[1m[2023-06-25 09:14:14,527][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:14:14,529][129146] mean_value=-47.80617294854263, max_value=305.11705769754803
[37m[1m[2023-06-25 09:14:14,532][129146] New mean coefficients: [[ 0.8686262  1.7029123  1.3034338 -1.5518897  1.3460023]]
[37m[1m[2023-06-25 09:14:14,533][129146] Moving the mean solution point...
[36m[2023-06-25 09:14:24,222][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 09:14:24,222][129146] FPS: 396387.48
[36m[2023-06-25 09:14:24,225][129146] itr=896, itrs=2000, Progress: 44.80%
[36m[2023-06-25 09:14:35,928][129146] train() took 11.68 seconds to complete
[36m[2023-06-25 09:14:35,928][129146] FPS: 328849.27
[36m[2023-06-25 09:14:40,746][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:14:40,747][129146] Reward + Measures: [[834.49017652   0.95032167   0.93033838   0.02474067   0.90594   ]]
[37m[1m[2023-06-25 09:14:40,747][129146] Max Reward on eval: 834.4901765226908
[37m[1m[2023-06-25 09:14:40,747][129146] Min Reward on eval: 834.4901765226908
[37m[1m[2023-06-25 09:14:40,747][129146] Mean Reward across all agents: 834.4901765226908
[37m[1m[2023-06-25 09:14:40,747][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:14:46,286][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:14:46,286][129146] Reward + Measures: [[1060.339         0.66140002    0.60840005    0.15019999    0.34639999]
[37m[1m [ 830.6922049     0.8915        0.86779994    0.0312        0.78479999]
[37m[1m [ 360.8295258     0.50959998    0.48719999    0.20290001    0.36569998]
[37m[1m ...
[37m[1m [ 628.74294038    0.62269998    0.55250001    0.1908        0.34390002]
[37m[1m [ 820.42736387    0.94849998    0.93709993    0.0188        0.90380001]
[37m[1m [  59.92447873    0.42419997    0.32500002    0.22000001    0.32820001]]
[37m[1m[2023-06-25 09:14:46,286][129146] Max Reward on eval: 1269.5436080095126
[37m[1m[2023-06-25 09:14:46,287][129146] Min Reward on eval: -151.26693179992725
[37m[1m[2023-06-25 09:14:46,287][129146] Mean Reward across all agents: 645.5052842118539
[37m[1m[2023-06-25 09:14:46,287][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:14:46,290][129146] mean_value=-643.5357591307063, max_value=951.2599434338103
[37m[1m[2023-06-25 09:14:46,293][129146] New mean coefficients: [[ 0.7744565   1.5459368   1.3291178  -0.38418424  1.6181387 ]]
[37m[1m[2023-06-25 09:14:46,294][129146] Moving the mean solution point...
[36m[2023-06-25 09:14:56,141][129146] train() took 9.85 seconds to complete
[36m[2023-06-25 09:14:56,141][129146] FPS: 390037.70
[36m[2023-06-25 09:14:56,143][129146] itr=897, itrs=2000, Progress: 44.85%
[36m[2023-06-25 09:15:07,776][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 09:15:07,776][129146] FPS: 330846.82
[36m[2023-06-25 09:15:12,550][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:15:12,551][129146] Reward + Measures: [[827.63154788   0.95265698   0.93586272   0.021682     0.91233033]]
[37m[1m[2023-06-25 09:15:12,551][129146] Max Reward on eval: 827.631547880509
[37m[1m[2023-06-25 09:15:12,551][129146] Min Reward on eval: 827.631547880509
[37m[1m[2023-06-25 09:15:12,551][129146] Mean Reward across all agents: 827.631547880509
[37m[1m[2023-06-25 09:15:12,551][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:15:18,146][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:15:18,147][129146] Reward + Measures: [[736.69556098   0.94660008   0.92750007   0.023        0.89959997]
[37m[1m [830.93588294   0.93430007   0.91820002   0.0277       0.87199992]
[37m[1m [700.14711101   0.88659996   0.86729997   0.021        0.88360006]
[37m[1m ...
[37m[1m [797.71902695   0.96470004   0.94680005   0.0156       0.92729998]
[37m[1m [775.4072761    0.93150008   0.91990006   0.0193       0.9052    ]
[37m[1m [656.29555652   0.94960004   0.75050002   0.20790003   0.90990001]]
[37m[1m[2023-06-25 09:15:18,147][129146] Max Reward on eval: 883.5858317889273
[37m[1m[2023-06-25 09:15:18,147][129146] Min Reward on eval: -52.547807866707444
[37m[1m[2023-06-25 09:15:18,147][129146] Mean Reward across all agents: 747.0982442601012
[37m[1m[2023-06-25 09:15:18,148][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:15:18,151][129146] mean_value=47.37645436836988, max_value=1077.502623091545
[37m[1m[2023-06-25 09:15:18,154][129146] New mean coefficients: [[0.5881795  2.3141317  0.23400807 0.7544249  1.9172406 ]]
[37m[1m[2023-06-25 09:15:18,155][129146] Moving the mean solution point...
[36m[2023-06-25 09:15:28,058][129146] train() took 9.90 seconds to complete
[36m[2023-06-25 09:15:28,058][129146] FPS: 387805.34
[36m[2023-06-25 09:15:28,061][129146] itr=898, itrs=2000, Progress: 44.90%
[36m[2023-06-25 09:15:39,756][129146] train() took 11.67 seconds to complete
[36m[2023-06-25 09:15:39,756][129146] FPS: 328978.93
[36m[2023-06-25 09:15:44,585][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:15:44,586][129146] Reward + Measures: [[821.45652964   0.95397967   0.93561703   0.02298466   0.9143486 ]]
[37m[1m[2023-06-25 09:15:44,586][129146] Max Reward on eval: 821.4565296403347
[37m[1m[2023-06-25 09:15:44,586][129146] Min Reward on eval: 821.4565296403347
[37m[1m[2023-06-25 09:15:44,586][129146] Mean Reward across all agents: 821.4565296403347
[37m[1m[2023-06-25 09:15:44,587][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:15:49,901][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:15:49,907][129146] Reward + Measures: [[733.66096526   0.89230007   0.8962       0.0272       0.88319999]
[37m[1m [514.20223713   0.70279998   0.5851       0.19949999   0.54220003]
[37m[1m [669.75597991   0.85659999   0.88379997   0.0209       0.86310005]
[37m[1m ...
[37m[1m [722.60236898   0.86789989   0.86149997   0.0402       0.79980004]
[37m[1m [386.76849083   0.80050004   0.68689996   0.15349999   0.85769999]
[37m[1m [387.84089667   0.70209998   0.54339999   0.24510001   0.51840001]]
[37m[1m[2023-06-25 09:15:49,907][129146] Max Reward on eval: 1103.3789239386563
[37m[1m[2023-06-25 09:15:49,907][129146] Min Reward on eval: -262.9205219828262
[37m[1m[2023-06-25 09:15:49,908][129146] Mean Reward across all agents: 580.1492353267213
[37m[1m[2023-06-25 09:15:49,908][129146] Average Trajectory Length: 998.7096666666666
[36m[2023-06-25 09:15:49,912][129146] mean_value=-380.53626514438685, max_value=1212.6394078853773
[37m[1m[2023-06-25 09:15:49,915][129146] New mean coefficients: [[ 0.6067152   1.9561403  -0.35834563 -0.6398946   2.0870109 ]]
[37m[1m[2023-06-25 09:15:49,916][129146] Moving the mean solution point...
[36m[2023-06-25 09:15:59,621][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 09:15:59,621][129146] FPS: 395719.83
[36m[2023-06-25 09:15:59,624][129146] itr=899, itrs=2000, Progress: 44.95%
[36m[2023-06-25 09:16:11,134][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 09:16:11,134][129146] FPS: 334354.04
[36m[2023-06-25 09:16:16,025][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:16:16,025][129146] Reward + Measures: [[823.21633037   0.95741063   0.93429428   0.026632     0.91839898]]
[37m[1m[2023-06-25 09:16:16,025][129146] Max Reward on eval: 823.2163303702042
[37m[1m[2023-06-25 09:16:16,026][129146] Min Reward on eval: 823.2163303702042
[37m[1m[2023-06-25 09:16:16,026][129146] Mean Reward across all agents: 823.2163303702042
[37m[1m[2023-06-25 09:16:16,026][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:16:21,650][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:16:21,655][129146] Reward + Measures: [[ 577.55367097    0.90630007    0.63999999    0.28150001    0.81840003]
[37m[1m [ 835.96386634    0.85210001    0.85229999    0.0298        0.85540003]
[37m[1m [   5.03616805    0.92819995    0.1874        0.83610004    0.75229996]
[37m[1m ...
[37m[1m [ 771.51000004    0.94540006    0.92449999    0.0246        0.89589995]
[37m[1m [-326.89182494    0.95720005    0.0392        0.97279996    0.81500006]
[37m[1m [-106.22106826    0.94060004    0.0834        0.87880003    0.83960003]]
[37m[1m[2023-06-25 09:16:21,656][129146] Max Reward on eval: 868.8291792894481
[37m[1m[2023-06-25 09:16:21,656][129146] Min Reward on eval: -326.8918249398936
[37m[1m[2023-06-25 09:16:21,656][129146] Mean Reward across all agents: 507.02862134915546
[37m[1m[2023-06-25 09:16:21,656][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:16:21,662][129146] mean_value=95.27204212471369, max_value=1173.0035541791817
[37m[1m[2023-06-25 09:16:21,665][129146] New mean coefficients: [[ 0.79897857  1.5983509  -0.4139949  -1.1732739   2.1860769 ]]
[37m[1m[2023-06-25 09:16:21,666][129146] Moving the mean solution point...
[36m[2023-06-25 09:16:31,441][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 09:16:31,441][129146] FPS: 392894.65
[36m[2023-06-25 09:16:31,443][129146] itr=900, itrs=2000, Progress: 45.00%
[37m[1m[2023-06-25 09:16:38,052][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000880
[36m[2023-06-25 09:16:49,824][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 09:16:49,824][129146] FPS: 332718.93
[36m[2023-06-25 09:16:54,698][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:16:54,698][129146] Reward + Measures: [[835.94925327   0.95607871   0.93080896   0.02866667   0.91707534]]
[37m[1m[2023-06-25 09:16:54,699][129146] Max Reward on eval: 835.949253273562
[37m[1m[2023-06-25 09:16:54,699][129146] Min Reward on eval: 835.949253273562
[37m[1m[2023-06-25 09:16:54,699][129146] Mean Reward across all agents: 835.949253273562
[37m[1m[2023-06-25 09:16:54,699][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:17:00,229][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:17:00,230][129146] Reward + Measures: [[631.08734586   0.74540007   0.62690002   0.16360001   0.6469    ]
[37m[1m [781.4322206    0.95200008   0.94229996   0.024        0.92529994]
[37m[1m [674.34628096   0.93349999   0.8229       0.12819999   0.87630004]
[37m[1m ...
[37m[1m [729.69037144   0.92830002   0.81770003   0.12620001   0.87309998]
[37m[1m [494.72959721   0.57800001   0.31650001   0.30820003   0.28239998]
[37m[1m [693.90135479   0.92270005   0.8854       0.0457       0.87550002]]
[37m[1m[2023-06-25 09:17:00,230][129146] Max Reward on eval: 995.4654285223921
[37m[1m[2023-06-25 09:17:00,230][129146] Min Reward on eval: -358.3938816972426
[37m[1m[2023-06-25 09:17:00,231][129146] Mean Reward across all agents: 590.7888307544628
[37m[1m[2023-06-25 09:17:00,231][129146] Average Trajectory Length: 998.1836666666667
[36m[2023-06-25 09:17:00,235][129146] mean_value=-291.4048233238224, max_value=1211.0720987835666
[37m[1m[2023-06-25 09:17:00,238][129146] New mean coefficients: [[ 0.6519577  1.766494  -0.812129  -0.7039568  2.0212016]]
[37m[1m[2023-06-25 09:17:00,239][129146] Moving the mean solution point...
[36m[2023-06-25 09:17:10,039][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 09:17:10,040][129146] FPS: 391883.35
[36m[2023-06-25 09:17:10,042][129146] itr=901, itrs=2000, Progress: 45.05%
[36m[2023-06-25 09:17:21,516][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 09:17:21,516][129146] FPS: 335314.76
[36m[2023-06-25 09:17:26,342][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:17:26,343][129146] Reward + Measures: [[848.88019847   0.95685333   0.93135035   0.02829167   0.92133433]]
[37m[1m[2023-06-25 09:17:26,343][129146] Max Reward on eval: 848.8801984660548
[37m[1m[2023-06-25 09:17:26,343][129146] Min Reward on eval: 848.8801984660548
[37m[1m[2023-06-25 09:17:26,343][129146] Mean Reward across all agents: 848.8801984660548
[37m[1m[2023-06-25 09:17:26,343][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:17:31,807][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:17:31,807][129146] Reward + Measures: [[ 483.50331886    0.86070007    0.56459999    0.30930001    0.88259995]
[37m[1m [ 736.04069865    0.86899996    0.74669999    0.12390001    0.86689997]
[37m[1m [ 669.91799667    0.94530004    0.94410002    0.0164        0.91310006]
[37m[1m ...
[37m[1m [1256.36858813    0.28260002    0.45539999    0.1464        0.44040003]
[37m[1m [ 496.34206617    0.89490002    0.79210007    0.12789999    0.84359998]
[37m[1m [ 167.72492382    0.79949999    0.36699998    0.45830002    0.70020002]]
[37m[1m[2023-06-25 09:17:31,807][129146] Max Reward on eval: 1409.5115115931026
[37m[1m[2023-06-25 09:17:31,808][129146] Min Reward on eval: -510.3893994108192
[37m[1m[2023-06-25 09:17:31,808][129146] Mean Reward across all agents: 645.8652542420458
[37m[1m[2023-06-25 09:17:31,808][129146] Average Trajectory Length: 998.7529999999999
[36m[2023-06-25 09:17:31,814][129146] mean_value=-243.07971944182975, max_value=1120.133957758257
[37m[1m[2023-06-25 09:17:31,816][129146] New mean coefficients: [[ 0.8667286   1.6486951  -0.6913935  -0.79329485  2.540183  ]]
[37m[1m[2023-06-25 09:17:31,817][129146] Moving the mean solution point...
[36m[2023-06-25 09:17:41,421][129146] train() took 9.60 seconds to complete
[36m[2023-06-25 09:17:41,421][129146] FPS: 399925.96
[36m[2023-06-25 09:17:41,423][129146] itr=902, itrs=2000, Progress: 45.10%
[36m[2023-06-25 09:17:52,849][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 09:17:52,849][129146] FPS: 336832.10
[36m[2023-06-25 09:17:57,530][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:17:57,531][129146] Reward + Measures: [[842.95064715   0.95615965   0.92671502   0.03210166   0.91818571]]
[37m[1m[2023-06-25 09:17:57,531][129146] Max Reward on eval: 842.9506471546789
[37m[1m[2023-06-25 09:17:57,531][129146] Min Reward on eval: 842.9506471546789
[37m[1m[2023-06-25 09:17:57,531][129146] Mean Reward across all agents: 842.9506471546789
[37m[1m[2023-06-25 09:17:57,532][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:18:02,970][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:18:02,976][129146] Reward + Measures: [[851.62108691   0.95459998   0.9375       0.0207       0.91679996]
[37m[1m [732.15506604   0.94919997   0.84100002   0.1139       0.91919994]
[37m[1m [640.67750339   0.95200008   0.7446       0.20970002   0.9169001 ]
[37m[1m ...
[37m[1m [817.88988776   0.96530002   0.94929999   0.0154       0.93349999]
[37m[1m [795.01244968   0.92559999   0.90959996   0.0265       0.8969    ]
[37m[1m [709.96898401   0.95020002   0.84390002   0.1135       0.92150003]]
[37m[1m[2023-06-25 09:18:02,976][129146] Max Reward on eval: 949.4383409703617
[37m[1m[2023-06-25 09:18:02,976][129146] Min Reward on eval: 222.29300385923125
[37m[1m[2023-06-25 09:18:02,976][129146] Mean Reward across all agents: 772.0404943142496
[37m[1m[2023-06-25 09:18:02,977][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:18:02,980][129146] mean_value=14.33298909945202, max_value=1157.3049938056618
[37m[1m[2023-06-25 09:18:02,982][129146] New mean coefficients: [[ 0.79744977  1.5347049  -0.6028387  -0.34055987  3.170146  ]]
[37m[1m[2023-06-25 09:18:02,983][129146] Moving the mean solution point...
[36m[2023-06-25 09:18:12,556][129146] train() took 9.57 seconds to complete
[36m[2023-06-25 09:18:12,556][129146] FPS: 401206.09
[36m[2023-06-25 09:18:12,558][129146] itr=903, itrs=2000, Progress: 45.15%
[36m[2023-06-25 09:18:24,012][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 09:18:24,012][129146] FPS: 335922.99
[36m[2023-06-25 09:18:28,646][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:18:28,646][129146] Reward + Measures: [[850.52213286   0.95603102   0.92124432   0.03615833   0.91851002]]
[37m[1m[2023-06-25 09:18:28,647][129146] Max Reward on eval: 850.5221328586514
[37m[1m[2023-06-25 09:18:28,647][129146] Min Reward on eval: 850.5221328586514
[37m[1m[2023-06-25 09:18:28,647][129146] Mean Reward across all agents: 850.5221328586514
[37m[1m[2023-06-25 09:18:28,647][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:18:34,214][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:18:34,214][129146] Reward + Measures: [[763.97258646   0.3876       0.56770003   0.11059999   0.4224    ]
[37m[1m [591.23829464   0.61449999   0.61370003   0.22360002   0.60950005]
[37m[1m [856.38595038   0.9332       0.92369998   0.0274       0.89910001]
[37m[1m ...
[37m[1m [203.87940001   0.40370002   0.54300004   0.1418       0.48839998]
[37m[1m [396.15263379   0.45919999   0.65129995   0.1189       0.56340003]
[37m[1m [392.41041765   0.6796       0.2277       0.67970002   0.57380003]]
[37m[1m[2023-06-25 09:18:34,215][129146] Max Reward on eval: 1145.0054328231374
[37m[1m[2023-06-25 09:18:34,215][129146] Min Reward on eval: -381.9236578341224
[37m[1m[2023-06-25 09:18:34,215][129146] Mean Reward across all agents: 396.8381404487083
[37m[1m[2023-06-25 09:18:34,216][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:18:34,222][129146] mean_value=-376.30160909473034, max_value=1088.3346523022128
[37m[1m[2023-06-25 09:18:34,224][129146] New mean coefficients: [[ 0.2637725   1.5029522  -0.66615015 -0.5062068   2.5365005 ]]
[37m[1m[2023-06-25 09:18:34,225][129146] Moving the mean solution point...
[36m[2023-06-25 09:18:43,836][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 09:18:43,836][129146] FPS: 399618.86
[36m[2023-06-25 09:18:43,839][129146] itr=904, itrs=2000, Progress: 45.20%
[36m[2023-06-25 09:18:55,239][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 09:18:55,239][129146] FPS: 337488.38
[36m[2023-06-25 09:19:00,010][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:19:00,011][129146] Reward + Measures: [[849.43342145   0.95776808   0.92439169   0.034548     0.92256963]]
[37m[1m[2023-06-25 09:19:00,011][129146] Max Reward on eval: 849.4334214484281
[37m[1m[2023-06-25 09:19:00,011][129146] Min Reward on eval: 849.4334214484281
[37m[1m[2023-06-25 09:19:00,011][129146] Mean Reward across all agents: 849.4334214484281
[37m[1m[2023-06-25 09:19:00,011][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:19:05,337][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:19:05,338][129146] Reward + Measures: [[694.76761576   0.91869992   0.89989996   0.0283       0.88980001]
[37m[1m [466.75299545   0.71330005   0.3371       0.44749999   0.40720001]
[37m[1m [431.29387485   0.69710004   0.2726       0.43739995   0.34770003]
[37m[1m ...
[37m[1m [831.29871973   0.96499997   0.95109999   0.0128       0.94280005]
[37m[1m [755.26445619   0.93870002   0.81739998   0.12720001   0.8854    ]
[37m[1m [555.55138388   0.6972       0.35939997   0.46400005   0.45619997]]
[37m[1m[2023-06-25 09:19:05,338][129146] Max Reward on eval: 931.7140775417444
[37m[1m[2023-06-25 09:19:05,338][129146] Min Reward on eval: -38.836309628817254
[37m[1m[2023-06-25 09:19:05,339][129146] Mean Reward across all agents: 669.9929035973072
[37m[1m[2023-06-25 09:19:05,339][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:19:05,343][129146] mean_value=-63.71161098145475, max_value=1138.4709139586473
[37m[1m[2023-06-25 09:19:05,346][129146] New mean coefficients: [[ 0.67416793  1.1868097  -1.1809859  -1.0186117   2.7293384 ]]
[37m[1m[2023-06-25 09:19:05,347][129146] Moving the mean solution point...
[36m[2023-06-25 09:19:14,984][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 09:19:14,984][129146] FPS: 398534.25
[36m[2023-06-25 09:19:14,986][129146] itr=905, itrs=2000, Progress: 45.25%
[36m[2023-06-25 09:19:26,553][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 09:19:26,553][129146] FPS: 332692.78
[36m[2023-06-25 09:19:31,363][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:19:31,363][129146] Reward + Measures: [[856.67948204   0.95861995   0.92017901   0.038532     0.92299169]]
[37m[1m[2023-06-25 09:19:31,364][129146] Max Reward on eval: 856.6794820380106
[37m[1m[2023-06-25 09:19:31,364][129146] Min Reward on eval: 856.6794820380106
[37m[1m[2023-06-25 09:19:31,364][129146] Mean Reward across all agents: 856.6794820380106
[37m[1m[2023-06-25 09:19:31,364][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:19:36,818][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:19:36,824][129146] Reward + Measures: [[586.91903057   0.84530002   0.81049997   0.31260002   0.71040004]
[37m[1m [633.12622254   0.8731001    0.87600005   0.021        0.77420002]
[37m[1m [410.20745912   0.61339998   0.8818       0.0364       0.87360001]
[37m[1m ...
[37m[1m [611.11143493   0.88900006   0.79430002   0.22160001   0.79220003]
[37m[1m [780.48854838   0.93150008   0.91580003   0.0191       0.92679995]
[37m[1m [644.269008     0.83059996   0.87820005   0.0237       0.83350003]]
[37m[1m[2023-06-25 09:19:36,824][129146] Max Reward on eval: 1098.8919812388253
[37m[1m[2023-06-25 09:19:36,824][129146] Min Reward on eval: 135.55858512248378
[37m[1m[2023-06-25 09:19:36,824][129146] Mean Reward across all agents: 612.21109068642
[37m[1m[2023-06-25 09:19:36,825][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:19:36,832][129146] mean_value=152.34879921374585, max_value=1161.038563486561
[37m[1m[2023-06-25 09:19:36,835][129146] New mean coefficients: [[ 0.59114206  2.4830143  -1.3454362  -0.98279035  3.5650933 ]]
[37m[1m[2023-06-25 09:19:36,836][129146] Moving the mean solution point...
[36m[2023-06-25 09:19:46,556][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 09:19:46,556][129146] FPS: 395112.36
[36m[2023-06-25 09:19:46,558][129146] itr=906, itrs=2000, Progress: 45.30%
[36m[2023-06-25 09:19:58,108][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 09:19:58,108][129146] FPS: 333132.48
[36m[2023-06-25 09:20:02,992][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:20:02,992][129146] Reward + Measures: [[853.37748636   0.95813525   0.92661506   0.03216767   0.92381495]]
[37m[1m[2023-06-25 09:20:02,992][129146] Max Reward on eval: 853.3774863605729
[37m[1m[2023-06-25 09:20:02,993][129146] Min Reward on eval: 853.3774863605729
[37m[1m[2023-06-25 09:20:02,993][129146] Mean Reward across all agents: 853.3774863605729
[37m[1m[2023-06-25 09:20:02,993][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:20:08,348][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:20:08,348][129146] Reward + Measures: [[-273.97931066    0.98320001    0.0236        0.97039998    0.94069999]
[37m[1m [ 871.29498603    0.92640001    0.90739995    0.0159        0.91379994]
[37m[1m [ 365.16986087    0.72310001    0.55610001    0.40610003    0.91499996]
[37m[1m ...
[37m[1m [ 573.71559854    0.83249998    0.75730002    0.13240001    0.84819996]
[37m[1m [-648.07925947    0.95190001    0.0099        0.9774        0.78100002]
[37m[1m [ 700.14174716    0.87190002    0.76789999    0.1293        0.81389999]]
[37m[1m[2023-06-25 09:20:08,348][129146] Max Reward on eval: 918.8536033335956
[37m[1m[2023-06-25 09:20:08,349][129146] Min Reward on eval: -648.0792594732018
[37m[1m[2023-06-25 09:20:08,349][129146] Mean Reward across all agents: 172.7349556980489
[37m[1m[2023-06-25 09:20:08,349][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:20:08,354][129146] mean_value=-78.09765530744245, max_value=1118.7719968054514
[37m[1m[2023-06-25 09:20:08,357][129146] New mean coefficients: [[ 0.278057   2.326598  -1.1634344 -0.885853   3.4255507]]
[37m[1m[2023-06-25 09:20:08,358][129146] Moving the mean solution point...
[36m[2023-06-25 09:20:18,208][129146] train() took 9.85 seconds to complete
[36m[2023-06-25 09:20:18,209][129146] FPS: 389887.98
[36m[2023-06-25 09:20:18,211][129146] itr=907, itrs=2000, Progress: 45.35%
[36m[2023-06-25 09:20:29,782][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 09:20:29,782][129146] FPS: 332519.53
[36m[2023-06-25 09:20:34,639][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:20:34,639][129146] Reward + Measures: [[854.50038819   0.95825958   0.91872072   0.03890167   0.92421502]]
[37m[1m[2023-06-25 09:20:34,640][129146] Max Reward on eval: 854.500388193879
[37m[1m[2023-06-25 09:20:34,640][129146] Min Reward on eval: 854.500388193879
[37m[1m[2023-06-25 09:20:34,640][129146] Mean Reward across all agents: 854.500388193879
[37m[1m[2023-06-25 09:20:34,640][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:20:40,023][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:20:40,024][129146] Reward + Measures: [[582.50631339   0.88849992   0.76750004   0.11870001   0.84060001]
[37m[1m [161.73203819   0.75120008   0.68580002   0.16410001   0.78360003]
[37m[1m [841.49594002   0.96250004   0.93909997   0.0211       0.92970002]
[37m[1m ...
[37m[1m [679.99976419   0.89750004   0.69909996   0.3572       0.63449997]
[37m[1m [623.89834375   0.96609992   0.65570009   0.30679998   0.92399997]
[37m[1m [359.11581261   0.88160002   0.70640004   0.19090001   0.86140007]]
[37m[1m[2023-06-25 09:20:40,024][129146] Max Reward on eval: 1110.424354865169
[37m[1m[2023-06-25 09:20:40,024][129146] Min Reward on eval: 2.8589019396691584
[37m[1m[2023-06-25 09:20:40,025][129146] Mean Reward across all agents: 488.23643967717226
[37m[1m[2023-06-25 09:20:40,025][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:20:40,031][129146] mean_value=49.71815267670093, max_value=1087.529867749568
[37m[1m[2023-06-25 09:20:40,034][129146] New mean coefficients: [[ 0.9528223   2.3278267  -1.5545112  -0.51060164  3.3571825 ]]
[37m[1m[2023-06-25 09:20:40,035][129146] Moving the mean solution point...
[36m[2023-06-25 09:20:49,813][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 09:20:49,813][129146] FPS: 392789.23
[36m[2023-06-25 09:20:49,815][129146] itr=908, itrs=2000, Progress: 45.40%
[36m[2023-06-25 09:21:01,271][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 09:21:01,271][129146] FPS: 335965.04
[36m[2023-06-25 09:21:06,187][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:21:06,188][129146] Reward + Measures: [[863.05886343   0.95682895   0.91886866   0.03543133   0.92127573]]
[37m[1m[2023-06-25 09:21:06,188][129146] Max Reward on eval: 863.0588634311491
[37m[1m[2023-06-25 09:21:06,188][129146] Min Reward on eval: 863.0588634311491
[37m[1m[2023-06-25 09:21:06,188][129146] Mean Reward across all agents: 863.0588634311491
[37m[1m[2023-06-25 09:21:06,189][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:21:11,789][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:21:11,790][129146] Reward + Measures: [[865.15330556   0.90900004   0.86900008   0.0407       0.83590001]
[37m[1m [902.87328745   0.8549       0.84240001   0.0445       0.79339999]
[37m[1m [553.60672456   0.87530005   0.69190001   0.23160003   0.86660004]
[37m[1m ...
[37m[1m [314.51566034   0.95539999   0.4614       0.50060004   0.93009996]
[37m[1m [544.92389807   0.63560003   0.69859999   0.125        0.58740002]
[37m[1m [907.13699884   0.75899994   0.6717       0.1538       0.49959999]]
[37m[1m[2023-06-25 09:21:11,790][129146] Max Reward on eval: 1096.6415098165046
[37m[1m[2023-06-25 09:21:11,790][129146] Min Reward on eval: -712.9957491890993
[37m[1m[2023-06-25 09:21:11,791][129146] Mean Reward across all agents: 606.8049718434793
[37m[1m[2023-06-25 09:21:11,791][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:21:11,795][129146] mean_value=-131.3978662251375, max_value=1110.547972848518
[37m[1m[2023-06-25 09:21:11,797][129146] New mean coefficients: [[ 0.4985894  2.4593296 -1.3719125 -0.8088782  3.5297673]]
[37m[1m[2023-06-25 09:21:11,798][129146] Moving the mean solution point...
[36m[2023-06-25 09:21:21,586][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 09:21:21,587][129146] FPS: 392379.33
[36m[2023-06-25 09:21:21,589][129146] itr=909, itrs=2000, Progress: 45.45%
[36m[2023-06-25 09:21:33,105][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 09:21:33,106][129146] FPS: 334082.06
[36m[2023-06-25 09:21:37,951][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:21:37,952][129146] Reward + Measures: [[-33.86860382   0.96232903   0.06654266   0.96470231   0.88460433]]
[37m[1m[2023-06-25 09:21:37,952][129146] Max Reward on eval: -33.86860382490991
[37m[1m[2023-06-25 09:21:37,952][129146] Min Reward on eval: -33.86860382490991
[37m[1m[2023-06-25 09:21:37,952][129146] Mean Reward across all agents: -33.86860382490991
[37m[1m[2023-06-25 09:21:37,952][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:21:43,411][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:21:43,412][129146] Reward + Measures: [[-224.10692305    0.96000004    0.0513        0.9325        0.94340003]
[37m[1m [  54.67280036    0.9939        0.73719996    0.98890001    0.1928    ]
[37m[1m [-743.82675529    0.98690003    0.0012        0.98949999    0.97500002]
[37m[1m ...
[37m[1m [-110.8313927     0.92500001    0.59689999    0.98579997    0.17129999]
[37m[1m [-349.57736685    0.98729992    0.33450001    0.98820001    0.55129999]
[37m[1m [ -80.12429313    0.98559999    0.3777        0.98669988    0.57660002]]
[37m[1m[2023-06-25 09:21:43,412][129146] Max Reward on eval: 1015.4023071745527
[37m[1m[2023-06-25 09:21:43,413][129146] Min Reward on eval: -1081.2991943036789
[37m[1m[2023-06-25 09:21:43,413][129146] Mean Reward across all agents: -219.85310492948378
[37m[1m[2023-06-25 09:21:43,413][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:21:43,416][129146] mean_value=-350.82442804518206, max_value=665.1720378527559
[37m[1m[2023-06-25 09:21:43,418][129146] New mean coefficients: [[ 0.043244   2.5984395 -0.7585971 -1.3810141  3.1016552]]
[37m[1m[2023-06-25 09:21:43,419][129146] Moving the mean solution point...
[36m[2023-06-25 09:21:53,241][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 09:21:53,241][129146] FPS: 391036.05
[36m[2023-06-25 09:21:53,244][129146] itr=910, itrs=2000, Progress: 45.50%
[37m[1m[2023-06-25 09:22:00,043][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000890
[36m[2023-06-25 09:22:11,896][129146] train() took 11.62 seconds to complete
[36m[2023-06-25 09:22:11,896][129146] FPS: 330453.98
[36m[2023-06-25 09:22:16,525][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:22:16,525][129146] Reward + Measures: [[-38.82635504   0.99136096   0.03016967   0.99093133   0.948066  ]]
[37m[1m[2023-06-25 09:22:16,526][129146] Max Reward on eval: -38.82635504165722
[37m[1m[2023-06-25 09:22:16,526][129146] Min Reward on eval: -38.82635504165722
[37m[1m[2023-06-25 09:22:16,526][129146] Mean Reward across all agents: -38.82635504165722
[37m[1m[2023-06-25 09:22:16,526][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:22:21,925][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:22:21,926][129146] Reward + Measures: [[314.22057805   0.58420002   0.43860003   0.31879997   0.49760005]
[37m[1m [ 27.73646642   0.59219998   0.23990002   0.59070003   0.2784    ]
[37m[1m [363.59751226   0.48640004   0.43360001   0.2422       0.47110006]
[37m[1m ...
[37m[1m [233.7273622    0.47389999   0.5995       0.14740001   0.62070006]
[37m[1m [ 20.61477822   0.41060001   0.34679997   0.30170003   0.3371    ]
[37m[1m [ 23.9875743    0.5122       0.40559998   0.3136       0.37840003]]
[37m[1m[2023-06-25 09:22:21,926][129146] Max Reward on eval: 630.9942917257198
[37m[1m[2023-06-25 09:22:21,926][129146] Min Reward on eval: -640.9500922197883
[37m[1m[2023-06-25 09:22:21,927][129146] Mean Reward across all agents: 81.17549398995307
[37m[1m[2023-06-25 09:22:21,927][129146] Average Trajectory Length: 992.7403333333333
[36m[2023-06-25 09:22:21,929][129146] mean_value=-983.1561925853665, max_value=525.8545055533441
[37m[1m[2023-06-25 09:22:21,932][129146] New mean coefficients: [[ 0.07884409  2.8643909  -0.9603476  -1.2786278   1.6940438 ]]
[37m[1m[2023-06-25 09:22:21,933][129146] Moving the mean solution point...
[36m[2023-06-25 09:22:31,563][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 09:22:31,563][129146] FPS: 398809.32
[36m[2023-06-25 09:22:31,565][129146] itr=911, itrs=2000, Progress: 45.55%
[36m[2023-06-25 09:22:42,952][129146] train() took 11.36 seconds to complete
[36m[2023-06-25 09:22:42,952][129146] FPS: 337913.80
[36m[2023-06-25 09:22:47,617][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:22:47,618][129146] Reward + Measures: [[-64.52056587   0.98845965   0.00978733   0.98823833   0.96851635]]
[37m[1m[2023-06-25 09:22:47,618][129146] Max Reward on eval: -64.52056587243456
[37m[1m[2023-06-25 09:22:47,618][129146] Min Reward on eval: -64.52056587243456
[37m[1m[2023-06-25 09:22:47,618][129146] Mean Reward across all agents: -64.52056587243456
[37m[1m[2023-06-25 09:22:47,619][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:22:53,042][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:22:53,043][129146] Reward + Measures: [[ -80.98872535    0.98690003    0.0021        0.98940003    0.96789998]
[37m[1m [-881.96465068    0.99080002    0.0018        0.9946        0.97060007]
[37m[1m [-691.75409447    0.94700003    0.0018        0.98479998    0.89560002]
[37m[1m ...
[37m[1m [-616.99108741    0.99329996    0.0018        0.991         0.9842    ]
[37m[1m [-438.26931042    0.991         0.0017        0.99139994    0.97420007]
[37m[1m [ -47.26330355    0.75209999    0.1265        0.73610002    0.5697    ]]
[37m[1m[2023-06-25 09:22:53,043][129146] Max Reward on eval: 806.1468699041288
[37m[1m[2023-06-25 09:22:53,043][129146] Min Reward on eval: -1154.6604845981115
[37m[1m[2023-06-25 09:22:53,043][129146] Mean Reward across all agents: -304.04649001358865
[37m[1m[2023-06-25 09:22:53,044][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:22:53,046][129146] mean_value=-481.35205829173015, max_value=774.2749141592606
[37m[1m[2023-06-25 09:22:53,049][129146] New mean coefficients: [[ 0.39560485  1.4882923   1.1075842  -1.1107091   2.137224  ]]
[37m[1m[2023-06-25 09:22:53,050][129146] Moving the mean solution point...
[36m[2023-06-25 09:23:02,749][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 09:23:02,749][129146] FPS: 395989.70
[36m[2023-06-25 09:23:02,751][129146] itr=912, itrs=2000, Progress: 45.60%
[36m[2023-06-25 09:23:14,222][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 09:23:14,222][129146] FPS: 335426.12
[36m[2023-06-25 09:23:19,091][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:23:19,092][129146] Reward + Measures: [[-28.13704718   0.9878397    0.03663567   0.97653097   0.94144434]]
[37m[1m[2023-06-25 09:23:19,092][129146] Max Reward on eval: -28.137047184686505
[37m[1m[2023-06-25 09:23:19,092][129146] Min Reward on eval: -28.137047184686505
[37m[1m[2023-06-25 09:23:19,092][129146] Mean Reward across all agents: -28.137047184686505
[37m[1m[2023-06-25 09:23:19,093][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:23:24,560][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:23:24,560][129146] Reward + Measures: [[-423.65147952    0.85649997    0.0716        0.954         0.62720007]
[37m[1m [-263.14603932    0.99420005    0.0022        0.99010003    0.97119999]
[37m[1m [ 214.5960318     0.75419998    0.42030001    0.39660001    0.50699997]
[37m[1m ...
[37m[1m [ -50.07683011    0.99049997    0.0022        0.98820001    0.95499992]
[37m[1m [ 784.55265544    0.77749997    0.7184        0.12719999    0.7148    ]
[37m[1m [-241.60376493    0.98950005    0.002         0.98920006    0.9677    ]]
[37m[1m[2023-06-25 09:23:24,560][129146] Max Reward on eval: 865.7025674401841
[37m[1m[2023-06-25 09:23:24,561][129146] Min Reward on eval: -1079.0742817444727
[37m[1m[2023-06-25 09:23:24,561][129146] Mean Reward across all agents: 58.18688595326845
[37m[1m[2023-06-25 09:23:24,561][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:23:24,566][129146] mean_value=-180.61995859731613, max_value=956.6898986004933
[37m[1m[2023-06-25 09:23:24,569][129146] New mean coefficients: [[-0.06545562  1.7507344   1.2079682  -0.33677644  1.8781055 ]]
[37m[1m[2023-06-25 09:23:24,570][129146] Moving the mean solution point...
[36m[2023-06-25 09:23:34,395][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 09:23:34,395][129146] FPS: 390891.96
[36m[2023-06-25 09:23:34,398][129146] itr=913, itrs=2000, Progress: 45.65%
[36m[2023-06-25 09:23:45,949][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 09:23:45,949][129146] FPS: 333183.20
[36m[2023-06-25 09:23:50,825][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:23:50,826][129146] Reward + Measures: [[-59.91628611   0.99042135   0.020528     0.98204529   0.9588623 ]]
[37m[1m[2023-06-25 09:23:50,826][129146] Max Reward on eval: -59.916286112670726
[37m[1m[2023-06-25 09:23:50,826][129146] Min Reward on eval: -59.916286112670726
[37m[1m[2023-06-25 09:23:50,826][129146] Mean Reward across all agents: -59.916286112670726
[37m[1m[2023-06-25 09:23:50,826][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:23:56,511][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:23:56,511][129146] Reward + Measures: [[ -98.46777754    0.99090004    0.0025        0.99040002    0.97630006]
[37m[1m [-150.52129509    0.99019998    0.0022        0.98660004    0.97010005]
[37m[1m [ -18.56453592    0.98620003    0.0022        0.98759997    0.96460003]
[37m[1m ...
[37m[1m [ 380.05886509    0.93900007    0.60350001    0.414         0.82569999]
[37m[1m [  -4.97744256    0.98559999    0.08760001    0.89499998    0.95949996]
[37m[1m [-127.5700613     0.98860008    0.0023        0.98820001    0.97049999]]
[37m[1m[2023-06-25 09:23:56,511][129146] Max Reward on eval: 770.4072480895557
[37m[1m[2023-06-25 09:23:56,512][129146] Min Reward on eval: -263.7035247011343
[37m[1m[2023-06-25 09:23:56,512][129146] Mean Reward across all agents: 1.8478047809351243
[37m[1m[2023-06-25 09:23:56,512][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:23:56,515][129146] mean_value=-147.06487740061525, max_value=782.650800110543
[37m[1m[2023-06-25 09:23:56,518][129146] New mean coefficients: [[ 0.35806936  0.918676    1.5852329  -1.1279426   0.8974359 ]]
[37m[1m[2023-06-25 09:23:56,519][129146] Moving the mean solution point...
[36m[2023-06-25 09:24:06,363][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 09:24:06,364][129146] FPS: 390127.33
[36m[2023-06-25 09:24:06,366][129146] itr=914, itrs=2000, Progress: 45.70%
[36m[2023-06-25 09:24:18,017][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 09:24:18,017][129146] FPS: 330260.94
[36m[2023-06-25 09:24:22,863][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:24:22,864][129146] Reward + Measures: [[-51.27325519   0.99071795   0.01685667   0.98273063   0.96234196]]
[37m[1m[2023-06-25 09:24:22,864][129146] Max Reward on eval: -51.2732551935866
[37m[1m[2023-06-25 09:24:22,864][129146] Min Reward on eval: -51.2732551935866
[37m[1m[2023-06-25 09:24:22,865][129146] Mean Reward across all agents: -51.2732551935866
[37m[1m[2023-06-25 09:24:22,865][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:24:28,332][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:24:28,332][129146] Reward + Measures: [[  7.91046012   0.89229995   0.09469999   0.90539998   0.79249996]
[37m[1m [-56.8626982    0.99080008   0.0039       0.98530006   0.97000009]
[37m[1m [ 24.47630008   0.98579997   0.0044       0.98590004   0.95109999]
[37m[1m ...
[37m[1m [ 46.89838974   0.97650003   0.1829       0.89020008   0.85719997]
[37m[1m [ -6.62456161   0.99189997   0.0056       0.98820013   0.96670002]
[37m[1m [ 28.68646609   0.99379998   0.0026       0.98699999   0.97240001]]
[37m[1m[2023-06-25 09:24:28,332][129146] Max Reward on eval: 373.74979897306184
[37m[1m[2023-06-25 09:24:28,333][129146] Min Reward on eval: -158.54466350818984
[37m[1m[2023-06-25 09:24:28,333][129146] Mean Reward across all agents: 2.4315757910722753
[37m[1m[2023-06-25 09:24:28,333][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:24:28,336][129146] mean_value=-127.57431658457034, max_value=759.8188819809282
[37m[1m[2023-06-25 09:24:28,339][129146] New mean coefficients: [[ 0.24935883 -0.17304373  3.0291457  -1.8270812   0.793706  ]]
[37m[1m[2023-06-25 09:24:28,340][129146] Moving the mean solution point...
[36m[2023-06-25 09:24:38,210][129146] train() took 9.87 seconds to complete
[36m[2023-06-25 09:24:38,210][129146] FPS: 389105.85
[36m[2023-06-25 09:24:38,213][129146] itr=915, itrs=2000, Progress: 45.75%
[36m[2023-06-25 09:24:49,744][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 09:24:49,744][129146] FPS: 333764.13
[36m[2023-06-25 09:24:54,556][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:24:54,557][129146] Reward + Measures: [[-45.89117198   0.98791534   0.02723367   0.97307301   0.95466405]]
[37m[1m[2023-06-25 09:24:54,557][129146] Max Reward on eval: -45.891171983584606
[37m[1m[2023-06-25 09:24:54,557][129146] Min Reward on eval: -45.891171983584606
[37m[1m[2023-06-25 09:24:54,557][129146] Mean Reward across all agents: -45.891171983584606
[37m[1m[2023-06-25 09:24:54,558][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:25:00,045][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:25:00,045][129146] Reward + Measures: [[-396.40115788    0.98240006    0.003         0.98199999    0.94020003]
[37m[1m [-189.500749      0.94          0.26890001    0.66149998    0.88959998]
[37m[1m [-219.12224148    0.93599999    0.1688        0.7518        0.81759995]
[37m[1m ...
[37m[1m [ 161.49377464    0.70810002    0.1769        0.44980001    0.44749999]
[37m[1m [-358.65199315    0.98140001    0.0029        0.98660004    0.95389998]
[37m[1m [-138.51486053    0.7349        0.10150001    0.4894        0.43059999]]
[37m[1m[2023-06-25 09:25:00,045][129146] Max Reward on eval: 517.7602722511626
[37m[1m[2023-06-25 09:25:00,046][129146] Min Reward on eval: -692.2301772702601
[37m[1m[2023-06-25 09:25:00,046][129146] Mean Reward across all agents: -189.77968323748766
[37m[1m[2023-06-25 09:25:00,046][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:25:00,049][129146] mean_value=-290.1223237361349, max_value=863.1580499757081
[37m[1m[2023-06-25 09:25:00,052][129146] New mean coefficients: [[ 0.29124498 -1.1343081   3.8803256  -2.0298488   0.8290767 ]]
[37m[1m[2023-06-25 09:25:00,053][129146] Moving the mean solution point...
[36m[2023-06-25 09:25:09,864][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 09:25:09,864][129146] FPS: 391485.21
[36m[2023-06-25 09:25:09,866][129146] itr=916, itrs=2000, Progress: 45.80%
[36m[2023-06-25 09:25:21,470][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 09:25:21,471][129146] FPS: 331547.03
[36m[2023-06-25 09:25:26,201][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:25:26,207][129146] Reward + Measures: [[-24.13147586   0.98610938   0.04448666   0.96868861   0.93899566]]
[37m[1m[2023-06-25 09:25:26,207][129146] Max Reward on eval: -24.131475863110545
[37m[1m[2023-06-25 09:25:26,208][129146] Min Reward on eval: -24.131475863110545
[37m[1m[2023-06-25 09:25:26,208][129146] Mean Reward across all agents: -24.131475863110545
[37m[1m[2023-06-25 09:25:26,208][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:25:31,831][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:25:31,832][129146] Reward + Measures: [[ 19.53669454   0.96519995   0.1006       0.86819994   0.91860002]
[37m[1m [166.31646619   0.94620001   0.26110002   0.70170003   0.92030001]
[37m[1m [-28.36499021   0.98590004   0.0027       0.9884001    0.96130002]
[37m[1m ...
[37m[1m [181.79759407   0.92189997   0.23800002   0.70530003   0.87879992]
[37m[1m [228.58843125   0.92870009   0.34889999   0.60149997   0.89660007]
[37m[1m [  8.23545985   0.95839995   0.08520001   0.8901       0.92410004]]
[37m[1m[2023-06-25 09:25:31,832][129146] Max Reward on eval: 938.0689978766255
[37m[1m[2023-06-25 09:25:31,832][129146] Min Reward on eval: -127.29161867504008
[37m[1m[2023-06-25 09:25:31,832][129146] Mean Reward across all agents: 185.24890286611432
[37m[1m[2023-06-25 09:25:31,833][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:25:31,837][129146] mean_value=29.31107274309122, max_value=852.5231548258741
[37m[1m[2023-06-25 09:25:31,840][129146] New mean coefficients: [[ 0.32064873 -3.1363165   4.464693   -2.4652715   0.02576852]]
[37m[1m[2023-06-25 09:25:31,841][129146] Moving the mean solution point...
[36m[2023-06-25 09:25:41,549][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 09:25:41,549][129146] FPS: 395618.24
[36m[2023-06-25 09:25:41,552][129146] itr=917, itrs=2000, Progress: 45.85%
[36m[2023-06-25 09:25:52,912][129146] train() took 11.34 seconds to complete
[36m[2023-06-25 09:25:52,912][129146] FPS: 338676.15
[36m[2023-06-25 09:25:57,666][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:25:57,666][129146] Reward + Measures: [[-4.01877693  0.98354006  0.06647533  0.95352066  0.92691696]]
[37m[1m[2023-06-25 09:25:57,667][129146] Max Reward on eval: -4.018776931962241
[37m[1m[2023-06-25 09:25:57,667][129146] Min Reward on eval: -4.018776931962241
[37m[1m[2023-06-25 09:25:57,667][129146] Mean Reward across all agents: -4.018776931962241
[37m[1m[2023-06-25 09:25:57,667][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:26:03,119][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:26:03,120][129146] Reward + Measures: [[210.09096252   0.94090003   0.1339       0.82199997   0.90070003]
[37m[1m [ 55.59780187   0.98999995   0.0022       0.98850006   0.97559994]
[37m[1m [256.09390619   0.90580004   0.22479999   0.70859998   0.861     ]
[37m[1m ...
[37m[1m [288.44823869   0.93000001   0.25190002   0.76539993   0.76510012]
[37m[1m [152.5083818    0.96899998   0.17150001   0.88640004   0.84899998]
[37m[1m [ 40.58337505   0.98339999   0.0147       0.98040003   0.93260002]]
[37m[1m[2023-06-25 09:26:03,120][129146] Max Reward on eval: 1028.0025991243776
[37m[1m[2023-06-25 09:26:03,120][129146] Min Reward on eval: -119.36765277865342
[37m[1m[2023-06-25 09:26:03,121][129146] Mean Reward across all agents: 402.878788404789
[37m[1m[2023-06-25 09:26:03,121][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:26:03,128][129146] mean_value=199.09979645567552, max_value=1136.4226859776186
[37m[1m[2023-06-25 09:26:03,130][129146] New mean coefficients: [[ 0.2632115  -3.76989     4.9343505  -3.132471   -0.44808817]]
[37m[1m[2023-06-25 09:26:03,131][129146] Moving the mean solution point...
[36m[2023-06-25 09:26:12,835][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 09:26:12,835][129146] FPS: 395780.30
[36m[2023-06-25 09:26:12,837][129146] itr=918, itrs=2000, Progress: 45.90%
[36m[2023-06-25 09:26:24,239][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 09:26:24,239][129146] FPS: 337459.54
[36m[2023-06-25 09:26:28,893][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:26:28,893][129146] Reward + Measures: [[50.01970798  0.9727667   0.12779133  0.8907553   0.90442127]]
[37m[1m[2023-06-25 09:26:28,893][129146] Max Reward on eval: 50.019707975856036
[37m[1m[2023-06-25 09:26:28,893][129146] Min Reward on eval: 50.019707975856036
[37m[1m[2023-06-25 09:26:28,894][129146] Mean Reward across all agents: 50.019707975856036
[37m[1m[2023-06-25 09:26:28,894][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:26:34,518][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:26:34,518][129146] Reward + Measures: [[753.44613237   0.82340002   0.74720001   0.13610001   0.79460001]
[37m[1m [327.97868119   0.9285       0.32740003   0.62380004   0.88429993]
[37m[1m [647.42207215   0.87010002   0.6839       0.22129999   0.84839994]
[37m[1m ...
[37m[1m [573.15986499   0.89530003   0.60140002   0.31420001   0.86330003]
[37m[1m [ 83.92564965   0.98629999   0.0838       0.9835       0.86180001]
[37m[1m [704.50735199   0.80699998   0.65619999   0.24150002   0.78860003]]
[37m[1m[2023-06-25 09:26:34,519][129146] Max Reward on eval: 1139.8961500831065
[37m[1m[2023-06-25 09:26:34,519][129146] Min Reward on eval: -138.12692509407643
[37m[1m[2023-06-25 09:26:34,519][129146] Mean Reward across all agents: 553.6430796160017
[37m[1m[2023-06-25 09:26:34,519][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:26:34,527][129146] mean_value=163.01030330023912, max_value=1109.6871345814634
[37m[1m[2023-06-25 09:26:34,529][129146] New mean coefficients: [[ 0.37428886 -5.9792266   5.614304   -2.9704456  -0.527954  ]]
[37m[1m[2023-06-25 09:26:34,531][129146] Moving the mean solution point...
[36m[2023-06-25 09:26:44,175][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 09:26:44,176][129146] FPS: 398218.27
[36m[2023-06-25 09:26:44,178][129146] itr=919, itrs=2000, Progress: 45.95%
[36m[2023-06-25 09:26:55,714][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 09:26:55,715][129146] FPS: 333486.31
[36m[2023-06-25 09:27:00,507][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:27:00,507][129146] Reward + Measures: [[125.79680703   0.95864564   0.22927067   0.80330437   0.86625558]]
[37m[1m[2023-06-25 09:27:00,508][129146] Max Reward on eval: 125.79680703402097
[37m[1m[2023-06-25 09:27:00,508][129146] Min Reward on eval: 125.79680703402097
[37m[1m[2023-06-25 09:27:00,508][129146] Mean Reward across all agents: 125.79680703402097
[37m[1m[2023-06-25 09:27:00,508][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:27:05,910][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:27:05,910][129146] Reward + Measures: [[708.65841567   0.57590002   0.36470002   0.36090001   0.44850001]
[37m[1m [792.55240607   0.76119995   0.80699998   0.0313       0.81660002]
[37m[1m [917.39024877   0.49530002   0.49179998   0.2185       0.46660003]
[37m[1m ...
[37m[1m [598.077386     0.85120004   0.57230002   0.31670001   0.83690006]
[37m[1m [877.23661326   0.6164       0.58060002   0.2184       0.55950004]
[37m[1m [715.97178081   0.74940002   0.47220001   0.39180002   0.69060004]]
[37m[1m[2023-06-25 09:27:05,911][129146] Max Reward on eval: 1140.25543363092
[37m[1m[2023-06-25 09:27:05,911][129146] Min Reward on eval: 270.2600588913774
[37m[1m[2023-06-25 09:27:05,911][129146] Mean Reward across all agents: 740.9025834405013
[37m[1m[2023-06-25 09:27:05,911][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:27:05,918][129146] mean_value=129.9391014539417, max_value=1005.384224058995
[37m[1m[2023-06-25 09:27:05,921][129146] New mean coefficients: [[ 0.5135125 -6.7109194  6.078075  -2.7914455 -0.8149383]]
[37m[1m[2023-06-25 09:27:05,922][129146] Moving the mean solution point...
[36m[2023-06-25 09:27:15,549][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 09:27:15,549][129146] FPS: 398935.03
[36m[2023-06-25 09:27:15,552][129146] itr=920, itrs=2000, Progress: 46.00%
[37m[1m[2023-06-25 09:27:22,356][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000900
[36m[2023-06-25 09:27:34,081][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 09:27:34,081][129146] FPS: 334249.38
[36m[2023-06-25 09:27:38,655][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:27:38,655][129146] Reward + Measures: [[291.71439298   0.93388069   0.39165902   0.63252831   0.83662271]]
[37m[1m[2023-06-25 09:27:38,655][129146] Max Reward on eval: 291.714392982181
[37m[1m[2023-06-25 09:27:38,655][129146] Min Reward on eval: 291.714392982181
[37m[1m[2023-06-25 09:27:38,656][129146] Mean Reward across all agents: 291.714392982181
[37m[1m[2023-06-25 09:27:38,656][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:27:44,037][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:27:44,038][129146] Reward + Measures: [[833.6796028    0.68520004   0.64209998   0.16760001   0.65230006]
[37m[1m [889.14176757   0.67819995   0.75649995   0.0624       0.67959994]
[37m[1m [788.68950657   0.59429997   0.81700003   0.0451       0.81800002]
[37m[1m ...
[37m[1m [155.08239941   0.92890006   0.0606       0.89540005   0.86779994]
[37m[1m [264.21902246   0.88679999   0.1673       0.764        0.84090006]
[37m[1m [173.37221035   0.96999997   0.0046       0.97399998   0.89910001]]
[37m[1m[2023-06-25 09:27:44,038][129146] Max Reward on eval: 1007.0478864072356
[37m[1m[2023-06-25 09:27:44,038][129146] Min Reward on eval: 111.39366099056788
[37m[1m[2023-06-25 09:27:44,038][129146] Mean Reward across all agents: 578.329828204507
[37m[1m[2023-06-25 09:27:44,039][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:27:44,046][129146] mean_value=143.1239540333877, max_value=1100.8048994820147
[37m[1m[2023-06-25 09:27:44,048][129146] New mean coefficients: [[ 0.5777369  -5.563577    6.7270727  -2.50472    -0.57135326]]
[37m[1m[2023-06-25 09:27:44,049][129146] Moving the mean solution point...
[36m[2023-06-25 09:27:53,749][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 09:27:53,749][129146] FPS: 395954.53
[36m[2023-06-25 09:27:53,751][129146] itr=921, itrs=2000, Progress: 46.05%
[36m[2023-06-25 09:28:05,245][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 09:28:05,245][129146] FPS: 334748.53
[36m[2023-06-25 09:28:10,118][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:28:10,118][129146] Reward + Measures: [[519.25407048   0.89619237   0.58810401   0.38690132   0.83925062]]
[37m[1m[2023-06-25 09:28:10,118][129146] Max Reward on eval: 519.254070481463
[37m[1m[2023-06-25 09:28:10,118][129146] Min Reward on eval: 519.254070481463
[37m[1m[2023-06-25 09:28:10,119][129146] Mean Reward across all agents: 519.254070481463
[37m[1m[2023-06-25 09:28:10,119][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:28:15,582][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:28:15,582][129146] Reward + Measures: [[779.03514416   0.88120002   0.75909996   0.1191       0.87690002]
[37m[1m [587.33935739   0.94779998   0.72060007   0.30930001   0.83809996]
[37m[1m [834.37338304   0.90140003   0.89719993   0.0176       0.91820002]
[37m[1m ...
[37m[1m [857.97076031   0.82650006   0.83689994   0.0259       0.82209998]
[37m[1m [849.75686827   0.87579995   0.89390004   0.0236       0.90600008]
[37m[1m [814.87180878   0.8434       0.84650004   0.0272       0.8689    ]]
[37m[1m[2023-06-25 09:28:15,583][129146] Max Reward on eval: 1094.0518088779645
[37m[1m[2023-06-25 09:28:15,583][129146] Min Reward on eval: 191.9999564498663
[37m[1m[2023-06-25 09:28:15,583][129146] Mean Reward across all agents: 809.5182135140453
[37m[1m[2023-06-25 09:28:15,583][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:28:15,587][129146] mean_value=22.419100458555512, max_value=1189.0913494174833
[37m[1m[2023-06-25 09:28:15,590][129146] New mean coefficients: [[ 0.6954421  -4.2437124   6.1739783  -2.1764836  -0.82590395]]
[37m[1m[2023-06-25 09:28:15,591][129146] Moving the mean solution point...
[36m[2023-06-25 09:28:25,335][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 09:28:25,336][129146] FPS: 394152.36
[36m[2023-06-25 09:28:25,338][129146] itr=922, itrs=2000, Progress: 46.10%
[36m[2023-06-25 09:28:36,890][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 09:28:36,891][129146] FPS: 333038.81
[36m[2023-06-25 09:28:41,716][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:28:41,717][129146] Reward + Measures: [[648.49220888   0.83147836   0.67564166   0.26214498   0.81435364]]
[37m[1m[2023-06-25 09:28:41,717][129146] Max Reward on eval: 648.4922088821214
[37m[1m[2023-06-25 09:28:41,717][129146] Min Reward on eval: 648.4922088821214
[37m[1m[2023-06-25 09:28:41,717][129146] Mean Reward across all agents: 648.4922088821214
[37m[1m[2023-06-25 09:28:41,717][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:28:47,271][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:28:47,272][129146] Reward + Measures: [[443.81082256   0.81639999   0.4982       0.42430001   0.81389999]
[37m[1m [643.48836476   0.80429995   0.75830001   0.15709999   0.83780003]
[37m[1m [583.32876967   0.77910006   0.67309999   0.24820001   0.8089    ]
[37m[1m ...
[37m[1m [668.07010002   0.8398       0.7626       0.13310002   0.85980004]
[37m[1m [548.74491804   0.84039992   0.61070007   0.3276       0.89179993]
[37m[1m [819.58920841   0.68219995   0.80930007   0.0662       0.70710003]]
[37m[1m[2023-06-25 09:28:47,277][129146] Max Reward on eval: 1009.6379996511853
[37m[1m[2023-06-25 09:28:47,277][129146] Min Reward on eval: 138.89209034673405
[37m[1m[2023-06-25 09:28:47,277][129146] Mean Reward across all agents: 624.073110085979
[37m[1m[2023-06-25 09:28:47,278][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:28:47,283][129146] mean_value=155.263833539482, max_value=1164.467990528734
[37m[1m[2023-06-25 09:28:47,286][129146] New mean coefficients: [[ 0.51844454 -4.2683306   6.2161503  -2.0160637  -0.7411408 ]]
[37m[1m[2023-06-25 09:28:47,287][129146] Moving the mean solution point...
[36m[2023-06-25 09:28:57,046][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 09:28:57,046][129146] FPS: 393535.07
[36m[2023-06-25 09:28:57,049][129146] itr=923, itrs=2000, Progress: 46.15%
[36m[2023-06-25 09:29:08,678][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 09:29:08,678][129146] FPS: 330836.71
[36m[2023-06-25 09:29:13,568][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:29:13,573][129146] Reward + Measures: [[769.20793693   0.75412434   0.76074904   0.15518266   0.78534669]]
[37m[1m[2023-06-25 09:29:13,574][129146] Max Reward on eval: 769.2079369265609
[37m[1m[2023-06-25 09:29:13,574][129146] Min Reward on eval: 769.2079369265609
[37m[1m[2023-06-25 09:29:13,574][129146] Mean Reward across all agents: 769.2079369265609
[37m[1m[2023-06-25 09:29:13,575][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:29:19,236][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:29:19,236][129146] Reward + Measures: [[751.22208903   0.8089       0.87550002   0.034        0.82909995]
[37m[1m [677.28480228   0.67030001   0.76980007   0.0548       0.69730008]
[37m[1m [763.54083847   0.7098       0.72500002   0.1499       0.7277    ]
[37m[1m ...
[37m[1m [633.18422422   0.79440004   0.75130004   0.1303       0.7827    ]
[37m[1m [812.58771808   0.72619992   0.85080004   0.051        0.77830005]
[37m[1m [858.62460455   0.68740004   0.81980002   0.0501       0.73639995]]
[37m[1m[2023-06-25 09:29:19,237][129146] Max Reward on eval: 933.7766447926522
[37m[1m[2023-06-25 09:29:19,237][129146] Min Reward on eval: 295.89304907934564
[37m[1m[2023-06-25 09:29:19,237][129146] Mean Reward across all agents: 745.6127485245713
[37m[1m[2023-06-25 09:29:19,238][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:29:19,241][129146] mean_value=-43.283030223699384, max_value=1306.3778210400476
[37m[1m[2023-06-25 09:29:19,244][129146] New mean coefficients: [[ 0.69119334 -3.1812198   5.0229297  -1.2596724  -0.20567006]]
[37m[1m[2023-06-25 09:29:19,245][129146] Moving the mean solution point...
[36m[2023-06-25 09:29:29,161][129146] train() took 9.92 seconds to complete
[36m[2023-06-25 09:29:29,162][129146] FPS: 387291.01
[36m[2023-06-25 09:29:29,164][129146] itr=924, itrs=2000, Progress: 46.20%
[36m[2023-06-25 09:29:40,836][129146] train() took 11.65 seconds to complete
[36m[2023-06-25 09:29:40,837][129146] FPS: 329720.42
[36m[2023-06-25 09:29:45,695][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:29:45,695][129146] Reward + Measures: [[908.4948542    0.64959604   0.79981631   0.09339833   0.72377628]]
[37m[1m[2023-06-25 09:29:45,695][129146] Max Reward on eval: 908.4948542011622
[37m[1m[2023-06-25 09:29:45,696][129146] Min Reward on eval: 908.4948542011622
[37m[1m[2023-06-25 09:29:45,696][129146] Mean Reward across all agents: 908.4948542011622
[37m[1m[2023-06-25 09:29:45,696][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:29:51,196][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:29:51,196][129146] Reward + Measures: [[ 846.87441212    0.5801        0.70359999    0.11979999    0.6688    ]
[37m[1m [ 984.93108899    0.54329997    0.78770006    0.0535        0.60810006]
[37m[1m [1012.57903687    0.50650001    0.69749999    0.1193        0.55089998]
[37m[1m ...
[37m[1m [ 987.73403636    0.57700002    0.73040003    0.055         0.52920002]
[37m[1m [ 630.6010321     0.8136        0.62400001    0.30250001    0.82790005]
[37m[1m [1080.23365823    0.53450006    0.76539993    0.08130001    0.53420001]]
[37m[1m[2023-06-25 09:29:51,197][129146] Max Reward on eval: 1327.8596204885748
[37m[1m[2023-06-25 09:29:51,197][129146] Min Reward on eval: 624.2391137179686
[37m[1m[2023-06-25 09:29:51,197][129146] Mean Reward across all agents: 931.359844057616
[37m[1m[2023-06-25 09:29:51,198][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:29:51,203][129146] mean_value=146.75959353232807, max_value=1209.2953762881623
[37m[1m[2023-06-25 09:29:51,206][129146] New mean coefficients: [[ 1.274317   -2.1367738   4.1352487  -0.43071705 -0.54551905]]
[37m[1m[2023-06-25 09:29:51,207][129146] Moving the mean solution point...
[36m[2023-06-25 09:30:01,150][129146] train() took 9.94 seconds to complete
[36m[2023-06-25 09:30:01,150][129146] FPS: 386269.58
[36m[2023-06-25 09:30:01,153][129146] itr=925, itrs=2000, Progress: 46.25%
[36m[2023-06-25 09:30:12,691][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 09:30:12,691][129146] FPS: 333570.38
[36m[2023-06-25 09:30:17,473][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:30:17,478][129146] Reward + Measures: [[994.11614407   0.58555901   0.80551797   0.08342867   0.67915666]]
[37m[1m[2023-06-25 09:30:17,479][129146] Max Reward on eval: 994.1161440744236
[37m[1m[2023-06-25 09:30:17,479][129146] Min Reward on eval: 994.1161440744236
[37m[1m[2023-06-25 09:30:17,479][129146] Mean Reward across all agents: 994.1161440744236
[37m[1m[2023-06-25 09:30:17,480][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:30:22,921][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:30:22,926][129146] Reward + Measures: [[ 831.17022332    0.75100005    0.75740004    0.1425        0.75079995]
[37m[1m [1020.61292743    0.67940003    0.83070004    0.0593        0.6972    ]
[37m[1m [1010.72075649    0.53899997    0.78830004    0.07030001    0.6419    ]
[37m[1m ...
[37m[1m [ 919.30158478    0.61980003    0.87190002    0.0453        0.80370009]
[37m[1m [1079.71921267    0.61570001    0.66420001    0.178         0.56770003]
[37m[1m [ 505.24922213    0.89519995    0.5248        0.40890002    0.91170007]]
[37m[1m[2023-06-25 09:30:22,927][129146] Max Reward on eval: 1206.7172394345048
[37m[1m[2023-06-25 09:30:22,927][129146] Min Reward on eval: 296.4669638189953
[37m[1m[2023-06-25 09:30:22,927][129146] Mean Reward across all agents: 819.0886082357039
[37m[1m[2023-06-25 09:30:22,927][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:30:22,935][129146] mean_value=198.72896087339234, max_value=1139.579244460614
[37m[1m[2023-06-25 09:30:22,938][129146] New mean coefficients: [[ 1.5184158 -1.8195364  3.162808  -0.226948  -0.7215284]]
[37m[1m[2023-06-25 09:30:22,939][129146] Moving the mean solution point...
[36m[2023-06-25 09:30:32,727][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 09:30:32,727][129146] FPS: 392387.72
[36m[2023-06-25 09:30:32,729][129146] itr=926, itrs=2000, Progress: 46.30%
[36m[2023-06-25 09:30:44,264][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 09:30:44,264][129146] FPS: 333576.59
[36m[2023-06-25 09:30:49,126][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:30:49,126][129146] Reward + Measures: [[1096.10728691    0.51532334    0.79328203    0.08592901    0.60963202]]
[37m[1m[2023-06-25 09:30:49,126][129146] Max Reward on eval: 1096.107286910962
[37m[1m[2023-06-25 09:30:49,127][129146] Min Reward on eval: 1096.107286910962
[37m[1m[2023-06-25 09:30:49,127][129146] Mean Reward across all agents: 1096.107286910962
[37m[1m[2023-06-25 09:30:49,127][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:30:54,584][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:30:54,585][129146] Reward + Measures: [[1071.27629813    0.51919997    0.7956        0.0729        0.60720003]
[37m[1m [ 892.89002579    0.68450004    0.85760003    0.0328        0.75910008]
[37m[1m [ 923.15018946    0.54530001    0.66990006    0.1471        0.58289999]
[37m[1m ...
[37m[1m [1062.18778978    0.51340002    0.77159995    0.07410001    0.60289997]
[37m[1m [1047.58060747    0.48919997    0.77739996    0.0775        0.6056    ]
[37m[1m [ 974.60644488    0.58240002    0.83590001    0.0526        0.68029994]]
[37m[1m[2023-06-25 09:30:54,585][129146] Max Reward on eval: 1211.3926028499613
[37m[1m[2023-06-25 09:30:54,585][129146] Min Reward on eval: 341.59180939220823
[37m[1m[2023-06-25 09:30:54,585][129146] Mean Reward across all agents: 957.160039217264
[37m[1m[2023-06-25 09:30:54,586][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:30:54,591][129146] mean_value=148.567618474647, max_value=1556.7539217754152
[37m[1m[2023-06-25 09:30:54,594][129146] New mean coefficients: [[ 1.5677024  -1.1206295   1.343958   -0.15104014 -0.84056336]]
[37m[1m[2023-06-25 09:30:54,594][129146] Moving the mean solution point...
[36m[2023-06-25 09:31:04,401][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 09:31:04,402][129146] FPS: 391631.43
[36m[2023-06-25 09:31:04,404][129146] itr=927, itrs=2000, Progress: 46.35%
[36m[2023-06-25 09:31:15,971][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 09:31:15,972][129146] FPS: 332668.67
[36m[2023-06-25 09:31:20,694][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:31:20,700][129146] Reward + Measures: [[1236.80053577    0.482499      0.77008432    0.08167367    0.532812  ]]
[37m[1m[2023-06-25 09:31:20,700][129146] Max Reward on eval: 1236.800535774357
[37m[1m[2023-06-25 09:31:20,700][129146] Min Reward on eval: 1236.800535774357
[37m[1m[2023-06-25 09:31:20,701][129146] Mean Reward across all agents: 1236.800535774357
[37m[1m[2023-06-25 09:31:20,701][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:31:26,071][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:31:26,072][129146] Reward + Measures: [[1331.25458399    0.4506        0.72290003    0.0982        0.47790003]
[37m[1m [1389.06167588    0.4461        0.70050007    0.0998        0.4384    ]
[37m[1m [1248.24252096    0.47830001    0.745         0.0882        0.53200001]
[37m[1m ...
[37m[1m [1233.24075127    0.4711        0.73580003    0.0821        0.5072    ]
[37m[1m [1408.91857729    0.4375        0.71179998    0.0975        0.45170003]
[37m[1m [1361.25002201    0.40650001    0.65420002    0.1268        0.41840002]]
[37m[1m[2023-06-25 09:31:26,072][129146] Max Reward on eval: 1503.6564604779007
[37m[1m[2023-06-25 09:31:26,072][129146] Min Reward on eval: 780.9169759574579
[37m[1m[2023-06-25 09:31:26,072][129146] Mean Reward across all agents: 1279.1933563923242
[37m[1m[2023-06-25 09:31:26,073][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:31:26,080][129146] mean_value=341.1296171664558, max_value=1581.2649449449275
[37m[1m[2023-06-25 09:31:26,083][129146] New mean coefficients: [[ 1.5755409 -2.096727   1.4396759 -0.7402538 -1.4801488]]
[37m[1m[2023-06-25 09:31:26,084][129146] Moving the mean solution point...
[36m[2023-06-25 09:31:35,747][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 09:31:35,747][129146] FPS: 397472.75
[36m[2023-06-25 09:31:35,749][129146] itr=928, itrs=2000, Progress: 46.40%
[36m[2023-06-25 09:31:47,287][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 09:31:47,288][129146] FPS: 333465.61
[36m[2023-06-25 09:31:52,099][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:31:52,099][129146] Reward + Measures: [[1403.33072773    0.45268399    0.73503733    0.08176134    0.47027865]]
[37m[1m[2023-06-25 09:31:52,100][129146] Max Reward on eval: 1403.3307277283147
[37m[1m[2023-06-25 09:31:52,100][129146] Min Reward on eval: 1403.3307277283147
[37m[1m[2023-06-25 09:31:52,100][129146] Mean Reward across all agents: 1403.3307277283147
[37m[1m[2023-06-25 09:31:52,100][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:31:57,585][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:31:57,586][129146] Reward + Measures: [[1384.7820851     0.4756        0.73570007    0.0852        0.47720003]
[37m[1m [1492.15576989    0.5363        0.69849998    0.0952        0.41090003]
[37m[1m [1053.32546191    0.4567        0.75800002    0.0735        0.55380005]
[37m[1m ...
[37m[1m [1531.63947184    0.45439997    0.71290004    0.08270001    0.41219997]
[37m[1m [1312.16423537    0.48729998    0.7044        0.08620001    0.44259998]
[37m[1m [1367.47005516    0.4499        0.72149998    0.08510001    0.46440002]]
[37m[1m[2023-06-25 09:31:57,586][129146] Max Reward on eval: 1562.6711155392463
[37m[1m[2023-06-25 09:31:57,586][129146] Min Reward on eval: 1001.8438042665133
[37m[1m[2023-06-25 09:31:57,586][129146] Mean Reward across all agents: 1343.1239087098093
[37m[1m[2023-06-25 09:31:57,587][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:31:57,591][129146] mean_value=120.95983852607957, max_value=1480.8653272590448
[37m[1m[2023-06-25 09:31:57,594][129146] New mean coefficients: [[ 1.1545142 -1.7321775  0.9115037 -0.6205343 -2.0433726]]
[37m[1m[2023-06-25 09:31:57,595][129146] Moving the mean solution point...
[36m[2023-06-25 09:32:07,296][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 09:32:07,296][129146] FPS: 395902.02
[36m[2023-06-25 09:32:07,298][129146] itr=929, itrs=2000, Progress: 46.45%
[36m[2023-06-25 09:32:18,822][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 09:32:18,822][129146] FPS: 333911.91
[36m[2023-06-25 09:32:23,613][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:32:23,613][129146] Reward + Measures: [[1679.71739909    0.44515428    0.66287434    0.07947833    0.37289533]]
[37m[1m[2023-06-25 09:32:23,613][129146] Max Reward on eval: 1679.7173990892995
[37m[1m[2023-06-25 09:32:23,614][129146] Min Reward on eval: 1679.7173990892995
[37m[1m[2023-06-25 09:32:23,614][129146] Mean Reward across all agents: 1679.7173990892995
[37m[1m[2023-06-25 09:32:23,614][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:32:29,006][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:32:29,006][129146] Reward + Measures: [[1154.67915259    0.53259999    0.7723        0.0854        0.56650001]
[37m[1m [1652.47201819    0.45320001    0.65259999    0.07520001    0.38069999]
[37m[1m [1688.94431335    0.48100001    0.64609998    0.0741        0.35749999]
[37m[1m ...
[37m[1m [1820.9534423     0.46569997    0.63780004    0.0703        0.34620005]
[37m[1m [1598.45762706    0.4677        0.67770004    0.1014        0.3732    ]
[37m[1m [1876.27463485    0.44119999    0.62779999    0.0799        0.30860001]]
[37m[1m[2023-06-25 09:32:29,006][129146] Max Reward on eval: 1876.2746348494431
[37m[1m[2023-06-25 09:32:29,007][129146] Min Reward on eval: 920.0911018073326
[37m[1m[2023-06-25 09:32:29,007][129146] Mean Reward across all agents: 1606.9365264442301
[37m[1m[2023-06-25 09:32:29,007][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:32:29,012][129146] mean_value=169.33639574592792, max_value=997.5152823672432
[37m[1m[2023-06-25 09:32:29,015][129146] New mean coefficients: [[ 0.7916728 -0.9318929  1.2488463 -0.5157524 -2.2726374]]
[37m[1m[2023-06-25 09:32:29,016][129146] Moving the mean solution point...
[36m[2023-06-25 09:32:38,726][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 09:32:38,726][129146] FPS: 395525.16
[36m[2023-06-25 09:32:38,729][129146] itr=930, itrs=2000, Progress: 46.50%
[37m[1m[2023-06-25 09:32:45,475][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000910
[36m[2023-06-25 09:32:57,188][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 09:32:57,188][129146] FPS: 333825.40
[36m[2023-06-25 09:33:02,031][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:33:02,031][129146] Reward + Measures: [[1955.19511381    0.42368367    0.60841596    0.073283      0.30491665]]
[37m[1m[2023-06-25 09:33:02,031][129146] Max Reward on eval: 1955.195113809781
[37m[1m[2023-06-25 09:33:02,032][129146] Min Reward on eval: 1955.195113809781
[37m[1m[2023-06-25 09:33:02,032][129146] Mean Reward across all agents: 1955.195113809781
[37m[1m[2023-06-25 09:33:02,032][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:33:07,573][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:33:07,574][129146] Reward + Measures: [[ 937.95407025    0.65409994    0.419         0.3836        0.51800001]
[37m[1m [1002.72956062    0.57270002    0.42420003    0.36240003    0.46490002]
[37m[1m [ 729.94830181    0.62600005    0.41560003    0.42540002    0.49820003]
[37m[1m ...
[37m[1m [1593.10024965    0.54820001    0.53500003    0.1584        0.28999999]
[37m[1m [1536.07789       0.39770001    0.54030007    0.18030001    0.32539997]
[37m[1m [1400.31050447    0.52730006    0.45840001    0.27380002    0.33399999]]
[37m[1m[2023-06-25 09:33:07,574][129146] Max Reward on eval: 2028.5801941415993
[37m[1m[2023-06-25 09:33:07,574][129146] Min Reward on eval: 329.16974370483075
[37m[1m[2023-06-25 09:33:07,574][129146] Mean Reward across all agents: 1239.1121348389863
[37m[1m[2023-06-25 09:33:07,575][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:33:07,581][129146] mean_value=67.39332012534996, max_value=1762.4561532374978
[37m[1m[2023-06-25 09:33:07,584][129146] New mean coefficients: [[ 0.7727033 -1.4746479  1.4623562 -0.4190662 -2.8078752]]
[37m[1m[2023-06-25 09:33:07,585][129146] Moving the mean solution point...
[36m[2023-06-25 09:33:17,384][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 09:33:17,384][129146] FPS: 391938.78
[36m[2023-06-25 09:33:17,387][129146] itr=931, itrs=2000, Progress: 46.55%
[36m[2023-06-25 09:33:28,897][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 09:33:28,897][129146] FPS: 334259.92
[36m[2023-06-25 09:33:33,627][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:33:33,627][129146] Reward + Measures: [[2106.60788174    0.40328768    0.60161132    0.068389      0.284446  ]]
[37m[1m[2023-06-25 09:33:33,627][129146] Max Reward on eval: 2106.607881737579
[37m[1m[2023-06-25 09:33:33,628][129146] Min Reward on eval: 2106.607881737579
[37m[1m[2023-06-25 09:33:33,628][129146] Mean Reward across all agents: 2106.607881737579
[37m[1m[2023-06-25 09:33:33,628][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:33:39,048][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:33:39,048][129146] Reward + Measures: [[1776.54581943    0.38280001    0.63440001    0.0591        0.36040002]
[37m[1m [1922.4164189     0.42149997    0.59230006    0.0604        0.33330002]
[37m[1m [2019.77191054    0.38589999    0.63429999    0.0654        0.31729999]
[37m[1m ...
[37m[1m [1551.0206179     0.4535        0.69060004    0.063         0.45360002]
[37m[1m [1763.42052183    0.44080001    0.6505        0.0553        0.41009998]
[37m[1m [1818.77968895    0.38180003    0.6577        0.0684        0.33670002]]
[37m[1m[2023-06-25 09:33:39,048][129146] Max Reward on eval: 2180.498073080671
[37m[1m[2023-06-25 09:33:39,049][129146] Min Reward on eval: 1030.6769459146658
[37m[1m[2023-06-25 09:33:39,049][129146] Mean Reward across all agents: 1916.389588409646
[37m[1m[2023-06-25 09:33:39,049][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:33:39,054][129146] mean_value=318.84379249004377, max_value=1530.9183252752405
[37m[1m[2023-06-25 09:33:39,056][129146] New mean coefficients: [[ 1.3409398 -2.6717455  1.1942508 -0.2599759 -2.384015 ]]
[37m[1m[2023-06-25 09:33:39,057][129146] Moving the mean solution point...
[36m[2023-06-25 09:33:48,851][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 09:33:48,851][129146] FPS: 392169.88
[36m[2023-06-25 09:33:48,853][129146] itr=932, itrs=2000, Progress: 46.60%
[36m[2023-06-25 09:34:00,452][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 09:34:00,453][129146] FPS: 331723.63
[36m[2023-06-25 09:34:05,332][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:34:05,332][129146] Reward + Measures: [[2258.6338868     0.37215137    0.5971247     0.06052167    0.26792598]]
[37m[1m[2023-06-25 09:34:05,332][129146] Max Reward on eval: 2258.6338867963063
[37m[1m[2023-06-25 09:34:05,333][129146] Min Reward on eval: 2258.6338867963063
[37m[1m[2023-06-25 09:34:05,333][129146] Mean Reward across all agents: 2258.6338867963063
[37m[1m[2023-06-25 09:34:05,333][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:34:11,007][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:34:11,013][129146] Reward + Measures: [[1578.50206359    0.52209997    0.56370002    0.1487        0.27620003]
[37m[1m [2254.85748209    0.35619998    0.58830005    0.0584        0.26970002]
[37m[1m [2044.22517524    0.3969        0.55150002    0.08970001    0.27760002]
[37m[1m ...
[37m[1m [1811.09636007    0.44110003    0.57790005    0.11680001    0.28860003]
[37m[1m [1502.69534804    0.51489997    0.56120002    0.1558        0.27770001]
[37m[1m [1598.05366606    0.48590001    0.54139996    0.13100001    0.29969999]]
[37m[1m[2023-06-25 09:34:11,013][129146] Max Reward on eval: 2335.834321799595
[37m[1m[2023-06-25 09:34:11,013][129146] Min Reward on eval: -256.46860240999376
[37m[1m[2023-06-25 09:34:11,014][129146] Mean Reward across all agents: 1478.942983041072
[37m[1m[2023-06-25 09:34:11,014][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:34:11,017][129146] mean_value=-241.8734133032177, max_value=1097.4436756930081
[37m[1m[2023-06-25 09:34:11,020][129146] New mean coefficients: [[ 1.6227171  -1.5953822   0.6057796   0.51706815 -1.9031732 ]]
[37m[1m[2023-06-25 09:34:11,021][129146] Moving the mean solution point...
[36m[2023-06-25 09:34:20,861][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 09:34:20,861][129146] FPS: 390299.95
[36m[2023-06-25 09:34:20,864][129146] itr=933, itrs=2000, Progress: 46.65%
[36m[2023-06-25 09:34:32,263][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 09:34:32,263][129146] FPS: 337527.50
[36m[2023-06-25 09:34:36,907][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:34:36,912][129146] Reward + Measures: [[2376.98416405    0.35568097    0.5873003     0.05981367    0.25855833]]
[37m[1m[2023-06-25 09:34:36,913][129146] Max Reward on eval: 2376.9841640520426
[37m[1m[2023-06-25 09:34:36,913][129146] Min Reward on eval: 2376.9841640520426
[37m[1m[2023-06-25 09:34:36,913][129146] Mean Reward across all agents: 2376.9841640520426
[37m[1m[2023-06-25 09:34:36,913][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:34:42,304][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:34:42,305][129146] Reward + Measures: [[ 330.38023525    0.6900999     0.24419999    0.45629999    0.2174    ]
[37m[1m [ 994.75243482    0.60570002    0.50770009    0.23080002    0.27950001]
[37m[1m [ 954.07673385    0.61260003    0.53729999    0.25310001    0.28330001]
[37m[1m ...
[37m[1m [1656.94047869    0.48109999    0.50510001    0.1558        0.2221    ]
[37m[1m [2056.87609641    0.38980001    0.54500002    0.0891        0.25320002]
[37m[1m [1159.19595429    0.5851        0.46290001    0.24089999    0.19140001]]
[37m[1m[2023-06-25 09:34:42,305][129146] Max Reward on eval: 2371.5075228972128
[37m[1m[2023-06-25 09:34:42,305][129146] Min Reward on eval: -48.345782262008285
[37m[1m[2023-06-25 09:34:42,306][129146] Mean Reward across all agents: 1096.546576144523
[37m[1m[2023-06-25 09:34:42,306][129146] Average Trajectory Length: 999.6796666666667
[36m[2023-06-25 09:34:42,308][129146] mean_value=-566.5434386737035, max_value=1073.2652610121183
[37m[1m[2023-06-25 09:34:42,310][129146] New mean coefficients: [[ 1.6229652  -1.0401003   0.41723353  0.29547572 -1.3808649 ]]
[37m[1m[2023-06-25 09:34:42,311][129146] Moving the mean solution point...
[36m[2023-06-25 09:34:51,880][129146] train() took 9.57 seconds to complete
[36m[2023-06-25 09:34:51,881][129146] FPS: 401349.35
[36m[2023-06-25 09:34:51,883][129146] itr=934, itrs=2000, Progress: 46.70%
[36m[2023-06-25 09:35:03,366][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 09:35:03,367][129146] FPS: 335175.55
[36m[2023-06-25 09:35:08,169][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:35:08,170][129146] Reward + Measures: [[2539.54076463    0.344248      0.57011896    0.05470867    0.24824898]]
[37m[1m[2023-06-25 09:35:08,170][129146] Max Reward on eval: 2539.540764633911
[37m[1m[2023-06-25 09:35:08,170][129146] Min Reward on eval: 2539.540764633911
[37m[1m[2023-06-25 09:35:08,170][129146] Mean Reward across all agents: 2539.540764633911
[37m[1m[2023-06-25 09:35:08,171][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:35:13,641][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:35:13,647][129146] Reward + Measures: [[2329.78497227    0.3434        0.60420001    0.0503        0.271     ]
[37m[1m [ 903.51287523    0.46480003    0.31720001    0.25630003    0.2182    ]
[37m[1m [2100.46099974    0.375         0.53749996    0.0755        0.2518    ]
[37m[1m ...
[37m[1m [1681.83819349    0.37110001    0.58390003    0.0785        0.33190003]
[37m[1m [1862.56625995    0.43940002    0.46779999    0.12720001    0.23220001]
[37m[1m [1805.70256437    0.46849999    0.44960004    0.15510002    0.24190001]]
[37m[1m[2023-06-25 09:35:13,647][129146] Max Reward on eval: 2582.065490497183
[37m[1m[2023-06-25 09:35:13,647][129146] Min Reward on eval: 275.36150430955456
[37m[1m[2023-06-25 09:35:13,647][129146] Mean Reward across all agents: 2060.909338252128
[37m[1m[2023-06-25 09:35:13,648][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:35:13,651][129146] mean_value=-93.16697689713241, max_value=655.7533775530587
[37m[1m[2023-06-25 09:35:13,654][129146] New mean coefficients: [[ 1.5104502  -0.53355    -0.09113485  0.0880529  -0.96202624]]
[37m[1m[2023-06-25 09:35:13,655][129146] Moving the mean solution point...
[36m[2023-06-25 09:35:23,217][129146] train() took 9.56 seconds to complete
[36m[2023-06-25 09:35:23,217][129146] FPS: 401652.82
[36m[2023-06-25 09:35:23,219][129146] itr=935, itrs=2000, Progress: 46.75%
[36m[2023-06-25 09:35:34,619][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 09:35:34,619][129146] FPS: 337518.83
[36m[2023-06-25 09:35:39,453][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:35:39,459][129146] Reward + Measures: [[2669.24597824    0.33429435    0.55439734    0.05043267    0.24496099]]
[37m[1m[2023-06-25 09:35:39,459][129146] Max Reward on eval: 2669.245978238619
[37m[1m[2023-06-25 09:35:39,459][129146] Min Reward on eval: 2669.245978238619
[37m[1m[2023-06-25 09:35:39,459][129146] Mean Reward across all agents: 2669.245978238619
[37m[1m[2023-06-25 09:35:39,460][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:35:44,960][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:35:44,965][129146] Reward + Measures: [[1339.09358525    0.45499998    0.66500008    0.1549        0.45520002]
[37m[1m [1954.92486501    0.42589998    0.62659997    0.14840001    0.32409999]
[37m[1m [1384.9471019     0.4409        0.65039998    0.15280001    0.44590002]
[37m[1m ...
[37m[1m [1011.34652446    0.45860004    0.65600008    0.1918        0.47020003]
[37m[1m [1364.09882842    0.4251        0.64310002    0.1639        0.44109997]
[37m[1m [ 963.62707379    0.76599997    0.2597        0.5388        0.60260004]]
[37m[1m[2023-06-25 09:35:44,966][129146] Max Reward on eval: 2671.431417647959
[37m[1m[2023-06-25 09:35:44,966][129146] Min Reward on eval: 505.813434960926
[37m[1m[2023-06-25 09:35:44,966][129146] Mean Reward across all agents: 1805.1501067108877
[37m[1m[2023-06-25 09:35:44,967][129146] Average Trajectory Length: 999.9309999999999
[36m[2023-06-25 09:35:44,970][129146] mean_value=-42.54625111881623, max_value=1084.8292150771156
[37m[1m[2023-06-25 09:35:44,973][129146] New mean coefficients: [[ 1.7671707   0.53175735 -0.05877899  0.41847822 -0.30048466]]
[37m[1m[2023-06-25 09:35:44,974][129146] Moving the mean solution point...
[36m[2023-06-25 09:35:54,594][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 09:35:54,594][129146] FPS: 399245.60
[36m[2023-06-25 09:35:54,596][129146] itr=936, itrs=2000, Progress: 46.80%
[36m[2023-06-25 09:36:06,141][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 09:36:06,141][129146] FPS: 333362.59
[36m[2023-06-25 09:36:10,863][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:36:10,864][129146] Reward + Measures: [[2810.4547578     0.33409598    0.55394834    0.04129034    0.2445    ]]
[37m[1m[2023-06-25 09:36:10,864][129146] Max Reward on eval: 2810.4547577954972
[37m[1m[2023-06-25 09:36:10,864][129146] Min Reward on eval: 2810.4547577954972
[37m[1m[2023-06-25 09:36:10,864][129146] Mean Reward across all agents: 2810.4547577954972
[37m[1m[2023-06-25 09:36:10,864][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:36:16,305][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:36:16,306][129146] Reward + Measures: [[ 533.67298673    0.55760002    0.60339993    0.46250001    0.60980004]
[37m[1m [2749.80745392    0.35129997    0.50950003    0.0515        0.22900002]
[37m[1m [2301.24405969    0.35820001    0.583         0.076         0.2775    ]
[37m[1m ...
[37m[1m [ 983.34369702    0.63050002    0.38369998    0.47310001    0.62589997]
[37m[1m [2257.60596065    0.44970003    0.50310004    0.0825        0.2385    ]
[37m[1m [2642.0133432     0.33000001    0.5564        0.0655        0.24129999]]
[37m[1m[2023-06-25 09:36:16,306][129146] Max Reward on eval: 2819.7964476906695
[37m[1m[2023-06-25 09:36:16,306][129146] Min Reward on eval: 286.80034300910484
[37m[1m[2023-06-25 09:36:16,307][129146] Mean Reward across all agents: 1622.7305399353586
[37m[1m[2023-06-25 09:36:16,307][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:36:16,313][129146] mean_value=-3.121805061675235, max_value=1043.2746206177617
[37m[1m[2023-06-25 09:36:16,316][129146] New mean coefficients: [[ 1.8892559   1.3912003  -0.41682637  1.057314   -0.55187166]]
[37m[1m[2023-06-25 09:36:16,317][129146] Moving the mean solution point...
[36m[2023-06-25 09:36:25,976][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 09:36:25,977][129146] FPS: 397615.04
[36m[2023-06-25 09:36:25,979][129146] itr=937, itrs=2000, Progress: 46.85%
[36m[2023-06-25 09:36:37,422][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 09:36:37,422][129146] FPS: 336343.72
[36m[2023-06-25 09:36:42,322][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:36:42,323][129146] Reward + Measures: [[2939.47460017    0.34147799    0.54163164    0.03268067    0.24198601]]
[37m[1m[2023-06-25 09:36:42,323][129146] Max Reward on eval: 2939.474600172182
[37m[1m[2023-06-25 09:36:42,323][129146] Min Reward on eval: 2939.474600172182
[37m[1m[2023-06-25 09:36:42,323][129146] Mean Reward across all agents: 2939.474600172182
[37m[1m[2023-06-25 09:36:42,324][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:36:47,988][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:36:47,989][129146] Reward + Measures: [[1464.99460241    0.49600002    0.46549997    0.15480001    0.2438    ]
[37m[1m [1937.8075444     0.4894        0.46950004    0.14369999    0.24620001]
[37m[1m [1192.20686187    0.53739995    0.54300004    0.12890001    0.39140001]
[37m[1m ...
[37m[1m [ 278.42426478    0.72659999    0.21040002    0.53530002    0.1804    ]
[37m[1m [ 663.0641065     0.69499999    0.26939997    0.4797        0.23249999]
[37m[1m [1968.91862636    0.50059998    0.50310004    0.13700001    0.2378    ]]
[37m[1m[2023-06-25 09:36:47,989][129146] Max Reward on eval: 2809.9853984340325
[37m[1m[2023-06-25 09:36:47,989][129146] Min Reward on eval: -368.16364955155296
[37m[1m[2023-06-25 09:36:47,990][129146] Mean Reward across all agents: 1110.012752327558
[37m[1m[2023-06-25 09:36:47,990][129146] Average Trajectory Length: 999.6766666666666
[36m[2023-06-25 09:36:47,995][129146] mean_value=-349.63803356789236, max_value=841.8389562181662
[37m[1m[2023-06-25 09:36:47,998][129146] New mean coefficients: [[ 2.122089    2.3926678  -1.0836346   1.2441987   0.40069497]]
[37m[1m[2023-06-25 09:36:47,999][129146] Moving the mean solution point...
[36m[2023-06-25 09:36:57,774][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 09:36:57,774][129146] FPS: 392900.54
[36m[2023-06-25 09:36:57,777][129146] itr=938, itrs=2000, Progress: 46.90%
[36m[2023-06-25 09:37:09,188][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 09:37:09,188][129146] FPS: 337162.36
[36m[2023-06-25 09:37:13,968][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:37:13,968][129146] Reward + Measures: [[3080.25103704    0.36398396    0.52832264    0.02116567    0.24089333]]
[37m[1m[2023-06-25 09:37:13,969][129146] Max Reward on eval: 3080.251037044745
[37m[1m[2023-06-25 09:37:13,969][129146] Min Reward on eval: 3080.251037044745
[37m[1m[2023-06-25 09:37:13,969][129146] Mean Reward across all agents: 3080.251037044745
[37m[1m[2023-06-25 09:37:13,970][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:37:19,428][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:37:19,429][129146] Reward + Measures: [[1748.29431729    0.52009994    0.46870002    0.1595        0.2282    ]
[37m[1m [ 174.68820808    0.34400001    0.354         0.22849999    0.20720001]
[37m[1m [ 471.27080677    0.3565        0.34119996    0.2753        0.30760002]
[37m[1m ...
[37m[1m [2391.32430844    0.44949999    0.4966        0.1141        0.2951    ]
[37m[1m [2004.99138305    0.4567        0.60410005    0.1365        0.26340002]
[37m[1m [1295.48838743    0.44110003    0.41640002    0.1884        0.28220001]]
[37m[1m[2023-06-25 09:37:19,429][129146] Max Reward on eval: 3053.170885623549
[37m[1m[2023-06-25 09:37:19,429][129146] Min Reward on eval: -50.90328451441601
[37m[1m[2023-06-25 09:37:19,429][129146] Mean Reward across all agents: 1434.910711425495
[37m[1m[2023-06-25 09:37:19,430][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:37:19,433][129146] mean_value=-829.7787315448693, max_value=1580.4072544396972
[37m[1m[2023-06-25 09:37:19,436][129146] New mean coefficients: [[ 1.9473315   2.0746453  -1.0721629   0.21138477 -0.293099  ]]
[37m[1m[2023-06-25 09:37:19,437][129146] Moving the mean solution point...
[36m[2023-06-25 09:37:29,346][129146] train() took 9.91 seconds to complete
[36m[2023-06-25 09:37:29,347][129146] FPS: 387569.10
[36m[2023-06-25 09:37:29,349][129146] itr=939, itrs=2000, Progress: 46.95%
[36m[2023-06-25 09:37:40,758][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 09:37:40,758][129146] FPS: 337235.92
[36m[2023-06-25 09:37:45,575][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:37:45,576][129146] Reward + Measures: [[3183.05994756    0.38462403    0.52031964    0.015439      0.23885933]]
[37m[1m[2023-06-25 09:37:45,576][129146] Max Reward on eval: 3183.059947556996
[37m[1m[2023-06-25 09:37:45,576][129146] Min Reward on eval: 3183.059947556996
[37m[1m[2023-06-25 09:37:45,577][129146] Mean Reward across all agents: 3183.059947556996
[37m[1m[2023-06-25 09:37:45,577][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:37:51,061][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:37:51,062][129146] Reward + Measures: [[1125.37385015    0.5381        0.58030003    0.115         0.38960001]
[37m[1m [1565.47275005    0.48359999    0.54280007    0.1566        0.23050001]
[37m[1m [1703.21292385    0.49580002    0.54230005    0.1372        0.23940001]
[37m[1m ...
[37m[1m [1018.95516828    0.64200002    0.68790001    0.0574        0.6354    ]
[37m[1m [ 911.0536469     0.79770005    0.84600002    0.0134        0.82200003]
[37m[1m [-392.40377872    0.88609999    0.15650001    0.7608        0.79789996]]
[37m[1m[2023-06-25 09:37:51,062][129146] Max Reward on eval: 3060.7789577507183
[37m[1m[2023-06-25 09:37:51,062][129146] Min Reward on eval: -2305.0437647183076
[37m[1m[2023-06-25 09:37:51,063][129146] Mean Reward across all agents: 783.8922404742974
[37m[1m[2023-06-25 09:37:51,063][129146] Average Trajectory Length: 999.6949999999999
[36m[2023-06-25 09:37:51,068][129146] mean_value=-400.91736721291636, max_value=1706.6460438506056
[37m[1m[2023-06-25 09:37:51,071][129146] New mean coefficients: [[ 2.1197064   1.7281183  -0.6669297   0.11225235 -0.56821275]]
[37m[1m[2023-06-25 09:37:51,072][129146] Moving the mean solution point...
[36m[2023-06-25 09:38:00,790][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 09:38:00,791][129146] FPS: 395205.45
[36m[2023-06-25 09:38:00,793][129146] itr=940, itrs=2000, Progress: 47.00%
[37m[1m[2023-06-25 09:38:07,543][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000920
[36m[2023-06-25 09:38:19,171][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 09:38:19,171][129146] FPS: 337301.68
[36m[2023-06-25 09:38:23,926][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:38:23,927][129146] Reward + Measures: [[3246.57708231    0.39266968    0.52002263    0.01578233    0.23661999]]
[37m[1m[2023-06-25 09:38:23,927][129146] Max Reward on eval: 3246.5770823107364
[37m[1m[2023-06-25 09:38:23,927][129146] Min Reward on eval: 3246.5770823107364
[37m[1m[2023-06-25 09:38:23,927][129146] Mean Reward across all agents: 3246.5770823107364
[37m[1m[2023-06-25 09:38:23,927][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:38:29,324][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:38:29,324][129146] Reward + Measures: [[ -17.81415193    0.83640003    0.74870008    0.1558        0.72670001]
[37m[1m [2867.34105109    0.35040003    0.50810003    0.0338        0.23930001]
[37m[1m [ 992.55637248    0.48800001    0.66090006    0.0911        0.55400002]
[37m[1m ...
[37m[1m [1379.44823128    0.48639998    0.59219998    0.1331        0.36470002]
[37m[1m [2948.68252642    0.37110001    0.49879995    0.0196        0.2326    ]
[37m[1m [ 798.25706985    0.69          0.67659998    0.17719999    0.66330004]]
[37m[1m[2023-06-25 09:38:29,324][129146] Max Reward on eval: 3069.5310227219247
[37m[1m[2023-06-25 09:38:29,325][129146] Min Reward on eval: -547.7202350190142
[37m[1m[2023-06-25 09:38:29,325][129146] Mean Reward across all agents: 1031.767956188817
[37m[1m[2023-06-25 09:38:29,325][129146] Average Trajectory Length: 997.9169999999999
[36m[2023-06-25 09:38:29,329][129146] mean_value=-557.5912703845438, max_value=990.6205404048436
[37m[1m[2023-06-25 09:38:29,332][129146] New mean coefficients: [[ 2.3317428   1.8389537  -0.17590624  0.78363544 -0.58772665]]
[37m[1m[2023-06-25 09:38:29,333][129146] Moving the mean solution point...
[36m[2023-06-25 09:38:39,056][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 09:38:39,056][129146] FPS: 395000.24
[36m[2023-06-25 09:38:39,059][129146] itr=941, itrs=2000, Progress: 47.05%
[36m[2023-06-25 09:38:50,655][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 09:38:50,656][129146] FPS: 331865.23
[36m[2023-06-25 09:38:55,509][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:38:55,509][129146] Reward + Measures: [[3371.38414479    0.38828498    0.52489632    0.01172667    0.23508532]]
[37m[1m[2023-06-25 09:38:55,509][129146] Max Reward on eval: 3371.384144789881
[37m[1m[2023-06-25 09:38:55,510][129146] Min Reward on eval: 3371.384144789881
[37m[1m[2023-06-25 09:38:55,510][129146] Mean Reward across all agents: 3371.384144789881
[37m[1m[2023-06-25 09:38:55,510][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:39:01,217][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:39:01,217][129146] Reward + Measures: [[3267.6316173     0.37740001    0.53380001    0.0167        0.24070001]
[37m[1m [ 645.17839995    0.42349997    0.4513        0.19420001    0.50629997]
[37m[1m [1357.34550897    0.48590001    0.40980002    0.2139        0.22790001]
[37m[1m ...
[37m[1m [1423.13862647    0.44509998    0.49780002    0.132         0.42430001]
[37m[1m [ 675.42542214    0.44439998    0.66939998    0.1201        0.76680005]
[37m[1m [ 873.1099271     0.47589999    0.84720004    0.10600001    0.85109997]]
[37m[1m[2023-06-25 09:39:01,218][129146] Max Reward on eval: 3278.9411664073123
[37m[1m[2023-06-25 09:39:01,218][129146] Min Reward on eval: -499.2737899546657
[37m[1m[2023-06-25 09:39:01,218][129146] Mean Reward across all agents: 1342.8125396978296
[37m[1m[2023-06-25 09:39:01,218][129146] Average Trajectory Length: 989.3683333333333
[36m[2023-06-25 09:39:01,223][129146] mean_value=-634.0486375349669, max_value=2266.938637797586
[37m[1m[2023-06-25 09:39:01,226][129146] New mean coefficients: [[ 2.2381508   1.8533078   0.3560199   0.56753296 -0.3993975 ]]
[37m[1m[2023-06-25 09:39:01,227][129146] Moving the mean solution point...
[36m[2023-06-25 09:39:10,987][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 09:39:10,987][129146] FPS: 393519.38
[36m[2023-06-25 09:39:10,989][129146] itr=942, itrs=2000, Progress: 47.10%
[36m[2023-06-25 09:39:22,480][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 09:39:22,481][129146] FPS: 334817.11
[36m[2023-06-25 09:39:27,271][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:39:27,272][129146] Reward + Measures: [[3484.11357516    0.388024      0.5250237     0.01147       0.23353399]]
[37m[1m[2023-06-25 09:39:27,272][129146] Max Reward on eval: 3484.113575164604
[37m[1m[2023-06-25 09:39:27,272][129146] Min Reward on eval: 3484.113575164604
[37m[1m[2023-06-25 09:39:27,272][129146] Mean Reward across all agents: 3484.113575164604
[37m[1m[2023-06-25 09:39:27,273][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:39:32,701][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:39:32,707][129146] Reward + Measures: [[1082.98519735    0.56680006    0.66260004    0.29670003    0.44140002]
[37m[1m [2036.89765102    0.52840006    0.4743        0.1569        0.234     ]
[37m[1m [1098.33203454    0.39069998    0.83999997    0.1435        0.74510002]
[37m[1m ...
[37m[1m [1420.95767041    0.49379998    0.47749996    0.22669999    0.24080001]
[37m[1m [1091.05882551    0.56229997    0.45280001    0.24699998    0.55870003]
[37m[1m [ 688.97746289    0.528         0.50980002    0.23639999    0.5851    ]]
[37m[1m[2023-06-25 09:39:32,707][129146] Max Reward on eval: 3340.9391197524733
[37m[1m[2023-06-25 09:39:32,707][129146] Min Reward on eval: -333.58263034217526
[37m[1m[2023-06-25 09:39:32,708][129146] Mean Reward across all agents: 1097.5854866424404
[37m[1m[2023-06-25 09:39:32,708][129146] Average Trajectory Length: 999.616
[36m[2023-06-25 09:39:32,712][129146] mean_value=-575.2558046313997, max_value=3215.254309479077
[37m[1m[2023-06-25 09:39:32,715][129146] New mean coefficients: [[2.3210118  1.6393598  0.7239406  1.0527995  0.21665984]]
[37m[1m[2023-06-25 09:39:32,716][129146] Moving the mean solution point...
[36m[2023-06-25 09:39:42,364][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 09:39:42,364][129146] FPS: 398086.73
[36m[2023-06-25 09:39:42,366][129146] itr=943, itrs=2000, Progress: 47.15%
[36m[2023-06-25 09:39:53,727][129146] train() took 11.34 seconds to complete
[36m[2023-06-25 09:39:53,727][129146] FPS: 338672.25
[36m[2023-06-25 09:39:58,536][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:39:58,537][129146] Reward + Measures: [[3567.67168943    0.38554037    0.53035963    0.00984833    0.233078  ]]
[37m[1m[2023-06-25 09:39:58,537][129146] Max Reward on eval: 3567.671689434336
[37m[1m[2023-06-25 09:39:58,537][129146] Min Reward on eval: 3567.671689434336
[37m[1m[2023-06-25 09:39:58,537][129146] Mean Reward across all agents: 3567.671689434336
[37m[1m[2023-06-25 09:39:58,537][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:40:04,036][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:40:04,037][129146] Reward + Measures: [[ 670.95134429    0.43884087    0.30584159    0.2066927     0.21872263]
[37m[1m [1851.87908881    0.54350001    0.56260008    0.13759999    0.2102    ]
[37m[1m [-447.76246551    0.33595726    0.35345653    0.2034512     0.13164251]
[37m[1m ...
[37m[1m [ 934.87432122    0.55380005    0.32609999    0.2922        0.2352    ]
[37m[1m [ 102.74501877    0.53170002    0.1964        0.34300002    0.26440001]
[37m[1m [-154.73207338    0.35550001    0.34080002    0.20390001    0.2321    ]]
[37m[1m[2023-06-25 09:40:04,037][129146] Max Reward on eval: 3177.2374518989586
[37m[1m[2023-06-25 09:40:04,037][129146] Min Reward on eval: -662.0092054944951
[37m[1m[2023-06-25 09:40:04,038][129146] Mean Reward across all agents: 761.4516403413954
[37m[1m[2023-06-25 09:40:04,038][129146] Average Trajectory Length: 973.1129999999999
[36m[2023-06-25 09:40:04,040][129146] mean_value=-1177.4671120512232, max_value=1898.6313899690922
[37m[1m[2023-06-25 09:40:04,043][129146] New mean coefficients: [[2.1015892  1.3820157  0.45347098 0.26705468 0.23095134]]
[37m[1m[2023-06-25 09:40:04,044][129146] Moving the mean solution point...
[36m[2023-06-25 09:40:13,694][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 09:40:13,695][129146] FPS: 397971.06
[36m[2023-06-25 09:40:13,697][129146] itr=944, itrs=2000, Progress: 47.20%
[36m[2023-06-25 09:40:25,214][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 09:40:25,214][129146] FPS: 334105.47
[36m[2023-06-25 09:40:30,002][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:40:30,002][129146] Reward + Measures: [[3695.3748496     0.38940766    0.52219635    0.00948333    0.23080066]]
[37m[1m[2023-06-25 09:40:30,003][129146] Max Reward on eval: 3695.374849596735
[37m[1m[2023-06-25 09:40:30,003][129146] Min Reward on eval: 3695.374849596735
[37m[1m[2023-06-25 09:40:30,003][129146] Mean Reward across all agents: 3695.374849596735
[37m[1m[2023-06-25 09:40:30,003][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:40:35,519][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:40:35,520][129146] Reward + Measures: [[ 951.02031252    0.55479997    0.49370003    0.23290001    0.28290001]
[37m[1m [ 984.50386344    0.48269996    0.39579999    0.227         0.24430001]
[37m[1m [1102.48136344    0.5323        0.5966        0.12060001    0.52609998]
[37m[1m ...
[37m[1m [1259.74722447    0.51810002    0.46669999    0.18900001    0.3689    ]
[37m[1m [1518.11231418    0.45640001    0.43700001    0.1911        0.2313    ]
[37m[1m [ 905.03206694    0.55610001    0.414         0.31479999    0.29229999]]
[37m[1m[2023-06-25 09:40:35,520][129146] Max Reward on eval: 3588.8088990476913
[37m[1m[2023-06-25 09:40:35,520][129146] Min Reward on eval: -251.6291664425633
[37m[1m[2023-06-25 09:40:35,520][129146] Mean Reward across all agents: 1550.5780971282438
[37m[1m[2023-06-25 09:40:35,521][129146] Average Trajectory Length: 998.8513333333333
[36m[2023-06-25 09:40:35,524][129146] mean_value=-815.0613816985131, max_value=2654.543408583105
[37m[1m[2023-06-25 09:40:35,526][129146] New mean coefficients: [[ 2.0946727   2.0106597  -0.11162218  0.22560813 -0.20313957]]
[37m[1m[2023-06-25 09:40:35,527][129146] Moving the mean solution point...
[36m[2023-06-25 09:40:45,194][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 09:40:45,194][129146] FPS: 397329.39
[36m[2023-06-25 09:40:45,196][129146] itr=945, itrs=2000, Progress: 47.25%
[36m[2023-06-25 09:40:56,761][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 09:40:56,762][129146] FPS: 332700.49
[36m[2023-06-25 09:41:01,566][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:41:01,566][129146] Reward + Measures: [[3716.22771674    0.395008      0.52157599    0.01136367    0.23457867]]
[37m[1m[2023-06-25 09:41:01,566][129146] Max Reward on eval: 3716.227716744269
[37m[1m[2023-06-25 09:41:01,566][129146] Min Reward on eval: 3716.227716744269
[37m[1m[2023-06-25 09:41:01,567][129146] Mean Reward across all agents: 3716.227716744269
[37m[1m[2023-06-25 09:41:01,567][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:41:07,114][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:41:07,115][129146] Reward + Measures: [[  25.38147259    0.6257        0.14930001    0.55460006    0.46490002]
[37m[1m [  38.0654503     0.72570002    0.1115        0.6886        0.477     ]
[37m[1m [1192.04434914    0.46820003    0.43459997    0.18889999    0.20550001]
[37m[1m ...
[37m[1m [2318.36267468    0.45480004    0.53369999    0.06390001    0.28299999]
[37m[1m [2049.0682687     0.4377        0.45679998    0.08280001    0.27380002]
[37m[1m [ 620.21064613    0.57550001    0.38569999    0.36610001    0.28      ]]
[37m[1m[2023-06-25 09:41:07,115][129146] Max Reward on eval: 3192.8477188868446
[37m[1m[2023-06-25 09:41:07,116][129146] Min Reward on eval: -226.34934318720934
[37m[1m[2023-06-25 09:41:07,116][129146] Mean Reward across all agents: 991.8279001653158
[37m[1m[2023-06-25 09:41:07,116][129146] Average Trajectory Length: 997.0566666666666
[36m[2023-06-25 09:41:07,118][129146] mean_value=-803.9014628999623, max_value=2729.1289998387965
[37m[1m[2023-06-25 09:41:07,121][129146] New mean coefficients: [[ 1.9196573   2.5387769  -0.02769054  0.46964085 -0.01499054]]
[37m[1m[2023-06-25 09:41:07,122][129146] Moving the mean solution point...
[36m[2023-06-25 09:41:16,782][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 09:41:16,782][129146] FPS: 397587.72
[36m[2023-06-25 09:41:16,784][129146] itr=946, itrs=2000, Progress: 47.30%
[36m[2023-06-25 09:41:28,204][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 09:41:28,204][129146] FPS: 336958.89
[36m[2023-06-25 09:41:32,954][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:41:32,960][129146] Reward + Measures: [[3768.30922142    0.42895034    0.50885338    0.01630133    0.22927366]]
[37m[1m[2023-06-25 09:41:32,960][129146] Max Reward on eval: 3768.309221423556
[37m[1m[2023-06-25 09:41:32,960][129146] Min Reward on eval: 3768.309221423556
[37m[1m[2023-06-25 09:41:32,961][129146] Mean Reward across all agents: 3768.309221423556
[37m[1m[2023-06-25 09:41:32,961][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:41:38,654][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:41:38,660][129146] Reward + Measures: [[ -36.7230794     0.52070004    0.21970001    0.50389999    0.29730001]
[37m[1m [1198.80244948    0.54549998    0.67629999    0.175         0.49140006]
[37m[1m [2367.86478697    0.59130001    0.41670004    0.21659997    0.30109999]
[37m[1m ...
[37m[1m [1306.62783498    0.44300005    0.33660004    0.178         0.18640001]
[37m[1m [ 416.37150029    0.41014799    0.278182      0.32243583    0.31676847]
[37m[1m [  48.07951893    0.31878856    0.25494963    0.28158092    0.23981069]]
[37m[1m[2023-06-25 09:41:38,660][129146] Max Reward on eval: 3434.455845470773
[37m[1m[2023-06-25 09:41:38,661][129146] Min Reward on eval: -504.4016864110599
[37m[1m[2023-06-25 09:41:38,661][129146] Mean Reward across all agents: 1001.7775670427923
[37m[1m[2023-06-25 09:41:38,661][129146] Average Trajectory Length: 990.2226666666667
[36m[2023-06-25 09:41:38,665][129146] mean_value=-733.6275923944503, max_value=1000.2021966025263
[37m[1m[2023-06-25 09:41:38,668][129146] New mean coefficients: [[ 1.6789215   1.8255286   0.17652845 -0.07529461  0.05925231]]
[37m[1m[2023-06-25 09:41:38,669][129146] Moving the mean solution point...
[36m[2023-06-25 09:41:48,550][129146] train() took 9.88 seconds to complete
[36m[2023-06-25 09:41:48,550][129146] FPS: 388679.33
[36m[2023-06-25 09:41:48,553][129146] itr=947, itrs=2000, Progress: 47.35%
[36m[2023-06-25 09:42:00,214][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 09:42:00,214][129146] FPS: 330042.74
[36m[2023-06-25 09:42:05,080][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:42:05,081][129146] Reward + Measures: [[3894.28115835    0.41133535    0.50649363    0.009536      0.23198099]]
[37m[1m[2023-06-25 09:42:05,081][129146] Max Reward on eval: 3894.281158351023
[37m[1m[2023-06-25 09:42:05,081][129146] Min Reward on eval: 3894.281158351023
[37m[1m[2023-06-25 09:42:05,082][129146] Mean Reward across all agents: 3894.281158351023
[37m[1m[2023-06-25 09:42:05,082][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:42:10,617][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:42:10,618][129146] Reward + Measures: [[ 248.65724138    0.41740003    0.22350001    0.0433        0.43439999]
[37m[1m [3080.05065415    0.40309998    0.51600003    0.0658        0.2198    ]
[37m[1m [ 786.74536325    0.49219999    0.67160004    0.0281        0.63069999]
[37m[1m ...
[37m[1m [ 661.01486252    0.59980005    0.34169999    0.29350001    0.37339997]
[37m[1m [ 915.85873261    0.62799996    0.74500006    0.0189        0.65720004]
[37m[1m [1639.64537418    0.44930002    0.56890005    0.0535        0.36880001]]
[37m[1m[2023-06-25 09:42:10,618][129146] Max Reward on eval: 3728.4722884202843
[37m[1m[2023-06-25 09:42:10,618][129146] Min Reward on eval: -427.4691391366534
[37m[1m[2023-06-25 09:42:10,619][129146] Mean Reward across all agents: 1572.0577293811266
[37m[1m[2023-06-25 09:42:10,619][129146] Average Trajectory Length: 998.1893333333333
[36m[2023-06-25 09:42:10,623][129146] mean_value=-568.1994055086301, max_value=1988.3233054822915
[37m[1m[2023-06-25 09:42:10,625][129146] New mean coefficients: [[ 1.5726639   2.2718408   0.03156978 -0.0511829  -0.12834217]]
[37m[1m[2023-06-25 09:42:10,626][129146] Moving the mean solution point...
[36m[2023-06-25 09:42:20,465][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 09:42:20,466][129146] FPS: 390344.06
[36m[2023-06-25 09:42:20,468][129146] itr=948, itrs=2000, Progress: 47.40%
[36m[2023-06-25 09:42:31,921][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 09:42:31,921][129146] FPS: 336049.84
[36m[2023-06-25 09:42:36,648][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:42:36,649][129146] Reward + Measures: [[2479.18659429    0.45291996    0.46988541    0.08962002    0.29063028]]
[37m[1m[2023-06-25 09:42:36,649][129146] Max Reward on eval: 2479.186594288755
[37m[1m[2023-06-25 09:42:36,649][129146] Min Reward on eval: 2479.186594288755
[37m[1m[2023-06-25 09:42:36,649][129146] Mean Reward across all agents: 2479.186594288755
[37m[1m[2023-06-25 09:42:36,649][129146] Average Trajectory Length: 989.7946666666667
[36m[2023-06-25 09:42:42,054][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:42:42,060][129146] Reward + Measures: [[ 149.50901004    0.43450004    0.25479999    0.34900004    0.2263    ]
[37m[1m [1359.58424289    0.50800002    0.3955        0.20580001    0.25530002]
[37m[1m [ 577.50460213    0.49959999    0.28420001    0.36390004    0.24489999]
[37m[1m ...
[37m[1m [ 285.83409419    0.52380002    0.38590002    0.31740001    0.3865    ]
[37m[1m [ 784.39325783    0.65069997    0.32290003    0.30930001    0.24590002]
[37m[1m [ 612.00392559    0.53280002    0.2897        0.3671        0.24829999]]
[37m[1m[2023-06-25 09:42:42,060][129146] Max Reward on eval: 2501.957318390254
[37m[1m[2023-06-25 09:42:42,061][129146] Min Reward on eval: -1274.0392863905522
[37m[1m[2023-06-25 09:42:42,061][129146] Mean Reward across all agents: 487.67852539615774
[37m[1m[2023-06-25 09:42:42,061][129146] Average Trajectory Length: 989.9653333333333
[36m[2023-06-25 09:42:42,063][129146] mean_value=-1187.9283509959478, max_value=1049.5350941162174
[37m[1m[2023-06-25 09:42:42,066][129146] New mean coefficients: [[ 1.7715403   0.7706977   0.23515704 -0.23124625 -0.01802737]]
[37m[1m[2023-06-25 09:42:42,067][129146] Moving the mean solution point...
[36m[2023-06-25 09:42:51,940][129146] train() took 9.87 seconds to complete
[36m[2023-06-25 09:42:51,940][129146] FPS: 389000.31
[36m[2023-06-25 09:42:51,942][129146] itr=949, itrs=2000, Progress: 47.45%
[36m[2023-06-25 09:43:03,576][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 09:43:03,577][129146] FPS: 330712.31
[36m[2023-06-25 09:43:08,432][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:43:08,432][129146] Reward + Measures: [[2826.54495963    0.46949443    0.4605118     0.08421831    0.24496281]]
[37m[1m[2023-06-25 09:43:08,432][129146] Max Reward on eval: 2826.5449596318927
[37m[1m[2023-06-25 09:43:08,433][129146] Min Reward on eval: 2826.5449596318927
[37m[1m[2023-06-25 09:43:08,433][129146] Mean Reward across all agents: 2826.5449596318927
[37m[1m[2023-06-25 09:43:08,433][129146] Average Trajectory Length: 992.5126666666666
[36m[2023-06-25 09:43:13,975][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:43:13,976][129146] Reward + Measures: [[1388.76070062    0.41170001    0.50340003    0.17550001    0.2033    ]
[37m[1m [ 358.87321419    0.42630002    0.3276        0.2999        0.2685    ]
[37m[1m [ 829.93922719    0.36589998    0.46160004    0.0993        0.34460002]
[37m[1m ...
[37m[1m [1969.14315743    0.44220001    0.53509998    0.1688        0.3019    ]
[37m[1m [1867.58280436    0.43189999    0.52579999    0.1701        0.22309999]
[37m[1m [ 466.74253601    0.62270004    0.54140002    0.47209999    0.57749999]]
[37m[1m[2023-06-25 09:43:13,976][129146] Max Reward on eval: 2872.252780616493
[37m[1m[2023-06-25 09:43:13,976][129146] Min Reward on eval: -895.180325496872
[37m[1m[2023-06-25 09:43:13,976][129146] Mean Reward across all agents: 890.981933410125
[37m[1m[2023-06-25 09:43:13,976][129146] Average Trajectory Length: 983.669
[36m[2023-06-25 09:43:13,979][129146] mean_value=-1000.458081091862, max_value=724.794800267332
[37m[1m[2023-06-25 09:43:13,982][129146] New mean coefficients: [[ 1.7530047  -0.409382    0.4223006  -0.32022753 -0.68349576]]
[37m[1m[2023-06-25 09:43:13,983][129146] Moving the mean solution point...
[36m[2023-06-25 09:43:23,755][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 09:43:23,755][129146] FPS: 393033.24
[36m[2023-06-25 09:43:23,757][129146] itr=950, itrs=2000, Progress: 47.50%
[37m[1m[2023-06-25 09:43:30,466][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000930
[36m[2023-06-25 09:43:42,094][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 09:43:42,094][129146] FPS: 336928.49
[36m[2023-06-25 09:43:46,891][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:43:46,896][129146] Reward + Measures: [[2780.68983385    0.3914856     0.46367767    0.10058854    0.22101398]]
[37m[1m[2023-06-25 09:43:46,896][129146] Max Reward on eval: 2780.6898338469537
[37m[1m[2023-06-25 09:43:46,897][129146] Min Reward on eval: 2780.6898338469537
[37m[1m[2023-06-25 09:43:46,897][129146] Mean Reward across all agents: 2780.6898338469537
[37m[1m[2023-06-25 09:43:46,897][129146] Average Trajectory Length: 998.5333333333333
[36m[2023-06-25 09:43:52,609][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:43:52,615][129146] Reward + Measures: [[-142.23875712    0.52330005    0.2597        0.25430003    0.24330001]
[37m[1m [ 121.34857709    0.3529        0.20180002    0.27270001    0.23940001]
[37m[1m [-132.45008365    0.51529998    0.2904        0.32909998    0.25190002]
[37m[1m ...
[37m[1m [1400.76781849    0.59450001    0.43730003    0.29170004    0.2773    ]
[37m[1m [ 376.43097543    0.26676878    0.2052846     0.0927019     0.12887502]
[37m[1m [ 257.25856902    0.53640002    0.31920001    0.26040003    0.24770001]]
[37m[1m[2023-06-25 09:43:52,615][129146] Max Reward on eval: 2584.524593172688
[37m[1m[2023-06-25 09:43:52,616][129146] Min Reward on eval: -1386.3525261674367
[37m[1m[2023-06-25 09:43:52,616][129146] Mean Reward across all agents: 594.611141296119
[37m[1m[2023-06-25 09:43:52,616][129146] Average Trajectory Length: 993.2223333333333
[36m[2023-06-25 09:43:52,618][129146] mean_value=-1154.5849061136605, max_value=898.4070519054624
[37m[1m[2023-06-25 09:43:52,621][129146] New mean coefficients: [[ 1.9664422  -0.51060325  0.2520281  -0.25224984 -0.7096752 ]]
[37m[1m[2023-06-25 09:43:52,622][129146] Moving the mean solution point...
[36m[2023-06-25 09:44:02,355][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 09:44:02,356][129146] FPS: 394586.95
[36m[2023-06-25 09:44:02,358][129146] itr=951, itrs=2000, Progress: 47.55%
[36m[2023-06-25 09:44:13,900][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 09:44:13,901][129146] FPS: 333396.74
[36m[2023-06-25 09:44:18,722][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:44:18,722][129146] Reward + Measures: [[3053.54645358    0.38039625    0.47663304    0.08125829    0.22281364]]
[37m[1m[2023-06-25 09:44:18,723][129146] Max Reward on eval: 3053.546453576686
[37m[1m[2023-06-25 09:44:18,723][129146] Min Reward on eval: 3053.546453576686
[37m[1m[2023-06-25 09:44:18,723][129146] Mean Reward across all agents: 3053.546453576686
[37m[1m[2023-06-25 09:44:18,723][129146] Average Trajectory Length: 998.7316666666667
[36m[2023-06-25 09:44:24,191][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:44:24,192][129146] Reward + Measures: [[1483.449165      0.47359997    0.52689999    0.1225        0.27779999]
[37m[1m [-105.00014505    0.41219997    0.3211        0.2436        0.21070002]
[37m[1m [1443.0354255     0.37144551    0.32074091    0.13758461    0.22800611]
[37m[1m ...
[37m[1m [-369.39605241    0.2606        0.19849999    0.11260001    0.14049999]
[37m[1m [1588.75829137    0.46230003    0.53310007    0.11870001    0.26419997]
[37m[1m [-732.20988088    0.2563757     0.21172149    0.19040468    0.1639159 ]]
[37m[1m[2023-06-25 09:44:24,192][129146] Max Reward on eval: 2870.784061423829
[37m[1m[2023-06-25 09:44:24,192][129146] Min Reward on eval: -732.2098808819807
[37m[1m[2023-06-25 09:44:24,192][129146] Mean Reward across all agents: 866.9828182580488
[37m[1m[2023-06-25 09:44:24,193][129146] Average Trajectory Length: 991.6093333333333
[36m[2023-06-25 09:44:24,195][129146] mean_value=-1150.6238586493598, max_value=470.29467615996526
[37m[1m[2023-06-25 09:44:24,197][129146] New mean coefficients: [[ 2.0999281  -0.30766225  0.31035003 -0.12626185 -0.50456095]]
[37m[1m[2023-06-25 09:44:24,198][129146] Moving the mean solution point...
[36m[2023-06-25 09:44:33,932][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 09:44:33,933][129146] FPS: 394531.75
[36m[2023-06-25 09:44:33,935][129146] itr=952, itrs=2000, Progress: 47.60%
[36m[2023-06-25 09:44:45,670][129146] train() took 11.71 seconds to complete
[36m[2023-06-25 09:44:45,670][129146] FPS: 327899.60
[36m[2023-06-25 09:44:50,491][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:44:50,491][129146] Reward + Measures: [[3284.7384002     0.36868018    0.48102999    0.06179538    0.22877331]]
[37m[1m[2023-06-25 09:44:50,491][129146] Max Reward on eval: 3284.73840020113
[37m[1m[2023-06-25 09:44:50,492][129146] Min Reward on eval: 3284.73840020113
[37m[1m[2023-06-25 09:44:50,492][129146] Mean Reward across all agents: 3284.73840020113
[37m[1m[2023-06-25 09:44:50,492][129146] Average Trajectory Length: 999.838
[36m[2023-06-25 09:44:55,919][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:44:55,920][129146] Reward + Measures: [[1083.30846666    0.65539998    0.41290003    0.30320001    0.40689999]
[37m[1m [1102.34297399    0.29679999    0.34920001    0.2088        0.23959999]
[37m[1m [1267.94672701    0.30149999    0.3396        0.07650001    0.24460001]
[37m[1m ...
[37m[1m [2341.39610365    0.33190003    0.45360002    0.1552        0.20900002]
[37m[1m [2153.28279091    0.38322836    0.35890746    0.14263134    0.2302821 ]
[37m[1m [ 889.67856572    0.42752299    0.36431831    0.27957377    0.233559  ]]
[37m[1m[2023-06-25 09:44:55,920][129146] Max Reward on eval: 3100.1798338458875
[37m[1m[2023-06-25 09:44:55,920][129146] Min Reward on eval: -741.8958446070435
[37m[1m[2023-06-25 09:44:55,920][129146] Mean Reward across all agents: 1112.0191325123405
[37m[1m[2023-06-25 09:44:55,920][129146] Average Trajectory Length: 991.1003333333333
[36m[2023-06-25 09:44:55,922][129146] mean_value=-1297.2489975458177, max_value=1384.9428308845218
[37m[1m[2023-06-25 09:44:55,925][129146] New mean coefficients: [[ 2.1415966   0.61428285  0.4938652   0.18974613 -0.4461501 ]]
[37m[1m[2023-06-25 09:44:55,926][129146] Moving the mean solution point...
[36m[2023-06-25 09:45:05,739][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 09:45:05,739][129146] FPS: 391369.42
[36m[2023-06-25 09:45:05,741][129146] itr=953, itrs=2000, Progress: 47.65%
[36m[2023-06-25 09:45:17,425][129146] train() took 11.66 seconds to complete
[36m[2023-06-25 09:45:17,425][129146] FPS: 329347.33
[36m[2023-06-25 09:45:22,125][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:45:22,126][129146] Reward + Measures: [[3524.20490325    0.37175781    0.49109375    0.03935956    0.22970076]]
[37m[1m[2023-06-25 09:45:22,126][129146] Max Reward on eval: 3524.2049032507475
[37m[1m[2023-06-25 09:45:22,126][129146] Min Reward on eval: 3524.2049032507475
[37m[1m[2023-06-25 09:45:22,126][129146] Mean Reward across all agents: 3524.2049032507475
[37m[1m[2023-06-25 09:45:22,126][129146] Average Trajectory Length: 999.283
[36m[2023-06-25 09:45:27,565][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:45:27,566][129146] Reward + Measures: [[ 406.89918672    0.43859997    0.23599999    0.28150001    0.2789    ]
[37m[1m [ 630.29892337    0.54119998    0.29790002    0.26230001    0.30689999]
[37m[1m [ 599.9128568     0.37694305    0.25332269    0.2013853     0.23786925]
[37m[1m ...
[37m[1m [ 430.19980054    0.51590002    0.41739997    0.31399998    0.31940004]
[37m[1m [1000.63807197    0.41170001    0.48280001    0.1186        0.48310003]
[37m[1m [ 498.53223768    0.65479994    0.2465        0.54979998    0.2667    ]]
[37m[1m[2023-06-25 09:45:27,566][129146] Max Reward on eval: 2995.0733932589646
[37m[1m[2023-06-25 09:45:27,566][129146] Min Reward on eval: -940.7327112144791
[37m[1m[2023-06-25 09:45:27,567][129146] Mean Reward across all agents: 718.8948211098445
[37m[1m[2023-06-25 09:45:27,567][129146] Average Trajectory Length: 993.1553333333333
[36m[2023-06-25 09:45:27,569][129146] mean_value=-1419.5316043335472, max_value=801.4046874737745
[37m[1m[2023-06-25 09:45:27,571][129146] New mean coefficients: [[ 2.227683    0.20826775  0.16215429  0.14621578 -0.45069113]]
[37m[1m[2023-06-25 09:45:27,572][129146] Moving the mean solution point...
[36m[2023-06-25 09:45:37,210][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 09:45:37,211][129146] FPS: 398482.95
[36m[2023-06-25 09:45:37,213][129146] itr=954, itrs=2000, Progress: 47.70%
[36m[2023-06-25 09:45:48,870][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 09:45:48,870][129146] FPS: 330085.82
[36m[2023-06-25 09:45:53,488][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:45:53,489][129146] Reward + Measures: [[3708.83428464    0.37109897    0.48912826    0.02757343    0.23226908]]
[37m[1m[2023-06-25 09:45:53,489][129146] Max Reward on eval: 3708.834284639151
[37m[1m[2023-06-25 09:45:53,489][129146] Min Reward on eval: 3708.834284639151
[37m[1m[2023-06-25 09:45:53,489][129146] Mean Reward across all agents: 3708.834284639151
[37m[1m[2023-06-25 09:45:53,490][129146] Average Trajectory Length: 999.573
[36m[2023-06-25 09:45:58,925][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:45:58,926][129146] Reward + Measures: [[ 643.84335289    0.5072        0.24529998    0.36939999    0.25429997]
[37m[1m [-312.26951       0.5521        0.0745        0.52740002    0.419     ]
[37m[1m [ 356.25475714    0.3556        0.32539997    0.14770001    0.2139    ]
[37m[1m ...
[37m[1m [1290.25603337    0.36760002    0.45810005    0.11980001    0.21589999]
[37m[1m [2336.57224716    0.3319        0.48020002    0.1239        0.21039999]
[37m[1m [2048.95617695    0.29440001    0.38280001    0.1067        0.30289999]]
[37m[1m[2023-06-25 09:45:58,926][129146] Max Reward on eval: 3292.1710165581667
[37m[1m[2023-06-25 09:45:58,926][129146] Min Reward on eval: -598.871014469117
[37m[1m[2023-06-25 09:45:58,926][129146] Mean Reward across all agents: 948.4679461003881
[37m[1m[2023-06-25 09:45:58,927][129146] Average Trajectory Length: 987.9459999999999
[36m[2023-06-25 09:45:58,928][129146] mean_value=-1324.066434184688, max_value=623.8092793523807
[37m[1m[2023-06-25 09:45:58,931][129146] New mean coefficients: [[ 2.1475596   0.1504778   0.11015075 -0.14770369  0.09994677]]
[37m[1m[2023-06-25 09:45:58,932][129146] Moving the mean solution point...
[36m[2023-06-25 09:46:08,560][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 09:46:08,560][129146] FPS: 398905.28
[36m[2023-06-25 09:46:08,562][129146] itr=955, itrs=2000, Progress: 47.75%
[36m[2023-06-25 09:46:20,018][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 09:46:20,018][129146] FPS: 335891.26
[36m[2023-06-25 09:46:24,778][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:46:24,778][129146] Reward + Measures: [[3838.96950051    0.36851501    0.47945932    0.01870133    0.23046835]]
[37m[1m[2023-06-25 09:46:24,778][129146] Max Reward on eval: 3838.9695005102126
[37m[1m[2023-06-25 09:46:24,778][129146] Min Reward on eval: 3838.9695005102126
[37m[1m[2023-06-25 09:46:24,779][129146] Mean Reward across all agents: 3838.9695005102126
[37m[1m[2023-06-25 09:46:24,779][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:46:30,342][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:46:30,343][129146] Reward + Measures: [[2253.34295715    0.34343943    0.44173941    0.07460606    0.28209093]
[37m[1m [ 999.99495082    0.55050004    0.36210001    0.2264        0.24609999]
[37m[1m [2028.77498289    0.46619996    0.33700001    0.14129999    0.25149998]
[37m[1m ...
[37m[1m [2081.27432133    0.50980008    0.37890002    0.15460001    0.24340001]
[37m[1m [1015.15086235    0.51529998    0.34020001    0.22920001    0.27770001]
[37m[1m [ 166.37133529    0.3759        0.211         0.31440002    0.1957    ]]
[37m[1m[2023-06-25 09:46:30,343][129146] Max Reward on eval: 3713.1660377033054
[37m[1m[2023-06-25 09:46:30,344][129146] Min Reward on eval: -841.1454455030965
[37m[1m[2023-06-25 09:46:30,344][129146] Mean Reward across all agents: 960.5131187876865
[37m[1m[2023-06-25 09:46:30,344][129146] Average Trajectory Length: 986.3133333333333
[36m[2023-06-25 09:46:30,346][129146] mean_value=-1127.9075587997033, max_value=1141.3958223053382
[37m[1m[2023-06-25 09:46:30,348][129146] New mean coefficients: [[ 2.2289388   0.5570663   0.12687144 -0.09138919 -0.15578881]]
[37m[1m[2023-06-25 09:46:30,349][129146] Moving the mean solution point...
[36m[2023-06-25 09:46:40,135][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 09:46:40,135][129146] FPS: 392467.42
[36m[2023-06-25 09:46:40,138][129146] itr=956, itrs=2000, Progress: 47.80%
[36m[2023-06-25 09:46:51,579][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 09:46:51,579][129146] FPS: 336270.88
[36m[2023-06-25 09:46:56,450][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:46:56,451][129146] Reward + Measures: [[1933.21813496    0.41237661    0.37026876    0.0853975     0.26386085]]
[37m[1m[2023-06-25 09:46:56,451][129146] Max Reward on eval: 1933.2181349604236
[37m[1m[2023-06-25 09:46:56,451][129146] Min Reward on eval: 1933.2181349604236
[37m[1m[2023-06-25 09:46:56,452][129146] Mean Reward across all agents: 1933.2181349604236
[37m[1m[2023-06-25 09:46:56,452][129146] Average Trajectory Length: 999.6603333333333
[36m[2023-06-25 09:47:01,987][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:47:01,987][129146] Reward + Measures: [[ 787.38092614    0.29520002    0.35800001    0.20560001    0.22540002]
[37m[1m [ 780.24334267    0.46699998    0.62210006    0.1948        0.43990001]
[37m[1m [ 367.6219075     0.38890001    0.2502        0.23099999    0.25419998]
[37m[1m ...
[37m[1m [ 904.93169629    0.54269999    0.48950005    0.19269998    0.32249996]
[37m[1m [1162.79193574    0.31480002    0.44589996    0.17609999    0.2651    ]
[37m[1m [ 843.32918757    0.66820002    0.3802        0.20039999    0.2383    ]]
[37m[1m[2023-06-25 09:47:01,987][129146] Max Reward on eval: 2386.6228756729747
[37m[1m[2023-06-25 09:47:01,988][129146] Min Reward on eval: -380.9186582921946
[37m[1m[2023-06-25 09:47:01,988][129146] Mean Reward across all agents: 1060.1182624086268
[37m[1m[2023-06-25 09:47:01,988][129146] Average Trajectory Length: 993.3886666666666
[36m[2023-06-25 09:47:01,990][129146] mean_value=-1314.941555144833, max_value=869.7821175605794
[37m[1m[2023-06-25 09:47:01,992][129146] New mean coefficients: [[ 2.2425945   0.48894823  0.2073979  -0.43664926  0.09152603]]
[37m[1m[2023-06-25 09:47:01,993][129146] Moving the mean solution point...
[36m[2023-06-25 09:47:11,739][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 09:47:11,739][129146] FPS: 394087.75
[36m[2023-06-25 09:47:11,741][129146] itr=957, itrs=2000, Progress: 47.85%
[36m[2023-06-25 09:47:23,373][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 09:47:23,373][129146] FPS: 330878.13
[36m[2023-06-25 09:47:28,262][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:47:28,262][129146] Reward + Measures: [[2107.68676865    0.41048965    0.37202764    0.08079133    0.260952  ]]
[37m[1m[2023-06-25 09:47:28,263][129146] Max Reward on eval: 2107.6867686482096
[37m[1m[2023-06-25 09:47:28,263][129146] Min Reward on eval: 2107.6867686482096
[37m[1m[2023-06-25 09:47:28,263][129146] Mean Reward across all agents: 2107.6867686482096
[37m[1m[2023-06-25 09:47:28,263][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:47:33,824][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:47:33,825][129146] Reward + Measures: [[1570.07959455    0.32080004    0.36269999    0.1489        0.25429997]
[37m[1m [ -43.75454178    0.36260003    0.26460001    0.1498        0.2132    ]
[37m[1m [ 544.58577763    0.35080001    0.31670001    0.0722        0.22549999]
[37m[1m ...
[37m[1m [1700.46644216    0.3513        0.44549999    0.19289999    0.2431    ]
[37m[1m [1933.17477873    0.37030002    0.43959999    0.1186        0.26089999]
[37m[1m [1469.25250683    0.29579997    0.38          0.18530001    0.24849999]]
[37m[1m[2023-06-25 09:47:33,825][129146] Max Reward on eval: 2258.344312805822
[37m[1m[2023-06-25 09:47:33,826][129146] Min Reward on eval: -430.27434881781227
[37m[1m[2023-06-25 09:47:33,826][129146] Mean Reward across all agents: 652.9805594762161
[37m[1m[2023-06-25 09:47:33,826][129146] Average Trajectory Length: 974.7943333333333
[36m[2023-06-25 09:47:33,827][129146] mean_value=-2199.6696280418932, max_value=276.1378822877698
[37m[1m[2023-06-25 09:47:33,830][129146] New mean coefficients: [[ 2.304704    0.1859906   0.6507241  -0.27504575 -0.14997527]]
[37m[1m[2023-06-25 09:47:33,831][129146] Moving the mean solution point...
[36m[2023-06-25 09:47:43,652][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 09:47:43,653][129146] FPS: 391065.90
[36m[2023-06-25 09:47:43,655][129146] itr=958, itrs=2000, Progress: 47.90%
[36m[2023-06-25 09:47:55,292][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 09:47:55,292][129146] FPS: 330659.13
[36m[2023-06-25 09:48:00,041][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:48:00,042][129146] Reward + Measures: [[2263.8405348     0.41068327    0.37343973    0.07396472    0.25948071]]
[37m[1m[2023-06-25 09:48:00,042][129146] Max Reward on eval: 2263.8405348048564
[37m[1m[2023-06-25 09:48:00,042][129146] Min Reward on eval: 2263.8405348048564
[37m[1m[2023-06-25 09:48:00,043][129146] Mean Reward across all agents: 2263.8405348048564
[37m[1m[2023-06-25 09:48:00,043][129146] Average Trajectory Length: 999.8946666666666
[36m[2023-06-25 09:48:05,441][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:48:05,442][129146] Reward + Measures: [[1129.7518633     0.43420002    0.48550007    0.18720001    0.33790001]
[37m[1m [2219.42948051    0.48839998    0.42770001    0.11670001    0.2529    ]
[37m[1m [-313.07954034    0.80970001    0.0385        0.84310001    0.67249995]
[37m[1m ...
[37m[1m [1240.35506438    0.39400005    0.50369996    0.14320001    0.34530002]
[37m[1m [2173.67313097    0.40230003    0.40529999    0.11180001    0.2491    ]
[37m[1m [ 732.1097692     0.3382        0.51960003    0.13210002    0.39120004]]
[37m[1m[2023-06-25 09:48:05,442][129146] Max Reward on eval: 2278.2093551119324
[37m[1m[2023-06-25 09:48:05,443][129146] Min Reward on eval: -1326.1326628615614
[37m[1m[2023-06-25 09:48:05,443][129146] Mean Reward across all agents: 647.3847881663933
[37m[1m[2023-06-25 09:48:05,443][129146] Average Trajectory Length: 999.5973333333333
[36m[2023-06-25 09:48:05,445][129146] mean_value=-1124.1592940475948, max_value=499.2388191237431
[37m[1m[2023-06-25 09:48:05,447][129146] New mean coefficients: [[ 2.4265556   0.36682636  1.0605457  -0.0133259  -0.2999049 ]]
[37m[1m[2023-06-25 09:48:05,448][129146] Moving the mean solution point...
[36m[2023-06-25 09:48:15,098][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 09:48:15,099][129146] FPS: 397995.10
[36m[2023-06-25 09:48:15,101][129146] itr=959, itrs=2000, Progress: 47.95%
[36m[2023-06-25 09:48:26,795][129146] train() took 11.67 seconds to complete
[36m[2023-06-25 09:48:26,795][129146] FPS: 329040.71
[36m[2023-06-25 09:48:31,530][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:48:31,530][129146] Reward + Measures: [[2419.50593809    0.40631536    0.3756313     0.06344467    0.25796968]]
[37m[1m[2023-06-25 09:48:31,530][129146] Max Reward on eval: 2419.505938092716
[37m[1m[2023-06-25 09:48:31,530][129146] Min Reward on eval: 2419.505938092716
[37m[1m[2023-06-25 09:48:31,530][129146] Mean Reward across all agents: 2419.505938092716
[37m[1m[2023-06-25 09:48:31,531][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:48:36,979][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:48:36,979][129146] Reward + Measures: [[ 360.47050472    0.5485        0.38499999    0.25549999    0.23270002]
[37m[1m [ 145.74326925    0.45570001    0.34350002    0.2225        0.26449999]
[37m[1m [ 479.68415823    0.38820001    0.29940003    0.17040001    0.2079    ]
[37m[1m ...
[37m[1m [ 729.89604537    0.51910001    0.31459999    0.24529998    0.2139    ]
[37m[1m [1339.39235818    0.54040003    0.38050002    0.1164        0.26390001]
[37m[1m [ 821.59657891    0.40540001    0.35260001    0.11570001    0.2538    ]]
[37m[1m[2023-06-25 09:48:36,979][129146] Max Reward on eval: 2217.35127822638
[37m[1m[2023-06-25 09:48:36,980][129146] Min Reward on eval: -827.4092325677914
[37m[1m[2023-06-25 09:48:36,980][129146] Mean Reward across all agents: 614.9580341416222
[37m[1m[2023-06-25 09:48:36,980][129146] Average Trajectory Length: 994.2763333333334
[36m[2023-06-25 09:48:36,982][129146] mean_value=-1823.3201533934105, max_value=1158.5111798928065
[37m[1m[2023-06-25 09:48:36,984][129146] New mean coefficients: [[ 2.4912288   0.24062154  0.7941651   0.20687278 -0.02215594]]
[37m[1m[2023-06-25 09:48:36,985][129146] Moving the mean solution point...
[36m[2023-06-25 09:48:46,724][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 09:48:46,724][129146] FPS: 394365.46
[36m[2023-06-25 09:48:46,727][129146] itr=960, itrs=2000, Progress: 48.00%
[37m[1m[2023-06-25 09:48:53,535][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000940
[36m[2023-06-25 09:49:05,269][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 09:49:05,270][129146] FPS: 334078.28
[36m[2023-06-25 09:49:10,010][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:49:10,010][129146] Reward + Measures: [[2698.89742958    0.39686066    0.39898235    0.04942233    0.25500032]]
[37m[1m[2023-06-25 09:49:10,011][129146] Max Reward on eval: 2698.897429579114
[37m[1m[2023-06-25 09:49:10,011][129146] Min Reward on eval: 2698.897429579114
[37m[1m[2023-06-25 09:49:10,011][129146] Mean Reward across all agents: 2698.897429579114
[37m[1m[2023-06-25 09:49:10,011][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:49:15,422][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:49:15,423][129146] Reward + Measures: [[1569.47412452    0.46689996    0.34390002    0.1577        0.23080002]
[37m[1m [1749.44599751    0.52560002    0.33559999    0.0411        0.23480001]
[37m[1m [1691.13709097    0.34689999    0.3206        0.1153        0.2376    ]
[37m[1m ...
[37m[1m [1365.02682771    0.43487272    0.50270909    0.21174546    0.2168182 ]
[37m[1m [ 972.56972673    0.46749997    0.47510004    0.24389999    0.25639999]
[37m[1m [1781.76357848    0.35459998    0.49390003    0.1653        0.25760004]]
[37m[1m[2023-06-25 09:49:15,423][129146] Max Reward on eval: 2765.265548275015
[37m[1m[2023-06-25 09:49:15,424][129146] Min Reward on eval: -146.61266687688186
[37m[1m[2023-06-25 09:49:15,424][129146] Mean Reward across all agents: 1237.6234802261044
[37m[1m[2023-06-25 09:49:15,424][129146] Average Trajectory Length: 987.2023333333333
[36m[2023-06-25 09:49:15,426][129146] mean_value=-1054.8553791906497, max_value=1507.4756176499427
[37m[1m[2023-06-25 09:49:15,428][129146] New mean coefficients: [[ 2.526195   -0.22540718  0.5977601   0.11486073  0.19514114]]
[37m[1m[2023-06-25 09:49:15,429][129146] Moving the mean solution point...
[36m[2023-06-25 09:49:25,200][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 09:49:25,200][129146] FPS: 393069.64
[36m[2023-06-25 09:49:25,202][129146] itr=961, itrs=2000, Progress: 48.05%
[36m[2023-06-25 09:49:36,742][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 09:49:36,742][129146] FPS: 333457.29
[36m[2023-06-25 09:49:41,525][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:49:41,525][129146] Reward + Measures: [[3008.19155578    0.386388      0.43895197    0.038016      0.25483033]]
[37m[1m[2023-06-25 09:49:41,525][129146] Max Reward on eval: 3008.191555779045
[37m[1m[2023-06-25 09:49:41,526][129146] Min Reward on eval: 3008.191555779045
[37m[1m[2023-06-25 09:49:41,526][129146] Mean Reward across all agents: 3008.191555779045
[37m[1m[2023-06-25 09:49:41,526][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 09:49:46,904][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:49:46,911][129146] Reward + Measures: [[2959.72802916    0.39410001    0.3876        0.0454        0.24630001]
[37m[1m [1862.88094234    0.38409999    0.32659999    0.09100001    0.2139    ]
[37m[1m [1507.70035885    0.36739999    0.34329998    0.08920001    0.2457    ]
[37m[1m ...
[37m[1m [2278.94784846    0.35660002    0.48580003    0.1288        0.2545    ]
[37m[1m [1693.31749631    0.38260004    0.53030008    0.16260001    0.25100002]
[37m[1m [ 390.73594981    0.3163        0.46599999    0.24789999    0.2599    ]]
[37m[1m[2023-06-25 09:49:46,911][129146] Max Reward on eval: 3005.6965680914464
[37m[1m[2023-06-25 09:49:46,911][129146] Min Reward on eval: -272.958816366567
[37m[1m[2023-06-25 09:49:46,912][129146] Mean Reward across all agents: 1126.2087948857297
[37m[1m[2023-06-25 09:49:46,912][129146] Average Trajectory Length: 988.4929999999999
[36m[2023-06-25 09:49:46,913][129146] mean_value=-1610.0747571471136, max_value=186.2765442113273
[37m[1m[2023-06-25 09:49:46,916][129146] New mean coefficients: [[ 2.1981573   0.8477841   0.2941287  -0.04931022  0.0598617 ]]
[37m[1m[2023-06-25 09:49:46,917][129146] Moving the mean solution point...
[36m[2023-06-25 09:49:56,569][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 09:49:56,569][129146] FPS: 397924.43
[36m[2023-06-25 09:49:56,571][129146] itr=962, itrs=2000, Progress: 48.10%
[36m[2023-06-25 09:50:08,027][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 09:50:08,028][129146] FPS: 335880.81
[36m[2023-06-25 09:50:12,914][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:50:12,914][129146] Reward + Measures: [[3255.71345889    0.37683839    0.44149092    0.02669794    0.25021404]]
[37m[1m[2023-06-25 09:50:12,914][129146] Max Reward on eval: 3255.7134588925765
[37m[1m[2023-06-25 09:50:12,914][129146] Min Reward on eval: 3255.7134588925765
[37m[1m[2023-06-25 09:50:12,915][129146] Mean Reward across all agents: 3255.7134588925765
[37m[1m[2023-06-25 09:50:12,915][129146] Average Trajectory Length: 999.7553333333333
[36m[2023-06-25 09:50:18,377][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:50:18,377][129146] Reward + Measures: [[ 915.36709614    0.50599998    0.3003        0.33049998    0.35769999]
[37m[1m [ 278.19356242    0.57459998    0.3125        0.2368        0.22320001]
[37m[1m [ -84.46723461    0.16970001    0.74220002    0.34130001    0.67080009]
[37m[1m ...
[37m[1m [1760.14133283    0.30650002    0.35080001    0.0812        0.21280001]
[37m[1m [2458.38731958    0.4585        0.41190001    0.1145        0.26789999]
[37m[1m [ 590.08584003    0.45480004    0.3928        0.19760001    0.20480001]]
[37m[1m[2023-06-25 09:50:18,378][129146] Max Reward on eval: 3028.5369513660435
[37m[1m[2023-06-25 09:50:18,378][129146] Min Reward on eval: -744.7116237725364
[37m[1m[2023-06-25 09:50:18,378][129146] Mean Reward across all agents: 1102.1354762532549
[37m[1m[2023-06-25 09:50:18,378][129146] Average Trajectory Length: 979.159
[36m[2023-06-25 09:50:18,380][129146] mean_value=-1469.769139786176, max_value=523.8372763261295
[37m[1m[2023-06-25 09:50:18,382][129146] New mean coefficients: [[ 2.4372456   0.3914275   0.07160825  0.15198036 -0.11098537]]
[37m[1m[2023-06-25 09:50:18,383][129146] Moving the mean solution point...
[36m[2023-06-25 09:50:28,087][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 09:50:28,087][129146] FPS: 395806.48
[36m[2023-06-25 09:50:28,089][129146] itr=963, itrs=2000, Progress: 48.15%
[36m[2023-06-25 09:50:39,523][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 09:50:39,524][129146] FPS: 336495.23
[36m[2023-06-25 09:50:44,325][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:50:44,326][129146] Reward + Measures: [[3456.19106004    0.36991015    0.4438453     0.01814003    0.24713396]]
[37m[1m[2023-06-25 09:50:44,326][129146] Max Reward on eval: 3456.191060043865
[37m[1m[2023-06-25 09:50:44,326][129146] Min Reward on eval: 3456.191060043865
[37m[1m[2023-06-25 09:50:44,326][129146] Mean Reward across all agents: 3456.191060043865
[37m[1m[2023-06-25 09:50:44,326][129146] Average Trajectory Length: 999.5326666666666
[36m[2023-06-25 09:50:49,781][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:50:49,782][129146] Reward + Measures: [[1476.18884033    0.27726611    0.28804049    0.09509008    0.24613224]
[37m[1m [1297.78016572    0.51630002    0.34460002    0.19129999    0.24770001]
[37m[1m [2016.28811711    0.51540005    0.5363        0.14689998    0.2579    ]
[37m[1m ...
[37m[1m [ 987.98349342    0.26290002    0.303         0.1596        0.24790001]
[37m[1m [1454.50961029    0.35500002    0.34920001    0.17990001    0.25460002]
[37m[1m [ 360.71582482    0.4034        0.24769998    0.18260001    0.2105    ]]
[37m[1m[2023-06-25 09:50:49,782][129146] Max Reward on eval: 3218.201425843802
[37m[1m[2023-06-25 09:50:49,782][129146] Min Reward on eval: -446.889726532856
[37m[1m[2023-06-25 09:50:49,782][129146] Mean Reward across all agents: 1127.3989045047397
[37m[1m[2023-06-25 09:50:49,783][129146] Average Trajectory Length: 969.5026666666666
[36m[2023-06-25 09:50:49,784][129146] mean_value=-1499.676689744552, max_value=118.55922472294878
[37m[1m[2023-06-25 09:50:49,787][129146] New mean coefficients: [[ 2.2677948   0.02732709  0.10590961 -0.04197194  0.25369588]]
[37m[1m[2023-06-25 09:50:49,787][129146] Moving the mean solution point...
[36m[2023-06-25 09:50:59,480][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 09:50:59,481][129146] FPS: 396231.47
[36m[2023-06-25 09:50:59,483][129146] itr=964, itrs=2000, Progress: 48.20%
[36m[2023-06-25 09:51:11,041][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 09:51:11,041][129146] FPS: 332879.54
[36m[2023-06-25 09:51:15,885][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:51:15,885][129146] Reward + Measures: [[3226.57593851    0.35213947    0.38523316    0.03635243    0.24063322]]
[37m[1m[2023-06-25 09:51:15,885][129146] Max Reward on eval: 3226.5759385112983
[37m[1m[2023-06-25 09:51:15,886][129146] Min Reward on eval: 3226.5759385112983
[37m[1m[2023-06-25 09:51:15,886][129146] Mean Reward across all agents: 3226.5759385112983
[37m[1m[2023-06-25 09:51:15,886][129146] Average Trajectory Length: 992.954
[36m[2023-06-25 09:51:21,408][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:51:21,409][129146] Reward + Measures: [[3016.59583826    0.33340001    0.4073        0.06700001    0.23599999]
[37m[1m [ 256.556749      0.33454561    0.22494102    0.16371317    0.20575996]
[37m[1m [1009.78979999    0.31329998    0.36029997    0.1207        0.27959999]
[37m[1m ...
[37m[1m [1352.0201908     0.41637936    0.38277584    0.10898966    0.25180346]
[37m[1m [1312.69605661    0.29230005    0.2782        0.10999999    0.23480001]
[37m[1m [1079.99412579    0.34068802    0.27556407    0.15537918    0.22275129]]
[37m[1m[2023-06-25 09:51:21,409][129146] Max Reward on eval: 3016.5958382613258
[37m[1m[2023-06-25 09:51:21,409][129146] Min Reward on eval: -353.90461893038594
[37m[1m[2023-06-25 09:51:21,409][129146] Mean Reward across all agents: 982.1460898731315
[37m[1m[2023-06-25 09:51:21,410][129146] Average Trajectory Length: 971.8703333333333
[36m[2023-06-25 09:51:21,411][129146] mean_value=-1537.0256923806212, max_value=583.5424258711976
[37m[1m[2023-06-25 09:51:21,414][129146] New mean coefficients: [[2.104572   0.10888536 0.26716256 0.28184265 0.2937861 ]]
[37m[1m[2023-06-25 09:51:21,415][129146] Moving the mean solution point...
[36m[2023-06-25 09:51:31,183][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 09:51:31,184][129146] FPS: 393152.80
[36m[2023-06-25 09:51:31,186][129146] itr=965, itrs=2000, Progress: 48.25%
[36m[2023-06-25 09:51:42,799][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 09:51:42,799][129146] FPS: 331318.14
[36m[2023-06-25 09:51:47,670][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:51:47,671][129146] Reward + Measures: [[3456.98930044    0.34646451    0.40102527    0.03151723    0.24541651]]
[37m[1m[2023-06-25 09:51:47,671][129146] Max Reward on eval: 3456.989300439745
[37m[1m[2023-06-25 09:51:47,671][129146] Min Reward on eval: 3456.989300439745
[37m[1m[2023-06-25 09:51:47,671][129146] Mean Reward across all agents: 3456.989300439745
[37m[1m[2023-06-25 09:51:47,672][129146] Average Trajectory Length: 996.7553333333333
[36m[2023-06-25 09:51:53,419][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:51:53,420][129146] Reward + Measures: [[2893.47061519    0.45549998    0.40970001    0.0911        0.24389999]
[37m[1m [1365.68628077    0.31400001    0.25690001    0.10829999    0.20829999]
[37m[1m [2457.24475701    0.35630003    0.5345        0.09679999    0.25390002]
[37m[1m ...
[37m[1m [2565.8951105     0.3935        0.40350005    0.08190001    0.25570002]
[37m[1m [1582.74996273    0.33140001    0.55629998    0.16110002    0.24919999]
[37m[1m [1397.52604419    0.5341        0.3337        0.2491        0.2325    ]]
[37m[1m[2023-06-25 09:51:53,420][129146] Max Reward on eval: 3531.8385927312074
[37m[1m[2023-06-25 09:51:53,420][129146] Min Reward on eval: -580.8417541228816
[37m[1m[2023-06-25 09:51:53,421][129146] Mean Reward across all agents: 1463.31340693445
[37m[1m[2023-06-25 09:51:53,421][129146] Average Trajectory Length: 986.3779999999999
[36m[2023-06-25 09:51:53,423][129146] mean_value=-1437.582083868423, max_value=647.6386706699041
[37m[1m[2023-06-25 09:51:53,425][129146] New mean coefficients: [[2.0041943  0.76113594 0.3458787  0.46368515 0.12424928]]
[37m[1m[2023-06-25 09:51:53,426][129146] Moving the mean solution point...
[36m[2023-06-25 09:52:03,330][129146] train() took 9.90 seconds to complete
[36m[2023-06-25 09:52:03,331][129146] FPS: 387792.24
[36m[2023-06-25 09:52:03,333][129146] itr=966, itrs=2000, Progress: 48.30%
[36m[2023-06-25 09:52:14,920][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 09:52:14,920][129146] FPS: 332047.90
[36m[2023-06-25 09:52:19,608][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:52:19,608][129146] Reward + Measures: [[3649.47922135    0.34019765    0.40794203    0.02518612    0.24573469]]
[37m[1m[2023-06-25 09:52:19,609][129146] Max Reward on eval: 3649.479221353914
[37m[1m[2023-06-25 09:52:19,609][129146] Min Reward on eval: 3649.479221353914
[37m[1m[2023-06-25 09:52:19,609][129146] Mean Reward across all agents: 3649.479221353914
[37m[1m[2023-06-25 09:52:19,609][129146] Average Trajectory Length: 995.8726666666666
[36m[2023-06-25 09:52:25,055][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:52:25,056][129146] Reward + Measures: [[ 793.95362738    0.4664        0.46349999    0.17230001    0.2428    ]
[37m[1m [ 926.48383177    0.49530002    0.29730001    0.16010001    0.20039999]
[37m[1m [2894.81236315    0.46109995    0.42400002    0.0485        0.22979999]
[37m[1m ...
[37m[1m [1300.11889448    0.52829999    0.37539998    0.18410002    0.24080001]
[37m[1m [1507.53340873    0.48730001    0.4718        0.1586        0.24130002]
[37m[1m [ 767.22516322    0.46340004    0.45660001    0.17160001    0.24330001]]
[37m[1m[2023-06-25 09:52:25,056][129146] Max Reward on eval: 3340.4172480924635
[37m[1m[2023-06-25 09:52:25,056][129146] Min Reward on eval: -173.43780768851283
[37m[1m[2023-06-25 09:52:25,057][129146] Mean Reward across all agents: 1475.5993682307999
[37m[1m[2023-06-25 09:52:25,057][129146] Average Trajectory Length: 978.809
[36m[2023-06-25 09:52:25,058][129146] mean_value=-1210.612832723112, max_value=211.2885635785109
[37m[1m[2023-06-25 09:52:25,061][129146] New mean coefficients: [[ 2.06173    -0.18620342  0.43944705  0.49883926  0.23198077]]
[37m[1m[2023-06-25 09:52:25,062][129146] Moving the mean solution point...
[36m[2023-06-25 09:52:34,769][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 09:52:34,769][129146] FPS: 395644.45
[36m[2023-06-25 09:52:34,772][129146] itr=967, itrs=2000, Progress: 48.35%
[36m[2023-06-25 09:52:46,353][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 09:52:46,353][129146] FPS: 332217.47
[36m[2023-06-25 09:52:51,199][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:52:51,199][129146] Reward + Measures: [[3821.05205038    0.3392663     0.43132275    0.02096367    0.24449329]]
[37m[1m[2023-06-25 09:52:51,199][129146] Max Reward on eval: 3821.0520503806933
[37m[1m[2023-06-25 09:52:51,199][129146] Min Reward on eval: 3821.0520503806933
[37m[1m[2023-06-25 09:52:51,199][129146] Mean Reward across all agents: 3821.0520503806933
[37m[1m[2023-06-25 09:52:51,200][129146] Average Trajectory Length: 997.4549999999999
[36m[2023-06-25 09:52:56,531][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:52:56,532][129146] Reward + Measures: [[ 974.04670497    0.42510006    0.4628        0.2155        0.29070002]
[37m[1m [2136.29803796    0.40880004    0.48140001    0.1375        0.23980001]
[37m[1m [1946.0620571     0.3362        0.50789994    0.106         0.28560001]
[37m[1m ...
[37m[1m [1747.46932868    0.39429998    0.37240002    0.10700001    0.24530001]
[37m[1m [1534.80001625    0.32610002    0.55610001    0.1464        0.32140002]
[37m[1m [1374.16090954    0.46040002    0.36069998    0.1691        0.25060001]]
[37m[1m[2023-06-25 09:52:56,532][129146] Max Reward on eval: 3622.9879844091834
[37m[1m[2023-06-25 09:52:56,532][129146] Min Reward on eval: -474.78977191017475
[37m[1m[2023-06-25 09:52:56,532][129146] Mean Reward across all agents: 1420.7182406245918
[37m[1m[2023-06-25 09:52:56,533][129146] Average Trajectory Length: 977.6113333333333
[36m[2023-06-25 09:52:56,534][129146] mean_value=-1351.391615022499, max_value=319.0769466913612
[37m[1m[2023-06-25 09:52:56,536][129146] New mean coefficients: [[2.0056925  0.29254106 0.10504529 0.4588382  0.15891773]]
[37m[1m[2023-06-25 09:52:56,537][129146] Moving the mean solution point...
[36m[2023-06-25 09:53:06,289][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 09:53:06,289][129146] FPS: 393850.68
[36m[2023-06-25 09:53:06,291][129146] itr=968, itrs=2000, Progress: 48.40%
[36m[2023-06-25 09:53:17,769][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 09:53:17,769][129146] FPS: 335320.50
[36m[2023-06-25 09:53:22,548][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:53:22,548][129146] Reward + Measures: [[3931.79276816    0.33736309    0.41998482    0.01752366    0.24461666]]
[37m[1m[2023-06-25 09:53:22,548][129146] Max Reward on eval: 3931.792768164268
[37m[1m[2023-06-25 09:53:22,549][129146] Min Reward on eval: 3931.792768164268
[37m[1m[2023-06-25 09:53:22,549][129146] Mean Reward across all agents: 3931.792768164268
[37m[1m[2023-06-25 09:53:22,549][129146] Average Trajectory Length: 998.533
[36m[2023-06-25 09:53:28,002][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:53:28,003][129146] Reward + Measures: [[2840.50063965    0.49230003    0.46420002    0.1362        0.22620001]
[37m[1m [2755.90831114    0.35260001    0.40360004    0.0948        0.2141    ]
[37m[1m [1163.64684291    0.29724407    0.36756751    0.10487048    0.24927655]
[37m[1m ...
[37m[1m [2227.74759124    0.43452454    0.43408069    0.11088246    0.23715439]
[37m[1m [3709.25915064    0.35440001    0.42089996    0.0314        0.24009998]
[37m[1m [ 659.23022964    0.41529998    0.40090004    0.1019        0.24889998]]
[37m[1m[2023-06-25 09:53:28,003][129146] Max Reward on eval: 3709.259150641598
[37m[1m[2023-06-25 09:53:28,003][129146] Min Reward on eval: -284.19167745938756
[37m[1m[2023-06-25 09:53:28,004][129146] Mean Reward across all agents: 1702.272273689
[37m[1m[2023-06-25 09:53:28,004][129146] Average Trajectory Length: 987.1066666666667
[36m[2023-06-25 09:53:28,006][129146] mean_value=-1288.1727972307067, max_value=649.5865691225586
[37m[1m[2023-06-25 09:53:28,008][129146] New mean coefficients: [[ 2.2795885  -0.28344902  0.01121879  0.67691696  0.12089323]]
[37m[1m[2023-06-25 09:53:28,009][129146] Moving the mean solution point...
[36m[2023-06-25 09:53:37,623][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 09:53:37,623][129146] FPS: 399495.35
[36m[2023-06-25 09:53:37,625][129146] itr=969, itrs=2000, Progress: 48.45%
[36m[2023-06-25 09:53:49,054][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 09:53:49,054][129146] FPS: 336775.30
[36m[2023-06-25 09:53:53,802][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:53:53,803][129146] Reward + Measures: [[4055.82333161    0.32149529    0.43574193    0.01646678    0.24820329]]
[37m[1m[2023-06-25 09:53:53,803][129146] Max Reward on eval: 4055.8233316123274
[37m[1m[2023-06-25 09:53:53,803][129146] Min Reward on eval: 4055.8233316123274
[37m[1m[2023-06-25 09:53:53,803][129146] Mean Reward across all agents: 4055.8233316123274
[37m[1m[2023-06-25 09:53:53,804][129146] Average Trajectory Length: 999.2303333333333
[36m[2023-06-25 09:53:59,212][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:53:59,212][129146] Reward + Measures: [[1348.36044571    0.40510002    0.35469997    0.15300001    0.25030002]
[37m[1m [1994.10436607    0.4515        0.3822        0.1184        0.21140002]
[37m[1m [2598.76747678    0.33910003    0.4427        0.094         0.22219999]
[37m[1m ...
[37m[1m [2121.13947392    0.44259998    0.49540001    0.17190002    0.24690001]
[37m[1m [ 584.12068868    0.33750001    0.36399999    0.3127        0.32130003]
[37m[1m [2029.18697768    0.43560001    0.47460005    0.12899999    0.23810001]]
[37m[1m[2023-06-25 09:53:59,212][129146] Max Reward on eval: 3572.9196043854813
[37m[1m[2023-06-25 09:53:59,213][129146] Min Reward on eval: -470.16426449109565
[37m[1m[2023-06-25 09:53:59,213][129146] Mean Reward across all agents: 1715.5819028413264
[37m[1m[2023-06-25 09:53:59,213][129146] Average Trajectory Length: 990.5626666666666
[36m[2023-06-25 09:53:59,215][129146] mean_value=-986.2988780034312, max_value=2359.587730593761
[37m[1m[2023-06-25 09:53:59,217][129146] New mean coefficients: [[ 2.3826797  -0.10907848 -0.00297091  0.6208841   0.22999851]]
[37m[1m[2023-06-25 09:53:59,218][129146] Moving the mean solution point...
[36m[2023-06-25 09:54:08,866][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 09:54:08,867][129146] FPS: 398065.58
[36m[2023-06-25 09:54:08,869][129146] itr=970, itrs=2000, Progress: 48.50%
[37m[1m[2023-06-25 09:54:15,678][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000950
[36m[2023-06-25 09:54:27,453][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 09:54:27,453][129146] FPS: 332893.10
[36m[2023-06-25 09:54:32,221][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:54:32,221][129146] Reward + Measures: [[4107.65962595    0.31112936    0.44390771    0.01921668    0.25322282]]
[37m[1m[2023-06-25 09:54:32,221][129146] Max Reward on eval: 4107.659625950615
[37m[1m[2023-06-25 09:54:32,222][129146] Min Reward on eval: 4107.659625950615
[37m[1m[2023-06-25 09:54:32,222][129146] Mean Reward across all agents: 4107.659625950615
[37m[1m[2023-06-25 09:54:32,222][129146] Average Trajectory Length: 998.7076666666667
[36m[2023-06-25 09:54:37,654][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:54:37,654][129146] Reward + Measures: [[1689.40383945    0.29850003    0.55459994    0.183         0.28649998]
[37m[1m [1746.05821664    0.29579997    0.61050004    0.16419999    0.29910001]
[37m[1m [1599.53962308    0.37639999    0.50800002    0.23670001    0.3355    ]
[37m[1m ...
[37m[1m [1808.94024267    0.40039998    0.62340003    0.09910001    0.3901    ]
[37m[1m [1828.35738411    0.31419998    0.53570002    0.14140001    0.24590002]
[37m[1m [1966.13145833    0.41140005    0.53450006    0.1033        0.32890001]]
[37m[1m[2023-06-25 09:54:37,655][129146] Max Reward on eval: 3742.645823744801
[37m[1m[2023-06-25 09:54:37,655][129146] Min Reward on eval: -375.09286094749115
[37m[1m[2023-06-25 09:54:37,655][129146] Mean Reward across all agents: 1816.2864841735689
[37m[1m[2023-06-25 09:54:37,655][129146] Average Trajectory Length: 995.2576666666666
[36m[2023-06-25 09:54:37,658][129146] mean_value=-1009.2913672833461, max_value=1014.0975173917886
[37m[1m[2023-06-25 09:54:37,660][129146] New mean coefficients: [[ 2.3957052   0.63269955 -0.2926629   0.47575605  0.3907668 ]]
[37m[1m[2023-06-25 09:54:37,661][129146] Moving the mean solution point...
[36m[2023-06-25 09:54:47,596][129146] train() took 9.93 seconds to complete
[36m[2023-06-25 09:54:47,597][129146] FPS: 386555.33
[36m[2023-06-25 09:54:47,599][129146] itr=971, itrs=2000, Progress: 48.55%
[36m[2023-06-25 09:54:59,152][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 09:54:59,152][129146] FPS: 333163.80
[36m[2023-06-25 09:55:04,014][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:55:04,014][129146] Reward + Measures: [[4217.04439133    0.31746754    0.42709729    0.01580331    0.2505824 ]]
[37m[1m[2023-06-25 09:55:04,014][129146] Max Reward on eval: 4217.044391325489
[37m[1m[2023-06-25 09:55:04,014][129146] Min Reward on eval: 4217.044391325489
[37m[1m[2023-06-25 09:55:04,015][129146] Mean Reward across all agents: 4217.044391325489
[37m[1m[2023-06-25 09:55:04,015][129146] Average Trajectory Length: 998.8026666666666
[36m[2023-06-25 09:55:09,528][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:55:09,528][129146] Reward + Measures: [[1343.29650406    0.23156793    0.49114022    0.15105294    0.22129853]
[37m[1m [2443.6586375     0.37760004    0.39809999    0.0691        0.229     ]
[37m[1m [ 633.86195725    0.43149328    0.35163468    0.19852462    0.26761156]
[37m[1m ...
[37m[1m [1073.78368111    0.29692221    0.32480001    0.16137779    0.21351114]
[37m[1m [3651.81842242    0.34150001    0.4285        0.0407        0.24730001]
[37m[1m [ 902.00017687    0.33974427    0.33466884    0.18795244    0.22519182]]
[37m[1m[2023-06-25 09:55:09,529][129146] Max Reward on eval: 4145.87871212312
[37m[1m[2023-06-25 09:55:09,529][129146] Min Reward on eval: -133.72703032741555
[37m[1m[2023-06-25 09:55:09,529][129146] Mean Reward across all agents: 1935.4083048562313
[37m[1m[2023-06-25 09:55:09,529][129146] Average Trajectory Length: 963.2516666666667
[36m[2023-06-25 09:55:09,532][129146] mean_value=-923.2397471598387, max_value=2465.1407920936294
[37m[1m[2023-06-25 09:55:09,534][129146] New mean coefficients: [[ 2.3792608   0.4235431  -0.21384135  0.3282209   0.20914517]]
[37m[1m[2023-06-25 09:55:09,535][129146] Moving the mean solution point...
[36m[2023-06-25 09:55:19,260][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 09:55:19,261][129146] FPS: 394899.39
[36m[2023-06-25 09:55:19,263][129146] itr=972, itrs=2000, Progress: 48.60%
[36m[2023-06-25 09:55:30,898][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 09:55:30,898][129146] FPS: 330676.44
[36m[2023-06-25 09:55:35,689][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:55:35,689][129146] Reward + Measures: [[4295.41146433    0.31768844    0.43166462    0.01515085    0.25256991]]
[37m[1m[2023-06-25 09:55:35,689][129146] Max Reward on eval: 4295.411464328184
[37m[1m[2023-06-25 09:55:35,689][129146] Min Reward on eval: 4295.411464328184
[37m[1m[2023-06-25 09:55:35,690][129146] Mean Reward across all agents: 4295.411464328184
[37m[1m[2023-06-25 09:55:35,690][129146] Average Trajectory Length: 998.4789999999999
[36m[2023-06-25 09:55:41,152][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:55:41,152][129146] Reward + Measures: [[2031.32630349    0.29249999    0.28599998    0.0693        0.1785    ]
[37m[1m [  90.71020062    0.34316558    0.26271912    0.15024701    0.18185082]
[37m[1m [2531.42171104    0.37196669    0.351475      0.06655833    0.24770001]
[37m[1m ...
[37m[1m [2140.50134624    0.4052        0.53420001    0.1883        0.27040002]
[37m[1m [1779.8633819     0.42090002    0.3946        0.19399999    0.24689999]
[37m[1m [2309.09280527    0.28967878    0.3977707     0.14051011    0.25154951]]
[37m[1m[2023-06-25 09:55:41,152][129146] Max Reward on eval: 3698.1753899117466
[37m[1m[2023-06-25 09:55:41,153][129146] Min Reward on eval: -639.8599659633007
[37m[1m[2023-06-25 09:55:41,153][129146] Mean Reward across all agents: 1411.4402173194317
[37m[1m[2023-06-25 09:55:41,153][129146] Average Trajectory Length: 898.0086666666666
[36m[2023-06-25 09:55:41,155][129146] mean_value=-1810.7947497415155, max_value=593.210337392336
[37m[1m[2023-06-25 09:55:41,157][129146] New mean coefficients: [[ 2.0402148  -0.06469488  0.19896096  0.23137878  0.04258454]]
[37m[1m[2023-06-25 09:55:41,158][129146] Moving the mean solution point...
[36m[2023-06-25 09:55:50,921][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 09:55:50,921][129146] FPS: 393377.05
[36m[2023-06-25 09:55:50,923][129146] itr=973, itrs=2000, Progress: 48.65%
[36m[2023-06-25 09:56:02,679][129146] train() took 11.73 seconds to complete
[36m[2023-06-25 09:56:02,679][129146] FPS: 327300.65
[36m[2023-06-25 09:56:07,539][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:56:07,539][129146] Reward + Measures: [[4406.50789146    0.31062275    0.4494825     0.01443784    0.24992663]]
[37m[1m[2023-06-25 09:56:07,539][129146] Max Reward on eval: 4406.507891464072
[37m[1m[2023-06-25 09:56:07,540][129146] Min Reward on eval: 4406.507891464072
[37m[1m[2023-06-25 09:56:07,540][129146] Mean Reward across all agents: 4406.507891464072
[37m[1m[2023-06-25 09:56:07,540][129146] Average Trajectory Length: 998.8073333333333
[36m[2023-06-25 09:56:13,109][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:56:13,110][129146] Reward + Measures: [[1294.32368031    0.34598082    0.39116272    0.16932146    0.20101638]
[37m[1m [2425.50938711    0.31582105    0.35478422    0.06327369    0.22116843]
[37m[1m [3794.96600492    0.26460001    0.51070005    0.0493        0.26279998]
[37m[1m ...
[37m[1m [3153.35138824    0.3831        0.41420004    0.031         0.21500002]
[37m[1m [3106.38896299    0.3017        0.46809998    0.08490001    0.25659999]
[37m[1m [1895.03855036    0.31990001    0.44520003    0.15589999    0.22260001]]
[37m[1m[2023-06-25 09:56:13,115][129146] Max Reward on eval: 4165.7773271786045
[37m[1m[2023-06-25 09:56:13,115][129146] Min Reward on eval: -377.2813748841407
[37m[1m[2023-06-25 09:56:13,116][129146] Mean Reward across all agents: 1999.813520943998
[37m[1m[2023-06-25 09:56:13,116][129146] Average Trajectory Length: 987.3443333333333
[36m[2023-06-25 09:56:13,119][129146] mean_value=-693.1209908211372, max_value=4478.363947640132
[37m[1m[2023-06-25 09:56:13,121][129146] New mean coefficients: [[ 1.9916128  -0.06140386  0.16086836  0.18719089 -0.05339364]]
[37m[1m[2023-06-25 09:56:13,123][129146] Moving the mean solution point...
[36m[2023-06-25 09:56:22,946][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 09:56:22,947][129146] FPS: 390948.75
[36m[2023-06-25 09:56:22,949][129146] itr=974, itrs=2000, Progress: 48.70%
[36m[2023-06-25 09:56:34,534][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 09:56:34,534][129146] FPS: 332226.22
[36m[2023-06-25 09:56:39,246][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:56:39,247][129146] Reward + Measures: [[4473.80194247    0.31263769    0.44802409    0.01514909    0.25164923]]
[37m[1m[2023-06-25 09:56:39,247][129146] Max Reward on eval: 4473.801942470949
[37m[1m[2023-06-25 09:56:39,247][129146] Min Reward on eval: 4473.801942470949
[37m[1m[2023-06-25 09:56:39,247][129146] Mean Reward across all agents: 4473.801942470949
[37m[1m[2023-06-25 09:56:39,248][129146] Average Trajectory Length: 999.6
[36m[2023-06-25 09:56:44,929][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:56:44,929][129146] Reward + Measures: [[ -72.7964616     0.35390002    0.26859999    0.13060001    0.1479    ]
[37m[1m [1199.33306634    0.38458019    0.33704135    0.2084157     0.25965372]
[37m[1m [2451.99336397    0.41140005    0.44180003    0.1218        0.25530002]
[37m[1m ...
[37m[1m [4064.3685502     0.29369998    0.46490002    0.0135        0.23800002]
[37m[1m [2454.30502284    0.31999999    0.57620001    0.07470001    0.26089999]
[37m[1m [1690.12771416    0.31697419    0.37530324    0.09025807    0.20778064]]
[37m[1m[2023-06-25 09:56:44,929][129146] Max Reward on eval: 4263.723135719355
[37m[1m[2023-06-25 09:56:44,930][129146] Min Reward on eval: -254.06856427550082
[37m[1m[2023-06-25 09:56:44,930][129146] Mean Reward across all agents: 1699.5163406756192
[37m[1m[2023-06-25 09:56:44,930][129146] Average Trajectory Length: 978.5696666666666
[36m[2023-06-25 09:56:44,932][129146] mean_value=-1559.6350623447006, max_value=966.890308146606
[37m[1m[2023-06-25 09:56:44,934][129146] New mean coefficients: [[ 2.209558   -0.11598055 -0.14229335 -0.00276318  0.09205198]]
[37m[1m[2023-06-25 09:56:44,935][129146] Moving the mean solution point...
[36m[2023-06-25 09:56:54,694][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 09:56:54,694][129146] FPS: 393560.46
[36m[2023-06-25 09:56:54,696][129146] itr=975, itrs=2000, Progress: 48.75%
[36m[2023-06-25 09:57:06,167][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 09:57:06,168][129146] FPS: 335398.63
[36m[2023-06-25 09:57:10,822][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:57:10,822][129146] Reward + Measures: [[4540.0026153     0.32216936    0.42432314    0.0132161     0.2495535 ]]
[37m[1m[2023-06-25 09:57:10,822][129146] Max Reward on eval: 4540.002615300011
[37m[1m[2023-06-25 09:57:10,822][129146] Min Reward on eval: 4540.002615300011
[37m[1m[2023-06-25 09:57:10,823][129146] Mean Reward across all agents: 4540.002615300011
[37m[1m[2023-06-25 09:57:10,823][129146] Average Trajectory Length: 998.246
[36m[2023-06-25 09:57:16,138][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:57:16,138][129146] Reward + Measures: [[2835.51837702    0.32201877    0.45440155    0.05913125    0.23378439]
[37m[1m [1957.21253829    0.34335837    0.32672009    0.06007225    0.22358803]
[37m[1m [ 230.24069345    0.49349999    0.27200001    0.43170005    0.45730004]
[37m[1m ...
[37m[1m [1720.4151459     0.2426919     0.34108698    0.13111658    0.23778167]
[37m[1m [3871.72630447    0.30630001    0.48890001    0.0821        0.25740001]
[37m[1m [3134.77456586    0.36270002    0.48289999    0.0786        0.2254    ]]
[37m[1m[2023-06-25 09:57:16,138][129146] Max Reward on eval: 4392.108241262356
[37m[1m[2023-06-25 09:57:16,139][129146] Min Reward on eval: 230.24069345080062
[37m[1m[2023-06-25 09:57:16,139][129146] Mean Reward across all agents: 2299.3349235951982
[37m[1m[2023-06-25 09:57:16,139][129146] Average Trajectory Length: 958.538
[36m[2023-06-25 09:57:16,142][129146] mean_value=-1211.129514358024, max_value=1594.2601061345401
[37m[1m[2023-06-25 09:57:16,144][129146] New mean coefficients: [[ 2.0442452   0.15574212  0.09999198  0.06648691 -0.14967626]]
[37m[1m[2023-06-25 09:57:16,145][129146] Moving the mean solution point...
[36m[2023-06-25 09:57:25,809][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 09:57:25,809][129146] FPS: 397417.13
[36m[2023-06-25 09:57:25,812][129146] itr=976, itrs=2000, Progress: 48.80%
[36m[2023-06-25 09:57:37,236][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 09:57:37,236][129146] FPS: 336825.01
[36m[2023-06-25 09:57:42,128][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:57:42,128][129146] Reward + Measures: [[4603.63750423    0.32292408    0.41900259    0.01131984    0.24946517]]
[37m[1m[2023-06-25 09:57:42,128][129146] Max Reward on eval: 4603.637504228072
[37m[1m[2023-06-25 09:57:42,129][129146] Min Reward on eval: 4603.637504228072
[37m[1m[2023-06-25 09:57:42,129][129146] Mean Reward across all agents: 4603.637504228072
[37m[1m[2023-06-25 09:57:42,129][129146] Average Trajectory Length: 998.6313333333333
[36m[2023-06-25 09:57:47,690][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:57:47,690][129146] Reward + Measures: [[ 817.10520845    0.45140001    0.28870001    0.1259        0.22379999]
[37m[1m [3202.73501052    0.30720001    0.53439999    0.11920001    0.25760004]
[37m[1m [3306.12982917    0.28529999    0.4831        0.0533        0.22420001]
[37m[1m ...
[37m[1m [3592.10893962    0.28300002    0.54259998    0.0506        0.26190001]
[37m[1m [2351.58979621    0.33769998    0.34599999    0.061         0.2493    ]
[37m[1m [ 364.10119492    0.3116        0.3671        0.1279        0.21430002]]
[37m[1m[2023-06-25 09:57:47,690][129146] Max Reward on eval: 4464.471698843816
[37m[1m[2023-06-25 09:57:47,691][129146] Min Reward on eval: 295.3263034802687
[37m[1m[2023-06-25 09:57:47,691][129146] Mean Reward across all agents: 2291.260474523237
[37m[1m[2023-06-25 09:57:47,691][129146] Average Trajectory Length: 979.3686666666666
[36m[2023-06-25 09:57:47,693][129146] mean_value=-840.4209969446301, max_value=2183.571500383331
[37m[1m[2023-06-25 09:57:47,696][129146] New mean coefficients: [[ 2.0319293   0.33500946  0.00473    -0.00154887 -0.1931983 ]]
[37m[1m[2023-06-25 09:57:47,697][129146] Moving the mean solution point...
[36m[2023-06-25 09:57:57,552][129146] train() took 9.85 seconds to complete
[36m[2023-06-25 09:57:57,552][129146] FPS: 389723.45
[36m[2023-06-25 09:57:57,554][129146] itr=977, itrs=2000, Progress: 48.85%
[36m[2023-06-25 09:58:09,086][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 09:58:09,086][129146] FPS: 333657.97
[36m[2023-06-25 09:58:13,883][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:58:13,884][129146] Reward + Measures: [[4663.27719324    0.31731859    0.42956191    0.00986708    0.25213319]]
[37m[1m[2023-06-25 09:58:13,884][129146] Max Reward on eval: 4663.277193243252
[37m[1m[2023-06-25 09:58:13,884][129146] Min Reward on eval: 4663.277193243252
[37m[1m[2023-06-25 09:58:13,884][129146] Mean Reward across all agents: 4663.277193243252
[37m[1m[2023-06-25 09:58:13,884][129146] Average Trajectory Length: 998.6909999999999
[36m[2023-06-25 09:58:19,368][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:58:19,368][129146] Reward + Measures: [[3377.12677836    0.37990001    0.40640002    0.05269999    0.2696    ]
[37m[1m [2789.56552592    0.31420001    0.43810001    0.1188        0.25180003]
[37m[1m [2187.35181588    0.30556628    0.35410133    0.11083224    0.21945719]
[37m[1m ...
[37m[1m [1442.34257926    0.57680005    0.41690001    0.21859999    0.17920001]
[37m[1m [2868.04952972    0.40949997    0.52630001    0.15280001    0.26500002]
[37m[1m [1044.75463175    0.5564        0.45889997    0.2588        0.30899999]]
[37m[1m[2023-06-25 09:58:19,368][129146] Max Reward on eval: 4600.325815720857
[37m[1m[2023-06-25 09:58:19,369][129146] Min Reward on eval: -179.87280044200014
[37m[1m[2023-06-25 09:58:19,369][129146] Mean Reward across all agents: 2155.416464056133
[37m[1m[2023-06-25 09:58:19,369][129146] Average Trajectory Length: 928.5169999999999
[36m[2023-06-25 09:58:19,371][129146] mean_value=-1090.1354801126674, max_value=993.6486627295471
[37m[1m[2023-06-25 09:58:19,374][129146] New mean coefficients: [[ 2.0263188   0.49298835 -0.12370455  0.02045622 -0.02431184]]
[37m[1m[2023-06-25 09:58:19,374][129146] Moving the mean solution point...
[36m[2023-06-25 09:58:29,166][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 09:58:29,166][129146] FPS: 392239.24
[36m[2023-06-25 09:58:29,169][129146] itr=978, itrs=2000, Progress: 48.90%
[36m[2023-06-25 09:58:40,777][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 09:58:40,777][129146] FPS: 331433.63
[36m[2023-06-25 09:58:45,612][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:58:45,612][129146] Reward + Measures: [[4743.06796312    0.3102904     0.4295761     0.01069893    0.25648507]]
[37m[1m[2023-06-25 09:58:45,612][129146] Max Reward on eval: 4743.06796311855
[37m[1m[2023-06-25 09:58:45,612][129146] Min Reward on eval: 4743.06796311855
[37m[1m[2023-06-25 09:58:45,612][129146] Mean Reward across all agents: 4743.06796311855
[37m[1m[2023-06-25 09:58:45,613][129146] Average Trajectory Length: 997.3153333333333
[36m[2023-06-25 09:58:51,139][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:58:51,140][129146] Reward + Measures: [[3139.48122261    0.3215        0.43900004    0.08270001    0.24800001]
[37m[1m [1233.73094569    0.37820002    0.52910006    0.1523        0.4664    ]
[37m[1m [1877.23517036    0.42520794    0.34758854    0.10637214    0.2088418 ]
[37m[1m ...
[37m[1m [3936.24218422    0.35080734    0.34943959    0.01640932    0.23070924]
[37m[1m [3094.62420386    0.30759999    0.38850001    0.0564        0.26890001]
[37m[1m [1551.52200259    0.38130003    0.32790002    0.12660001    0.2314    ]]
[37m[1m[2023-06-25 09:58:51,140][129146] Max Reward on eval: 4493.604332228564
[37m[1m[2023-06-25 09:58:51,140][129146] Min Reward on eval: 348.4773932826938
[37m[1m[2023-06-25 09:58:51,140][129146] Mean Reward across all agents: 2366.429557301068
[37m[1m[2023-06-25 09:58:51,141][129146] Average Trajectory Length: 978.5206666666667
[36m[2023-06-25 09:58:51,143][129146] mean_value=-962.6908025117278, max_value=1847.1137648624363
[37m[1m[2023-06-25 09:58:51,145][129146] New mean coefficients: [[ 2.0957499   0.20505172 -0.18187873  0.22484049 -0.26083302]]
[37m[1m[2023-06-25 09:58:51,146][129146] Moving the mean solution point...
[36m[2023-06-25 09:59:00,991][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 09:59:00,992][129146] FPS: 390103.02
[36m[2023-06-25 09:59:00,994][129146] itr=979, itrs=2000, Progress: 48.95%
[36m[2023-06-25 09:59:12,618][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 09:59:12,619][129146] FPS: 330980.70
[36m[2023-06-25 09:59:17,465][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:59:17,465][129146] Reward + Measures: [[4747.88273841    0.32047871    0.42426082    0.01127705    0.25852191]]
[37m[1m[2023-06-25 09:59:17,466][129146] Max Reward on eval: 4747.882738412989
[37m[1m[2023-06-25 09:59:17,466][129146] Min Reward on eval: 4747.882738412989
[37m[1m[2023-06-25 09:59:17,466][129146] Mean Reward across all agents: 4747.882738412989
[37m[1m[2023-06-25 09:59:17,466][129146] Average Trajectory Length: 999.0126666666666
[36m[2023-06-25 09:59:23,151][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:59:23,152][129146] Reward + Measures: [[2706.54839508    0.30940002    0.32839999    0.0517        0.252     ]
[37m[1m [3149.58546267    0.32736453    0.33283493    0.04875604    0.25566521]
[37m[1m [3496.02931468    0.34979999    0.45630002    0.0541        0.25960001]
[37m[1m ...
[37m[1m [3367.54297289    0.3263        0.41669998    0.0695        0.26799998]
[37m[1m [2304.1157982     0.33844325    0.27813426    0.03650901    0.25022611]
[37m[1m [3627.92304999    0.35160002    0.44420001    0.06239999    0.25830001]]
[37m[1m[2023-06-25 09:59:23,152][129146] Max Reward on eval: 4553.474040568073
[37m[1m[2023-06-25 09:59:23,152][129146] Min Reward on eval: -56.50448618743394
[37m[1m[2023-06-25 09:59:23,153][129146] Mean Reward across all agents: 2291.462307259775
[37m[1m[2023-06-25 09:59:23,153][129146] Average Trajectory Length: 981.5926666666667
[36m[2023-06-25 09:59:23,155][129146] mean_value=-1262.6235642898127, max_value=704.0741129118169
[37m[1m[2023-06-25 09:59:23,157][129146] New mean coefficients: [[ 1.8171264   0.12232446  0.05849224  0.17058843 -0.09195802]]
[37m[1m[2023-06-25 09:59:23,158][129146] Moving the mean solution point...
[36m[2023-06-25 09:59:32,934][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 09:59:32,935][129146] FPS: 392833.53
[36m[2023-06-25 09:59:32,937][129146] itr=980, itrs=2000, Progress: 49.00%
[37m[1m[2023-06-25 09:59:39,736][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000960
[36m[2023-06-25 09:59:51,498][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 09:59:51,498][129146] FPS: 332818.33
[36m[2023-06-25 09:59:56,328][129146] Finished Evaluation Step
[37m[1m[2023-06-25 09:59:56,329][129146] Reward + Measures: [[4835.78885255    0.31891179    0.41536483    0.00896202    0.2553176 ]]
[37m[1m[2023-06-25 09:59:56,329][129146] Max Reward on eval: 4835.788852552182
[37m[1m[2023-06-25 09:59:56,329][129146] Min Reward on eval: 4835.788852552182
[37m[1m[2023-06-25 09:59:56,329][129146] Mean Reward across all agents: 4835.788852552182
[37m[1m[2023-06-25 09:59:56,330][129146] Average Trajectory Length: 998.8236666666667
[36m[2023-06-25 10:00:01,864][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:00:01,865][129146] Reward + Measures: [[1514.3702628     0.35119534    0.45353809    0.19850741    0.25818551]
[37m[1m [2369.37649063    0.2992        0.43420002    0.1094        0.233     ]
[37m[1m [1894.13927804    0.4002285     0.49217144    0.17786424    0.25438663]
[37m[1m ...
[37m[1m [3109.34110797    0.37310001    0.33150002    0.047         0.24910001]
[37m[1m [3543.17306713    0.33400002    0.39770001    0.027         0.24460001]
[37m[1m [1430.19616959    0.44150001    0.4138        0.14490001    0.24600001]]
[37m[1m[2023-06-25 10:00:01,865][129146] Max Reward on eval: 4513.099900866951
[37m[1m[2023-06-25 10:00:01,865][129146] Min Reward on eval: -354.5306214381941
[37m[1m[2023-06-25 10:00:01,866][129146] Mean Reward across all agents: 2058.078809087781
[37m[1m[2023-06-25 10:00:01,866][129146] Average Trajectory Length: 948.2463333333333
[36m[2023-06-25 10:00:01,867][129146] mean_value=-1712.6970676903309, max_value=792.7293180467123
[37m[1m[2023-06-25 10:00:01,869][129146] New mean coefficients: [[ 1.7390203  -0.16154352  0.06156378  0.08229643 -0.20684668]]
[37m[1m[2023-06-25 10:00:01,870][129146] Moving the mean solution point...
[36m[2023-06-25 10:00:11,534][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 10:00:11,535][129146] FPS: 397417.52
[36m[2023-06-25 10:00:11,537][129146] itr=981, itrs=2000, Progress: 49.05%
[36m[2023-06-25 10:00:23,164][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 10:00:23,165][129146] FPS: 330923.56
[36m[2023-06-25 10:00:27,975][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:00:27,975][129146] Reward + Measures: [[4910.60619249    0.31140938    0.42486191    0.00871063    0.25277701]]
[37m[1m[2023-06-25 10:00:27,976][129146] Max Reward on eval: 4910.606192493484
[37m[1m[2023-06-25 10:00:27,976][129146] Min Reward on eval: 4910.606192493484
[37m[1m[2023-06-25 10:00:27,976][129146] Mean Reward across all agents: 4910.606192493484
[37m[1m[2023-06-25 10:00:27,977][129146] Average Trajectory Length: 998.7113333333333
[36m[2023-06-25 10:00:33,452][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:00:33,452][129146] Reward + Measures: [[4257.46595289    0.3698        0.34640002    0.0162        0.22830001]
[37m[1m [2280.11845196    0.32820004    0.38890001    0.105         0.22500001]
[37m[1m [ 525.03994999    0.35514256    0.27219015    0.15686704    0.14793427]
[37m[1m ...
[37m[1m [3956.66317336    0.30815008    0.36239484    0.02243723    0.25197396]
[37m[1m [2417.92448402    0.30430001    0.43930003    0.12279999    0.2588    ]
[37m[1m [4003.82572485    0.3026        0.39589998    0.0165        0.2184    ]]
[37m[1m[2023-06-25 10:00:33,452][129146] Max Reward on eval: 4935.673343339283
[37m[1m[2023-06-25 10:00:33,453][129146] Min Reward on eval: 314.1771537285749
[37m[1m[2023-06-25 10:00:33,453][129146] Mean Reward across all agents: 2573.015516063077
[37m[1m[2023-06-25 10:00:33,453][129146] Average Trajectory Length: 963.352
[36m[2023-06-25 10:00:33,455][129146] mean_value=-872.6800668007567, max_value=1460.5069931176993
[37m[1m[2023-06-25 10:00:33,457][129146] New mean coefficients: [[ 1.5902897  -0.09523009  0.04890996  0.01807463 -0.21139933]]
[37m[1m[2023-06-25 10:00:33,458][129146] Moving the mean solution point...
[36m[2023-06-25 10:00:43,293][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 10:00:43,293][129146] FPS: 390497.75
[36m[2023-06-25 10:00:43,296][129146] itr=982, itrs=2000, Progress: 49.10%
[36m[2023-06-25 10:00:55,019][129146] train() took 11.70 seconds to complete
[36m[2023-06-25 10:00:55,019][129146] FPS: 328226.22
[36m[2023-06-25 10:00:59,928][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:00:59,933][129146] Reward + Measures: [[4974.51229538    0.30530724    0.44326717    0.01093021    0.25807446]]
[37m[1m[2023-06-25 10:00:59,933][129146] Max Reward on eval: 4974.512295379818
[37m[1m[2023-06-25 10:00:59,934][129146] Min Reward on eval: 4974.512295379818
[37m[1m[2023-06-25 10:00:59,934][129146] Mean Reward across all agents: 4974.512295379818
[37m[1m[2023-06-25 10:00:59,934][129146] Average Trajectory Length: 999.468
[36m[2023-06-25 10:01:05,426][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:01:05,427][129146] Reward + Measures: [[2625.28089372    0.29516366    0.36629832    0.11130055    0.22047928]
[37m[1m [1970.71936122    0.40840003    0.33019999    0.0623        0.29130003]
[37m[1m [3628.92114075    0.28753698    0.34332588    0.03603408    0.24027625]
[37m[1m ...
[37m[1m [4164.6728967     0.27289999    0.45479998    0.0471        0.25110003]
[37m[1m [3875.97970835    0.27770001    0.53299999    0.0785        0.24419999]
[37m[1m [3836.49518838    0.30230001    0.33920002    0.029         0.2422    ]]
[37m[1m[2023-06-25 10:01:05,427][129146] Max Reward on eval: 4950.794947045191
[37m[1m[2023-06-25 10:01:05,427][129146] Min Reward on eval: 470.5818521687994
[37m[1m[2023-06-25 10:01:05,428][129146] Mean Reward across all agents: 3128.999752030557
[37m[1m[2023-06-25 10:01:05,428][129146] Average Trajectory Length: 978.8123333333333
[36m[2023-06-25 10:01:05,430][129146] mean_value=-786.2847014616201, max_value=2749.435662299754
[37m[1m[2023-06-25 10:01:05,433][129146] New mean coefficients: [[ 1.3762383   0.14198552  0.00911098 -0.15756021 -0.24405035]]
[37m[1m[2023-06-25 10:01:05,434][129146] Moving the mean solution point...
[36m[2023-06-25 10:01:15,197][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 10:01:15,198][129146] FPS: 393363.98
[36m[2023-06-25 10:01:15,200][129146] itr=983, itrs=2000, Progress: 49.15%
[36m[2023-06-25 10:01:26,885][129146] train() took 11.66 seconds to complete
[36m[2023-06-25 10:01:26,885][129146] FPS: 329248.81
[36m[2023-06-25 10:01:31,766][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:01:31,772][129146] Reward + Measures: [[5009.8915403     0.3072471     0.43416211    0.00958571    0.25853443]]
[37m[1m[2023-06-25 10:01:31,772][129146] Max Reward on eval: 5009.8915402999955
[37m[1m[2023-06-25 10:01:31,773][129146] Min Reward on eval: 5009.8915402999955
[37m[1m[2023-06-25 10:01:31,773][129146] Mean Reward across all agents: 5009.8915402999955
[37m[1m[2023-06-25 10:01:31,773][129146] Average Trajectory Length: 999.6003333333333
[36m[2023-06-25 10:01:37,376][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:01:37,377][129146] Reward + Measures: [[ -41.55357428    0.41415834    0.25111908    0.24327739    0.23301904]
[37m[1m [4900.97484667    0.28620002    0.42980003    0.0136        0.25320002]
[37m[1m [ -95.71203964    0.44875231    0.25476056    0.29801497    0.26009947]
[37m[1m ...
[37m[1m [  57.92592663    0.49067989    0.29001674    0.24630538    0.23331879]
[37m[1m [4808.65832574    0.29749998    0.45249996    0.022         0.26789999]
[37m[1m [2820.90734805    0.31005582    0.34334382    0.06703892    0.23528159]]
[37m[1m[2023-06-25 10:01:37,377][129146] Max Reward on eval: 4917.548586452031
[37m[1m[2023-06-25 10:01:37,377][129146] Min Reward on eval: -323.72505876653594
[37m[1m[2023-06-25 10:01:37,377][129146] Mean Reward across all agents: 2728.5595829934123
[37m[1m[2023-06-25 10:01:37,378][129146] Average Trajectory Length: 928.3943333333333
[36m[2023-06-25 10:01:37,380][129146] mean_value=-1154.5835496990644, max_value=901.1280383787603
[37m[1m[2023-06-25 10:01:37,382][129146] New mean coefficients: [[ 1.1982298   0.02770826 -0.06388777 -0.11860293  0.00716951]]
[37m[1m[2023-06-25 10:01:37,383][129146] Moving the mean solution point...
[36m[2023-06-25 10:01:47,242][129146] train() took 9.86 seconds to complete
[36m[2023-06-25 10:01:47,242][129146] FPS: 389549.26
[36m[2023-06-25 10:01:47,245][129146] itr=984, itrs=2000, Progress: 49.20%
[36m[2023-06-25 10:01:58,768][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 10:01:58,769][129146] FPS: 333978.48
[36m[2023-06-25 10:02:03,547][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:02:03,547][129146] Reward + Measures: [[4924.27639749    0.31039289    0.44827059    0.02267833    0.26105103]]
[37m[1m[2023-06-25 10:02:03,548][129146] Max Reward on eval: 4924.276397488552
[37m[1m[2023-06-25 10:02:03,548][129146] Min Reward on eval: 4924.276397488552
[37m[1m[2023-06-25 10:02:03,548][129146] Mean Reward across all agents: 4924.276397488552
[37m[1m[2023-06-25 10:02:03,548][129146] Average Trajectory Length: 999.0946666666666
[36m[2023-06-25 10:02:09,225][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:02:09,226][129146] Reward + Measures: [[4888.70165296    0.31020001    0.40810004    0.0147        0.24440001]
[37m[1m [3502.25352602    0.33671379    0.39312416    0.04467241    0.25252417]
[37m[1m [4178.51223717    0.32253179    0.39513585    0.02051267    0.23038518]
[37m[1m ...
[37m[1m [3637.87198574    0.29080001    0.49499997    0.0553        0.26950002]
[37m[1m [2290.05374815    0.39813867    0.45452824    0.12371857    0.2469648 ]
[37m[1m [4820.94225777    0.3046        0.4224        0.0183        0.2483    ]]
[37m[1m[2023-06-25 10:02:09,226][129146] Max Reward on eval: 4993.470044416934
[37m[1m[2023-06-25 10:02:09,226][129146] Min Reward on eval: -239.7896176559414
[37m[1m[2023-06-25 10:02:09,227][129146] Mean Reward across all agents: 3061.5521316474724
[37m[1m[2023-06-25 10:02:09,227][129146] Average Trajectory Length: 974.5883333333333
[36m[2023-06-25 10:02:09,229][129146] mean_value=-996.5550330350422, max_value=1587.48203077373
[37m[1m[2023-06-25 10:02:09,231][129146] New mean coefficients: [[ 0.8369957  -0.06667839  0.1001865  -0.0180463   0.07782058]]
[37m[1m[2023-06-25 10:02:09,232][129146] Moving the mean solution point...
[36m[2023-06-25 10:02:18,910][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 10:02:18,911][129146] FPS: 396838.38
[36m[2023-06-25 10:02:18,913][129146] itr=985, itrs=2000, Progress: 49.25%
[36m[2023-06-25 10:02:30,801][129146] train() took 11.86 seconds to complete
[36m[2023-06-25 10:02:30,801][129146] FPS: 323653.42
[36m[2023-06-25 10:02:35,533][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:02:35,533][129146] Reward + Measures: [[5012.2369446     0.30658638    0.44083431    0.01513365    0.25670111]]
[37m[1m[2023-06-25 10:02:35,533][129146] Max Reward on eval: 5012.23694460369
[37m[1m[2023-06-25 10:02:35,534][129146] Min Reward on eval: 5012.23694460369
[37m[1m[2023-06-25 10:02:35,534][129146] Mean Reward across all agents: 5012.23694460369
[37m[1m[2023-06-25 10:02:35,534][129146] Average Trajectory Length: 998.771
[36m[2023-06-25 10:02:40,906][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:02:40,906][129146] Reward + Measures: [[4446.39068251    0.28          0.39120004    0.0254        0.23740001]
[37m[1m [3161.05361837    0.28560001    0.41299996    0.11939999    0.25770003]
[37m[1m [4061.64082741    0.2836        0.46970001    0.0676        0.25799999]
[37m[1m ...
[37m[1m [4088.02200432    0.3235794     0.40013406    0.02250071    0.23579502]
[37m[1m [2836.32055086    0.41570002    0.49599996    0.07250001    0.26980004]
[37m[1m [1752.65297886    0.28469786    0.3423149     0.07294681    0.23874044]]
[37m[1m[2023-06-25 10:02:40,907][129146] Max Reward on eval: 5073.128650231054
[37m[1m[2023-06-25 10:02:40,907][129146] Min Reward on eval: 129.09569449687842
[37m[1m[2023-06-25 10:02:40,907][129146] Mean Reward across all agents: 3163.2874483790943
[37m[1m[2023-06-25 10:02:40,907][129146] Average Trajectory Length: 963.7256666666666
[36m[2023-06-25 10:02:40,909][129146] mean_value=-1025.5969921387914, max_value=494.20165704541705
[37m[1m[2023-06-25 10:02:40,911][129146] New mean coefficients: [[ 0.65714496 -0.012821    0.12689053 -0.03856255 -0.07407553]]
[37m[1m[2023-06-25 10:02:40,912][129146] Moving the mean solution point...
[36m[2023-06-25 10:02:50,544][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 10:02:50,545][129146] FPS: 398711.99
[36m[2023-06-25 10:02:50,547][129146] itr=986, itrs=2000, Progress: 49.30%
[36m[2023-06-25 10:03:01,997][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 10:03:01,998][129146] FPS: 336036.49
[36m[2023-06-25 10:03:06,782][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:03:06,783][129146] Reward + Measures: [[5091.5864216     0.30886921    0.44076416    0.01066733    0.25397578]]
[37m[1m[2023-06-25 10:03:06,783][129146] Max Reward on eval: 5091.586421595808
[37m[1m[2023-06-25 10:03:06,783][129146] Min Reward on eval: 5091.586421595808
[37m[1m[2023-06-25 10:03:06,784][129146] Mean Reward across all agents: 5091.586421595808
[37m[1m[2023-06-25 10:03:06,784][129146] Average Trajectory Length: 998.9226666666666
[36m[2023-06-25 10:03:12,212][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:03:12,213][129146] Reward + Measures: [[2247.0838635     0.39660111    0.31777892    0.07210594    0.24721566]
[37m[1m [3294.90878934    0.38001415    0.31382832    0.04773133    0.25420514]
[37m[1m [3219.41933841    0.35896683    0.38507712    0.03593243    0.23099914]
[37m[1m ...
[37m[1m [2787.62961358    0.41877809    0.30571768    0.03232095    0.2408403 ]
[37m[1m [4784.51616019    0.32750002    0.40229997    0.0087        0.25140002]
[37m[1m [3317.37983654    0.33926088    0.32937393    0.04118695    0.25601742]]
[37m[1m[2023-06-25 10:03:12,213][129146] Max Reward on eval: 5074.704247805663
[37m[1m[2023-06-25 10:03:12,213][129146] Min Reward on eval: 1431.4821230429575
[37m[1m[2023-06-25 10:03:12,214][129146] Mean Reward across all agents: 3630.352349275854
[37m[1m[2023-06-25 10:03:12,214][129146] Average Trajectory Length: 972.9123333333333
[36m[2023-06-25 10:03:12,216][129146] mean_value=-609.9149908773878, max_value=1240.2944182985348
[37m[1m[2023-06-25 10:03:12,218][129146] New mean coefficients: [[ 0.82412726  0.04601044  0.21162027 -0.0098361  -0.05424434]]
[37m[1m[2023-06-25 10:03:12,219][129146] Moving the mean solution point...
[36m[2023-06-25 10:03:22,085][129146] train() took 9.86 seconds to complete
[36m[2023-06-25 10:03:22,085][129146] FPS: 389283.66
[36m[2023-06-25 10:03:22,088][129146] itr=987, itrs=2000, Progress: 49.35%
[36m[2023-06-25 10:03:33,657][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 10:03:33,658][129146] FPS: 332575.32
[36m[2023-06-25 10:03:38,471][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:03:38,472][129146] Reward + Measures: [[4940.38542501    0.32030937    0.40264517    0.01955791    0.2482565 ]]
[37m[1m[2023-06-25 10:03:38,472][129146] Max Reward on eval: 4940.385425005336
[37m[1m[2023-06-25 10:03:38,472][129146] Min Reward on eval: 4940.385425005336
[37m[1m[2023-06-25 10:03:38,472][129146] Mean Reward across all agents: 4940.385425005336
[37m[1m[2023-06-25 10:03:38,472][129146] Average Trajectory Length: 993.0616666666666
[36m[2023-06-25 10:03:43,943][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:03:43,949][129146] Reward + Measures: [[2099.20955046    0.4998022     0.23583727    0.29643893    0.30543894]
[37m[1m [1993.72596883    0.42930791    0.295351      0.0683975     0.21145923]
[37m[1m [2901.31920881    0.32722235    0.44716835    0.08314009    0.24130674]
[37m[1m ...
[37m[1m [3121.18252039    0.3256        0.48340002    0.13280001    0.24689999]
[37m[1m [1553.12321859    0.34783357    0.26508096    0.09468793    0.21046881]
[37m[1m [2990.87606457    0.33610001    0.45629999    0.11659999    0.24949999]]
[37m[1m[2023-06-25 10:03:43,949][129146] Max Reward on eval: 4927.846016080584
[37m[1m[2023-06-25 10:03:43,949][129146] Min Reward on eval: -120.52560772240395
[37m[1m[2023-06-25 10:03:43,950][129146] Mean Reward across all agents: 2514.94223183825
[37m[1m[2023-06-25 10:03:43,950][129146] Average Trajectory Length: 901.4436666666667
[36m[2023-06-25 10:03:43,952][129146] mean_value=-1277.3214734379023, max_value=640.9016502706097
[37m[1m[2023-06-25 10:03:43,954][129146] New mean coefficients: [[ 0.5529929   0.05331578 -0.01375356  0.0770296  -0.01113454]]
[37m[1m[2023-06-25 10:03:43,955][129146] Moving the mean solution point...
[36m[2023-06-25 10:03:53,707][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 10:03:53,707][129146] FPS: 393842.74
[36m[2023-06-25 10:03:53,709][129146] itr=988, itrs=2000, Progress: 49.40%
[36m[2023-06-25 10:04:05,181][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 10:04:05,181][129146] FPS: 335387.32
[36m[2023-06-25 10:04:09,923][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:04:09,924][129146] Reward + Measures: [[5059.38145925    0.31716114    0.39487475    0.0149383     0.25365129]]
[37m[1m[2023-06-25 10:04:09,924][129146] Max Reward on eval: 5059.381459248663
[37m[1m[2023-06-25 10:04:09,924][129146] Min Reward on eval: 5059.381459248663
[37m[1m[2023-06-25 10:04:09,924][129146] Mean Reward across all agents: 5059.381459248663
[37m[1m[2023-06-25 10:04:09,924][129146] Average Trajectory Length: 997.7493333333333
[36m[2023-06-25 10:04:15,590][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:04:15,590][129146] Reward + Measures: [[4649.66795956    0.28220004    0.47129998    0.0399        0.25549999]
[37m[1m [3855.08143506    0.329         0.442         0.0748        0.26420003]
[37m[1m [4587.1946095     0.31314054    0.34723008    0.02162946    0.25151521]
[37m[1m ...
[37m[1m [2890.94321639    0.34163737    0.32379577    0.06295653    0.2214094 ]
[37m[1m [4293.85340234    0.32430002    0.53870004    0.0487        0.25409999]
[37m[1m [4700.08684369    0.294         0.47220001    0.0369        0.26699999]]
[37m[1m[2023-06-25 10:04:15,590][129146] Max Reward on eval: 5089.368308937922
[37m[1m[2023-06-25 10:04:15,591][129146] Min Reward on eval: -476.34521052205383
[37m[1m[2023-06-25 10:04:15,591][129146] Mean Reward across all agents: 3743.5159288548925
[37m[1m[2023-06-25 10:04:15,591][129146] Average Trajectory Length: 975.053
[36m[2023-06-25 10:04:15,593][129146] mean_value=-654.3036890441011, max_value=977.9094693259722
[37m[1m[2023-06-25 10:04:15,595][129146] New mean coefficients: [[ 0.5188267   0.2366099   0.1343612   0.14073381 -0.04702027]]
[37m[1m[2023-06-25 10:04:15,596][129146] Moving the mean solution point...
[36m[2023-06-25 10:04:25,368][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 10:04:25,368][129146] FPS: 393030.16
[36m[2023-06-25 10:04:25,370][129146] itr=989, itrs=2000, Progress: 49.45%
[36m[2023-06-25 10:04:36,843][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 10:04:36,844][129146] FPS: 335376.69
[36m[2023-06-25 10:04:41,560][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:04:41,561][129146] Reward + Measures: [[5042.08564754    0.31963652    0.41372004    0.01235524    0.26110822]]
[37m[1m[2023-06-25 10:04:41,561][129146] Max Reward on eval: 5042.085647541052
[37m[1m[2023-06-25 10:04:41,561][129146] Min Reward on eval: 5042.085647541052
[37m[1m[2023-06-25 10:04:41,562][129146] Mean Reward across all agents: 5042.085647541052
[37m[1m[2023-06-25 10:04:41,562][129146] Average Trajectory Length: 998.5419999999999
[36m[2023-06-25 10:04:46,996][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:04:46,997][129146] Reward + Measures: [[ 135.87864616    0.25371397    0.34813711    0.18982729    0.23042679]
[37m[1m [-123.83887685    0.58830005    0.2343        0.33039999    0.26859999]
[37m[1m [3057.62341355    0.35964426    0.36431915    0.05728088    0.25049725]
[37m[1m ...
[37m[1m [2868.77131045    0.3732833     0.40919885    0.0957498     0.26379666]
[37m[1m [2587.60631823    0.28931552    0.37442032    0.08938181    0.25661489]
[37m[1m [1936.66211816    0.42812285    0.37983489    0.17819953    0.2691589 ]]
[37m[1m[2023-06-25 10:04:46,997][129146] Max Reward on eval: 4977.835105145886
[37m[1m[2023-06-25 10:04:46,997][129146] Min Reward on eval: -396.81749574069397
[37m[1m[2023-06-25 10:04:46,998][129146] Mean Reward across all agents: 1819.8705642058856
[37m[1m[2023-06-25 10:04:46,998][129146] Average Trajectory Length: 975.4119999999999
[36m[2023-06-25 10:04:46,999][129146] mean_value=-1965.6550731895582, max_value=165.5274834620086
[37m[1m[2023-06-25 10:04:47,001][129146] New mean coefficients: [[-0.18402863  0.25916892  0.14813706  0.05086586  0.05169394]]
[37m[1m[2023-06-25 10:04:47,002][129146] Moving the mean solution point...
[36m[2023-06-25 10:04:56,641][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 10:04:56,641][129146] FPS: 398462.64
[36m[2023-06-25 10:04:56,643][129146] itr=990, itrs=2000, Progress: 49.50%
[37m[1m[2023-06-25 10:05:03,279][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000970
[36m[2023-06-25 10:05:14,961][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 10:05:14,961][129146] FPS: 335398.86
[36m[2023-06-25 10:05:19,676][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:05:19,676][129146] Reward + Measures: [[4912.30345015    0.33128154    0.3902286     0.01319366    0.25949925]]
[37m[1m[2023-06-25 10:05:19,677][129146] Max Reward on eval: 4912.303450149399
[37m[1m[2023-06-25 10:05:19,677][129146] Min Reward on eval: 4912.303450149399
[37m[1m[2023-06-25 10:05:19,677][129146] Mean Reward across all agents: 4912.303450149399
[37m[1m[2023-06-25 10:05:19,677][129146] Average Trajectory Length: 996.2593333333333
[36m[2023-06-25 10:05:25,141][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:05:25,141][129146] Reward + Measures: [[1494.99728572    0.52820009    0.33539999    0.14720002    0.2177    ]
[37m[1m [4583.21364174    0.33919999    0.37979999    0.0135        0.2457    ]
[37m[1m [2327.70939685    0.47460005    0.36230001    0.0395        0.21250001]
[37m[1m ...
[37m[1m [3986.28719634    0.37732607    0.32672176    0.02739131    0.23957391]
[37m[1m [2454.77346047    0.49519998    0.35549998    0.0444        0.21610001]
[37m[1m [ 734.3689201     0.33258167    0.36224517    0.15233962    0.20969181]]
[37m[1m[2023-06-25 10:05:25,142][129146] Max Reward on eval: 4976.115030123527
[37m[1m[2023-06-25 10:05:25,142][129146] Min Reward on eval: -300.35286563985574
[37m[1m[2023-06-25 10:05:25,142][129146] Mean Reward across all agents: 2187.419657732095
[37m[1m[2023-06-25 10:05:25,142][129146] Average Trajectory Length: 908.5293333333333
[36m[2023-06-25 10:05:25,145][129146] mean_value=-1287.7454314927484, max_value=2008.9784641361418
[37m[1m[2023-06-25 10:05:25,147][129146] New mean coefficients: [[0.8542353  0.22603354 0.12508056 0.0431826  0.0646994 ]]
[37m[1m[2023-06-25 10:05:25,148][129146] Moving the mean solution point...
[36m[2023-06-25 10:05:34,971][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 10:05:34,972][129146] FPS: 390967.36
[36m[2023-06-25 10:05:34,974][129146] itr=991, itrs=2000, Progress: 49.55%
[36m[2023-06-25 10:05:46,405][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 10:05:46,405][129146] FPS: 336691.77
[36m[2023-06-25 10:05:51,105][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:05:51,105][129146] Reward + Measures: [[4989.14832496    0.32026368    0.38452068    0.01128518    0.25829974]]
[37m[1m[2023-06-25 10:05:51,106][129146] Max Reward on eval: 4989.1483249645
[37m[1m[2023-06-25 10:05:51,106][129146] Min Reward on eval: 4989.1483249645
[37m[1m[2023-06-25 10:05:51,106][129146] Mean Reward across all agents: 4989.1483249645
[37m[1m[2023-06-25 10:05:51,106][129146] Average Trajectory Length: 997.6516666666666
[36m[2023-06-25 10:05:56,673][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:05:56,674][129146] Reward + Measures: [[4404.96777733    0.34770003    0.38459998    0.015         0.25530002]
[37m[1m [4150.11485524    0.27250001    0.48750001    0.0345        0.24959998]
[37m[1m [4205.52918485    0.35699999    0.43859997    0.0209        0.25120002]
[37m[1m ...
[37m[1m [1424.33680143    0.32990676    0.29517272    0.13302687    0.24380246]
[37m[1m [1086.21169163    0.33433962    0.3333528     0.1768        0.25101885]
[37m[1m [1088.6631309     0.47402307    0.26889229    0.25962308    0.31644616]]
[37m[1m[2023-06-25 10:05:56,674][129146] Max Reward on eval: 5059.958282839786
[37m[1m[2023-06-25 10:05:56,674][129146] Min Reward on eval: -291.7319513200142
[37m[1m[2023-06-25 10:05:56,675][129146] Mean Reward across all agents: 1971.3460087511519
[37m[1m[2023-06-25 10:05:56,675][129146] Average Trajectory Length: 951.141
[36m[2023-06-25 10:05:56,677][129146] mean_value=-1952.4082644683954, max_value=2472.821515794489
[37m[1m[2023-06-25 10:05:56,679][129146] New mean coefficients: [[-0.05608982  0.2688419   0.04461557  0.15478209  0.14073831]]
[37m[1m[2023-06-25 10:05:56,680][129146] Moving the mean solution point...
[36m[2023-06-25 10:06:06,339][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 10:06:06,340][129146] FPS: 397604.33
[36m[2023-06-25 10:06:06,342][129146] itr=992, itrs=2000, Progress: 49.60%
[36m[2023-06-25 10:06:17,945][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 10:06:17,945][129146] FPS: 331572.41
[36m[2023-06-25 10:06:22,839][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:06:22,840][129146] Reward + Measures: [[4923.89617231    0.32729444    0.36673239    0.01131672    0.2545616 ]]
[37m[1m[2023-06-25 10:06:22,840][129146] Max Reward on eval: 4923.896172307023
[37m[1m[2023-06-25 10:06:22,840][129146] Min Reward on eval: 4923.896172307023
[37m[1m[2023-06-25 10:06:22,841][129146] Mean Reward across all agents: 4923.896172307023
[37m[1m[2023-06-25 10:06:22,841][129146] Average Trajectory Length: 995.1453333333333
[36m[2023-06-25 10:06:28,538][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:06:28,539][129146] Reward + Measures: [[1314.99679648    0.34240004    0.2595        0.12860002    0.2198    ]
[37m[1m [4838.71469347    0.31740001    0.41880003    0.0152        0.2577    ]
[37m[1m [3487.89258001    0.33930001    0.37209997    0.0374        0.25680003]
[37m[1m ...
[37m[1m [4300.86680156    0.33520001    0.36129999    0.019         0.26430002]
[37m[1m [2360.76860368    0.35489997    0.35039997    0.0687        0.24010001]
[37m[1m [3047.55075627    0.34430003    0.37220001    0.055         0.26180002]]
[37m[1m[2023-06-25 10:06:28,539][129146] Max Reward on eval: 4983.863674104074
[37m[1m[2023-06-25 10:06:28,539][129146] Min Reward on eval: -129.99153185422475
[37m[1m[2023-06-25 10:06:28,539][129146] Mean Reward across all agents: 2679.2729674347893
[37m[1m[2023-06-25 10:06:28,540][129146] Average Trajectory Length: 973.1946666666666
[36m[2023-06-25 10:06:28,541][129146] mean_value=-1463.929013378146, max_value=350.3238801547259
[37m[1m[2023-06-25 10:06:28,543][129146] New mean coefficients: [[-0.23226999  0.27097937  0.1678519   0.10017733  0.0783785 ]]
[37m[1m[2023-06-25 10:06:28,544][129146] Moving the mean solution point...
[36m[2023-06-25 10:06:38,273][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 10:06:38,273][129146] FPS: 394772.64
[36m[2023-06-25 10:06:38,276][129146] itr=993, itrs=2000, Progress: 49.65%
[36m[2023-06-25 10:06:49,872][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 10:06:49,872][129146] FPS: 331786.36
[36m[2023-06-25 10:06:54,693][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:06:54,693][129146] Reward + Measures: [[3633.75559779    0.37066177    0.32208964    0.02399256    0.2311842 ]]
[37m[1m[2023-06-25 10:06:54,693][129146] Max Reward on eval: 3633.7555977878856
[37m[1m[2023-06-25 10:06:54,694][129146] Min Reward on eval: 3633.7555977878856
[37m[1m[2023-06-25 10:06:54,694][129146] Mean Reward across all agents: 3633.7555977878856
[37m[1m[2023-06-25 10:06:54,694][129146] Average Trajectory Length: 901.2153333333333
[36m[2023-06-25 10:07:00,100][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:07:00,101][129146] Reward + Measures: [[4303.07825168    0.36059996    0.34220001    0.0223        0.22989999]
[37m[1m [3148.40064402    0.29930001    0.3829        0.06460001    0.2227    ]
[37m[1m [3070.42594889    0.46560001    0.33340001    0.0105        0.22780001]
[37m[1m ...
[37m[1m [3525.28710497    0.45300004    0.3531        0.0148        0.2172    ]
[37m[1m [1161.66536975    0.47950003    0.36569998    0.26500002    0.26209998]
[37m[1m [2868.77190096    0.37310001    0.42600003    0.0406        0.21859999]]
[37m[1m[2023-06-25 10:07:00,101][129146] Max Reward on eval: 4546.470944193867
[37m[1m[2023-06-25 10:07:00,101][129146] Min Reward on eval: 88.9185428142664
[37m[1m[2023-06-25 10:07:00,102][129146] Mean Reward across all agents: 2794.648940987076
[37m[1m[2023-06-25 10:07:00,102][129146] Average Trajectory Length: 946.8929999999999
[36m[2023-06-25 10:07:00,104][129146] mean_value=-922.4858904684208, max_value=1519.102213760868
[37m[1m[2023-06-25 10:07:00,106][129146] New mean coefficients: [[-0.47570646  0.36206     0.23813039  0.04304401  0.09369364]]
[37m[1m[2023-06-25 10:07:00,107][129146] Moving the mean solution point...
[36m[2023-06-25 10:07:09,750][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 10:07:09,750][129146] FPS: 398303.51
[36m[2023-06-25 10:07:09,753][129146] itr=994, itrs=2000, Progress: 49.70%
[36m[2023-06-25 10:07:21,255][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 10:07:21,256][129146] FPS: 334624.81
[36m[2023-06-25 10:07:26,050][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:07:26,051][129146] Reward + Measures: [[2649.11990887    0.38340673    0.30458349    0.03702253    0.22713444]]
[37m[1m[2023-06-25 10:07:26,051][129146] Max Reward on eval: 2649.1199088697285
[37m[1m[2023-06-25 10:07:26,051][129146] Min Reward on eval: 2649.1199088697285
[37m[1m[2023-06-25 10:07:26,052][129146] Mean Reward across all agents: 2649.1199088697285
[37m[1m[2023-06-25 10:07:26,052][129146] Average Trajectory Length: 808.7866666666666
[36m[2023-06-25 10:07:31,435][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:07:31,435][129146] Reward + Measures: [[2174.20317591    0.32788417    0.34693494    0.08473512    0.24981998]
[37m[1m [3049.56000024    0.37970003    0.3836        0.0746        0.24489999]
[37m[1m [1709.60231109    0.34900001    0.3536        0.1053        0.25299999]
[37m[1m ...
[37m[1m [1483.88341565    0.28930002    0.32340002    0.0978        0.24070001]
[37m[1m [2391.16281511    0.35764733    0.32141611    0.03058369    0.2333933 ]
[37m[1m [-125.20529735    0.2296181     0.27690029    0.16966455    0.25416994]]
[37m[1m[2023-06-25 10:07:31,435][129146] Max Reward on eval: 3694.39742564921
[37m[1m[2023-06-25 10:07:31,436][129146] Min Reward on eval: -127.26700933662941
[37m[1m[2023-06-25 10:07:31,436][129146] Mean Reward across all agents: 1831.1331130763422
[37m[1m[2023-06-25 10:07:31,436][129146] Average Trajectory Length: 891.1593333333333
[36m[2023-06-25 10:07:31,437][129146] mean_value=-2188.171483282827, max_value=-318.1806833740711
[36m[2023-06-25 10:07:31,439][129146] XNES is restarting with a new solution whose measures are [0.79030001 0.93470001 0.76660007 0.8957001 ] and objective is 292.7877981611993
[36m[2023-06-25 10:07:31,440][129146] Emitter restarted. Changing the mean agent...
[37m[1m[2023-06-25 10:07:31,442][129146] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]]
[37m[1m[2023-06-25 10:07:31,443][129146] Moving the mean solution point...
[36m[2023-06-25 10:07:41,104][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 10:07:41,104][129146] FPS: 397551.06
[36m[2023-06-25 10:07:41,106][129146] itr=995, itrs=2000, Progress: 49.75%
[36m[2023-06-25 10:07:52,650][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 10:07:52,650][129146] FPS: 333288.22
[36m[2023-06-25 10:07:57,502][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:07:57,502][129146] Reward + Measures: [[-701.68730996    0.71268564    0.7651791     0.13863118    0.73623097]]
[37m[1m[2023-06-25 10:07:57,502][129146] Max Reward on eval: -701.6873099617602
[37m[1m[2023-06-25 10:07:57,502][129146] Min Reward on eval: -701.6873099617602
[37m[1m[2023-06-25 10:07:57,503][129146] Mean Reward across all agents: -701.6873099617602
[37m[1m[2023-06-25 10:07:57,503][129146] Average Trajectory Length: 994.3476666666667
[36m[2023-06-25 10:08:03,089][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:08:03,090][129146] Reward + Measures: [[ -689.89041611     0.2974         0.39850003     0.22309999
[37m[1m      0.4219    ]
[37m[1m [-2859.91518649     0.88290006     0.87760001     0.0054
[37m[1m      0.88859999]
[37m[1m [-1854.72939184     0.43709999     0.47140002     0.1257
[37m[1m      0.41249999]
[37m[1m ...
[37m[1m [-1317.08305605     0.39132291     0.29731628     0.27072659
[37m[1m      0.3109698 ]
[37m[1m [-1995.78719553     0.25269458     0.27205488     0.13233522
[37m[1m      0.27300766]
[37m[1m [-1217.80824551     0.22868414     0.20827733     0.15155159
[37m[1m      0.15997525]]
[37m[1m[2023-06-25 10:08:03,090][129146] Max Reward on eval: -73.53059631297364
[37m[1m[2023-06-25 10:08:03,090][129146] Min Reward on eval: -3108.0857102692125
[37m[1m[2023-06-25 10:08:03,091][129146] Mean Reward across all agents: -1256.7887162499592
[37m[1m[2023-06-25 10:08:03,091][129146] Average Trajectory Length: 859.4023333333333
[36m[2023-06-25 10:08:03,092][129146] mean_value=-2807.5007821690388, max_value=-610.5586474158599
[36m[2023-06-25 10:08:03,094][129146] XNES is restarting with a new solution whose measures are [0.77940005 0.34639999 0.59610003 0.15369999] and objective is 672.1975838984479
[36m[2023-06-25 10:08:03,095][129146] Emitter restarted. Changing the mean agent...
[37m[1m[2023-06-25 10:08:03,098][129146] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]]
[37m[1m[2023-06-25 10:08:03,099][129146] Moving the mean solution point...
[36m[2023-06-25 10:08:12,867][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 10:08:12,867][129146] FPS: 393175.96
[36m[2023-06-25 10:08:12,869][129146] itr=996, itrs=2000, Progress: 49.80%
[36m[2023-06-25 10:08:24,354][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 10:08:24,354][129146] FPS: 335095.79
[36m[2023-06-25 10:08:29,131][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:08:29,131][129146] Reward + Measures: [[971.4499396    0.60938656   0.43453056   0.24255484   0.31065601]]
[37m[1m[2023-06-25 10:08:29,131][129146] Max Reward on eval: 971.4499395962364
[37m[1m[2023-06-25 10:08:29,131][129146] Min Reward on eval: 971.4499395962364
[37m[1m[2023-06-25 10:08:29,132][129146] Mean Reward across all agents: 971.4499395962364
[37m[1m[2023-06-25 10:08:29,132][129146] Average Trajectory Length: 999.8206666666666
[36m[2023-06-25 10:08:34,524][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:08:34,525][129146] Reward + Measures: [[ 949.51805384    0.5891        0.3946        0.3019        0.22239999]
[37m[1m [  13.86149765    0.4526        0.33750001    0.2163        0.26789999]
[37m[1m [-516.00408601    0.27730003    0.30599999    0.18190001    0.2304    ]
[37m[1m ...
[37m[1m [  20.73562378    0.53430003    0.46219999    0.34020001    0.30679998]
[37m[1m [-336.80941783    0.30399588    0.26192084    0.15274036    0.21208297]
[37m[1m [-664.67100181    0.22129999    0.27610001    0.1832        0.2429    ]]
[37m[1m[2023-06-25 10:08:34,525][129146] Max Reward on eval: 1078.1804436061298
[37m[1m[2023-06-25 10:08:34,525][129146] Min Reward on eval: -1167.5354806528426
[37m[1m[2023-06-25 10:08:34,526][129146] Mean Reward across all agents: -125.17446544708602
[37m[1m[2023-06-25 10:08:34,526][129146] Average Trajectory Length: 956.9359999999999
[36m[2023-06-25 10:08:34,528][129146] mean_value=-1888.804896694378, max_value=543.6960435660475
[37m[1m[2023-06-25 10:08:34,530][129146] New mean coefficients: [[ 0.21771249 -0.63454044  0.08805317 -0.2802955  -0.6595125 ]]
[37m[1m[2023-06-25 10:08:34,531][129146] Moving the mean solution point...
[36m[2023-06-25 10:08:44,206][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 10:08:44,207][129146] FPS: 396937.43
[36m[2023-06-25 10:08:44,209][129146] itr=997, itrs=2000, Progress: 49.85%
[36m[2023-06-25 10:08:55,936][129146] train() took 11.70 seconds to complete
[36m[2023-06-25 10:08:55,937][129146] FPS: 328173.11
[36m[2023-06-25 10:09:00,671][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:09:00,671][129146] Reward + Measures: [[1027.40063273    0.5832023     0.43435302    0.23136665    0.30371565]]
[37m[1m[2023-06-25 10:09:00,671][129146] Max Reward on eval: 1027.400632733843
[37m[1m[2023-06-25 10:09:00,672][129146] Min Reward on eval: 1027.400632733843
[37m[1m[2023-06-25 10:09:00,672][129146] Mean Reward across all agents: 1027.400632733843
[37m[1m[2023-06-25 10:09:00,672][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:09:06,300][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:09:06,300][129146] Reward + Measures: [[-451.498779      0.27411944    0.27778515    0.2189014     0.18071683]
[37m[1m [-562.11966994    0.68360007    0.0404        0.80060005    0.47490001]
[37m[1m [-478.33037872    0.2673654     0.29762623    0.12068422    0.17700957]
[37m[1m ...
[37m[1m [-397.21326863    0.32485309    0.16954437    0.20746265    0.18952458]
[37m[1m [-780.93676782    0.30274007    0.22443068    0.16172591    0.14865987]
[37m[1m [-345.57225555    0.46539998    0.2881        0.30930001    0.22219999]]
[37m[1m[2023-06-25 10:09:06,300][129146] Max Reward on eval: 1290.8441300374573
[37m[1m[2023-06-25 10:09:06,301][129146] Min Reward on eval: -1451.5306032306048
[37m[1m[2023-06-25 10:09:06,301][129146] Mean Reward across all agents: -303.34077371764994
[37m[1m[2023-06-25 10:09:06,301][129146] Average Trajectory Length: 867.3006666666666
[36m[2023-06-25 10:09:06,303][129146] mean_value=-1377.7907879165919, max_value=766.8337139904813
[37m[1m[2023-06-25 10:09:06,305][129146] New mean coefficients: [[ 0.36285454 -0.1917502  -0.22769883  0.39541596  1.0060775 ]]
[37m[1m[2023-06-25 10:09:06,306][129146] Moving the mean solution point...
[36m[2023-06-25 10:09:16,039][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 10:09:16,039][129146] FPS: 394616.89
[36m[2023-06-25 10:09:16,041][129146] itr=998, itrs=2000, Progress: 49.90%
[36m[2023-06-25 10:09:27,722][129146] train() took 11.65 seconds to complete
[36m[2023-06-25 10:09:27,722][129146] FPS: 329499.82
[36m[2023-06-25 10:09:32,493][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:09:32,494][129146] Reward + Measures: [[628.36179683   0.56164837   0.44838467   0.20208466   0.36884367]]
[37m[1m[2023-06-25 10:09:32,494][129146] Max Reward on eval: 628.3617968283924
[37m[1m[2023-06-25 10:09:32,494][129146] Min Reward on eval: 628.3617968283924
[37m[1m[2023-06-25 10:09:32,494][129146] Mean Reward across all agents: 628.3617968283924
[37m[1m[2023-06-25 10:09:32,494][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:09:37,919][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:09:37,920][129146] Reward + Measures: [[ 162.43084605    0.52330005    0.2568        0.4032        0.21529999]
[37m[1m [-882.34291255    0.2669        0.2827        0.26700002    0.1978    ]
[37m[1m [-281.3512909     0.47459999    0.33809999    0.41079998    0.26949999]
[37m[1m ...
[37m[1m [-158.29476322    0.35763377    0.21885911    0.33066091    0.1279459 ]
[37m[1m [ 504.32551048    0.52060002    0.49600002    0.1705        0.40189996]
[37m[1m [  -1.62395773    0.58649999    0.3802        0.28849998    0.33510002]]
[37m[1m[2023-06-25 10:09:37,920][129146] Max Reward on eval: 679.9895292456728
[37m[1m[2023-06-25 10:09:37,920][129146] Min Reward on eval: -970.892685413314
[37m[1m[2023-06-25 10:09:37,920][129146] Mean Reward across all agents: -126.02571944835732
[37m[1m[2023-06-25 10:09:37,921][129146] Average Trajectory Length: 961.9993333333333
[36m[2023-06-25 10:09:37,922][129146] mean_value=-1640.6202842811279, max_value=-166.75641464602018
[36m[2023-06-25 10:09:37,924][129146] XNES is restarting with a new solution whose measures are [0.51830006 0.456      0.0588     0.23870002] and objective is 3366.222747444734
[36m[2023-06-25 10:09:37,925][129146] Emitter restarted. Changing the mean agent...
[37m[1m[2023-06-25 10:09:37,927][129146] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]]
[37m[1m[2023-06-25 10:09:37,928][129146] Moving the mean solution point...
[36m[2023-06-25 10:09:47,579][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 10:09:47,579][129146] FPS: 397949.64
[36m[2023-06-25 10:09:47,582][129146] itr=999, itrs=2000, Progress: 49.95%
[36m[2023-06-25 10:09:59,094][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 10:09:59,094][129146] FPS: 334321.49
[36m[2023-06-25 10:10:03,865][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:10:03,866][129146] Reward + Measures: [[778.07234353   0.3534686    0.2767694    0.11199068   0.21653722]]
[37m[1m[2023-06-25 10:10:03,866][129146] Max Reward on eval: 778.072343532022
[37m[1m[2023-06-25 10:10:03,866][129146] Min Reward on eval: 778.072343532022
[37m[1m[2023-06-25 10:10:03,866][129146] Mean Reward across all agents: 778.072343532022
[37m[1m[2023-06-25 10:10:03,867][129146] Average Trajectory Length: 826.6616666666666
[36m[2023-06-25 10:10:09,296][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:10:09,297][129146] Reward + Measures: [[ -264.76269115     0.37989998     0.28550002     0.15790001
[37m[1m      0.24099998]
[37m[1m [ -438.4022706      0.40579996     0.26250002     0.33800003
[37m[1m      0.3053    ]
[37m[1m [   -7.23956302     0.33140001     0.2723         0.32980001
[37m[1m      0.30309999]
[37m[1m ...
[37m[1m [ -130.49171217     0.40960002     0.2378         0.38490003
[37m[1m      0.2987    ]
[37m[1m [ -732.06887124     0.34990573     0.21973376     0.37258345
[37m[1m      0.27778918]
[37m[1m [-1223.76102751     0.21233805     0.22762311     0.14046562
[37m[1m      0.14829193]]
[37m[1m[2023-06-25 10:10:09,297][129146] Max Reward on eval: 1097.1027866649442
[37m[1m[2023-06-25 10:10:09,297][129146] Min Reward on eval: -1483.5137251132633
[37m[1m[2023-06-25 10:10:09,297][129146] Mean Reward across all agents: -467.6736107252929
[37m[1m[2023-06-25 10:10:09,298][129146] Average Trajectory Length: 898.7963333333333
[36m[2023-06-25 10:10:09,299][129146] mean_value=-2168.0611392853193, max_value=-313.8478683144806
[36m[2023-06-25 10:10:09,301][129146] XNES is restarting with a new solution whose measures are [0.32640001 0.48259997 0.28760001 0.57080001] and objective is 1135.6574451267138
[36m[2023-06-25 10:10:09,302][129146] Emitter restarted. Changing the mean agent...
[37m[1m[2023-06-25 10:10:09,305][129146] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]]
[37m[1m[2023-06-25 10:10:09,305][129146] Moving the mean solution point...
[36m[2023-06-25 10:10:19,119][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 10:10:19,119][129146] FPS: 391348.57
[36m[2023-06-25 10:10:19,122][129146] itr=1000, itrs=2000, Progress: 50.00%
[37m[1m[2023-06-25 10:10:25,849][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000980
[36m[2023-06-25 10:10:37,671][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 10:10:37,672][129146] FPS: 331649.49
[36m[2023-06-25 10:10:42,517][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:10:42,517][129146] Reward + Measures: [[1209.71835933    0.32826349    0.38139322    0.13169211    0.428424  ]]
[37m[1m[2023-06-25 10:10:42,517][129146] Max Reward on eval: 1209.7183593344125
[37m[1m[2023-06-25 10:10:42,518][129146] Min Reward on eval: 1209.7183593344125
[37m[1m[2023-06-25 10:10:42,518][129146] Mean Reward across all agents: 1209.7183593344125
[37m[1m[2023-06-25 10:10:42,518][129146] Average Trajectory Length: 995.78
[36m[2023-06-25 10:10:48,023][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:10:48,029][129146] Reward + Measures: [[1134.06300268    0.38890001    0.38799998    0.13090001    0.42469999]
[37m[1m [1141.59423945    0.37849998    0.37630001    0.11790001    0.46199998]
[37m[1m [ 973.8876214     0.33010003    0.37699997    0.0854        0.4752    ]
[37m[1m ...
[37m[1m [1071.17919583    0.3506        0.35429999    0.1332        0.42570001]
[37m[1m [ 990.85317817    0.44103175    0.38676986    0.16907302    0.43229684]
[37m[1m [ 612.10604356    0.38568267    0.36023858    0.07957401    0.4691866 ]]
[37m[1m[2023-06-25 10:10:48,029][129146] Max Reward on eval: 1374.5083360957506
[37m[1m[2023-06-25 10:10:48,030][129146] Min Reward on eval: 130.76331373929278
[37m[1m[2023-06-25 10:10:48,030][129146] Mean Reward across all agents: 920.9230288327205
[37m[1m[2023-06-25 10:10:48,030][129146] Average Trajectory Length: 994.3773333333334
[36m[2023-06-25 10:10:48,033][129146] mean_value=-1263.5212027495786, max_value=1112.0674086209235
[37m[1m[2023-06-25 10:10:48,036][129146] New mean coefficients: [[ 0.56988597 -0.22519195 -1.4090583  -1.5704514  -1.7489996 ]]
[37m[1m[2023-06-25 10:10:48,037][129146] Moving the mean solution point...
[36m[2023-06-25 10:10:57,628][129146] train() took 9.59 seconds to complete
[36m[2023-06-25 10:10:57,628][129146] FPS: 400427.25
[36m[2023-06-25 10:10:57,630][129146] itr=1001, itrs=2000, Progress: 50.05%
[36m[2023-06-25 10:11:09,181][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 10:11:09,182][129146] FPS: 333160.31
[36m[2023-06-25 10:11:14,030][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:11:14,031][129146] Reward + Measures: [[1251.17859424    0.31814557    0.33357766    0.09670423    0.36936286]]
[37m[1m[2023-06-25 10:11:14,031][129146] Max Reward on eval: 1251.1785942359174
[37m[1m[2023-06-25 10:11:14,031][129146] Min Reward on eval: 1251.1785942359174
[37m[1m[2023-06-25 10:11:14,032][129146] Mean Reward across all agents: 1251.1785942359174
[37m[1m[2023-06-25 10:11:14,032][129146] Average Trajectory Length: 990.819
[36m[2023-06-25 10:11:19,759][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:11:19,760][129146] Reward + Measures: [[ 932.83307246    0.46310002    0.46820003    0.1054        0.5158    ]
[37m[1m [ 716.12536003    0.32563332    0.27256665    0.07863333    0.33276668]
[37m[1m [1112.79299783    0.36129999    0.43259999    0.13870001    0.42810002]
[37m[1m ...
[37m[1m [ 957.28678434    0.2922        0.25960001    0.08790001    0.32160002]
[37m[1m [1004.69672043    0.41949996    0.47220001    0.28040001    0.3919    ]
[37m[1m [1398.95913896    0.38079998    0.40650001    0.0963        0.42430001]]
[37m[1m[2023-06-25 10:11:19,760][129146] Max Reward on eval: 1472.4983172055217
[37m[1m[2023-06-25 10:11:19,760][129146] Min Reward on eval: 420.8395785642322
[37m[1m[2023-06-25 10:11:19,760][129146] Mean Reward across all agents: 987.1982155860628
[37m[1m[2023-06-25 10:11:19,761][129146] Average Trajectory Length: 972.0073333333333
[36m[2023-06-25 10:11:19,762][129146] mean_value=-2148.6270942068613, max_value=714.8826254318146
[37m[1m[2023-06-25 10:11:19,765][129146] New mean coefficients: [[ 1.6248888  -1.0011711  -0.48522097 -3.6166925  -0.7946864 ]]
[37m[1m[2023-06-25 10:11:19,766][129146] Moving the mean solution point...
[36m[2023-06-25 10:11:29,566][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 10:11:29,566][129146] FPS: 391895.42
[36m[2023-06-25 10:11:29,568][129146] itr=1002, itrs=2000, Progress: 50.10%
[36m[2023-06-25 10:11:41,255][129146] train() took 11.66 seconds to complete
[36m[2023-06-25 10:11:41,255][129146] FPS: 329305.23
[36m[2023-06-25 10:11:45,998][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:11:45,999][129146] Reward + Measures: [[1331.37562298    0.31108838    0.31475934    0.08373124    0.3501237 ]]
[37m[1m[2023-06-25 10:11:45,999][129146] Max Reward on eval: 1331.3756229810463
[37m[1m[2023-06-25 10:11:45,999][129146] Min Reward on eval: 1331.3756229810463
[37m[1m[2023-06-25 10:11:45,999][129146] Mean Reward across all agents: 1331.3756229810463
[37m[1m[2023-06-25 10:11:46,000][129146] Average Trajectory Length: 988.3823333333333
[36m[2023-06-25 10:11:51,511][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:11:51,517][129146] Reward + Measures: [[-194.4048576     0.4111        0.46450004    0.32909998    0.46900001]
[37m[1m [-136.01781712    0.44099998    0.26900002    0.4226        0.38120005]
[37m[1m [1377.02163074    0.32100001    0.35549998    0.14099999    0.3872    ]
[37m[1m ...
[37m[1m [ 382.19877861    0.44930002    0.5011        0.17749999    0.56060004]
[37m[1m [ 113.07967149    0.48259997    0.48249999    0.2185        0.5697    ]
[37m[1m [ 227.31811789    0.42940003    0.55600005    0.2184        0.59000003]]
[37m[1m[2023-06-25 10:11:51,517][129146] Max Reward on eval: 1586.350364771625
[37m[1m[2023-06-25 10:11:51,517][129146] Min Reward on eval: -1107.9713246035856
[37m[1m[2023-06-25 10:11:51,518][129146] Mean Reward across all agents: 140.66595461840686
[37m[1m[2023-06-25 10:11:51,518][129146] Average Trajectory Length: 997.1986666666667
[36m[2023-06-25 10:11:51,520][129146] mean_value=-1599.7218222208612, max_value=474.7244320720913
[37m[1m[2023-06-25 10:11:51,522][129146] New mean coefficients: [[ 1.5051721  -0.33607525 -0.6483339  -2.2144876   0.29257745]]
[37m[1m[2023-06-25 10:11:51,523][129146] Moving the mean solution point...
[36m[2023-06-25 10:12:01,376][129146] train() took 9.85 seconds to complete
[36m[2023-06-25 10:12:01,376][129146] FPS: 389805.23
[36m[2023-06-25 10:12:01,378][129146] itr=1003, itrs=2000, Progress: 50.15%
[36m[2023-06-25 10:12:12,977][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 10:12:12,977][129146] FPS: 331852.13
[36m[2023-06-25 10:12:17,805][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:12:17,805][129146] Reward + Measures: [[1472.57197223    0.31298175    0.31071818    0.08313142    0.34011754]]
[37m[1m[2023-06-25 10:12:17,806][129146] Max Reward on eval: 1472.571972228166
[37m[1m[2023-06-25 10:12:17,806][129146] Min Reward on eval: 1472.571972228166
[37m[1m[2023-06-25 10:12:17,806][129146] Mean Reward across all agents: 1472.571972228166
[37m[1m[2023-06-25 10:12:17,806][129146] Average Trajectory Length: 989.1876666666666
[36m[2023-06-25 10:12:23,321][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:12:23,321][129146] Reward + Measures: [[1664.91202957    0.36360002    0.39250001    0.16530001    0.35520002]
[37m[1m [1262.1853156     0.32734212    0.3183189     0.08716873    0.3489573 ]
[37m[1m [ 924.28972911    0.26989999    0.29050002    0.1096        0.37180001]
[37m[1m ...
[37m[1m [1630.07965446    0.37200001    0.35170004    0.11659999    0.35429999]
[37m[1m [ 880.07451869    0.30940002    0.26400003    0.08523335    0.3303667 ]
[37m[1m [1528.54981526    0.34081158    0.40473482    0.16606088    0.40446812]]
[37m[1m[2023-06-25 10:12:23,322][129146] Max Reward on eval: 1842.89002982761
[37m[1m[2023-06-25 10:12:23,322][129146] Min Reward on eval: 266.42794501448225
[37m[1m[2023-06-25 10:12:23,322][129146] Mean Reward across all agents: 1220.831307969954
[37m[1m[2023-06-25 10:12:23,322][129146] Average Trajectory Length: 985.6703333333332
[36m[2023-06-25 10:12:23,324][129146] mean_value=-2310.301313397656, max_value=450.9025663471639
[37m[1m[2023-06-25 10:12:23,326][129146] New mean coefficients: [[ 0.71476847 -1.6606948   0.6322919  -1.7107145   1.5377321 ]]
[37m[1m[2023-06-25 10:12:23,327][129146] Moving the mean solution point...
[36m[2023-06-25 10:12:33,076][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 10:12:33,077][129146] FPS: 393948.69
[36m[2023-06-25 10:12:33,079][129146] itr=1004, itrs=2000, Progress: 50.20%
[36m[2023-06-25 10:12:44,642][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 10:12:44,642][129146] FPS: 332744.73
[36m[2023-06-25 10:12:49,456][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:12:49,456][129146] Reward + Measures: [[1616.91082935    0.31620601    0.32139152    0.08404843    0.36407965]]
[37m[1m[2023-06-25 10:12:49,456][129146] Max Reward on eval: 1616.9108293473496
[37m[1m[2023-06-25 10:12:49,457][129146] Min Reward on eval: 1616.9108293473496
[37m[1m[2023-06-25 10:12:49,457][129146] Mean Reward across all agents: 1616.9108293473496
[37m[1m[2023-06-25 10:12:49,457][129146] Average Trajectory Length: 991.0623333333333
[36m[2023-06-25 10:12:54,923][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:12:54,929][129146] Reward + Measures: [[1523.65146109    0.30270001    0.29390001    0.09970001    0.35170004]
[37m[1m [ 820.62041117    0.37980002    0.44779998    0.0686        0.61280006]
[37m[1m [1565.55870844    0.33769998    0.41700003    0.13960002    0.4368    ]
[37m[1m ...
[37m[1m [1640.74543463    0.31890002    0.3303        0.1184        0.3831    ]
[37m[1m [ 778.62033362    0.28740001    0.44840002    0.3396        0.52540004]
[37m[1m [1258.51753828    0.31470001    0.41610003    0.1851        0.4409    ]]
[37m[1m[2023-06-25 10:12:54,929][129146] Max Reward on eval: 1855.5103395549552
[37m[1m[2023-06-25 10:12:54,930][129146] Min Reward on eval: 90.86895790345734
[37m[1m[2023-06-25 10:12:54,930][129146] Mean Reward across all agents: 1202.8636768671079
[37m[1m[2023-06-25 10:12:54,930][129146] Average Trajectory Length: 993.9753333333333
[36m[2023-06-25 10:12:54,933][129146] mean_value=-1266.401890628481, max_value=1084.4856627558038
[37m[1m[2023-06-25 10:12:54,935][129146] New mean coefficients: [[ 0.2962672 -1.3888748  0.8900603 -0.8485754  3.2940373]]
[37m[1m[2023-06-25 10:12:54,936][129146] Moving the mean solution point...
[36m[2023-06-25 10:13:04,643][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 10:13:04,643][129146] FPS: 395686.80
[36m[2023-06-25 10:13:04,645][129146] itr=1005, itrs=2000, Progress: 50.25%
[36m[2023-06-25 10:13:16,211][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 10:13:16,211][129146] FPS: 332777.50
[36m[2023-06-25 10:13:21,077][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:13:21,077][129146] Reward + Measures: [[1670.08042035    0.3187533     0.33215404    0.08291168    0.39964995]]
[37m[1m[2023-06-25 10:13:21,077][129146] Max Reward on eval: 1670.0804203468736
[37m[1m[2023-06-25 10:13:21,077][129146] Min Reward on eval: 1670.0804203468736
[37m[1m[2023-06-25 10:13:21,077][129146] Mean Reward across all agents: 1670.0804203468736
[37m[1m[2023-06-25 10:13:21,078][129146] Average Trajectory Length: 992.9036666666666
[36m[2023-06-25 10:13:26,529][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:13:26,530][129146] Reward + Measures: [[1292.39503546    0.33865359    0.42587367    0.11284702    0.48327932]
[37m[1m [ 292.15154475    0.44350001    0.37450001    0.099         0.537     ]
[37m[1m [-636.350737      0.32350001    0.3378        0.32679999    0.28150001]
[37m[1m ...
[37m[1m [ 381.47789414    0.36070001    0.3407        0.2316        0.38660002]
[37m[1m [ 924.89535729    0.35479999    0.3558        0.1073        0.46160004]
[37m[1m [-136.31511242    0.35189995    0.3662        0.3098        0.3475    ]]
[37m[1m[2023-06-25 10:13:26,530][129146] Max Reward on eval: 1730.1877916075755
[37m[1m[2023-06-25 10:13:26,530][129146] Min Reward on eval: -1431.871571768436
[37m[1m[2023-06-25 10:13:26,531][129146] Mean Reward across all agents: 426.11586412702195
[37m[1m[2023-06-25 10:13:26,531][129146] Average Trajectory Length: 994.7846666666667
[36m[2023-06-25 10:13:26,533][129146] mean_value=-927.908237695711, max_value=942.5568905110705
[37m[1m[2023-06-25 10:13:26,536][129146] New mean coefficients: [[ 1.1844258  -2.9186535   0.21533978 -0.9927077   4.6734986 ]]
[37m[1m[2023-06-25 10:13:26,537][129146] Moving the mean solution point...
[36m[2023-06-25 10:13:36,281][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 10:13:36,282][129146] FPS: 394131.24
[36m[2023-06-25 10:13:36,284][129146] itr=1006, itrs=2000, Progress: 50.30%
[36m[2023-06-25 10:13:47,901][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 10:13:47,901][129146] FPS: 331209.15
[36m[2023-06-25 10:13:52,712][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:13:52,712][129146] Reward + Measures: [[1765.7548438     0.31357095    0.33911183    0.08616947    0.42481241]]
[37m[1m[2023-06-25 10:13:52,712][129146] Max Reward on eval: 1765.754843801343
[37m[1m[2023-06-25 10:13:52,713][129146] Min Reward on eval: 1765.754843801343
[37m[1m[2023-06-25 10:13:52,713][129146] Mean Reward across all agents: 1765.754843801343
[37m[1m[2023-06-25 10:13:52,713][129146] Average Trajectory Length: 993.3246666666666
[36m[2023-06-25 10:13:58,364][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:13:58,365][129146] Reward + Measures: [[ 158.86558348    0.28199998    0.49620005    0.2251        0.49460003]
[37m[1m [-235.99323915    0.3908        0.56129998    0.32969999    0.50990003]
[37m[1m [-344.24340678    0.53459996    0.6092        0.46479997    0.56800002]
[37m[1m ...
[37m[1m [-525.22319085    0.53509998    0.63330001    0.48740003    0.57129997]
[37m[1m [-990.09353054    0.54640001    0.50150007    0.49079999    0.46340004]
[37m[1m [ 198.75612961    0.44099998    0.54590005    0.1191        0.55229998]]
[37m[1m[2023-06-25 10:13:58,365][129146] Max Reward on eval: 1780.404498341505
[37m[1m[2023-06-25 10:13:58,366][129146] Min Reward on eval: -1409.6905912051036
[37m[1m[2023-06-25 10:13:58,366][129146] Mean Reward across all agents: 350.1343806600132
[37m[1m[2023-06-25 10:13:58,366][129146] Average Trajectory Length: 992.8533333333334
[36m[2023-06-25 10:13:58,368][129146] mean_value=-1078.9598007861596, max_value=351.3156018943944
[37m[1m[2023-06-25 10:13:58,370][129146] New mean coefficients: [[ 1.560251   -3.9287648   0.04391465 -0.6140814   5.304993  ]]
[37m[1m[2023-06-25 10:13:58,371][129146] Moving the mean solution point...
[36m[2023-06-25 10:14:08,148][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 10:14:08,148][129146] FPS: 392825.21
[36m[2023-06-25 10:14:08,151][129146] itr=1007, itrs=2000, Progress: 50.35%
[36m[2023-06-25 10:14:19,772][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 10:14:19,772][129146] FPS: 331075.01
[36m[2023-06-25 10:14:24,579][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:14:24,580][129146] Reward + Measures: [[1881.62544344    0.30957124    0.34179068    0.09229016    0.44266695]]
[37m[1m[2023-06-25 10:14:24,580][129146] Max Reward on eval: 1881.625443439022
[37m[1m[2023-06-25 10:14:24,580][129146] Min Reward on eval: 1881.625443439022
[37m[1m[2023-06-25 10:14:24,581][129146] Mean Reward across all agents: 1881.625443439022
[37m[1m[2023-06-25 10:14:24,581][129146] Average Trajectory Length: 994.3883333333333
[36m[2023-06-25 10:14:30,016][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:14:30,022][129146] Reward + Measures: [[1203.71195801    0.30510002    0.42299995    0.14690001    0.51340002]
[37m[1m [1393.2973312     0.2714        0.36159998    0.12670001    0.44390002]
[37m[1m [1001.2594749     0.30089998    0.4526        0.14839999    0.49810001]
[37m[1m ...
[37m[1m [1253.0001371     0.3109        0.4021        0.1619        0.53900003]
[37m[1m [1388.34039783    0.36129999    0.38730001    0.0953        0.50029999]
[37m[1m [1224.35477098    0.31459999    0.42570001    0.133         0.52899998]]
[37m[1m[2023-06-25 10:14:30,022][129146] Max Reward on eval: 1946.6744926208164
[37m[1m[2023-06-25 10:14:30,022][129146] Min Reward on eval: 459.5123113379814
[37m[1m[2023-06-25 10:14:30,023][129146] Mean Reward across all agents: 1275.5346514094485
[37m[1m[2023-06-25 10:14:30,023][129146] Average Trajectory Length: 998.5466666666666
[36m[2023-06-25 10:14:30,026][129146] mean_value=-475.23639442843154, max_value=913.1995886851024
[37m[1m[2023-06-25 10:14:30,028][129146] New mean coefficients: [[ 0.87183994 -2.5631752  -0.10820025  0.11396068  5.5717974 ]]
[37m[1m[2023-06-25 10:14:30,029][129146] Moving the mean solution point...
[36m[2023-06-25 10:14:39,786][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 10:14:39,786][129146] FPS: 393645.69
[36m[2023-06-25 10:14:39,788][129146] itr=1008, itrs=2000, Progress: 50.40%
[36m[2023-06-25 10:14:51,368][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 10:14:51,368][129146] FPS: 332277.39
[36m[2023-06-25 10:14:56,218][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:14:56,218][129146] Reward + Measures: [[1937.24181868    0.30300161    0.35161719    0.10272105    0.47644281]]
[37m[1m[2023-06-25 10:14:56,219][129146] Max Reward on eval: 1937.2418186811115
[37m[1m[2023-06-25 10:14:56,219][129146] Min Reward on eval: 1937.2418186811115
[37m[1m[2023-06-25 10:14:56,219][129146] Mean Reward across all agents: 1937.2418186811115
[37m[1m[2023-06-25 10:14:56,220][129146] Average Trajectory Length: 996.3753333333333
[36m[2023-06-25 10:15:01,731][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:15:01,732][129146] Reward + Measures: [[1813.85992422    0.301         0.43009996    0.15010002    0.48539996]
[37m[1m [ 604.68061546    0.2872        0.29790002    0.1653        0.34240004]
[37m[1m [ 404.06864361    0.27340001    0.68990004    0.1023        0.80150002]
[37m[1m ...
[37m[1m [ 570.89545355    0.30710003    0.67540008    0.0887        0.77930003]
[37m[1m [ 889.78025682    0.32769999    0.4298        0.2131        0.57140005]
[37m[1m [ -18.8402199     0.33723477    0.39661136    0.26678488    0.42122003]]
[37m[1m[2023-06-25 10:15:01,732][129146] Max Reward on eval: 1997.2420457207365
[37m[1m[2023-06-25 10:15:01,733][129146] Min Reward on eval: -1137.3697838879423
[37m[1m[2023-06-25 10:15:01,733][129146] Mean Reward across all agents: 588.8007187567443
[37m[1m[2023-06-25 10:15:01,733][129146] Average Trajectory Length: 987.2716666666666
[36m[2023-06-25 10:15:01,737][129146] mean_value=-1041.571029946075, max_value=1597.0821981160784
[37m[1m[2023-06-25 10:15:01,739][129146] New mean coefficients: [[ 0.20325142 -1.8161613  -1.2453465   1.3340733   5.776601  ]]
[37m[1m[2023-06-25 10:15:01,740][129146] Moving the mean solution point...
[36m[2023-06-25 10:15:11,491][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 10:15:11,491][129146] FPS: 393888.59
[36m[2023-06-25 10:15:11,494][129146] itr=1009, itrs=2000, Progress: 50.45%
[36m[2023-06-25 10:15:23,118][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 10:15:23,118][129146] FPS: 330988.25
[36m[2023-06-25 10:15:27,974][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:15:27,974][129146] Reward + Measures: [[1821.31892932    0.29696295    0.37201539    0.11229338    0.53922755]]
[37m[1m[2023-06-25 10:15:27,975][129146] Max Reward on eval: 1821.3189293196297
[37m[1m[2023-06-25 10:15:27,975][129146] Min Reward on eval: 1821.3189293196297
[37m[1m[2023-06-25 10:15:27,975][129146] Mean Reward across all agents: 1821.3189293196297
[37m[1m[2023-06-25 10:15:27,975][129146] Average Trajectory Length: 999.74
[36m[2023-06-25 10:15:33,455][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:15:33,456][129146] Reward + Measures: [[  808.85731885     0.31080002     0.39690003     0.1743
[37m[1m      0.52220005]
[37m[1m [  467.35974        0.3294         0.41339999     0.24510001
[37m[1m      0.4461    ]
[37m[1m [  278.88272063     0.42130002     0.59510005     0.23899999
[37m[1m      0.68160003]
[37m[1m ...
[37m[1m [  663.61822079     0.38350001     0.52590001     0.1303
[37m[1m      0.6573    ]
[37m[1m [   68.68145682     0.17780001     0.28010002     0.23099999
[37m[1m      0.28099999]
[37m[1m [-1679.73031904     0.85100001     0.0939         0.83969992
[37m[1m      0.34660003]]
[37m[1m[2023-06-25 10:15:33,456][129146] Max Reward on eval: 1766.1597864229902
[37m[1m[2023-06-25 10:15:33,456][129146] Min Reward on eval: -1679.7303190419684
[37m[1m[2023-06-25 10:15:33,457][129146] Mean Reward across all agents: 50.24230145736846
[37m[1m[2023-06-25 10:15:33,457][129146] Average Trajectory Length: 997.9093333333333
[36m[2023-06-25 10:15:33,459][129146] mean_value=-768.0948848131451, max_value=1159.2056923510918
[37m[1m[2023-06-25 10:15:33,462][129146] New mean coefficients: [[-0.46167684 -1.0988555  -0.566271    0.15408301  5.472235  ]]
[37m[1m[2023-06-25 10:15:33,463][129146] Moving the mean solution point...
[36m[2023-06-25 10:15:43,275][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 10:15:43,275][129146] FPS: 391409.98
[36m[2023-06-25 10:15:43,278][129146] itr=1010, itrs=2000, Progress: 50.50%
[37m[1m[2023-06-25 10:15:50,034][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000990
[36m[2023-06-25 10:16:01,896][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 10:16:01,896][129146] FPS: 330171.89
[36m[2023-06-25 10:16:06,800][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:16:06,800][129146] Reward + Measures: [[1460.10695225    0.28988406    0.40574527    0.11366283    0.62796402]]
[37m[1m[2023-06-25 10:16:06,801][129146] Max Reward on eval: 1460.1069522515224
[37m[1m[2023-06-25 10:16:06,801][129146] Min Reward on eval: 1460.1069522515224
[37m[1m[2023-06-25 10:16:06,801][129146] Mean Reward across all agents: 1460.1069522515224
[37m[1m[2023-06-25 10:16:06,801][129146] Average Trajectory Length: 999.2043333333334
[36m[2023-06-25 10:16:12,523][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:16:12,524][129146] Reward + Measures: [[ 521.55467949    0.41560003    0.5169        0.18410002    0.66119999]
[37m[1m [ 715.95877584    0.35569999    0.51620001    0.15689999    0.6638    ]
[37m[1m [ 626.61591126    0.32890001    0.465         0.14359999    0.60170001]
[37m[1m ...
[37m[1m [1050.76071261    0.30899999    0.46070004    0.2323        0.60210001]
[37m[1m [ 484.2085506     0.32099998    0.54610002    0.1097        0.71200001]
[37m[1m [ 590.5657163     0.26749998    0.4052        0.32230002    0.46300003]]
[37m[1m[2023-06-25 10:16:12,524][129146] Max Reward on eval: 1389.718247054494
[37m[1m[2023-06-25 10:16:12,525][129146] Min Reward on eval: -1122.7259692874736
[37m[1m[2023-06-25 10:16:12,525][129146] Mean Reward across all agents: 628.84155311857
[37m[1m[2023-06-25 10:16:12,525][129146] Average Trajectory Length: 996.9259999999999
[36m[2023-06-25 10:16:12,528][129146] mean_value=-447.49815380709947, max_value=1802.858392286938
[37m[1m[2023-06-25 10:16:12,531][129146] New mean coefficients: [[-0.287823   -1.4365232  -0.44339937  0.36379367  4.572499  ]]
[37m[1m[2023-06-25 10:16:12,532][129146] Moving the mean solution point...
[36m[2023-06-25 10:16:22,264][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 10:16:22,264][129146] FPS: 394633.96
[36m[2023-06-25 10:16:22,266][129146] itr=1011, itrs=2000, Progress: 50.55%
[36m[2023-06-25 10:16:33,782][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 10:16:33,782][129146] FPS: 334111.10
[36m[2023-06-25 10:16:38,656][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:16:38,657][129146] Reward + Measures: [[1072.14104994    0.30352202    0.48274365    0.10860699    0.71964097]]
[37m[1m[2023-06-25 10:16:38,657][129146] Max Reward on eval: 1072.1410499420099
[37m[1m[2023-06-25 10:16:38,657][129146] Min Reward on eval: 1072.1410499420099
[37m[1m[2023-06-25 10:16:38,658][129146] Mean Reward across all agents: 1072.1410499420099
[37m[1m[2023-06-25 10:16:38,658][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:16:44,079][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:16:44,080][129146] Reward + Measures: [[  436.6510866      0.35439998     0.5007         0.36789998
[37m[1m      0.61830008]
[37m[1m [  173.51429951     0.2493         0.4522         0.2287
[37m[1m      0.40079999]
[37m[1m [-1112.66674618     0.8057         0.71820003     0.94410002
[37m[1m      0.92250007]
[37m[1m ...
[37m[1m [  -94.65398686     0.23800002     0.5352         0.2
[37m[1m      0.39089999]
[37m[1m [ -245.44703733     0.43090001     0.58339995     0.6663
[37m[1m      0.75369996]
[37m[1m [  807.85993666     0.36219999     0.58880001     0.1104
[37m[1m      0.68950003]]
[37m[1m[2023-06-25 10:16:44,080][129146] Max Reward on eval: 1324.646038356144
[37m[1m[2023-06-25 10:16:44,080][129146] Min Reward on eval: -1675.9690011350438
[37m[1m[2023-06-25 10:16:44,081][129146] Mean Reward across all agents: 436.2341347717044
[37m[1m[2023-06-25 10:16:44,081][129146] Average Trajectory Length: 997.8446666666666
[36m[2023-06-25 10:16:44,083][129146] mean_value=-731.6446977679992, max_value=1272.3090611433747
[37m[1m[2023-06-25 10:16:44,086][129146] New mean coefficients: [[-0.38570285 -1.0499909  -0.14077652 -0.42281568  4.3336735 ]]
[37m[1m[2023-06-25 10:16:44,087][129146] Moving the mean solution point...
[36m[2023-06-25 10:16:53,736][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 10:16:53,736][129146] FPS: 398049.14
[36m[2023-06-25 10:16:53,738][129146] itr=1012, itrs=2000, Progress: 50.60%
[36m[2023-06-25 10:17:05,202][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 10:17:05,203][129146] FPS: 335697.48
[36m[2023-06-25 10:17:09,844][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:17:09,845][129146] Reward + Measures: [[767.32381493   0.33595732   0.57595634   0.087915     0.79513597]]
[37m[1m[2023-06-25 10:17:09,845][129146] Max Reward on eval: 767.3238149257813
[37m[1m[2023-06-25 10:17:09,845][129146] Min Reward on eval: 767.3238149257813
[37m[1m[2023-06-25 10:17:09,846][129146] Mean Reward across all agents: 767.3238149257813
[37m[1m[2023-06-25 10:17:09,846][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:17:15,340][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:17:15,341][129146] Reward + Measures: [[ 615.51096149    0.32690001    0.4928        0.1655        0.65000004]
[37m[1m [-412.32556697    0.4427        0.47540003    0.53530002    0.59359998]
[37m[1m [ 588.46093606    0.3687        0.64679998    0.0922        0.81280005]
[37m[1m ...
[37m[1m [ 282.49682432    0.50489998    0.30590001    0.51790005    0.47930002]
[37m[1m [ 241.84351577    0.38279998    0.82730001    0.1432        0.88360006]
[37m[1m [-234.5377052     0.34930003    0.43099999    0.47010002    0.61520004]]
[37m[1m[2023-06-25 10:17:15,341][129146] Max Reward on eval: 1150.4546990274453
[37m[1m[2023-06-25 10:17:15,341][129146] Min Reward on eval: -787.7572583483067
[37m[1m[2023-06-25 10:17:15,342][129146] Mean Reward across all agents: 289.4179489357328
[37m[1m[2023-06-25 10:17:15,342][129146] Average Trajectory Length: 995.3226666666666
[36m[2023-06-25 10:17:15,347][129146] mean_value=62.256986919189146, max_value=1441.5313504944884
[37m[1m[2023-06-25 10:17:15,350][129146] New mean coefficients: [[ 0.17857468 -0.32096165  0.2839245   0.15898556  3.9714794 ]]
[37m[1m[2023-06-25 10:17:15,351][129146] Moving the mean solution point...
[36m[2023-06-25 10:17:25,257][129146] train() took 9.90 seconds to complete
[36m[2023-06-25 10:17:25,257][129146] FPS: 387715.03
[36m[2023-06-25 10:17:25,260][129146] itr=1013, itrs=2000, Progress: 50.65%
[36m[2023-06-25 10:17:36,779][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 10:17:36,779][129146] FPS: 334042.10
[36m[2023-06-25 10:17:41,610][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:17:41,610][129146] Reward + Measures: [[600.05187759   0.37829036   0.67472035   0.07063667   0.85379928]]
[37m[1m[2023-06-25 10:17:41,611][129146] Max Reward on eval: 600.0518775897299
[37m[1m[2023-06-25 10:17:41,611][129146] Min Reward on eval: 600.0518775897299
[37m[1m[2023-06-25 10:17:41,611][129146] Mean Reward across all agents: 600.0518775897299
[37m[1m[2023-06-25 10:17:41,611][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:17:47,041][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:17:47,042][129146] Reward + Measures: [[ 420.24794518    0.44449997    0.44290003    0.0254        0.60490006]
[37m[1m [ -98.84884447    0.3328        0.62490004    0.3066        0.634     ]
[37m[1m [ 332.0050296     0.26589999    0.52220005    0.43309999    0.5851    ]
[37m[1m ...
[37m[1m [-142.11769061    0.3511        0.57370001    0.31220001    0.55410004]
[37m[1m [ 737.31003644    0.31479999    0.45339999    0.37680003    0.52279997]
[37m[1m [ 509.44143917    0.34200001    0.60000002    0.21170001    0.71560001]]
[37m[1m[2023-06-25 10:17:47,042][129146] Max Reward on eval: 1261.725026912312
[37m[1m[2023-06-25 10:17:47,042][129146] Min Reward on eval: -1116.0054482253968
[37m[1m[2023-06-25 10:17:47,042][129146] Mean Reward across all agents: 332.0797607748159
[37m[1m[2023-06-25 10:17:47,043][129146] Average Trajectory Length: 998.4403333333333
[36m[2023-06-25 10:17:47,047][129146] mean_value=-335.4805128080254, max_value=1148.035087441883
[37m[1m[2023-06-25 10:17:47,049][129146] New mean coefficients: [[ 0.6980709   1.0630298  -0.15458053  0.01077494  3.961855  ]]
[37m[1m[2023-06-25 10:17:47,050][129146] Moving the mean solution point...
[36m[2023-06-25 10:17:56,805][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 10:17:56,805][129146] FPS: 393736.63
[36m[2023-06-25 10:17:56,807][129146] itr=1014, itrs=2000, Progress: 50.70%
[36m[2023-06-25 10:18:08,340][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 10:18:08,340][129146] FPS: 333629.38
[36m[2023-06-25 10:18:13,015][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:18:13,016][129146] Reward + Measures: [[562.27894531   0.42390168   0.73104769   0.04940933   0.88601667]]
[37m[1m[2023-06-25 10:18:13,016][129146] Max Reward on eval: 562.2789453107695
[37m[1m[2023-06-25 10:18:13,016][129146] Min Reward on eval: 562.2789453107695
[37m[1m[2023-06-25 10:18:13,017][129146] Mean Reward across all agents: 562.2789453107695
[37m[1m[2023-06-25 10:18:13,017][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:18:18,403][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:18:18,404][129146] Reward + Measures: [[ 515.74520331    0.3258        0.53759998    0.3876        0.69080001]
[37m[1m [ 104.33542258    0.3856        0.50940001    0.3585        0.56169999]
[37m[1m [ 144.36354969    0.42349997    0.75040001    0.20220001    0.82369995]
[37m[1m ...
[37m[1m [ 323.38805117    0.43380004    0.74419999    0.0684        0.829     ]
[37m[1m [-721.94690886    0.33500001    0.4355        0.48160002    0.43590003]
[37m[1m [-503.20552939    0.3177        0.4113        0.44619998    0.41540003]]
[37m[1m[2023-06-25 10:18:18,404][129146] Max Reward on eval: 836.4278122627293
[37m[1m[2023-06-25 10:18:18,405][129146] Min Reward on eval: -942.7652877646163
[37m[1m[2023-06-25 10:18:18,405][129146] Mean Reward across all agents: 113.30843225076792
[37m[1m[2023-06-25 10:18:18,405][129146] Average Trajectory Length: 996.1469999999999
[36m[2023-06-25 10:18:18,409][129146] mean_value=-479.4138835281881, max_value=1121.58696758484
[37m[1m[2023-06-25 10:18:18,412][129146] New mean coefficients: [[-0.659274   -0.4025613   0.34246817 -0.03536509  3.8954353 ]]
[37m[1m[2023-06-25 10:18:18,413][129146] Moving the mean solution point...
[36m[2023-06-25 10:18:28,195][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 10:18:28,196][129146] FPS: 392619.35
[36m[2023-06-25 10:18:28,198][129146] itr=1015, itrs=2000, Progress: 50.75%
[36m[2023-06-25 10:18:39,643][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 10:18:39,644][129146] FPS: 336173.18
[36m[2023-06-25 10:18:44,360][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:18:44,361][129146] Reward + Measures: [[454.61944043   0.48599067   0.8155604    0.03856666   0.92076862]]
[37m[1m[2023-06-25 10:18:44,361][129146] Max Reward on eval: 454.61944042796154
[37m[1m[2023-06-25 10:18:44,361][129146] Min Reward on eval: 454.61944042796154
[37m[1m[2023-06-25 10:18:44,362][129146] Mean Reward across all agents: 454.61944042796154
[37m[1m[2023-06-25 10:18:44,362][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:18:49,933][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:18:49,933][129146] Reward + Measures: [[212.25346164   0.3897       0.83710003   0.40530005   0.91759998]
[37m[1m [301.35862911   0.47980005   0.83409995   0.0512       0.91769999]
[37m[1m [388.17987358   0.47220001   0.82159996   0.0465       0.92570001]
[37m[1m ...
[37m[1m [444.66707691   0.48319998   0.7432       0.1019       0.84559995]
[37m[1m [246.71417807   0.50470001   0.81549996   0.08090001   0.88070005]
[37m[1m [407.91822362   0.47540003   0.82540005   0.0641       0.91280001]]
[37m[1m[2023-06-25 10:18:49,933][129146] Max Reward on eval: 810.4702908752137
[37m[1m[2023-06-25 10:18:49,934][129146] Min Reward on eval: 105.46406838304829
[37m[1m[2023-06-25 10:18:49,934][129146] Mean Reward across all agents: 338.36107435591055
[37m[1m[2023-06-25 10:18:49,934][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:18:49,940][129146] mean_value=179.50314875893724, max_value=885.6989962716237
[37m[1m[2023-06-25 10:18:49,943][129146] New mean coefficients: [[-0.2208094  -1.2616184   0.48915356  0.20401855  4.439584  ]]
[37m[1m[2023-06-25 10:18:49,944][129146] Moving the mean solution point...
[36m[2023-06-25 10:18:59,713][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 10:18:59,714][129146] FPS: 393127.50
[36m[2023-06-25 10:18:59,716][129146] itr=1016, itrs=2000, Progress: 50.80%
[36m[2023-06-25 10:19:11,204][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 10:19:11,204][129146] FPS: 334905.44
[36m[2023-06-25 10:19:15,875][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:19:15,875][129146] Reward + Measures: [[428.54406889   0.43631265   0.82505232   0.053692     0.92847502]]
[37m[1m[2023-06-25 10:19:15,876][129146] Max Reward on eval: 428.5440688949061
[37m[1m[2023-06-25 10:19:15,876][129146] Min Reward on eval: 428.5440688949061
[37m[1m[2023-06-25 10:19:15,876][129146] Mean Reward across all agents: 428.5440688949061
[37m[1m[2023-06-25 10:19:15,876][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:19:21,280][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:19:21,281][129146] Reward + Measures: [[139.15200779   0.46359998   0.51310003   0.35500002   0.60580003]
[37m[1m [150.21130835   0.204        0.73880005   0.16469999   0.86370003]
[37m[1m [531.26949083   0.44689998   0.41260004   0.40900001   0.54980004]
[37m[1m ...
[37m[1m [431.7222353    0.33580002   0.7554       0.1355       0.90690005]
[37m[1m [211.66731078   0.40240002   0.56610006   0.40429997   0.67099994]
[37m[1m [322.67528086   0.47440001   0.42570001   0.46350002   0.57250005]]
[37m[1m[2023-06-25 10:19:21,281][129146] Max Reward on eval: 887.8867303828476
[37m[1m[2023-06-25 10:19:21,281][129146] Min Reward on eval: -712.3951733500347
[37m[1m[2023-06-25 10:19:21,282][129146] Mean Reward across all agents: 63.41281215835132
[37m[1m[2023-06-25 10:19:21,282][129146] Average Trajectory Length: 997.8133333333333
[36m[2023-06-25 10:19:21,288][129146] mean_value=-119.22803264887892, max_value=1092.6464543470065
[37m[1m[2023-06-25 10:19:21,291][129146] New mean coefficients: [[ 0.052486   -1.44327     0.6843074   0.06908345  4.526675  ]]
[37m[1m[2023-06-25 10:19:21,292][129146] Moving the mean solution point...
[36m[2023-06-25 10:19:30,983][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 10:19:30,983][129146] FPS: 396308.14
[36m[2023-06-25 10:19:30,986][129146] itr=1017, itrs=2000, Progress: 50.85%
[36m[2023-06-25 10:19:42,415][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 10:19:42,415][129146] FPS: 336750.55
[36m[2023-06-25 10:19:47,123][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:19:47,123][129146] Reward + Measures: [[450.54436227   0.38535529   0.8140856    0.06126367   0.93019402]]
[37m[1m[2023-06-25 10:19:47,123][129146] Max Reward on eval: 450.54436226708134
[37m[1m[2023-06-25 10:19:47,124][129146] Min Reward on eval: 450.54436226708134
[37m[1m[2023-06-25 10:19:47,124][129146] Mean Reward across all agents: 450.54436226708134
[37m[1m[2023-06-25 10:19:47,124][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:19:52,525][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:19:52,525][129146] Reward + Measures: [[-293.85838119    0.0885        0.82700008    0.84079999    0.91110003]
[37m[1m [ 376.37453429    0.47139999    0.61700004    0.15180001    0.76210004]
[37m[1m [ 471.12667655    0.41700003    0.83499998    0.15140001    0.91079998]
[37m[1m ...
[37m[1m [ 358.1377429     0.3274        0.75000006    0.1851        0.84909993]
[37m[1m [ 238.93001871    0.41270003    0.68829995    0.50949997    0.73309994]
[37m[1m [ 518.36143362    0.34890002    0.70990002    0.18499999    0.86180001]]
[37m[1m[2023-06-25 10:19:52,526][129146] Max Reward on eval: 614.7618506530358
[37m[1m[2023-06-25 10:19:52,526][129146] Min Reward on eval: -752.923862325307
[37m[1m[2023-06-25 10:19:52,526][129146] Mean Reward across all agents: 299.7316691500025
[37m[1m[2023-06-25 10:19:52,526][129146] Average Trajectory Length: 999.8723333333332
[36m[2023-06-25 10:19:52,532][129146] mean_value=103.9454677774819, max_value=1114.7618506530357
[37m[1m[2023-06-25 10:19:52,535][129146] New mean coefficients: [[ 0.00944284 -1.5253757   1.0999568  -0.00180868  4.8560114 ]]
[37m[1m[2023-06-25 10:19:52,536][129146] Moving the mean solution point...
[36m[2023-06-25 10:20:02,273][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 10:20:02,273][129146] FPS: 394440.17
[36m[2023-06-25 10:20:02,276][129146] itr=1018, itrs=2000, Progress: 50.90%
[36m[2023-06-25 10:20:13,696][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 10:20:13,696][129146] FPS: 336940.46
[36m[2023-06-25 10:20:18,574][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:20:18,574][129146] Reward + Measures: [[453.34322399   0.33728999   0.82652032   0.08763466   0.93463266]]
[37m[1m[2023-06-25 10:20:18,574][129146] Max Reward on eval: 453.34322398681115
[37m[1m[2023-06-25 10:20:18,574][129146] Min Reward on eval: 453.34322398681115
[37m[1m[2023-06-25 10:20:18,574][129146] Mean Reward across all agents: 453.34322398681115
[37m[1m[2023-06-25 10:20:18,575][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:20:24,051][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:20:24,052][129146] Reward + Measures: [[  277.98979521     0.38690001     0.85579997     0.20869999
[37m[1m      0.91830009]
[37m[1m [ -111.2477872      0.56779999     0.84450001     0.36980003
[37m[1m      0.91250002]
[37m[1m [-1091.36322694     0.92620003     0.0861         0.91689998
[37m[1m      0.80310005]
[37m[1m ...
[37m[1m [ -894.76788896     0.96789998     0.0086         0.97080004
[37m[1m      0.78549999]
[37m[1m [-1379.14266443     0.90469998     0.06040001     0.90150005
[37m[1m      0.76389998]
[37m[1m [-1244.19243672     0.94759446     0.01633236     0.95590639
[37m[1m      0.79143614]]
[37m[1m[2023-06-25 10:20:24,052][129146] Max Reward on eval: 1188.904589370091
[37m[1m[2023-06-25 10:20:24,052][129146] Min Reward on eval: -2375.237856531504
[37m[1m[2023-06-25 10:20:24,052][129146] Mean Reward across all agents: -111.14368462779603
[37m[1m[2023-06-25 10:20:24,053][129146] Average Trajectory Length: 998.8729999999999
[36m[2023-06-25 10:20:24,057][129146] mean_value=-556.4620798432218, max_value=1391.1468625773443
[37m[1m[2023-06-25 10:20:24,060][129146] New mean coefficients: [[ 0.8722865  -0.55523396  1.389467    0.24008599  4.549631  ]]
[37m[1m[2023-06-25 10:20:24,061][129146] Moving the mean solution point...
[36m[2023-06-25 10:20:33,764][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 10:20:33,764][129146] FPS: 395825.32
[36m[2023-06-25 10:20:33,767][129146] itr=1019, itrs=2000, Progress: 50.95%
[36m[2023-06-25 10:20:45,203][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 10:20:45,203][129146] FPS: 336484.01
[36m[2023-06-25 10:20:50,041][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:20:50,042][129146] Reward + Measures: [[472.6584171    0.32033834   0.83972031   0.09650533   0.94154263]]
[37m[1m[2023-06-25 10:20:50,042][129146] Max Reward on eval: 472.65841709519486
[37m[1m[2023-06-25 10:20:50,042][129146] Min Reward on eval: 472.65841709519486
[37m[1m[2023-06-25 10:20:50,043][129146] Mean Reward across all agents: 472.65841709519486
[37m[1m[2023-06-25 10:20:50,043][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:20:55,555][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:20:55,608][129146] Reward + Measures: [[ 550.64185925    0.39360002    0.80500001    0.0506        0.92320007]
[37m[1m [-530.5992158     0.45830002    0.4226        0.3466        0.4436    ]
[37m[1m [ 112.5961849     0.40279999    0.78080004    0.20369999    0.81989998]
[37m[1m ...
[37m[1m [-182.22939255    0.7353        0.2538        0.6669001     0.40239999]
[37m[1m [ 490.28842648    0.42050001    0.65130007    0.11860001    0.7626    ]
[37m[1m [ 449.1989783     0.29100001    0.72430003    0.2369        0.83389997]]
[37m[1m[2023-06-25 10:20:55,608][129146] Max Reward on eval: 883.0826601726003
[37m[1m[2023-06-25 10:20:55,608][129146] Min Reward on eval: -530.5992157957633
[37m[1m[2023-06-25 10:20:55,609][129146] Mean Reward across all agents: 265.50835233786717
[37m[1m[2023-06-25 10:20:55,609][129146] Average Trajectory Length: 999.3056666666666
[36m[2023-06-25 10:20:55,615][129146] mean_value=-35.2942102443934, max_value=854.60225638366
[37m[1m[2023-06-25 10:20:55,617][129146] New mean coefficients: [[ 0.39048678 -0.05930963  1.5619287   0.45585638  4.671361  ]]
[37m[1m[2023-06-25 10:20:55,618][129146] Moving the mean solution point...
[36m[2023-06-25 10:21:05,367][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 10:21:05,368][129146] FPS: 393939.57
[36m[2023-06-25 10:21:05,370][129146] itr=1020, itrs=2000, Progress: 51.00%
[37m[1m[2023-06-25 10:21:12,218][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001000
[36m[2023-06-25 10:21:24,013][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 10:21:24,014][129146] FPS: 331942.35
[36m[2023-06-25 10:21:28,857][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:21:28,858][129146] Reward + Measures: [[483.83812899   0.30600834   0.85943937   0.12421799   0.94562799]]
[37m[1m[2023-06-25 10:21:28,858][129146] Max Reward on eval: 483.8381289933041
[37m[1m[2023-06-25 10:21:28,858][129146] Min Reward on eval: 483.8381289933041
[37m[1m[2023-06-25 10:21:28,858][129146] Mean Reward across all agents: 483.8381289933041
[37m[1m[2023-06-25 10:21:28,858][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:21:34,196][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:21:34,197][129146] Reward + Measures: [[397.29321289   0.38260004   0.73920006   0.34829998   0.84750003]
[37m[1m [450.80811274   0.41820002   0.73029995   0.31130001   0.86189997]
[37m[1m [464.09425635   0.29720002   0.75040007   0.20200001   0.86330003]
[37m[1m ...
[37m[1m [341.11354728   0.4172       0.88099998   0.61449999   0.92989999]
[37m[1m [468.05952752   0.37690002   0.77460003   0.0604       0.90459996]
[37m[1m [415.87608527   0.37909999   0.83219999   0.0501       0.92770004]]
[37m[1m[2023-06-25 10:21:34,197][129146] Max Reward on eval: 572.2912303843186
[37m[1m[2023-06-25 10:21:34,197][129146] Min Reward on eval: 187.15873595081501
[37m[1m[2023-06-25 10:21:34,198][129146] Mean Reward across all agents: 433.9289805023459
[37m[1m[2023-06-25 10:21:34,198][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:21:34,206][129146] mean_value=446.093696765122, max_value=993.1869356802432
[37m[1m[2023-06-25 10:21:34,209][129146] New mean coefficients: [[0.625539   0.10816604 2.6194682  0.3934522  3.0471735 ]]
[37m[1m[2023-06-25 10:21:34,210][129146] Moving the mean solution point...
[36m[2023-06-25 10:21:43,858][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 10:21:43,858][129146] FPS: 398075.17
[36m[2023-06-25 10:21:43,860][129146] itr=1021, itrs=2000, Progress: 51.05%
[36m[2023-06-25 10:21:55,442][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 10:21:55,442][129146] FPS: 332238.81
[36m[2023-06-25 10:22:00,233][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:22:00,234][129146] Reward + Measures: [[481.68823458   0.27430233   0.88895541   0.28797266   0.95204532]]
[37m[1m[2023-06-25 10:22:00,234][129146] Max Reward on eval: 481.6882345786804
[37m[1m[2023-06-25 10:22:00,234][129146] Min Reward on eval: 481.6882345786804
[37m[1m[2023-06-25 10:22:00,235][129146] Mean Reward across all agents: 481.6882345786804
[37m[1m[2023-06-25 10:22:00,235][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:22:05,813][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:22:05,814][129146] Reward + Measures: [[ 345.86985898    0.66030002    0.45629999    0.64969999    0.5237    ]
[37m[1m [ 535.53288904    0.5244        0.78290004    0.48559999    0.77960002]
[37m[1m [-618.2631758     0.58649999    0.35780001    0.57080001    0.5643    ]
[37m[1m ...
[37m[1m [ -87.76677284    0.83140004    0.88679999    0.85249996    0.92469996]
[37m[1m [  30.1816104     0.63150001    0.57319993    0.53509998    0.68470001]
[37m[1m [ -10.95079207    0.47060004    0.7985        0.1895        0.89270002]]
[37m[1m[2023-06-25 10:22:05,814][129146] Max Reward on eval: 909.9287354994565
[37m[1m[2023-06-25 10:22:05,814][129146] Min Reward on eval: -2723.658139022044
[37m[1m[2023-06-25 10:22:05,815][129146] Mean Reward across all agents: -245.89375629596984
[37m[1m[2023-06-25 10:22:05,815][129146] Average Trajectory Length: 999.0976666666667
[36m[2023-06-25 10:22:05,819][129146] mean_value=-541.1832856950756, max_value=852.2254005015129
[37m[1m[2023-06-25 10:22:05,822][129146] New mean coefficients: [[ 0.39082387 -0.43680334  2.9235914   0.7386019   3.1450913 ]]
[37m[1m[2023-06-25 10:22:05,823][129146] Moving the mean solution point...
[36m[2023-06-25 10:22:15,711][129146] train() took 9.89 seconds to complete
[36m[2023-06-25 10:22:15,711][129146] FPS: 388424.80
[36m[2023-06-25 10:22:15,713][129146] itr=1022, itrs=2000, Progress: 51.10%
[36m[2023-06-25 10:22:27,237][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 10:22:27,237][129146] FPS: 333897.54
[36m[2023-06-25 10:22:32,004][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:22:32,005][129146] Reward + Measures: [[498.7603864    0.20977633   0.92679465   0.60420501   0.96017635]]
[37m[1m[2023-06-25 10:22:32,005][129146] Max Reward on eval: 498.7603863996239
[37m[1m[2023-06-25 10:22:32,005][129146] Min Reward on eval: 498.7603863996239
[37m[1m[2023-06-25 10:22:32,005][129146] Mean Reward across all agents: 498.7603863996239
[37m[1m[2023-06-25 10:22:32,006][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:22:37,500][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:22:37,501][129146] Reward + Measures: [[ -886.36279052     0.63639998     0.39120001     0.57110006
[37m[1m      0.44409999]
[37m[1m [-1259.24368616     0.86119998     0.79020005     0.82240003
[37m[1m      0.83840001]
[37m[1m [ -197.17900712     0.0937         0.83880007     0.90830004
[37m[1m      0.95319998]
[37m[1m ...
[37m[1m [ -735.44669729     0.91429996     0.4664         0.93640006
[37m[1m      0.88910007]
[37m[1m [  947.42159232     0.2775         0.60900003     0.1982
[37m[1m      0.64230001]
[37m[1m [ -930.69533332     0.91860002     0.0033         0.95359993
[37m[1m      0.83549994]]
[37m[1m[2023-06-25 10:22:37,501][129146] Max Reward on eval: 1152.4523569124635
[37m[1m[2023-06-25 10:22:37,501][129146] Min Reward on eval: -1363.9671155527699
[37m[1m[2023-06-25 10:22:37,501][129146] Mean Reward across all agents: -140.17920181178877
[37m[1m[2023-06-25 10:22:37,501][129146] Average Trajectory Length: 998.9526666666667
[36m[2023-06-25 10:22:37,506][129146] mean_value=-516.876274261427, max_value=888.1020294699701
[37m[1m[2023-06-25 10:22:37,509][129146] New mean coefficients: [[0.61412036 0.5831063  2.1390653  0.24669352 2.727116  ]]
[37m[1m[2023-06-25 10:22:37,510][129146] Moving the mean solution point...
[36m[2023-06-25 10:22:47,262][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 10:22:47,263][129146] FPS: 393805.34
[36m[2023-06-25 10:22:47,265][129146] itr=1023, itrs=2000, Progress: 51.15%
[36m[2023-06-25 10:22:58,681][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 10:22:58,682][129146] FPS: 337155.62
[36m[2023-06-25 10:23:03,450][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:23:03,450][129146] Reward + Measures: [[475.0628859    0.36468637   0.95736402   0.79897565   0.96990073]]
[37m[1m[2023-06-25 10:23:03,451][129146] Max Reward on eval: 475.062885903966
[37m[1m[2023-06-25 10:23:03,451][129146] Min Reward on eval: 475.062885903966
[37m[1m[2023-06-25 10:23:03,451][129146] Mean Reward across all agents: 475.062885903966
[37m[1m[2023-06-25 10:23:03,451][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:23:08,892][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:23:08,893][129146] Reward + Measures: [[ -4.189069     0.58250004   0.66520005   0.6128       0.79790002]
[37m[1m [-94.15038811   0.78169996   0.54530001   0.7899       0.81879997]
[37m[1m [101.98563682   0.39590001   0.69589996   0.52889997   0.81050009]
[37m[1m ...
[37m[1m [137.00278827   0.454        0.86960012   0.1227       0.88749999]
[37m[1m [ 69.31938622   0.44310004   0.76400006   0.84300005   0.89700001]
[37m[1m [-96.86856292   0.70450002   0.65539998   0.65290004   0.83149999]]
[37m[1m[2023-06-25 10:23:08,893][129146] Max Reward on eval: 1087.2381270804558
[37m[1m[2023-06-25 10:23:08,893][129146] Min Reward on eval: -1085.5615558989346
[37m[1m[2023-06-25 10:23:08,894][129146] Mean Reward across all agents: -87.21223027443666
[37m[1m[2023-06-25 10:23:08,894][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:23:08,899][129146] mean_value=-327.1299428927336, max_value=945.5653764841379
[37m[1m[2023-06-25 10:23:08,902][129146] New mean coefficients: [[0.45653892 0.11551422 2.5606499  0.15511768 2.5738914 ]]
[37m[1m[2023-06-25 10:23:08,903][129146] Moving the mean solution point...
[36m[2023-06-25 10:23:18,576][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 10:23:18,576][129146] FPS: 397056.47
[36m[2023-06-25 10:23:18,578][129146] itr=1024, itrs=2000, Progress: 51.20%
[36m[2023-06-25 10:23:30,247][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 10:23:30,247][129146] FPS: 329823.96
[36m[2023-06-25 10:23:35,042][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:23:35,043][129146] Reward + Measures: [[530.4514601    0.24437468   0.96428102   0.83288234   0.97076768]]
[37m[1m[2023-06-25 10:23:35,043][129146] Max Reward on eval: 530.4514600991816
[37m[1m[2023-06-25 10:23:35,043][129146] Min Reward on eval: 530.4514600991816
[37m[1m[2023-06-25 10:23:35,043][129146] Mean Reward across all agents: 530.4514600991816
[37m[1m[2023-06-25 10:23:35,044][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:23:40,544][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:23:40,545][129146] Reward + Measures: [[-238.21064083    0.049         0.85890001    0.62280005    0.91219997]
[37m[1m [  52.87157431    0.0894        0.75269997    0.6771        0.75889999]
[37m[1m [ -10.9349417     0.0442        0.72330004    0.53000003    0.81090003]
[37m[1m ...
[37m[1m [ 434.5138256     0.1374        0.66379994    0.45390001    0.70570004]
[37m[1m [ 301.24802505    0.56050003    0.64560002    0.48930001    0.68080002]
[37m[1m [-180.67335026    0.76230001    0.76560003    0.1276        0.76159996]]
[37m[1m[2023-06-25 10:23:40,545][129146] Max Reward on eval: 520.2224986863555
[37m[1m[2023-06-25 10:23:40,545][129146] Min Reward on eval: -1213.6710610333364
[37m[1m[2023-06-25 10:23:40,546][129146] Mean Reward across all agents: 2.9557807213674536
[37m[1m[2023-06-25 10:23:40,546][129146] Average Trajectory Length: 997.6453333333333
[36m[2023-06-25 10:23:40,552][129146] mean_value=-212.1935070397472, max_value=804.8862825197564
[37m[1m[2023-06-25 10:23:40,555][129146] New mean coefficients: [[0.3563714  0.40385574 2.1335158  0.07426184 2.8111477 ]]
[37m[1m[2023-06-25 10:23:40,556][129146] Moving the mean solution point...
[36m[2023-06-25 10:23:50,362][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 10:23:50,363][129146] FPS: 391641.44
[36m[2023-06-25 10:23:50,365][129146] itr=1025, itrs=2000, Progress: 51.25%
[36m[2023-06-25 10:24:01,778][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 10:24:01,779][129146] FPS: 337152.96
[36m[2023-06-25 10:24:06,478][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:24:06,478][129146] Reward + Measures: [[525.87055398   0.41800633   0.97113705   0.91755331   0.97369874]]
[37m[1m[2023-06-25 10:24:06,478][129146] Max Reward on eval: 525.8705539774884
[37m[1m[2023-06-25 10:24:06,478][129146] Min Reward on eval: 525.8705539774884
[37m[1m[2023-06-25 10:24:06,479][129146] Mean Reward across all agents: 525.8705539774884
[37m[1m[2023-06-25 10:24:06,479][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:24:12,067][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:24:12,067][129146] Reward + Measures: [[569.43435672   0.6013       0.96649998   0.9774       0.98750001]
[37m[1m [ 31.23038907   0.39989999   0.46849999   0.33660001   0.51360005]
[37m[1m [719.73164418   0.75209999   0.792        0.76109999   0.87709999]
[37m[1m ...
[37m[1m [655.10878013   0.64419997   0.63280004   0.45479998   0.76010001]
[37m[1m [285.57112969   0.25209999   0.77850002   0.39970002   0.89639997]
[37m[1m [-60.17684405   0.62029999   0.79070002   0.71419996   0.78650004]]
[37m[1m[2023-06-25 10:24:12,068][129146] Max Reward on eval: 867.1552740314626
[37m[1m[2023-06-25 10:24:12,068][129146] Min Reward on eval: -1037.3308153669118
[37m[1m[2023-06-25 10:24:12,068][129146] Mean Reward across all agents: 155.0410640726636
[37m[1m[2023-06-25 10:24:12,068][129146] Average Trajectory Length: 999.8343333333333
[36m[2023-06-25 10:24:12,075][129146] mean_value=-105.03449761282168, max_value=1149.3807833379785
[37m[1m[2023-06-25 10:24:12,078][129146] New mean coefficients: [[1.047164   0.5433521  1.9705166  0.26202714 2.720865  ]]
[37m[1m[2023-06-25 10:24:12,079][129146] Moving the mean solution point...
[36m[2023-06-25 10:24:21,747][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 10:24:21,747][129146] FPS: 397256.05
[36m[2023-06-25 10:24:21,749][129146] itr=1026, itrs=2000, Progress: 51.30%
[36m[2023-06-25 10:24:33,279][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 10:24:33,279][129146] FPS: 333733.90
[36m[2023-06-25 10:24:38,094][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:24:38,095][129146] Reward + Measures: [[472.13497489   0.62919801   0.97567034   0.94567466   0.97589093]]
[37m[1m[2023-06-25 10:24:38,095][129146] Max Reward on eval: 472.13497488913896
[37m[1m[2023-06-25 10:24:38,095][129146] Min Reward on eval: 472.13497488913896
[37m[1m[2023-06-25 10:24:38,095][129146] Mean Reward across all agents: 472.13497488913896
[37m[1m[2023-06-25 10:24:38,096][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:24:43,524][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:24:43,525][129146] Reward + Measures: [[ 332.21147235    0.42300001    0.82960004    0.3888        0.88529998]
[37m[1m [ 111.62290391    0.63620001    0.3337        0.87400001    0.72469997]
[37m[1m [ 426.27821171    0.27079999    0.80290002    0.18429999    0.88090003]
[37m[1m ...
[37m[1m [ 492.37872091    0.27329999    0.80129999    0.13259999    0.89379996]
[37m[1m [ 175.88634589    0.36410001    0.81810009    0.45690003    0.87840003]
[37m[1m [-860.80100074    0.59890002    0.96649998    0.85270005    0.97710001]]
[37m[1m[2023-06-25 10:24:43,525][129146] Max Reward on eval: 1005.5572760618176
[37m[1m[2023-06-25 10:24:43,525][129146] Min Reward on eval: -890.3330154039664
[37m[1m[2023-06-25 10:24:43,525][129146] Mean Reward across all agents: 205.47418519305586
[37m[1m[2023-06-25 10:24:43,526][129146] Average Trajectory Length: 999.463
[36m[2023-06-25 10:24:43,535][129146] mean_value=108.42693357850156, max_value=1092.6473108552048
[37m[1m[2023-06-25 10:24:43,537][129146] New mean coefficients: [[ 0.5655843   0.567669    1.9989668  -0.12494546  2.580963  ]]
[37m[1m[2023-06-25 10:24:43,538][129146] Moving the mean solution point...
[36m[2023-06-25 10:24:53,327][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 10:24:53,327][129146] FPS: 392360.20
[36m[2023-06-25 10:24:53,329][129146] itr=1027, itrs=2000, Progress: 51.35%
[36m[2023-06-25 10:25:04,967][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 10:25:04,967][129146] FPS: 330628.38
[36m[2023-06-25 10:25:09,913][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:25:09,914][129146] Reward + Measures: [[465.08346047   0.81127602   0.97829294   0.96400928   0.97712594]]
[37m[1m[2023-06-25 10:25:09,914][129146] Max Reward on eval: 465.0834604660408
[37m[1m[2023-06-25 10:25:09,914][129146] Min Reward on eval: 465.0834604660408
[37m[1m[2023-06-25 10:25:09,914][129146] Mean Reward across all agents: 465.0834604660408
[37m[1m[2023-06-25 10:25:09,915][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:25:15,452][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:25:15,453][129146] Reward + Measures: [[ 61.07834406   0.5557       0.89200002   0.76719999   0.86260003]
[37m[1m [192.8996528    0.41219997   0.74119997   0.08220001   0.7281    ]
[37m[1m [302.85350219   0.60350001   0.92350006   0.45320001   0.92820007]
[37m[1m ...
[37m[1m [174.47016328   0.94449997   0.93940002   0.97799999   0.94770002]
[37m[1m [ 90.31925091   0.3784       0.60879999   0.2818       0.66660005]
[37m[1m [425.27048696   0.79549998   0.90490001   0.66090006   0.91949999]]
[37m[1m[2023-06-25 10:25:15,453][129146] Max Reward on eval: 689.531126503786
[37m[1m[2023-06-25 10:25:15,454][129146] Min Reward on eval: -1174.5825107312994
[37m[1m[2023-06-25 10:25:15,454][129146] Mean Reward across all agents: 138.6617952127938
[37m[1m[2023-06-25 10:25:15,454][129146] Average Trajectory Length: 998.8903333333333
[36m[2023-06-25 10:25:15,461][129146] mean_value=-150.43885678210265, max_value=1111.8285862552934
[37m[1m[2023-06-25 10:25:15,464][129146] New mean coefficients: [[ 0.04503864 -0.03626293  1.8138227  -0.11451638  2.7914276 ]]
[37m[1m[2023-06-25 10:25:15,465][129146] Moving the mean solution point...
[36m[2023-06-25 10:25:25,278][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 10:25:25,278][129146] FPS: 391391.59
[36m[2023-06-25 10:25:25,280][129146] itr=1028, itrs=2000, Progress: 51.40%
[36m[2023-06-25 10:25:36,825][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 10:25:36,826][129146] FPS: 333276.35
[36m[2023-06-25 10:25:41,625][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:25:41,626][129146] Reward + Measures: [[468.84901036   0.83491999   0.98106903   0.97755802   0.98005158]]
[37m[1m[2023-06-25 10:25:41,626][129146] Max Reward on eval: 468.84901035897633
[37m[1m[2023-06-25 10:25:41,626][129146] Min Reward on eval: 468.84901035897633
[37m[1m[2023-06-25 10:25:41,626][129146] Mean Reward across all agents: 468.84901035897633
[37m[1m[2023-06-25 10:25:41,627][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:25:47,091][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:25:47,092][129146] Reward + Measures: [[623.55302132   0.1362       0.90930003   0.73390007   0.93120003]
[37m[1m [113.03404947   0.838        0.77100003   0.97690004   0.97200006]
[37m[1m [-14.47161368   0.3529       0.47769997   0.52719998   0.5007    ]
[37m[1m ...
[37m[1m [343.53297779   0.33680001   0.64850003   0.43650004   0.61330003]
[37m[1m [-20.20413539   0.98260003   0.94679993   0.99190009   0.9781    ]
[37m[1m [-88.98315777   0.0279       0.87260002   0.65320003   0.95349997]]
[37m[1m[2023-06-25 10:25:47,092][129146] Max Reward on eval: 708.1153291775496
[37m[1m[2023-06-25 10:25:47,092][129146] Min Reward on eval: -678.6495950012701
[37m[1m[2023-06-25 10:25:47,093][129146] Mean Reward across all agents: 181.0630301520047
[37m[1m[2023-06-25 10:25:47,093][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:25:47,101][129146] mean_value=35.985490538708085, max_value=1096.2540950137657
[37m[1m[2023-06-25 10:25:47,104][129146] New mean coefficients: [[ 0.21168132 -0.41230756  2.423076   -0.28310525  3.5421183 ]]
[37m[1m[2023-06-25 10:25:47,105][129146] Moving the mean solution point...
[36m[2023-06-25 10:25:56,865][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 10:25:56,866][129146] FPS: 393495.23
[36m[2023-06-25 10:25:56,868][129146] itr=1029, itrs=2000, Progress: 51.45%
[36m[2023-06-25 10:26:08,408][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 10:26:08,409][129146] FPS: 333397.08
[36m[2023-06-25 10:26:13,143][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:26:13,144][129146] Reward + Measures: [[536.41495809   0.46530598   0.98206967   0.9681294    0.98338771]]
[37m[1m[2023-06-25 10:26:13,144][129146] Max Reward on eval: 536.4149580913517
[37m[1m[2023-06-25 10:26:13,144][129146] Min Reward on eval: 536.4149580913517
[37m[1m[2023-06-25 10:26:13,145][129146] Mean Reward across all agents: 536.4149580913517
[37m[1m[2023-06-25 10:26:13,145][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:26:18,683][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:26:18,684][129146] Reward + Measures: [[521.50333361   0.61840004   0.58840007   0.52689999   0.58190006]
[37m[1m [588.89637075   0.2696       0.78780001   0.3091       0.81580001]
[37m[1m [313.73973032   0.0336       0.96000004   0.91040003   0.95460004]
[37m[1m ...
[37m[1m [550.3224976    0.5176       0.61429995   0.54539996   0.61009997]
[37m[1m [204.54109114   0.22809999   0.97399998   0.97440004   0.98320001]
[37m[1m [537.66118851   0.95959997   0.98180002   0.97980005   0.98290008]]
[37m[1m[2023-06-25 10:26:18,684][129146] Max Reward on eval: 725.0488788217539
[37m[1m[2023-06-25 10:26:18,684][129146] Min Reward on eval: -1010.3453211861372
[37m[1m[2023-06-25 10:26:18,685][129146] Mean Reward across all agents: 337.955462710812
[37m[1m[2023-06-25 10:26:18,685][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:26:18,691][129146] mean_value=-27.8423928545688, max_value=1034.452749913442
[37m[1m[2023-06-25 10:26:18,694][129146] New mean coefficients: [[ 0.9526639   0.08527088  2.3345444  -0.22270459  2.768211  ]]
[37m[1m[2023-06-25 10:26:18,695][129146] Moving the mean solution point...
[36m[2023-06-25 10:26:28,499][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 10:26:28,499][129146] FPS: 391717.65
[36m[2023-06-25 10:26:28,502][129146] itr=1030, itrs=2000, Progress: 51.50%
[37m[1m[2023-06-25 10:26:35,624][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001010
[36m[2023-06-25 10:26:47,275][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 10:26:47,276][129146] FPS: 336220.51
[36m[2023-06-25 10:26:52,048][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:26:52,049][129146] Reward + Measures: [[627.26393821   0.007718     0.98547864   0.92801303   0.98707867]]
[37m[1m[2023-06-25 10:26:52,049][129146] Max Reward on eval: 627.263938214435
[37m[1m[2023-06-25 10:26:52,049][129146] Min Reward on eval: 627.263938214435
[37m[1m[2023-06-25 10:26:52,049][129146] Mean Reward across all agents: 627.263938214435
[37m[1m[2023-06-25 10:26:52,049][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:26:57,473][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:26:57,473][129146] Reward + Measures: [[687.72436736   0.95020002   0.99340004   0.9945001    0.99160004]
[37m[1m [762.96068472   0.98919994   0.98940003   0.99299997   0.9896    ]
[37m[1m [630.45196341   0.25590003   0.98369998   0.97299999   0.99119997]
[37m[1m ...
[37m[1m [192.5494626    0.37809998   0.7956       0.14479999   0.90649998]
[37m[1m [740.58845616   0.97650003   0.97970003   0.98880005   0.97980005]
[37m[1m [650.62723433   0.82670003   0.99350005   0.99230003   0.99270004]]
[37m[1m[2023-06-25 10:26:57,474][129146] Max Reward on eval: 794.8924838813022
[37m[1m[2023-06-25 10:26:57,474][129146] Min Reward on eval: -688.8454188361299
[37m[1m[2023-06-25 10:26:57,474][129146] Mean Reward across all agents: 530.8305311812273
[37m[1m[2023-06-25 10:26:57,474][129146] Average Trajectory Length: 999.308
[36m[2023-06-25 10:26:57,483][129146] mean_value=208.74736742441326, max_value=1200.196086337976
[37m[1m[2023-06-25 10:26:57,486][129146] New mean coefficients: [[ 1.1885998   0.37206393  2.6087644  -0.17904843  2.198777  ]]
[37m[1m[2023-06-25 10:26:57,487][129146] Moving the mean solution point...
[36m[2023-06-25 10:27:07,187][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 10:27:07,187][129146] FPS: 395939.14
[36m[2023-06-25 10:27:07,190][129146] itr=1031, itrs=2000, Progress: 51.55%
[36m[2023-06-25 10:27:18,677][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 10:27:18,677][129146] FPS: 334944.07
[36m[2023-06-25 10:27:23,369][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:27:23,369][129146] Reward + Measures: [[-116.94923122    0.00179133    0.98844963    0.98692364    0.9901976 ]]
[37m[1m[2023-06-25 10:27:23,370][129146] Max Reward on eval: -116.94923122229375
[37m[1m[2023-06-25 10:27:23,370][129146] Min Reward on eval: -116.94923122229375
[37m[1m[2023-06-25 10:27:23,370][129146] Mean Reward across all agents: -116.94923122229375
[37m[1m[2023-06-25 10:27:23,370][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:27:28,836][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:27:28,836][129146] Reward + Measures: [[ -746.90410634     0.0928         0.69840002     0.66460001
[37m[1m      0.76609999]
[37m[1m [ -486.37450153     0.0215         0.93639994     0.93479997
[37m[1m      0.96090001]
[37m[1m [-1447.16040701     0.0381         0.94580001     0.96119994
[37m[1m      0.98019999]
[37m[1m ...
[37m[1m [-1302.27969317     0.30510002     0.85659999     0.08540001
[37m[1m      0.85690004]
[37m[1m [-1430.55868723     0.1167217      0.5471698      0.53303766
[37m[1m      0.52546233]
[37m[1m [-1269.2582343      0.19927143     0.68341434     0.68539047
[37m[1m      0.66431904]]
[37m[1m[2023-06-25 10:27:28,836][129146] Max Reward on eval: 336.18989325851436
[37m[1m[2023-06-25 10:27:28,837][129146] Min Reward on eval: -2313.164894492866
[37m[1m[2023-06-25 10:27:28,837][129146] Mean Reward across all agents: -978.2407954101026
[37m[1m[2023-06-25 10:27:28,837][129146] Average Trajectory Length: 920.0636666666667
[36m[2023-06-25 10:27:28,839][129146] mean_value=-1340.4019009052881, max_value=480.3260911197328
[37m[1m[2023-06-25 10:27:28,841][129146] New mean coefficients: [[ 1.1682587  -0.33621562  2.6629505  -0.15627463  0.63420033]]
[37m[1m[2023-06-25 10:27:28,842][129146] Moving the mean solution point...
[36m[2023-06-25 10:27:38,605][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 10:27:38,605][129146] FPS: 393398.39
[36m[2023-06-25 10:27:38,608][129146] itr=1032, itrs=2000, Progress: 51.60%
[36m[2023-06-25 10:27:50,199][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 10:27:50,199][129146] FPS: 331984.21
[36m[2023-06-25 10:27:54,932][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:27:54,932][129146] Reward + Measures: [[166.43775384   0.28619832   0.59670299   0.43104264   0.75321501]]
[37m[1m[2023-06-25 10:27:54,932][129146] Max Reward on eval: 166.43775383593922
[37m[1m[2023-06-25 10:27:54,933][129146] Min Reward on eval: 166.43775383593922
[37m[1m[2023-06-25 10:27:54,933][129146] Mean Reward across all agents: 166.43775383593922
[37m[1m[2023-06-25 10:27:54,933][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:27:59,813][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:27:59,813][129146] Reward + Measures: [[394.74064805   0.50990003   0.8235001    0.65140003   0.90640014]
[37m[1m [268.10953364   0.36470002   0.63690001   0.56949997   0.80760002]
[37m[1m [397.31327843   0.41240001   0.75080007   0.54710001   0.85680002]
[37m[1m ...
[37m[1m [182.71761541   0.22390001   0.57470006   0.42230001   0.69250005]
[37m[1m [767.56445932   0.1602       0.66639996   0.3646       0.76600009]
[37m[1m [610.31646092   0.1619       0.62410003   0.40150005   0.75379997]]
[37m[1m[2023-06-25 10:27:59,813][129146] Max Reward on eval: 796.1061765524443
[37m[1m[2023-06-25 10:27:59,814][129146] Min Reward on eval: 151.5348783297639
[37m[1m[2023-06-25 10:27:59,814][129146] Mean Reward across all agents: 446.4108618587533
[37m[1m[2023-06-25 10:27:59,814][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:27:59,823][129146] mean_value=470.1882152909385, max_value=1143.5897792860283
[37m[1m[2023-06-25 10:27:59,826][129146] New mean coefficients: [[ 1.5165806  -0.7833269   3.0500293  -0.16306455  1.2805531 ]]
[37m[1m[2023-06-25 10:27:59,827][129146] Moving the mean solution point...
[36m[2023-06-25 10:28:09,064][129146] train() took 9.24 seconds to complete
[36m[2023-06-25 10:28:09,064][129146] FPS: 415801.00
[36m[2023-06-25 10:28:09,067][129146] itr=1033, itrs=2000, Progress: 51.65%
[36m[2023-06-25 10:28:20,651][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 10:28:20,651][129146] FPS: 332182.63
[36m[2023-06-25 10:28:25,501][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:28:25,501][129146] Reward + Measures: [[181.14348105   0.37471965   0.77879006   0.47249898   0.87740695]]
[37m[1m[2023-06-25 10:28:25,501][129146] Max Reward on eval: 181.14348105389544
[37m[1m[2023-06-25 10:28:25,501][129146] Min Reward on eval: 181.14348105389544
[37m[1m[2023-06-25 10:28:25,502][129146] Mean Reward across all agents: 181.14348105389544
[37m[1m[2023-06-25 10:28:25,502][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:28:30,893][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:28:30,894][129146] Reward + Measures: [[339.9039648    0.6347       0.89309996   0.68620002   0.90570003]
[37m[1m [333.87107894   0.44850001   0.85009998   0.55860001   0.87770003]
[37m[1m [365.95500616   0.59779996   0.90799993   0.671        0.93569994]
[37m[1m ...
[37m[1m [199.93117605   0.34730002   0.68119997   0.6221       0.76209998]
[37m[1m [483.23361003   0.41799998   0.86840004   0.59440005   0.87      ]
[37m[1m [359.66688268   0.3547       0.82050002   0.54900002   0.85939997]]
[37m[1m[2023-06-25 10:28:30,894][129146] Max Reward on eval: 677.377892852074
[37m[1m[2023-06-25 10:28:30,894][129146] Min Reward on eval: -591.816082890681
[37m[1m[2023-06-25 10:28:30,894][129146] Mean Reward across all agents: 296.0014597010801
[37m[1m[2023-06-25 10:28:30,894][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:28:30,900][129146] mean_value=102.66136084016404, max_value=1025.406966283585
[37m[1m[2023-06-25 10:28:30,903][129146] New mean coefficients: [[ 1.4742749  -1.0020082   3.9637794   0.00168844  1.1822892 ]]
[37m[1m[2023-06-25 10:28:30,904][129146] Moving the mean solution point...
[36m[2023-06-25 10:28:40,599][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 10:28:40,600][129146] FPS: 396129.35
[36m[2023-06-25 10:28:40,602][129146] itr=1034, itrs=2000, Progress: 51.70%
[36m[2023-06-25 10:28:52,090][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 10:28:52,090][129146] FPS: 335027.60
[36m[2023-06-25 10:28:56,844][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:28:56,844][129146] Reward + Measures: [[544.14379822   0.17438799   0.87942433   0.26680836   0.93353403]]
[37m[1m[2023-06-25 10:28:56,844][129146] Max Reward on eval: 544.1437982218409
[37m[1m[2023-06-25 10:28:56,844][129146] Min Reward on eval: 544.1437982218409
[37m[1m[2023-06-25 10:28:56,845][129146] Mean Reward across all agents: 544.1437982218409
[37m[1m[2023-06-25 10:28:56,845][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:29:02,340][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:29:02,341][129146] Reward + Measures: [[635.76965346   0.44819999   0.69110006   0.42309999   0.81110001]
[37m[1m [775.80977127   0.50299996   0.42020002   0.49130002   0.66940004]
[37m[1m [108.52985943   0.31119999   0.88990003   0.30090001   0.95179999]
[37m[1m ...
[37m[1m [597.35877714   0.24450003   0.55980003   0.43510005   0.64790004]
[37m[1m [575.96855763   0.36890003   0.70860004   0.42039999   0.84650004]
[37m[1m [623.69498008   0.47720003   0.64420003   0.46240002   0.80610001]]
[37m[1m[2023-06-25 10:29:02,341][129146] Max Reward on eval: 835.4812945644837
[37m[1m[2023-06-25 10:29:02,341][129146] Min Reward on eval: 108.5298594284919
[37m[1m[2023-06-25 10:29:02,341][129146] Mean Reward across all agents: 628.554764562125
[37m[1m[2023-06-25 10:29:02,342][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:29:02,351][129146] mean_value=613.2013376990438, max_value=1196.9201119530248
[37m[1m[2023-06-25 10:29:02,353][129146] New mean coefficients: [[ 1.8405427  -0.815173    3.3622637   0.10455059  1.3855982 ]]
[37m[1m[2023-06-25 10:29:02,354][129146] Moving the mean solution point...
[36m[2023-06-25 10:29:11,962][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 10:29:11,962][129146] FPS: 399745.91
[36m[2023-06-25 10:29:11,965][129146] itr=1035, itrs=2000, Progress: 51.75%
[36m[2023-06-25 10:29:23,366][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 10:29:23,366][129146] FPS: 337490.44
[36m[2023-06-25 10:29:28,090][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:29:28,090][129146] Reward + Measures: [[159.57766285   0.00310867   0.98325598   0.76951498   0.96224159]]
[37m[1m[2023-06-25 10:29:28,090][129146] Max Reward on eval: 159.5776628498543
[37m[1m[2023-06-25 10:29:28,091][129146] Min Reward on eval: 159.5776628498543
[37m[1m[2023-06-25 10:29:28,091][129146] Mean Reward across all agents: 159.5776628498543
[37m[1m[2023-06-25 10:29:28,091][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:29:33,790][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:29:33,791][129146] Reward + Measures: [[ 266.5081365     0.77770007    0.5352        0.85369998    0.1066    ]
[37m[1m [-180.81848872    0.72550005    0.82860005    0.65409994    0.84380007]
[37m[1m [  58.0444412     0.82450002    0.59280002    0.90280002    0.1147    ]
[37m[1m ...
[37m[1m [  63.05091902    0.58459997    0.53930002    0.42410001    0.34989998]
[37m[1m [-223.62281906    0.85729998    0.64729995    0.84179991    0.55050004]
[37m[1m [-146.42737343    0.91149998    0.63309997    0.85330003    0.2552    ]]
[37m[1m[2023-06-25 10:29:33,791][129146] Max Reward on eval: 571.9818589183735
[37m[1m[2023-06-25 10:29:33,791][129146] Min Reward on eval: -1489.8433070715516
[37m[1m[2023-06-25 10:29:33,791][129146] Mean Reward across all agents: -112.88712645459617
[37m[1m[2023-06-25 10:29:33,792][129146] Average Trajectory Length: 998.0983333333332
[36m[2023-06-25 10:29:33,797][129146] mean_value=-270.7920621715132, max_value=1030.7401994368643
[37m[1m[2023-06-25 10:29:33,800][129146] New mean coefficients: [[ 1.6264647  -0.6888607   3.3069265   0.07118216  0.9078096 ]]
[37m[1m[2023-06-25 10:29:33,801][129146] Moving the mean solution point...
[36m[2023-06-25 10:29:43,499][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 10:29:43,499][129146] FPS: 396014.16
[36m[2023-06-25 10:29:43,502][129146] itr=1036, itrs=2000, Progress: 51.80%
[36m[2023-06-25 10:29:55,094][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 10:29:55,094][129146] FPS: 331901.32
[36m[2023-06-25 10:30:00,006][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:30:00,006][129146] Reward + Measures: [[601.23979243   0.32427967   0.91127425   0.5025053    0.93245667]]
[37m[1m[2023-06-25 10:30:00,006][129146] Max Reward on eval: 601.2397924286785
[37m[1m[2023-06-25 10:30:00,007][129146] Min Reward on eval: 601.2397924286785
[37m[1m[2023-06-25 10:30:00,007][129146] Mean Reward across all agents: 601.2397924286785
[37m[1m[2023-06-25 10:30:00,007][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:30:05,535][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:30:05,535][129146] Reward + Measures: [[163.62298981   0.1507       0.88920003   0.69750005   0.85509998]
[37m[1m [510.89694163   0.10950001   0.95809996   0.3612       0.95730013]
[37m[1m [332.43744641   0.18880001   0.83919996   0.57020003   0.80430001]
[37m[1m ...
[37m[1m [633.88521289   0.1024       0.85709995   0.45469999   0.78930002]
[37m[1m [-77.27214813   0.36069998   0.83409995   0.69800001   0.74590009]
[37m[1m [434.76969424   0.3682       0.95460004   0.13340001   0.9691    ]]
[37m[1m[2023-06-25 10:30:05,536][129146] Max Reward on eval: 738.6618260018644
[37m[1m[2023-06-25 10:30:05,536][129146] Min Reward on eval: -1623.30887865189
[37m[1m[2023-06-25 10:30:05,536][129146] Mean Reward across all agents: 279.4643990648238
[37m[1m[2023-06-25 10:30:05,536][129146] Average Trajectory Length: 999.5843333333333
[36m[2023-06-25 10:30:05,545][129146] mean_value=209.02475084598422, max_value=1175.4756993487129
[37m[1m[2023-06-25 10:30:05,547][129146] New mean coefficients: [[ 1.5610574  -0.06516516  3.4238117   0.06222029  0.9274804 ]]
[37m[1m[2023-06-25 10:30:05,548][129146] Moving the mean solution point...
[36m[2023-06-25 10:30:15,339][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 10:30:15,339][129146] FPS: 392276.77
[36m[2023-06-25 10:30:15,341][129146] itr=1037, itrs=2000, Progress: 51.85%
[36m[2023-06-25 10:30:26,759][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 10:30:26,759][129146] FPS: 337024.93
[36m[2023-06-25 10:30:31,490][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:30:31,491][129146] Reward + Measures: [[576.69565653   0.36819869   0.95127356   0.54282266   0.96381736]]
[37m[1m[2023-06-25 10:30:31,491][129146] Max Reward on eval: 576.695656531536
[37m[1m[2023-06-25 10:30:31,491][129146] Min Reward on eval: 576.695656531536
[37m[1m[2023-06-25 10:30:31,491][129146] Mean Reward across all agents: 576.695656531536
[37m[1m[2023-06-25 10:30:31,492][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:30:36,929][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:30:36,929][129146] Reward + Measures: [[540.62271854   0.40330002   0.96180004   0.60570002   0.95290005]
[37m[1m [512.31403696   0.759        0.98730004   0.72710001   0.98159999]
[37m[1m [502.36797042   0.35729998   0.96960002   0.5061       0.96490002]
[37m[1m ...
[37m[1m [355.81065851   0.94250005   0.94279999   0.94569999   0.88990003]
[37m[1m [498.56844493   0.27700001   0.94489998   0.6051001    0.9497    ]
[37m[1m [490.84956938   0.78050005   0.96070004   0.86230004   0.90420002]]
[37m[1m[2023-06-25 10:30:36,929][129146] Max Reward on eval: 685.2522202686872
[37m[1m[2023-06-25 10:30:36,930][129146] Min Reward on eval: 5.518079554266296
[37m[1m[2023-06-25 10:30:36,930][129146] Mean Reward across all agents: 480.20800911750956
[37m[1m[2023-06-25 10:30:36,930][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:30:36,939][129146] mean_value=581.3182837586342, max_value=1111.0157759923022
[37m[1m[2023-06-25 10:30:36,942][129146] New mean coefficients: [[ 1.7775881  -0.21480317  4.772749    0.1386893   0.9420945 ]]
[37m[1m[2023-06-25 10:30:36,943][129146] Moving the mean solution point...
[36m[2023-06-25 10:30:46,701][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 10:30:46,701][129146] FPS: 393578.43
[36m[2023-06-25 10:30:46,703][129146] itr=1038, itrs=2000, Progress: 51.90%
[36m[2023-06-25 10:30:58,152][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 10:30:58,153][129146] FPS: 336059.39
[36m[2023-06-25 10:31:02,949][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:31:02,949][129146] Reward + Measures: [[808.42340624   0.4694187    0.87164468   0.65315068   0.67571265]]
[37m[1m[2023-06-25 10:31:02,949][129146] Max Reward on eval: 808.423406243409
[37m[1m[2023-06-25 10:31:02,950][129146] Min Reward on eval: 808.423406243409
[37m[1m[2023-06-25 10:31:02,950][129146] Mean Reward across all agents: 808.423406243409
[37m[1m[2023-06-25 10:31:02,950][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:31:08,445][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:31:08,446][129146] Reward + Measures: [[786.45862982   0.12660001   0.87509996   0.61000007   0.85610002]
[37m[1m [885.48501031   0.22729997   0.91049999   0.6383       0.75130004]
[37m[1m [905.05937933   0.37289998   0.90389997   0.72670001   0.71200007]
[37m[1m ...
[37m[1m [888.57083442   0.0757       0.8731001    0.65309995   0.78650004]
[37m[1m [770.28412198   0.18969999   0.89790004   0.52940005   0.84629995]
[37m[1m [722.52342112   0.30689999   0.8524       0.50640005   0.80779999]]
[37m[1m[2023-06-25 10:31:08,446][129146] Max Reward on eval: 951.7820428452571
[37m[1m[2023-06-25 10:31:08,446][129146] Min Reward on eval: 701.721447397396
[37m[1m[2023-06-25 10:31:08,446][129146] Mean Reward across all agents: 853.4467613957964
[37m[1m[2023-06-25 10:31:08,447][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:31:08,458][129146] mean_value=984.1630500291212, max_value=1445.0533661739203
[37m[1m[2023-06-25 10:31:08,461][129146] New mean coefficients: [[ 1.7400513   0.21851568  4.4785757   0.446725   -0.63230115]]
[37m[1m[2023-06-25 10:31:08,462][129146] Moving the mean solution point...
[36m[2023-06-25 10:31:18,270][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 10:31:18,271][129146] FPS: 391571.47
[36m[2023-06-25 10:31:18,273][129146] itr=1039, itrs=2000, Progress: 51.95%
[36m[2023-06-25 10:31:29,809][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 10:31:29,809][129146] FPS: 333542.37
[36m[2023-06-25 10:31:34,579][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:31:34,580][129146] Reward + Measures: [[616.50432681   0.42422765   0.83796102   0.46864131   0.57028562]]
[37m[1m[2023-06-25 10:31:34,580][129146] Max Reward on eval: 616.5043268142937
[37m[1m[2023-06-25 10:31:34,580][129146] Min Reward on eval: 616.5043268142937
[37m[1m[2023-06-25 10:31:34,580][129146] Mean Reward across all agents: 616.5043268142937
[37m[1m[2023-06-25 10:31:34,580][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:31:40,014][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:31:40,015][129146] Reward + Measures: [[617.72507296   0.54210007   0.87129992   0.64679998   0.76730007]
[37m[1m [674.57206678   0.54220003   0.8459       0.6038       0.66790003]
[37m[1m [632.78576354   0.44600001   0.86940002   0.54980004   0.71090001]
[37m[1m ...
[37m[1m [697.11699818   0.47460005   0.86919993   0.59869999   0.69030005]
[37m[1m [662.90926992   0.48529997   0.85690004   0.55080003   0.64720005]
[37m[1m [712.17037023   0.42539999   0.82880002   0.51680005   0.58110005]]
[37m[1m[2023-06-25 10:31:40,015][129146] Max Reward on eval: 756.3235024306341
[37m[1m[2023-06-25 10:31:40,015][129146] Min Reward on eval: 507.8126461072592
[37m[1m[2023-06-25 10:31:40,016][129146] Mean Reward across all agents: 665.6065428200494
[37m[1m[2023-06-25 10:31:40,016][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:31:40,022][129146] mean_value=550.3074515646215, max_value=1218.8562415502033
[37m[1m[2023-06-25 10:31:40,025][129146] New mean coefficients: [[ 1.7599951  0.5032201  4.8864985  0.5308408 -2.293917 ]]
[37m[1m[2023-06-25 10:31:40,026][129146] Moving the mean solution point...
[36m[2023-06-25 10:31:49,823][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 10:31:49,823][129146] FPS: 392041.04
[36m[2023-06-25 10:31:49,825][129146] itr=1040, itrs=2000, Progress: 52.00%
[37m[1m[2023-06-25 10:31:56,978][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001020
[36m[2023-06-25 10:32:08,799][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 10:32:08,799][129146] FPS: 331132.71
[36m[2023-06-25 10:32:13,608][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:32:13,613][129146] Reward + Measures: [[588.4694747    0.493725     0.87169564   0.58476865   0.41915601]]
[37m[1m[2023-06-25 10:32:13,613][129146] Max Reward on eval: 588.4694747010581
[37m[1m[2023-06-25 10:32:13,613][129146] Min Reward on eval: 588.4694747010581
[37m[1m[2023-06-25 10:32:13,614][129146] Mean Reward across all agents: 588.4694747010581
[37m[1m[2023-06-25 10:32:13,614][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:32:19,074][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:32:19,080][129146] Reward + Measures: [[720.42695643   0.35010001   0.74370003   0.43920001   0.465     ]
[37m[1m [580.583632     0.49770004   0.85020012   0.5952       0.38060004]
[37m[1m [447.11796639   0.72170001   0.84780008   0.82749999   0.2448    ]
[37m[1m ...
[37m[1m [655.41378617   0.45820004   0.83810008   0.55500001   0.40570003]
[37m[1m [425.39240513   0.71780002   0.94239998   0.79069996   0.6997    ]
[37m[1m [459.40584419   0.93580008   0.97840005   0.94550002   0.93809998]]
[37m[1m[2023-06-25 10:32:19,080][129146] Max Reward on eval: 771.9262044260977
[37m[1m[2023-06-25 10:32:19,081][129146] Min Reward on eval: 270.4740713053849
[37m[1m[2023-06-25 10:32:19,081][129146] Mean Reward across all agents: 568.7603962750287
[37m[1m[2023-06-25 10:32:19,081][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:32:19,089][129146] mean_value=917.6010182681086, max_value=1271.9262044260977
[37m[1m[2023-06-25 10:32:19,092][129146] New mean coefficients: [[ 2.1178133  0.5859992  3.6378584  0.7091193 -2.2922938]]
[37m[1m[2023-06-25 10:32:19,093][129146] Moving the mean solution point...
[36m[2023-06-25 10:32:28,854][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 10:32:28,854][129146] FPS: 393491.07
[36m[2023-06-25 10:32:28,856][129146] itr=1041, itrs=2000, Progress: 52.05%
[36m[2023-06-25 10:32:40,474][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 10:32:40,475][129146] FPS: 331310.89
[36m[2023-06-25 10:32:45,255][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:32:45,256][129146] Reward + Measures: [[507.63779716   0.33486164   0.67696035   0.52213633   0.64704663]]
[37m[1m[2023-06-25 10:32:45,256][129146] Max Reward on eval: 507.6377971567603
[37m[1m[2023-06-25 10:32:45,256][129146] Min Reward on eval: 507.6377971567603
[37m[1m[2023-06-25 10:32:45,256][129146] Mean Reward across all agents: 507.6377971567603
[37m[1m[2023-06-25 10:32:45,256][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:32:50,677][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:32:50,677][129146] Reward + Measures: [[481.60875355   0.30949998   0.69559997   0.67530006   0.67759997]
[37m[1m [537.81280455   0.57569999   0.80390006   0.59260005   0.75830001]
[37m[1m [482.28959217   0.54659998   0.78290004   0.6063       0.61760002]
[37m[1m ...
[37m[1m [551.85934882   0.4894       0.7554       0.5302       0.75340003]
[37m[1m [516.6481485    0.6498       0.85280001   0.66530001   0.8229    ]
[37m[1m [524.92717087   0.31189999   0.7166       0.60409993   0.67650002]]
[37m[1m[2023-06-25 10:32:50,678][129146] Max Reward on eval: 686.3131857682165
[37m[1m[2023-06-25 10:32:50,678][129146] Min Reward on eval: 23.020443216594867
[37m[1m[2023-06-25 10:32:50,678][129146] Mean Reward across all agents: 515.5147840558295
[37m[1m[2023-06-25 10:32:50,678][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:32:50,686][129146] mean_value=256.5396840981653, max_value=1001.2449432093185
[37m[1m[2023-06-25 10:32:50,688][129146] New mean coefficients: [[ 2.0424337   0.19854364  4.062176    0.7208691  -3.759116  ]]
[37m[1m[2023-06-25 10:32:50,689][129146] Moving the mean solution point...
[36m[2023-06-25 10:33:00,359][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 10:33:00,359][129146] FPS: 397198.89
[36m[2023-06-25 10:33:00,361][129146] itr=1042, itrs=2000, Progress: 52.10%
[36m[2023-06-25 10:33:11,844][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 10:33:11,844][129146] FPS: 335163.60
[36m[2023-06-25 10:33:16,333][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:33:16,333][129146] Reward + Measures: [[632.60118299   0.34346667   0.68982631   0.54419965   0.59162831]]
[37m[1m[2023-06-25 10:33:16,334][129146] Max Reward on eval: 632.601182990646
[37m[1m[2023-06-25 10:33:16,334][129146] Min Reward on eval: 632.601182990646
[37m[1m[2023-06-25 10:33:16,334][129146] Mean Reward across all agents: 632.601182990646
[37m[1m[2023-06-25 10:33:16,334][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:33:21,343][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:33:21,344][129146] Reward + Measures: [[717.66210013   0.33520001   0.65070003   0.69550002   0.49849996]
[37m[1m [586.67499153   0.45889997   0.54420006   0.62980002   0.4341    ]
[37m[1m [573.56243787   0.62840003   0.37310001   0.69999999   0.39580002]
[37m[1m ...
[37m[1m [741.02739217   0.47100002   0.5151       0.6766001    0.414     ]
[37m[1m [687.70974741   0.32080001   0.76120001   0.62800002   0.65170002]
[37m[1m [692.18043147   0.38439998   0.64970005   0.57930005   0.53390002]]
[37m[1m[2023-06-25 10:33:21,344][129146] Max Reward on eval: 991.2162918257993
[37m[1m[2023-06-25 10:33:21,344][129146] Min Reward on eval: 438.68273248229525
[37m[1m[2023-06-25 10:33:21,344][129146] Mean Reward across all agents: 707.0616568791049
[37m[1m[2023-06-25 10:33:21,345][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:33:21,355][129146] mean_value=900.9639723200124, max_value=1491.2162918257993
[37m[1m[2023-06-25 10:33:21,358][129146] New mean coefficients: [[ 2.5960627 -0.8393297  5.120499   0.5202744 -3.8834784]]
[37m[1m[2023-06-25 10:33:21,359][129146] Moving the mean solution point...
[36m[2023-06-25 10:33:30,699][129146] train() took 9.34 seconds to complete
[36m[2023-06-25 10:33:30,700][129146] FPS: 411194.37
[36m[2023-06-25 10:33:30,702][129146] itr=1043, itrs=2000, Progress: 52.15%
[36m[2023-06-25 10:33:42,274][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 10:33:42,275][129146] FPS: 332591.28
[36m[2023-06-25 10:33:47,088][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:33:47,094][129146] Reward + Measures: [[749.33626178   0.33739901   0.72272295   0.52497059   0.55557835]]
[37m[1m[2023-06-25 10:33:47,094][129146] Max Reward on eval: 749.3362617785397
[37m[1m[2023-06-25 10:33:47,095][129146] Min Reward on eval: 749.3362617785397
[37m[1m[2023-06-25 10:33:47,095][129146] Mean Reward across all agents: 749.3362617785397
[37m[1m[2023-06-25 10:33:47,095][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:33:52,511][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:33:52,517][129146] Reward + Measures: [[483.58544644   0.4251       0.82790005   0.61869997   0.62290001]
[37m[1m [666.63641612   0.38030002   0.74620003   0.44910002   0.6081    ]
[37m[1m [504.44843277   0.46250001   0.76390004   0.5751       0.54860002]
[37m[1m ...
[37m[1m [785.54173598   0.3572       0.73939997   0.47389999   0.56940001]
[37m[1m [829.77995368   0.3053       0.79110003   0.49969998   0.55840009]
[37m[1m [798.66325253   0.26550001   0.79090005   0.48769999   0.65969998]]
[37m[1m[2023-06-25 10:33:52,517][129146] Max Reward on eval: 1002.9882765868039
[37m[1m[2023-06-25 10:33:52,517][129146] Min Reward on eval: 140.28905364352977
[37m[1m[2023-06-25 10:33:52,518][129146] Mean Reward across all agents: 680.9164109595567
[37m[1m[2023-06-25 10:33:52,518][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:33:52,526][129146] mean_value=513.5801654185449, max_value=1478.225419339654
[37m[1m[2023-06-25 10:33:52,529][129146] New mean coefficients: [[ 2.5120547 -0.6632956  3.7064795  0.7597806 -3.4050598]]
[37m[1m[2023-06-25 10:33:52,530][129146] Moving the mean solution point...
[36m[2023-06-25 10:34:02,276][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 10:34:02,277][129146] FPS: 394045.91
[36m[2023-06-25 10:34:02,279][129146] itr=1044, itrs=2000, Progress: 52.20%
[36m[2023-06-25 10:34:13,874][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 10:34:13,875][129146] FPS: 331865.43
[36m[2023-06-25 10:34:18,635][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:34:18,635][129146] Reward + Measures: [[901.71497172   0.33054066   0.74949527   0.50393796   0.50336534]]
[37m[1m[2023-06-25 10:34:18,635][129146] Max Reward on eval: 901.7149717222375
[37m[1m[2023-06-25 10:34:18,636][129146] Min Reward on eval: 901.7149717222375
[37m[1m[2023-06-25 10:34:18,636][129146] Mean Reward across all agents: 901.7149717222375
[37m[1m[2023-06-25 10:34:18,636][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:34:24,083][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:34:24,083][129146] Reward + Measures: [[ 860.44141036    0.35690001    0.73440003    0.66780007    0.55340004]
[37m[1m [1064.29816999    0.2191        0.8398        0.48839998    0.50880003]
[37m[1m [ 972.91219243    0.36410001    0.75999999    0.5406        0.4402    ]
[37m[1m ...
[37m[1m [ 515.1784999     0.5456        0.57590002    0.67939997    0.5316    ]
[37m[1m [ 700.94856262    0.4632        0.63849998    0.76790005    0.5219    ]
[37m[1m [1006.4123953     0.22719999    0.81890005    0.50910002    0.55890006]]
[37m[1m[2023-06-25 10:34:24,083][129146] Max Reward on eval: 1098.23454710457
[37m[1m[2023-06-25 10:34:24,084][129146] Min Reward on eval: 309.4070257113315
[37m[1m[2023-06-25 10:34:24,084][129146] Mean Reward across all agents: 673.8698917688722
[37m[1m[2023-06-25 10:34:24,084][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:34:24,095][129146] mean_value=653.7137037134196, max_value=1564.2981699851225
[37m[1m[2023-06-25 10:34:24,097][129146] New mean coefficients: [[ 2.5766149  -0.40440953  3.7769587   0.10725242 -3.2410796 ]]
[37m[1m[2023-06-25 10:34:24,098][129146] Moving the mean solution point...
[36m[2023-06-25 10:34:33,779][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 10:34:33,779][129146] FPS: 396731.57
[36m[2023-06-25 10:34:33,782][129146] itr=1045, itrs=2000, Progress: 52.25%
[36m[2023-06-25 10:34:45,313][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 10:34:45,313][129146] FPS: 333784.29
[36m[2023-06-25 10:34:50,135][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:34:50,140][129146] Reward + Measures: [[1223.74496765    0.33569565    0.77477068    0.46067032    0.4229607 ]]
[37m[1m[2023-06-25 10:34:50,141][129146] Max Reward on eval: 1223.7449676474848
[37m[1m[2023-06-25 10:34:50,141][129146] Min Reward on eval: 1223.7449676474848
[37m[1m[2023-06-25 10:34:50,141][129146] Mean Reward across all agents: 1223.7449676474848
[37m[1m[2023-06-25 10:34:50,142][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:34:55,855][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:34:55,856][129146] Reward + Measures: [[1195.30955371    0.19670001    0.8398        0.41820002    0.49689999]
[37m[1m [ 977.71566       0.10420001    0.83619994    0.39300001    0.60000002]
[37m[1m [ 880.60767664    0.10780001    0.82730007    0.37380001    0.60129994]
[37m[1m ...
[37m[1m [1085.29864019    0.13          0.83010006    0.42020002    0.5474    ]
[37m[1m [1065.65052693    0.1176        0.83099997    0.39860001    0.57270002]
[37m[1m [ 812.57761314    0.07080001    0.833         0.37460002    0.69039994]]
[37m[1m[2023-06-25 10:34:55,856][129146] Max Reward on eval: 1270.4337223475334
[37m[1m[2023-06-25 10:34:55,856][129146] Min Reward on eval: 754.9425411119591
[37m[1m[2023-06-25 10:34:55,857][129146] Mean Reward across all agents: 1049.0468130324346
[37m[1m[2023-06-25 10:34:55,857][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:34:55,865][129146] mean_value=1102.0445552549008, max_value=1770.4337223475334
[37m[1m[2023-06-25 10:34:55,868][129146] New mean coefficients: [[ 2.3222666  -0.2545852   3.7094305  -0.22202685 -3.4230356 ]]
[37m[1m[2023-06-25 10:34:55,869][129146] Moving the mean solution point...
[36m[2023-06-25 10:35:05,633][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 10:35:05,633][129146] FPS: 393345.91
[36m[2023-06-25 10:35:05,636][129146] itr=1046, itrs=2000, Progress: 52.30%
[36m[2023-06-25 10:35:17,235][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 10:35:17,236][129146] FPS: 331707.84
[36m[2023-06-25 10:35:21,950][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:35:21,950][129146] Reward + Measures: [[1351.34751411    0.25669199    0.78775299    0.45076829    0.38083935]]
[37m[1m[2023-06-25 10:35:21,951][129146] Max Reward on eval: 1351.3475141115464
[37m[1m[2023-06-25 10:35:21,951][129146] Min Reward on eval: 1351.3475141115464
[37m[1m[2023-06-25 10:35:21,951][129146] Mean Reward across all agents: 1351.3475141115464
[37m[1m[2023-06-25 10:35:21,951][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:35:27,406][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:35:27,407][129146] Reward + Measures: [[1224.15013407    0.36379999    0.76309997    0.50980008    0.33419999]
[37m[1m [ 951.11340079    0.46799999    0.64889997    0.64569998    0.32980001]
[37m[1m [1203.2535907     0.3698        0.73990005    0.53260005    0.32679999]
[37m[1m ...
[37m[1m [1116.39084275    0.36180001    0.73439997    0.57010001    0.36810002]
[37m[1m [1155.49349513    0.37739998    0.74890006    0.53590006    0.35370001]
[37m[1m [ 949.6169353     0.4513        0.66580003    0.63009995    0.33919999]]
[37m[1m[2023-06-25 10:35:27,407][129146] Max Reward on eval: 1471.077731159213
[37m[1m[2023-06-25 10:35:27,408][129146] Min Reward on eval: 730.5302086648996
[37m[1m[2023-06-25 10:35:27,408][129146] Mean Reward across all agents: 1132.6345815747243
[37m[1m[2023-06-25 10:35:27,408][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:35:27,416][129146] mean_value=1419.8137874823121, max_value=1971.077731159213
[37m[1m[2023-06-25 10:35:27,419][129146] New mean coefficients: [[ 2.6376452  -0.7163008   3.226274   -0.39677098 -2.2402883 ]]
[37m[1m[2023-06-25 10:35:27,420][129146] Moving the mean solution point...
[36m[2023-06-25 10:35:37,129][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 10:35:37,129][129146] FPS: 395569.42
[36m[2023-06-25 10:35:37,131][129146] itr=1047, itrs=2000, Progress: 52.35%
[36m[2023-06-25 10:35:48,763][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 10:35:48,764][129146] FPS: 330859.14
[36m[2023-06-25 10:35:53,506][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:35:53,506][129146] Reward + Measures: [[1638.21751819    0.25259665    0.80322397    0.39958167    0.33162966]]
[37m[1m[2023-06-25 10:35:53,506][129146] Max Reward on eval: 1638.2175181886369
[37m[1m[2023-06-25 10:35:53,506][129146] Min Reward on eval: 1638.2175181886369
[37m[1m[2023-06-25 10:35:53,507][129146] Mean Reward across all agents: 1638.2175181886369
[37m[1m[2023-06-25 10:35:53,507][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:35:58,959][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:35:58,959][129146] Reward + Measures: [[-528.05951526    0.46384755    0.17098689    0.38648853    0.34018853]
[37m[1m [-786.17170538    0.32304808    0.17451584    0.22598326    0.25934896]
[37m[1m [ 424.74652709    0.5061        0.3335        0.47639999    0.42899999]
[37m[1m ...
[37m[1m [1099.31795672    0.1903        0.76519996    0.41280004    0.43490002]
[37m[1m [-412.41489546    0.49424252    0.1845106     0.3797462     0.36778504]
[37m[1m [-446.21774274    0.40636545    0.1730864     0.33248639    0.29557893]]
[37m[1m[2023-06-25 10:35:58,959][129146] Max Reward on eval: 1581.5963705505244
[37m[1m[2023-06-25 10:35:58,960][129146] Min Reward on eval: -1408.5634450743614
[37m[1m[2023-06-25 10:35:58,960][129146] Mean Reward across all agents: 324.5008843431651
[37m[1m[2023-06-25 10:35:58,960][129146] Average Trajectory Length: 949.477
[36m[2023-06-25 10:35:58,964][129146] mean_value=-244.14818775284476, max_value=1976.9386420573107
[37m[1m[2023-06-25 10:35:58,967][129146] New mean coefficients: [[ 2.7998717  -0.8703506   3.0535963  -0.25368702 -0.7565347 ]]
[37m[1m[2023-06-25 10:35:58,968][129146] Moving the mean solution point...
[36m[2023-06-25 10:36:08,671][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 10:36:08,671][129146] FPS: 395818.42
[36m[2023-06-25 10:36:08,674][129146] itr=1048, itrs=2000, Progress: 52.40%
[36m[2023-06-25 10:36:20,173][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 10:36:20,173][129146] FPS: 334695.48
[36m[2023-06-25 10:36:24,976][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:36:24,976][129146] Reward + Measures: [[1918.96926637    0.21291301    0.80918139    0.35291597    0.30137134]]
[37m[1m[2023-06-25 10:36:24,976][129146] Max Reward on eval: 1918.969266370411
[37m[1m[2023-06-25 10:36:24,976][129146] Min Reward on eval: 1918.969266370411
[37m[1m[2023-06-25 10:36:24,977][129146] Mean Reward across all agents: 1918.969266370411
[37m[1m[2023-06-25 10:36:24,977][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:36:30,542][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:36:30,542][129146] Reward + Measures: [[390.37701688   0.62889999   0.63129997   0.65239996   0.27110001]
[37m[1m [788.70294348   0.0746       0.7202       0.52640003   0.69749999]
[37m[1m [960.82815045   0.08050001   0.72069997   0.51270002   0.6214    ]
[37m[1m ...
[37m[1m [500.05110393   0.0212       0.66090006   0.56709999   0.85170001]
[37m[1m [565.41956975   0.0298       0.65189999   0.5499       0.82700008]
[37m[1m [277.50963011   0.55310005   0.62110001   0.61149997   0.34060001]]
[37m[1m[2023-06-25 10:36:30,542][129146] Max Reward on eval: 1857.1825959831708
[37m[1m[2023-06-25 10:36:30,543][129146] Min Reward on eval: -1413.187300644943
[37m[1m[2023-06-25 10:36:30,543][129146] Mean Reward across all agents: 425.8060488731674
[37m[1m[2023-06-25 10:36:30,543][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:36:30,550][129146] mean_value=160.01783378078514, max_value=2223.1085515923564
[37m[1m[2023-06-25 10:36:30,553][129146] New mean coefficients: [[ 2.9486063  -0.6583516   2.4779973  -0.18185224  0.17598742]]
[37m[1m[2023-06-25 10:36:30,554][129146] Moving the mean solution point...
[36m[2023-06-25 10:36:40,340][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 10:36:40,341][129146] FPS: 392453.71
[36m[2023-06-25 10:36:40,343][129146] itr=1049, itrs=2000, Progress: 52.45%
[36m[2023-06-25 10:36:51,922][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 10:36:51,922][129146] FPS: 332318.99
[36m[2023-06-25 10:36:56,598][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:36:56,598][129146] Reward + Measures: [[2158.4762022     0.20984933    0.7987293     0.34953898    0.28471866]]
[37m[1m[2023-06-25 10:36:56,599][129146] Max Reward on eval: 2158.4762022011882
[37m[1m[2023-06-25 10:36:56,599][129146] Min Reward on eval: 2158.4762022011882
[37m[1m[2023-06-25 10:36:56,599][129146] Mean Reward across all agents: 2158.4762022011882
[37m[1m[2023-06-25 10:36:56,599][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:37:02,105][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:37:02,106][129146] Reward + Measures: [[ 382.86514742    0.28029999    0.65679997    0.1455        0.69909996]
[37m[1m [1482.04653799    0.25350001    0.72209996    0.40380001    0.34880003]
[37m[1m [-763.47870894    0.0018        0.97790003    0.94939995    0.97080004]
[37m[1m ...
[37m[1m [ 874.35210338    0.0859        0.76230007    0.54410005    0.56740004]
[37m[1m [1036.01142302    0.20770001    0.78200001    0.51110005    0.53680003]
[37m[1m [-118.6981136     0.22590001    0.59200001    0.20549999    0.583     ]]
[37m[1m[2023-06-25 10:37:02,106][129146] Max Reward on eval: 2059.5256593488157
[37m[1m[2023-06-25 10:37:02,106][129146] Min Reward on eval: -1002.3595958061051
[37m[1m[2023-06-25 10:37:02,106][129146] Mean Reward across all agents: 809.9328639260766
[37m[1m[2023-06-25 10:37:02,107][129146] Average Trajectory Length: 999.2503333333333
[36m[2023-06-25 10:37:02,113][129146] mean_value=474.8753217721618, max_value=2402.2785521942883
[37m[1m[2023-06-25 10:37:02,116][129146] New mean coefficients: [[ 2.7294087  -0.9539186   2.7009594  -0.21704984 -0.2865664 ]]
[37m[1m[2023-06-25 10:37:02,117][129146] Moving the mean solution point...
[36m[2023-06-25 10:37:11,733][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 10:37:11,733][129146] FPS: 399397.99
[36m[2023-06-25 10:37:11,735][129146] itr=1050, itrs=2000, Progress: 52.50%
[37m[1m[2023-06-25 10:37:18,962][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001030
[36m[2023-06-25 10:37:30,632][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 10:37:30,633][129146] FPS: 335649.51
[36m[2023-06-25 10:37:35,416][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:37:35,417][129146] Reward + Measures: [[1169.48497701    0.34412599    0.68963695    0.4118273     0.44942567]]
[37m[1m[2023-06-25 10:37:35,417][129146] Max Reward on eval: 1169.48497700816
[37m[1m[2023-06-25 10:37:35,417][129146] Min Reward on eval: 1169.48497700816
[37m[1m[2023-06-25 10:37:35,418][129146] Mean Reward across all agents: 1169.48497700816
[37m[1m[2023-06-25 10:37:35,418][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:37:40,871][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:37:40,872][129146] Reward + Measures: [[1049.40831179    0.37110001    0.75400001    0.49969998    0.3443    ]
[37m[1m [ 122.82678106    0.72329998    0.77979994    0.89700001    0.0714    ]
[37m[1m [1033.30679063    0.32640001    0.72460002    0.42459998    0.36660001]
[37m[1m ...
[37m[1m [1029.44712485    0.32340002    0.73520005    0.41140005    0.5323    ]
[37m[1m [ 868.45839719    0.29230002    0.52489996    0.33129999    0.43000004]
[37m[1m [ 590.11973386    0.48389998    0.53039998    0.39789999    0.4542    ]]
[37m[1m[2023-06-25 10:37:40,872][129146] Max Reward on eval: 1463.2846389654792
[37m[1m[2023-06-25 10:37:40,872][129146] Min Reward on eval: -556.6084100858018
[37m[1m[2023-06-25 10:37:40,873][129146] Mean Reward across all agents: 927.4090162655917
[37m[1m[2023-06-25 10:37:40,873][129146] Average Trajectory Length: 996.153
[36m[2023-06-25 10:37:40,879][129146] mean_value=167.9295387040657, max_value=1580.838102919492
[37m[1m[2023-06-25 10:37:40,882][129146] New mean coefficients: [[ 2.6514316  -1.144399    2.3731742   0.01880385 -0.328151  ]]
[37m[1m[2023-06-25 10:37:40,883][129146] Moving the mean solution point...
[36m[2023-06-25 10:37:50,636][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 10:37:50,636][129146] FPS: 393805.56
[36m[2023-06-25 10:37:50,638][129146] itr=1051, itrs=2000, Progress: 52.55%
[36m[2023-06-25 10:38:02,032][129146] train() took 11.37 seconds to complete
[36m[2023-06-25 10:38:02,032][129146] FPS: 337713.79
[36m[2023-06-25 10:38:06,760][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:38:06,761][129146] Reward + Measures: [[1598.68803833    0.27739465    0.70415664    0.42428532    0.36133936]]
[37m[1m[2023-06-25 10:38:06,761][129146] Max Reward on eval: 1598.6880383252737
[37m[1m[2023-06-25 10:38:06,761][129146] Min Reward on eval: 1598.6880383252737
[37m[1m[2023-06-25 10:38:06,761][129146] Mean Reward across all agents: 1598.6880383252737
[37m[1m[2023-06-25 10:38:06,761][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:38:12,179][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:38:12,179][129146] Reward + Measures: [[707.2360228    0.21400002   0.60369998   0.31290004   0.55830002]
[37m[1m [490.316956     0.7633       0.64840001   0.68949997   0.27690002]
[37m[1m [-68.87922052   0.1193       0.46820003   0.38080001   0.44490001]
[37m[1m ...
[37m[1m [ 72.86489032   0.44630003   0.62970001   0.52789998   0.58859998]
[37m[1m [-76.10460154   0.70499998   0.80930007   0.72640008   0.76179999]
[37m[1m [  0.95531311   0.30720001   0.52249998   0.37900001   0.38270003]]
[37m[1m[2023-06-25 10:38:12,179][129146] Max Reward on eval: 1536.7256124351757
[37m[1m[2023-06-25 10:38:12,180][129146] Min Reward on eval: -813.0763690962224
[37m[1m[2023-06-25 10:38:12,180][129146] Mean Reward across all agents: 532.5251967815685
[37m[1m[2023-06-25 10:38:12,180][129146] Average Trajectory Length: 999.5156666666667
[36m[2023-06-25 10:38:12,187][129146] mean_value=-58.84875139354238, max_value=1822.032065854466
[37m[1m[2023-06-25 10:38:12,189][129146] New mean coefficients: [[ 2.8912678  -0.6637807   2.111522    0.23000702 -0.31162128]]
[37m[1m[2023-06-25 10:38:12,190][129146] Moving the mean solution point...
[36m[2023-06-25 10:38:21,895][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 10:38:21,895][129146] FPS: 395769.15
[36m[2023-06-25 10:38:21,897][129146] itr=1052, itrs=2000, Progress: 52.60%
[36m[2023-06-25 10:38:33,339][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 10:38:33,340][129146] FPS: 336266.72
[36m[2023-06-25 10:38:37,819][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:38:37,819][129146] Reward + Measures: [[1702.34240904    0.20977193    0.70429385    0.30706218    0.28654134]]
[37m[1m[2023-06-25 10:38:37,819][129146] Max Reward on eval: 1702.3424090355973
[37m[1m[2023-06-25 10:38:37,819][129146] Min Reward on eval: 1702.3424090355973
[37m[1m[2023-06-25 10:38:37,820][129146] Mean Reward across all agents: 1702.3424090355973
[37m[1m[2023-06-25 10:38:37,820][129146] Average Trajectory Length: 998.1723333333333
[36m[2023-06-25 10:38:43,187][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:38:43,187][129146] Reward + Measures: [[1150.48008372    0.16840002    0.63440001    0.34010002    0.32160002]
[37m[1m [1078.99084404    0.34190002    0.71640009    0.22850001    0.45000002]
[37m[1m [ 981.64293057    0.4066        0.588         0.30310002    0.41260004]
[37m[1m ...
[37m[1m [1371.23139719    0.25570002    0.76200002    0.25440001    0.38550001]
[37m[1m [1364.32796704    0.21700001    0.61970001    0.2359        0.37579998]
[37m[1m [1235.18209278    0.2098        0.44670001    0.24920002    0.30239999]]
[37m[1m[2023-06-25 10:38:43,188][129146] Max Reward on eval: 1654.006572978571
[37m[1m[2023-06-25 10:38:43,188][129146] Min Reward on eval: 359.56226487862875
[37m[1m[2023-06-25 10:38:43,188][129146] Mean Reward across all agents: 1134.0519570090205
[37m[1m[2023-06-25 10:38:43,188][129146] Average Trajectory Length: 992.0886666666667
[36m[2023-06-25 10:38:43,195][129146] mean_value=284.4618613523084, max_value=1985.0632394093786
[37m[1m[2023-06-25 10:38:43,198][129146] New mean coefficients: [[ 3.108167    0.2837835   1.569098    0.49070346 -0.5698933 ]]
[37m[1m[2023-06-25 10:38:43,199][129146] Moving the mean solution point...
[36m[2023-06-25 10:38:53,152][129146] train() took 9.95 seconds to complete
[36m[2023-06-25 10:38:53,153][129146] FPS: 385866.95
[36m[2023-06-25 10:38:53,155][129146] itr=1053, itrs=2000, Progress: 52.65%
[36m[2023-06-25 10:39:04,830][129146] train() took 11.65 seconds to complete
[36m[2023-06-25 10:39:04,830][129146] FPS: 329613.06
[36m[2023-06-25 10:39:09,735][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:39:09,736][129146] Reward + Measures: [[1940.37191025    0.18495202    0.73792636    0.28035638    0.26971439]]
[37m[1m[2023-06-25 10:39:09,736][129146] Max Reward on eval: 1940.3719102518157
[37m[1m[2023-06-25 10:39:09,736][129146] Min Reward on eval: 1940.3719102518157
[37m[1m[2023-06-25 10:39:09,736][129146] Mean Reward across all agents: 1940.3719102518157
[37m[1m[2023-06-25 10:39:09,737][129146] Average Trajectory Length: 999.7379999999999
[36m[2023-06-25 10:39:15,279][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:39:15,280][129146] Reward + Measures: [[ 406.52220036    0.27080002    0.30320001    0.42870003    0.398     ]
[37m[1m [1028.14447209    0.2034        0.78459996    0.40260002    0.43290001]
[37m[1m [ 720.09308742    0.22090001    0.68180001    0.3888        0.51590002]
[37m[1m ...
[37m[1m [ 944.52343155    0.22069998    0.31739998    0.2201        0.24489999]
[37m[1m [1950.03032636    0.20190001    0.77360004    0.28100002    0.27280003]
[37m[1m [ 548.09879528    0.26163623    0.3222599     0.27802035    0.27666056]]
[37m[1m[2023-06-25 10:39:15,280][129146] Max Reward on eval: 1955.4429626916303
[37m[1m[2023-06-25 10:39:15,280][129146] Min Reward on eval: -441.37095198524185
[37m[1m[2023-06-25 10:39:15,281][129146] Mean Reward across all agents: 963.0484185246312
[37m[1m[2023-06-25 10:39:15,281][129146] Average Trajectory Length: 982.5716666666666
[36m[2023-06-25 10:39:15,288][129146] mean_value=12.987820877962998, max_value=1651.1505905447784
[37m[1m[2023-06-25 10:39:15,290][129146] New mean coefficients: [[ 3.2402225   0.3421598   1.8420191   0.7628315  -0.32327068]]
[37m[1m[2023-06-25 10:39:15,291][129146] Moving the mean solution point...
[36m[2023-06-25 10:39:25,187][129146] train() took 9.89 seconds to complete
[36m[2023-06-25 10:39:25,187][129146] FPS: 388118.50
[36m[2023-06-25 10:39:25,189][129146] itr=1054, itrs=2000, Progress: 52.70%
[36m[2023-06-25 10:39:36,870][129146] train() took 11.65 seconds to complete
[36m[2023-06-25 10:39:36,870][129146] FPS: 329505.18
[36m[2023-06-25 10:39:41,638][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:39:41,638][129146] Reward + Measures: [[2109.02548152    0.17284106    0.75788623    0.26054519    0.2610797 ]]
[37m[1m[2023-06-25 10:39:41,638][129146] Max Reward on eval: 2109.025481515226
[37m[1m[2023-06-25 10:39:41,638][129146] Min Reward on eval: 2109.025481515226
[37m[1m[2023-06-25 10:39:41,638][129146] Mean Reward across all agents: 2109.025481515226
[37m[1m[2023-06-25 10:39:41,639][129146] Average Trajectory Length: 999.8439999999999
[36m[2023-06-25 10:39:47,289][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:39:47,290][129146] Reward + Measures: [[1569.62257621    0.22360002    0.6372        0.2383        0.24260001]
[37m[1m [1747.95244368    0.2138        0.57499999    0.23459999    0.28939998]
[37m[1m [ 261.06305171    0.60070002    0.65810007    0.45949998    0.4289    ]
[37m[1m ...
[37m[1m [1933.05524672    0.17999999    0.66990006    0.26050001    0.28659996]
[37m[1m [1071.29839341    0.23699999    0.40040001    0.19770001    0.22000001]
[37m[1m [ 177.20576973    0.27080002    0.41599998    0.2254        0.3197    ]]
[37m[1m[2023-06-25 10:39:47,290][129146] Max Reward on eval: 2107.270275324886
[37m[1m[2023-06-25 10:39:47,290][129146] Min Reward on eval: -299.18061341621217
[37m[1m[2023-06-25 10:39:47,291][129146] Mean Reward across all agents: 932.7409905923145
[37m[1m[2023-06-25 10:39:47,291][129146] Average Trajectory Length: 973.307
[36m[2023-06-25 10:39:47,295][129146] mean_value=-810.1515534201928, max_value=2187.2389460456325
[37m[1m[2023-06-25 10:39:47,298][129146] New mean coefficients: [[ 3.0264926  -0.37263972  1.8180767   0.34806573 -0.14981812]]
[37m[1m[2023-06-25 10:39:47,299][129146] Moving the mean solution point...
[36m[2023-06-25 10:39:57,034][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 10:39:57,034][129146] FPS: 394515.07
[36m[2023-06-25 10:39:57,037][129146] itr=1055, itrs=2000, Progress: 52.75%
[36m[2023-06-25 10:40:08,458][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 10:40:08,458][129146] FPS: 337026.27
[36m[2023-06-25 10:40:13,218][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:40:13,219][129146] Reward + Measures: [[2271.10054322    0.160557      0.77945358    0.24790233    0.25060266]]
[37m[1m[2023-06-25 10:40:13,219][129146] Max Reward on eval: 2271.100543218782
[37m[1m[2023-06-25 10:40:13,219][129146] Min Reward on eval: 2271.100543218782
[37m[1m[2023-06-25 10:40:13,219][129146] Mean Reward across all agents: 2271.100543218782
[37m[1m[2023-06-25 10:40:13,220][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:40:18,680][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:40:18,680][129146] Reward + Measures: [[ 120.41828145    0.2494258     0.32163587    0.22709684    0.2017415 ]
[37m[1m [ 694.34014087    0.35020003    0.63920003    0.30830002    0.3935    ]
[37m[1m [1129.7250702     0.26989999    0.69840002    0.34650001    0.26989999]
[37m[1m ...
[37m[1m [1576.52045556    0.20920001    0.65570003    0.24620001    0.24749999]
[37m[1m [  61.69059598    0.21113606    0.2988126     0.19560282    0.186378  ]
[37m[1m [ 710.93969915    0.43560001    0.64840001    0.38499999    0.48610002]]
[37m[1m[2023-06-25 10:40:18,681][129146] Max Reward on eval: 2222.5090646516765
[37m[1m[2023-06-25 10:40:18,681][129146] Min Reward on eval: -274.3061009945639
[37m[1m[2023-06-25 10:40:18,681][129146] Mean Reward across all agents: 1033.3872992469323
[37m[1m[2023-06-25 10:40:18,681][129146] Average Trajectory Length: 981.2276666666667
[36m[2023-06-25 10:40:18,686][129146] mean_value=-161.2502139306788, max_value=2562.1066957125904
[37m[1m[2023-06-25 10:40:18,689][129146] New mean coefficients: [[ 2.9171605  -0.16835704  2.0549273   0.0280807  -0.48375165]]
[37m[1m[2023-06-25 10:40:18,690][129146] Moving the mean solution point...
[36m[2023-06-25 10:40:28,408][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 10:40:28,408][129146] FPS: 395213.45
[36m[2023-06-25 10:40:28,411][129146] itr=1056, itrs=2000, Progress: 52.80%
[36m[2023-06-25 10:40:39,858][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 10:40:39,859][129146] FPS: 336227.71
[36m[2023-06-25 10:40:44,641][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:40:44,641][129146] Reward + Measures: [[2425.23862383    0.14927867    0.798163      0.234565      0.24293864]]
[37m[1m[2023-06-25 10:40:44,642][129146] Max Reward on eval: 2425.238623833078
[37m[1m[2023-06-25 10:40:44,642][129146] Min Reward on eval: 2425.238623833078
[37m[1m[2023-06-25 10:40:44,642][129146] Mean Reward across all agents: 2425.238623833078
[37m[1m[2023-06-25 10:40:44,642][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:40:50,061][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:40:50,061][129146] Reward + Measures: [[1952.41583677    0.20690003    0.68099993    0.31549999    0.21479999]
[37m[1m [1436.84134061    0.23080002    0.5377        0.3696        0.22729997]
[37m[1m [1441.81937932    0.30370003    0.66670001    0.2579        0.30899999]
[37m[1m ...
[37m[1m [1327.95926333    0.2694        0.64600003    0.45560002    0.30020005]
[37m[1m [1570.66965574    0.23400001    0.74909997    0.38280001    0.31009999]
[37m[1m [1552.54099642    0.22830001    0.74860001    0.32460001    0.32530004]]
[37m[1m[2023-06-25 10:40:50,061][129146] Max Reward on eval: 2418.656059885863
[37m[1m[2023-06-25 10:40:50,062][129146] Min Reward on eval: -306.9257680149167
[37m[1m[2023-06-25 10:40:50,062][129146] Mean Reward across all agents: 1191.6895251469593
[37m[1m[2023-06-25 10:40:50,062][129146] Average Trajectory Length: 997.8763333333333
[36m[2023-06-25 10:40:50,071][129146] mean_value=523.2846899277372, max_value=2452.415836769482
[37m[1m[2023-06-25 10:40:50,074][129146] New mean coefficients: [[ 2.8808546   0.00881404  1.9605345  -0.30580515 -0.98137176]]
[37m[1m[2023-06-25 10:40:50,075][129146] Moving the mean solution point...
[36m[2023-06-25 10:40:59,852][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 10:40:59,853][129146] FPS: 392798.27
[36m[2023-06-25 10:40:59,855][129146] itr=1057, itrs=2000, Progress: 52.85%
[36m[2023-06-25 10:41:11,543][129146] train() took 11.66 seconds to complete
[36m[2023-06-25 10:41:11,543][129146] FPS: 329228.77
[36m[2023-06-25 10:41:16,347][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:41:16,347][129146] Reward + Measures: [[2566.8251007     0.14614834    0.80096632    0.22234668    0.23545198]]
[37m[1m[2023-06-25 10:41:16,348][129146] Max Reward on eval: 2566.825100698231
[37m[1m[2023-06-25 10:41:16,348][129146] Min Reward on eval: 2566.825100698231
[37m[1m[2023-06-25 10:41:16,348][129146] Mean Reward across all agents: 2566.825100698231
[37m[1m[2023-06-25 10:41:16,348][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:41:21,906][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:41:21,906][129146] Reward + Measures: [[1197.3653351     0.23590003    0.53610003    0.28749999    0.34489998]
[37m[1m [1013.66290874    0.57770002    0.35910001    0.39560002    0.60410005]
[37m[1m [2196.70874528    0.22760001    0.76920003    0.23989999    0.24800001]
[37m[1m ...
[37m[1m [1509.41998729    0.2405        0.5316        0.24860001    0.39100003]
[37m[1m [1278.93210784    0.3258        0.72650003    0.2326        0.50839996]
[37m[1m [1045.74673777    0.55910003    0.60409993    0.45490003    0.68090004]]
[37m[1m[2023-06-25 10:41:21,906][129146] Max Reward on eval: 2438.688253921294
[37m[1m[2023-06-25 10:41:21,907][129146] Min Reward on eval: -353.690104031947
[37m[1m[2023-06-25 10:41:21,907][129146] Mean Reward across all agents: 1195.5370370933524
[37m[1m[2023-06-25 10:41:21,907][129146] Average Trajectory Length: 996.9143333333333
[36m[2023-06-25 10:41:21,914][129146] mean_value=53.67599774936076, max_value=2765.067764772661
[37m[1m[2023-06-25 10:41:21,917][129146] New mean coefficients: [[ 2.8256488   0.00382729  1.927268   -0.03694454 -1.0829709 ]]
[37m[1m[2023-06-25 10:41:21,918][129146] Moving the mean solution point...
[36m[2023-06-25 10:41:31,718][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 10:41:31,718][129146] FPS: 391898.42
[36m[2023-06-25 10:41:31,721][129146] itr=1058, itrs=2000, Progress: 52.90%
[36m[2023-06-25 10:41:43,347][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 10:41:43,348][129146] FPS: 330972.88
[36m[2023-06-25 10:41:48,198][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:41:48,199][129146] Reward + Measures: [[2679.68834139    0.14385       0.81651902    0.21072       0.22422297]]
[37m[1m[2023-06-25 10:41:48,199][129146] Max Reward on eval: 2679.68834139227
[37m[1m[2023-06-25 10:41:48,199][129146] Min Reward on eval: 2679.68834139227
[37m[1m[2023-06-25 10:41:48,199][129146] Mean Reward across all agents: 2679.68834139227
[37m[1m[2023-06-25 10:41:48,200][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:41:53,749][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:41:53,749][129146] Reward + Measures: [[1486.19002023    0.27830002    0.5826        0.36539999    0.27399999]
[37m[1m [1993.20113304    0.22500001    0.61320001    0.29000002    0.25470001]
[37m[1m [1865.58301485    0.20900002    0.61180001    0.28350002    0.25100002]
[37m[1m ...
[37m[1m [2120.1391844     0.19000001    0.64720005    0.32119998    0.23810001]
[37m[1m [1400.97613692    0.21279998    0.50770003    0.29850003    0.30779999]
[37m[1m [1956.05597452    0.21760002    0.70920002    0.35090002    0.2357    ]]
[37m[1m[2023-06-25 10:41:53,750][129146] Max Reward on eval: 2629.9865070337196
[37m[1m[2023-06-25 10:41:53,750][129146] Min Reward on eval: -446.0051930619753
[37m[1m[2023-06-25 10:41:53,750][129146] Mean Reward across all agents: 1283.693966824095
[37m[1m[2023-06-25 10:41:53,750][129146] Average Trajectory Length: 998.4066666666666
[36m[2023-06-25 10:41:53,756][129146] mean_value=-11.098966478922666, max_value=1991.7271733574125
[37m[1m[2023-06-25 10:41:53,759][129146] New mean coefficients: [[ 2.9868035  -0.21020295  2.1145902   0.33321145 -0.9376674 ]]
[37m[1m[2023-06-25 10:41:53,759][129146] Moving the mean solution point...
[36m[2023-06-25 10:42:03,503][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 10:42:03,503][129146] FPS: 394187.56
[36m[2023-06-25 10:42:03,505][129146] itr=1059, itrs=2000, Progress: 52.95%
[36m[2023-06-25 10:42:14,941][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 10:42:14,941][129146] FPS: 336571.57
[36m[2023-06-25 10:42:19,640][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:42:19,640][129146] Reward + Measures: [[2805.40509473    0.14404866    0.82288599    0.20425431    0.22672801]]
[37m[1m[2023-06-25 10:42:19,640][129146] Max Reward on eval: 2805.405094734906
[37m[1m[2023-06-25 10:42:19,640][129146] Min Reward on eval: 2805.405094734906
[37m[1m[2023-06-25 10:42:19,641][129146] Mean Reward across all agents: 2805.405094734906
[37m[1m[2023-06-25 10:42:19,641][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:42:25,211][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:42:25,212][129146] Reward + Measures: [[ 895.38634332    0.41980001    0.51859999    0.48179999    0.32220003]
[37m[1m [2159.83140483    0.1796        0.62970001    0.26050004    0.212     ]
[37m[1m [1235.76286312    0.4152        0.54570001    0.39170003    0.30329999]
[37m[1m ...
[37m[1m [2171.15145015    0.198         0.76969999    0.27649999    0.27719998]
[37m[1m [1860.71545081    0.2455        0.83499998    0.27109998    0.28050002]
[37m[1m [1434.08123737    0.29560003    0.68279999    0.35910001    0.32210001]]
[37m[1m[2023-06-25 10:42:25,212][129146] Max Reward on eval: 2780.5571463764413
[37m[1m[2023-06-25 10:42:25,212][129146] Min Reward on eval: -140.91665982517878
[37m[1m[2023-06-25 10:42:25,212][129146] Mean Reward across all agents: 1524.4970259363065
[37m[1m[2023-06-25 10:42:25,213][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:42:25,218][129146] mean_value=175.57584654393776, max_value=2767.7141744215182
[37m[1m[2023-06-25 10:42:25,221][129146] New mean coefficients: [[ 3.0268414  -0.4257514   2.4532943   0.38080525 -0.96401626]]
[37m[1m[2023-06-25 10:42:25,222][129146] Moving the mean solution point...
[36m[2023-06-25 10:42:34,884][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 10:42:34,885][129146] FPS: 397489.34
[36m[2023-06-25 10:42:34,887][129146] itr=1060, itrs=2000, Progress: 53.00%
[37m[1m[2023-06-25 10:42:41,909][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001040
[36m[2023-06-25 10:42:53,793][129146] train() took 11.66 seconds to complete
[36m[2023-06-25 10:42:53,793][129146] FPS: 329322.98
[36m[2023-06-25 10:42:58,497][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:42:58,498][129146] Reward + Measures: [[2907.94962785    0.14265633    0.8332516     0.19689368    0.22812134]]
[37m[1m[2023-06-25 10:42:58,498][129146] Max Reward on eval: 2907.9496278487854
[37m[1m[2023-06-25 10:42:58,498][129146] Min Reward on eval: 2907.9496278487854
[37m[1m[2023-06-25 10:42:58,498][129146] Mean Reward across all agents: 2907.9496278487854
[37m[1m[2023-06-25 10:42:58,499][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:43:03,801][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:43:03,861][129146] Reward + Measures: [[1747.72688952    0.16          0.71750003    0.35679999    0.32409999]
[37m[1m [1165.79316007    0.13570002    0.62810004    0.30859998    0.27490002]
[37m[1m [1285.27958607    0.1232        0.65539998    0.3177        0.28460002]
[37m[1m ...
[37m[1m [1517.30501708    0.24150001    0.60509998    0.38370001    0.30560002]
[37m[1m [ 928.65762604    0.36630002    0.49489999    0.47709998    0.29490003]
[37m[1m [1030.71570568    0.32880002    0.61440003    0.29960001    0.39919996]]
[37m[1m[2023-06-25 10:43:03,862][129146] Max Reward on eval: 2828.768599401694
[37m[1m[2023-06-25 10:43:03,862][129146] Min Reward on eval: -133.1117553884629
[37m[1m[2023-06-25 10:43:03,862][129146] Mean Reward across all agents: 1119.7614728112535
[37m[1m[2023-06-25 10:43:03,862][129146] Average Trajectory Length: 993.1136666666666
[36m[2023-06-25 10:43:03,869][129146] mean_value=212.9907955605381, max_value=2671.1703012173994
[37m[1m[2023-06-25 10:43:03,872][129146] New mean coefficients: [[ 3.0849125  -0.36654827  2.496561    0.21046028 -0.5258094 ]]
[37m[1m[2023-06-25 10:43:03,873][129146] Moving the mean solution point...
[36m[2023-06-25 10:43:13,520][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 10:43:13,520][129146] FPS: 398130.14
[36m[2023-06-25 10:43:13,522][129146] itr=1061, itrs=2000, Progress: 53.05%
[36m[2023-06-25 10:43:25,190][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 10:43:25,191][129146] FPS: 329860.69
[36m[2023-06-25 10:43:29,836][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:43:29,837][129146] Reward + Measures: [[3019.87316859    0.14104201    0.83789933    0.19372867    0.22860834]]
[37m[1m[2023-06-25 10:43:29,837][129146] Max Reward on eval: 3019.8731685916187
[37m[1m[2023-06-25 10:43:29,837][129146] Min Reward on eval: 3019.8731685916187
[37m[1m[2023-06-25 10:43:29,837][129146] Mean Reward across all agents: 3019.8731685916187
[37m[1m[2023-06-25 10:43:29,838][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:43:35,163][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:43:35,164][129146] Reward + Measures: [[2254.11590254    0.171         0.77700007    0.32549998    0.23939998]
[37m[1m [1331.34189017    0.29480001    0.59219998    0.308         0.46599999]
[37m[1m [1570.20127394    0.24150001    0.66790003    0.33080003    0.43459997]
[37m[1m ...
[37m[1m [1605.00200507    0.2457        0.65600002    0.3154        0.4086    ]
[37m[1m [2491.32330217    0.17820001    0.76929998    0.2165        0.28470001]
[37m[1m [ 866.31830683    0.26589999    0.57920009    0.40200001    0.35210004]]
[37m[1m[2023-06-25 10:43:35,164][129146] Max Reward on eval: 2898.7471885534005
[37m[1m[2023-06-25 10:43:35,164][129146] Min Reward on eval: 105.71706410541374
[37m[1m[2023-06-25 10:43:35,164][129146] Mean Reward across all agents: 1903.257434754617
[37m[1m[2023-06-25 10:43:35,165][129146] Average Trajectory Length: 999.6
[36m[2023-06-25 10:43:35,171][129146] mean_value=382.9199728981351, max_value=2590.6883996908555
[37m[1m[2023-06-25 10:43:35,174][129146] New mean coefficients: [[ 3.1253269  -0.7951256   2.7735465   0.16815013 -0.5654954 ]]
[37m[1m[2023-06-25 10:43:35,175][129146] Moving the mean solution point...
[36m[2023-06-25 10:43:44,914][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 10:43:44,914][129146] FPS: 394370.62
[36m[2023-06-25 10:43:44,916][129146] itr=1062, itrs=2000, Progress: 53.10%
[36m[2023-06-25 10:43:56,374][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 10:43:56,374][129146] FPS: 335853.65
[36m[2023-06-25 10:44:01,102][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:44:01,102][129146] Reward + Measures: [[2862.75462326    0.17690867    0.78288502    0.19802767    0.18439968]]
[37m[1m[2023-06-25 10:44:01,102][129146] Max Reward on eval: 2862.7546232632394
[37m[1m[2023-06-25 10:44:01,102][129146] Min Reward on eval: 2862.7546232632394
[37m[1m[2023-06-25 10:44:01,103][129146] Mean Reward across all agents: 2862.7546232632394
[37m[1m[2023-06-25 10:44:01,103][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:44:06,608][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:44:06,609][129146] Reward + Measures: [[1751.93543751    0.21339999    0.47520003    0.39309999    0.18800001]
[37m[1m [2278.99543529    0.18130001    0.76700002    0.25          0.2033    ]
[37m[1m [ 959.54190879    0.2525        0.49430004    0.36209998    0.3204    ]
[37m[1m ...
[37m[1m [ 369.14168542    0.35447693    0.55411541    0.32276154    0.47887692]
[37m[1m [ 974.15150475    0.28027806    0.69996589    0.22714634    0.45521709]
[37m[1m [ 146.54790877    0.26504165    0.45737448    0.29446658    0.36849114]]
[37m[1m[2023-06-25 10:44:06,609][129146] Max Reward on eval: 2789.8095795881004
[37m[1m[2023-06-25 10:44:06,609][129146] Min Reward on eval: -651.8553001366424
[37m[1m[2023-06-25 10:44:06,609][129146] Mean Reward across all agents: 1169.2939377428906
[37m[1m[2023-06-25 10:44:06,610][129146] Average Trajectory Length: 928.1123333333333
[36m[2023-06-25 10:44:06,615][129146] mean_value=-111.09838487670709, max_value=2266.3425643125847
[37m[1m[2023-06-25 10:44:06,617][129146] New mean coefficients: [[ 3.141696   -1.0255014   1.4732573   0.28333992 -0.18286568]]
[37m[1m[2023-06-25 10:44:06,618][129146] Moving the mean solution point...
[36m[2023-06-25 10:44:16,289][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 10:44:16,289][129146] FPS: 397157.54
[36m[2023-06-25 10:44:16,291][129146] itr=1063, itrs=2000, Progress: 53.15%
[36m[2023-06-25 10:44:27,662][129146] train() took 11.35 seconds to complete
[36m[2023-06-25 10:44:27,662][129146] FPS: 338434.32
[36m[2023-06-25 10:44:32,423][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:44:32,423][129146] Reward + Measures: [[3085.30343945    0.18160444    0.76874888    0.19889544    0.17975496]]
[37m[1m[2023-06-25 10:44:32,423][129146] Max Reward on eval: 3085.303439454851
[37m[1m[2023-06-25 10:44:32,424][129146] Min Reward on eval: 3085.303439454851
[37m[1m[2023-06-25 10:44:32,424][129146] Mean Reward across all agents: 3085.303439454851
[37m[1m[2023-06-25 10:44:32,424][129146] Average Trajectory Length: 999.8513333333333
[36m[2023-06-25 10:44:37,793][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:44:37,793][129146] Reward + Measures: [[2212.82399458    0.17129083    0.62985355    0.26312143    0.16989084]
[37m[1m [1645.36310305    0.22060001    0.72699994    0.25139999    0.2264    ]
[37m[1m [1687.40164958    0.2163        0.63850003    0.24969999    0.2739    ]
[37m[1m ...
[37m[1m [ 494.11021319    0.42560002    0.8193        0.55860001    0.62330002]
[37m[1m [2158.46212866    0.23          0.70489997    0.23240001    0.2218    ]
[37m[1m [2002.26915948    0.212         0.69690001    0.24059999    0.24340001]]
[37m[1m[2023-06-25 10:44:37,793][129146] Max Reward on eval: 2824.7232132694685
[37m[1m[2023-06-25 10:44:37,794][129146] Min Reward on eval: 31.29416195410304
[37m[1m[2023-06-25 10:44:37,794][129146] Mean Reward across all agents: 1519.7087321079973
[37m[1m[2023-06-25 10:44:37,794][129146] Average Trajectory Length: 989.5196666666666
[36m[2023-06-25 10:44:37,797][129146] mean_value=-432.4609006153169, max_value=2032.3107891080392
[37m[1m[2023-06-25 10:44:37,800][129146] New mean coefficients: [[ 3.1519902  -0.21500438  0.6858271   0.08709644 -0.32561654]]
[37m[1m[2023-06-25 10:44:37,801][129146] Moving the mean solution point...
[36m[2023-06-25 10:44:47,410][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 10:44:47,410][129146] FPS: 399692.76
[36m[2023-06-25 10:44:47,413][129146] itr=1064, itrs=2000, Progress: 53.20%
[36m[2023-06-25 10:44:58,857][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 10:44:58,858][129146] FPS: 336203.59
[36m[2023-06-25 10:45:03,644][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:45:03,645][129146] Reward + Measures: [[3266.85858694    0.18270698    0.7667923     0.19754234    0.17907299]]
[37m[1m[2023-06-25 10:45:03,645][129146] Max Reward on eval: 3266.858586944045
[37m[1m[2023-06-25 10:45:03,645][129146] Min Reward on eval: 3266.858586944045
[37m[1m[2023-06-25 10:45:03,645][129146] Mean Reward across all agents: 3266.858586944045
[37m[1m[2023-06-25 10:45:03,645][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:45:08,992][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:45:08,993][129146] Reward + Measures: [[1782.63813378    0.23720001    0.44330001    0.19880001    0.17690001]
[37m[1m [1301.8342404     0.1874215     0.43242857    0.26173005    0.22345804]
[37m[1m [ 379.89613024    0.25641307    0.41258249    0.30716601    0.3841086 ]
[37m[1m ...
[37m[1m [ 500.20372814    0.28509524    0.42590475    0.31115714    0.33291745]
[37m[1m [-354.37448293    0.43829998    0.5176        0.4147        0.31890002]
[37m[1m [ 513.19989742    0.2306        0.48030001    0.27869999    0.41960001]]
[37m[1m[2023-06-25 10:45:08,993][129146] Max Reward on eval: 3126.090257783607
[37m[1m[2023-06-25 10:45:08,993][129146] Min Reward on eval: -585.0932560626185
[37m[1m[2023-06-25 10:45:08,994][129146] Mean Reward across all agents: 1235.140135569645
[37m[1m[2023-06-25 10:45:08,994][129146] Average Trajectory Length: 967.2706666666667
[36m[2023-06-25 10:45:08,998][129146] mean_value=-342.6505888983937, max_value=1646.3417618000242
[37m[1m[2023-06-25 10:45:09,000][129146] New mean coefficients: [[ 3.0921836  -0.23494765  0.19036171  0.17361021 -0.14369437]]
[37m[1m[2023-06-25 10:45:09,001][129146] Moving the mean solution point...
[36m[2023-06-25 10:45:18,606][129146] train() took 9.60 seconds to complete
[36m[2023-06-25 10:45:18,606][129146] FPS: 399868.83
[36m[2023-06-25 10:45:18,609][129146] itr=1065, itrs=2000, Progress: 53.25%
[36m[2023-06-25 10:45:29,993][129146] train() took 11.36 seconds to complete
[36m[2023-06-25 10:45:29,993][129146] FPS: 337979.11
[36m[2023-06-25 10:45:34,864][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:45:34,864][129146] Reward + Measures: [[3434.70467529    0.18309523    0.75794905    0.19368052    0.17677848]]
[37m[1m[2023-06-25 10:45:34,865][129146] Max Reward on eval: 3434.704675291243
[37m[1m[2023-06-25 10:45:34,865][129146] Min Reward on eval: 3434.704675291243
[37m[1m[2023-06-25 10:45:34,865][129146] Mean Reward across all agents: 3434.704675291243
[37m[1m[2023-06-25 10:45:34,865][129146] Average Trajectory Length: 999.8166666666666
[36m[2023-06-25 10:45:40,269][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:45:40,270][129146] Reward + Measures: [[1019.85073346    0.19990002    0.75690001    0.27970001    0.45609999]
[37m[1m [1051.37950262    0.26880002    0.79050004    0.20280002    0.57050008]
[37m[1m [1887.06967284    0.29519999    0.72570002    0.18069999    0.28130004]
[37m[1m ...
[37m[1m [2727.96366573    0.21370001    0.7604        0.2148        0.21560001]
[37m[1m [2370.73529939    0.20390001    0.69550002    0.20810001    0.2079    ]
[37m[1m [2695.63688885    0.16330002    0.74570006    0.21640001    0.1779    ]]
[37m[1m[2023-06-25 10:45:40,270][129146] Max Reward on eval: 3272.7957728012116
[37m[1m[2023-06-25 10:45:40,270][129146] Min Reward on eval: 123.5834101836197
[37m[1m[2023-06-25 10:45:40,271][129146] Mean Reward across all agents: 1927.3967394894764
[37m[1m[2023-06-25 10:45:40,271][129146] Average Trajectory Length: 990.1246666666666
[36m[2023-06-25 10:45:40,276][129146] mean_value=35.05626076893852, max_value=3595.3232374648564
[37m[1m[2023-06-25 10:45:40,279][129146] New mean coefficients: [[ 3.078474    0.00810921  0.6346276   0.08422089 -0.08584899]]
[37m[1m[2023-06-25 10:45:40,280][129146] Moving the mean solution point...
[36m[2023-06-25 10:45:50,021][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 10:45:50,021][129146] FPS: 394274.35
[36m[2023-06-25 10:45:50,023][129146] itr=1066, itrs=2000, Progress: 53.30%
[36m[2023-06-25 10:46:01,430][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 10:46:01,431][129146] FPS: 337331.43
[36m[2023-06-25 10:46:06,048][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:46:06,049][129146] Reward + Measures: [[2289.79167603    0.16864434    0.75831503    0.23395471    0.20046248]]
[37m[1m[2023-06-25 10:46:06,049][129146] Max Reward on eval: 2289.7916760276003
[37m[1m[2023-06-25 10:46:06,049][129146] Min Reward on eval: 2289.7916760276003
[37m[1m[2023-06-25 10:46:06,049][129146] Mean Reward across all agents: 2289.7916760276003
[37m[1m[2023-06-25 10:46:06,050][129146] Average Trajectory Length: 999.6959999999999
[36m[2023-06-25 10:46:11,410][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:46:11,464][129146] Reward + Measures: [[1811.8683502     0.13310002    0.66430002    0.2624        0.245     ]
[37m[1m [ 379.56234785    0.4409        0.76070005    0.52670002    0.70770001]
[37m[1m [1149.61944434    0.19630001    0.66090006    0.32280001    0.33010003]
[37m[1m ...
[37m[1m [2100.46863238    0.16510001    0.71420002    0.30850002    0.229     ]
[37m[1m [1453.2172347     0.1018        0.56680006    0.3154        0.28470001]
[37m[1m [ 720.47168015    0.41190001    0.72180003    0.42519999    0.61090004]]
[37m[1m[2023-06-25 10:46:11,465][129146] Max Reward on eval: 2390.610670606629
[37m[1m[2023-06-25 10:46:11,466][129146] Min Reward on eval: -240.22769612107658
[37m[1m[2023-06-25 10:46:11,467][129146] Mean Reward across all agents: 1131.0618394696958
[37m[1m[2023-06-25 10:46:11,468][129146] Average Trajectory Length: 990.954
[36m[2023-06-25 10:46:11,484][129146] mean_value=-180.92226446877245, max_value=2635.8892463016323
[37m[1m[2023-06-25 10:46:11,494][129146] New mean coefficients: [[ 2.946151   -0.22337489 -0.86957884  0.09548184  0.20508453]]
[37m[1m[2023-06-25 10:46:11,497][129146] Moving the mean solution point...
[36m[2023-06-25 10:46:21,072][129146] train() took 9.57 seconds to complete
[36m[2023-06-25 10:46:21,072][129146] FPS: 401191.67
[36m[2023-06-25 10:46:21,074][129146] itr=1067, itrs=2000, Progress: 53.35%
[36m[2023-06-25 10:46:32,497][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 10:46:32,497][129146] FPS: 336848.11
[36m[2023-06-25 10:46:37,263][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:46:37,263][129146] Reward + Measures: [[2514.49693878    0.15972488    0.73150229    0.2288619     0.1916206 ]]
[37m[1m[2023-06-25 10:46:37,263][129146] Max Reward on eval: 2514.4969387764954
[37m[1m[2023-06-25 10:46:37,263][129146] Min Reward on eval: 2514.4969387764954
[37m[1m[2023-06-25 10:46:37,264][129146] Mean Reward across all agents: 2514.4969387764954
[37m[1m[2023-06-25 10:46:37,264][129146] Average Trajectory Length: 999.8836666666666
[36m[2023-06-25 10:46:42,893][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:46:42,893][129146] Reward + Measures: [[ 533.53481304    0.26989999    0.90620005    0.0816        0.85089999]
[37m[1m [2112.75130954    0.15090001    0.7511        0.2421        0.20990001]
[37m[1m [ 780.94123139    0.3098        0.63959998    0.1462        0.5316    ]
[37m[1m ...
[37m[1m [ 251.37616618    0.26820001    0.83649999    0.33940002    0.75279999]
[37m[1m [1407.00981647    0.30809999    0.72240001    0.21530001    0.34279999]
[37m[1m [ 702.55677672    0.33320001    0.63819999    0.22850001    0.3414    ]]
[37m[1m[2023-06-25 10:46:42,893][129146] Max Reward on eval: 2388.873080216022
[37m[1m[2023-06-25 10:46:42,894][129146] Min Reward on eval: -1216.5770003592247
[37m[1m[2023-06-25 10:46:42,894][129146] Mean Reward across all agents: 859.0163503008691
[37m[1m[2023-06-25 10:46:42,894][129146] Average Trajectory Length: 985.4863333333333
[36m[2023-06-25 10:46:42,899][129146] mean_value=-251.10667921556626, max_value=1766.7040420849867
[37m[1m[2023-06-25 10:46:42,902][129146] New mean coefficients: [[ 2.9179447  -0.24651092 -0.7527919  -0.01740516  0.39147788]]
[37m[1m[2023-06-25 10:46:42,903][129146] Moving the mean solution point...
[36m[2023-06-25 10:46:52,607][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 10:46:52,607][129146] FPS: 395798.77
[36m[2023-06-25 10:46:52,609][129146] itr=1068, itrs=2000, Progress: 53.40%
[36m[2023-06-25 10:47:04,312][129146] train() took 11.68 seconds to complete
[36m[2023-06-25 10:47:04,312][129146] FPS: 328880.25
[36m[2023-06-25 10:47:09,207][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:47:09,207][129146] Reward + Measures: [[2672.91214445    0.16102721    0.71878791    0.21869288    0.18690816]]
[37m[1m[2023-06-25 10:47:09,207][129146] Max Reward on eval: 2672.9121444498505
[37m[1m[2023-06-25 10:47:09,207][129146] Min Reward on eval: 2672.9121444498505
[37m[1m[2023-06-25 10:47:09,208][129146] Mean Reward across all agents: 2672.9121444498505
[37m[1m[2023-06-25 10:47:09,208][129146] Average Trajectory Length: 999.6016666666667
[36m[2023-06-25 10:47:14,701][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:47:14,702][129146] Reward + Measures: [[ 874.82251411    0.15820001    0.69889998    0.34240004    0.53940004]
[37m[1m [   2.77289156    0.67720002    0.6656        0.23309998    0.61879998]
[37m[1m [1291.37364025    0.1701        0.64510006    0.33160001    0.26820001]
[37m[1m ...
[37m[1m [2368.53566844    0.1714        0.74970001    0.25780001    0.21170001]
[37m[1m [ 370.13449119    0.40400001    0.61070001    0.50790006    0.39820001]
[37m[1m [1492.17521795    0.199         0.4914        0.2967        0.21430002]]
[37m[1m[2023-06-25 10:47:14,702][129146] Max Reward on eval: 2664.6919977420707
[37m[1m[2023-06-25 10:47:14,702][129146] Min Reward on eval: -862.4195619969512
[37m[1m[2023-06-25 10:47:14,703][129146] Mean Reward across all agents: 1284.860007685385
[37m[1m[2023-06-25 10:47:14,703][129146] Average Trajectory Length: 988.7153333333333
[36m[2023-06-25 10:47:14,707][129146] mean_value=-188.25282060264868, max_value=2793.274495171765
[37m[1m[2023-06-25 10:47:14,710][129146] New mean coefficients: [[ 3.1614668  -0.21034238 -0.26243746  0.24124548  0.60806334]]
[37m[1m[2023-06-25 10:47:14,711][129146] Moving the mean solution point...
[36m[2023-06-25 10:47:24,610][129146] train() took 9.90 seconds to complete
[36m[2023-06-25 10:47:24,611][129146] FPS: 387956.80
[36m[2023-06-25 10:47:24,613][129146] itr=1069, itrs=2000, Progress: 53.45%
[36m[2023-06-25 10:47:36,160][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 10:47:36,160][129146] FPS: 333234.00
[36m[2023-06-25 10:47:40,980][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:47:40,980][129146] Reward + Measures: [[2796.89188399    0.16337933    0.70346177    0.21430932    0.18327035]]
[37m[1m[2023-06-25 10:47:40,980][129146] Max Reward on eval: 2796.891883992405
[37m[1m[2023-06-25 10:47:40,981][129146] Min Reward on eval: 2796.891883992405
[37m[1m[2023-06-25 10:47:40,981][129146] Mean Reward across all agents: 2796.891883992405
[37m[1m[2023-06-25 10:47:40,981][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:47:46,531][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:47:46,560][129146] Reward + Measures: [[ 534.53296092    0.1814        0.52920002    0.19219999    0.31890002]
[37m[1m [ 382.35532227    0.1619        0.76449996    0.21660002    0.71320003]
[37m[1m [1662.19457032    0.16110002    0.70650005    0.24600001    0.30310002]
[37m[1m ...
[37m[1m [2199.0840861     0.1319        0.77900004    0.23629999    0.25560001]
[37m[1m [1482.88525626    0.20510001    0.6609        0.22780001    0.33400002]
[37m[1m [2466.60567519    0.13810001    0.79829997    0.29459998    0.18789999]]
[37m[1m[2023-06-25 10:47:46,560][129146] Max Reward on eval: 2760.922877961234
[37m[1m[2023-06-25 10:47:46,561][129146] Min Reward on eval: -161.78951732936548
[37m[1m[2023-06-25 10:47:46,561][129146] Mean Reward across all agents: 1554.3878008975446
[37m[1m[2023-06-25 10:47:46,561][129146] Average Trajectory Length: 999.512
[36m[2023-06-25 10:47:46,565][129146] mean_value=-144.66328437491407, max_value=2707.53683521946
[37m[1m[2023-06-25 10:47:46,567][129146] New mean coefficients: [[ 3.0040283  -0.06228188 -0.31993547  0.19605577  0.46693254]]
[37m[1m[2023-06-25 10:47:46,568][129146] Moving the mean solution point...
[36m[2023-06-25 10:47:56,249][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 10:47:56,250][129146] FPS: 396715.71
[36m[2023-06-25 10:47:56,252][129146] itr=1070, itrs=2000, Progress: 53.50%
[37m[1m[2023-06-25 10:48:03,439][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001050
[36m[2023-06-25 10:48:15,240][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 10:48:15,240][129146] FPS: 332330.79
[36m[2023-06-25 10:48:19,944][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:48:19,945][129146] Reward + Measures: [[2577.65198078    0.20171842    0.64152741    0.20240079    0.20441699]]
[37m[1m[2023-06-25 10:48:19,945][129146] Max Reward on eval: 2577.651980776143
[37m[1m[2023-06-25 10:48:19,945][129146] Min Reward on eval: 2577.651980776143
[37m[1m[2023-06-25 10:48:19,945][129146] Mean Reward across all agents: 2577.651980776143
[37m[1m[2023-06-25 10:48:19,946][129146] Average Trajectory Length: 996.3929999999999
[36m[2023-06-25 10:48:25,359][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:48:25,360][129146] Reward + Measures: [[1169.6773317     0.26720002    0.50819999    0.4111        0.23800002]
[37m[1m [1504.20077461    0.14170001    0.80690002    0.182         0.26999998]
[37m[1m [ 780.76988443    0.17990001    0.75279999    0.1998        0.37270001]
[37m[1m ...
[37m[1m [ 779.1327143     0.2638        0.5546        0.3942        0.31430003]
[37m[1m [1948.96422608    0.22600003    0.54500002    0.19219999    0.21610001]
[37m[1m [1901.71214561    0.22239999    0.62169999    0.23740001    0.23149998]]
[37m[1m[2023-06-25 10:48:25,360][129146] Max Reward on eval: 2658.8019035156817
[37m[1m[2023-06-25 10:48:25,360][129146] Min Reward on eval: 277.80670616645364
[37m[1m[2023-06-25 10:48:25,360][129146] Mean Reward across all agents: 1535.1207007830117
[37m[1m[2023-06-25 10:48:25,361][129146] Average Trajectory Length: 994.8763333333333
[36m[2023-06-25 10:48:25,364][129146] mean_value=-306.46777808526645, max_value=1764.7094068399747
[37m[1m[2023-06-25 10:48:25,367][129146] New mean coefficients: [[ 2.935739    0.11766429 -0.02699256  0.3086129  -0.06290406]]
[37m[1m[2023-06-25 10:48:25,368][129146] Moving the mean solution point...
[36m[2023-06-25 10:48:35,156][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 10:48:35,156][129146] FPS: 392373.00
[36m[2023-06-25 10:48:35,158][129146] itr=1071, itrs=2000, Progress: 53.55%
[36m[2023-06-25 10:48:46,583][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 10:48:46,583][129146] FPS: 336849.62
[36m[2023-06-25 10:48:51,411][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:48:51,412][129146] Reward + Measures: [[2790.92565781    0.194986      0.64219183    0.20030972    0.2024045 ]]
[37m[1m[2023-06-25 10:48:51,412][129146] Max Reward on eval: 2790.925657814708
[37m[1m[2023-06-25 10:48:51,412][129146] Min Reward on eval: 2790.925657814708
[37m[1m[2023-06-25 10:48:51,412][129146] Mean Reward across all agents: 2790.925657814708
[37m[1m[2023-06-25 10:48:51,413][129146] Average Trajectory Length: 996.9386666666667
[36m[2023-06-25 10:48:57,092][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:48:57,093][129146] Reward + Measures: [[ 952.19149448    0.11721086    0.64846253    0.26915607    0.26559696]
[37m[1m [2107.10505371    0.21545331    0.59651059    0.17412762    0.21224792]
[37m[1m [2271.66549061    0.25350001    0.5262        0.18639998    0.20250002]
[37m[1m ...
[37m[1m [1125.61014616    0.1243        0.55579996    0.25200003    0.2115    ]
[37m[1m [1936.57554785    0.2256        0.63609999    0.1758        0.245     ]
[37m[1m [1970.95604535    0.2105        0.5693        0.29949999    0.23029999]]
[37m[1m[2023-06-25 10:48:57,093][129146] Max Reward on eval: 2818.057383808261
[37m[1m[2023-06-25 10:48:57,094][129146] Min Reward on eval: 195.86774057775037
[37m[1m[2023-06-25 10:48:57,094][129146] Mean Reward across all agents: 1878.7629361935549
[37m[1m[2023-06-25 10:48:57,094][129146] Average Trajectory Length: 962.5426666666666
[36m[2023-06-25 10:48:57,098][129146] mean_value=-256.13039501614946, max_value=1414.710613227549
[37m[1m[2023-06-25 10:48:57,101][129146] New mean coefficients: [[ 2.9919784  -0.15773745 -0.19330847  0.2758902   0.11515193]]
[37m[1m[2023-06-25 10:48:57,102][129146] Moving the mean solution point...
[36m[2023-06-25 10:49:06,857][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 10:49:06,857][129146] FPS: 393702.29
[36m[2023-06-25 10:49:06,859][129146] itr=1072, itrs=2000, Progress: 53.60%
[36m[2023-06-25 10:49:18,496][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 10:49:18,496][129146] FPS: 330688.86
[36m[2023-06-25 10:49:23,306][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:49:23,306][129146] Reward + Measures: [[2970.33424822    0.18514672    0.64306808    0.19920516    0.19782025]]
[37m[1m[2023-06-25 10:49:23,307][129146] Max Reward on eval: 2970.334248215087
[37m[1m[2023-06-25 10:49:23,307][129146] Min Reward on eval: 2970.334248215087
[37m[1m[2023-06-25 10:49:23,307][129146] Mean Reward across all agents: 2970.334248215087
[37m[1m[2023-06-25 10:49:23,307][129146] Average Trajectory Length: 993.453
[36m[2023-06-25 10:49:28,779][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:49:28,780][129146] Reward + Measures: [[2907.94018188    0.2165        0.62740004    0.19020002    0.21110001]
[37m[1m [1599.04651402    0.35120511    0.51576781    0.18472712    0.2935136 ]
[37m[1m [2513.96983625    0.24129999    0.63910002    0.20469999    0.22839999]
[37m[1m ...
[37m[1m [2733.71058892    0.211         0.69319999    0.20050001    0.2122    ]
[37m[1m [2382.07979072    0.11979999    0.70290005    0.234         0.22220002]
[37m[1m [2093.19282155    0.168         0.83579999    0.227         0.19559999]]
[37m[1m[2023-06-25 10:49:28,780][129146] Max Reward on eval: 3005.0814212201394
[37m[1m[2023-06-25 10:49:28,780][129146] Min Reward on eval: 254.26678280913038
[37m[1m[2023-06-25 10:49:28,781][129146] Mean Reward across all agents: 1992.988188565175
[37m[1m[2023-06-25 10:49:28,781][129146] Average Trajectory Length: 993.516
[36m[2023-06-25 10:49:28,785][129146] mean_value=-234.72496897693046, max_value=991.1487630591264
[37m[1m[2023-06-25 10:49:28,788][129146] New mean coefficients: [[ 2.8829474   0.07300316 -0.03374237  0.2398111  -0.22983001]]
[37m[1m[2023-06-25 10:49:28,789][129146] Moving the mean solution point...
[36m[2023-06-25 10:49:38,591][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 10:49:38,592][129146] FPS: 391808.84
[36m[2023-06-25 10:49:38,594][129146] itr=1073, itrs=2000, Progress: 53.65%
[36m[2023-06-25 10:49:50,171][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 10:49:50,171][129146] FPS: 332505.68
[36m[2023-06-25 10:49:54,983][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:49:54,984][129146] Reward + Measures: [[3167.37549717    0.18037918    0.65089804    0.20158182    0.19306144]]
[37m[1m[2023-06-25 10:49:54,984][129146] Max Reward on eval: 3167.3754971652165
[37m[1m[2023-06-25 10:49:54,984][129146] Min Reward on eval: 3167.3754971652165
[37m[1m[2023-06-25 10:49:54,984][129146] Mean Reward across all agents: 3167.3754971652165
[37m[1m[2023-06-25 10:49:54,985][129146] Average Trajectory Length: 992.4443333333332
[36m[2023-06-25 10:50:00,490][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:50:00,491][129146] Reward + Measures: [[2916.29184244    0.1997        0.68370003    0.2342        0.20109999]
[37m[1m [1307.17548697    0.19204903    0.44274145    0.23815911    0.20311217]
[37m[1m [2133.20398093    0.19296189    0.52139515    0.26727721    0.2052599 ]
[37m[1m ...
[37m[1m [2753.79608542    0.22590001    0.64650005    0.21200001    0.1957    ]
[37m[1m [2881.22107054    0.21899998    0.67119998    0.2146        0.2045    ]
[37m[1m [1646.27189949    0.14629601    0.5163154     0.22639771    0.20215969]]
[37m[1m[2023-06-25 10:50:00,491][129146] Max Reward on eval: 3346.5002558407373
[37m[1m[2023-06-25 10:50:00,491][129146] Min Reward on eval: 474.0926129179541
[37m[1m[2023-06-25 10:50:00,491][129146] Mean Reward across all agents: 2124.9906558005964
[37m[1m[2023-06-25 10:50:00,492][129146] Average Trajectory Length: 968.9303333333334
[36m[2023-06-25 10:50:00,497][129146] mean_value=-21.296552950786257, max_value=2194.271418997005
[37m[1m[2023-06-25 10:50:00,500][129146] New mean coefficients: [[ 2.925202    0.13203645  0.02648775 -0.00317784 -0.3136461 ]]
[37m[1m[2023-06-25 10:50:00,501][129146] Moving the mean solution point...
[36m[2023-06-25 10:50:10,283][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 10:50:10,283][129146] FPS: 392622.09
[36m[2023-06-25 10:50:10,285][129146] itr=1074, itrs=2000, Progress: 53.70%
[36m[2023-06-25 10:50:21,745][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 10:50:21,745][129146] FPS: 335752.62
[36m[2023-06-25 10:50:26,577][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:50:26,578][129146] Reward + Measures: [[3387.38441776    0.1733361     0.6656546     0.19995613    0.18842739]]
[37m[1m[2023-06-25 10:50:26,578][129146] Max Reward on eval: 3387.384417757018
[37m[1m[2023-06-25 10:50:26,578][129146] Min Reward on eval: 3387.384417757018
[37m[1m[2023-06-25 10:50:26,578][129146] Mean Reward across all agents: 3387.384417757018
[37m[1m[2023-06-25 10:50:26,579][129146] Average Trajectory Length: 994.1476666666666
[36m[2023-06-25 10:50:31,981][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:50:31,982][129146] Reward + Measures: [[2651.24199441    0.14560001    0.61480004    0.1973        0.19059999]
[37m[1m [ 514.18272251    0.19950001    0.51569998    0.41610003    0.40330002]
[37m[1m [ 999.07735216    0.3021147     0.5084995     0.2500262     0.27835813]
[37m[1m ...
[37m[1m [1925.60350318    0.21040002    0.63160002    0.34500003    0.2165    ]
[37m[1m [1473.58319809    0.24589999    0.62589997    0.29460001    0.27480003]
[37m[1m [1066.08136687    0.23339999    0.68170005    0.22309999    0.27830002]]
[37m[1m[2023-06-25 10:50:31,982][129146] Max Reward on eval: 3251.592341887858
[37m[1m[2023-06-25 10:50:31,982][129146] Min Reward on eval: -1463.3119124650257
[37m[1m[2023-06-25 10:50:31,982][129146] Mean Reward across all agents: 1691.2573528775793
[37m[1m[2023-06-25 10:50:31,983][129146] Average Trajectory Length: 989.4476666666666
[36m[2023-06-25 10:50:31,986][129146] mean_value=-330.8380270872266, max_value=1293.4824426319326
[37m[1m[2023-06-25 10:50:31,989][129146] New mean coefficients: [[ 2.9552884   0.3205738   0.75041014 -0.3017186  -0.4834832 ]]
[37m[1m[2023-06-25 10:50:31,990][129146] Moving the mean solution point...
[36m[2023-06-25 10:50:41,654][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 10:50:41,654][129146] FPS: 397434.07
[36m[2023-06-25 10:50:41,656][129146] itr=1075, itrs=2000, Progress: 53.75%
[36m[2023-06-25 10:50:53,062][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 10:50:53,062][129146] FPS: 337469.44
[36m[2023-06-25 10:50:57,928][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:50:57,928][129146] Reward + Measures: [[3568.26648847    0.16800982    0.68044752    0.19607757    0.18496054]]
[37m[1m[2023-06-25 10:50:57,929][129146] Max Reward on eval: 3568.266488468057
[37m[1m[2023-06-25 10:50:57,929][129146] Min Reward on eval: 3568.266488468057
[37m[1m[2023-06-25 10:50:57,929][129146] Mean Reward across all agents: 3568.266488468057
[37m[1m[2023-06-25 10:50:57,930][129146] Average Trajectory Length: 994.3796666666666
[36m[2023-06-25 10:51:03,414][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:51:03,415][129146] Reward + Measures: [[1308.23387167    0.28080001    0.48680001    0.16600001    0.26680002]
[37m[1m [2829.02235641    0.20030001    0.66219997    0.30050001    0.222     ]
[37m[1m [2440.80609733    0.24569999    0.58120006    0.2122        0.2397    ]
[37m[1m ...
[37m[1m [1964.13027152    0.29190001    0.52410001    0.23210001    0.26789999]
[37m[1m [1861.37612854    0.30590001    0.59729999    0.27200001    0.233     ]
[37m[1m [ 378.85653873    0.2947        0.61420006    0.08990001    0.31400001]]
[37m[1m[2023-06-25 10:51:03,415][129146] Max Reward on eval: 3399.5436663218775
[37m[1m[2023-06-25 10:51:03,415][129146] Min Reward on eval: -157.33135134667538
[37m[1m[2023-06-25 10:51:03,416][129146] Mean Reward across all agents: 1935.4674993326196
[37m[1m[2023-06-25 10:51:03,416][129146] Average Trajectory Length: 986.0213333333332
[36m[2023-06-25 10:51:03,419][129146] mean_value=-511.66991779101664, max_value=2485.1258125798777
[37m[1m[2023-06-25 10:51:03,422][129146] New mean coefficients: [[ 2.9732597  -0.13210577  0.90336263 -0.25386068 -0.34736532]]
[37m[1m[2023-06-25 10:51:03,423][129146] Moving the mean solution point...
[36m[2023-06-25 10:51:13,114][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 10:51:13,114][129146] FPS: 396339.72
[36m[2023-06-25 10:51:13,116][129146] itr=1076, itrs=2000, Progress: 53.80%
[36m[2023-06-25 10:51:24,602][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 10:51:24,603][129146] FPS: 334990.05
[36m[2023-06-25 10:51:29,361][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:51:29,361][129146] Reward + Measures: [[3766.574874      0.16513309    0.70482332    0.19186598    0.18140657]]
[37m[1m[2023-06-25 10:51:29,361][129146] Max Reward on eval: 3766.574873998035
[37m[1m[2023-06-25 10:51:29,361][129146] Min Reward on eval: 3766.574873998035
[37m[1m[2023-06-25 10:51:29,362][129146] Mean Reward across all agents: 3766.574873998035
[37m[1m[2023-06-25 10:51:29,362][129146] Average Trajectory Length: 997.9309999999999
[36m[2023-06-25 10:51:35,014][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:51:35,015][129146] Reward + Measures: [[ 456.89048259    0.26590002    0.625         0.49729997    0.48909998]
[37m[1m [3226.56798683    0.2467        0.69360006    0.20110002    0.2089    ]
[37m[1m [2787.24370612    0.1444        0.71779996    0.31380001    0.22269998]
[37m[1m ...
[37m[1m [2032.32755155    0.14380001    0.69090003    0.3795        0.2458    ]
[37m[1m [2527.2629849     0.19160001    0.66730005    0.1833        0.2016    ]
[37m[1m [2872.21151009    0.2744        0.61559999    0.1885        0.20870002]]
[37m[1m[2023-06-25 10:51:35,015][129146] Max Reward on eval: 3730.0968256659808
[37m[1m[2023-06-25 10:51:35,016][129146] Min Reward on eval: 263.99546836861407
[37m[1m[2023-06-25 10:51:35,016][129146] Mean Reward across all agents: 2179.0741964034005
[37m[1m[2023-06-25 10:51:35,016][129146] Average Trajectory Length: 997.4733333333332
[36m[2023-06-25 10:51:35,022][129146] mean_value=128.74974573235778, max_value=3202.6525157164783
[37m[1m[2023-06-25 10:51:35,025][129146] New mean coefficients: [[ 3.0541155   0.11851907  1.3736812  -0.1507314  -0.5684427 ]]
[37m[1m[2023-06-25 10:51:35,026][129146] Moving the mean solution point...
[36m[2023-06-25 10:51:44,873][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 10:51:44,873][129146] FPS: 390047.70
[36m[2023-06-25 10:51:44,876][129146] itr=1077, itrs=2000, Progress: 53.85%
[36m[2023-06-25 10:51:56,399][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 10:51:56,399][129146] FPS: 334006.72
[36m[2023-06-25 10:52:01,237][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:52:01,237][129146] Reward + Measures: [[3874.57543731    0.17661633    0.72246242    0.20059367    0.190163  ]]
[37m[1m[2023-06-25 10:52:01,237][129146] Max Reward on eval: 3874.5754373052705
[37m[1m[2023-06-25 10:52:01,238][129146] Min Reward on eval: 3874.5754373052705
[37m[1m[2023-06-25 10:52:01,238][129146] Mean Reward across all agents: 3874.5754373052705
[37m[1m[2023-06-25 10:52:01,238][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:52:06,742][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:52:06,742][129146] Reward + Measures: [[1845.80249285    0.23819999    0.68550009    0.19329999    0.31560001]
[37m[1m [2328.99088133    0.28790003    0.68240005    0.25799999    0.31180003]
[37m[1m [3053.10378908    0.25319999    0.63980001    0.23030002    0.2158    ]
[37m[1m ...
[37m[1m [1434.10772828    0.3432        0.5183        0.2911        0.27239999]
[37m[1m [2778.84854166    0.27789998    0.5643        0.23940001    0.2332    ]
[37m[1m [2506.20382565    0.15769999    0.80559999    0.20780002    0.26089999]]
[37m[1m[2023-06-25 10:52:06,742][129146] Max Reward on eval: 3783.4626544939356
[37m[1m[2023-06-25 10:52:06,743][129146] Min Reward on eval: 355.1527255888599
[37m[1m[2023-06-25 10:52:06,743][129146] Mean Reward across all agents: 2180.705142246914
[37m[1m[2023-06-25 10:52:06,743][129146] Average Trajectory Length: 996.4393333333333
[36m[2023-06-25 10:52:06,747][129146] mean_value=-167.89561015901856, max_value=2507.8349093204497
[37m[1m[2023-06-25 10:52:06,750][129146] New mean coefficients: [[ 3.1647322   0.16946918  0.95388013 -0.22680274 -0.22250408]]
[37m[1m[2023-06-25 10:52:06,751][129146] Moving the mean solution point...
[36m[2023-06-25 10:52:16,446][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 10:52:16,446][129146] FPS: 396164.46
[36m[2023-06-25 10:52:16,449][129146] itr=1078, itrs=2000, Progress: 53.90%
[36m[2023-06-25 10:52:28,079][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 10:52:28,079][129146] FPS: 330944.28
[36m[2023-06-25 10:52:32,776][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:52:32,776][129146] Reward + Measures: [[3928.22050936    0.17243087    0.74265438    0.18374211    0.17727207]]
[37m[1m[2023-06-25 10:52:32,776][129146] Max Reward on eval: 3928.2205093599823
[37m[1m[2023-06-25 10:52:32,777][129146] Min Reward on eval: 3928.2205093599823
[37m[1m[2023-06-25 10:52:32,777][129146] Mean Reward across all agents: 3928.2205093599823
[37m[1m[2023-06-25 10:52:32,777][129146] Average Trajectory Length: 999.8276666666667
[36m[2023-06-25 10:52:38,193][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:52:38,193][129146] Reward + Measures: [[3049.31142336    0.1761        0.6893        0.30669999    0.1947    ]
[37m[1m [1631.25353569    0.2016        0.60250002    0.29840001    0.29550001]
[37m[1m [1345.48597257    0.11380001    0.66720003    0.29060003    0.33020002]
[37m[1m ...
[37m[1m [2768.51027373    0.17730002    0.77419996    0.22250001    0.19000001]
[37m[1m [3356.229933      0.1692        0.76860005    0.20660003    0.1894    ]
[37m[1m [2706.04615802    0.17800002    0.67049998    0.31030002    0.22400001]]
[37m[1m[2023-06-25 10:52:38,194][129146] Max Reward on eval: 3686.3856377455404
[37m[1m[2023-06-25 10:52:38,194][129146] Min Reward on eval: 827.9213363965624
[37m[1m[2023-06-25 10:52:38,194][129146] Mean Reward across all agents: 2544.1311585956228
[37m[1m[2023-06-25 10:52:38,194][129146] Average Trajectory Length: 998.331
[36m[2023-06-25 10:52:38,199][129146] mean_value=-141.80014073861528, max_value=2322.7849437962404
[37m[1m[2023-06-25 10:52:38,202][129146] New mean coefficients: [[ 3.2556913   0.2589997   0.6863028  -0.12791264  0.26562715]]
[37m[1m[2023-06-25 10:52:38,203][129146] Moving the mean solution point...
[36m[2023-06-25 10:52:47,851][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 10:52:47,852][129146] FPS: 398044.44
[36m[2023-06-25 10:52:47,854][129146] itr=1079, itrs=2000, Progress: 53.95%
[36m[2023-06-25 10:52:59,255][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 10:52:59,255][129146] FPS: 337503.10
[36m[2023-06-25 10:53:04,081][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:53:04,081][129146] Reward + Measures: [[3857.58873045    0.17307878    0.71903163    0.18279204    0.17854634]]
[37m[1m[2023-06-25 10:53:04,081][129146] Max Reward on eval: 3857.5887304451926
[37m[1m[2023-06-25 10:53:04,082][129146] Min Reward on eval: 3857.5887304451926
[37m[1m[2023-06-25 10:53:04,082][129146] Mean Reward across all agents: 3857.5887304451926
[37m[1m[2023-06-25 10:53:04,082][129146] Average Trajectory Length: 999.8303333333333
[36m[2023-06-25 10:53:09,519][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:53:09,520][129146] Reward + Measures: [[2689.5638059     0.19340001    0.64710003    0.19400001    0.1848    ]
[37m[1m [2296.1345837     0.1847        0.67970002    0.18049999    0.19090001]
[37m[1m [2692.90147582    0.19599999    0.6681        0.19670001    0.19520001]
[37m[1m ...
[37m[1m [2652.55805001    0.20410001    0.63029999    0.21599999    0.19719999]
[37m[1m [3290.11601471    0.175         0.62229997    0.19700001    0.1859    ]
[37m[1m [2323.74482265    0.27700001    0.62660003    0.21519999    0.25530002]]
[37m[1m[2023-06-25 10:53:09,520][129146] Max Reward on eval: 3688.7649409849196
[37m[1m[2023-06-25 10:53:09,520][129146] Min Reward on eval: 993.820148594759
[37m[1m[2023-06-25 10:53:09,521][129146] Mean Reward across all agents: 2587.5709468978675
[37m[1m[2023-06-25 10:53:09,521][129146] Average Trajectory Length: 995.3766666666667
[36m[2023-06-25 10:53:09,523][129146] mean_value=-517.2876037280257, max_value=551.0375804921514
[37m[1m[2023-06-25 10:53:09,526][129146] New mean coefficients: [[ 3.1392426   0.3523829   0.8090616  -0.08137833 -0.07047188]]
[37m[1m[2023-06-25 10:53:09,527][129146] Moving the mean solution point...
[36m[2023-06-25 10:53:19,221][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 10:53:19,221][129146] FPS: 396205.10
[36m[2023-06-25 10:53:19,223][129146] itr=1080, itrs=2000, Progress: 54.00%
[37m[1m[2023-06-25 10:53:26,835][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001060
[36m[2023-06-25 10:53:38,659][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 10:53:38,659][129146] FPS: 331027.58
[36m[2023-06-25 10:53:43,551][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:53:43,552][129146] Reward + Measures: [[4068.46747839    0.17474924    0.73707712    0.18410611    0.17986985]]
[37m[1m[2023-06-25 10:53:43,552][129146] Max Reward on eval: 4068.467478387571
[37m[1m[2023-06-25 10:53:43,552][129146] Min Reward on eval: 4068.467478387571
[37m[1m[2023-06-25 10:53:43,552][129146] Mean Reward across all agents: 4068.467478387571
[37m[1m[2023-06-25 10:53:43,552][129146] Average Trajectory Length: 999.3376666666667
[36m[2023-06-25 10:53:49,123][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:53:49,124][129146] Reward + Measures: [[1713.21935668    0.33310002    0.52320004    0.2359        0.21870001]
[37m[1m [3488.49471336    0.1998        0.7112        0.19700001    0.19010001]
[37m[1m [2601.06291776    0.1732        0.77600002    0.2264        0.24080001]
[37m[1m ...
[37m[1m [2163.4006062     0.21668668    0.5644471     0.22668262    0.21327667]
[37m[1m [2600.03422835    0.1904        0.82359999    0.24890001    0.2332    ]
[37m[1m [1994.94839198    0.21548751    0.5925709     0.2318625     0.23709166]]
[37m[1m[2023-06-25 10:53:49,124][129146] Max Reward on eval: 3827.2401902875163
[37m[1m[2023-06-25 10:53:49,124][129146] Min Reward on eval: 43.356516183004715
[37m[1m[2023-06-25 10:53:49,124][129146] Mean Reward across all agents: 2071.458915423548
[37m[1m[2023-06-25 10:53:49,125][129146] Average Trajectory Length: 986.068
[36m[2023-06-25 10:53:49,129][129146] mean_value=-414.14147969499965, max_value=1091.9787506849993
[37m[1m[2023-06-25 10:53:49,131][129146] New mean coefficients: [[ 3.0287054   0.2518071   0.14057863 -0.23880175  0.01059786]]
[37m[1m[2023-06-25 10:53:49,132][129146] Moving the mean solution point...
[36m[2023-06-25 10:53:59,062][129146] train() took 9.93 seconds to complete
[36m[2023-06-25 10:53:59,062][129146] FPS: 386786.02
[36m[2023-06-25 10:53:59,064][129146] itr=1081, itrs=2000, Progress: 54.05%
[36m[2023-06-25 10:54:10,618][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 10:54:10,618][129146] FPS: 333040.74
[36m[2023-06-25 10:54:15,499][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:54:15,500][129146] Reward + Measures: [[3215.81779736    0.17741811    0.55825734    0.20789078    0.18133056]]
[37m[1m[2023-06-25 10:54:15,500][129146] Max Reward on eval: 3215.8177973630463
[37m[1m[2023-06-25 10:54:15,500][129146] Min Reward on eval: 3215.8177973630463
[37m[1m[2023-06-25 10:54:15,500][129146] Mean Reward across all agents: 3215.8177973630463
[37m[1m[2023-06-25 10:54:15,501][129146] Average Trajectory Length: 999.5943333333333
[36m[2023-06-25 10:54:21,078][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:54:21,079][129146] Reward + Measures: [[2170.17581159    0.20449999    0.62249994    0.2314        0.22360002]
[37m[1m [1719.32131845    0.16779999    0.61790001    0.26680002    0.22519998]
[37m[1m [2188.29903919    0.14219999    0.45580003    0.19990002    0.19959998]
[37m[1m ...
[37m[1m [2832.79794003    0.1455        0.69020003    0.228         0.18769999]
[37m[1m [2204.93373372    0.2079        0.61409998    0.27290002    0.24860001]
[37m[1m [2365.0388224     0.1513        0.6049        0.2595        0.1823    ]]
[37m[1m[2023-06-25 10:54:21,079][129146] Max Reward on eval: 3313.0742559282107
[37m[1m[2023-06-25 10:54:21,079][129146] Min Reward on eval: -164.98674350242362
[37m[1m[2023-06-25 10:54:21,080][129146] Mean Reward across all agents: 1888.152935195264
[37m[1m[2023-06-25 10:54:21,080][129146] Average Trajectory Length: 925.6993333333334
[36m[2023-06-25 10:54:21,083][129146] mean_value=-425.0844268462247, max_value=805.6669652342212
[37m[1m[2023-06-25 10:54:21,085][129146] New mean coefficients: [[ 2.9322028   0.17018996 -0.1984579  -0.22356635 -0.26139653]]
[37m[1m[2023-06-25 10:54:21,086][129146] Moving the mean solution point...
[36m[2023-06-25 10:54:30,813][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 10:54:30,818][129146] FPS: 394849.99
[36m[2023-06-25 10:54:30,821][129146] itr=1082, itrs=2000, Progress: 54.10%
[36m[2023-06-25 10:54:42,296][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 10:54:42,296][129146] FPS: 335294.28
[36m[2023-06-25 10:54:46,926][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:54:46,926][129146] Reward + Measures: [[1373.29163489    0.15280662    0.5698182     0.36700192    0.2880021 ]]
[37m[1m[2023-06-25 10:54:46,926][129146] Max Reward on eval: 1373.2916348851827
[37m[1m[2023-06-25 10:54:46,927][129146] Min Reward on eval: 1373.2916348851827
[37m[1m[2023-06-25 10:54:46,927][129146] Mean Reward across all agents: 1373.2916348851827
[37m[1m[2023-06-25 10:54:46,927][129146] Average Trajectory Length: 997.6136666666666
[36m[2023-06-25 10:54:52,422][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:54:52,423][129146] Reward + Measures: [[1248.51899949    0.24150001    0.50470001    0.36210001    0.28079998]
[37m[1m [ 569.0849989     0.27426466    0.45262033    0.31828347    0.31363308]
[37m[1m [1467.10229348    0.1269        0.63409996    0.38070002    0.25830001]
[37m[1m ...
[37m[1m [ 628.44091614    0.24447596    0.44496223    0.30758238    0.28629699]
[37m[1m [ 873.37532277    0.2192        0.52650005    0.33669999    0.3732    ]
[37m[1m [1115.50799043    0.15180002    0.49220005    0.29990003    0.29350001]]
[37m[1m[2023-06-25 10:54:52,423][129146] Max Reward on eval: 1656.4827435649581
[37m[1m[2023-06-25 10:54:52,423][129146] Min Reward on eval: 129.50163058645268
[37m[1m[2023-06-25 10:54:52,424][129146] Mean Reward across all agents: 1032.7229502640785
[37m[1m[2023-06-25 10:54:52,424][129146] Average Trajectory Length: 988.0843333333333
[36m[2023-06-25 10:54:52,427][129146] mean_value=-404.07944897181204, max_value=1382.7234580005008
[37m[1m[2023-06-25 10:54:52,429][129146] New mean coefficients: [[ 2.8459077  -0.08892615  0.10270715 -0.11298719 -0.34835935]]
[37m[1m[2023-06-25 10:54:52,430][129146] Moving the mean solution point...
[36m[2023-06-25 10:55:02,063][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 10:55:02,064][129146] FPS: 398697.56
[36m[2023-06-25 10:55:02,066][129146] itr=1083, itrs=2000, Progress: 54.15%
[36m[2023-06-25 10:55:13,506][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 10:55:13,507][129146] FPS: 336481.90
[36m[2023-06-25 10:55:18,327][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:55:18,327][129146] Reward + Measures: [[1606.10639756    0.13648462    0.59738815    0.37188354    0.2684294 ]]
[37m[1m[2023-06-25 10:55:18,327][129146] Max Reward on eval: 1606.10639756394
[37m[1m[2023-06-25 10:55:18,328][129146] Min Reward on eval: 1606.10639756394
[37m[1m[2023-06-25 10:55:18,328][129146] Mean Reward across all agents: 1606.10639756394
[37m[1m[2023-06-25 10:55:18,328][129146] Average Trajectory Length: 997.0063333333333
[36m[2023-06-25 10:55:23,782][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:55:23,783][129146] Reward + Measures: [[ 633.25830328    0.3558        0.51120001    0.39700001    0.39430001]
[37m[1m [1495.38663821    0.16980001    0.6049        0.35500002    0.27379999]
[37m[1m [ 533.14549203    0.1142        0.57540005    0.31679997    0.3003    ]
[37m[1m ...
[37m[1m [ 685.75397387    0.39300001    0.61870003    0.5158        0.49870005]
[37m[1m [ 490.64334641    0.25409999    0.45760003    0.29910001    0.345     ]
[37m[1m [ 731.11514731    0.25518629    0.55760586    0.26668629    0.37111175]]
[37m[1m[2023-06-25 10:55:23,783][129146] Max Reward on eval: 1809.2192599793198
[37m[1m[2023-06-25 10:55:23,783][129146] Min Reward on eval: 184.4417252617597
[37m[1m[2023-06-25 10:55:23,783][129146] Mean Reward across all agents: 1021.4207124134867
[37m[1m[2023-06-25 10:55:23,784][129146] Average Trajectory Length: 986.6313333333333
[36m[2023-06-25 10:55:23,788][129146] mean_value=-200.3481199272537, max_value=2210.024412195012
[37m[1m[2023-06-25 10:55:23,791][129146] New mean coefficients: [[ 2.9284902   0.10516684  0.41099012 -0.05476972 -0.04810441]]
[37m[1m[2023-06-25 10:55:23,792][129146] Moving the mean solution point...
[36m[2023-06-25 10:55:33,580][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 10:55:33,581][129146] FPS: 392374.34
[36m[2023-06-25 10:55:33,583][129146] itr=1084, itrs=2000, Progress: 54.20%
[36m[2023-06-25 10:55:44,999][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 10:55:44,999][129146] FPS: 337060.45
[36m[2023-06-25 10:55:49,805][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:55:49,805][129146] Reward + Measures: [[1766.78962073    0.1251473     0.60387319    0.3688224     0.24970593]]
[37m[1m[2023-06-25 10:55:49,806][129146] Max Reward on eval: 1766.7896207324143
[37m[1m[2023-06-25 10:55:49,806][129146] Min Reward on eval: 1766.7896207324143
[37m[1m[2023-06-25 10:55:49,806][129146] Mean Reward across all agents: 1766.7896207324143
[37m[1m[2023-06-25 10:55:49,806][129146] Average Trajectory Length: 991.8273333333333
[36m[2023-06-25 10:55:55,256][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:55:55,256][129146] Reward + Measures: [[1572.6032421     0.18980001    0.57770002    0.39210001    0.2723    ]
[37m[1m [1448.35116233    0.15300001    0.46740004    0.37910002    0.21490002]
[37m[1m [1747.34450817    0.1427        0.48909998    0.3603        0.18980001]
[37m[1m ...
[37m[1m [1948.94029205    0.16180001    0.53490001    0.39540002    0.21350001]
[37m[1m [1805.16979327    0.13570002    0.6408        0.37149999    0.27849999]
[37m[1m [1419.15893335    0.12731129    0.55712354    0.31962258    0.22629729]]
[37m[1m[2023-06-25 10:55:55,257][129146] Max Reward on eval: 2089.9247197829186
[37m[1m[2023-06-25 10:55:55,257][129146] Min Reward on eval: 184.28800004013465
[37m[1m[2023-06-25 10:55:55,257][129146] Mean Reward across all agents: 1318.4268624804174
[37m[1m[2023-06-25 10:55:55,257][129146] Average Trajectory Length: 984.933
[36m[2023-06-25 10:55:55,263][129146] mean_value=22.385170025223537, max_value=2321.223742755735
[37m[1m[2023-06-25 10:55:55,265][129146] New mean coefficients: [[ 2.8215902  -0.09133369  0.03513068 -0.11209085  0.14812008]]
[37m[1m[2023-06-25 10:55:55,266][129146] Moving the mean solution point...
[36m[2023-06-25 10:56:04,953][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 10:56:04,953][129146] FPS: 396504.08
[36m[2023-06-25 10:56:04,955][129146] itr=1085, itrs=2000, Progress: 54.25%
[36m[2023-06-25 10:56:16,468][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 10:56:16,468][129146] FPS: 334370.76
[36m[2023-06-25 10:56:21,231][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:56:21,232][129146] Reward + Measures: [[2054.59133906    0.12102832    0.61535454    0.36643025    0.22923575]]
[37m[1m[2023-06-25 10:56:21,232][129146] Max Reward on eval: 2054.5913390607197
[37m[1m[2023-06-25 10:56:21,232][129146] Min Reward on eval: 2054.5913390607197
[37m[1m[2023-06-25 10:56:21,232][129146] Mean Reward across all agents: 2054.5913390607197
[37m[1m[2023-06-25 10:56:21,232][129146] Average Trajectory Length: 991.8399999999999
[36m[2023-06-25 10:56:26,814][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:56:26,815][129146] Reward + Measures: [[ 744.78618197    0.33140001    0.47450003    0.33539999    0.47510001]
[37m[1m [ 726.23222916    0.39639997    0.51840001    0.382         0.5169    ]
[37m[1m [2193.33368457    0.14170001    0.62690002    0.26530001    0.25709999]
[37m[1m ...
[37m[1m [1950.12850758    0.13024929    0.50238228    0.35475537    0.20553184]
[37m[1m [2313.05280618    0.15260001    0.55179995    0.32640001    0.2184    ]
[37m[1m [1058.63221873    0.1365        0.5072        0.33010003    0.25829998]]
[37m[1m[2023-06-25 10:56:26,815][129146] Max Reward on eval: 2590.838016926334
[37m[1m[2023-06-25 10:56:26,815][129146] Min Reward on eval: 557.5711698847066
[37m[1m[2023-06-25 10:56:26,815][129146] Mean Reward across all agents: 1648.1067661575794
[37m[1m[2023-06-25 10:56:26,816][129146] Average Trajectory Length: 986.6023333333333
[36m[2023-06-25 10:56:26,819][129146] mean_value=-148.9234906782046, max_value=1677.8715988739639
[37m[1m[2023-06-25 10:56:26,822][129146] New mean coefficients: [[ 2.7439961  -0.10126567 -0.13321133 -0.11402283  0.3879394 ]]
[37m[1m[2023-06-25 10:56:26,823][129146] Moving the mean solution point...
[36m[2023-06-25 10:56:36,393][129146] train() took 9.57 seconds to complete
[36m[2023-06-25 10:56:36,393][129146] FPS: 401318.81
[36m[2023-06-25 10:56:36,395][129146] itr=1086, itrs=2000, Progress: 54.30%
[36m[2023-06-25 10:56:47,837][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 10:56:47,837][129146] FPS: 336310.94
[36m[2023-06-25 10:56:52,696][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:56:52,696][129146] Reward + Measures: [[2581.61873838    0.209057      0.62990737    0.30201599    0.22972766]]
[37m[1m[2023-06-25 10:56:52,696][129146] Max Reward on eval: 2581.618738383629
[37m[1m[2023-06-25 10:56:52,697][129146] Min Reward on eval: 2581.618738383629
[37m[1m[2023-06-25 10:56:52,697][129146] Mean Reward across all agents: 2581.618738383629
[37m[1m[2023-06-25 10:56:52,697][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:56:58,173][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:56:58,174][129146] Reward + Measures: [[ 827.51183309    0.2922        0.46440002    0.44100004    0.40550002]
[37m[1m [2877.47787979    0.21510001    0.62199998    0.24789999    0.2079    ]
[37m[1m [1794.80725663    0.25420001    0.55919999    0.37789997    0.29520002]
[37m[1m ...
[37m[1m [1402.07753164    0.2606        0.51730007    0.44109997    0.35520002]
[37m[1m [1633.32042139    0.27059999    0.53219998    0.3863        0.32609996]
[37m[1m [1848.39235685    0.25209999    0.55610007    0.36530003    0.29050002]]
[37m[1m[2023-06-25 10:56:58,174][129146] Max Reward on eval: 3017.9310659596
[37m[1m[2023-06-25 10:56:58,174][129146] Min Reward on eval: 827.5118330854457
[37m[1m[2023-06-25 10:56:58,175][129146] Mean Reward across all agents: 1851.9645375378457
[37m[1m[2023-06-25 10:56:58,175][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:56:58,179][129146] mean_value=207.76976127105664, max_value=1965.420295488549
[37m[1m[2023-06-25 10:56:58,182][129146] New mean coefficients: [[ 3.0935163   0.03713413 -0.08985265 -0.04118697  1.0743519 ]]
[37m[1m[2023-06-25 10:56:58,183][129146] Moving the mean solution point...
[36m[2023-06-25 10:57:08,064][129146] train() took 9.88 seconds to complete
[36m[2023-06-25 10:57:08,064][129146] FPS: 388663.03
[36m[2023-06-25 10:57:08,067][129146] itr=1087, itrs=2000, Progress: 54.35%
[36m[2023-06-25 10:57:19,601][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 10:57:19,601][129146] FPS: 333595.36
[36m[2023-06-25 10:57:24,330][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:57:24,331][129146] Reward + Measures: [[2007.25582998    0.28958434    0.52341431    0.28664434    0.22188467]]
[37m[1m[2023-06-25 10:57:24,331][129146] Max Reward on eval: 2007.2558299795548
[37m[1m[2023-06-25 10:57:24,331][129146] Min Reward on eval: 2007.2558299795548
[37m[1m[2023-06-25 10:57:24,331][129146] Mean Reward across all agents: 2007.2558299795548
[37m[1m[2023-06-25 10:57:24,332][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:57:29,812][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:57:29,812][129146] Reward + Measures: [[ 766.99433017    0.22910002    0.36939999    0.3705        0.26820001]
[37m[1m [2054.41011653    0.28330001    0.52749997    0.3021        0.24730001]
[37m[1m [1609.18896386    0.25470001    0.43179998    0.34299999    0.2448    ]
[37m[1m ...
[37m[1m [ 944.2221514     0.26019999    0.40289998    0.34220001    0.25869998]
[37m[1m [1508.61352189    0.22449999    0.45140001    0.3849        0.23310001]
[37m[1m [1724.9132736     0.2244        0.47880003    0.37670001    0.22760001]]
[37m[1m[2023-06-25 10:57:29,812][129146] Max Reward on eval: 2290.088029248733
[37m[1m[2023-06-25 10:57:29,813][129146] Min Reward on eval: 641.4216352164163
[37m[1m[2023-06-25 10:57:29,813][129146] Mean Reward across all agents: 1676.274816365209
[37m[1m[2023-06-25 10:57:29,813][129146] Average Trajectory Length: 999.4533333333333
[36m[2023-06-25 10:57:29,817][129146] mean_value=50.85961855206886, max_value=832.3196631448159
[37m[1m[2023-06-25 10:57:29,820][129146] New mean coefficients: [[ 3.057975   -0.15077852 -0.5457458  -0.13047408  1.056307  ]]
[37m[1m[2023-06-25 10:57:29,821][129146] Moving the mean solution point...
[36m[2023-06-25 10:57:39,556][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 10:57:39,556][129146] FPS: 394522.71
[36m[2023-06-25 10:57:39,558][129146] itr=1088, itrs=2000, Progress: 54.40%
[36m[2023-06-25 10:57:51,094][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 10:57:51,094][129146] FPS: 333567.76
[36m[2023-06-25 10:57:55,769][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:57:55,769][129146] Reward + Measures: [[2239.59900249    0.27645779    0.55440903    0.26325241    0.2047832 ]]
[37m[1m[2023-06-25 10:57:55,769][129146] Max Reward on eval: 2239.5990024931543
[37m[1m[2023-06-25 10:57:55,769][129146] Min Reward on eval: 2239.5990024931543
[37m[1m[2023-06-25 10:57:55,770][129146] Mean Reward across all agents: 2239.5990024931543
[37m[1m[2023-06-25 10:57:55,770][129146] Average Trajectory Length: 999.8593333333333
[36m[2023-06-25 10:58:01,192][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:58:01,192][129146] Reward + Measures: [[ 982.64727495    0.31920001    0.47440001    0.3416        0.3734    ]
[37m[1m [2015.78356874    0.244         0.542         0.29800001    0.21340001]
[37m[1m [1597.06102834    0.32079998    0.52079999    0.2911        0.2965    ]
[37m[1m ...
[37m[1m [ 444.18330565    0.1084        0.67629999    0.51499999    0.7518    ]
[37m[1m [ 549.0943338     0.28659999    0.50349998    0.39569998    0.48030001]
[37m[1m [ 629.73453096    0.32969999    0.40720001    0.40820003    0.3804    ]]
[37m[1m[2023-06-25 10:58:01,193][129146] Max Reward on eval: 2316.026776404632
[37m[1m[2023-06-25 10:58:01,193][129146] Min Reward on eval: 277.91670875323473
[37m[1m[2023-06-25 10:58:01,193][129146] Mean Reward across all agents: 1033.1724975504765
[37m[1m[2023-06-25 10:58:01,193][129146] Average Trajectory Length: 999.5616666666666
[36m[2023-06-25 10:58:01,197][129146] mean_value=-190.15806673028513, max_value=1003.7540731218153
[37m[1m[2023-06-25 10:58:01,200][129146] New mean coefficients: [[ 3.26105    -0.55975556 -0.848963   -0.02823831  1.1926665 ]]
[37m[1m[2023-06-25 10:58:01,201][129146] Moving the mean solution point...
[36m[2023-06-25 10:58:10,783][129146] train() took 9.58 seconds to complete
[36m[2023-06-25 10:58:10,783][129146] FPS: 400806.31
[36m[2023-06-25 10:58:10,786][129146] itr=1089, itrs=2000, Progress: 54.45%
[36m[2023-06-25 10:58:22,371][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 10:58:22,372][129146] FPS: 332107.57
[36m[2023-06-25 10:58:27,200][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:58:27,200][129146] Reward + Measures: [[2577.13022348    0.25334799    0.58787835    0.237341      0.18998133]]
[37m[1m[2023-06-25 10:58:27,200][129146] Max Reward on eval: 2577.1302234785217
[37m[1m[2023-06-25 10:58:27,201][129146] Min Reward on eval: 2577.1302234785217
[37m[1m[2023-06-25 10:58:27,201][129146] Mean Reward across all agents: 2577.1302234785217
[37m[1m[2023-06-25 10:58:27,201][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:58:32,684][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:58:32,685][129146] Reward + Measures: [[1770.85078822    0.27430001    0.47010002    0.3035        0.22449999]
[37m[1m [1199.39588975    0.31039998    0.4068        0.30340001    0.27509999]
[37m[1m [1704.54664615    0.31850001    0.46989998    0.30270001    0.2431    ]
[37m[1m ...
[37m[1m [1375.22030967    0.31920001    0.4224        0.31020004    0.26610002]
[37m[1m [1821.77744558    0.28720003    0.4955        0.29520002    0.24330001]
[37m[1m [1083.10406528    0.36300001    0.37729999    0.31670001    0.28119999]]
[37m[1m[2023-06-25 10:58:32,685][129146] Max Reward on eval: 2401.8875017973596
[37m[1m[2023-06-25 10:58:32,686][129146] Min Reward on eval: 827.1357690234902
[37m[1m[2023-06-25 10:58:32,686][129146] Mean Reward across all agents: 1662.7080198059145
[37m[1m[2023-06-25 10:58:32,686][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 10:58:32,688][129146] mean_value=-414.0112783559729, max_value=262.6930883390137
[37m[1m[2023-06-25 10:58:32,690][129146] New mean coefficients: [[ 3.0753284  -0.52266616 -0.8883577  -0.0937501   1.1140546 ]]
[37m[1m[2023-06-25 10:58:32,691][129146] Moving the mean solution point...
[36m[2023-06-25 10:58:42,299][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 10:58:42,299][129146] FPS: 399768.18
[36m[2023-06-25 10:58:42,301][129146] itr=1090, itrs=2000, Progress: 54.50%
[37m[1m[2023-06-25 10:58:49,575][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001070
[36m[2023-06-25 10:59:01,163][129146] train() took 11.36 seconds to complete
[36m[2023-06-25 10:59:01,163][129146] FPS: 337968.13
[36m[2023-06-25 10:59:05,920][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:59:05,926][129146] Reward + Measures: [[1583.97728766    0.20257886    0.60830045    0.20146206    0.29838353]]
[37m[1m[2023-06-25 10:59:05,926][129146] Max Reward on eval: 1583.9772876594625
[37m[1m[2023-06-25 10:59:05,926][129146] Min Reward on eval: 1583.9772876594625
[37m[1m[2023-06-25 10:59:05,927][129146] Mean Reward across all agents: 1583.9772876594625
[37m[1m[2023-06-25 10:59:05,927][129146] Average Trajectory Length: 999.6853333333333
[36m[2023-06-25 10:59:11,352][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:59:11,353][129146] Reward + Measures: [[1383.08917381    0.21270001    0.45490003    0.2217        0.22579999]
[37m[1m [1151.06568924    0.19050001    0.44460002    0.26430002    0.2309    ]
[37m[1m [1478.32339394    0.23080002    0.51069993    0.17150001    0.27200004]
[37m[1m ...
[37m[1m [ 731.0454242     0.1753        0.40212637    0.24937132    0.21667598]
[37m[1m [1238.8836134     0.2124        0.38690001    0.2282        0.19670001]
[37m[1m [1114.69414832    0.19410968    0.4378452     0.2707645     0.20589678]]
[37m[1m[2023-06-25 10:59:11,353][129146] Max Reward on eval: 1777.9358300923952
[37m[1m[2023-06-25 10:59:11,353][129146] Min Reward on eval: -876.911674108915
[37m[1m[2023-06-25 10:59:11,353][129146] Mean Reward across all agents: 833.0361526425105
[37m[1m[2023-06-25 10:59:11,354][129146] Average Trajectory Length: 961.555
[36m[2023-06-25 10:59:11,356][129146] mean_value=-897.3654995199083, max_value=438.966043899403
[37m[1m[2023-06-25 10:59:11,358][129146] New mean coefficients: [[ 2.9330955  -0.6532793  -0.6817229   0.07202135  0.9484892 ]]
[37m[1m[2023-06-25 10:59:11,359][129146] Moving the mean solution point...
[36m[2023-06-25 10:59:21,008][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 10:59:21,008][129146] FPS: 398046.79
[36m[2023-06-25 10:59:21,010][129146] itr=1091, itrs=2000, Progress: 54.55%
[36m[2023-06-25 10:59:32,374][129146] train() took 11.34 seconds to complete
[36m[2023-06-25 10:59:32,374][129146] FPS: 338714.38
[36m[2023-06-25 10:59:36,949][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:59:36,949][129146] Reward + Measures: [[1745.24409489    0.19920395    0.60115725    0.19730778    0.28044373]]
[37m[1m[2023-06-25 10:59:36,949][129146] Max Reward on eval: 1745.2440948885005
[37m[1m[2023-06-25 10:59:36,949][129146] Min Reward on eval: 1745.2440948885005
[37m[1m[2023-06-25 10:59:36,950][129146] Mean Reward across all agents: 1745.2440948885005
[37m[1m[2023-06-25 10:59:36,950][129146] Average Trajectory Length: 998.5723333333333
[36m[2023-06-25 10:59:42,305][129146] Finished Evaluation Step
[37m[1m[2023-06-25 10:59:42,305][129146] Reward + Measures: [[ 132.8259622     0.19310002    0.56569999    0.189         0.3906    ]
[37m[1m [ 800.32940054    0.3285        0.56970006    0.30000001    0.42750001]
[37m[1m [1297.88588181    0.21280001    0.59289998    0.205         0.33410004]
[37m[1m ...
[37m[1m [-161.42668618    0.27379999    0.60600007    0.3251        0.51739997]
[37m[1m [ 447.40512982    0.1899        0.57069999    0.1846        0.38519999]
[37m[1m [ 563.82179294    0.29840001    0.50949997    0.26639998    0.42430001]]
[37m[1m[2023-06-25 10:59:42,305][129146] Max Reward on eval: 1813.7449399797247
[37m[1m[2023-06-25 10:59:42,306][129146] Min Reward on eval: -326.6772709181998
[37m[1m[2023-06-25 10:59:42,306][129146] Mean Reward across all agents: 926.991113813245
[37m[1m[2023-06-25 10:59:42,306][129146] Average Trajectory Length: 988.0476666666666
[36m[2023-06-25 10:59:42,308][129146] mean_value=-840.6646095702677, max_value=637.0008127928566
[37m[1m[2023-06-25 10:59:42,310][129146] New mean coefficients: [[ 2.9276788  -0.42352864 -0.03694874  0.04719813  0.6150803 ]]
[37m[1m[2023-06-25 10:59:42,311][129146] Moving the mean solution point...
[36m[2023-06-25 10:59:51,938][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 10:59:51,938][129146] FPS: 398946.40
[36m[2023-06-25 10:59:51,941][129146] itr=1092, itrs=2000, Progress: 54.60%
[36m[2023-06-25 11:00:03,383][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 11:00:03,384][129146] FPS: 336253.53
[36m[2023-06-25 11:00:08,166][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:00:08,167][129146] Reward + Measures: [[1898.40433954    0.1965518     0.5979901     0.1986932     0.26590386]]
[37m[1m[2023-06-25 11:00:08,167][129146] Max Reward on eval: 1898.4043395413805
[37m[1m[2023-06-25 11:00:08,167][129146] Min Reward on eval: 1898.4043395413805
[37m[1m[2023-06-25 11:00:08,167][129146] Mean Reward across all agents: 1898.4043395413805
[37m[1m[2023-06-25 11:00:08,167][129146] Average Trajectory Length: 998.9443333333332
[36m[2023-06-25 11:00:13,685][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:00:13,685][129146] Reward + Measures: [[1109.45946546    0.26270002    0.60820001    0.16190001    0.30510002]
[37m[1m [ 255.24141529    0.60190004    0.81049997    0.66489995    0.76370001]
[37m[1m [ 519.12630388    0.22264488    0.50830412    0.39512101    0.3267059 ]
[37m[1m ...
[37m[1m [1373.32418815    0.18810001    0.5284        0.2273        0.2638    ]
[37m[1m [ 975.38829637    0.22259998    0.56339997    0.22380002    0.32290003]
[37m[1m [1624.12731418    0.21500002    0.64810002    0.15970002    0.3224    ]]
[37m[1m[2023-06-25 11:00:13,686][129146] Max Reward on eval: 2079.6522962160875
[37m[1m[2023-06-25 11:00:13,686][129146] Min Reward on eval: -170.2322773870488
[37m[1m[2023-06-25 11:00:13,686][129146] Mean Reward across all agents: 914.5824486658792
[37m[1m[2023-06-25 11:00:13,686][129146] Average Trajectory Length: 983.669
[36m[2023-06-25 11:00:13,688][129146] mean_value=-874.0540805067892, max_value=871.1417061757666
[37m[1m[2023-06-25 11:00:13,691][129146] New mean coefficients: [[ 2.9016006  -0.3371473  -0.5736994  -0.00235631  0.5579313 ]]
[37m[1m[2023-06-25 11:00:13,692][129146] Moving the mean solution point...
[36m[2023-06-25 11:00:23,479][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 11:00:23,480][129146] FPS: 392392.29
[36m[2023-06-25 11:00:23,482][129146] itr=1093, itrs=2000, Progress: 54.65%
[36m[2023-06-25 11:00:34,896][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 11:00:34,896][129146] FPS: 337138.16
[36m[2023-06-25 11:00:39,811][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:00:39,811][129146] Reward + Measures: [[2036.01698469    0.19462317    0.59351075    0.19985151    0.25520489]]
[37m[1m[2023-06-25 11:00:39,812][129146] Max Reward on eval: 2036.016984693744
[37m[1m[2023-06-25 11:00:39,812][129146] Min Reward on eval: 2036.016984693744
[37m[1m[2023-06-25 11:00:39,812][129146] Mean Reward across all agents: 2036.016984693744
[37m[1m[2023-06-25 11:00:39,812][129146] Average Trajectory Length: 999.0116666666667
[36m[2023-06-25 11:00:45,304][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:00:45,305][129146] Reward + Measures: [[1165.09734983    0.2597        0.61700004    0.36149999    0.32619998]
[37m[1m [1783.43389327    0.2017        0.60300004    0.206         0.27690002]
[37m[1m [1542.94922669    0.16150193    0.49203703    0.18799178    0.19624406]
[37m[1m ...
[37m[1m [ 629.63627572    0.3689        0.55430001    0.4068        0.43220001]
[37m[1m [1017.45581076    0.21799998    0.5255        0.21970001    0.32480001]
[37m[1m [1235.096971      0.25119999    0.6092        0.24249999    0.31060001]]
[37m[1m[2023-06-25 11:00:45,305][129146] Max Reward on eval: 2130.8910772217437
[37m[1m[2023-06-25 11:00:45,305][129146] Min Reward on eval: -365.04317849329675
[37m[1m[2023-06-25 11:00:45,305][129146] Mean Reward across all agents: 1208.4066425274325
[37m[1m[2023-06-25 11:00:45,305][129146] Average Trajectory Length: 998.7383333333333
[36m[2023-06-25 11:00:45,307][129146] mean_value=-646.7374570770809, max_value=618.4818215692762
[37m[1m[2023-06-25 11:00:45,310][129146] New mean coefficients: [[ 2.6792326  -0.08518007 -0.62582517 -0.08299862  0.25338668]]
[37m[1m[2023-06-25 11:00:45,311][129146] Moving the mean solution point...
[36m[2023-06-25 11:00:55,006][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 11:00:55,011][129146] FPS: 396156.99
[36m[2023-06-25 11:00:55,014][129146] itr=1094, itrs=2000, Progress: 54.70%
[36m[2023-06-25 11:01:06,545][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 11:01:06,545][129146] FPS: 333713.77
[36m[2023-06-25 11:01:11,220][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:01:11,220][129146] Reward + Measures: [[2145.28056244    0.19393982    0.59553993    0.19746484    0.24371476]]
[37m[1m[2023-06-25 11:01:11,220][129146] Max Reward on eval: 2145.2805624364787
[37m[1m[2023-06-25 11:01:11,220][129146] Min Reward on eval: 2145.2805624364787
[37m[1m[2023-06-25 11:01:11,221][129146] Mean Reward across all agents: 2145.2805624364787
[37m[1m[2023-06-25 11:01:11,221][129146] Average Trajectory Length: 999.4086666666666
[36m[2023-06-25 11:01:16,621][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:01:16,627][129146] Reward + Measures: [[ 112.47382306    0.4752        0.77279997    0.2599        0.62550002]
[37m[1m [ 416.77875915    0.42670003    0.70050001    0.1188        0.55479997]
[37m[1m [ 984.73715804    0.27599999    0.597         0.19750002    0.32069999]
[37m[1m ...
[37m[1m [2016.58630301    0.2253        0.53700006    0.2326        0.22430001]
[37m[1m [1914.55823713    0.20420001    0.5898        0.17940001    0.2631    ]
[37m[1m [-178.29577782    0.36320001    0.68380004    0.13090001    0.51789999]]
[37m[1m[2023-06-25 11:01:16,627][129146] Max Reward on eval: 2303.274291796447
[37m[1m[2023-06-25 11:01:16,628][129146] Min Reward on eval: -721.9166172155994
[37m[1m[2023-06-25 11:01:16,628][129146] Mean Reward across all agents: 922.8778955371658
[37m[1m[2023-06-25 11:01:16,628][129146] Average Trajectory Length: 988.284
[36m[2023-06-25 11:01:16,630][129146] mean_value=-804.334295113957, max_value=627.4591794889274
[37m[1m[2023-06-25 11:01:16,632][129146] New mean coefficients: [[ 2.7054937   0.04709575 -0.6121446  -0.00346617  0.26591492]]
[37m[1m[2023-06-25 11:01:16,633][129146] Moving the mean solution point...
[36m[2023-06-25 11:01:26,311][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 11:01:26,311][129146] FPS: 396868.96
[36m[2023-06-25 11:01:26,313][129146] itr=1095, itrs=2000, Progress: 54.75%
[36m[2023-06-25 11:01:37,771][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 11:01:37,771][129146] FPS: 335938.59
[36m[2023-06-25 11:01:42,451][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:01:42,451][129146] Reward + Measures: [[2250.61192323    0.1891585     0.60206085    0.19644983    0.23757429]]
[37m[1m[2023-06-25 11:01:42,452][129146] Max Reward on eval: 2250.6119232254127
[37m[1m[2023-06-25 11:01:42,452][129146] Min Reward on eval: 2250.6119232254127
[37m[1m[2023-06-25 11:01:42,452][129146] Mean Reward across all agents: 2250.6119232254127
[37m[1m[2023-06-25 11:01:42,452][129146] Average Trajectory Length: 998.8496666666666
[36m[2023-06-25 11:01:48,049][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:01:48,050][129146] Reward + Measures: [[1868.09431115    0.20140003    0.57600003    0.26809999    0.25830004]
[37m[1m [2045.58899867    0.16900001    0.60700005    0.2086        0.25740001]
[37m[1m [1826.32393159    0.22140001    0.58339995    0.1981        0.25869998]
[37m[1m ...
[37m[1m [2098.92775238    0.20709999    0.60690004    0.20799999    0.25929999]
[37m[1m [1673.70975915    0.2105        0.55810004    0.20240001    0.27860001]
[37m[1m [ 842.12969176    0.27710003    0.51030004    0.3364        0.32550001]]
[37m[1m[2023-06-25 11:01:48,050][129146] Max Reward on eval: 2320.8472396917177
[37m[1m[2023-06-25 11:01:48,050][129146] Min Reward on eval: -86.27894512497588
[37m[1m[2023-06-25 11:01:48,050][129146] Mean Reward across all agents: 1597.4683553978548
[37m[1m[2023-06-25 11:01:48,051][129146] Average Trajectory Length: 991.3273333333333
[36m[2023-06-25 11:01:48,052][129146] mean_value=-926.8657057620791, max_value=488.58474777841445
[37m[1m[2023-06-25 11:01:48,055][129146] New mean coefficients: [[ 2.7487428  -0.2365444  -0.85910356  0.05489127  0.44326723]]
[37m[1m[2023-06-25 11:01:48,056][129146] Moving the mean solution point...
[36m[2023-06-25 11:01:57,688][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 11:01:57,688][129146] FPS: 398738.79
[36m[2023-06-25 11:01:57,690][129146] itr=1096, itrs=2000, Progress: 54.80%
[36m[2023-06-25 11:02:09,286][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 11:02:09,287][129146] FPS: 331859.21
[36m[2023-06-25 11:02:14,038][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:02:14,038][129146] Reward + Measures: [[2385.46979876    0.19053991    0.59777826    0.19773494    0.23318549]]
[37m[1m[2023-06-25 11:02:14,038][129146] Max Reward on eval: 2385.4697987623595
[37m[1m[2023-06-25 11:02:14,039][129146] Min Reward on eval: 2385.4697987623595
[37m[1m[2023-06-25 11:02:14,039][129146] Mean Reward across all agents: 2385.4697987623595
[37m[1m[2023-06-25 11:02:14,039][129146] Average Trajectory Length: 999.6696666666667
[36m[2023-06-25 11:02:19,513][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:02:19,514][129146] Reward + Measures: [[1985.46349646    0.1939        0.59619999    0.1938        0.19780001]
[37m[1m [1637.4057427     0.20900002    0.62490004    0.20420001    0.25680003]
[37m[1m [1855.45782677    0.193         0.6142        0.1953        0.19599999]
[37m[1m ...
[37m[1m [1412.06734952    0.19680001    0.43959999    0.3163        0.2552    ]
[37m[1m [2149.41602036    0.19          0.63670009    0.20159999    0.22360002]
[37m[1m [ 712.6673684     0.17920001    0.35419998    0.26110002    0.26840001]]
[37m[1m[2023-06-25 11:02:19,514][129146] Max Reward on eval: 2375.6314826568123
[37m[1m[2023-06-25 11:02:19,514][129146] Min Reward on eval: 322.77775545519546
[37m[1m[2023-06-25 11:02:19,515][129146] Mean Reward across all agents: 1653.1609145695836
[37m[1m[2023-06-25 11:02:19,515][129146] Average Trajectory Length: 994.9173333333333
[36m[2023-06-25 11:02:19,517][129146] mean_value=-625.6853466365637, max_value=1428.8941895867456
[37m[1m[2023-06-25 11:02:19,520][129146] New mean coefficients: [[ 2.8005073  -0.36259878 -0.9070556   0.24283482  0.8881721 ]]
[37m[1m[2023-06-25 11:02:19,521][129146] Moving the mean solution point...
[36m[2023-06-25 11:02:29,283][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 11:02:29,283][129146] FPS: 393436.74
[36m[2023-06-25 11:02:29,285][129146] itr=1097, itrs=2000, Progress: 54.85%
[36m[2023-06-25 11:02:40,862][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 11:02:40,862][129146] FPS: 332374.26
[36m[2023-06-25 11:02:45,683][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:02:45,684][129146] Reward + Measures: [[2499.08217052    0.18682908    0.59938973    0.19884254    0.22740658]]
[37m[1m[2023-06-25 11:02:45,684][129146] Max Reward on eval: 2499.0821705200756
[37m[1m[2023-06-25 11:02:45,684][129146] Min Reward on eval: 2499.0821705200756
[37m[1m[2023-06-25 11:02:45,684][129146] Mean Reward across all agents: 2499.0821705200756
[37m[1m[2023-06-25 11:02:45,685][129146] Average Trajectory Length: 999.7106666666666
[36m[2023-06-25 11:02:51,145][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:02:51,145][129146] Reward + Measures: [[1244.59259211    0.23437743    0.4678688     0.25533763    0.22223118]
[37m[1m [1498.71993542    0.23989999    0.51780003    0.27799997    0.25099999]
[37m[1m [1805.82210863    0.22409999    0.51800001    0.24360001    0.23029999]
[37m[1m ...
[37m[1m [ 669.20134201    0.28560001    0.45109996    0.33270001    0.25209999]
[37m[1m [ 977.71564214    0.25129843    0.46387443    0.27560973    0.22146372]
[37m[1m [ 658.89133224    0.23548131    0.40509531    0.27962795    0.2128495 ]]
[37m[1m[2023-06-25 11:02:51,146][129146] Max Reward on eval: 2512.2124859990085
[37m[1m[2023-06-25 11:02:51,146][129146] Min Reward on eval: -59.65361397543457
[37m[1m[2023-06-25 11:02:51,146][129146] Mean Reward across all agents: 1429.243253122879
[37m[1m[2023-06-25 11:02:51,146][129146] Average Trajectory Length: 977.6996666666666
[36m[2023-06-25 11:02:51,148][129146] mean_value=-899.7043839314426, max_value=192.87550129223746
[37m[1m[2023-06-25 11:02:51,150][129146] New mean coefficients: [[ 2.5693069  -0.30246735 -0.98828137  0.14312753  0.65140927]]
[37m[1m[2023-06-25 11:02:51,151][129146] Moving the mean solution point...
[36m[2023-06-25 11:03:00,835][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 11:03:00,835][129146] FPS: 396623.84
[36m[2023-06-25 11:03:00,837][129146] itr=1098, itrs=2000, Progress: 54.90%
[36m[2023-06-25 11:03:12,325][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 11:03:12,325][129146] FPS: 334943.01
[36m[2023-06-25 11:03:17,111][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:03:17,111][129146] Reward + Measures: [[2614.77160805    0.185389      0.59329742    0.20010893    0.22087121]]
[37m[1m[2023-06-25 11:03:17,112][129146] Max Reward on eval: 2614.7716080505493
[37m[1m[2023-06-25 11:03:17,112][129146] Min Reward on eval: 2614.7716080505493
[37m[1m[2023-06-25 11:03:17,112][129146] Mean Reward across all agents: 2614.7716080505493
[37m[1m[2023-06-25 11:03:17,112][129146] Average Trajectory Length: 999.3166666666666
[36m[2023-06-25 11:03:22,526][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:03:22,527][129146] Reward + Measures: [[2232.00407088    0.1839        0.6462        0.2168        0.25229999]
[37m[1m [ 413.00304637    0.44420004    0.57369995    0.4481        0.3513    ]
[37m[1m [2055.25758479    0.2132        0.52950001    0.22930001    0.27560002]
[37m[1m ...
[37m[1m [1684.38197039    0.1918        0.7022        0.24789999    0.2983    ]
[37m[1m [ 385.73291868    0.37450001    0.56800002    0.34810001    0.38440001]
[37m[1m [2205.38175609    0.1837        0.64499998    0.2031        0.24940002]]
[37m[1m[2023-06-25 11:03:22,527][129146] Max Reward on eval: 2724.871063369559
[37m[1m[2023-06-25 11:03:22,527][129146] Min Reward on eval: -337.84175011507176
[37m[1m[2023-06-25 11:03:22,527][129146] Mean Reward across all agents: 1309.3379236100852
[37m[1m[2023-06-25 11:03:22,528][129146] Average Trajectory Length: 999.3776666666666
[36m[2023-06-25 11:03:22,530][129146] mean_value=-696.8291808984457, max_value=571.6128293504346
[37m[1m[2023-06-25 11:03:22,532][129146] New mean coefficients: [[ 2.6227844  -0.45454478 -0.70215786  0.20857695  0.5805101 ]]
[37m[1m[2023-06-25 11:03:22,533][129146] Moving the mean solution point...
[36m[2023-06-25 11:03:32,270][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 11:03:32,271][129146] FPS: 394440.88
[36m[2023-06-25 11:03:32,273][129146] itr=1099, itrs=2000, Progress: 54.95%
[36m[2023-06-25 11:03:43,922][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 11:03:43,923][129146] FPS: 330287.80
[36m[2023-06-25 11:03:48,743][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:03:48,743][129146] Reward + Measures: [[2747.98776766    0.18093868    0.58868194    0.20527247    0.21282925]]
[37m[1m[2023-06-25 11:03:48,743][129146] Max Reward on eval: 2747.9877676583824
[37m[1m[2023-06-25 11:03:48,743][129146] Min Reward on eval: 2747.9877676583824
[37m[1m[2023-06-25 11:03:48,744][129146] Mean Reward across all agents: 2747.9877676583824
[37m[1m[2023-06-25 11:03:48,744][129146] Average Trajectory Length: 999.462
[36m[2023-06-25 11:03:54,182][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:03:54,183][129146] Reward + Measures: [[1730.5249648     0.25139999    0.66100001    0.1666        0.2678    ]
[37m[1m [ 499.56634179    0.24990001    0.50159997    0.1671        0.2421    ]
[37m[1m [1997.86285371    0.17770001    0.56510007    0.25689998    0.26340002]
[37m[1m ...
[37m[1m [ 893.80612302    0.18645492    0.56931061    0.25896621    0.2831125 ]
[37m[1m [1511.30851999    0.2419        0.63029999    0.19599999    0.25050002]
[37m[1m [1604.17188698    0.2339        0.59429997    0.25100002    0.28239998]]
[37m[1m[2023-06-25 11:03:54,183][129146] Max Reward on eval: 2738.662170169596
[37m[1m[2023-06-25 11:03:54,183][129146] Min Reward on eval: 137.07418114712635
[37m[1m[2023-06-25 11:03:54,183][129146] Mean Reward across all agents: 1849.793280820443
[37m[1m[2023-06-25 11:03:54,184][129146] Average Trajectory Length: 992.9316666666666
[36m[2023-06-25 11:03:54,185][129146] mean_value=-987.2884591540574, max_value=501.2281915152024
[37m[1m[2023-06-25 11:03:54,188][129146] New mean coefficients: [[ 2.5183103  -0.5148005   0.1551351   0.16178581 -0.016168  ]]
[37m[1m[2023-06-25 11:03:54,189][129146] Moving the mean solution point...
[36m[2023-06-25 11:04:04,045][129146] train() took 9.85 seconds to complete
[36m[2023-06-25 11:04:04,045][129146] FPS: 389661.26
[36m[2023-06-25 11:04:04,048][129146] itr=1100, itrs=2000, Progress: 55.00%
[37m[1m[2023-06-25 11:04:11,613][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001080
[36m[2023-06-25 11:04:23,407][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 11:04:23,407][129146] FPS: 331958.38
[36m[2023-06-25 11:04:28,220][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:04:28,221][129146] Reward + Measures: [[2877.42559901    0.18307072    0.57815456    0.20750806    0.20420697]]
[37m[1m[2023-06-25 11:04:28,221][129146] Max Reward on eval: 2877.4255990098186
[37m[1m[2023-06-25 11:04:28,221][129146] Min Reward on eval: 2877.4255990098186
[37m[1m[2023-06-25 11:04:28,222][129146] Mean Reward across all agents: 2877.4255990098186
[37m[1m[2023-06-25 11:04:28,222][129146] Average Trajectory Length: 999.4259999999999
[36m[2023-06-25 11:04:33,674][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:04:33,675][129146] Reward + Measures: [[2493.98325632    0.16320001    0.63029999    0.1901        0.24059999]
[37m[1m [ 249.30810844    0.3493        0.75239998    0.04390001    0.76170009]
[37m[1m [2332.73505343    0.19430001    0.54120004    0.24130002    0.19719999]
[37m[1m ...
[37m[1m [2436.44836923    0.1455        0.63030005    0.1744        0.22580002]
[37m[1m [2045.36052903    0.18200001    0.50389999    0.23969999    0.2149    ]
[37m[1m [1967.22706879    0.15900001    0.68690002    0.1691        0.26100001]]
[37m[1m[2023-06-25 11:04:33,675][129146] Max Reward on eval: 2847.543901295122
[37m[1m[2023-06-25 11:04:33,675][129146] Min Reward on eval: -28.30820511444581
[37m[1m[2023-06-25 11:04:33,676][129146] Mean Reward across all agents: 1857.1895937731613
[37m[1m[2023-06-25 11:04:33,676][129146] Average Trajectory Length: 990.765
[36m[2023-06-25 11:04:33,679][129146] mean_value=-477.53407171267247, max_value=2622.258399564284
[37m[1m[2023-06-25 11:04:33,681][129146] New mean coefficients: [[ 2.5850945  -0.7753165  -0.4700876   0.18147519  0.4361292 ]]
[37m[1m[2023-06-25 11:04:33,682][129146] Moving the mean solution point...
[36m[2023-06-25 11:04:43,360][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 11:04:43,360][129146] FPS: 396843.60
[36m[2023-06-25 11:04:43,363][129146] itr=1101, itrs=2000, Progress: 55.05%
[36m[2023-06-25 11:04:55,010][129146] train() took 11.62 seconds to complete
[36m[2023-06-25 11:04:55,010][129146] FPS: 330505.68
[36m[2023-06-25 11:04:59,806][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:04:59,807][129146] Reward + Measures: [[3025.47764002    0.18381122    0.57026261    0.21092603    0.19958609]]
[37m[1m[2023-06-25 11:04:59,807][129146] Max Reward on eval: 3025.4776400244114
[37m[1m[2023-06-25 11:04:59,807][129146] Min Reward on eval: 3025.4776400244114
[37m[1m[2023-06-25 11:04:59,808][129146] Mean Reward across all agents: 3025.4776400244114
[37m[1m[2023-06-25 11:04:59,808][129146] Average Trajectory Length: 999.2603333333333
[36m[2023-06-25 11:05:05,337][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:05:05,337][129146] Reward + Measures: [[2041.88785805    0.20079999    0.50649995    0.21030001    0.21040002]
[37m[1m [1449.49330582    0.17110363    0.40649828    0.22786391    0.21824372]
[37m[1m [1137.07061984    0.24041358    0.4627254     0.27536273    0.23212881]
[37m[1m ...
[37m[1m [2244.71698973    0.1837        0.63810009    0.21540001    0.23479998]
[37m[1m [2744.26179378    0.18699999    0.55949998    0.2237        0.21110001]
[37m[1m [ 687.01742586    0.23362444    0.52141988    0.22269738    0.29174015]]
[37m[1m[2023-06-25 11:05:05,338][129146] Max Reward on eval: 3004.1111386676785
[37m[1m[2023-06-25 11:05:05,338][129146] Min Reward on eval: -72.8231755763758
[37m[1m[2023-06-25 11:05:05,338][129146] Mean Reward across all agents: 1715.6026187180094
[37m[1m[2023-06-25 11:05:05,338][129146] Average Trajectory Length: 989.7723333333333
[36m[2023-06-25 11:05:05,340][129146] mean_value=-744.7072652678027, max_value=1276.1524315909364
[37m[1m[2023-06-25 11:05:05,343][129146] New mean coefficients: [[ 2.6846495  -0.638523   -0.10998574  0.12571491  0.31557205]]
[37m[1m[2023-06-25 11:05:05,344][129146] Moving the mean solution point...
[36m[2023-06-25 11:05:15,236][129146] train() took 9.89 seconds to complete
[36m[2023-06-25 11:05:15,236][129146] FPS: 388244.99
[36m[2023-06-25 11:05:15,238][129146] itr=1102, itrs=2000, Progress: 55.10%
[36m[2023-06-25 11:05:26,941][129146] train() took 11.68 seconds to complete
[36m[2023-06-25 11:05:26,941][129146] FPS: 328863.53
[36m[2023-06-25 11:05:31,764][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:05:31,765][129146] Reward + Measures: [[3199.46576295    0.18660925    0.56387562    0.21279719    0.19830079]]
[37m[1m[2023-06-25 11:05:31,765][129146] Max Reward on eval: 3199.465762954132
[37m[1m[2023-06-25 11:05:31,765][129146] Min Reward on eval: 3199.465762954132
[37m[1m[2023-06-25 11:05:31,765][129146] Mean Reward across all agents: 3199.465762954132
[37m[1m[2023-06-25 11:05:31,766][129146] Average Trajectory Length: 999.7956666666666
[36m[2023-06-25 11:05:37,308][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:05:37,308][129146] Reward + Measures: [[2956.83824119    0.17030001    0.58780003    0.23529999    0.24270001]
[37m[1m [1911.20820338    0.19340001    0.54680002    0.21100001    0.19499999]
[37m[1m [1464.74614375    0.20390001    0.52060002    0.20130001    0.22319999]
[37m[1m ...
[37m[1m [2789.91106107    0.20289998    0.63380003    0.2333        0.24679999]
[37m[1m [2314.1398192     0.17150001    0.58380002    0.24730001    0.27169999]
[37m[1m [1430.49122472    0.19949999    0.48559999    0.21110001    0.18540001]]
[37m[1m[2023-06-25 11:05:37,308][129146] Max Reward on eval: 3187.2696804834995
[37m[1m[2023-06-25 11:05:37,309][129146] Min Reward on eval: 274.6957697883452
[37m[1m[2023-06-25 11:05:37,309][129146] Mean Reward across all agents: 2225.7698078238327
[37m[1m[2023-06-25 11:05:37,309][129146] Average Trajectory Length: 993.581
[36m[2023-06-25 11:05:37,312][129146] mean_value=-444.7692784059422, max_value=682.0225719700229
[37m[1m[2023-06-25 11:05:37,314][129146] New mean coefficients: [[ 2.7631087  -0.5884681  -0.6082499   0.06358173  0.7435629 ]]
[37m[1m[2023-06-25 11:05:37,315][129146] Moving the mean solution point...
[36m[2023-06-25 11:05:47,167][129146] train() took 9.85 seconds to complete
[36m[2023-06-25 11:05:47,167][129146] FPS: 389858.77
[36m[2023-06-25 11:05:47,169][129146] itr=1103, itrs=2000, Progress: 55.15%
[36m[2023-06-25 11:05:58,806][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 11:05:58,806][129146] FPS: 330652.01
[36m[2023-06-25 11:06:03,618][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:06:03,618][129146] Reward + Measures: [[3367.13711116    0.18453401    0.55772138    0.21353105    0.19355924]]
[37m[1m[2023-06-25 11:06:03,618][129146] Max Reward on eval: 3367.137111161548
[37m[1m[2023-06-25 11:06:03,619][129146] Min Reward on eval: 3367.137111161548
[37m[1m[2023-06-25 11:06:03,619][129146] Mean Reward across all agents: 3367.137111161548
[37m[1m[2023-06-25 11:06:03,619][129146] Average Trajectory Length: 999.6113333333333
[36m[2023-06-25 11:06:09,094][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:06:09,095][129146] Reward + Measures: [[2375.22649685    0.19780001    0.55390006    0.234         0.22839999]
[37m[1m [2558.9626732     0.17900001    0.58279997    0.23199999    0.2122    ]
[37m[1m [1352.33301999    0.1908        0.65750003    0.2563        0.28420001]
[37m[1m ...
[37m[1m [3137.89392119    0.18970001    0.54589999    0.21630001    0.18840002]
[37m[1m [2310.0653415     0.16830002    0.63589996    0.2242        0.21970001]
[37m[1m [ 654.96801565    0.2529        0.48330003    0.26640001    0.33580002]]
[37m[1m[2023-06-25 11:06:09,095][129146] Max Reward on eval: 3213.6908257856967
[37m[1m[2023-06-25 11:06:09,095][129146] Min Reward on eval: -399.5036021847103
[37m[1m[2023-06-25 11:06:09,095][129146] Mean Reward across all agents: 1613.7758133392604
[37m[1m[2023-06-25 11:06:09,096][129146] Average Trajectory Length: 993.6396666666666
[36m[2023-06-25 11:06:09,097][129146] mean_value=-845.3037006168487, max_value=389.26144645884233
[37m[1m[2023-06-25 11:06:09,100][129146] New mean coefficients: [[ 2.5720396  -0.49214247 -0.15652171  0.15598127  0.11483794]]
[37m[1m[2023-06-25 11:06:09,101][129146] Moving the mean solution point...
[36m[2023-06-25 11:06:18,863][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 11:06:18,863][129146] FPS: 393416.08
[36m[2023-06-25 11:06:18,865][129146] itr=1104, itrs=2000, Progress: 55.20%
[36m[2023-06-25 11:06:30,326][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 11:06:30,327][129146] FPS: 335721.99
[36m[2023-06-25 11:06:35,158][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:06:35,159][129146] Reward + Measures: [[3539.24073071    0.1852524     0.55338901    0.21260402    0.18904428]]
[37m[1m[2023-06-25 11:06:35,159][129146] Max Reward on eval: 3539.240730708757
[37m[1m[2023-06-25 11:06:35,159][129146] Min Reward on eval: 3539.240730708757
[37m[1m[2023-06-25 11:06:35,159][129146] Mean Reward across all agents: 3539.240730708757
[37m[1m[2023-06-25 11:06:35,160][129146] Average Trajectory Length: 999.7256666666666
[36m[2023-06-25 11:06:40,857][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:06:40,863][129146] Reward + Measures: [[2971.91974595    0.23339999    0.55479997    0.18910001    0.18380001]
[37m[1m [2661.2916416     0.16330001    0.61260003    0.2667        0.22409999]
[37m[1m [2588.63472351    0.26079997    0.52630001    0.19670001    0.19430001]
[37m[1m ...
[37m[1m [2518.73391834    0.20720001    0.58090001    0.2016        0.20079999]
[37m[1m [1579.00779686    0.1925444     0.46109554    0.20445333    0.17863248]
[37m[1m [1937.59189067    0.20904998    0.53757918    0.21636251    0.18840419]]
[37m[1m[2023-06-25 11:06:40,864][129146] Max Reward on eval: 3438.0668758766724
[37m[1m[2023-06-25 11:06:40,865][129146] Min Reward on eval: -199.0070909922011
[37m[1m[2023-06-25 11:06:40,865][129146] Mean Reward across all agents: 1980.8439119959828
[37m[1m[2023-06-25 11:06:40,866][129146] Average Trajectory Length: 973.0006666666667
[36m[2023-06-25 11:06:40,871][129146] mean_value=-575.9901050785319, max_value=1242.996105573311
[37m[1m[2023-06-25 11:06:40,876][129146] New mean coefficients: [[ 2.404789   -0.4192238   0.10833433  0.0213764  -0.02139123]]
[37m[1m[2023-06-25 11:06:40,878][129146] Moving the mean solution point...
[36m[2023-06-25 11:06:50,626][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 11:06:50,627][129146] FPS: 393982.37
[36m[2023-06-25 11:06:50,629][129146] itr=1105, itrs=2000, Progress: 55.25%
[36m[2023-06-25 11:07:02,136][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 11:07:02,136][129146] FPS: 334397.61
[36m[2023-06-25 11:07:06,836][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:07:06,842][129146] Reward + Measures: [[3611.87966286    0.19330013    0.54801345    0.21437871    0.18821916]]
[37m[1m[2023-06-25 11:07:06,842][129146] Max Reward on eval: 3611.879662863669
[37m[1m[2023-06-25 11:07:06,842][129146] Min Reward on eval: 3611.879662863669
[37m[1m[2023-06-25 11:07:06,843][129146] Mean Reward across all agents: 3611.879662863669
[37m[1m[2023-06-25 11:07:06,843][129146] Average Trajectory Length: 999.9693333333333
[36m[2023-06-25 11:07:12,249][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:07:12,249][129146] Reward + Measures: [[3335.06731282    0.19750001    0.55050004    0.22500001    0.20369999]
[37m[1m [2007.09630408    0.2098        0.49750003    0.331         0.20820001]
[37m[1m [2463.14226517    0.19939999    0.64359999    0.1578        0.25100002]
[37m[1m ...
[37m[1m [3107.10120021    0.1822        0.57179999    0.2112        0.21929999]
[37m[1m [2583.17874264    0.21540001    0.59710002    0.1857        0.20890002]
[37m[1m [3076.32207996    0.16669999    0.56999999    0.23410001    0.18610001]]
[37m[1m[2023-06-25 11:07:12,250][129146] Max Reward on eval: 3454.440545992181
[37m[1m[2023-06-25 11:07:12,250][129146] Min Reward on eval: -341.4764743262087
[37m[1m[2023-06-25 11:07:12,250][129146] Mean Reward across all agents: 2351.1961571699953
[37m[1m[2023-06-25 11:07:12,250][129146] Average Trajectory Length: 995.2156666666666
[36m[2023-06-25 11:07:12,254][129146] mean_value=-218.75262791056866, max_value=1107.9924628011395
[37m[1m[2023-06-25 11:07:12,257][129146] New mean coefficients: [[ 2.2711859  -0.33863086 -0.49358526 -0.21259359  0.01416595]]
[37m[1m[2023-06-25 11:07:12,258][129146] Moving the mean solution point...
[36m[2023-06-25 11:07:21,933][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 11:07:21,933][129146] FPS: 396973.46
[36m[2023-06-25 11:07:21,935][129146] itr=1106, itrs=2000, Progress: 55.30%
[36m[2023-06-25 11:07:33,555][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 11:07:33,556][129146] FPS: 331148.97
[36m[2023-06-25 11:07:38,362][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:07:38,362][129146] Reward + Measures: [[3705.9261844     0.19079714    0.54565603    0.21360341    0.18821523]]
[37m[1m[2023-06-25 11:07:38,363][129146] Max Reward on eval: 3705.9261844024027
[37m[1m[2023-06-25 11:07:38,363][129146] Min Reward on eval: 3705.9261844024027
[37m[1m[2023-06-25 11:07:38,363][129146] Mean Reward across all agents: 3705.9261844024027
[37m[1m[2023-06-25 11:07:38,363][129146] Average Trajectory Length: 999.9616666666666
[36m[2023-06-25 11:07:43,900][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:07:43,901][129146] Reward + Measures: [[3016.01253289    0.1717        0.56310004    0.29050002    0.21180001]
[37m[1m [ 242.49879365    0.35810003    0.69569999    0.39750001    0.61379999]
[37m[1m [3219.08820587    0.24340001    0.52200001    0.2053        0.20469999]
[37m[1m ...
[37m[1m [1900.68727579    0.24459998    0.52349997    0.26760003    0.235     ]
[37m[1m [2151.11049267    0.2122        0.49349999    0.28870001    0.21440001]
[37m[1m [3311.34576671    0.17659999    0.56739998    0.2703        0.19960001]]
[37m[1m[2023-06-25 11:07:43,901][129146] Max Reward on eval: 3720.887093057111
[37m[1m[2023-06-25 11:07:43,901][129146] Min Reward on eval: 242.49879364702937
[37m[1m[2023-06-25 11:07:43,902][129146] Mean Reward across all agents: 2376.6563839513983
[37m[1m[2023-06-25 11:07:43,902][129146] Average Trajectory Length: 998.8156666666666
[36m[2023-06-25 11:07:43,905][129146] mean_value=-381.50787329445205, max_value=836.6960092554541
[37m[1m[2023-06-25 11:07:43,908][129146] New mean coefficients: [[ 2.0753217  -0.2255066  -0.6132688  -0.32174635 -0.05163699]]
[37m[1m[2023-06-25 11:07:43,909][129146] Moving the mean solution point...
[36m[2023-06-25 11:07:53,644][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 11:07:53,644][129146] FPS: 394540.37
[36m[2023-06-25 11:07:53,646][129146] itr=1107, itrs=2000, Progress: 55.35%
[36m[2023-06-25 11:08:05,089][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 11:08:05,090][129146] FPS: 336357.70
[36m[2023-06-25 11:08:09,815][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:08:09,815][129146] Reward + Measures: [[3897.17720824    0.191571      0.54602468    0.20809865    0.18692568]]
[37m[1m[2023-06-25 11:08:09,816][129146] Max Reward on eval: 3897.177208244507
[37m[1m[2023-06-25 11:08:09,816][129146] Min Reward on eval: 3897.177208244507
[37m[1m[2023-06-25 11:08:09,816][129146] Mean Reward across all agents: 3897.177208244507
[37m[1m[2023-06-25 11:08:09,816][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:08:15,212][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:08:15,213][129146] Reward + Measures: [[2950.83471389    0.1839        0.58450001    0.26330003    0.23710001]
[37m[1m [3326.00057092    0.189         0.4659        0.27950001    0.20369999]
[37m[1m [3022.70187922    0.1815        0.56129998    0.25079998    0.2325    ]
[37m[1m ...
[37m[1m [2639.39077546    0.17710002    0.57490009    0.29949999    0.27400002]
[37m[1m [3042.55504673    0.18550001    0.57639998    0.23169999    0.23530002]
[37m[1m [2736.52359692    0.1698        0.56840003    0.3091        0.2093    ]]
[37m[1m[2023-06-25 11:08:15,213][129146] Max Reward on eval: 3776.3216137103736
[37m[1m[2023-06-25 11:08:15,213][129146] Min Reward on eval: -275.1132313943235
[37m[1m[2023-06-25 11:08:15,213][129146] Mean Reward across all agents: 2035.8933157991285
[37m[1m[2023-06-25 11:08:15,214][129146] Average Trajectory Length: 999.5073333333333
[36m[2023-06-25 11:08:15,219][129146] mean_value=-103.50374493925247, max_value=1184.3022439166878
[37m[1m[2023-06-25 11:08:15,221][129146] New mean coefficients: [[ 2.0567482  -0.20842743 -0.6785998  -0.5029887   0.1060054 ]]
[37m[1m[2023-06-25 11:08:15,222][129146] Moving the mean solution point...
[36m[2023-06-25 11:08:24,854][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 11:08:24,854][129146] FPS: 398756.85
[36m[2023-06-25 11:08:24,856][129146] itr=1108, itrs=2000, Progress: 55.40%
[36m[2023-06-25 11:08:36,312][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 11:08:36,312][129146] FPS: 335985.13
[36m[2023-06-25 11:08:41,103][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:08:41,103][129146] Reward + Measures: [[3947.63471435    0.20125835    0.53637069    0.22909999    0.19182466]]
[37m[1m[2023-06-25 11:08:41,104][129146] Max Reward on eval: 3947.6347143513904
[37m[1m[2023-06-25 11:08:41,104][129146] Min Reward on eval: 3947.6347143513904
[37m[1m[2023-06-25 11:08:41,104][129146] Mean Reward across all agents: 3947.6347143513904
[37m[1m[2023-06-25 11:08:41,104][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:08:46,569][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:08:46,569][129146] Reward + Measures: [[1656.00586386    0.16120002    0.4711        0.2766        0.25430003]
[37m[1m [2330.55852554    0.1743        0.55789995    0.28780001    0.2067    ]
[37m[1m [1139.09265204    0.21259999    0.55660003    0.33300003    0.2735    ]
[37m[1m ...
[37m[1m [2896.74754985    0.2198        0.54650003    0.29730001    0.24850002]
[37m[1m [2142.46889166    0.18480001    0.57490009    0.32969999    0.23989999]
[37m[1m [2370.95319252    0.2247        0.56569999    0.19579999    0.19389999]]
[37m[1m[2023-06-25 11:08:46,569][129146] Max Reward on eval: 3876.841972810775
[37m[1m[2023-06-25 11:08:46,570][129146] Min Reward on eval: 135.35176273289835
[37m[1m[2023-06-25 11:08:46,570][129146] Mean Reward across all agents: 2077.5395106210667
[37m[1m[2023-06-25 11:08:46,570][129146] Average Trajectory Length: 997.4006666666667
[36m[2023-06-25 11:08:46,573][129146] mean_value=-595.3588320021759, max_value=1125.9678763427196
[37m[1m[2023-06-25 11:08:46,575][129146] New mean coefficients: [[ 2.0349135  -0.14870977 -0.3959122  -0.3365844   0.31508696]]
[37m[1m[2023-06-25 11:08:46,576][129146] Moving the mean solution point...
[36m[2023-06-25 11:08:56,243][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 11:08:56,244][129146] FPS: 397272.42
[36m[2023-06-25 11:08:56,246][129146] itr=1109, itrs=2000, Progress: 55.45%
[36m[2023-06-25 11:09:07,731][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 11:09:07,731][129146] FPS: 335121.75
[36m[2023-06-25 11:09:12,519][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:09:12,520][129146] Reward + Measures: [[4106.22493789    0.20213173    0.53451139    0.21968563    0.18799339]]
[37m[1m[2023-06-25 11:09:12,520][129146] Max Reward on eval: 4106.224937893352
[37m[1m[2023-06-25 11:09:12,520][129146] Min Reward on eval: 4106.224937893352
[37m[1m[2023-06-25 11:09:12,520][129146] Mean Reward across all agents: 4106.224937893352
[37m[1m[2023-06-25 11:09:12,521][129146] Average Trajectory Length: 999.8086666666667
[36m[2023-06-25 11:09:18,190][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:09:18,190][129146] Reward + Measures: [[2489.81626945    0.18350001    0.43090007    0.2669        0.19650002]
[37m[1m [1588.37659282    0.16600001    0.3506        0.2911        0.21270001]
[37m[1m [2066.8581894     0.1954        0.42379999    0.2304        0.1839    ]
[37m[1m ...
[37m[1m [1402.92834241    0.2775        0.3651        0.37990001    0.25009999]
[37m[1m [ 843.44161705    0.2465        0.46560001    0.43759999    0.36359999]
[37m[1m [3826.34150472    0.20480001    0.5219        0.2309        0.18599999]]
[37m[1m[2023-06-25 11:09:18,195][129146] Max Reward on eval: 4015.0947190578795
[37m[1m[2023-06-25 11:09:18,196][129146] Min Reward on eval: 344.68473744010555
[37m[1m[2023-06-25 11:09:18,196][129146] Mean Reward across all agents: 2362.8479832791973
[37m[1m[2023-06-25 11:09:18,196][129146] Average Trajectory Length: 996.766
[36m[2023-06-25 11:09:18,201][129146] mean_value=19.225034961812877, max_value=1650.0701832406746
[37m[1m[2023-06-25 11:09:18,204][129146] New mean coefficients: [[ 2.1713371  -0.30501303 -0.6990924  -0.2699968   0.8548981 ]]
[37m[1m[2023-06-25 11:09:18,205][129146] Moving the mean solution point...
[36m[2023-06-25 11:09:28,029][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 11:09:28,030][129146] FPS: 390943.31
[36m[2023-06-25 11:09:28,032][129146] itr=1110, itrs=2000, Progress: 55.50%
[37m[1m[2023-06-25 11:09:35,363][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001090
[36m[2023-06-25 11:09:47,112][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 11:09:47,112][129146] FPS: 333009.06
[36m[2023-06-25 11:09:51,905][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:09:51,905][129146] Reward + Measures: [[3224.80201364    0.21700573    0.53463501    0.28269255    0.19567968]]
[37m[1m[2023-06-25 11:09:51,905][129146] Max Reward on eval: 3224.802013637739
[37m[1m[2023-06-25 11:09:51,905][129146] Min Reward on eval: 3224.802013637739
[37m[1m[2023-06-25 11:09:51,906][129146] Mean Reward across all agents: 3224.802013637739
[37m[1m[2023-06-25 11:09:51,906][129146] Average Trajectory Length: 999.414
[36m[2023-06-25 11:09:57,321][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:09:57,382][129146] Reward + Measures: [[2466.65398396    0.204         0.57750005    0.26139998    0.2024    ]
[37m[1m [1830.73233971    0.21210001    0.63389999    0.26659998    0.2246    ]
[37m[1m [1985.38516459    0.21314445    0.61468518    0.25641853    0.21554814]
[37m[1m ...
[37m[1m [ 316.9458947     0.35069999    0.7719        0.18459998    0.57870001]
[37m[1m [ 956.02145549    0.22519998    0.71499997    0.3073        0.29380003]
[37m[1m [ 182.43221362    0.39669999    0.77520007    0.17209999    0.72750002]]
[37m[1m[2023-06-25 11:09:57,384][129146] Max Reward on eval: 3398.611092324648
[37m[1m[2023-06-25 11:09:57,385][129146] Min Reward on eval: 7.012643931724597
[37m[1m[2023-06-25 11:09:57,386][129146] Mean Reward across all agents: 1619.5451439529522
[37m[1m[2023-06-25 11:09:57,387][129146] Average Trajectory Length: 997.6586666666666
[36m[2023-06-25 11:09:57,394][129146] mean_value=-1256.9403180709767, max_value=793.482708479932
[37m[1m[2023-06-25 11:09:57,405][129146] New mean coefficients: [[ 2.082287   -0.35881585 -0.5573946  -0.05520992  0.68633777]]
[37m[1m[2023-06-25 11:09:57,409][129146] Moving the mean solution point...
[36m[2023-06-25 11:10:07,114][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 11:10:07,115][129146] FPS: 395872.94
[36m[2023-06-25 11:10:07,117][129146] itr=1111, itrs=2000, Progress: 55.55%
[36m[2023-06-25 11:10:18,741][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 11:10:18,741][129146] FPS: 331044.70
[36m[2023-06-25 11:10:23,631][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:10:23,631][129146] Reward + Measures: [[3449.5435741     0.21590568    0.52679998    0.28492233    0.19538099]]
[37m[1m[2023-06-25 11:10:23,631][129146] Max Reward on eval: 3449.543574097857
[37m[1m[2023-06-25 11:10:23,632][129146] Min Reward on eval: 3449.543574097857
[37m[1m[2023-06-25 11:10:23,632][129146] Mean Reward across all agents: 3449.543574097857
[37m[1m[2023-06-25 11:10:23,632][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:10:29,067][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:10:29,067][129146] Reward + Measures: [[2822.13771683    0.22849999    0.54650003    0.27169999    0.23410001]
[37m[1m [2034.64892108    0.22719999    0.45299998    0.2318        0.2201    ]
[37m[1m [2375.4412816     0.19460002    0.41850001    0.23029999    0.1655    ]
[37m[1m ...
[37m[1m [2382.13553684    0.2131        0.56620002    0.47410002    0.24150001]
[37m[1m [2316.35390229    0.23099999    0.56740004    0.39340001    0.25310001]
[37m[1m [2179.61286515    0.2119        0.56589997    0.43679997    0.23340002]]
[37m[1m[2023-06-25 11:10:29,068][129146] Max Reward on eval: 3455.8663738847126
[37m[1m[2023-06-25 11:10:29,068][129146] Min Reward on eval: -154.159913843323
[37m[1m[2023-06-25 11:10:29,068][129146] Mean Reward across all agents: 1996.457894052033
[37m[1m[2023-06-25 11:10:29,068][129146] Average Trajectory Length: 998.444
[36m[2023-06-25 11:10:29,073][129146] mean_value=-33.09121824205693, max_value=2677.968618320022
[37m[1m[2023-06-25 11:10:29,076][129146] New mean coefficients: [[ 1.9353828  -0.42574552 -0.37989727  0.2893266   0.6294951 ]]
[37m[1m[2023-06-25 11:10:29,077][129146] Moving the mean solution point...
[36m[2023-06-25 11:10:38,804][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 11:10:38,805][129146] FPS: 394864.17
[36m[2023-06-25 11:10:38,807][129146] itr=1112, itrs=2000, Progress: 55.60%
[36m[2023-06-25 11:10:50,360][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 11:10:50,360][129146] FPS: 333039.56
[36m[2023-06-25 11:10:55,130][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:10:55,131][129146] Reward + Measures: [[3622.29939157    0.21410333    0.53388929    0.28369367    0.193731  ]]
[37m[1m[2023-06-25 11:10:55,131][129146] Max Reward on eval: 3622.299391570651
[37m[1m[2023-06-25 11:10:55,131][129146] Min Reward on eval: 3622.299391570651
[37m[1m[2023-06-25 11:10:55,131][129146] Mean Reward across all agents: 3622.299391570651
[37m[1m[2023-06-25 11:10:55,132][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:11:00,745][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:11:00,750][129146] Reward + Measures: [[3231.39988175    0.24819998    0.55240005    0.24129999    0.20100001]
[37m[1m [2356.92089823    0.26089999    0.60320008    0.2026        0.1797    ]
[37m[1m [2797.30972361    0.20498219    0.53408855    0.23155223    0.18481211]
[37m[1m ...
[37m[1m [2623.13096832    0.22519998    0.52459997    0.32699999    0.21530001]
[37m[1m [1951.52167921    0.30170003    0.51310009    0.2428        0.23510002]
[37m[1m [2239.36511698    0.26830003    0.50190002    0.20809999    0.1758    ]]
[37m[1m[2023-06-25 11:11:00,750][129146] Max Reward on eval: 3553.6823141416535
[37m[1m[2023-06-25 11:11:00,750][129146] Min Reward on eval: 386.40341223833093
[37m[1m[2023-06-25 11:11:00,751][129146] Mean Reward across all agents: 2201.327208899345
[37m[1m[2023-06-25 11:11:00,751][129146] Average Trajectory Length: 994.9626666666667
[36m[2023-06-25 11:11:00,754][129146] mean_value=-594.8367678895959, max_value=2982.079304548516
[37m[1m[2023-06-25 11:11:00,757][129146] New mean coefficients: [[ 1.6962909  -0.2789569  -0.12632191  0.31038082  0.08424431]]
[37m[1m[2023-06-25 11:11:00,758][129146] Moving the mean solution point...
[36m[2023-06-25 11:11:10,469][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 11:11:10,470][129146] FPS: 395467.85
[36m[2023-06-25 11:11:10,472][129146] itr=1113, itrs=2000, Progress: 55.65%
[36m[2023-06-25 11:11:21,960][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 11:11:21,961][129146] FPS: 335034.96
[36m[2023-06-25 11:11:26,574][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:11:26,574][129146] Reward + Measures: [[3758.52000662    0.20871873    0.52757829    0.27900431    0.19278572]]
[37m[1m[2023-06-25 11:11:26,574][129146] Max Reward on eval: 3758.520006616068
[37m[1m[2023-06-25 11:11:26,574][129146] Min Reward on eval: 3758.520006616068
[37m[1m[2023-06-25 11:11:26,575][129146] Mean Reward across all agents: 3758.520006616068
[37m[1m[2023-06-25 11:11:26,575][129146] Average Trajectory Length: 999.834
[36m[2023-06-25 11:11:32,057][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:11:32,057][129146] Reward + Measures: [[2734.32414641    0.25549999    0.60509998    0.20920001    0.17019999]
[37m[1m [1485.31311214    0.21569887    0.47867247    0.23892644    0.20984943]
[37m[1m [3182.69330609    0.20610002    0.60639995    0.27110001    0.1973    ]
[37m[1m ...
[37m[1m [2276.11245552    0.19780001    0.5686        0.34059998    0.21229999]
[37m[1m [2972.72693055    0.22140001    0.58560002    0.22189999    0.19490002]
[37m[1m [3158.87208687    0.24090002    0.57690001    0.19859999    0.18139999]]
[37m[1m[2023-06-25 11:11:32,058][129146] Max Reward on eval: 3731.264950463781
[37m[1m[2023-06-25 11:11:32,058][129146] Min Reward on eval: 387.48821192503675
[37m[1m[2023-06-25 11:11:32,058][129146] Mean Reward across all agents: 2661.6644687028866
[37m[1m[2023-06-25 11:11:32,058][129146] Average Trajectory Length: 992.5493333333333
[36m[2023-06-25 11:11:32,060][129146] mean_value=-741.0943469962766, max_value=958.9660952812233
[37m[1m[2023-06-25 11:11:32,063][129146] New mean coefficients: [[ 1.5482852  -0.13791056 -0.11376089  0.12503652 -0.05221885]]
[37m[1m[2023-06-25 11:11:32,064][129146] Moving the mean solution point...
[36m[2023-06-25 11:11:41,700][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 11:11:41,700][129146] FPS: 398572.07
[36m[2023-06-25 11:11:41,702][129146] itr=1114, itrs=2000, Progress: 55.70%
[36m[2023-06-25 11:11:53,266][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 11:11:53,266][129146] FPS: 332850.35
[36m[2023-06-25 11:11:57,949][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:11:57,949][129146] Reward + Measures: [[2047.65113248    0.21392444    0.35274446    0.26876825    0.24272788]]
[37m[1m[2023-06-25 11:11:57,950][129146] Max Reward on eval: 2047.6511324770097
[37m[1m[2023-06-25 11:11:57,950][129146] Min Reward on eval: 2047.6511324770097
[37m[1m[2023-06-25 11:11:57,950][129146] Mean Reward across all agents: 2047.6511324770097
[37m[1m[2023-06-25 11:11:57,950][129146] Average Trajectory Length: 995.2666666666667
[36m[2023-06-25 11:12:03,323][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:12:03,324][129146] Reward + Measures: [[1907.30751816    0.16660002    0.33330002    0.3062        0.21100001]
[37m[1m [ 261.53554699    0.2491        0.52310002    0.24100001    0.41160002]
[37m[1m [1325.32959055    0.25680003    0.42179999    0.2658        0.26530001]
[37m[1m ...
[37m[1m [1999.36247692    0.2246        0.3558        0.26269999    0.24899998]
[37m[1m [ 965.30326604    0.21839999    0.3242        0.2392        0.2554    ]
[37m[1m [1495.21541518    0.20203678    0.2920782     0.21588735    0.20904024]]
[37m[1m[2023-06-25 11:12:03,324][129146] Max Reward on eval: 2507.1645835289964
[37m[1m[2023-06-25 11:12:03,324][129146] Min Reward on eval: 261.5355469904258
[37m[1m[2023-06-25 11:12:03,324][129146] Mean Reward across all agents: 1658.204012094482
[37m[1m[2023-06-25 11:12:03,325][129146] Average Trajectory Length: 995.3343333333333
[36m[2023-06-25 11:12:03,328][129146] mean_value=-33.426736861435636, max_value=1768.0875302070995
[37m[1m[2023-06-25 11:12:03,331][129146] New mean coefficients: [[ 1.330068    0.07668881 -0.04566142  0.00221138 -0.48264605]]
[37m[1m[2023-06-25 11:12:03,332][129146] Moving the mean solution point...
[36m[2023-06-25 11:12:12,858][129146] train() took 9.52 seconds to complete
[36m[2023-06-25 11:12:12,858][129146] FPS: 403165.81
[36m[2023-06-25 11:12:12,860][129146] itr=1115, itrs=2000, Progress: 55.75%
[36m[2023-06-25 11:12:24,475][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 11:12:24,475][129146] FPS: 331276.26
[36m[2023-06-25 11:12:29,255][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:12:29,256][129146] Reward + Measures: [[2468.61888631    0.19639188    0.34099665    0.28750435    0.21251948]]
[37m[1m[2023-06-25 11:12:29,256][129146] Max Reward on eval: 2468.618886310098
[37m[1m[2023-06-25 11:12:29,256][129146] Min Reward on eval: 2468.618886310098
[37m[1m[2023-06-25 11:12:29,256][129146] Mean Reward across all agents: 2468.618886310098
[37m[1m[2023-06-25 11:12:29,257][129146] Average Trajectory Length: 988.1693333333333
[36m[2023-06-25 11:12:34,783][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:12:34,784][129146] Reward + Measures: [[1002.52279132    0.15848112    0.25627774    0.18789676    0.18101312]
[37m[1m [ 295.40818695    0.10696449    0.16306663    0.19051678    0.18140616]
[37m[1m [1927.88273351    0.1983        0.46830001    0.4041        0.2023    ]
[37m[1m ...
[37m[1m [1988.67362537    0.21860002    0.389         0.30590001    0.20710002]
[37m[1m [2567.66741599    0.23099999    0.37909999    0.31240001    0.21139999]
[37m[1m [1892.62045615    0.19895001    0.33911252    0.24512501    0.2017    ]]
[37m[1m[2023-06-25 11:12:34,784][129146] Max Reward on eval: 2639.6842975539853
[37m[1m[2023-06-25 11:12:34,784][129146] Min Reward on eval: -219.38527957131154
[37m[1m[2023-06-25 11:12:34,785][129146] Mean Reward across all agents: 1668.7681091278737
[37m[1m[2023-06-25 11:12:34,785][129146] Average Trajectory Length: 980.6379999999999
[36m[2023-06-25 11:12:34,788][129146] mean_value=-233.3231583394316, max_value=1500.825395259859
[37m[1m[2023-06-25 11:12:34,791][129146] New mean coefficients: [[ 1.3256763  -0.04415123 -0.300559   -0.1337245  -0.20405537]]
[37m[1m[2023-06-25 11:12:34,792][129146] Moving the mean solution point...
[36m[2023-06-25 11:12:44,641][129146] train() took 9.85 seconds to complete
[36m[2023-06-25 11:12:44,642][129146] FPS: 389947.82
[36m[2023-06-25 11:12:44,644][129146] itr=1116, itrs=2000, Progress: 55.80%
[36m[2023-06-25 11:12:56,292][129146] train() took 11.62 seconds to complete
[36m[2023-06-25 11:12:56,293][129146] FPS: 330412.17
[36m[2023-06-25 11:13:01,155][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:13:01,156][129146] Reward + Measures: [[2634.68230533    0.19371784    0.3396388     0.27786535    0.2076225 ]]
[37m[1m[2023-06-25 11:13:01,156][129146] Max Reward on eval: 2634.6823053293874
[37m[1m[2023-06-25 11:13:01,156][129146] Min Reward on eval: 2634.6823053293874
[37m[1m[2023-06-25 11:13:01,157][129146] Mean Reward across all agents: 2634.6823053293874
[37m[1m[2023-06-25 11:13:01,157][129146] Average Trajectory Length: 983.9313333333333
[36m[2023-06-25 11:13:06,610][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:13:06,616][129146] Reward + Measures: [[1047.35434655    0.26190001    0.47749996    0.29280001    0.29640001]
[37m[1m [2444.77270346    0.2198        0.34179997    0.29890001    0.2304    ]
[37m[1m [2345.5366045     0.27660003    0.398         0.2868        0.20130001]
[37m[1m ...
[37m[1m [2110.68485381    0.21619999    0.38100001    0.26700002    0.24450003]
[37m[1m [1548.86691784    0.1494        0.25940001    0.21829998    0.15079999]
[37m[1m [1142.30335297    0.23220001    0.4797        0.30140001    0.32869998]]
[37m[1m[2023-06-25 11:13:06,616][129146] Max Reward on eval: 2822.330749855796
[37m[1m[2023-06-25 11:13:06,616][129146] Min Reward on eval: 657.1274138750799
[37m[1m[2023-06-25 11:13:06,617][129146] Mean Reward across all agents: 1937.4065000395524
[37m[1m[2023-06-25 11:13:06,617][129146] Average Trajectory Length: 992.507
[36m[2023-06-25 11:13:06,620][129146] mean_value=-189.70208804597695, max_value=1570.2580962683742
[37m[1m[2023-06-25 11:13:06,623][129146] New mean coefficients: [[ 1.4572486  -0.11945283 -0.17566885 -0.06235471 -0.16343714]]
[37m[1m[2023-06-25 11:13:06,624][129146] Moving the mean solution point...
[36m[2023-06-25 11:13:16,152][129146] train() took 9.53 seconds to complete
[36m[2023-06-25 11:13:16,152][129146] FPS: 403094.33
[36m[2023-06-25 11:13:16,154][129146] itr=1117, itrs=2000, Progress: 55.85%
[36m[2023-06-25 11:13:27,635][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 11:13:27,635][129146] FPS: 335250.45
[36m[2023-06-25 11:13:32,404][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:13:32,404][129146] Reward + Measures: [[2790.12847321    0.19276494    0.33256221    0.27003691    0.20145142]]
[37m[1m[2023-06-25 11:13:32,404][129146] Max Reward on eval: 2790.1284732064005
[37m[1m[2023-06-25 11:13:32,405][129146] Min Reward on eval: 2790.1284732064005
[37m[1m[2023-06-25 11:13:32,405][129146] Mean Reward across all agents: 2790.1284732064005
[37m[1m[2023-06-25 11:13:32,405][129146] Average Trajectory Length: 989.6903333333333
[36m[2023-06-25 11:13:37,993][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:13:37,994][129146] Reward + Measures: [[1180.23124363    0.18216668    0.34763333    0.26884848    0.22126667]
[37m[1m [2014.29998021    0.23340002    0.36300001    0.2976        0.24969999]
[37m[1m [1768.09499906    0.21435122    0.45994636    0.24612927    0.16787317]
[37m[1m ...
[37m[1m [ 378.02857018    0.18797615    0.33090052    0.21459281    0.21910393]
[37m[1m [1881.71919819    0.19800001    0.31820002    0.27830002    0.2386    ]
[37m[1m [1870.87526222    0.20680001    0.40790007    0.29050002    0.19780001]]
[37m[1m[2023-06-25 11:13:37,994][129146] Max Reward on eval: 3063.4450093471446
[37m[1m[2023-06-25 11:13:37,994][129146] Min Reward on eval: 64.49254531164188
[37m[1m[2023-06-25 11:13:37,995][129146] Mean Reward across all agents: 1868.6832585898771
[37m[1m[2023-06-25 11:13:37,995][129146] Average Trajectory Length: 980.3846666666666
[36m[2023-06-25 11:13:37,998][129146] mean_value=-446.23034798549827, max_value=749.7027813300167
[37m[1m[2023-06-25 11:13:38,000][129146] New mean coefficients: [[ 1.3964481  -0.08700896 -0.12367629 -0.01164567 -0.14979608]]
[37m[1m[2023-06-25 11:13:38,001][129146] Moving the mean solution point...
[36m[2023-06-25 11:13:47,704][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 11:13:47,704][129146] FPS: 395836.00
[36m[2023-06-25 11:13:47,706][129146] itr=1118, itrs=2000, Progress: 55.90%
[36m[2023-06-25 11:13:59,201][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 11:13:59,201][129146] FPS: 334750.31
[36m[2023-06-25 11:14:03,878][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:14:03,878][129146] Reward + Measures: [[2967.84478752    0.19372514    0.33429891    0.25717774    0.1967085 ]]
[37m[1m[2023-06-25 11:14:03,879][129146] Max Reward on eval: 2967.844787522542
[37m[1m[2023-06-25 11:14:03,879][129146] Min Reward on eval: 2967.844787522542
[37m[1m[2023-06-25 11:14:03,879][129146] Mean Reward across all agents: 2967.844787522542
[37m[1m[2023-06-25 11:14:03,879][129146] Average Trajectory Length: 988.1833333333333
[36m[2023-06-25 11:14:09,176][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:14:09,176][129146] Reward + Measures: [[ 454.57855885    0.16851819    0.2718485     0.21944241    0.14829698]
[37m[1m [1061.26323033    0.2411        0.33680001    0.3204        0.1549    ]
[37m[1m [3122.87619096    0.1998        0.3714        0.2227        0.1998    ]
[37m[1m ...
[37m[1m [2678.40107705    0.18169574    0.35208428    0.25724894    0.2016141 ]
[37m[1m [2315.55235364    0.2349        0.3601        0.23169999    0.1839    ]
[37m[1m [2530.66605239    0.19940001    0.34709999    0.32769999    0.21299998]]
[37m[1m[2023-06-25 11:14:09,177][129146] Max Reward on eval: 3218.7873762808276
[37m[1m[2023-06-25 11:14:09,177][129146] Min Reward on eval: 217.41933314818598
[37m[1m[2023-06-25 11:14:09,177][129146] Mean Reward across all agents: 1875.7560354081836
[37m[1m[2023-06-25 11:14:09,177][129146] Average Trajectory Length: 962.5699999999999
[36m[2023-06-25 11:14:09,180][129146] mean_value=-275.3502893829776, max_value=2383.630349925114
[37m[1m[2023-06-25 11:14:09,183][129146] New mean coefficients: [[ 1.3008083  -0.03388594  0.11397249  0.10290341 -0.35214078]]
[37m[1m[2023-06-25 11:14:09,184][129146] Moving the mean solution point...
[36m[2023-06-25 11:14:18,870][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 11:14:18,871][129146] FPS: 396499.65
[36m[2023-06-25 11:14:18,873][129146] itr=1119, itrs=2000, Progress: 55.95%
[36m[2023-06-25 11:14:30,416][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 11:14:30,417][129146] FPS: 333316.36
[36m[2023-06-25 11:14:35,222][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:14:35,223][129146] Reward + Measures: [[3288.82709806    0.19645067    0.34355134    0.25095525    0.19195235]]
[37m[1m[2023-06-25 11:14:35,223][129146] Max Reward on eval: 3288.8270980617453
[37m[1m[2023-06-25 11:14:35,223][129146] Min Reward on eval: 3288.8270980617453
[37m[1m[2023-06-25 11:14:35,223][129146] Mean Reward across all agents: 3288.8270980617453
[37m[1m[2023-06-25 11:14:35,223][129146] Average Trajectory Length: 992.0943333333333
[36m[2023-06-25 11:14:40,726][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:14:40,732][129146] Reward + Measures: [[2408.74640273    0.16970001    0.3098        0.23550001    0.18430002]
[37m[1m [3134.24515236    0.17349999    0.34619996    0.28410003    0.18719999]
[37m[1m [2635.5518736     0.1925        0.36200002    0.33399999    0.19350001]
[37m[1m ...
[37m[1m [2058.33366278    0.20420001    0.45480004    0.37750003    0.1911    ]
[37m[1m [1969.13573218    0.17820001    0.2879        0.1991        0.17900001]
[37m[1m [2379.20195349    0.18159999    0.32539999    0.2579        0.18670002]]
[37m[1m[2023-06-25 11:14:40,732][129146] Max Reward on eval: 3479.9629223810507
[37m[1m[2023-06-25 11:14:40,732][129146] Min Reward on eval: -595.5105121245142
[37m[1m[2023-06-25 11:14:40,733][129146] Mean Reward across all agents: 1779.8436811373322
[37m[1m[2023-06-25 11:14:40,733][129146] Average Trajectory Length: 985.3693333333333
[36m[2023-06-25 11:14:40,736][129146] mean_value=-484.24269375436415, max_value=1023.7331405183409
[37m[1m[2023-06-25 11:14:40,739][129146] New mean coefficients: [[ 1.4635115  -0.06263956  0.250891    0.03326003 -0.33151034]]
[37m[1m[2023-06-25 11:14:40,740][129146] Moving the mean solution point...
[36m[2023-06-25 11:14:50,714][129146] train() took 9.97 seconds to complete
[36m[2023-06-25 11:14:50,715][129146] FPS: 385041.87
[36m[2023-06-25 11:14:50,717][129146] itr=1120, itrs=2000, Progress: 56.00%
[37m[1m[2023-06-25 11:14:57,958][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001100
[36m[2023-06-25 11:15:09,862][129146] train() took 11.69 seconds to complete
[36m[2023-06-25 11:15:09,862][129146] FPS: 328607.32
[36m[2023-06-25 11:15:14,598][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:15:14,598][129146] Reward + Measures: [[3524.93338549    0.19995846    0.34540698    0.24745068    0.19097172]]
[37m[1m[2023-06-25 11:15:14,599][129146] Max Reward on eval: 3524.9333854880797
[37m[1m[2023-06-25 11:15:14,599][129146] Min Reward on eval: 3524.9333854880797
[37m[1m[2023-06-25 11:15:14,599][129146] Mean Reward across all agents: 3524.9333854880797
[37m[1m[2023-06-25 11:15:14,599][129146] Average Trajectory Length: 992.5533333333333
[36m[2023-06-25 11:15:19,984][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:15:19,985][129146] Reward + Measures: [[2326.45394152    0.17290001    0.36719999    0.3039        0.18800001]
[37m[1m [2156.97627043    0.17709887    0.3600854     0.37242347    0.18114935]
[37m[1m [2312.86385956    0.15625814    0.30872566    0.24018632    0.16858463]
[37m[1m ...
[37m[1m [2218.61600807    0.22849999    0.4061        0.23179999    0.23290002]
[37m[1m [1846.84961083    0.1869        0.36469999    0.23899999    0.22740002]
[37m[1m [2143.07183255    0.18550001    0.33539999    0.2491        0.19509999]]
[37m[1m[2023-06-25 11:15:19,985][129146] Max Reward on eval: 3597.6770080439746
[37m[1m[2023-06-25 11:15:19,985][129146] Min Reward on eval: 956.1917040064582
[37m[1m[2023-06-25 11:15:19,986][129146] Mean Reward across all agents: 2387.092817631517
[37m[1m[2023-06-25 11:15:19,986][129146] Average Trajectory Length: 985.682
[36m[2023-06-25 11:15:19,989][129146] mean_value=-300.47138906319145, max_value=2908.8500947739253
[37m[1m[2023-06-25 11:15:19,991][129146] New mean coefficients: [[ 1.5822201  -0.15081725  0.1777843  -0.00122611  0.06372833]]
[37m[1m[2023-06-25 11:15:19,992][129146] Moving the mean solution point...
[36m[2023-06-25 11:15:29,629][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 11:15:29,629][129146] FPS: 398536.64
[36m[2023-06-25 11:15:29,632][129146] itr=1121, itrs=2000, Progress: 56.05%
[36m[2023-06-25 11:15:41,124][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 11:15:41,124][129146] FPS: 334842.08
[36m[2023-06-25 11:15:45,834][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:15:45,839][129146] Reward + Measures: [[3699.53242007    0.19883746    0.33792672    0.24672216    0.18954712]]
[37m[1m[2023-06-25 11:15:45,840][129146] Max Reward on eval: 3699.5324200745604
[37m[1m[2023-06-25 11:15:45,840][129146] Min Reward on eval: 3699.5324200745604
[37m[1m[2023-06-25 11:15:45,840][129146] Mean Reward across all agents: 3699.5324200745604
[37m[1m[2023-06-25 11:15:45,841][129146] Average Trajectory Length: 994.7153333333333
[36m[2023-06-25 11:15:51,532][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:15:51,532][129146] Reward + Measures: [[1675.66759507    0.25970003    0.39229998    0.30039999    0.205     ]
[37m[1m [2732.12831027    0.19150001    0.37329999    0.2606        0.1859    ]
[37m[1m [1992.91041001    0.1796        0.38620001    0.23450001    0.18879999]
[37m[1m ...
[37m[1m [3283.66755131    0.2043        0.33769998    0.23989999    0.19450001]
[37m[1m [3093.58506017    0.18090001    0.3536        0.26009998    0.19240001]
[37m[1m [2753.73505899    0.2306        0.37090001    0.2561        0.1911    ]]
[37m[1m[2023-06-25 11:15:51,532][129146] Max Reward on eval: 3777.4826466511468
[37m[1m[2023-06-25 11:15:51,533][129146] Min Reward on eval: -347.67648145122223
[37m[1m[2023-06-25 11:15:51,533][129146] Mean Reward across all agents: 2138.2041831137963
[37m[1m[2023-06-25 11:15:51,533][129146] Average Trajectory Length: 991.3043333333333
[36m[2023-06-25 11:15:51,535][129146] mean_value=-818.8314672598465, max_value=1943.2894702810056
[37m[1m[2023-06-25 11:15:51,538][129146] New mean coefficients: [[ 1.3776171  -0.0533852  -0.0423608  -0.07205528  0.03975811]]
[37m[1m[2023-06-25 11:15:51,539][129146] Moving the mean solution point...
[36m[2023-06-25 11:16:01,415][129146] train() took 9.87 seconds to complete
[36m[2023-06-25 11:16:01,416][129146] FPS: 388866.66
[36m[2023-06-25 11:16:01,418][129146] itr=1122, itrs=2000, Progress: 56.10%
[36m[2023-06-25 11:16:13,044][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 11:16:13,044][129146] FPS: 331071.29
[36m[2023-06-25 11:16:17,828][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:16:17,829][129146] Reward + Measures: [[3872.03637817    0.20077321    0.34137142    0.23663871    0.18699583]]
[37m[1m[2023-06-25 11:16:17,829][129146] Max Reward on eval: 3872.036378167089
[37m[1m[2023-06-25 11:16:17,829][129146] Min Reward on eval: 3872.036378167089
[37m[1m[2023-06-25 11:16:17,830][129146] Mean Reward across all agents: 3872.036378167089
[37m[1m[2023-06-25 11:16:17,830][129146] Average Trajectory Length: 995.6113333333333
[36m[2023-06-25 11:16:23,140][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:16:23,141][129146] Reward + Measures: [[3385.24914168    0.1913        0.34459999    0.27540001    0.18380001]
[37m[1m [2597.35391145    0.2271        0.34349999    0.23450001    0.22980002]
[37m[1m [2926.39634452    0.1716        0.39379999    0.34930003    0.19060002]
[37m[1m ...
[37m[1m [3678.30476357    0.2131        0.34660003    0.22060001    0.18900001]
[37m[1m [3586.61128236    0.20020001    0.34489998    0.27239999    0.18380001]
[37m[1m [3549.4917205     0.1832        0.37779999    0.26730001    0.1925    ]]
[37m[1m[2023-06-25 11:16:23,141][129146] Max Reward on eval: 3918.4677034741267
[37m[1m[2023-06-25 11:16:23,141][129146] Min Reward on eval: 1427.6509765718365
[37m[1m[2023-06-25 11:16:23,142][129146] Mean Reward across all agents: 3008.388716962502
[37m[1m[2023-06-25 11:16:23,142][129146] Average Trajectory Length: 992.0016666666667
[36m[2023-06-25 11:16:23,146][129146] mean_value=-21.471458034200964, max_value=2860.2464192721905
[37m[1m[2023-06-25 11:16:23,148][129146] New mean coefficients: [[ 1.3054113  -0.03680519 -0.14162539 -0.08366064  0.12502973]]
[37m[1m[2023-06-25 11:16:23,149][129146] Moving the mean solution point...
[36m[2023-06-25 11:16:32,891][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 11:16:32,891][129146] FPS: 394240.71
[36m[2023-06-25 11:16:32,894][129146] itr=1123, itrs=2000, Progress: 56.15%
[36m[2023-06-25 11:16:44,329][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 11:16:44,329][129146] FPS: 336511.40
[36m[2023-06-25 11:16:49,104][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:16:49,104][129146] Reward + Measures: [[4024.0980416     0.19950756    0.33812192    0.24127461    0.18586193]]
[37m[1m[2023-06-25 11:16:49,105][129146] Max Reward on eval: 4024.0980416034845
[37m[1m[2023-06-25 11:16:49,105][129146] Min Reward on eval: 4024.0980416034845
[37m[1m[2023-06-25 11:16:49,105][129146] Mean Reward across all agents: 4024.0980416034845
[37m[1m[2023-06-25 11:16:49,105][129146] Average Trajectory Length: 996.8923333333333
[36m[2023-06-25 11:16:54,534][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:16:54,539][129146] Reward + Measures: [[2588.59171888    0.25510025    0.37264037    0.21895123    0.18085718]
[37m[1m [3443.25822319    0.2149        0.34630001    0.22359999    0.1838    ]
[37m[1m [2339.73913856    0.2256        0.32230005    0.22309999    0.17550002]
[37m[1m ...
[37m[1m [2121.44707399    0.2316        0.40599999    0.25410002    0.1714    ]
[37m[1m [2788.35410663    0.19090001    0.2999        0.21350001    0.1753    ]
[37m[1m [3798.9534225     0.18669999    0.3299        0.26019999    0.18269999]]
[37m[1m[2023-06-25 11:16:54,540][129146] Max Reward on eval: 4066.9309041626752
[37m[1m[2023-06-25 11:16:54,540][129146] Min Reward on eval: 874.0846769760828
[37m[1m[2023-06-25 11:16:54,540][129146] Mean Reward across all agents: 2877.9142310600555
[37m[1m[2023-06-25 11:16:54,541][129146] Average Trajectory Length: 991.775
[36m[2023-06-25 11:16:54,544][129146] mean_value=-465.6999621977759, max_value=1675.22349141588
[37m[1m[2023-06-25 11:16:54,546][129146] New mean coefficients: [[ 1.2723818  -0.02793334 -0.07077517 -0.05183697 -0.06138965]]
[37m[1m[2023-06-25 11:16:54,547][129146] Moving the mean solution point...
[36m[2023-06-25 11:17:04,235][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 11:17:04,236][129146] FPS: 396417.74
[36m[2023-06-25 11:17:04,238][129146] itr=1124, itrs=2000, Progress: 56.20%
[36m[2023-06-25 11:17:15,676][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 11:17:15,676][129146] FPS: 336516.21
[36m[2023-06-25 11:17:20,499][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:17:20,499][129146] Reward + Measures: [[4169.252264      0.2054377     0.33968467    0.23072954    0.18310697]]
[37m[1m[2023-06-25 11:17:20,500][129146] Max Reward on eval: 4169.2522639955105
[37m[1m[2023-06-25 11:17:20,500][129146] Min Reward on eval: 4169.2522639955105
[37m[1m[2023-06-25 11:17:20,500][129146] Mean Reward across all agents: 4169.2522639955105
[37m[1m[2023-06-25 11:17:20,500][129146] Average Trajectory Length: 996.7816666666666
[36m[2023-06-25 11:17:25,977][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:17:25,978][129146] Reward + Measures: [[3074.84893433    0.2156        0.30810001    0.2158        0.193     ]
[37m[1m [1680.61058804    0.20634575    0.30498487    0.20405424    0.16338821]
[37m[1m [2992.09113588    0.19000001    0.3283        0.23700002    0.20420001]
[37m[1m ...
[37m[1m [3434.79087954    0.1796        0.4258        0.32370001    0.18440001]
[37m[1m [3664.19232141    0.21330002    0.32839999    0.2369        0.1946    ]
[37m[1m [2732.34132071    0.21510001    0.30380002    0.21660002    0.1925    ]]
[37m[1m[2023-06-25 11:17:25,978][129146] Max Reward on eval: 4244.963851461466
[37m[1m[2023-06-25 11:17:25,978][129146] Min Reward on eval: 502.00969045876116
[37m[1m[2023-06-25 11:17:25,978][129146] Mean Reward across all agents: 2863.89740264733
[37m[1m[2023-06-25 11:17:25,979][129146] Average Trajectory Length: 985.7283333333334
[36m[2023-06-25 11:17:25,981][129146] mean_value=-705.4023985420725, max_value=1525.9475826083628
[37m[1m[2023-06-25 11:17:25,984][129146] New mean coefficients: [[ 1.2386987   0.0030605   0.06653605 -0.03085398  0.09380105]]
[37m[1m[2023-06-25 11:17:25,985][129146] Moving the mean solution point...
[36m[2023-06-25 11:17:35,710][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 11:17:35,710][129146] FPS: 394927.67
[36m[2023-06-25 11:17:35,713][129146] itr=1125, itrs=2000, Progress: 56.25%
[36m[2023-06-25 11:17:47,332][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 11:17:47,332][129146] FPS: 331139.16
[36m[2023-06-25 11:17:52,004][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:17:52,004][129146] Reward + Measures: [[4285.19341298    0.20633176    0.33568794    0.22665901    0.18270068]]
[37m[1m[2023-06-25 11:17:52,005][129146] Max Reward on eval: 4285.193412983601
[37m[1m[2023-06-25 11:17:52,005][129146] Min Reward on eval: 4285.193412983601
[37m[1m[2023-06-25 11:17:52,005][129146] Mean Reward across all agents: 4285.193412983601
[37m[1m[2023-06-25 11:17:52,005][129146] Average Trajectory Length: 998.5169999999999
[36m[2023-06-25 11:17:57,498][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:17:57,498][129146] Reward + Measures: [[3406.3578341     0.2165        0.33270001    0.2721        0.20420001]
[37m[1m [4104.50918117    0.19700001    0.34650001    0.26250002    0.18679999]
[37m[1m [3773.47547952    0.19690001    0.3436        0.2402        0.17470001]
[37m[1m ...
[37m[1m [3920.93182299    0.18200001    0.35409999    0.28780001    0.18179999]
[37m[1m [3482.20638897    0.19840001    0.34819999    0.23050001    0.19230001]
[37m[1m [2268.9999113     0.16360001    0.36670002    0.31020001    0.1849    ]]
[37m[1m[2023-06-25 11:17:57,498][129146] Max Reward on eval: 4267.861198727833
[37m[1m[2023-06-25 11:17:57,499][129146] Min Reward on eval: 285.2439478129148
[37m[1m[2023-06-25 11:17:57,499][129146] Mean Reward across all agents: 3334.4933932594413
[37m[1m[2023-06-25 11:17:57,499][129146] Average Trajectory Length: 994.4386666666667
[36m[2023-06-25 11:17:57,502][129146] mean_value=-344.3812085148857, max_value=3135.529271509941
[37m[1m[2023-06-25 11:17:57,505][129146] New mean coefficients: [[ 1.1551644  -0.00906301  0.06292155  0.03574526 -0.05290113]]
[37m[1m[2023-06-25 11:17:57,506][129146] Moving the mean solution point...
[36m[2023-06-25 11:18:07,305][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 11:18:07,305][129146] FPS: 391956.85
[36m[2023-06-25 11:18:07,307][129146] itr=1126, itrs=2000, Progress: 56.30%
[36m[2023-06-25 11:18:18,869][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 11:18:18,869][129146] FPS: 332794.36
[36m[2023-06-25 11:18:23,724][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:18:23,725][129146] Reward + Measures: [[2829.99571743    0.17247136    0.36899585    0.31036699    0.20173024]]
[37m[1m[2023-06-25 11:18:23,725][129146] Max Reward on eval: 2829.99571742954
[37m[1m[2023-06-25 11:18:23,725][129146] Min Reward on eval: 2829.99571742954
[37m[1m[2023-06-25 11:18:23,725][129146] Mean Reward across all agents: 2829.99571742954
[37m[1m[2023-06-25 11:18:23,726][129146] Average Trajectory Length: 993.4436666666667
[36m[2023-06-25 11:18:29,392][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:18:29,392][129146] Reward + Measures: [[3412.82399638    0.20539999    0.40039998    0.27180001    0.2079    ]
[37m[1m [2338.70423652    0.21611634    0.41502109    0.21503606    0.23214082]
[37m[1m [3737.29674206    0.1935        0.41620001    0.2378        0.1973    ]
[37m[1m ...
[37m[1m [3025.33618303    0.21040002    0.42420003    0.23340002    0.22500001]
[37m[1m [2419.88589058    0.18350001    0.3849        0.30829999    0.23239999]
[37m[1m [1995.04946104    0.17820001    0.29930001    0.23080002    0.18320002]]
[37m[1m[2023-06-25 11:18:29,392][129146] Max Reward on eval: 3737.296742055565
[37m[1m[2023-06-25 11:18:29,393][129146] Min Reward on eval: 429.35241414989576
[37m[1m[2023-06-25 11:18:29,393][129146] Mean Reward across all agents: 2411.3754600853085
[37m[1m[2023-06-25 11:18:29,393][129146] Average Trajectory Length: 979.8026666666666
[36m[2023-06-25 11:18:29,395][129146] mean_value=-1186.264635645236, max_value=1465.044244012755
[37m[1m[2023-06-25 11:18:29,397][129146] New mean coefficients: [[ 1.1150241   0.0222433   0.12122036 -0.07253978 -0.22599396]]
[37m[1m[2023-06-25 11:18:29,398][129146] Moving the mean solution point...
[36m[2023-06-25 11:18:39,274][129146] train() took 9.87 seconds to complete
[36m[2023-06-25 11:18:39,274][129146] FPS: 388919.37
[36m[2023-06-25 11:18:39,276][129146] itr=1127, itrs=2000, Progress: 56.35%
[36m[2023-06-25 11:18:50,862][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 11:18:50,863][129146] FPS: 332095.38
[36m[2023-06-25 11:18:55,694][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:18:55,695][129146] Reward + Measures: [[3176.86039323    0.17261495    0.36856601    0.30500335    0.19619389]]
[37m[1m[2023-06-25 11:18:55,695][129146] Max Reward on eval: 3176.8603932325573
[37m[1m[2023-06-25 11:18:55,695][129146] Min Reward on eval: 3176.8603932325573
[37m[1m[2023-06-25 11:18:55,696][129146] Mean Reward across all agents: 3176.8603932325573
[37m[1m[2023-06-25 11:18:55,696][129146] Average Trajectory Length: 990.6846666666667
[36m[2023-06-25 11:19:01,207][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:19:01,208][129146] Reward + Measures: [[2829.51034299    0.18862365    0.356585      0.30550107    0.2042613 ]
[37m[1m [3316.18039988    0.17390001    0.39340001    0.29860002    0.20350002]
[37m[1m [2828.2546067     0.16059999    0.39420003    0.30000001    0.2079    ]
[37m[1m ...
[37m[1m [2632.02982405    0.1991        0.31530002    0.27470002    0.1969    ]
[37m[1m [3105.07092747    0.18670002    0.398         0.2782        0.20369999]
[37m[1m [2980.36528135    0.16818361    0.38271475    0.29229346    0.19985738]]
[37m[1m[2023-06-25 11:19:01,208][129146] Max Reward on eval: 3516.707951652934
[37m[1m[2023-06-25 11:19:01,209][129146] Min Reward on eval: 1316.9006928041867
[37m[1m[2023-06-25 11:19:01,209][129146] Mean Reward across all agents: 2705.0665789601667
[37m[1m[2023-06-25 11:19:01,209][129146] Average Trajectory Length: 979.5016666666667
[36m[2023-06-25 11:19:01,211][129146] mean_value=-988.3920513717167, max_value=1289.4198607955705
[37m[1m[2023-06-25 11:19:01,213][129146] New mean coefficients: [[ 1.0276318   0.0034212  -0.10236382 -0.18406439 -0.19446653]]
[37m[1m[2023-06-25 11:19:01,214][129146] Moving the mean solution point...
[36m[2023-06-25 11:19:10,899][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 11:19:10,899][129146] FPS: 396551.45
[36m[2023-06-25 11:19:10,901][129146] itr=1128, itrs=2000, Progress: 56.40%
[36m[2023-06-25 11:19:22,404][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 11:19:22,405][129146] FPS: 334492.00
[36m[2023-06-25 11:19:27,186][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:19:27,186][129146] Reward + Measures: [[2219.98771046    0.17346655    0.39083043    0.2263888     0.19714339]]
[37m[1m[2023-06-25 11:19:27,186][129146] Max Reward on eval: 2219.9877104607313
[37m[1m[2023-06-25 11:19:27,187][129146] Min Reward on eval: 2219.9877104607313
[37m[1m[2023-06-25 11:19:27,187][129146] Mean Reward across all agents: 2219.9877104607313
[37m[1m[2023-06-25 11:19:27,187][129146] Average Trajectory Length: 984.3053333333334
[36m[2023-06-25 11:19:32,707][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:19:32,713][129146] Reward + Measures: [[2008.92762512    0.17460001    0.37490001    0.2024        0.18169999]
[37m[1m [1221.68366077    0.18340002    0.3502        0.22180001    0.20829999]
[37m[1m [2220.76524394    0.19090001    0.42539999    0.20799999    0.2217    ]
[37m[1m ...
[37m[1m [1132.94810151    0.18840002    0.3204        0.1584        0.1753    ]
[37m[1m [2050.10218368    0.17304705    0.38893744    0.23140107    0.17971444]
[37m[1m [2378.81363697    0.1913        0.41359997    0.2349        0.20179999]]
[37m[1m[2023-06-25 11:19:32,713][129146] Max Reward on eval: 2511.264991512103
[37m[1m[2023-06-25 11:19:32,714][129146] Min Reward on eval: 73.0435601134377
[37m[1m[2023-06-25 11:19:32,714][129146] Mean Reward across all agents: 1835.9980645555968
[37m[1m[2023-06-25 11:19:32,714][129146] Average Trajectory Length: 987.8613333333333
[36m[2023-06-25 11:19:32,716][129146] mean_value=-1628.7323240992819, max_value=694.886529565656
[37m[1m[2023-06-25 11:19:32,718][129146] New mean coefficients: [[ 1.0652181  -0.00010707  0.10652343 -0.17393306 -0.24722983]]
[37m[1m[2023-06-25 11:19:32,719][129146] Moving the mean solution point...
[36m[2023-06-25 11:19:42,435][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 11:19:42,435][129146] FPS: 395316.67
[36m[2023-06-25 11:19:42,437][129146] itr=1129, itrs=2000, Progress: 56.45%
[36m[2023-06-25 11:19:53,855][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 11:19:53,855][129146] FPS: 336999.68
[36m[2023-06-25 11:19:58,659][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:19:58,660][129146] Reward + Measures: [[2386.86114478    0.17118986    0.39416364    0.22416118    0.1938221 ]]
[37m[1m[2023-06-25 11:19:58,660][129146] Max Reward on eval: 2386.861144784723
[37m[1m[2023-06-25 11:19:58,660][129146] Min Reward on eval: 2386.861144784723
[37m[1m[2023-06-25 11:19:58,661][129146] Mean Reward across all agents: 2386.861144784723
[37m[1m[2023-06-25 11:19:58,661][129146] Average Trajectory Length: 981.1783333333333
[36m[2023-06-25 11:20:04,122][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:20:04,122][129146] Reward + Measures: [[1868.48367504    0.16805561    0.38084087    0.21153079    0.17961006]
[37m[1m [1978.15440455    0.20280002    0.41370001    0.27599999    0.2545    ]
[37m[1m [2043.70541876    0.17559999    0.3405        0.2026        0.18520001]
[37m[1m ...
[37m[1m [1546.41610982    0.21619999    0.38210002    0.24570003    0.2588    ]
[37m[1m [2242.95633194    0.17522       0.38483998    0.21274002    0.18065999]
[37m[1m [2165.81091277    0.17          0.39330003    0.23150001    0.20290001]]
[37m[1m[2023-06-25 11:20:04,123][129146] Max Reward on eval: 2921.3731076394674
[37m[1m[2023-06-25 11:20:04,123][129146] Min Reward on eval: 798.2181058947929
[37m[1m[2023-06-25 11:20:04,123][129146] Mean Reward across all agents: 1944.014078858444
[37m[1m[2023-06-25 11:20:04,123][129146] Average Trajectory Length: 977.5613333333333
[36m[2023-06-25 11:20:04,125][129146] mean_value=-1508.7127844723616, max_value=991.5572691087905
[37m[1m[2023-06-25 11:20:04,127][129146] New mean coefficients: [[ 1.2656895  -0.09396122  0.12872364 -0.08231471 -0.01673366]]
[37m[1m[2023-06-25 11:20:04,128][129146] Moving the mean solution point...
[36m[2023-06-25 11:20:13,811][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 11:20:13,811][129146] FPS: 396654.93
[36m[2023-06-25 11:20:13,813][129146] itr=1130, itrs=2000, Progress: 56.50%
[37m[1m[2023-06-25 11:20:20,894][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001110
[36m[2023-06-25 11:20:32,523][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 11:20:32,523][129146] FPS: 336693.22
[36m[2023-06-25 11:20:37,272][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:20:37,273][129146] Reward + Measures: [[2609.78603211    0.17360565    0.40315095    0.22285202    0.19054654]]
[37m[1m[2023-06-25 11:20:37,273][129146] Max Reward on eval: 2609.7860321132644
[37m[1m[2023-06-25 11:20:37,273][129146] Min Reward on eval: 2609.7860321132644
[37m[1m[2023-06-25 11:20:37,273][129146] Mean Reward across all agents: 2609.7860321132644
[37m[1m[2023-06-25 11:20:37,274][129146] Average Trajectory Length: 987.0686666666667
[36m[2023-06-25 11:20:42,932][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:20:42,932][129146] Reward + Measures: [[2540.09193243    0.17933485    0.39873403    0.22697301    0.196661  ]
[37m[1m [1814.63137398    0.19557355    0.37562439    0.20542541    0.22230315]
[37m[1m [2302.78801121    0.18614791    0.41509667    0.2493        0.19711764]
[37m[1m ...
[37m[1m [2404.24572792    0.21080001    0.41630003    0.22119999    0.22430001]
[37m[1m [2609.44800754    0.17539148    0.42161489    0.24568725    0.20044892]
[37m[1m [2485.66172049    0.18180001    0.43779999    0.2474        0.19950001]]
[37m[1m[2023-06-25 11:20:42,932][129146] Max Reward on eval: 2833.847513479553
[37m[1m[2023-06-25 11:20:42,933][129146] Min Reward on eval: 1102.9477226140793
[37m[1m[2023-06-25 11:20:42,933][129146] Mean Reward across all agents: 2317.448488311712
[37m[1m[2023-06-25 11:20:42,933][129146] Average Trajectory Length: 977.689
[36m[2023-06-25 11:20:42,935][129146] mean_value=-1090.223123822945, max_value=197.79318547073444
[37m[1m[2023-06-25 11:20:42,937][129146] New mean coefficients: [[ 1.1349076   0.01893499  0.24552274 -0.04252539 -0.1540995 ]]
[37m[1m[2023-06-25 11:20:42,938][129146] Moving the mean solution point...
[36m[2023-06-25 11:20:52,703][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 11:20:52,703][129146] FPS: 393309.22
[36m[2023-06-25 11:20:52,705][129146] itr=1131, itrs=2000, Progress: 56.55%
[36m[2023-06-25 11:21:04,101][129146] train() took 11.37 seconds to complete
[36m[2023-06-25 11:21:04,102][129146] FPS: 337756.46
[36m[2023-06-25 11:21:08,871][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:21:08,871][129146] Reward + Measures: [[2816.3282006     0.17106579    0.40932062    0.21867308    0.18628515]]
[37m[1m[2023-06-25 11:21:08,872][129146] Max Reward on eval: 2816.3282006005807
[37m[1m[2023-06-25 11:21:08,872][129146] Min Reward on eval: 2816.3282006005807
[37m[1m[2023-06-25 11:21:08,872][129146] Mean Reward across all agents: 2816.3282006005807
[37m[1m[2023-06-25 11:21:08,872][129146] Average Trajectory Length: 987.4146666666667
[36m[2023-06-25 11:21:14,307][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:21:14,308][129146] Reward + Measures: [[2523.8392077     0.14659999    0.3479        0.2256        0.21250001]
[37m[1m [2814.38315602    0.19319999    0.39289999    0.1948        0.1921    ]
[37m[1m [2276.07093809    0.15010001    0.33239999    0.1894        0.16860001]
[37m[1m ...
[37m[1m [2194.61840548    0.1648        0.34690002    0.19509999    0.17770003]
[37m[1m [ 600.97654769    0.13829999    0.1904        0.14489999    0.1578    ]
[37m[1m [2328.11925568    0.1928        0.43520004    0.23510002    0.1988    ]]
[37m[1m[2023-06-25 11:21:14,308][129146] Max Reward on eval: 3136.709915572789
[37m[1m[2023-06-25 11:21:14,308][129146] Min Reward on eval: 600.9765476928849
[37m[1m[2023-06-25 11:21:14,309][129146] Mean Reward across all agents: 2396.5076956476596
[37m[1m[2023-06-25 11:21:14,309][129146] Average Trajectory Length: 977.4019999999999
[36m[2023-06-25 11:21:14,311][129146] mean_value=-929.0764494696384, max_value=696.6390756002215
[37m[1m[2023-06-25 11:21:14,313][129146] New mean coefficients: [[ 1.1484264  -0.10088347  0.02541955 -0.07079016 -0.1337098 ]]
[37m[1m[2023-06-25 11:21:14,314][129146] Moving the mean solution point...
[36m[2023-06-25 11:21:24,114][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 11:21:24,114][129146] FPS: 391926.03
[36m[2023-06-25 11:21:24,116][129146] itr=1132, itrs=2000, Progress: 56.60%
[36m[2023-06-25 11:21:35,637][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 11:21:35,637][129146] FPS: 333984.47
[36m[2023-06-25 11:21:40,333][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:21:40,333][129146] Reward + Measures: [[3039.74917041    0.17155151    0.41383544    0.21041727    0.17963846]]
[37m[1m[2023-06-25 11:21:40,334][129146] Max Reward on eval: 3039.749170406256
[37m[1m[2023-06-25 11:21:40,334][129146] Min Reward on eval: 3039.749170406256
[37m[1m[2023-06-25 11:21:40,334][129146] Mean Reward across all agents: 3039.749170406256
[37m[1m[2023-06-25 11:21:40,334][129146] Average Trajectory Length: 988.3689999999999
[36m[2023-06-25 11:21:45,787][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:21:45,788][129146] Reward + Measures: [[2473.68388769    0.1806        0.41680002    0.20050001    0.18359999]
[37m[1m [2823.61218708    0.18370001    0.41770002    0.22070001    0.20730002]
[37m[1m [2926.5057018     0.1724        0.4066        0.2184        0.17380001]
[37m[1m ...
[37m[1m [2253.71929621    0.20156775    0.41188389    0.20424195    0.18791614]
[37m[1m [2846.79593287    0.16509999    0.4183        0.22220002    0.19050001]
[37m[1m [3052.59805873    0.1769        0.41660005    0.22070001    0.18800001]]
[37m[1m[2023-06-25 11:21:45,788][129146] Max Reward on eval: 3375.0023329362275
[37m[1m[2023-06-25 11:21:45,788][129146] Min Reward on eval: 1391.207736293826
[37m[1m[2023-06-25 11:21:45,788][129146] Mean Reward across all agents: 2714.8396848892326
[37m[1m[2023-06-25 11:21:45,789][129146] Average Trajectory Length: 988.3136666666667
[36m[2023-06-25 11:21:45,791][129146] mean_value=-450.7330165062807, max_value=877.5411573572237
[37m[1m[2023-06-25 11:21:45,794][129146] New mean coefficients: [[ 0.9735805   0.11463712  0.20401846 -0.03033873 -0.20358923]]
[37m[1m[2023-06-25 11:21:45,795][129146] Moving the mean solution point...
[36m[2023-06-25 11:21:55,422][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 11:21:55,422][129146] FPS: 398934.59
[36m[2023-06-25 11:21:55,424][129146] itr=1133, itrs=2000, Progress: 56.65%
[36m[2023-06-25 11:22:06,882][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 11:22:06,883][129146] FPS: 335839.81
[36m[2023-06-25 11:22:11,628][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:22:11,628][129146] Reward + Measures: [[3266.15818201    0.17396669    0.41616952    0.21035723    0.17864767]]
[37m[1m[2023-06-25 11:22:11,629][129146] Max Reward on eval: 3266.158182009681
[37m[1m[2023-06-25 11:22:11,629][129146] Min Reward on eval: 3266.158182009681
[37m[1m[2023-06-25 11:22:11,629][129146] Mean Reward across all agents: 3266.158182009681
[37m[1m[2023-06-25 11:22:11,629][129146] Average Trajectory Length: 991.8486666666666
[36m[2023-06-25 11:22:17,086][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:22:17,086][129146] Reward + Measures: [[3238.86048423    0.17269546    0.40778753    0.21971934    0.18231373]
[37m[1m [2339.68667427    0.1858        0.42480001    0.26610002    0.17110001]
[37m[1m [2705.2756533     0.18370001    0.45140001    0.22839999    0.1772    ]
[37m[1m ...
[37m[1m [2934.5825415     0.18120001    0.42939997    0.23670001    0.1796    ]
[37m[1m [1624.92776402    0.16741428    0.36886427    0.19567858    0.17832142]
[37m[1m [2973.1106141     0.1833        0.43319997    0.23179999    0.18169999]]
[37m[1m[2023-06-25 11:22:17,086][129146] Max Reward on eval: 3510.428200669377
[37m[1m[2023-06-25 11:22:17,087][129146] Min Reward on eval: 484.5448855529015
[37m[1m[2023-06-25 11:22:17,087][129146] Mean Reward across all agents: 2693.0440891572334
[37m[1m[2023-06-25 11:22:17,087][129146] Average Trajectory Length: 968.8566666666667
[36m[2023-06-25 11:22:17,089][129146] mean_value=-800.7170837630597, max_value=560.6217312051017
[37m[1m[2023-06-25 11:22:17,092][129146] New mean coefficients: [[ 0.9972666   0.1027275   0.04373443 -0.13732554 -0.12099352]]
[37m[1m[2023-06-25 11:22:17,093][129146] Moving the mean solution point...
[36m[2023-06-25 11:22:26,855][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 11:22:26,855][129146] FPS: 393428.79
[36m[2023-06-25 11:22:26,857][129146] itr=1134, itrs=2000, Progress: 56.70%
[36m[2023-06-25 11:22:38,339][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 11:22:38,339][129146] FPS: 335128.36
[36m[2023-06-25 11:22:43,094][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:22:43,095][129146] Reward + Measures: [[3481.62458896    0.17520328    0.42358267    0.20527911    0.17515877]]
[37m[1m[2023-06-25 11:22:43,095][129146] Max Reward on eval: 3481.624588956767
[37m[1m[2023-06-25 11:22:43,095][129146] Min Reward on eval: 3481.624588956767
[37m[1m[2023-06-25 11:22:43,095][129146] Mean Reward across all agents: 3481.624588956767
[37m[1m[2023-06-25 11:22:43,096][129146] Average Trajectory Length: 995.9656666666666
[36m[2023-06-25 11:22:48,492][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:22:48,493][129146] Reward + Measures: [[3228.8585975     0.19270001    0.44439998    0.2027        0.1858    ]
[37m[1m [2512.30798146    0.20760003    0.46960002    0.21250001    0.2023    ]
[37m[1m [3441.60590671    0.18069999    0.42500001    0.19950001    0.18280001]
[37m[1m ...
[37m[1m [3356.86841025    0.1884        0.43999997    0.20840001    0.18450001]
[37m[1m [3566.98236765    0.17480002    0.41280004    0.19500001    0.1795    ]
[37m[1m [3116.81046367    0.1603        0.41630003    0.22449999    0.1706    ]]
[37m[1m[2023-06-25 11:22:48,493][129146] Max Reward on eval: 3630.7875938166862
[37m[1m[2023-06-25 11:22:48,493][129146] Min Reward on eval: 1456.1278613834525
[37m[1m[2023-06-25 11:22:48,494][129146] Mean Reward across all agents: 3049.7521884626694
[37m[1m[2023-06-25 11:22:48,494][129146] Average Trajectory Length: 990.9716666666666
[36m[2023-06-25 11:22:48,496][129146] mean_value=-319.49322035652256, max_value=385.99145779185665
[37m[1m[2023-06-25 11:22:48,499][129146] New mean coefficients: [[ 0.8312837   0.05479974 -0.04121874 -0.03678951 -0.07994937]]
[37m[1m[2023-06-25 11:22:48,500][129146] Moving the mean solution point...
[36m[2023-06-25 11:22:58,043][129146] train() took 9.54 seconds to complete
[36m[2023-06-25 11:22:58,043][129146] FPS: 402434.57
[36m[2023-06-25 11:22:58,046][129146] itr=1135, itrs=2000, Progress: 56.75%
[36m[2023-06-25 11:23:09,519][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 11:23:09,520][129146] FPS: 335481.84
[36m[2023-06-25 11:23:14,263][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:23:14,263][129146] Reward + Measures: [[3683.92002332    0.17480426    0.42215088    0.2000186     0.17339851]]
[37m[1m[2023-06-25 11:23:14,263][129146] Max Reward on eval: 3683.920023319101
[37m[1m[2023-06-25 11:23:14,264][129146] Min Reward on eval: 3683.920023319101
[37m[1m[2023-06-25 11:23:14,264][129146] Mean Reward across all agents: 3683.920023319101
[37m[1m[2023-06-25 11:23:14,264][129146] Average Trajectory Length: 994.444
[36m[2023-06-25 11:23:19,872][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:23:19,873][129146] Reward + Measures: [[3541.991215      0.19059999    0.44479999    0.20510001    0.1736    ]
[37m[1m [2147.77180271    0.19490001    0.43689999    0.25799999    0.17709999]
[37m[1m [2841.98155342    0.1779        0.39360005    0.1948        0.176     ]
[37m[1m ...
[37m[1m [3503.5790674     0.17270002    0.4278        0.2174        0.18239999]
[37m[1m [3303.31704554    0.17850001    0.4075        0.198         0.17690001]
[37m[1m [3368.93222415    0.1846        0.42820001    0.2172        0.17550002]]
[37m[1m[2023-06-25 11:23:19,873][129146] Max Reward on eval: 3777.7553759706907
[37m[1m[2023-06-25 11:23:19,874][129146] Min Reward on eval: 1048.8181529114518
[37m[1m[2023-06-25 11:23:19,874][129146] Mean Reward across all agents: 3085.8117441690088
[37m[1m[2023-06-25 11:23:19,874][129146] Average Trajectory Length: 988.1956666666666
[36m[2023-06-25 11:23:19,877][129146] mean_value=-457.80629108257574, max_value=442.57027903469316
[37m[1m[2023-06-25 11:23:19,879][129146] New mean coefficients: [[ 0.7159016   0.03184674  0.13260359 -0.01168505  0.01014403]]
[37m[1m[2023-06-25 11:23:19,880][129146] Moving the mean solution point...
[36m[2023-06-25 11:23:29,655][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 11:23:29,655][129146] FPS: 392922.39
[36m[2023-06-25 11:23:29,657][129146] itr=1136, itrs=2000, Progress: 56.80%
[36m[2023-06-25 11:23:41,117][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 11:23:41,117][129146] FPS: 335771.81
[36m[2023-06-25 11:23:45,859][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:23:45,859][129146] Reward + Measures: [[3920.73359902    0.17753346    0.42761084    0.19390625    0.17389859]]
[37m[1m[2023-06-25 11:23:45,860][129146] Max Reward on eval: 3920.7335990226143
[37m[1m[2023-06-25 11:23:45,860][129146] Min Reward on eval: 3920.7335990226143
[37m[1m[2023-06-25 11:23:45,860][129146] Mean Reward across all agents: 3920.7335990226143
[37m[1m[2023-06-25 11:23:45,860][129146] Average Trajectory Length: 997.202
[36m[2023-06-25 11:23:51,252][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:23:51,252][129146] Reward + Measures: [[3307.50993988    0.1798        0.42950001    0.19499999    0.18100001]
[37m[1m [3737.24427751    0.17949998    0.41820002    0.20190001    0.17550002]
[37m[1m [3946.4348675     0.18170001    0.43109998    0.1971        0.18129998]
[37m[1m ...
[37m[1m [3484.32114481    0.1721728     0.42422491    0.2029587     0.17372091]
[37m[1m [3590.1888041     0.177         0.40710002    0.20410001    0.1638    ]
[37m[1m [3899.90722242    0.17900001    0.43059999    0.1964        0.17569999]]
[37m[1m[2023-06-25 11:23:51,252][129146] Max Reward on eval: 4060.1508033467458
[37m[1m[2023-06-25 11:23:51,253][129146] Min Reward on eval: 1576.748598904279
[37m[1m[2023-06-25 11:23:51,253][129146] Mean Reward across all agents: 3615.073997177837
[37m[1m[2023-06-25 11:23:51,253][129146] Average Trajectory Length: 997.475
[36m[2023-06-25 11:23:51,257][129146] mean_value=23.80660224773704, max_value=442.5831723850206
[37m[1m[2023-06-25 11:23:51,260][129146] New mean coefficients: [[ 0.4739591   0.08477764  0.04738981 -0.13984104  0.05338506]]
[37m[1m[2023-06-25 11:23:51,261][129146] Moving the mean solution point...
[36m[2023-06-25 11:24:00,975][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 11:24:00,975][129146] FPS: 395373.47
[36m[2023-06-25 11:24:00,978][129146] itr=1137, itrs=2000, Progress: 56.85%
[36m[2023-06-25 11:24:12,610][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 11:24:12,610][129146] FPS: 330871.82
[36m[2023-06-25 11:24:17,422][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:24:17,422][129146] Reward + Measures: [[4053.04720477    0.17249648    0.42967457    0.19244747    0.17125607]]
[37m[1m[2023-06-25 11:24:17,422][129146] Max Reward on eval: 4053.047204770683
[37m[1m[2023-06-25 11:24:17,423][129146] Min Reward on eval: 4053.047204770683
[37m[1m[2023-06-25 11:24:17,423][129146] Mean Reward across all agents: 4053.047204770683
[37m[1m[2023-06-25 11:24:17,423][129146] Average Trajectory Length: 996.3929999999999
[36m[2023-06-25 11:24:22,930][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:24:22,936][129146] Reward + Measures: [[3928.93685031    0.1653        0.42069998    0.19630001    0.16790001]
[37m[1m [3235.46052104    0.15790497    0.40852806    0.23957692    0.16339381]
[37m[1m [3673.27456317    0.1619        0.42230001    0.20060001    0.1626    ]
[37m[1m ...
[37m[1m [3694.24764408    0.19430001    0.44700003    0.24980001    0.18260001]
[37m[1m [3760.77738625    0.17340001    0.41590005    0.2177        0.1778    ]
[37m[1m [3944.47664244    0.17470001    0.42280003    0.18929999    0.16869999]]
[37m[1m[2023-06-25 11:24:22,936][129146] Max Reward on eval: 4177.827370624431
[37m[1m[2023-06-25 11:24:22,936][129146] Min Reward on eval: 1379.317864890599
[37m[1m[2023-06-25 11:24:22,937][129146] Mean Reward across all agents: 3468.6502876777463
[37m[1m[2023-06-25 11:24:22,937][129146] Average Trajectory Length: 993.0253333333333
[36m[2023-06-25 11:24:22,939][129146] mean_value=-377.1268026169843, max_value=311.56463781077036
[37m[1m[2023-06-25 11:24:22,942][129146] New mean coefficients: [[ 0.38628706  0.0895351  -0.06423187 -0.13006108 -0.11222107]]
[37m[1m[2023-06-25 11:24:22,943][129146] Moving the mean solution point...
[36m[2023-06-25 11:24:32,640][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 11:24:32,640][129146] FPS: 396077.21
[36m[2023-06-25 11:24:32,643][129146] itr=1138, itrs=2000, Progress: 56.90%
[36m[2023-06-25 11:24:44,101][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 11:24:44,101][129146] FPS: 335903.13
[36m[2023-06-25 11:24:48,846][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:24:48,846][129146] Reward + Measures: [[4192.239718      0.1743741     0.41770723    0.18883361    0.17355947]]
[37m[1m[2023-06-25 11:24:48,846][129146] Max Reward on eval: 4192.239717999149
[37m[1m[2023-06-25 11:24:48,846][129146] Min Reward on eval: 4192.239717999149
[37m[1m[2023-06-25 11:24:48,847][129146] Mean Reward across all agents: 4192.239717999149
[37m[1m[2023-06-25 11:24:48,847][129146] Average Trajectory Length: 998.135
[36m[2023-06-25 11:24:54,204][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:24:54,205][129146] Reward + Measures: [[3315.69968514    0.16040002    0.36450002    0.19509999    0.19380002]
[37m[1m [1843.50307532    0.14476065    0.30668363    0.18927869    0.17802459]
[37m[1m [4077.81848546    0.1772        0.41420004    0.19899999    0.17730001]
[37m[1m ...
[37m[1m [3997.53771602    0.1689        0.419         0.19510001    0.17040001]
[37m[1m [4051.64618602    0.1832        0.39330003    0.20130001    0.18790001]
[37m[1m [3820.8328915     0.17766336    0.42424607    0.1985576     0.18301049]]
[37m[1m[2023-06-25 11:24:54,205][129146] Max Reward on eval: 4310.621046217531
[37m[1m[2023-06-25 11:24:54,205][129146] Min Reward on eval: 1843.5030753217172
[37m[1m[2023-06-25 11:24:54,206][129146] Mean Reward across all agents: 3699.0889322260364
[37m[1m[2023-06-25 11:24:54,206][129146] Average Trajectory Length: 988.3103333333333
[36m[2023-06-25 11:24:54,209][129146] mean_value=-261.1374396727494, max_value=325.9418990770582
[37m[1m[2023-06-25 11:24:54,211][129146] New mean coefficients: [[ 0.28617907  0.16636357  0.00166458 -0.07645212 -0.09392153]]
[37m[1m[2023-06-25 11:24:54,212][129146] Moving the mean solution point...
[36m[2023-06-25 11:25:03,818][129146] train() took 9.60 seconds to complete
[36m[2023-06-25 11:25:03,818][129146] FPS: 399840.46
[36m[2023-06-25 11:25:03,820][129146] itr=1139, itrs=2000, Progress: 56.95%
[36m[2023-06-25 11:25:15,272][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 11:25:15,272][129146] FPS: 336050.54
[36m[2023-06-25 11:25:20,038][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:25:20,039][129146] Reward + Measures: [[4291.99016302    0.17601223    0.41578764    0.18488505    0.17341764]]
[37m[1m[2023-06-25 11:25:20,039][129146] Max Reward on eval: 4291.990163016354
[37m[1m[2023-06-25 11:25:20,039][129146] Min Reward on eval: 4291.990163016354
[37m[1m[2023-06-25 11:25:20,039][129146] Mean Reward across all agents: 4291.990163016354
[37m[1m[2023-06-25 11:25:20,039][129146] Average Trajectory Length: 997.799
[36m[2023-06-25 11:25:25,333][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:25:25,338][129146] Reward + Measures: [[3739.57865344    0.18208987    0.41695219    0.18761595    0.18124059]
[37m[1m [3243.59128504    0.1521        0.39820001    0.22830001    0.17670001]
[37m[1m [4114.96962532    0.18239999    0.41480002    0.18770002    0.1796    ]
[37m[1m ...
[37m[1m [4240.27482767    0.17741752    0.42464295    0.17931823    0.17271681]
[37m[1m [4102.3365562     0.1883        0.41870004    0.18460001    0.17920001]
[37m[1m [3701.13724602    0.16510001    0.41330001    0.1901        0.17259999]]
[37m[1m[2023-06-25 11:25:25,339][129146] Max Reward on eval: 4343.938977561146
[37m[1m[2023-06-25 11:25:25,339][129146] Min Reward on eval: 1994.2456572359747
[37m[1m[2023-06-25 11:25:25,339][129146] Mean Reward across all agents: 3731.8298255304503
[37m[1m[2023-06-25 11:25:25,339][129146] Average Trajectory Length: 991.4406666666666
[36m[2023-06-25 11:25:25,342][129146] mean_value=-337.9940185533251, max_value=299.9742433010251
[37m[1m[2023-06-25 11:25:25,344][129146] New mean coefficients: [[ 0.2957643   0.04044989 -0.02405736 -0.0340135  -0.13014661]]
[37m[1m[2023-06-25 11:25:25,345][129146] Moving the mean solution point...
[36m[2023-06-25 11:25:34,967][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 11:25:34,967][129146] FPS: 399159.18
[36m[2023-06-25 11:25:34,969][129146] itr=1140, itrs=2000, Progress: 57.00%
[37m[1m[2023-06-25 11:25:42,461][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001120
[36m[2023-06-25 11:25:54,492][129146] train() took 11.81 seconds to complete
[36m[2023-06-25 11:25:54,493][129146] FPS: 325119.68
[36m[2023-06-25 11:25:59,213][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:25:59,213][129146] Reward + Measures: [[4369.61527035    0.17691487    0.4154956     0.18509428    0.1730364 ]]
[37m[1m[2023-06-25 11:25:59,213][129146] Max Reward on eval: 4369.615270349047
[37m[1m[2023-06-25 11:25:59,214][129146] Min Reward on eval: 4369.615270349047
[37m[1m[2023-06-25 11:25:59,214][129146] Mean Reward across all agents: 4369.615270349047
[37m[1m[2023-06-25 11:25:59,214][129146] Average Trajectory Length: 998.3556666666666
[36m[2023-06-25 11:26:04,543][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:26:04,603][129146] Reward + Measures: [[4132.59769444    0.16790001    0.41850001    0.19329999    0.1637    ]
[37m[1m [4082.23737473    0.17490001    0.41339999    0.1875        0.1631    ]
[37m[1m [4242.5180979     0.17709999    0.41999999    0.1868        0.17289999]
[37m[1m ...
[37m[1m [4222.59824286    0.17400001    0.4165        0.18780001    0.1707    ]
[37m[1m [3864.92356301    0.17419998    0.41019997    0.20640002    0.18710001]
[37m[1m [3611.51370639    0.1602        0.40110001    0.1927        0.1698    ]]
[37m[1m[2023-06-25 11:26:04,604][129146] Max Reward on eval: 4437.961566860508
[37m[1m[2023-06-25 11:26:04,604][129146] Min Reward on eval: 1948.2282471682906
[37m[1m[2023-06-25 11:26:04,604][129146] Mean Reward across all agents: 3830.561778511596
[37m[1m[2023-06-25 11:26:04,604][129146] Average Trajectory Length: 982.56
[36m[2023-06-25 11:26:04,606][129146] mean_value=-353.13528767913454, max_value=753.749819849166
[37m[1m[2023-06-25 11:26:04,609][129146] New mean coefficients: [[ 0.265046   -0.03185999 -0.05922877  0.00900882 -0.14987007]]
[37m[1m[2023-06-25 11:26:04,610][129146] Moving the mean solution point...
[36m[2023-06-25 11:26:14,353][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 11:26:14,353][129146] FPS: 394180.52
[36m[2023-06-25 11:26:14,356][129146] itr=1141, itrs=2000, Progress: 57.05%
[36m[2023-06-25 11:26:25,939][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 11:26:25,940][129146] FPS: 332271.65
[36m[2023-06-25 11:26:30,691][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:26:30,691][129146] Reward + Measures: [[4528.35176952    0.17609678    0.4240315     0.18307015    0.17126365]]
[37m[1m[2023-06-25 11:26:30,692][129146] Max Reward on eval: 4528.351769519919
[37m[1m[2023-06-25 11:26:30,692][129146] Min Reward on eval: 4528.351769519919
[37m[1m[2023-06-25 11:26:30,692][129146] Mean Reward across all agents: 4528.351769519919
[37m[1m[2023-06-25 11:26:30,692][129146] Average Trajectory Length: 998.139
[36m[2023-06-25 11:26:36,079][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:26:36,079][129146] Reward + Measures: [[3995.56548392    0.1851        0.41490003    0.20910001    0.17760001]
[37m[1m [4244.85164546    0.18501313    0.44108602    0.19645393    0.17270963]
[37m[1m [4346.91474966    0.1688        0.44709998    0.1908        0.17150001]
[37m[1m ...
[37m[1m [4198.19057446    0.1681        0.39429998    0.17709999    0.168     ]
[37m[1m [4537.54619812    0.1689        0.43350002    0.18060002    0.1672    ]
[37m[1m [3686.6750549     0.17380001    0.43109998    0.19630001    0.17639999]]
[37m[1m[2023-06-25 11:26:36,080][129146] Max Reward on eval: 4620.579630886414
[37m[1m[2023-06-25 11:26:36,080][129146] Min Reward on eval: 3171.453171858587
[37m[1m[2023-06-25 11:26:36,080][129146] Mean Reward across all agents: 4199.925757980538
[37m[1m[2023-06-25 11:26:36,080][129146] Average Trajectory Length: 998.0293333333333
[36m[2023-06-25 11:26:36,084][129146] mean_value=-39.05547450737404, max_value=3526.80108788314
[37m[1m[2023-06-25 11:26:36,086][129146] New mean coefficients: [[ 0.19386008 -0.02407563 -0.11768438  0.00119349 -0.07298499]]
[37m[1m[2023-06-25 11:26:36,087][129146] Moving the mean solution point...
[36m[2023-06-25 11:26:45,723][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 11:26:45,723][129146] FPS: 398601.55
[36m[2023-06-25 11:26:45,725][129146] itr=1142, itrs=2000, Progress: 57.10%
[36m[2023-06-25 11:26:57,169][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 11:26:57,169][129146] FPS: 336340.87
[36m[2023-06-25 11:27:01,932][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:27:01,933][129146] Reward + Measures: [[4597.78738625    0.17192747    0.42945573    0.1825968     0.16911951]]
[37m[1m[2023-06-25 11:27:01,933][129146] Max Reward on eval: 4597.787386251904
[37m[1m[2023-06-25 11:27:01,933][129146] Min Reward on eval: 4597.787386251904
[37m[1m[2023-06-25 11:27:01,933][129146] Mean Reward across all agents: 4597.787386251904
[37m[1m[2023-06-25 11:27:01,934][129146] Average Trajectory Length: 997.8756666666667
[36m[2023-06-25 11:27:07,320][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:27:07,321][129146] Reward + Measures: [[4295.85587964    0.1837        0.41459998    0.17980002    0.17629997]
[37m[1m [3853.69318896    0.18889999    0.4461        0.1927        0.1829    ]
[37m[1m [4534.66228554    0.17730001    0.42560002    0.18350001    0.17379999]
[37m[1m ...
[37m[1m [4304.59250226    0.1646        0.43470001    0.20820001    0.176     ]
[37m[1m [4459.21372219    0.18740001    0.43250003    0.17879999    0.1752    ]
[37m[1m [4415.99755309    0.17900001    0.42829999    0.18260001    0.178     ]]
[37m[1m[2023-06-25 11:27:07,321][129146] Max Reward on eval: 4679.289953513444
[37m[1m[2023-06-25 11:27:07,321][129146] Min Reward on eval: 2509.6299180216447
[37m[1m[2023-06-25 11:27:07,322][129146] Mean Reward across all agents: 4042.1424772109262
[37m[1m[2023-06-25 11:27:07,322][129146] Average Trajectory Length: 996.7076666666667
[36m[2023-06-25 11:27:07,324][129146] mean_value=-343.0237810355355, max_value=447.81928607818645
[37m[1m[2023-06-25 11:27:07,326][129146] New mean coefficients: [[ 0.18686303  0.04296006 -0.0269905   0.08298117 -0.07885757]]
[37m[1m[2023-06-25 11:27:07,327][129146] Moving the mean solution point...
[36m[2023-06-25 11:27:16,930][129146] train() took 9.60 seconds to complete
[36m[2023-06-25 11:27:16,930][129146] FPS: 399951.62
[36m[2023-06-25 11:27:16,933][129146] itr=1143, itrs=2000, Progress: 57.15%
[36m[2023-06-25 11:27:28,541][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 11:27:28,541][129146] FPS: 331474.05
[36m[2023-06-25 11:27:33,398][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:27:33,399][129146] Reward + Measures: [[4662.43519393    0.17538084    0.42327082    0.19002038    0.17033352]]
[37m[1m[2023-06-25 11:27:33,399][129146] Max Reward on eval: 4662.435193934528
[37m[1m[2023-06-25 11:27:33,399][129146] Min Reward on eval: 4662.435193934528
[37m[1m[2023-06-25 11:27:33,400][129146] Mean Reward across all agents: 4662.435193934528
[37m[1m[2023-06-25 11:27:33,400][129146] Average Trajectory Length: 998.6626666666666
[36m[2023-06-25 11:27:38,807][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:27:38,807][129146] Reward + Measures: [[4309.42478656    0.18139999    0.41370001    0.1786        0.18799999]
[37m[1m [4192.15685473    0.1627        0.48079997    0.19039999    0.17230001]
[37m[1m [4204.45372716    0.16567761    0.44862986    0.19307613    0.16713731]
[37m[1m ...
[37m[1m [4585.16819244    0.17300001    0.41350004    0.1996        0.16880001]
[37m[1m [4654.2914365     0.17379999    0.41070005    0.18959999    0.1753    ]
[37m[1m [4071.31593638    0.17424677    0.40424606    0.19544388    0.17850505]]
[37m[1m[2023-06-25 11:27:38,808][129146] Max Reward on eval: 4745.2060370927675
[37m[1m[2023-06-25 11:27:38,808][129146] Min Reward on eval: 3074.934430165334
[37m[1m[2023-06-25 11:27:38,808][129146] Mean Reward across all agents: 4367.802233767536
[37m[1m[2023-06-25 11:27:38,808][129146] Average Trajectory Length: 996.942
[36m[2023-06-25 11:27:38,811][129146] mean_value=-141.3469527073488, max_value=735.2485305022401
[37m[1m[2023-06-25 11:27:38,814][129146] New mean coefficients: [[ 0.33473444  0.0383782   0.06531315  0.11176988 -0.09376059]]
[37m[1m[2023-06-25 11:27:38,815][129146] Moving the mean solution point...
[36m[2023-06-25 11:27:48,388][129146] train() took 9.57 seconds to complete
[36m[2023-06-25 11:27:48,389][129146] FPS: 401187.94
[36m[2023-06-25 11:27:48,391][129146] itr=1144, itrs=2000, Progress: 57.20%
[36m[2023-06-25 11:27:59,834][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 11:27:59,835][129146] FPS: 336353.83
[36m[2023-06-25 11:28:04,575][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:28:04,576][129146] Reward + Measures: [[3505.42215401    0.21021537    0.39878678    0.18664834    0.17797311]]
[37m[1m[2023-06-25 11:28:04,576][129146] Max Reward on eval: 3505.422154014446
[37m[1m[2023-06-25 11:28:04,576][129146] Min Reward on eval: 3505.422154014446
[37m[1m[2023-06-25 11:28:04,576][129146] Mean Reward across all agents: 3505.422154014446
[37m[1m[2023-06-25 11:28:04,577][129146] Average Trajectory Length: 985.193
[36m[2023-06-25 11:28:09,952][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:28:09,952][129146] Reward + Measures: [[3681.30724483    0.2115        0.39630005    0.1904        0.1779    ]
[37m[1m [3671.08037792    0.20720001    0.39839998    0.18540001    0.18139999]
[37m[1m [3835.98771518    0.204         0.39179999    0.1815        0.17999999]
[37m[1m ...
[37m[1m [3413.96517457    0.20550001    0.40799999    0.1902        0.16980001]
[37m[1m [3418.15733038    0.21360002    0.40910003    0.1892        0.18200001]
[37m[1m [3569.60456222    0.21110001    0.42829999    0.1908        0.17839999]]
[37m[1m[2023-06-25 11:28:09,953][129146] Max Reward on eval: 3970.4777155014917
[37m[1m[2023-06-25 11:28:09,953][129146] Min Reward on eval: 873.2954640611075
[37m[1m[2023-06-25 11:28:09,953][129146] Mean Reward across all agents: 3380.4513454359426
[37m[1m[2023-06-25 11:28:09,953][129146] Average Trajectory Length: 978.432
[36m[2023-06-25 11:28:09,955][129146] mean_value=-765.5077404333502, max_value=844.8081928498254
[37m[1m[2023-06-25 11:28:09,958][129146] New mean coefficients: [[ 0.26620048  0.01259719  0.10454381  0.14002551 -0.22626892]]
[37m[1m[2023-06-25 11:28:09,959][129146] Moving the mean solution point...
[36m[2023-06-25 11:28:19,555][129146] train() took 9.59 seconds to complete
[36m[2023-06-25 11:28:19,555][129146] FPS: 400222.94
[36m[2023-06-25 11:28:19,558][129146] itr=1145, itrs=2000, Progress: 57.25%
[36m[2023-06-25 11:28:31,023][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 11:28:31,023][129146] FPS: 335633.92
[36m[2023-06-25 11:28:35,778][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:28:35,779][129146] Reward + Measures: [[3758.35005306    0.20793848    0.40393233    0.18826613    0.17803878]]
[37m[1m[2023-06-25 11:28:35,779][129146] Max Reward on eval: 3758.3500530558254
[37m[1m[2023-06-25 11:28:35,779][129146] Min Reward on eval: 3758.3500530558254
[37m[1m[2023-06-25 11:28:35,780][129146] Mean Reward across all agents: 3758.3500530558254
[37m[1m[2023-06-25 11:28:35,780][129146] Average Trajectory Length: 992.0036666666666
[36m[2023-06-25 11:28:41,358][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:28:41,359][129146] Reward + Measures: [[2909.66349731    0.19310001    0.37583336    0.17336668    0.19286668]
[37m[1m [3305.20915138    0.2105        0.4357        0.21070002    0.1769    ]
[37m[1m [2611.63752039    0.21000002    0.42660004    0.23450001    0.1815    ]
[37m[1m ...
[37m[1m [3705.09821549    0.2027        0.41389999    0.18889999    0.17569999]
[37m[1m [3712.64545278    0.20760003    0.38260001    0.19360001    0.1796    ]
[37m[1m [3183.45720965    0.19304444    0.38755557    0.17679723    0.17338613]]
[37m[1m[2023-06-25 11:28:41,359][129146] Max Reward on eval: 3928.6721093716565
[37m[1m[2023-06-25 11:28:41,359][129146] Min Reward on eval: 1908.907533356297
[37m[1m[2023-06-25 11:28:41,360][129146] Mean Reward across all agents: 3279.100699035588
[37m[1m[2023-06-25 11:28:41,360][129146] Average Trajectory Length: 979.5583333333333
[36m[2023-06-25 11:28:41,361][129146] mean_value=-1057.2190409617683, max_value=347.2117572174575
[37m[1m[2023-06-25 11:28:41,369][129146] New mean coefficients: [[ 0.28350744 -0.06477939 -0.01688845  0.05370869 -0.1329609 ]]
[37m[1m[2023-06-25 11:28:41,370][129146] Moving the mean solution point...
[36m[2023-06-25 11:28:51,074][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 11:28:51,074][129146] FPS: 395788.29
[36m[2023-06-25 11:28:51,076][129146] itr=1146, itrs=2000, Progress: 57.30%
[36m[2023-06-25 11:29:02,561][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 11:29:02,562][129146] FPS: 335118.39
[36m[2023-06-25 11:29:07,278][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:29:07,278][129146] Reward + Measures: [[4005.01711292    0.20738499    0.40104219    0.18907173    0.17982796]]
[37m[1m[2023-06-25 11:29:07,278][129146] Max Reward on eval: 4005.0171129217297
[37m[1m[2023-06-25 11:29:07,279][129146] Min Reward on eval: 4005.0171129217297
[37m[1m[2023-06-25 11:29:07,279][129146] Mean Reward across all agents: 4005.0171129217297
[37m[1m[2023-06-25 11:29:07,279][129146] Average Trajectory Length: 995.6943333333332
[36m[2023-06-25 11:29:12,678][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:29:12,679][129146] Reward + Measures: [[3729.95992692    0.22420001    0.4217        0.18810001    0.18179999]
[37m[1m [3711.19991539    0.2113        0.4068        0.20299999    0.1815    ]
[37m[1m [2749.15612585    0.17816375    0.33771819    0.14607942    0.16371576]
[37m[1m ...
[37m[1m [3915.79563476    0.21090002    0.40840003    0.19329999    0.18069999]
[37m[1m [3977.60392463    0.2124        0.41839996    0.1901        0.18099999]
[37m[1m [4127.48645477    0.20030001    0.40640002    0.1795        0.18450001]]
[37m[1m[2023-06-25 11:29:12,679][129146] Max Reward on eval: 4213.8346651932225
[37m[1m[2023-06-25 11:29:12,680][129146] Min Reward on eval: 2278.928499850602
[37m[1m[2023-06-25 11:29:12,680][129146] Mean Reward across all agents: 3769.2310781638153
[37m[1m[2023-06-25 11:29:12,680][129146] Average Trajectory Length: 993.8953333333333
[36m[2023-06-25 11:29:12,681][129146] mean_value=-547.1843627102957, max_value=429.02617838127617
[37m[1m[2023-06-25 11:29:12,684][129146] New mean coefficients: [[ 0.17488575  0.0117638  -0.00246468 -0.0358831  -0.20965387]]
[37m[1m[2023-06-25 11:29:12,685][129146] Moving the mean solution point...
[36m[2023-06-25 11:29:22,393][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 11:29:22,393][129146] FPS: 395599.97
[36m[2023-06-25 11:29:22,396][129146] itr=1147, itrs=2000, Progress: 57.35%
[36m[2023-06-25 11:29:33,921][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 11:29:33,921][129146] FPS: 333952.95
[36m[2023-06-25 11:29:38,698][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:29:38,699][129146] Reward + Measures: [[4232.17222265    0.20714118    0.39772755    0.18914779    0.18126485]]
[37m[1m[2023-06-25 11:29:38,699][129146] Max Reward on eval: 4232.172222650174
[37m[1m[2023-06-25 11:29:38,699][129146] Min Reward on eval: 4232.172222650174
[37m[1m[2023-06-25 11:29:38,700][129146] Mean Reward across all agents: 4232.172222650174
[37m[1m[2023-06-25 11:29:38,700][129146] Average Trajectory Length: 996.361
[36m[2023-06-25 11:29:44,186][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:29:44,186][129146] Reward + Measures: [[3245.0744171     0.25330001    0.39799997    0.1745        0.21730001]
[37m[1m [3857.73866893    0.18090001    0.38139996    0.1816        0.16990001]
[37m[1m [3162.24833864    0.2119        0.37020001    0.17599998    0.21140002]
[37m[1m ...
[37m[1m [3791.78644374    0.18112989    0.40343189    0.19162481    0.17711599]
[37m[1m [2189.58279       0.24890061    0.3196573     0.14482787    0.19082384]
[37m[1m [4064.73259861    0.21469998    0.40300003    0.18429999    0.1806    ]]
[37m[1m[2023-06-25 11:29:44,187][129146] Max Reward on eval: 4495.861791196396
[37m[1m[2023-06-25 11:29:44,187][129146] Min Reward on eval: 187.64827548280738
[37m[1m[2023-06-25 11:29:44,187][129146] Mean Reward across all agents: 3706.9323134525926
[37m[1m[2023-06-25 11:29:44,187][129146] Average Trajectory Length: 991.8266666666666
[36m[2023-06-25 11:29:44,189][129146] mean_value=-471.7691556846362, max_value=648.7335684634409
[37m[1m[2023-06-25 11:29:44,192][129146] New mean coefficients: [[ 0.18149494 -0.0278415   0.00610571  0.01090822 -0.13275653]]
[37m[1m[2023-06-25 11:29:44,193][129146] Moving the mean solution point...
[36m[2023-06-25 11:29:53,929][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 11:29:53,929][129146] FPS: 394491.13
[36m[2023-06-25 11:29:53,931][129146] itr=1148, itrs=2000, Progress: 57.40%
[36m[2023-06-25 11:30:05,451][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 11:30:05,452][129146] FPS: 334104.45
[36m[2023-06-25 11:30:10,225][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:30:10,225][129146] Reward + Measures: [[4442.09134554    0.20099138    0.39491186    0.18724035    0.1793641 ]]
[37m[1m[2023-06-25 11:30:10,225][129146] Max Reward on eval: 4442.091345544728
[37m[1m[2023-06-25 11:30:10,226][129146] Min Reward on eval: 4442.091345544728
[37m[1m[2023-06-25 11:30:10,226][129146] Mean Reward across all agents: 4442.091345544728
[37m[1m[2023-06-25 11:30:10,226][129146] Average Trajectory Length: 998.2729999999999
[36m[2023-06-25 11:30:15,581][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:30:15,582][129146] Reward + Measures: [[4515.86794563    0.21430002    0.40629998    0.1867        0.18210001]
[37m[1m [4317.88780904    0.2106        0.3973        0.185         0.17830001]
[37m[1m [4325.66904654    0.1895        0.39850003    0.19020002    0.17920002]
[37m[1m ...
[37m[1m [3544.2614968     0.23340002    0.39559996    0.17740001    0.18100001]
[37m[1m [3989.17934098    0.20829999    0.3838        0.1869        0.17560001]
[37m[1m [4608.84531697    0.19369999    0.40599999    0.18139999    0.1793    ]]
[37m[1m[2023-06-25 11:30:15,582][129146] Max Reward on eval: 4649.17588005194
[37m[1m[2023-06-25 11:30:15,582][129146] Min Reward on eval: 1097.15832108077
[37m[1m[2023-06-25 11:30:15,582][129146] Mean Reward across all agents: 4059.378491120438
[37m[1m[2023-06-25 11:30:15,583][129146] Average Trajectory Length: 987.4423333333333
[36m[2023-06-25 11:30:15,585][129146] mean_value=-308.9450398261743, max_value=1190.5377618461453
[37m[1m[2023-06-25 11:30:15,588][129146] New mean coefficients: [[ 0.39017165 -0.10994542 -0.00688478  0.11168565  0.00812797]]
[37m[1m[2023-06-25 11:30:15,589][129146] Moving the mean solution point...
[36m[2023-06-25 11:30:25,276][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 11:30:25,276][129146] FPS: 396467.14
[36m[2023-06-25 11:30:25,278][129146] itr=1149, itrs=2000, Progress: 57.45%
[36m[2023-06-25 11:30:36,859][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 11:30:36,859][129146] FPS: 332268.86
[36m[2023-06-25 11:30:41,615][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:30:41,616][129146] Reward + Measures: [[4605.98105695    0.1960936     0.39749691    0.18636717    0.17816298]]
[37m[1m[2023-06-25 11:30:41,616][129146] Max Reward on eval: 4605.981056952926
[37m[1m[2023-06-25 11:30:41,616][129146] Min Reward on eval: 4605.981056952926
[37m[1m[2023-06-25 11:30:41,617][129146] Mean Reward across all agents: 4605.981056952926
[37m[1m[2023-06-25 11:30:41,617][129146] Average Trajectory Length: 997.3833333333333
[36m[2023-06-25 11:30:47,095][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:30:47,096][129146] Reward + Measures: [[4472.27401119    0.19960001    0.40879998    0.19070001    0.17910002]
[37m[1m [4061.95909265    0.19679277    0.41332754    0.18543914    0.18988697]
[37m[1m [4468.96218209    0.19890001    0.4127        0.2033        0.1734    ]
[37m[1m ...
[37m[1m [4614.01103202    0.1909        0.41030002    0.1883        0.1727    ]
[37m[1m [4079.89777295    0.19394286    0.3997286     0.19772856    0.17614286]
[37m[1m [4321.46109846    0.19079672    0.40963116    0.19053607    0.18041311]]
[37m[1m[2023-06-25 11:30:47,096][129146] Max Reward on eval: 4702.76963026044
[37m[1m[2023-06-25 11:30:47,096][129146] Min Reward on eval: 2099.8745479330537
[37m[1m[2023-06-25 11:30:47,096][129146] Mean Reward across all agents: 4216.727063216072
[37m[1m[2023-06-25 11:30:47,097][129146] Average Trajectory Length: 994.7056666666666
[36m[2023-06-25 11:30:47,099][129146] mean_value=-267.52583849175295, max_value=298.328429546395
[37m[1m[2023-06-25 11:30:47,102][129146] New mean coefficients: [[ 0.15897532 -0.05562386 -0.00574933  0.00341137 -0.06709368]]
[37m[1m[2023-06-25 11:30:47,103][129146] Moving the mean solution point...
[36m[2023-06-25 11:30:56,904][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 11:30:56,904][129146] FPS: 391836.85
[36m[2023-06-25 11:30:56,907][129146] itr=1150, itrs=2000, Progress: 57.50%
[37m[1m[2023-06-25 11:31:04,493][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001130
[36m[2023-06-25 11:31:16,296][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 11:31:16,296][129146] FPS: 331675.38
[36m[2023-06-25 11:31:21,055][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:31:21,055][129146] Reward + Measures: [[4262.85129911    0.18463309    0.37728092    0.19304556    0.17977563]]
[37m[1m[2023-06-25 11:31:21,055][129146] Max Reward on eval: 4262.851299106148
[37m[1m[2023-06-25 11:31:21,055][129146] Min Reward on eval: 4262.851299106148
[37m[1m[2023-06-25 11:31:21,056][129146] Mean Reward across all agents: 4262.851299106148
[37m[1m[2023-06-25 11:31:21,056][129146] Average Trajectory Length: 986.5836666666667
[36m[2023-06-25 11:31:26,483][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:31:26,484][129146] Reward + Measures: [[3532.55883335    0.18601052    0.34359342    0.17933819    0.16334459]
[37m[1m [4345.49174943    0.18830001    0.3856        0.19589999    0.1842    ]
[37m[1m [3702.86922878    0.19474795    0.37229866    0.21046029    0.1902055 ]
[37m[1m ...
[37m[1m [3038.71489683    0.17848022    0.33908021    0.1927347     0.18874861]
[37m[1m [3954.89225883    0.17489998    0.37360001    0.2026        0.1831    ]
[37m[1m [3873.25288607    0.18450771    0.3646231     0.18811537    0.18122308]]
[37m[1m[2023-06-25 11:31:26,484][129146] Max Reward on eval: 4493.085922021419
[37m[1m[2023-06-25 11:31:26,484][129146] Min Reward on eval: 2194.759459240653
[37m[1m[2023-06-25 11:31:26,484][129146] Mean Reward across all agents: 3980.3080429020447
[37m[1m[2023-06-25 11:31:26,485][129146] Average Trajectory Length: 970.141
[36m[2023-06-25 11:31:26,486][129146] mean_value=-556.9246091495249, max_value=584.7295498396516
[37m[1m[2023-06-25 11:31:26,489][129146] New mean coefficients: [[ 0.19356196 -0.03573025  0.0354932   0.00036529  0.01082791]]
[37m[1m[2023-06-25 11:31:26,490][129146] Moving the mean solution point...
[36m[2023-06-25 11:31:36,288][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 11:31:36,288][129146] FPS: 391983.57
[36m[2023-06-25 11:31:36,290][129146] itr=1151, itrs=2000, Progress: 57.55%
[36m[2023-06-25 11:31:47,884][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 11:31:47,885][129146] FPS: 331970.68
[36m[2023-06-25 11:31:52,695][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:31:52,696][129146] Reward + Measures: [[4456.67928653    0.18401672    0.37709072    0.19121623    0.17773522]]
[37m[1m[2023-06-25 11:31:52,696][129146] Max Reward on eval: 4456.679286532572
[37m[1m[2023-06-25 11:31:52,696][129146] Min Reward on eval: 4456.679286532572
[37m[1m[2023-06-25 11:31:52,696][129146] Mean Reward across all agents: 4456.679286532572
[37m[1m[2023-06-25 11:31:52,697][129146] Average Trajectory Length: 987.933
[36m[2023-06-25 11:31:58,120][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:31:58,121][129146] Reward + Measures: [[4127.63137901    0.18530564    0.39204082    0.20501409    0.18615775]
[37m[1m [4504.57956584    0.18699999    0.38830003    0.20079999    0.1793    ]
[37m[1m [4518.32095893    0.1822        0.37550002    0.20089999    0.1829    ]
[37m[1m ...
[37m[1m [3938.00422375    0.18753333    0.39032501    0.22018333    0.17940831]
[37m[1m [3395.04394815    0.1837927     0.33853903    0.16287805    0.17446342]
[37m[1m [3527.59296765    0.17459348    0.34674129    0.17797826    0.16875   ]]
[37m[1m[2023-06-25 11:31:58,121][129146] Max Reward on eval: 4615.322577512497
[37m[1m[2023-06-25 11:31:58,122][129146] Min Reward on eval: 1923.0404085905116
[37m[1m[2023-06-25 11:31:58,122][129146] Mean Reward across all agents: 4083.741080165662
[37m[1m[2023-06-25 11:31:58,122][129146] Average Trajectory Length: 973.7959999999999
[36m[2023-06-25 11:31:58,124][129146] mean_value=-420.57377109918735, max_value=695.093934229853
[37m[1m[2023-06-25 11:31:58,126][129146] New mean coefficients: [[ 0.17446458 -0.03332105  0.00012296  0.04328117  0.0156585 ]]
[37m[1m[2023-06-25 11:31:58,127][129146] Moving the mean solution point...
[36m[2023-06-25 11:32:07,778][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 11:32:07,778][129146] FPS: 397984.82
[36m[2023-06-25 11:32:07,780][129146] itr=1152, itrs=2000, Progress: 57.60%
[36m[2023-06-25 11:32:19,259][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 11:32:19,259][129146] FPS: 335207.74
[36m[2023-06-25 11:32:23,999][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:32:24,004][129146] Reward + Measures: [[4695.48615463    0.18619153    0.37887165    0.18982479    0.1769637 ]]
[37m[1m[2023-06-25 11:32:24,005][129146] Max Reward on eval: 4695.486154625487
[37m[1m[2023-06-25 11:32:24,005][129146] Min Reward on eval: 4695.486154625487
[37m[1m[2023-06-25 11:32:24,005][129146] Mean Reward across all agents: 4695.486154625487
[37m[1m[2023-06-25 11:32:24,006][129146] Average Trajectory Length: 995.6343333333333
[36m[2023-06-25 11:32:29,386][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:32:29,392][129146] Reward + Measures: [[4648.39424011    0.18379998    0.37480003    0.1893        0.1796    ]
[37m[1m [4762.66108985    0.18739998    0.3761        0.19420001    0.17820001]
[37m[1m [4608.7908851     0.17730001    0.39890003    0.19800001    0.17839999]
[37m[1m ...
[37m[1m [3998.76454479    0.18771152    0.34633365    0.18639171    0.18576589]
[37m[1m [3626.08355666    0.2138        0.41580001    0.19750002    0.21180001]
[37m[1m [4488.01408133    0.2014        0.37930003    0.18969999    0.18380001]]
[37m[1m[2023-06-25 11:32:29,392][129146] Max Reward on eval: 4825.995366249093
[37m[1m[2023-06-25 11:32:29,393][129146] Min Reward on eval: 3626.0835566642695
[37m[1m[2023-06-25 11:32:29,393][129146] Mean Reward across all agents: 4482.7936897914515
[37m[1m[2023-06-25 11:32:29,393][129146] Average Trajectory Length: 992.3726666666666
[36m[2023-06-25 11:32:29,396][129146] mean_value=-108.14753913657376, max_value=696.824177724684
[37m[1m[2023-06-25 11:32:29,399][129146] New mean coefficients: [[ 0.05854403  0.01476106 -0.05224973 -0.03122911 -0.01680398]]
[37m[1m[2023-06-25 11:32:29,400][129146] Moving the mean solution point...
[36m[2023-06-25 11:32:39,143][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 11:32:39,143][129146] FPS: 394214.56
[36m[2023-06-25 11:32:39,145][129146] itr=1153, itrs=2000, Progress: 57.65%
[36m[2023-06-25 11:32:50,590][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 11:32:50,591][129146] FPS: 336295.84
[36m[2023-06-25 11:32:55,383][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:32:55,384][129146] Reward + Measures: [[3225.82577632    0.18093951    0.45113122    0.18820292    0.15715089]]
[37m[1m[2023-06-25 11:32:55,384][129146] Max Reward on eval: 3225.8257763246993
[37m[1m[2023-06-25 11:32:55,384][129146] Min Reward on eval: 3225.8257763246993
[37m[1m[2023-06-25 11:32:55,384][129146] Mean Reward across all agents: 3225.8257763246993
[37m[1m[2023-06-25 11:32:55,385][129146] Average Trajectory Length: 992.5976666666667
[36m[2023-06-25 11:33:00,955][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:33:00,956][129146] Reward + Measures: [[3159.02487925    0.1741        0.4443        0.18539999    0.152     ]
[37m[1m [3349.29322071    0.17493011    0.43670216    0.18909161    0.15436479]
[37m[1m [3381.32824648    0.17809999    0.43919998    0.18450001    0.15650001]
[37m[1m ...
[37m[1m [3094.25851132    0.19520001    0.46650001    0.1954        0.15910001]
[37m[1m [3075.47194124    0.18310001    0.46080002    0.1902        0.15610002]
[37m[1m [3356.04670698    0.17130001    0.42430001    0.1918        0.16340001]]
[37m[1m[2023-06-25 11:33:00,956][129146] Max Reward on eval: 3587.198098828946
[37m[1m[2023-06-25 11:33:00,956][129146] Min Reward on eval: 1922.6922781543776
[37m[1m[2023-06-25 11:33:00,957][129146] Mean Reward across all agents: 3113.98700180381
[37m[1m[2023-06-25 11:33:00,957][129146] Average Trajectory Length: 988.8046666666667
[36m[2023-06-25 11:33:00,958][129146] mean_value=-1441.7313665099903, max_value=-1039.2431932414615
[36m[2023-06-25 11:33:00,960][129146] XNES is restarting with a new solution whose measures are [0.91250002 0.90760005 0.0316     0.85210001] and objective is 894.3903258963488
[36m[2023-06-25 11:33:00,961][129146] Emitter restarted. Changing the mean agent...
[37m[1m[2023-06-25 11:33:00,964][129146] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]]
[37m[1m[2023-06-25 11:33:00,965][129146] Moving the mean solution point...
[36m[2023-06-25 11:33:10,614][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 11:33:10,614][129146] FPS: 398048.66
[36m[2023-06-25 11:33:10,616][129146] itr=1154, itrs=2000, Progress: 57.70%
[36m[2023-06-25 11:33:22,108][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 11:33:22,108][129146] FPS: 334814.86
[36m[2023-06-25 11:33:26,921][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:33:26,921][129146] Reward + Measures: [[1412.90606502    0.66115069    0.64149761    0.11329333    0.36796898]]
[37m[1m[2023-06-25 11:33:26,922][129146] Max Reward on eval: 1412.9060650154308
[37m[1m[2023-06-25 11:33:26,922][129146] Min Reward on eval: 1412.9060650154308
[37m[1m[2023-06-25 11:33:26,922][129146] Mean Reward across all agents: 1412.9060650154308
[37m[1m[2023-06-25 11:33:26,922][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:33:32,398][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:33:32,399][129146] Reward + Measures: [[1135.33714139    0.61479998    0.55580002    0.18840002    0.2228    ]
[37m[1m [-525.77273277    0.25243375    0.23667879    0.15007094    0.12138569]
[37m[1m [-564.08188544    0.25280055    0.31550983    0.11491088    0.26384032]
[37m[1m ...
[37m[1m [ 516.79322681    0.86720002    0.88819999    0.0261        0.85270005]
[37m[1m [-836.6282764     0.28480574    0.40544596    0.08597845    0.3285462 ]
[37m[1m [-641.47652605    0.58700001    0.1153        0.40540001    0.33470002]]
[37m[1m[2023-06-25 11:33:32,399][129146] Max Reward on eval: 1489.429812380299
[37m[1m[2023-06-25 11:33:32,399][129146] Min Reward on eval: -1361.8831488337369
[37m[1m[2023-06-25 11:33:32,399][129146] Mean Reward across all agents: 40.988790570536544
[37m[1m[2023-06-25 11:33:32,400][129146] Average Trajectory Length: 955.4433333333333
[36m[2023-06-25 11:33:32,401][129146] mean_value=-1808.2842282837048, max_value=1356.0683349817655
[37m[1m[2023-06-25 11:33:32,404][129146] New mean coefficients: [[ 0.11315358 -0.46687818 -0.44557238 -0.93675816 -1.2441607 ]]
[37m[1m[2023-06-25 11:33:32,405][129146] Moving the mean solution point...
[36m[2023-06-25 11:33:42,243][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 11:33:42,244][129146] FPS: 390367.00
[36m[2023-06-25 11:33:42,246][129146] itr=1155, itrs=2000, Progress: 57.75%
[36m[2023-06-25 11:33:53,875][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 11:33:53,875][129146] FPS: 330882.12
[36m[2023-06-25 11:33:58,648][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:33:58,648][129146] Reward + Measures: [[1513.8436231     0.63017303    0.59840965    0.13199267    0.28527167]]
[37m[1m[2023-06-25 11:33:58,648][129146] Max Reward on eval: 1513.8436231013263
[37m[1m[2023-06-25 11:33:58,648][129146] Min Reward on eval: 1513.8436231013263
[37m[1m[2023-06-25 11:33:58,649][129146] Mean Reward across all agents: 1513.8436231013263
[37m[1m[2023-06-25 11:33:58,649][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:34:04,088][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:34:04,088][129146] Reward + Measures: [[ -391.59657726     0.42347422     0.34444386     0.25200328
[37m[1m      0.22217058]
[37m[1m [  -55.83585664     0.59900421     0.19585688     0.42376289
[37m[1m      0.2428581 ]
[37m[1m [   71.62463282     0.36320004     0.44230005     0.22589998
[37m[1m      0.39000002]
[37m[1m ...
[37m[1m [-1211.96016631     0.2378         0.27970001     0.2888
[37m[1m      0.25830001]
[37m[1m [  854.26057597     0.59419996     0.46469998     0.2465
[37m[1m      0.28860003]
[37m[1m [  333.83098668     0.45354605     0.42029366     0.25638574
[37m[1m      0.38016191]]
[37m[1m[2023-06-25 11:34:04,089][129146] Max Reward on eval: 1715.9863875787473
[37m[1m[2023-06-25 11:34:04,089][129146] Min Reward on eval: -1211.9601663096807
[37m[1m[2023-06-25 11:34:04,089][129146] Mean Reward across all agents: 59.00250997934891
[37m[1m[2023-06-25 11:34:04,089][129146] Average Trajectory Length: 970.3566666666667
[36m[2023-06-25 11:34:04,091][129146] mean_value=-1723.32151138164, max_value=1439.9366133627502
[37m[1m[2023-06-25 11:34:04,093][129146] New mean coefficients: [[ 0.74834573  0.23002118  0.29360217 -0.07365215  0.14778113]]
[37m[1m[2023-06-25 11:34:04,094][129146] Moving the mean solution point...
[36m[2023-06-25 11:34:13,863][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 11:34:13,863][129146] FPS: 393165.53
[36m[2023-06-25 11:34:13,865][129146] itr=1156, itrs=2000, Progress: 57.80%
[36m[2023-06-25 11:34:25,432][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 11:34:25,433][129146] FPS: 332643.78
[36m[2023-06-25 11:34:30,258][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:34:30,258][129146] Reward + Measures: [[1581.35950698    0.62899697    0.58981329    0.13094133    0.27194533]]
[37m[1m[2023-06-25 11:34:30,258][129146] Max Reward on eval: 1581.3595069784055
[37m[1m[2023-06-25 11:34:30,259][129146] Min Reward on eval: 1581.3595069784055
[37m[1m[2023-06-25 11:34:30,259][129146] Mean Reward across all agents: 1581.3595069784055
[37m[1m[2023-06-25 11:34:30,259][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:34:35,730][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:34:35,783][129146] Reward + Measures: [[1066.47559095    0.588         0.5733        0.0602        0.35749999]
[37m[1m [-486.22763525    0.24633248    0.24409452    0.18471806    0.16480431]
[37m[1m [-212.90042534    0.2546764     0.2156219     0.19206782    0.15340473]
[37m[1m ...
[37m[1m [-182.07804679    0.43826959    0.42638516    0.20476857    0.38784841]
[37m[1m [-362.42502374    0.39531538    0.36747691    0.18485385    0.38870004]
[37m[1m [ 119.70685466    0.43090001    0.26910001    0.2872        0.25190002]]
[37m[1m[2023-06-25 11:34:35,783][129146] Max Reward on eval: 1587.6075599663077
[37m[1m[2023-06-25 11:34:35,783][129146] Min Reward on eval: -1453.1822271671845
[37m[1m[2023-06-25 11:34:35,783][129146] Mean Reward across all agents: 312.1507776063526
[37m[1m[2023-06-25 11:34:35,784][129146] Average Trajectory Length: 986.9626666666667
[36m[2023-06-25 11:34:35,785][129146] mean_value=-1688.3364879178405, max_value=1083.4043553566535
[37m[1m[2023-06-25 11:34:35,788][129146] New mean coefficients: [[0.15900749 0.36432844 0.13445456 0.18536338 0.21465795]]
[37m[1m[2023-06-25 11:34:35,789][129146] Moving the mean solution point...
[36m[2023-06-25 11:34:45,646][129146] train() took 9.86 seconds to complete
[36m[2023-06-25 11:34:45,646][129146] FPS: 389629.51
[36m[2023-06-25 11:34:45,648][129146] itr=1157, itrs=2000, Progress: 57.85%
[36m[2023-06-25 11:34:57,200][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 11:34:57,200][129146] FPS: 333099.21
[36m[2023-06-25 11:35:01,994][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:35:01,995][129146] Reward + Measures: [[1613.07810031    0.6399523     0.60104996    0.12091566    0.283705  ]]
[37m[1m[2023-06-25 11:35:01,995][129146] Max Reward on eval: 1613.0781003092932
[37m[1m[2023-06-25 11:35:01,995][129146] Min Reward on eval: 1613.0781003092932
[37m[1m[2023-06-25 11:35:01,995][129146] Mean Reward across all agents: 1613.0781003092932
[37m[1m[2023-06-25 11:35:01,996][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:35:07,522][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:35:07,523][129146] Reward + Measures: [[-750.67851707    0.38419968    0.37263247    0.20955244    0.22328244]
[37m[1m [ 826.8560543     0.4278        0.4817        0.1243        0.33320001]
[37m[1m [-102.22929607    0.3860794     0.43349138    0.30513588    0.30187243]
[37m[1m ...
[37m[1m [1328.31334138    0.52280009    0.54430002    0.16660002    0.25470001]
[37m[1m [1180.94506615    0.63920003    0.58990002    0.16990001    0.35980001]
[37m[1m [ 994.51273058    0.41209999    0.57050002    0.2096        0.25630003]]
[37m[1m[2023-06-25 11:35:07,523][129146] Max Reward on eval: 1733.3350787909935
[37m[1m[2023-06-25 11:35:07,523][129146] Min Reward on eval: -983.2278447910561
[37m[1m[2023-06-25 11:35:07,523][129146] Mean Reward across all agents: 396.11065104886353
[37m[1m[2023-06-25 11:35:07,524][129146] Average Trajectory Length: 977.5416666666666
[36m[2023-06-25 11:35:07,525][129146] mean_value=-1429.2097660105292, max_value=1239.7374665081918
[37m[1m[2023-06-25 11:35:07,528][129146] New mean coefficients: [[-0.22651055  0.63059366 -0.35333043 -0.19350109  0.18287164]]
[37m[1m[2023-06-25 11:35:07,529][129146] Moving the mean solution point...
[36m[2023-06-25 11:35:17,314][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 11:35:17,315][129146] FPS: 392485.21
[36m[2023-06-25 11:35:17,317][129146] itr=1158, itrs=2000, Progress: 57.90%
[36m[2023-06-25 11:35:28,793][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 11:35:28,793][129146] FPS: 335387.03
[36m[2023-06-25 11:35:33,607][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:35:33,608][129146] Reward + Measures: [[1518.16589387    0.66288531    0.61025101    0.11759634    0.308263  ]]
[37m[1m[2023-06-25 11:35:33,608][129146] Max Reward on eval: 1518.1658938708522
[37m[1m[2023-06-25 11:35:33,608][129146] Min Reward on eval: 1518.1658938708522
[37m[1m[2023-06-25 11:35:33,608][129146] Mean Reward across all agents: 1518.1658938708522
[37m[1m[2023-06-25 11:35:33,608][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:35:39,287][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:35:39,287][129146] Reward + Measures: [[1082.13003732    0.68559998    0.63139999    0.16320001    0.40229997]
[37m[1m [ 512.7083579     0.5794        0.5891        0.16240001    0.29140002]
[37m[1m [1180.3844589     0.57190007    0.59289998    0.073         0.39430004]
[37m[1m ...
[37m[1m [1323.08952978    0.58020002    0.62029999    0.1349        0.39589998]
[37m[1m [ 867.58316401    0.50060004    0.66950005    0.1506        0.44399998]
[37m[1m [-355.45512449    0.39420003    0.4474        0.1098        0.31389999]]
[37m[1m[2023-06-25 11:35:39,287][129146] Max Reward on eval: 1411.1169503502083
[37m[1m[2023-06-25 11:35:39,288][129146] Min Reward on eval: -1379.3176977708004
[37m[1m[2023-06-25 11:35:39,288][129146] Mean Reward across all agents: 265.8064713380357
[37m[1m[2023-06-25 11:35:39,288][129146] Average Trajectory Length: 990.4549999999999
[36m[2023-06-25 11:35:39,290][129146] mean_value=-1233.8078056531938, max_value=1330.72120241254
[37m[1m[2023-06-25 11:35:39,293][129146] New mean coefficients: [[-0.0674579   0.9200561  -1.0620215  -0.06261858  0.04826903]]
[37m[1m[2023-06-25 11:35:39,294][129146] Moving the mean solution point...
[36m[2023-06-25 11:35:48,966][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 11:35:48,966][129146] FPS: 397067.66
[36m[2023-06-25 11:35:48,969][129146] itr=1159, itrs=2000, Progress: 57.95%
[36m[2023-06-25 11:36:00,407][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 11:36:00,407][129146] FPS: 336384.94
[36m[2023-06-25 11:36:05,143][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:36:05,143][129146] Reward + Measures: [[1505.91410295    0.68428969    0.58574498    0.130289      0.29378498]]
[37m[1m[2023-06-25 11:36:05,143][129146] Max Reward on eval: 1505.914102947038
[37m[1m[2023-06-25 11:36:05,143][129146] Min Reward on eval: 1505.914102947038
[37m[1m[2023-06-25 11:36:05,144][129146] Mean Reward across all agents: 1505.914102947038
[37m[1m[2023-06-25 11:36:05,144][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:36:10,598][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:36:10,604][129146] Reward + Measures: [[ 103.12598682    0.347         0.42750001    0.21519999    0.31389999]
[37m[1m [-161.78732132    0.33800003    0.44490001    0.21999998    0.32009998]
[37m[1m [-457.15964218    0.14821601    0.17660062    0.11350788    0.13940611]
[37m[1m ...
[37m[1m [ 665.04413676    0.46630001    0.45819998    0.14960001    0.26810002]
[37m[1m [-117.52631551    0.45619994    0.32210001    0.21990001    0.24349999]
[37m[1m [ 370.56340337    0.46698618    0.41000375    0.17169943    0.26075512]]
[37m[1m[2023-06-25 11:36:10,604][129146] Max Reward on eval: 1646.291705166304
[37m[1m[2023-06-25 11:36:10,604][129146] Min Reward on eval: -1012.6146149453241
[37m[1m[2023-06-25 11:36:10,605][129146] Mean Reward across all agents: 405.96257888908855
[37m[1m[2023-06-25 11:36:10,605][129146] Average Trajectory Length: 980.1523333333333
[36m[2023-06-25 11:36:10,607][129146] mean_value=-1385.5516162437036, max_value=790.8952218664783
[37m[1m[2023-06-25 11:36:10,609][129146] New mean coefficients: [[-0.25221267  1.0287429  -1.5236416  -0.36057308  0.12243938]]
[37m[1m[2023-06-25 11:36:10,610][129146] Moving the mean solution point...
[36m[2023-06-25 11:36:20,271][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 11:36:20,271][129146] FPS: 397547.52
[36m[2023-06-25 11:36:20,274][129146] itr=1160, itrs=2000, Progress: 58.00%
[37m[1m[2023-06-25 11:36:27,323][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001140
[36m[2023-06-25 11:36:39,091][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 11:36:39,092][129146] FPS: 332786.35
[36m[2023-06-25 11:36:43,907][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:36:43,907][129146] Reward + Measures: [[1477.42612505    0.69512761    0.55802935    0.14011733    0.28103235]]
[37m[1m[2023-06-25 11:36:43,907][129146] Max Reward on eval: 1477.4261250490413
[37m[1m[2023-06-25 11:36:43,907][129146] Min Reward on eval: 1477.4261250490413
[37m[1m[2023-06-25 11:36:43,908][129146] Mean Reward across all agents: 1477.4261250490413
[37m[1m[2023-06-25 11:36:43,908][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:36:49,383][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:36:49,383][129146] Reward + Measures: [[ 417.36455875    0.66880006    0.67679995    0.25209999    0.56190008]
[37m[1m [ 643.77361686    0.87330002    0.80089998    0.1093        0.7766    ]
[37m[1m [  77.37628087    0.4876        0.4729        0.2289        0.19930001]
[37m[1m ...
[37m[1m [1256.56030333    0.60250008    0.54710001    0.15110001    0.24419999]
[37m[1m [-119.47524345    0.23081346    0.17780352    0.12964971    0.0810462 ]
[37m[1m [-430.29307773    0.44799885    0.2807436     0.26978272    0.21479967]]
[37m[1m[2023-06-25 11:36:49,384][129146] Max Reward on eval: 1414.8727092499553
[37m[1m[2023-06-25 11:36:49,384][129146] Min Reward on eval: -568.0091069074348
[37m[1m[2023-06-25 11:36:49,384][129146] Mean Reward across all agents: 378.15531496035146
[37m[1m[2023-06-25 11:36:49,384][129146] Average Trajectory Length: 947.533
[36m[2023-06-25 11:36:49,387][129146] mean_value=-1234.212519910175, max_value=1859.5243788893335
[37m[1m[2023-06-25 11:36:49,390][129146] New mean coefficients: [[-0.35973787  1.0285523  -1.2425051  -0.8449997  -1.394845  ]]
[37m[1m[2023-06-25 11:36:49,391][129146] Moving the mean solution point...
[36m[2023-06-25 11:36:59,294][129146] train() took 9.90 seconds to complete
[36m[2023-06-25 11:36:59,295][129146] FPS: 387798.94
[36m[2023-06-25 11:36:59,297][129146] itr=1161, itrs=2000, Progress: 58.05%
[36m[2023-06-25 11:37:10,920][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 11:37:10,920][129146] FPS: 331048.24
[36m[2023-06-25 11:37:15,573][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:37:15,573][129146] Reward + Measures: [[1448.0936046     0.70450729    0.53200167    0.157729      0.25248232]]
[37m[1m[2023-06-25 11:37:15,573][129146] Max Reward on eval: 1448.0936045984074
[37m[1m[2023-06-25 11:37:15,573][129146] Min Reward on eval: 1448.0936045984074
[37m[1m[2023-06-25 11:37:15,574][129146] Mean Reward across all agents: 1448.0936045984074
[37m[1m[2023-06-25 11:37:15,574][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:37:21,073][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:37:21,073][129146] Reward + Measures: [[ 321.63525557    0.44650003    0.40890002    0.16470002    0.32259998]
[37m[1m [ 614.52199278    0.45749998    0.56650001    0.13950001    0.45369998]
[37m[1m [-215.72925639    0.3168        0.3784        0.1591        0.29390001]
[37m[1m ...
[37m[1m [1067.8868158     0.48810002    0.3969        0.153         0.22500001]
[37m[1m [-409.9299944     0.31637588    0.35832167    0.21275225    0.27100214]
[37m[1m [ 931.61482385    0.5158        0.47199997    0.18620001    0.23249999]]
[37m[1m[2023-06-25 11:37:21,073][129146] Max Reward on eval: 1500.1989395279204
[37m[1m[2023-06-25 11:37:21,074][129146] Min Reward on eval: -815.9381245331955
[37m[1m[2023-06-25 11:37:21,074][129146] Mean Reward across all agents: 261.2760415198538
[37m[1m[2023-06-25 11:37:21,074][129146] Average Trajectory Length: 973.2163333333333
[36m[2023-06-25 11:37:21,076][129146] mean_value=-1749.6572946337246, max_value=597.8141830806185
[37m[1m[2023-06-25 11:37:21,079][129146] New mean coefficients: [[ 0.19797933  1.8218694  -1.1528953   0.68184566 -0.97592473]]
[37m[1m[2023-06-25 11:37:21,080][129146] Moving the mean solution point...
[36m[2023-06-25 11:37:30,776][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 11:37:30,777][129146] FPS: 396075.85
[36m[2023-06-25 11:37:30,779][129146] itr=1162, itrs=2000, Progress: 58.10%
[36m[2023-06-25 11:37:42,241][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 11:37:42,241][129146] FPS: 335779.32
[36m[2023-06-25 11:37:47,127][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:37:47,128][129146] Reward + Measures: [[1448.74399478    0.71614027    0.51548272    0.17287101    0.23368099]]
[37m[1m[2023-06-25 11:37:47,128][129146] Max Reward on eval: 1448.7439947789528
[37m[1m[2023-06-25 11:37:47,128][129146] Min Reward on eval: 1448.7439947789528
[37m[1m[2023-06-25 11:37:47,128][129146] Mean Reward across all agents: 1448.7439947789528
[37m[1m[2023-06-25 11:37:47,129][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:37:52,554][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:37:52,560][129146] Reward + Measures: [[1381.21235942    0.60600001    0.5097        0.19030002    0.23580001]
[37m[1m [ -19.35645645    0.78690004    0.0787        0.91280001    0.3994    ]
[37m[1m [1118.47313202    0.5104        0.52399999    0.1393        0.30070001]
[37m[1m ...
[37m[1m [ 956.69092343    0.50739998    0.51940006    0.15620001    0.39020002]
[37m[1m [1040.65058911    0.63589996    0.45070001    0.15470001    0.2538    ]
[37m[1m [ 293.11095861    0.78310001    0.1912        0.63639998    0.61840004]]
[37m[1m[2023-06-25 11:37:52,560][129146] Max Reward on eval: 1528.5906723685098
[37m[1m[2023-06-25 11:37:52,560][129146] Min Reward on eval: -551.5621498141321
[37m[1m[2023-06-25 11:37:52,561][129146] Mean Reward across all agents: 534.7853991450738
[37m[1m[2023-06-25 11:37:52,561][129146] Average Trajectory Length: 985.0023333333334
[36m[2023-06-25 11:37:52,564][129146] mean_value=-1091.9085356768683, max_value=1305.8978113022215
[37m[1m[2023-06-25 11:37:52,567][129146] New mean coefficients: [[ 0.86572945  1.6051712  -1.0386398  -0.58852804 -1.2656846 ]]
[37m[1m[2023-06-25 11:37:52,568][129146] Moving the mean solution point...
[36m[2023-06-25 11:38:02,297][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 11:38:02,298][129146] FPS: 394728.11
[36m[2023-06-25 11:38:02,300][129146] itr=1163, itrs=2000, Progress: 58.15%
[36m[2023-06-25 11:38:13,876][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 11:38:13,876][129146] FPS: 332421.34
[36m[2023-06-25 11:38:18,615][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:38:18,616][129146] Reward + Measures: [[1536.27192568    0.72348869    0.51503599    0.16801101    0.22523035]]
[37m[1m[2023-06-25 11:38:18,616][129146] Max Reward on eval: 1536.271925680592
[37m[1m[2023-06-25 11:38:18,616][129146] Min Reward on eval: 1536.271925680592
[37m[1m[2023-06-25 11:38:18,616][129146] Mean Reward across all agents: 1536.271925680592
[37m[1m[2023-06-25 11:38:18,617][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:38:24,016][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:38:24,016][129146] Reward + Measures: [[  75.69203755    0.43090001    0.22000001    0.35310003    0.24849999]
[37m[1m [ -17.51694672    0.43330002    0.35050002    0.3484        0.28770003]
[37m[1m [-432.37299262    0.56389999    0.29360002    0.38470003    0.29940003]
[37m[1m ...
[37m[1m [ 489.61921632    0.64380002    0.33399999    0.50200003    0.41569996]
[37m[1m [ -50.50519383    0.28050002    0.2755        0.2604        0.23020001]
[37m[1m [-616.66423682    0.32220003    0.21110001    0.1313        0.2304    ]]
[37m[1m[2023-06-25 11:38:24,017][129146] Max Reward on eval: 1600.7438834479776
[37m[1m[2023-06-25 11:38:24,017][129146] Min Reward on eval: -686.7403139313653
[37m[1m[2023-06-25 11:38:24,017][129146] Mean Reward across all agents: 537.6697694540959
[37m[1m[2023-06-25 11:38:24,017][129146] Average Trajectory Length: 991.5373333333333
[36m[2023-06-25 11:38:24,019][129146] mean_value=-1224.5292032626348, max_value=1871.5684921000677
[37m[1m[2023-06-25 11:38:24,022][129146] New mean coefficients: [[ 0.3059479   1.8845367  -0.9698577  -0.21118087 -0.28398108]]
[37m[1m[2023-06-25 11:38:24,023][129146] Moving the mean solution point...
[36m[2023-06-25 11:38:33,654][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 11:38:33,654][129146] FPS: 398779.87
[36m[2023-06-25 11:38:33,656][129146] itr=1164, itrs=2000, Progress: 58.20%
[36m[2023-06-25 11:38:45,361][129146] train() took 11.68 seconds to complete
[36m[2023-06-25 11:38:45,361][129146] FPS: 328788.91
[36m[2023-06-25 11:38:50,223][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:38:50,223][129146] Reward + Measures: [[1551.10118117    0.74223566    0.50167835    0.16627699    0.22290434]]
[37m[1m[2023-06-25 11:38:50,223][129146] Max Reward on eval: 1551.1011811721423
[37m[1m[2023-06-25 11:38:50,224][129146] Min Reward on eval: 1551.1011811721423
[37m[1m[2023-06-25 11:38:50,224][129146] Mean Reward across all agents: 1551.1011811721423
[37m[1m[2023-06-25 11:38:50,224][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:38:55,660][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:38:55,661][129146] Reward + Measures: [[250.61237317   0.48319998   0.52320004   0.091        0.57960004]
[37m[1m [ 81.52596949   0.51229995   0.24320002   0.3671       0.31690001]
[37m[1m [757.97573826   0.6699       0.47610003   0.26760003   0.22330001]
[37m[1m ...
[37m[1m [475.72640942   0.64480001   0.72360003   0.0484       0.57490003]
[37m[1m [741.24199164   0.69130003   0.53590006   0.3242       0.23140001]
[37m[1m [817.99586526   0.5499       0.63010001   0.1249       0.4032    ]]
[37m[1m[2023-06-25 11:38:55,661][129146] Max Reward on eval: 1675.148687361437
[37m[1m[2023-06-25 11:38:55,661][129146] Min Reward on eval: -937.1344032494293
[37m[1m[2023-06-25 11:38:55,662][129146] Mean Reward across all agents: 452.251004774049
[37m[1m[2023-06-25 11:38:55,662][129146] Average Trajectory Length: 985.8653333333333
[36m[2023-06-25 11:38:55,664][129146] mean_value=-937.1070598443578, max_value=1627.2372037302262
[37m[1m[2023-06-25 11:38:55,667][129146] New mean coefficients: [[ 0.23919511  1.7905713  -0.8379697  -0.02513227  0.1192601 ]]
[37m[1m[2023-06-25 11:38:55,668][129146] Moving the mean solution point...
[36m[2023-06-25 11:39:05,426][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 11:39:05,427][129146] FPS: 393555.62
[36m[2023-06-25 11:39:05,429][129146] itr=1165, itrs=2000, Progress: 58.25%
[36m[2023-06-25 11:39:17,051][129146] train() took 11.60 seconds to complete
[36m[2023-06-25 11:39:17,051][129146] FPS: 331139.57
[36m[2023-06-25 11:39:21,812][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:39:21,813][129146] Reward + Measures: [[1597.70049355    0.75927293    0.49405366    0.16499734    0.21970564]]
[37m[1m[2023-06-25 11:39:21,813][129146] Max Reward on eval: 1597.7004935453438
[37m[1m[2023-06-25 11:39:21,813][129146] Min Reward on eval: 1597.7004935453438
[37m[1m[2023-06-25 11:39:21,814][129146] Mean Reward across all agents: 1597.7004935453438
[37m[1m[2023-06-25 11:39:21,814][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:39:27,257][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:39:27,257][129146] Reward + Measures: [[1315.09058886    0.58169997    0.52040005    0.17530002    0.23519997]
[37m[1m [ 938.06214385    0.62489998    0.54069996    0.2744        0.28749999]
[37m[1m [ 207.03866921    0.4341        0.4289        0.1804        0.32780001]
[37m[1m ...
[37m[1m [ 318.74257636    0.57129306    0.47470698    0.25623256    0.22330466]
[37m[1m [  63.46703459    0.66790003    0.3691        0.60960001    0.1495    ]
[37m[1m [1468.15391635    0.70720005    0.58690006    0.15530001    0.29010001]]
[37m[1m[2023-06-25 11:39:27,257][129146] Max Reward on eval: 1606.4492614185322
[37m[1m[2023-06-25 11:39:27,258][129146] Min Reward on eval: -608.8266281859717
[37m[1m[2023-06-25 11:39:27,258][129146] Mean Reward across all agents: 490.4267850146362
[37m[1m[2023-06-25 11:39:27,258][129146] Average Trajectory Length: 986.7819999999999
[36m[2023-06-25 11:39:27,260][129146] mean_value=-1010.4440476392102, max_value=606.6322056972782
[37m[1m[2023-06-25 11:39:27,262][129146] New mean coefficients: [[ 0.22134969  1.5887733  -0.83269906  0.21171714  0.38215607]]
[37m[1m[2023-06-25 11:39:27,263][129146] Moving the mean solution point...
[36m[2023-06-25 11:39:37,111][129146] train() took 9.85 seconds to complete
[36m[2023-06-25 11:39:37,111][129146] FPS: 390019.53
[36m[2023-06-25 11:39:37,113][129146] itr=1166, itrs=2000, Progress: 58.30%
[36m[2023-06-25 11:39:48,611][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 11:39:48,612][129146] FPS: 334735.13
[36m[2023-06-25 11:39:53,425][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:39:53,425][129146] Reward + Measures: [[1587.7127946     0.77354831    0.48042172    0.16845767    0.22006398]]
[37m[1m[2023-06-25 11:39:53,425][129146] Max Reward on eval: 1587.7127946034689
[37m[1m[2023-06-25 11:39:53,426][129146] Min Reward on eval: 1587.7127946034689
[37m[1m[2023-06-25 11:39:53,426][129146] Mean Reward across all agents: 1587.7127946034689
[37m[1m[2023-06-25 11:39:53,426][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:39:59,074][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:39:59,075][129146] Reward + Measures: [[1331.58653604    0.71810001    0.42750001    0.16779999    0.19859999]
[37m[1m [-182.44083777    0.47919998    0.3626        0.45050001    0.42069998]
[37m[1m [1355.71629634    0.60759997    0.42389998    0.17820001    0.22230001]
[37m[1m ...
[37m[1m [1314.99721937    0.72980011    0.48780003    0.20039999    0.20840001]
[37m[1m [-224.86210131    0.80580008    0.20299999    0.76130003    0.2246    ]
[37m[1m [ 129.17790661    0.7446        0.33199999    0.69240004    0.18939999]]
[37m[1m[2023-06-25 11:39:59,075][129146] Max Reward on eval: 1649.870346355997
[37m[1m[2023-06-25 11:39:59,076][129146] Min Reward on eval: -619.0191130875377
[37m[1m[2023-06-25 11:39:59,076][129146] Mean Reward across all agents: 531.9175916295029
[37m[1m[2023-06-25 11:39:59,076][129146] Average Trajectory Length: 992.6493333333333
[36m[2023-06-25 11:39:59,079][129146] mean_value=-725.0564180720867, max_value=1942.8847185081918
[37m[1m[2023-06-25 11:39:59,082][129146] New mean coefficients: [[ 0.66037774  1.7571356  -1.1009238  -0.2828939   0.39266008]]
[37m[1m[2023-06-25 11:39:59,083][129146] Moving the mean solution point...
[36m[2023-06-25 11:40:08,757][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 11:40:08,757][129146] FPS: 396988.26
[36m[2023-06-25 11:40:08,759][129146] itr=1167, itrs=2000, Progress: 58.35%
[36m[2023-06-25 11:40:20,166][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 11:40:20,167][129146] FPS: 337336.25
[36m[2023-06-25 11:40:24,822][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:40:24,823][129146] Reward + Measures: [[1637.21127311    0.78271133    0.48020166    0.16482033    0.22430566]]
[37m[1m[2023-06-25 11:40:24,823][129146] Max Reward on eval: 1637.2112731136913
[37m[1m[2023-06-25 11:40:24,823][129146] Min Reward on eval: 1637.2112731136913
[37m[1m[2023-06-25 11:40:24,823][129146] Mean Reward across all agents: 1637.2112731136913
[37m[1m[2023-06-25 11:40:24,824][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:40:30,304][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:40:30,304][129146] Reward + Measures: [[1398.18702686    0.4461        0.41409999    0.16049999    0.23410001]
[37m[1m [ 578.31311162    0.81650001    0.34819999    0.50699997    0.71490002]
[37m[1m [1274.32117796    0.48119998    0.53610003    0.16070001    0.21250001]
[37m[1m ...
[37m[1m [ 691.6741694     0.56450003    0.59310001    0.21530001    0.4492    ]
[37m[1m [1258.67729191    0.6347        0.62530005    0.1027        0.37149999]
[37m[1m [1572.82544294    0.64789999    0.4736        0.17910002    0.2189    ]]
[37m[1m[2023-06-25 11:40:30,305][129146] Max Reward on eval: 1853.0650479470612
[37m[1m[2023-06-25 11:40:30,305][129146] Min Reward on eval: -548.5940449794871
[37m[1m[2023-06-25 11:40:30,305][129146] Mean Reward across all agents: 901.6998622747998
[37m[1m[2023-06-25 11:40:30,305][129146] Average Trajectory Length: 994.7363333333333
[36m[2023-06-25 11:40:30,309][129146] mean_value=-876.6559951706067, max_value=1885.1137242280022
[37m[1m[2023-06-25 11:40:30,311][129146] New mean coefficients: [[ 0.15336561  2.086439   -1.6107936   0.6917832  -0.05536234]]
[37m[1m[2023-06-25 11:40:30,312][129146] Moving the mean solution point...
[36m[2023-06-25 11:40:40,033][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 11:40:40,033][129146] FPS: 395117.27
[36m[2023-06-25 11:40:40,035][129146] itr=1168, itrs=2000, Progress: 58.40%
[36m[2023-06-25 11:40:51,468][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 11:40:51,468][129146] FPS: 336669.24
[36m[2023-06-25 11:40:56,246][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:40:56,246][129146] Reward + Measures: [[1587.75929282    0.79885668    0.46471599    0.18022433    0.22104099]]
[37m[1m[2023-06-25 11:40:56,246][129146] Max Reward on eval: 1587.7592928181352
[37m[1m[2023-06-25 11:40:56,247][129146] Min Reward on eval: 1587.7592928181352
[37m[1m[2023-06-25 11:40:56,247][129146] Mean Reward across all agents: 1587.7592928181352
[37m[1m[2023-06-25 11:40:56,247][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:41:01,757][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:41:01,758][129146] Reward + Measures: [[ 858.32042614    0.66979998    0.54900002    0.2784        0.2753    ]
[37m[1m [ 596.77616445    0.58689994    0.49580002    0.17749999    0.50430006]
[37m[1m [1065.78071519    0.47159997    0.51710004    0.1481        0.31000003]
[37m[1m ...
[37m[1m [ 485.77577649    0.82679999    0.18050002    0.71079999    0.61799997]
[37m[1m [  61.5346408     0.68489999    0.30590001    0.52080005    0.3387    ]
[37m[1m [1298.02614085    0.76020002    0.6178        0.138         0.36500001]]
[37m[1m[2023-06-25 11:41:01,758][129146] Max Reward on eval: 1722.42390165003
[37m[1m[2023-06-25 11:41:01,758][129146] Min Reward on eval: -506.1642928701942
[37m[1m[2023-06-25 11:41:01,758][129146] Mean Reward across all agents: 645.4164147312978
[37m[1m[2023-06-25 11:41:01,759][129146] Average Trajectory Length: 994.0056666666667
[36m[2023-06-25 11:41:01,763][129146] mean_value=-497.69580844730274, max_value=1985.7503175409977
[37m[1m[2023-06-25 11:41:01,766][129146] New mean coefficients: [[ 0.23859161  2.5560026  -1.3109471   0.5136018  -0.5281254 ]]
[37m[1m[2023-06-25 11:41:01,767][129146] Moving the mean solution point...
[36m[2023-06-25 11:41:11,498][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 11:41:11,498][129146] FPS: 394685.29
[36m[2023-06-25 11:41:11,501][129146] itr=1169, itrs=2000, Progress: 58.45%
[36m[2023-06-25 11:41:23,166][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 11:41:23,166][129146] FPS: 329969.26
[36m[2023-06-25 11:41:27,980][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:41:27,981][129146] Reward + Measures: [[1553.99595254    0.81514698    0.45808131    0.18837333    0.225559  ]]
[37m[1m[2023-06-25 11:41:27,981][129146] Max Reward on eval: 1553.995952538638
[37m[1m[2023-06-25 11:41:27,981][129146] Min Reward on eval: 1553.995952538638
[37m[1m[2023-06-25 11:41:27,981][129146] Mean Reward across all agents: 1553.995952538638
[37m[1m[2023-06-25 11:41:27,981][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:41:33,495][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:41:33,496][129146] Reward + Measures: [[  78.46234731    0.44909999    0.34349999    0.25540003    0.27490002]
[37m[1m [1539.85483613    0.76890004    0.45630002    0.1824        0.2122    ]
[37m[1m [1127.47781705    0.68690008    0.5575        0.20820001    0.27990001]
[37m[1m ...
[37m[1m [ 858.96190939    0.58279997    0.51460004    0.1767        0.30270001]
[37m[1m [ 931.91985404    0.7335        0.45000002    0.27070001    0.42850003]
[37m[1m [ 976.65571261    0.60409999    0.4262        0.30270001    0.23140001]]
[37m[1m[2023-06-25 11:41:33,496][129146] Max Reward on eval: 1680.7591292422032
[37m[1m[2023-06-25 11:41:33,496][129146] Min Reward on eval: -519.4332710191608
[37m[1m[2023-06-25 11:41:33,496][129146] Mean Reward across all agents: 656.2823759791262
[37m[1m[2023-06-25 11:41:33,497][129146] Average Trajectory Length: 989.5673333333333
[36m[2023-06-25 11:41:33,501][129146] mean_value=-603.0230111833155, max_value=1882.4239068732406
[37m[1m[2023-06-25 11:41:33,504][129146] New mean coefficients: [[-0.18616897  2.8484704  -1.8455348   0.04673979 -0.7044151 ]]
[37m[1m[2023-06-25 11:41:33,505][129146] Moving the mean solution point...
[36m[2023-06-25 11:41:43,404][129146] train() took 9.90 seconds to complete
[36m[2023-06-25 11:41:43,404][129146] FPS: 387969.59
[36m[2023-06-25 11:41:43,407][129146] itr=1170, itrs=2000, Progress: 58.50%
[37m[1m[2023-06-25 11:41:50,700][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001150
[36m[2023-06-25 11:42:02,487][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 11:42:02,487][129146] FPS: 331961.92
[36m[2023-06-25 11:42:07,230][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:42:07,230][129146] Reward + Measures: [[1478.20689183    0.82922804    0.43193701    0.21088232    0.23548901]]
[37m[1m[2023-06-25 11:42:07,231][129146] Max Reward on eval: 1478.2068918322357
[37m[1m[2023-06-25 11:42:07,231][129146] Min Reward on eval: 1478.2068918322357
[37m[1m[2023-06-25 11:42:07,231][129146] Mean Reward across all agents: 1478.2068918322357
[37m[1m[2023-06-25 11:42:07,231][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:42:12,843][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:42:12,844][129146] Reward + Measures: [[ 411.56570512    0.83059996    0.2613        0.64989996    0.5399    ]
[37m[1m [ 819.63938767    0.70060009    0.50669998    0.24520002    0.4165    ]
[37m[1m [-145.98814002    0.98379993    0.005         0.98780006    0.9091    ]
[37m[1m ...
[37m[1m [1084.12871673    0.86830008    0.31240001    0.45380002    0.3513    ]
[37m[1m [1240.91348017    0.76900005    0.58100003    0.1947        0.2757    ]
[37m[1m [ 680.63822982    0.73660004    0.3653        0.45100003    0.49959999]]
[37m[1m[2023-06-25 11:42:12,844][129146] Max Reward on eval: 1638.9759638005169
[37m[1m[2023-06-25 11:42:12,844][129146] Min Reward on eval: -604.6420458746841
[37m[1m[2023-06-25 11:42:12,844][129146] Mean Reward across all agents: 468.07311941516616
[37m[1m[2023-06-25 11:42:12,844][129146] Average Trajectory Length: 995.6353333333333
[36m[2023-06-25 11:42:12,850][129146] mean_value=-304.4845084916046, max_value=1727.1129773284542
[37m[1m[2023-06-25 11:42:12,853][129146] New mean coefficients: [[-0.3317843   2.7743945  -1.345176    0.16486868 -0.14455193]]
[37m[1m[2023-06-25 11:42:12,854][129146] Moving the mean solution point...
[36m[2023-06-25 11:42:22,517][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 11:42:22,518][129146] FPS: 397459.85
[36m[2023-06-25 11:42:22,520][129146] itr=1171, itrs=2000, Progress: 58.55%
[36m[2023-06-25 11:42:34,048][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 11:42:34,049][129146] FPS: 333782.24
[36m[2023-06-25 11:42:38,740][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:42:38,740][129146] Reward + Measures: [[1171.47894498    0.85566503    0.33374134    0.36493164    0.31829068]]
[37m[1m[2023-06-25 11:42:38,741][129146] Max Reward on eval: 1171.4789449812954
[37m[1m[2023-06-25 11:42:38,741][129146] Min Reward on eval: 1171.4789449812954
[37m[1m[2023-06-25 11:42:38,741][129146] Mean Reward across all agents: 1171.4789449812954
[37m[1m[2023-06-25 11:42:38,742][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:42:44,117][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:42:44,118][129146] Reward + Measures: [[1361.93074465    0.71679997    0.47219998    0.2026        0.27940002]
[37m[1m [1180.70354521    0.71799999    0.40799999    0.2027        0.32179999]
[37m[1m [1121.15728375    0.74120003    0.43380004    0.375         0.24759999]
[37m[1m ...
[37m[1m [1089.11043389    0.55830002    0.38789999    0.25210002    0.27360001]
[37m[1m [ 362.76600688    0.5133        0.4786        0.22909999    0.32179999]
[37m[1m [ 929.68817872    0.81379998    0.28850001    0.48010001    0.38820001]]
[37m[1m[2023-06-25 11:42:44,118][129146] Max Reward on eval: 1701.544931793306
[37m[1m[2023-06-25 11:42:44,118][129146] Min Reward on eval: -594.801798487478
[37m[1m[2023-06-25 11:42:44,119][129146] Mean Reward across all agents: 708.2810538018653
[37m[1m[2023-06-25 11:42:44,119][129146] Average Trajectory Length: 999.4826666666667
[36m[2023-06-25 11:42:44,125][129146] mean_value=-18.794296287062703, max_value=1693.7036270261742
[37m[1m[2023-06-25 11:42:44,128][129146] New mean coefficients: [[-0.28540713  3.0649025  -0.97127885 -0.25002226 -0.05003662]]
[37m[1m[2023-06-25 11:42:44,129][129146] Moving the mean solution point...
[36m[2023-06-25 11:42:53,947][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 11:42:53,947][129146] FPS: 391168.37
[36m[2023-06-25 11:42:53,949][129146] itr=1172, itrs=2000, Progress: 58.60%
[36m[2023-06-25 11:43:05,585][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 11:43:05,585][129146] FPS: 330805.19
[36m[2023-06-25 11:43:10,367][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:43:10,368][129146] Reward + Measures: [[-58.27843525   0.95750803   0.023547     0.93925935   0.74544466]]
[37m[1m[2023-06-25 11:43:10,368][129146] Max Reward on eval: -58.27843525095355
[37m[1m[2023-06-25 11:43:10,368][129146] Min Reward on eval: -58.27843525095355
[37m[1m[2023-06-25 11:43:10,368][129146] Mean Reward across all agents: -58.27843525095355
[37m[1m[2023-06-25 11:43:10,368][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:43:15,759][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:43:15,765][129146] Reward + Measures: [[405.74004186   0.85089999   0.1293       0.72030002   0.47080001]
[37m[1m [271.4721604    0.76370001   0.15869999   0.64499998   0.35700002]
[37m[1m [759.60199737   0.58770001   0.25209999   0.42790005   0.30180001]
[37m[1m ...
[37m[1m [817.79849556   0.65679997   0.27050003   0.44570002   0.35969999]
[37m[1m [884.6958538    0.45339999   0.39649996   0.15250002   0.39320001]
[37m[1m [880.34097811   0.46680003   0.2906       0.33070001   0.30770001]]
[37m[1m[2023-06-25 11:43:15,765][129146] Max Reward on eval: 1392.5960164626827
[37m[1m[2023-06-25 11:43:15,766][129146] Min Reward on eval: -1071.4505407528952
[37m[1m[2023-06-25 11:43:15,766][129146] Mean Reward across all agents: 261.77110730621763
[37m[1m[2023-06-25 11:43:15,766][129146] Average Trajectory Length: 982.9613333333333
[36m[2023-06-25 11:43:15,768][129146] mean_value=-559.0223396515103, max_value=655.0837428700092
[37m[1m[2023-06-25 11:43:15,771][129146] New mean coefficients: [[-0.3384016   3.3860216  -0.4447719  -0.20577157 -0.37277317]]
[37m[1m[2023-06-25 11:43:15,772][129146] Moving the mean solution point...
[36m[2023-06-25 11:43:25,465][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 11:43:25,465][129146] FPS: 396252.04
[36m[2023-06-25 11:43:25,467][129146] itr=1173, itrs=2000, Progress: 58.65%
[36m[2023-06-25 11:43:36,876][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 11:43:36,877][129146] FPS: 337361.06
[36m[2023-06-25 11:43:41,594][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:43:41,595][129146] Reward + Measures: [[-213.70392853    0.99297833    0.00200133    0.99059528    0.94877529]]
[37m[1m[2023-06-25 11:43:41,595][129146] Max Reward on eval: -213.7039285340777
[37m[1m[2023-06-25 11:43:41,595][129146] Min Reward on eval: -213.7039285340777
[37m[1m[2023-06-25 11:43:41,595][129146] Mean Reward across all agents: -213.7039285340777
[37m[1m[2023-06-25 11:43:41,595][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:43:47,092][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:43:47,093][129146] Reward + Measures: [[   50.73856832     0.93559998     0.0176         0.92640001
[37m[1m      0.69060004]
[37m[1m [-1130.42512139     0.98829997     0.0034         0.99130005
[37m[1m      0.93530005]
[37m[1m [ -434.59468068     0.96880001     0.0098         0.97609997
[37m[1m      0.82870001]
[37m[1m ...
[37m[1m [ -567.10349906     0.98780006     0.0038         0.98619998
[37m[1m      0.92559999]
[37m[1m [  281.91475322     0.64270002     0.32909998     0.32479998
[37m[1m      0.26050001]
[37m[1m [  -94.91684104     0.97659999     0.0016         0.98159999
[37m[1m      0.91820002]]
[37m[1m[2023-06-25 11:43:47,093][129146] Max Reward on eval: 1026.9959081118693
[37m[1m[2023-06-25 11:43:47,094][129146] Min Reward on eval: -1130.4251213920768
[37m[1m[2023-06-25 11:43:47,094][129146] Mean Reward across all agents: -124.68935224401476
[37m[1m[2023-06-25 11:43:47,094][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:43:47,095][129146] mean_value=-513.6576583978218, max_value=335.3761874854622
[37m[1m[2023-06-25 11:43:47,098][129146] New mean coefficients: [[-0.86408114  3.437273   -0.4849921   0.24189961  0.08627138]]
[37m[1m[2023-06-25 11:43:47,099][129146] Moving the mean solution point...
[36m[2023-06-25 11:43:56,794][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 11:43:56,794][129146] FPS: 396151.14
[36m[2023-06-25 11:43:56,796][129146] itr=1174, itrs=2000, Progress: 58.70%
[36m[2023-06-25 11:44:08,223][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 11:44:08,223][129146] FPS: 336829.86
[36m[2023-06-25 11:44:13,071][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:44:13,071][129146] Reward + Measures: [[-745.1847708     0.95397455    0.03911713    0.92163241    0.64463747]]
[37m[1m[2023-06-25 11:44:13,071][129146] Max Reward on eval: -745.1847708032748
[37m[1m[2023-06-25 11:44:13,072][129146] Min Reward on eval: -745.1847708032748
[37m[1m[2023-06-25 11:44:13,072][129146] Mean Reward across all agents: -745.1847708032748
[37m[1m[2023-06-25 11:44:13,072][129146] Average Trajectory Length: 999.9989999999999
[36m[2023-06-25 11:44:18,478][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:44:18,484][129146] Reward + Measures: [[ -808.79338048     0.93580008     0.0251         0.94510001
[37m[1m      0.77100003]
[37m[1m [ -875.70736201     0.93580002     0.0227         0.95699996
[37m[1m      0.62880003]
[37m[1m [ -479.19335193     0.90960008     0.06300001     0.89000005
[37m[1m      0.79640001]
[37m[1m ...
[37m[1m [-1198.10792212     0.9594         0.0149         0.98619998
[37m[1m      0.63420004]
[37m[1m [-1006.38004563     0.97719997     0.0024         0.98250002
[37m[1m      0.7518    ]
[37m[1m [ -891.6979531      0.96159995     0.0115         0.97830003
[37m[1m      0.65439999]]
[37m[1m[2023-06-25 11:44:18,484][129146] Max Reward on eval: -387.32782719929236
[37m[1m[2023-06-25 11:44:18,484][129146] Min Reward on eval: -1280.0792941402644
[37m[1m[2023-06-25 11:44:18,485][129146] Mean Reward across all agents: -898.688155543692
[37m[1m[2023-06-25 11:44:18,485][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:44:18,486][129146] mean_value=-1209.8083203296628, max_value=-58.13290924240113
[36m[2023-06-25 11:44:18,488][129146] XNES is restarting with a new solution whose measures are [0.65979999 0.46529999 0.85270005 0.56980002] and objective is 383.38474664641546
[36m[2023-06-25 11:44:18,490][129146] Emitter restarted. Changing the mean agent...
[37m[1m[2023-06-25 11:44:18,492][129146] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]]
[37m[1m[2023-06-25 11:44:18,493][129146] Moving the mean solution point...
[36m[2023-06-25 11:44:28,265][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 11:44:28,265][129146] FPS: 393017.24
[36m[2023-06-25 11:44:28,268][129146] itr=1175, itrs=2000, Progress: 58.75%
[36m[2023-06-25 11:44:39,696][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 11:44:39,697][129146] FPS: 336675.94
[36m[2023-06-25 11:44:44,571][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:44:44,571][129146] Reward + Measures: [[239.0467969    0.44621733   0.66134131   0.84529245   0.61752725]]
[37m[1m[2023-06-25 11:44:44,571][129146] Max Reward on eval: 239.04679690472815
[37m[1m[2023-06-25 11:44:44,572][129146] Min Reward on eval: 239.04679690472815
[37m[1m[2023-06-25 11:44:44,572][129146] Mean Reward across all agents: 239.04679690472815
[37m[1m[2023-06-25 11:44:44,572][129146] Average Trajectory Length: 999.7163333333333
[36m[2023-06-25 11:44:50,173][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:44:50,174][129146] Reward + Measures: [[  403.75954205     0.43720004     0.6936         0.72849995
[37m[1m      0.59320003]
[37m[1m [ -925.45440568     0.57609999     0.15009999     0.58770007
[37m[1m      0.12630001]
[37m[1m [ -143.94506388     0.70006478     0.2891624      0.67736018
[37m[1m      0.17430116]
[37m[1m ...
[37m[1m [ -441.28044805     0.39885455     0.31351817     0.34970003
[37m[1m      0.25205454]
[37m[1m [-1007.09898781     0.71910006     0.2253         0.6875
[37m[1m      0.096     ]
[37m[1m [ -604.53462941     0.30577156     0.31032735     0.27498609
[37m[1m      0.32174215]]
[37m[1m[2023-06-25 11:44:50,174][129146] Max Reward on eval: 403.759542051598
[37m[1m[2023-06-25 11:44:50,175][129146] Min Reward on eval: -1426.85222076911
[37m[1m[2023-06-25 11:44:50,175][129146] Mean Reward across all agents: -334.4807992192554
[37m[1m[2023-06-25 11:44:50,175][129146] Average Trajectory Length: 965.434
[36m[2023-06-25 11:44:50,178][129146] mean_value=-1112.323529158914, max_value=723.5000901811829
[37m[1m[2023-06-25 11:44:50,180][129146] New mean coefficients: [[-0.1762782   0.1005336  -0.13924545 -1.6880999  -0.5207825 ]]
[37m[1m[2023-06-25 11:44:50,181][129146] Moving the mean solution point...
[36m[2023-06-25 11:44:59,890][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 11:44:59,890][129146] FPS: 395595.44
[36m[2023-06-25 11:44:59,892][129146] itr=1176, itrs=2000, Progress: 58.80%
[36m[2023-06-25 11:45:11,365][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 11:45:11,365][129146] FPS: 335505.32
[36m[2023-06-25 11:45:16,148][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:45:16,148][129146] Reward + Measures: [[315.84138077   0.59291697   0.63721436   0.80491567   0.32753134]]
[37m[1m[2023-06-25 11:45:16,148][129146] Max Reward on eval: 315.8413807661013
[37m[1m[2023-06-25 11:45:16,149][129146] Min Reward on eval: 315.8413807661013
[37m[1m[2023-06-25 11:45:16,149][129146] Mean Reward across all agents: 315.8413807661013
[37m[1m[2023-06-25 11:45:16,149][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:45:21,529][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:45:21,530][129146] Reward + Measures: [[177.08739025   0.69503558   0.33348575   0.68728828   0.2090544 ]
[37m[1m [148.92126605   0.62280005   0.38030002   0.74580002   0.36939999]
[37m[1m [111.77026555   0.54929256   0.48216295   0.6153037    0.35858151]
[37m[1m ...
[37m[1m [200.8344505    0.55949998   0.50629997   0.72670001   0.47389999]
[37m[1m [204.57111234   0.62180007   0.42460003   0.72320002   0.2958    ]
[37m[1m [412.4521292    0.82170004   0.60980004   0.8071       0.125     ]]
[37m[1m[2023-06-25 11:45:21,530][129146] Max Reward on eval: 506.0462041973369
[37m[1m[2023-06-25 11:45:21,530][129146] Min Reward on eval: -451.1799682305544
[37m[1m[2023-06-25 11:45:21,531][129146] Mean Reward across all agents: 168.21430893217683
[37m[1m[2023-06-25 11:45:21,531][129146] Average Trajectory Length: 998.6429999999999
[36m[2023-06-25 11:45:21,538][129146] mean_value=98.12762408711002, max_value=842.4944299436361
[37m[1m[2023-06-25 11:45:21,541][129146] New mean coefficients: [[ 0.6515262  -0.21568334  0.19671321 -0.61694694 -0.10247278]]
[37m[1m[2023-06-25 11:45:21,542][129146] Moving the mean solution point...
[36m[2023-06-25 11:45:31,204][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 11:45:31,204][129146] FPS: 397515.29
[36m[2023-06-25 11:45:31,207][129146] itr=1177, itrs=2000, Progress: 58.85%
[36m[2023-06-25 11:45:42,741][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 11:45:42,741][129146] FPS: 333630.73
[36m[2023-06-25 11:45:47,508][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:45:47,508][129146] Reward + Measures: [[379.64348147   0.64544594   0.6789217    0.78307503   0.19460608]]
[37m[1m[2023-06-25 11:45:47,509][129146] Max Reward on eval: 379.64348147087355
[37m[1m[2023-06-25 11:45:47,509][129146] Min Reward on eval: 379.64348147087355
[37m[1m[2023-06-25 11:45:47,509][129146] Mean Reward across all agents: 379.64348147087355
[37m[1m[2023-06-25 11:45:47,509][129146] Average Trajectory Length: 999.2133333333333
[36m[2023-06-25 11:45:52,911][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:45:52,916][129146] Reward + Measures: [[ 143.00334494    0.58070004    0.73180002    0.73449999    0.523     ]
[37m[1m [-440.49652959    0.31889999    0.53149998    0.58540004    0.45089999]
[37m[1m [-292.83017296    0.0942        0.81640005    0.81560004    0.80170006]
[37m[1m ...
[37m[1m [ 350.42711184    0.66930002    0.58289999    0.79050004    0.44460002]
[37m[1m [ 445.82786174    0.76789999    0.16940001    0.80800003    0.49680004]
[37m[1m [  42.94544727    0.74650002    0.32750002    0.63970006    0.2145    ]]
[37m[1m[2023-06-25 11:45:52,916][129146] Max Reward on eval: 574.5903074787581
[37m[1m[2023-06-25 11:45:52,917][129146] Min Reward on eval: -833.9889840513002
[37m[1m[2023-06-25 11:45:52,917][129146] Mean Reward across all agents: 59.60907493250423
[37m[1m[2023-06-25 11:45:52,917][129146] Average Trajectory Length: 996.0906666666666
[36m[2023-06-25 11:45:52,924][129146] mean_value=-149.10609649767792, max_value=723.5574151620444
[37m[1m[2023-06-25 11:45:52,927][129146] New mean coefficients: [[ 1.3942211  -0.24167052  0.20079643 -0.32491562 -0.24652134]]
[37m[1m[2023-06-25 11:45:52,928][129146] Moving the mean solution point...
[36m[2023-06-25 11:46:02,644][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 11:46:02,645][129146] FPS: 395266.77
[36m[2023-06-25 11:46:02,647][129146] itr=1178, itrs=2000, Progress: 58.90%
[36m[2023-06-25 11:46:14,432][129146] train() took 11.76 seconds to complete
[36m[2023-06-25 11:46:14,432][129146] FPS: 326545.42
[36m[2023-06-25 11:46:19,184][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:46:19,185][129146] Reward + Measures: [[432.74942684   0.68890429   0.67818069   0.76867002   0.14816296]]
[37m[1m[2023-06-25 11:46:19,185][129146] Max Reward on eval: 432.74942683688806
[37m[1m[2023-06-25 11:46:19,185][129146] Min Reward on eval: 432.74942683688806
[37m[1m[2023-06-25 11:46:19,185][129146] Mean Reward across all agents: 432.74942683688806
[37m[1m[2023-06-25 11:46:19,186][129146] Average Trajectory Length: 999.1743333333333
[36m[2023-06-25 11:46:24,644][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:46:24,645][129146] Reward + Measures: [[322.52936252   0.53750002   0.64750004   0.74730003   0.2942    ]
[37m[1m [438.46244544   0.68340003   0.5485       0.76139998   0.20970002]
[37m[1m [-76.03152181   0.94160002   0.90570003   0.82880002   0.81339997]
[37m[1m ...
[37m[1m [429.34960778   0.68670005   0.5205       0.79170001   0.28029999]
[37m[1m [457.35318636   0.6261       0.52250004   0.63239998   0.18620001]
[37m[1m [329.5382701    0.92810005   0.91660005   0.80079997   0.79420006]]
[37m[1m[2023-06-25 11:46:24,645][129146] Max Reward on eval: 567.0051802741015
[37m[1m[2023-06-25 11:46:24,645][129146] Min Reward on eval: -1779.6191249953117
[37m[1m[2023-06-25 11:46:24,645][129146] Mean Reward across all agents: 96.44221137723892
[37m[1m[2023-06-25 11:46:24,646][129146] Average Trajectory Length: 998.7656666666667
[36m[2023-06-25 11:46:24,654][129146] mean_value=-112.4703583416401, max_value=883.279374445727
[37m[1m[2023-06-25 11:46:24,656][129146] New mean coefficients: [[ 1.2576139  -0.1379179   1.2161524   0.14048892 -0.6847224 ]]
[37m[1m[2023-06-25 11:46:24,658][129146] Moving the mean solution point...
[36m[2023-06-25 11:46:34,378][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 11:46:34,378][129146] FPS: 395117.72
[36m[2023-06-25 11:46:34,381][129146] itr=1179, itrs=2000, Progress: 58.95%
[36m[2023-06-25 11:46:45,903][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 11:46:45,903][129146] FPS: 333987.94
[36m[2023-06-25 11:46:50,673][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:46:50,674][129146] Reward + Measures: [[457.74651638   0.70307487   0.70079541   0.77330124   0.13573426]]
[37m[1m[2023-06-25 11:46:50,674][129146] Max Reward on eval: 457.7465163783541
[37m[1m[2023-06-25 11:46:50,674][129146] Min Reward on eval: 457.7465163783541
[37m[1m[2023-06-25 11:46:50,674][129146] Mean Reward across all agents: 457.7465163783541
[37m[1m[2023-06-25 11:46:50,674][129146] Average Trajectory Length: 999.4316666666666
[36m[2023-06-25 11:46:56,192][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:46:56,193][129146] Reward + Measures: [[  427.91128918     0.77079999     0.43810001     0.85939997
[37m[1m      0.2112    ]
[37m[1m [    1.58569869     0.37729999     0.76880002     0.71050006
[37m[1m      0.60030001]
[37m[1m [ -431.12783827     0.2138         0.85820001     0.89390004
[37m[1m      0.84219998]
[37m[1m ...
[37m[1m [  136.1127851      0.9016         0.27810001     0.87080002
[37m[1m      0.57180005]
[37m[1m [  -71.92427068     0.53570002     0.63309997     0.76799995
[37m[1m      0.433     ]
[37m[1m [-1114.98534796     0.20389998     0.76450002     0.74960005
[37m[1m      0.71670002]]
[37m[1m[2023-06-25 11:46:56,193][129146] Max Reward on eval: 558.0065109178191
[37m[1m[2023-06-25 11:46:56,193][129146] Min Reward on eval: -1433.645536491135
[37m[1m[2023-06-25 11:46:56,194][129146] Mean Reward across all agents: -73.71510200605447
[37m[1m[2023-06-25 11:46:56,194][129146] Average Trajectory Length: 994.1643333333333
[36m[2023-06-25 11:46:56,200][129146] mean_value=-352.8839923989988, max_value=842.1221798461238
[37m[1m[2023-06-25 11:46:56,203][129146] New mean coefficients: [[ 0.69842875  0.50322104  1.2860202   0.7255955  -0.7608078 ]]
[37m[1m[2023-06-25 11:46:56,204][129146] Moving the mean solution point...
[36m[2023-06-25 11:47:06,097][129146] train() took 9.89 seconds to complete
[36m[2023-06-25 11:47:06,097][129146] FPS: 388211.58
[36m[2023-06-25 11:47:06,099][129146] itr=1180, itrs=2000, Progress: 59.00%
[37m[1m[2023-06-25 11:47:13,756][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001160
[36m[2023-06-25 11:47:25,511][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 11:47:25,512][129146] FPS: 332925.69
[36m[2023-06-25 11:47:30,211][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:47:30,211][129146] Reward + Measures: [[477.79546133   0.74458319   0.74598545   0.80977166   0.11654092]]
[37m[1m[2023-06-25 11:47:30,212][129146] Max Reward on eval: 477.79546132991976
[37m[1m[2023-06-25 11:47:30,212][129146] Min Reward on eval: 477.79546132991976
[37m[1m[2023-06-25 11:47:30,212][129146] Mean Reward across all agents: 477.79546132991976
[37m[1m[2023-06-25 11:47:30,212][129146] Average Trajectory Length: 999.7473333333332
[36m[2023-06-25 11:47:35,699][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:47:35,700][129146] Reward + Measures: [[ -11.69457676    0.59060001    0.6753        0.8120001     0.2043    ]
[37m[1m [ 435.01963441    0.63160002    0.67080003    0.708         0.2115    ]
[37m[1m [-851.99087759    0.6943        0.68290001    0.8634001     0.50710005]
[37m[1m ...
[37m[1m [ 267.21212894    0.6882        0.57069999    0.81779999    0.2393    ]
[37m[1m [-211.87490667    0.63410372    0.68186301    0.76481849    0.19805555]
[37m[1m [ 446.15754988    0.6997        0.69849998    0.81540006    0.11780001]]
[37m[1m[2023-06-25 11:47:35,700][129146] Max Reward on eval: 600.7231532439124
[37m[1m[2023-06-25 11:47:35,700][129146] Min Reward on eval: -1273.5400170345442
[37m[1m[2023-06-25 11:47:35,701][129146] Mean Reward across all agents: 65.75581629890735
[37m[1m[2023-06-25 11:47:35,701][129146] Average Trajectory Length: 999.4483333333333
[36m[2023-06-25 11:47:35,708][129146] mean_value=24.979423520892038, max_value=1044.3926039627986
[37m[1m[2023-06-25 11:47:35,710][129146] New mean coefficients: [[ 0.95074034 -0.59198725  1.9802775   1.5657234  -0.05149996]]
[37m[1m[2023-06-25 11:47:35,711][129146] Moving the mean solution point...
[36m[2023-06-25 11:47:45,387][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 11:47:45,388][129146] FPS: 396916.43
[36m[2023-06-25 11:47:45,390][129146] itr=1181, itrs=2000, Progress: 59.05%
[36m[2023-06-25 11:47:56,901][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 11:47:56,901][129146] FPS: 334271.67
[36m[2023-06-25 11:48:01,567][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:48:01,568][129146] Reward + Measures: [[497.14657681   0.76790065   0.78755778   0.83611834   0.11833894]]
[37m[1m[2023-06-25 11:48:01,568][129146] Max Reward on eval: 497.14657680720137
[37m[1m[2023-06-25 11:48:01,568][129146] Min Reward on eval: 497.14657680720137
[37m[1m[2023-06-25 11:48:01,568][129146] Mean Reward across all agents: 497.14657680720137
[37m[1m[2023-06-25 11:48:01,569][129146] Average Trajectory Length: 999.4156666666667
[36m[2023-06-25 11:48:07,110][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:48:07,110][129146] Reward + Measures: [[-423.74997904    0.96540004    0.96680003    0.97900003    0.97210008]
[37m[1m [ 120.80234493    0.44840002    0.85670006    0.82499999    0.73830003]
[37m[1m [ 318.56578307    0.63389999    0.2906        0.57400006    0.18610001]
[37m[1m ...
[37m[1m [ 446.12350142    0.75660002    0.59750003    0.76660001    0.15360001]
[37m[1m [ 334.83888022    0.51210004    0.32399997    0.47269997    0.20350002]
[37m[1m [ 287.15258789    0.57980007    0.33520001    0.56020004    0.26620004]]
[37m[1m[2023-06-25 11:48:07,110][129146] Max Reward on eval: 573.2109004162136
[37m[1m[2023-06-25 11:48:07,111][129146] Min Reward on eval: -589.1739708941896
[37m[1m[2023-06-25 11:48:07,111][129146] Mean Reward across all agents: 258.61702887401304
[37m[1m[2023-06-25 11:48:07,111][129146] Average Trajectory Length: 999.2909999999999
[36m[2023-06-25 11:48:07,118][129146] mean_value=18.42291904089946, max_value=949.3148434263305
[37m[1m[2023-06-25 11:48:07,121][129146] New mean coefficients: [[ 1.0930531  -0.9723161   1.6507514   2.2808597  -0.27147624]]
[37m[1m[2023-06-25 11:48:07,122][129146] Moving the mean solution point...
[36m[2023-06-25 11:48:16,931][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 11:48:16,931][129146] FPS: 391533.53
[36m[2023-06-25 11:48:16,934][129146] itr=1182, itrs=2000, Progress: 59.10%
[36m[2023-06-25 11:48:28,463][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 11:48:28,463][129146] FPS: 333793.90
[36m[2023-06-25 11:48:33,288][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:48:33,288][129146] Reward + Measures: [[425.68231914   0.56942731   0.63790566   0.72891903   0.30340162]]
[37m[1m[2023-06-25 11:48:33,288][129146] Max Reward on eval: 425.68231914391305
[37m[1m[2023-06-25 11:48:33,289][129146] Min Reward on eval: 425.68231914391305
[37m[1m[2023-06-25 11:48:33,289][129146] Mean Reward across all agents: 425.68231914391305
[37m[1m[2023-06-25 11:48:33,289][129146] Average Trajectory Length: 999.7356666666666
[36m[2023-06-25 11:48:38,882][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:48:38,882][129146] Reward + Measures: [[522.1119019    0.65320009   0.65060002   0.70590001   0.17459999]
[37m[1m [398.05394207   0.55109996   0.42910001   0.65110004   0.2836    ]
[37m[1m [361.46201779   0.42589998   0.36829996   0.46560001   0.3161    ]
[37m[1m ...
[37m[1m [336.77770066   0.65979999   0.57570004   0.67300004   0.60340005]
[37m[1m [267.58928041   0.52019995   0.23810001   0.5654       0.30580002]
[37m[1m [349.96422563   0.66580003   0.67270005   0.70900005   0.62290001]]
[37m[1m[2023-06-25 11:48:38,883][129146] Max Reward on eval: 610.537984886719
[37m[1m[2023-06-25 11:48:38,883][129146] Min Reward on eval: -445.74523628901807
[37m[1m[2023-06-25 11:48:38,883][129146] Mean Reward across all agents: 267.83784177067196
[37m[1m[2023-06-25 11:48:38,884][129146] Average Trajectory Length: 999.0006666666667
[36m[2023-06-25 11:48:38,891][129146] mean_value=124.60927556653509, max_value=837.6142609284609
[37m[1m[2023-06-25 11:48:38,894][129146] New mean coefficients: [[ 0.7563033  -0.4066773   2.2404323   1.8569298   0.06195611]]
[37m[1m[2023-06-25 11:48:38,895][129146] Moving the mean solution point...
[36m[2023-06-25 11:48:48,653][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 11:48:48,653][129146] FPS: 393605.18
[36m[2023-06-25 11:48:48,655][129146] itr=1183, itrs=2000, Progress: 59.15%
[36m[2023-06-25 11:49:00,191][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 11:49:00,191][129146] FPS: 333598.72
[36m[2023-06-25 11:49:04,983][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:49:04,984][129146] Reward + Measures: [[322.58124522   0.38794312   0.78889996   0.7821691    0.52629393]]
[37m[1m[2023-06-25 11:49:04,984][129146] Max Reward on eval: 322.581245220863
[37m[1m[2023-06-25 11:49:04,984][129146] Min Reward on eval: 322.581245220863
[37m[1m[2023-06-25 11:49:04,984][129146] Mean Reward across all agents: 322.581245220863
[37m[1m[2023-06-25 11:49:04,985][129146] Average Trajectory Length: 999.7276666666667
[36m[2023-06-25 11:49:10,439][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:49:10,440][129146] Reward + Measures: [[155.6943131    0.29969999   0.9018001    0.85869998   0.77520007]
[37m[1m [569.64105034   0.65610003   0.67610002   0.70350003   0.1568    ]
[37m[1m [296.3344057    0.4664       0.79189998   0.82420009   0.53439999]
[37m[1m ...
[37m[1m [284.95495011   0.63869995   0.69100004   0.8915       0.35569999]
[37m[1m [117.86156176   0.10390001   0.91750002   0.81720001   0.82800007]
[37m[1m [445.24964427   0.61690009   0.72960001   0.70230001   0.26100001]]
[37m[1m[2023-06-25 11:49:10,440][129146] Max Reward on eval: 583.7034327255503
[37m[1m[2023-06-25 11:49:10,440][129146] Min Reward on eval: -208.78451095868368
[37m[1m[2023-06-25 11:49:10,440][129146] Mean Reward across all agents: 320.2762498508787
[37m[1m[2023-06-25 11:49:10,441][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:49:10,451][129146] mean_value=375.13390604638033, max_value=1001.6535735163955
[37m[1m[2023-06-25 11:49:10,454][129146] New mean coefficients: [[ 1.7037058  -0.3142304   1.8112749   3.188379   -0.38852856]]
[37m[1m[2023-06-25 11:49:10,455][129146] Moving the mean solution point...
[36m[2023-06-25 11:49:20,150][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 11:49:20,150][129146] FPS: 396149.03
[36m[2023-06-25 11:49:20,152][129146] itr=1184, itrs=2000, Progress: 59.20%
[36m[2023-06-25 11:49:31,934][129146] train() took 11.76 seconds to complete
[36m[2023-06-25 11:49:31,934][129146] FPS: 326651.09
[36m[2023-06-25 11:49:36,691][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:49:36,692][129146] Reward + Measures: [[353.40913953   0.39813867   0.81069744   0.79066795   0.55375713]]
[37m[1m[2023-06-25 11:49:36,692][129146] Max Reward on eval: 353.4091395261201
[37m[1m[2023-06-25 11:49:36,692][129146] Min Reward on eval: 353.4091395261201
[37m[1m[2023-06-25 11:49:36,693][129146] Mean Reward across all agents: 353.4091395261201
[37m[1m[2023-06-25 11:49:36,693][129146] Average Trajectory Length: 999.6963333333333
[36m[2023-06-25 11:49:42,396][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:49:42,397][129146] Reward + Measures: [[552.9774052    0.66530001   0.59180003   0.76550001   0.28529999]
[37m[1m [609.41265217   0.741        0.55050009   0.77590001   0.23380001]
[37m[1m [507.35813253   0.65390003   0.51800007   0.78000003   0.36310002]
[37m[1m ...
[37m[1m [611.96590767   0.71579999   0.63590002   0.77790004   0.23860002]
[37m[1m [455.46870797   0.40009999   0.83920002   0.66900009   0.60400003]
[37m[1m [371.35071248   0.4677       0.72530001   0.75690001   0.4883    ]]
[37m[1m[2023-06-25 11:49:42,397][129146] Max Reward on eval: 668.1480501121957
[37m[1m[2023-06-25 11:49:42,397][129146] Min Reward on eval: -76.66807827957673
[37m[1m[2023-06-25 11:49:42,397][129146] Mean Reward across all agents: 403.3529011064105
[37m[1m[2023-06-25 11:49:42,398][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:49:42,407][129146] mean_value=252.86180288042146, max_value=997.365667298597
[37m[1m[2023-06-25 11:49:42,410][129146] New mean coefficients: [[ 1.2502887  -0.93866074  2.4144456   3.083111   -0.59055233]]
[37m[1m[2023-06-25 11:49:42,411][129146] Moving the mean solution point...
[36m[2023-06-25 11:49:52,209][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 11:49:52,209][129146] FPS: 391961.73
[36m[2023-06-25 11:49:52,212][129146] itr=1185, itrs=2000, Progress: 59.25%
[36m[2023-06-25 11:50:03,667][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 11:50:03,667][129146] FPS: 335949.29
[36m[2023-06-25 11:50:08,572][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:50:08,573][129146] Reward + Measures: [[397.06533573   0.37656868   0.85495603   0.79592872   0.60124135]]
[37m[1m[2023-06-25 11:50:08,573][129146] Max Reward on eval: 397.0653357284125
[37m[1m[2023-06-25 11:50:08,573][129146] Min Reward on eval: 397.0653357284125
[37m[1m[2023-06-25 11:50:08,573][129146] Mean Reward across all agents: 397.0653357284125
[37m[1m[2023-06-25 11:50:08,573][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:50:14,021][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:50:14,022][129146] Reward + Measures: [[213.51469782   0.44670001   0.75740004   0.87420005   0.56889999]
[37m[1m [437.68511727   0.70410001   0.78439999   0.86520004   0.31209999]
[37m[1m [580.15643087   0.77870005   0.64440006   0.83100003   0.16820002]
[37m[1m ...
[37m[1m [346.27697555   0.58779997   0.72760004   0.86580002   0.39410001]
[37m[1m [443.25847703   0.7895       0.65239996   0.88759995   0.3053    ]
[37m[1m [511.74593386   0.65100002   0.85060006   0.70110005   0.40270004]]
[37m[1m[2023-06-25 11:50:14,022][129146] Max Reward on eval: 605.3775989495159
[37m[1m[2023-06-25 11:50:14,022][129146] Min Reward on eval: 22.615715681225993
[37m[1m[2023-06-25 11:50:14,022][129146] Mean Reward across all agents: 384.759164665662
[37m[1m[2023-06-25 11:50:14,023][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:50:14,032][129146] mean_value=378.05903638656895, max_value=1036.5379136579345
[37m[1m[2023-06-25 11:50:14,035][129146] New mean coefficients: [[ 1.858484    0.03763658  3.3136086   3.6424267  -0.41203642]]
[37m[1m[2023-06-25 11:50:14,036][129146] Moving the mean solution point...
[36m[2023-06-25 11:50:23,958][129146] train() took 9.92 seconds to complete
[36m[2023-06-25 11:50:23,958][129146] FPS: 387082.13
[36m[2023-06-25 11:50:23,960][129146] itr=1186, itrs=2000, Progress: 59.30%
[36m[2023-06-25 11:50:35,373][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 11:50:35,373][129146] FPS: 337265.71
[36m[2023-06-25 11:50:40,160][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:50:40,160][129146] Reward + Measures: [[347.20756858   0.25748798   0.90012336   0.81472898   0.75387973]]
[37m[1m[2023-06-25 11:50:40,160][129146] Max Reward on eval: 347.207568583136
[37m[1m[2023-06-25 11:50:40,161][129146] Min Reward on eval: 347.207568583136
[37m[1m[2023-06-25 11:50:40,161][129146] Mean Reward across all agents: 347.207568583136
[37m[1m[2023-06-25 11:50:40,161][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:50:45,624][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:50:45,625][129146] Reward + Measures: [[169.06880496   0.2904       0.88529998   0.80120003   0.77960008]
[37m[1m [382.81730153   0.33360001   0.86730003   0.83500004   0.75789994]
[37m[1m [308.77990542   0.26390001   0.92220002   0.86940002   0.73369998]
[37m[1m ...
[37m[1m [ 77.24719193   0.1628       0.9077       0.81560004   0.87199992]
[37m[1m [ 87.15837059   0.0913       0.9465       0.87260002   0.92940009]
[37m[1m [210.45706114   0.0913       0.95100003   0.88050002   0.92390007]]
[37m[1m[2023-06-25 11:50:45,625][129146] Max Reward on eval: 648.7176450069295
[37m[1m[2023-06-25 11:50:45,626][129146] Min Reward on eval: -113.98676998201991
[37m[1m[2023-06-25 11:50:45,626][129146] Mean Reward across all agents: 248.5116355627101
[37m[1m[2023-06-25 11:50:45,626][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:50:45,631][129146] mean_value=10.6820126025978, max_value=1009.6660350418028
[37m[1m[2023-06-25 11:50:45,634][129146] New mean coefficients: [[ 1.6750703  -1.4457035   2.1355233   2.7382588  -0.89901483]]
[37m[1m[2023-06-25 11:50:45,635][129146] Moving the mean solution point...
[36m[2023-06-25 11:50:55,430][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 11:50:55,430][129146] FPS: 392109.45
[36m[2023-06-25 11:50:55,432][129146] itr=1187, itrs=2000, Progress: 59.35%
[36m[2023-06-25 11:51:06,989][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 11:51:06,989][129146] FPS: 333061.93
[36m[2023-06-25 11:51:11,807][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:51:11,808][129146] Reward + Measures: [[478.19578594   0.36978766   0.63231963   0.63637197   0.57812798]]
[37m[1m[2023-06-25 11:51:11,808][129146] Max Reward on eval: 478.1957859394995
[37m[1m[2023-06-25 11:51:11,808][129146] Min Reward on eval: 478.1957859394995
[37m[1m[2023-06-25 11:51:11,808][129146] Mean Reward across all agents: 478.1957859394995
[37m[1m[2023-06-25 11:51:11,809][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:51:17,355][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:51:17,356][129146] Reward + Measures: [[-405.89484312    0.4382        0.60170001    0.55910003    0.62270004]
[37m[1m [ 360.69662992    0.66500008    0.45030004    0.71810001    0.25050002]
[37m[1m [-459.02971931    0.44490004    0.75270003    0.71630001    0.7719    ]
[37m[1m ...
[37m[1m [ 375.39007463    0.80179995    0.78000003    0.85720009    0.62040001]
[37m[1m [ -80.05251272    0.16200002    0.79369998    0.79750007    0.80130005]
[37m[1m [ 409.52699894    0.204         0.51809996    0.35630003    0.65940005]]
[37m[1m[2023-06-25 11:51:17,356][129146] Max Reward on eval: 573.1470676257625
[37m[1m[2023-06-25 11:51:17,357][129146] Min Reward on eval: -1092.2242415784742
[37m[1m[2023-06-25 11:51:17,357][129146] Mean Reward across all agents: 36.499425176993995
[37m[1m[2023-06-25 11:51:17,357][129146] Average Trajectory Length: 988.9993333333333
[36m[2023-06-25 11:51:17,362][129146] mean_value=-261.9197828168597, max_value=699.4017056019776
[37m[1m[2023-06-25 11:51:17,365][129146] New mean coefficients: [[ 2.0737169   0.43298066  1.6880555   3.2391522  -0.7208887 ]]
[37m[1m[2023-06-25 11:51:17,366][129146] Moving the mean solution point...
[36m[2023-06-25 11:51:27,014][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 11:51:27,014][129146] FPS: 398082.78
[36m[2023-06-25 11:51:27,016][129146] itr=1188, itrs=2000, Progress: 59.40%
[36m[2023-06-25 11:51:38,527][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 11:51:38,527][129146] FPS: 334288.64
[36m[2023-06-25 11:51:43,286][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:51:43,287][129146] Reward + Measures: [[45.66270904  0.89407533  0.9883467   0.95651531  0.96348035]]
[37m[1m[2023-06-25 11:51:43,287][129146] Max Reward on eval: 45.66270904133359
[37m[1m[2023-06-25 11:51:43,287][129146] Min Reward on eval: 45.66270904133359
[37m[1m[2023-06-25 11:51:43,288][129146] Mean Reward across all agents: 45.66270904133359
[37m[1m[2023-06-25 11:51:43,288][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:51:48,778][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:51:48,779][129146] Reward + Measures: [[179.47871916   0.48610002   0.95720005   0.84819996   0.85290003]
[37m[1m [208.03221897   0.0207       0.89659995   0.82420009   0.75760001]
[37m[1m [216.099089     0.026        0.86949998   0.78380001   0.73449999]
[37m[1m ...
[37m[1m [174.81533807   0.0208       0.85670006   0.8125       0.74079996]
[37m[1m [230.9033333    0.0304       0.85350001   0.79950005   0.71610004]
[37m[1m [240.5301889    0.0312       0.90560001   0.8077001    0.75730002]]
[37m[1m[2023-06-25 11:51:48,779][129146] Max Reward on eval: 397.027530378825
[37m[1m[2023-06-25 11:51:48,779][129146] Min Reward on eval: -87.12691837563179
[37m[1m[2023-06-25 11:51:48,780][129146] Mean Reward across all agents: 183.0498576394813
[37m[1m[2023-06-25 11:51:48,780][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:51:48,784][129146] mean_value=19.396252914073898, max_value=771.713453083589
[37m[1m[2023-06-25 11:51:48,787][129146] New mean coefficients: [[ 2.1311526  -0.88537323  1.3936788   3.207761   -1.1597242 ]]
[37m[1m[2023-06-25 11:51:48,788][129146] Moving the mean solution point...
[36m[2023-06-25 11:51:58,572][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 11:51:58,572][129146] FPS: 392553.40
[36m[2023-06-25 11:51:58,575][129146] itr=1189, itrs=2000, Progress: 59.45%
[36m[2023-06-25 11:52:10,056][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 11:52:10,056][129146] FPS: 335152.19
[36m[2023-06-25 11:52:14,771][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:52:14,771][129146] Reward + Measures: [[294.04625747   0.70124429   0.38904768   0.69160235   0.61658603]]
[37m[1m[2023-06-25 11:52:14,772][129146] Max Reward on eval: 294.04625747282074
[37m[1m[2023-06-25 11:52:14,772][129146] Min Reward on eval: 294.04625747282074
[37m[1m[2023-06-25 11:52:14,772][129146] Mean Reward across all agents: 294.04625747282074
[37m[1m[2023-06-25 11:52:14,772][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:52:20,361][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:52:20,362][129146] Reward + Measures: [[358.22688703   0.71990007   0.5643       0.71219999   0.35690001]
[37m[1m [392.73048975   0.57599998   0.55760002   0.77940005   0.72960001]
[37m[1m [415.72359534   0.5029       0.41629997   0.71169996   0.76410002]
[37m[1m ...
[37m[1m [357.73647082   0.50490004   0.42750001   0.65679997   0.66920006]
[37m[1m [201.93988335   0.66580003   0.63310003   0.69020003   0.68190002]
[37m[1m [267.42344932   0.74419999   0.41360003   0.72100002   0.47769997]]
[37m[1m[2023-06-25 11:52:20,362][129146] Max Reward on eval: 506.2256608087453
[37m[1m[2023-06-25 11:52:20,362][129146] Min Reward on eval: 16.769429102301366
[37m[1m[2023-06-25 11:52:20,362][129146] Mean Reward across all agents: 325.72292098137393
[37m[1m[2023-06-25 11:52:20,363][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:52:20,370][129146] mean_value=281.1982039470586, max_value=916.3544174186303
[37m[1m[2023-06-25 11:52:20,373][129146] New mean coefficients: [[ 3.5437422  -0.45983738  0.6196834   4.052684   -0.6271808 ]]
[37m[1m[2023-06-25 11:52:20,374][129146] Moving the mean solution point...
[36m[2023-06-25 11:52:30,111][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 11:52:30,111][129146] FPS: 394452.36
[36m[2023-06-25 11:52:30,113][129146] itr=1190, itrs=2000, Progress: 59.50%
[37m[1m[2023-06-25 11:52:37,271][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001170
[36m[2023-06-25 11:52:48,888][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 11:52:48,889][129146] FPS: 337516.90
[36m[2023-06-25 11:52:53,686][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:52:53,686][129146] Reward + Measures: [[399.61963302   0.61353403   0.72899562   0.60226828   0.83784002]]
[37m[1m[2023-06-25 11:52:53,686][129146] Max Reward on eval: 399.6196330173188
[37m[1m[2023-06-25 11:52:53,687][129146] Min Reward on eval: 399.6196330173188
[37m[1m[2023-06-25 11:52:53,687][129146] Mean Reward across all agents: 399.6196330173188
[37m[1m[2023-06-25 11:52:53,687][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:52:59,203][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:52:59,204][129146] Reward + Measures: [[ -583.0242155      0.70710003     0.49050006     0.38730001
[37m[1m      0.56110001]
[37m[1m [ -744.51799832     0.93829995     0.91720003     0.0759
[37m[1m      0.92060006]
[37m[1m [-1168.90671891     0.94490004     0.98600006     0.0049
[37m[1m      0.9896    ]
[37m[1m ...
[37m[1m [  339.47373763     0.2493         0.40790001     0.40159997
[37m[1m      0.47680002]
[37m[1m [  160.08376713     0.82550001     0.76179999     0.31770003
[37m[1m      0.67640001]
[37m[1m [  -43.82664793     0.67799997     0.44209996     0.5309
[37m[1m      0.45179996]]
[37m[1m[2023-06-25 11:52:59,204][129146] Max Reward on eval: 389.044965448766
[37m[1m[2023-06-25 11:52:59,204][129146] Min Reward on eval: -1312.407962179277
[37m[1m[2023-06-25 11:52:59,205][129146] Mean Reward across all agents: -213.72734455568707
[37m[1m[2023-06-25 11:52:59,205][129146] Average Trajectory Length: 995.0853333333333
[36m[2023-06-25 11:52:59,209][129146] mean_value=-465.09202793517323, max_value=793.3660345171113
[37m[1m[2023-06-25 11:52:59,211][129146] New mean coefficients: [[ 2.1228209   0.50048727  0.6779713   3.6392083  -0.97291696]]
[37m[1m[2023-06-25 11:52:59,212][129146] Moving the mean solution point...
[36m[2023-06-25 11:53:08,841][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 11:53:08,842][129146] FPS: 398862.83
[36m[2023-06-25 11:53:08,844][129146] itr=1191, itrs=2000, Progress: 59.55%
[36m[2023-06-25 11:53:20,395][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 11:53:20,395][129146] FPS: 333149.00
[36m[2023-06-25 11:53:25,220][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:53:25,221][129146] Reward + Measures: [[368.94230572   0.70769972   0.11302233   0.75909734   0.40736833]]
[37m[1m[2023-06-25 11:53:25,221][129146] Max Reward on eval: 368.94230571789603
[37m[1m[2023-06-25 11:53:25,221][129146] Min Reward on eval: 368.94230571789603
[37m[1m[2023-06-25 11:53:25,221][129146] Mean Reward across all agents: 368.94230571789603
[37m[1m[2023-06-25 11:53:25,222][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:53:30,760][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:53:30,766][129146] Reward + Measures: [[222.03416048   0.71880001   0.0406       0.80250007   0.49090001]
[37m[1m [262.09641619   0.69049996   0.0661       0.80189991   0.44870001]
[37m[1m [374.45608331   0.65630001   0.0232       0.72940004   0.57750005]
[37m[1m ...
[37m[1m [377.53091035   0.67080003   0.1125       0.74790001   0.43670002]
[37m[1m [384.98838808   0.57450002   0.31209999   0.67270005   0.2823    ]
[37m[1m [451.33431245   0.64830005   0.0634       0.70230007   0.51029998]]
[37m[1m[2023-06-25 11:53:30,766][129146] Max Reward on eval: 559.9277479682117
[37m[1m[2023-06-25 11:53:30,766][129146] Min Reward on eval: 200.9401457408152
[37m[1m[2023-06-25 11:53:30,767][129146] Mean Reward across all agents: 361.33188862875346
[37m[1m[2023-06-25 11:53:30,767][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:53:30,774][129146] mean_value=431.97388728284943, max_value=958.9237435234536
[37m[1m[2023-06-25 11:53:30,777][129146] New mean coefficients: [[ 2.5750916  -0.69610626  1.5956218   3.734203   -1.0599357 ]]
[37m[1m[2023-06-25 11:53:30,778][129146] Moving the mean solution point...
[36m[2023-06-25 11:53:40,547][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 11:53:40,547][129146] FPS: 393136.14
[36m[2023-06-25 11:53:40,550][129146] itr=1192, itrs=2000, Progress: 59.60%
[36m[2023-06-25 11:53:52,083][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 11:53:52,083][129146] FPS: 333660.86
[36m[2023-06-25 11:53:56,927][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:53:56,928][129146] Reward + Measures: [[503.16389064   0.67963201   0.18166167   0.75323898   0.3427943 ]]
[37m[1m[2023-06-25 11:53:56,928][129146] Max Reward on eval: 503.16389063593806
[37m[1m[2023-06-25 11:53:56,928][129146] Min Reward on eval: 503.16389063593806
[37m[1m[2023-06-25 11:53:56,928][129146] Mean Reward across all agents: 503.16389063593806
[37m[1m[2023-06-25 11:53:56,929][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:54:02,447][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:54:02,448][129146] Reward + Measures: [[587.02373624   0.667        0.2332       0.71240002   0.32730004]
[37m[1m [539.16541173   0.6943       0.16940001   0.74589998   0.36039999]
[37m[1m [533.17294209   0.65000004   0.1894       0.70210004   0.38229999]
[37m[1m ...
[37m[1m [455.73608974   0.73219997   0.28309998   0.8125       0.1768    ]
[37m[1m [503.96354365   0.65010005   0.26890001   0.71999997   0.33580002]
[37m[1m [529.84626053   0.6261       0.31060001   0.72270006   0.25650001]]
[37m[1m[2023-06-25 11:54:02,448][129146] Max Reward on eval: 612.8862496222486
[37m[1m[2023-06-25 11:54:02,449][129146] Min Reward on eval: 334.34394039193865
[37m[1m[2023-06-25 11:54:02,449][129146] Mean Reward across all agents: 499.0749144626986
[37m[1m[2023-06-25 11:54:02,449][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:54:02,456][129146] mean_value=263.1804480611845, max_value=1014.503420801647
[37m[1m[2023-06-25 11:54:02,459][129146] New mean coefficients: [[ 3.0783825  -0.92455     1.8893883   4.25461    -0.95055974]]
[37m[1m[2023-06-25 11:54:02,460][129146] Moving the mean solution point...
[36m[2023-06-25 11:54:12,370][129146] train() took 9.91 seconds to complete
[36m[2023-06-25 11:54:12,370][129146] FPS: 387572.55
[36m[2023-06-25 11:54:12,373][129146] itr=1193, itrs=2000, Progress: 59.65%
[36m[2023-06-25 11:54:23,926][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 11:54:23,927][129146] FPS: 333105.45
[36m[2023-06-25 11:54:28,682][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:54:28,682][129146] Reward + Measures: [[617.9569811    0.65136069   0.24433866   0.739775     0.29867333]]
[37m[1m[2023-06-25 11:54:28,682][129146] Max Reward on eval: 617.9569810999609
[37m[1m[2023-06-25 11:54:28,682][129146] Min Reward on eval: 617.9569810999609
[37m[1m[2023-06-25 11:54:28,683][129146] Mean Reward across all agents: 617.9569810999609
[37m[1m[2023-06-25 11:54:28,683][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:54:34,354][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:54:34,355][129146] Reward + Measures: [[640.09452587   0.62970001   0.30170003   0.72270006   0.27579999]
[37m[1m [611.15943122   0.65920001   0.23450001   0.73069996   0.3272    ]
[37m[1m [526.85944365   0.72250003   0.23099999   0.80129999   0.2696    ]
[37m[1m ...
[37m[1m [651.2577201    0.63330001   0.2744       0.73380005   0.29070002]
[37m[1m [628.8288878    0.65210003   0.2552       0.74769998   0.28930002]
[37m[1m [476.06492021   0.7287001    0.1504       0.80739993   0.34      ]]
[37m[1m[2023-06-25 11:54:34,355][129146] Max Reward on eval: 732.4644249777077
[37m[1m[2023-06-25 11:54:34,355][129146] Min Reward on eval: 419.23580531605984
[37m[1m[2023-06-25 11:54:34,355][129146] Mean Reward across all agents: 612.157867719712
[37m[1m[2023-06-25 11:54:34,355][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:54:34,362][129146] mean_value=280.635298485763, max_value=1055.81330030567
[37m[1m[2023-06-25 11:54:34,365][129146] New mean coefficients: [[ 3.9516153  -1.2271633   3.016089    3.8263059  -0.39499205]]
[37m[1m[2023-06-25 11:54:34,366][129146] Moving the mean solution point...
[36m[2023-06-25 11:54:44,084][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 11:54:44,084][129146] FPS: 395234.87
[36m[2023-06-25 11:54:44,086][129146] itr=1194, itrs=2000, Progress: 59.70%
[36m[2023-06-25 11:54:55,536][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 11:54:55,536][129146] FPS: 336154.17
[36m[2023-06-25 11:55:00,319][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:55:00,319][129146] Reward + Measures: [[679.31567354   0.62696433   0.28832      0.72756267   0.27515799]]
[37m[1m[2023-06-25 11:55:00,320][129146] Max Reward on eval: 679.315673537671
[37m[1m[2023-06-25 11:55:00,320][129146] Min Reward on eval: 679.315673537671
[37m[1m[2023-06-25 11:55:00,320][129146] Mean Reward across all agents: 679.315673537671
[37m[1m[2023-06-25 11:55:00,320][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:55:05,665][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:55:05,666][129146] Reward + Measures: [[695.57992542   0.5844       0.3001       0.7288       0.26910001]
[37m[1m [706.9146231    0.65469998   0.30000001   0.75         0.28830001]
[37m[1m [696.94235247   0.48160002   0.56170005   0.53970003   0.34310001]
[37m[1m ...
[37m[1m [586.57126254   0.71110004   0.17389999   0.80979997   0.35740003]
[37m[1m [706.25654366   0.51430005   0.373        0.61870003   0.3145    ]
[37m[1m [797.23443479   0.50140005   0.38170001   0.58789998   0.38100001]]
[37m[1m[2023-06-25 11:55:05,666][129146] Max Reward on eval: 842.5912324452074
[37m[1m[2023-06-25 11:55:05,666][129146] Min Reward on eval: 460.40479204661676
[37m[1m[2023-06-25 11:55:05,666][129146] Mean Reward across all agents: 717.0403317062136
[37m[1m[2023-06-25 11:55:05,667][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:55:05,675][129146] mean_value=315.40507807565604, max_value=1286.9464604849925
[37m[1m[2023-06-25 11:55:05,678][129146] New mean coefficients: [[ 4.1982737 -3.1413746  2.9666026  4.0607867  0.1247499]]
[37m[1m[2023-06-25 11:55:05,679][129146] Moving the mean solution point...
[36m[2023-06-25 11:55:15,406][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 11:55:15,406][129146] FPS: 394857.01
[36m[2023-06-25 11:55:15,408][129146] itr=1195, itrs=2000, Progress: 59.75%
[36m[2023-06-25 11:55:26,795][129146] train() took 11.36 seconds to complete
[36m[2023-06-25 11:55:26,795][129146] FPS: 337987.92
[36m[2023-06-25 11:55:31,508][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:55:31,509][129146] Reward + Measures: [[763.74422129   0.59984869   0.30822766   0.70903802   0.27494332]]
[37m[1m[2023-06-25 11:55:31,509][129146] Max Reward on eval: 763.744221292911
[37m[1m[2023-06-25 11:55:31,509][129146] Min Reward on eval: 763.744221292911
[37m[1m[2023-06-25 11:55:31,510][129146] Mean Reward across all agents: 763.744221292911
[37m[1m[2023-06-25 11:55:31,510][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:55:37,003][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:55:37,003][129146] Reward + Measures: [[766.5690238    0.59559995   0.36470002   0.68879998   0.27489999]
[37m[1m [769.98435887   0.59849995   0.27669999   0.7137       0.296     ]
[37m[1m [662.69661327   0.53299999   0.4298       0.63819999   0.24639998]
[37m[1m ...
[37m[1m [750.24210533   0.51660007   0.46760002   0.66949999   0.31720001]
[37m[1m [745.00308999   0.50769997   0.35700002   0.64050001   0.29930004]
[37m[1m [773.36125118   0.47659999   0.39660001   0.61059999   0.28010002]]
[37m[1m[2023-06-25 11:55:37,004][129146] Max Reward on eval: 833.3157162559219
[37m[1m[2023-06-25 11:55:37,004][129146] Min Reward on eval: 526.8055916244048
[37m[1m[2023-06-25 11:55:37,004][129146] Mean Reward across all agents: 720.1107995561174
[37m[1m[2023-06-25 11:55:37,005][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:55:37,012][129146] mean_value=365.55145966236705, max_value=1292.016594382848
[37m[1m[2023-06-25 11:55:37,015][129146] New mean coefficients: [[ 4.4257545  -4.278988    1.9800119   4.2917037  -0.26228687]]
[37m[1m[2023-06-25 11:55:37,016][129146] Moving the mean solution point...
[36m[2023-06-25 11:55:46,670][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 11:55:46,671][129146] FPS: 397807.46
[36m[2023-06-25 11:55:46,673][129146] itr=1196, itrs=2000, Progress: 59.80%
[36m[2023-06-25 11:55:58,099][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 11:55:58,100][129146] FPS: 336780.94
[36m[2023-06-25 11:56:02,894][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:56:02,894][129146] Reward + Measures: [[841.78664052   0.57174033   0.33272368   0.69342637   0.26239631]]
[37m[1m[2023-06-25 11:56:02,894][129146] Max Reward on eval: 841.7866405190438
[37m[1m[2023-06-25 11:56:02,895][129146] Min Reward on eval: 841.7866405190438
[37m[1m[2023-06-25 11:56:02,895][129146] Mean Reward across all agents: 841.7866405190438
[37m[1m[2023-06-25 11:56:02,895][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:56:08,476][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:56:08,477][129146] Reward + Measures: [[592.70621564   0.35800001   0.30960003   0.58219999   0.39210001]
[37m[1m [761.03103616   0.43709999   0.29519999   0.63709992   0.33970001]
[37m[1m [596.14396139   0.42360002   0.26830003   0.60330003   0.43660003]
[37m[1m ...
[37m[1m [746.07288685   0.55299997   0.27169999   0.72240001   0.3152    ]
[37m[1m [733.29400097   0.42490003   0.3274       0.59820002   0.3664    ]
[37m[1m [644.6620627    0.43350002   0.30599999   0.59380001   0.42950001]]
[37m[1m[2023-06-25 11:56:08,477][129146] Max Reward on eval: 914.5244570537237
[37m[1m[2023-06-25 11:56:08,478][129146] Min Reward on eval: 379.5568175750726
[37m[1m[2023-06-25 11:56:08,478][129146] Mean Reward across all agents: 695.0538273564664
[37m[1m[2023-06-25 11:56:08,478][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:56:08,486][129146] mean_value=649.1488803282836, max_value=1340.3410771210097
[37m[1m[2023-06-25 11:56:08,489][129146] New mean coefficients: [[ 4.350902   -6.2822776   0.19041252  4.0537887  -0.5734055 ]]
[37m[1m[2023-06-25 11:56:08,491][129146] Moving the mean solution point...
[36m[2023-06-25 11:56:18,305][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 11:56:18,305][129146] FPS: 391348.58
[36m[2023-06-25 11:56:18,307][129146] itr=1197, itrs=2000, Progress: 59.85%
[36m[2023-06-25 11:56:30,031][129146] train() took 11.70 seconds to complete
[36m[2023-06-25 11:56:30,032][129146] FPS: 328292.69
[36m[2023-06-25 11:56:34,892][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:56:34,892][129146] Reward + Measures: [[895.38682275   0.54908836   0.343474     0.68126702   0.25031033]]
[37m[1m[2023-06-25 11:56:34,892][129146] Max Reward on eval: 895.3868227503377
[37m[1m[2023-06-25 11:56:34,893][129146] Min Reward on eval: 895.3868227503377
[37m[1m[2023-06-25 11:56:34,893][129146] Mean Reward across all agents: 895.3868227503377
[37m[1m[2023-06-25 11:56:34,893][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:56:40,521][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:56:40,521][129146] Reward + Measures: [[607.01723101   0.49779996   0.45510003   0.6358       0.29500002]
[37m[1m [781.06828602   0.46580002   0.47620001   0.57609999   0.33680001]
[37m[1m [871.67041027   0.46920004   0.40529999   0.6189       0.24450003]
[37m[1m ...
[37m[1m [727.93250503   0.52490002   0.38519999   0.67819995   0.1665    ]
[37m[1m [598.60241678   0.51310003   0.43990001   0.66800004   0.15550001]
[37m[1m [562.83052868   0.41640002   0.44329998   0.62989998   0.19470002]]
[37m[1m[2023-06-25 11:56:40,521][129146] Max Reward on eval: 977.389709000499
[37m[1m[2023-06-25 11:56:40,522][129146] Min Reward on eval: 380.14534721692326
[37m[1m[2023-06-25 11:56:40,522][129146] Mean Reward across all agents: 803.0822385855995
[37m[1m[2023-06-25 11:56:40,522][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:56:40,530][129146] mean_value=421.0668511430841, max_value=1191.174809475185
[37m[1m[2023-06-25 11:56:40,532][129146] New mean coefficients: [[ 3.4184542  -7.4759283  -0.38525033  2.0698385  -0.7975661 ]]
[37m[1m[2023-06-25 11:56:40,533][129146] Moving the mean solution point...
[36m[2023-06-25 11:56:50,295][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 11:56:50,295][129146] FPS: 393445.02
[36m[2023-06-25 11:56:50,298][129146] itr=1198, itrs=2000, Progress: 59.90%
[36m[2023-06-25 11:57:01,728][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 11:57:01,729][129146] FPS: 336736.49
[36m[2023-06-25 11:57:06,686][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:57:06,686][129146] Reward + Measures: [[936.42388838   0.51092869   0.34592202   0.66117632   0.241     ]]
[37m[1m[2023-06-25 11:57:06,687][129146] Max Reward on eval: 936.4238883835922
[37m[1m[2023-06-25 11:57:06,687][129146] Min Reward on eval: 936.4238883835922
[37m[1m[2023-06-25 11:57:06,687][129146] Mean Reward across all agents: 936.4238883835922
[37m[1m[2023-06-25 11:57:06,687][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:57:12,101][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:57:12,102][129146] Reward + Measures: [[ 515.6315006     0.45989999    0.27779999    0.56550002    0.37450001]
[37m[1m [ 552.49528088    0.4567        0.3633        0.60140002    0.32969999]
[37m[1m [ 658.10326131    0.37750003    0.33880001    0.57320005    0.34299999]
[37m[1m ...
[37m[1m [ 514.88853609    0.45429999    0.29449999    0.56969994    0.3459    ]
[37m[1m [ 846.12457916    0.36939999    0.4113        0.58350003    0.29310003]
[37m[1m [-140.90795654    0.40868464    0.70175391    0.68725389    0.46948463]]
[37m[1m[2023-06-25 11:57:12,102][129146] Max Reward on eval: 938.2176016789861
[37m[1m[2023-06-25 11:57:12,102][129146] Min Reward on eval: -776.5226704142057
[37m[1m[2023-06-25 11:57:12,103][129146] Mean Reward across all agents: 558.9844329714234
[37m[1m[2023-06-25 11:57:12,103][129146] Average Trajectory Length: 994.5113333333333
[36m[2023-06-25 11:57:12,108][129146] mean_value=200.52137239651668, max_value=1343.1749488962116
[37m[1m[2023-06-25 11:57:12,111][129146] New mean coefficients: [[ 3.2416167 -6.986205   0.7768371  2.0442195 -1.1023303]]
[37m[1m[2023-06-25 11:57:12,112][129146] Moving the mean solution point...
[36m[2023-06-25 11:57:21,711][129146] train() took 9.60 seconds to complete
[36m[2023-06-25 11:57:21,711][129146] FPS: 400146.55
[36m[2023-06-25 11:57:21,713][129146] itr=1199, itrs=2000, Progress: 59.95%
[36m[2023-06-25 11:57:33,164][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 11:57:33,164][129146] FPS: 336141.55
[36m[2023-06-25 11:57:37,947][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:57:37,948][129146] Reward + Measures: [[982.18441945   0.48093969   0.35095695   0.65019697   0.22807233]]
[37m[1m[2023-06-25 11:57:37,948][129146] Max Reward on eval: 982.1844194501372
[37m[1m[2023-06-25 11:57:37,948][129146] Min Reward on eval: 982.1844194501372
[37m[1m[2023-06-25 11:57:37,948][129146] Mean Reward across all agents: 982.1844194501372
[37m[1m[2023-06-25 11:57:37,949][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:57:43,398][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:57:43,399][129146] Reward + Measures: [[580.19121991   0.46090004   0.38569999   0.57610005   0.20840001]
[37m[1m [ 60.40478328   0.78289998   0.47530004   0.78399998   0.0946    ]
[37m[1m [396.71471952   0.53360003   0.36760002   0.61390001   0.12250002]
[37m[1m ...
[37m[1m [345.23359869   0.5363       0.32470003   0.60890001   0.14140001]
[37m[1m [566.34447459   0.63810009   0.37799999   0.73870003   0.14659999]
[37m[1m [529.40498126   0.51490003   0.36900002   0.60180008   0.156     ]]
[37m[1m[2023-06-25 11:57:43,399][129146] Max Reward on eval: 871.6012590269092
[37m[1m[2023-06-25 11:57:43,399][129146] Min Reward on eval: -63.77166489621741
[37m[1m[2023-06-25 11:57:43,400][129146] Mean Reward across all agents: 458.95565186039295
[37m[1m[2023-06-25 11:57:43,400][129146] Average Trajectory Length: 992.5223333333333
[36m[2023-06-25 11:57:43,404][129146] mean_value=-3.0614164795008643, max_value=1276.3748861563067
[37m[1m[2023-06-25 11:57:43,407][129146] New mean coefficients: [[ 2.5791597  -5.8937016  -0.65877044  2.4855316  -1.7196642 ]]
[37m[1m[2023-06-25 11:57:43,408][129146] Moving the mean solution point...
[36m[2023-06-25 11:57:53,087][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 11:57:53,087][129146] FPS: 396801.63
[36m[2023-06-25 11:57:53,089][129146] itr=1200, itrs=2000, Progress: 60.00%
[37m[1m[2023-06-25 11:58:00,356][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001180
[36m[2023-06-25 11:58:11,989][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 11:58:11,990][129146] FPS: 336325.18
[36m[2023-06-25 11:58:16,818][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:58:16,819][129146] Reward + Measures: [[1010.90655539    0.45888597    0.34798631    0.64754802    0.21184598]]
[37m[1m[2023-06-25 11:58:16,819][129146] Max Reward on eval: 1010.9065553948728
[37m[1m[2023-06-25 11:58:16,819][129146] Min Reward on eval: 1010.9065553948728
[37m[1m[2023-06-25 11:58:16,819][129146] Mean Reward across all agents: 1010.9065553948728
[37m[1m[2023-06-25 11:58:16,819][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:58:22,306][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:58:22,306][129146] Reward + Measures: [[913.94547192   0.42740002   0.30790001   0.54260004   0.14399999]
[37m[1m [570.47035974   0.6469       0.2825       0.70600003   0.10290001]
[37m[1m [848.03965024   0.51380002   0.31619999   0.57890004   0.12890001]
[37m[1m ...
[37m[1m [864.43631089   0.52090001   0.31750003   0.63920003   0.1646    ]
[37m[1m [979.33106164   0.43179998   0.3524       0.60430002   0.1558    ]
[37m[1m [598.28658977   0.42120001   0.32769999   0.4436       0.13440001]]
[37m[1m[2023-06-25 11:58:22,306][129146] Max Reward on eval: 1039.202449138381
[37m[1m[2023-06-25 11:58:22,307][129146] Min Reward on eval: 438.839478968177
[37m[1m[2023-06-25 11:58:22,307][129146] Mean Reward across all agents: 843.6910311919058
[37m[1m[2023-06-25 11:58:22,307][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:58:22,314][129146] mean_value=361.6148319909144, max_value=1316.0306314763905
[37m[1m[2023-06-25 11:58:22,317][129146] New mean coefficients: [[ 2.18176   -6.492311  -2.6763458  3.2193758 -1.7316817]]
[37m[1m[2023-06-25 11:58:22,318][129146] Moving the mean solution point...
[36m[2023-06-25 11:58:32,232][129146] train() took 9.91 seconds to complete
[36m[2023-06-25 11:58:32,232][129146] FPS: 387378.77
[36m[2023-06-25 11:58:32,235][129146] itr=1201, itrs=2000, Progress: 60.05%
[36m[2023-06-25 11:58:43,835][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 11:58:43,835][129146] FPS: 331817.55
[36m[2023-06-25 11:58:48,634][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:58:48,635][129146] Reward + Measures: [[1034.98020493    0.43100032    0.34580803    0.64287966    0.204412  ]]
[37m[1m[2023-06-25 11:58:48,635][129146] Max Reward on eval: 1034.9802049260095
[37m[1m[2023-06-25 11:58:48,635][129146] Min Reward on eval: 1034.9802049260095
[37m[1m[2023-06-25 11:58:48,635][129146] Mean Reward across all agents: 1034.9802049260095
[37m[1m[2023-06-25 11:58:48,635][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:58:54,243][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:58:54,244][129146] Reward + Measures: [[ 351.87316209    0.55660003    0.189         0.65640002    0.26199999]
[37m[1m [-709.83012337    0.33399543    0.14503637    0.37409434    0.31277046]
[37m[1m [-530.43128383    0.3490645     0.13385162    0.38271293    0.34886131]
[37m[1m ...
[37m[1m [ -59.81034149    0.42899999    0.10470001    0.45790002    0.31650004]
[37m[1m [-390.74183081    0.37210003    0.11669999    0.39050004    0.32520002]
[37m[1m [  68.3587805     0.50060004    0.2194        0.59430003    0.2287    ]]
[37m[1m[2023-06-25 11:58:54,244][129146] Max Reward on eval: 1024.8271255984437
[37m[1m[2023-06-25 11:58:54,244][129146] Min Reward on eval: -1076.1055923487409
[37m[1m[2023-06-25 11:58:54,244][129146] Mean Reward across all agents: -172.6967777252581
[37m[1m[2023-06-25 11:58:54,245][129146] Average Trajectory Length: 974.7126666666667
[36m[2023-06-25 11:58:54,248][129146] mean_value=-170.25718867411615, max_value=932.030394064582
[37m[1m[2023-06-25 11:58:54,251][129146] New mean coefficients: [[ 2.652554  -4.113842  -2.2718775  4.2991905 -1.5863209]]
[37m[1m[2023-06-25 11:58:54,252][129146] Moving the mean solution point...
[36m[2023-06-25 11:59:04,117][129146] train() took 9.86 seconds to complete
[36m[2023-06-25 11:59:04,117][129146] FPS: 389298.06
[36m[2023-06-25 11:59:04,119][129146] itr=1202, itrs=2000, Progress: 60.10%
[36m[2023-06-25 11:59:15,654][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 11:59:15,655][129146] FPS: 333620.69
[36m[2023-06-25 11:59:20,341][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:59:20,341][129146] Reward + Measures: [[1058.27381048    0.41536734    0.33807197    0.64544564    0.20516667]]
[37m[1m[2023-06-25 11:59:20,341][129146] Max Reward on eval: 1058.2738104757789
[37m[1m[2023-06-25 11:59:20,342][129146] Min Reward on eval: 1058.2738104757789
[37m[1m[2023-06-25 11:59:20,342][129146] Mean Reward across all agents: 1058.2738104757789
[37m[1m[2023-06-25 11:59:20,342][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:59:25,720][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:59:25,720][129146] Reward + Measures: [[1021.68507268    0.40120003    0.32780001    0.6613        0.22909999]
[37m[1m [ 558.68548216    0.44080001    0.2339        0.69470006    0.324     ]
[37m[1m [ 849.07954853    0.3687        0.30149999    0.64920002    0.2687    ]
[37m[1m ...
[37m[1m [ 940.89125437    0.45169997    0.3057        0.6882        0.23410001]
[37m[1m [ 915.48286545    0.39929998    0.29899999    0.69080001    0.2379    ]
[37m[1m [ 915.73089706    0.42350003    0.29980001    0.68589997    0.24749999]]
[37m[1m[2023-06-25 11:59:25,721][129146] Max Reward on eval: 1112.8683219995232
[37m[1m[2023-06-25 11:59:25,721][129146] Min Reward on eval: 471.0960001867032
[37m[1m[2023-06-25 11:59:25,721][129146] Mean Reward across all agents: 799.8646811008858
[37m[1m[2023-06-25 11:59:25,721][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 11:59:25,727][129146] mean_value=607.372856168738, max_value=1477.9439625509317
[37m[1m[2023-06-25 11:59:25,730][129146] New mean coefficients: [[ 3.1832151 -3.7563522 -1.7315071  4.061206  -1.2910441]]
[37m[1m[2023-06-25 11:59:25,731][129146] Moving the mean solution point...
[36m[2023-06-25 11:59:35,515][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 11:59:35,515][129146] FPS: 392553.33
[36m[2023-06-25 11:59:35,517][129146] itr=1203, itrs=2000, Progress: 60.15%
[36m[2023-06-25 11:59:47,107][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 11:59:47,107][129146] FPS: 331999.26
[36m[2023-06-25 11:59:51,896][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:59:51,897][129146] Reward + Measures: [[1108.77007876    0.40928203    0.33502445    0.64440316    0.19670641]]
[37m[1m[2023-06-25 11:59:51,897][129146] Max Reward on eval: 1108.7700787628612
[37m[1m[2023-06-25 11:59:51,897][129146] Min Reward on eval: 1108.7700787628612
[37m[1m[2023-06-25 11:59:51,897][129146] Mean Reward across all agents: 1108.7700787628612
[37m[1m[2023-06-25 11:59:51,897][129146] Average Trajectory Length: 999.9816666666667
[36m[2023-06-25 11:59:57,357][129146] Finished Evaluation Step
[37m[1m[2023-06-25 11:59:57,358][129146] Reward + Measures: [[-498.60125767    0.22716045    0.26016435    0.29805726    0.20024167]
[37m[1m [-165.23502058    0.25509772    0.27536392    0.38182631    0.24187294]
[37m[1m [-139.06696673    0.35381481    0.25812593    0.42499509    0.25571606]
[37m[1m ...
[37m[1m [-503.83667766    0.28390399    0.23885174    0.32911944    0.22680211]
[37m[1m [ 701.0879811     0.50520003    0.28140002    0.68780005    0.28280002]
[37m[1m [-367.69206006    0.26015058    0.26719666    0.34843892    0.260252  ]]
[37m[1m[2023-06-25 11:59:57,358][129146] Max Reward on eval: 1061.1206770221295
[37m[1m[2023-06-25 11:59:57,358][129146] Min Reward on eval: -829.1551145265112
[37m[1m[2023-06-25 11:59:57,358][129146] Mean Reward across all agents: 129.40049124076415
[37m[1m[2023-06-25 11:59:57,359][129146] Average Trajectory Length: 920.4676666666667
[36m[2023-06-25 11:59:57,362][129146] mean_value=-550.7290881141887, max_value=1301.0595417378704
[37m[1m[2023-06-25 11:59:57,364][129146] New mean coefficients: [[ 3.2924638 -1.7100904 -2.3458285  4.4850717 -1.2645535]]
[37m[1m[2023-06-25 11:59:57,366][129146] Moving the mean solution point...
[36m[2023-06-25 12:00:07,389][129146] train() took 10.02 seconds to complete
[36m[2023-06-25 12:00:07,390][129146] FPS: 383158.97
[36m[2023-06-25 12:00:07,392][129146] itr=1204, itrs=2000, Progress: 60.20%
[36m[2023-06-25 12:00:19,043][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 12:00:19,044][129146] FPS: 330242.78
[36m[2023-06-25 12:00:23,888][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:00:23,888][129146] Reward + Measures: [[1154.734875      0.40519741    0.33092123    0.64516997    0.18916947]]
[37m[1m[2023-06-25 12:00:23,888][129146] Max Reward on eval: 1154.7348749987996
[37m[1m[2023-06-25 12:00:23,889][129146] Min Reward on eval: 1154.7348749987996
[37m[1m[2023-06-25 12:00:23,889][129146] Mean Reward across all agents: 1154.7348749987996
[37m[1m[2023-06-25 12:00:23,889][129146] Average Trajectory Length: 999.9756666666666
[36m[2023-06-25 12:00:29,319][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:00:29,320][129146] Reward + Measures: [[ 646.98193831    0.49400002    0.31800002    0.55830002    0.1737    ]
[37m[1m [1150.18746601    0.41430002    0.35870001    0.57959998    0.1778    ]
[37m[1m [ 967.63195531    0.49689999    0.2793        0.69880003    0.1909    ]
[37m[1m ...
[37m[1m [ 830.56688498    0.43179998    0.35029998    0.50309998    0.25579998]
[37m[1m [-414.94222108    0.98260003    0.95690006    0.98719996    0.96460003]
[37m[1m [ 140.08113476    0.46960002    0.32769999    0.4862        0.1682    ]]
[37m[1m[2023-06-25 12:00:29,320][129146] Max Reward on eval: 1211.30963105415
[37m[1m[2023-06-25 12:00:29,320][129146] Min Reward on eval: -1412.1249317337758
[37m[1m[2023-06-25 12:00:29,321][129146] Mean Reward across all agents: 690.8254733344857
[37m[1m[2023-06-25 12:00:29,321][129146] Average Trajectory Length: 997.0293333333333
[36m[2023-06-25 12:00:29,326][129146] mean_value=-21.40261640356337, max_value=908.9584933129877
[37m[1m[2023-06-25 12:00:29,329][129146] New mean coefficients: [[ 3.141565  -1.0854855 -2.2195194  4.316371  -1.0133185]]
[37m[1m[2023-06-25 12:00:29,330][129146] Moving the mean solution point...
[36m[2023-06-25 12:00:39,114][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 12:00:39,115][129146] FPS: 392526.37
[36m[2023-06-25 12:00:39,117][129146] itr=1205, itrs=2000, Progress: 60.25%
[36m[2023-06-25 12:00:50,659][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 12:00:50,659][129146] FPS: 333376.39
[36m[2023-06-25 12:00:55,487][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:00:55,488][129146] Reward + Measures: [[1194.48640118    0.39912334    0.32323265    0.65279531    0.19104333]]
[37m[1m[2023-06-25 12:00:55,488][129146] Max Reward on eval: 1194.486401181357
[37m[1m[2023-06-25 12:00:55,488][129146] Min Reward on eval: 1194.486401181357
[37m[1m[2023-06-25 12:00:55,488][129146] Mean Reward across all agents: 1194.486401181357
[37m[1m[2023-06-25 12:00:55,489][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:01:00,995][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:01:00,996][129146] Reward + Measures: [[1104.49049369    0.42230001    0.3107        0.65560001    0.2076    ]
[37m[1m [1096.58788281    0.42160001    0.2969        0.66819996    0.22270003]
[37m[1m [1181.27322182    0.39299998    0.34140003    0.63810003    0.19460002]
[37m[1m ...
[37m[1m [ 760.90028868    0.42220002    0.26180002    0.62090003    0.1997    ]
[37m[1m [ 889.231484      0.4375        0.26180002    0.65990001    0.20840001]
[37m[1m [1026.89897603    0.45369998    0.28480002    0.6904        0.23099999]]
[37m[1m[2023-06-25 12:01:00,996][129146] Max Reward on eval: 1234.3231539861067
[37m[1m[2023-06-25 12:01:00,996][129146] Min Reward on eval: 675.8919544333127
[37m[1m[2023-06-25 12:01:00,997][129146] Mean Reward across all agents: 1049.0782538752128
[37m[1m[2023-06-25 12:01:00,997][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:01:01,000][129146] mean_value=12.753496061104048, max_value=586.0110617742331
[37m[1m[2023-06-25 12:01:01,003][129146] New mean coefficients: [[ 2.4863825  -0.64381814 -1.2984492   3.3117087  -0.76297814]]
[37m[1m[2023-06-25 12:01:01,004][129146] Moving the mean solution point...
[36m[2023-06-25 12:01:10,688][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 12:01:10,688][129146] FPS: 396589.20
[36m[2023-06-25 12:01:10,691][129146] itr=1206, itrs=2000, Progress: 60.30%
[36m[2023-06-25 12:01:22,199][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 12:01:22,199][129146] FPS: 334367.56
[36m[2023-06-25 12:01:26,935][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:01:26,935][129146] Reward + Measures: [[1249.99247692    0.40379834    0.31913733    0.66035861    0.18176833]]
[37m[1m[2023-06-25 12:01:26,935][129146] Max Reward on eval: 1249.992476918887
[37m[1m[2023-06-25 12:01:26,936][129146] Min Reward on eval: 1249.992476918887
[37m[1m[2023-06-25 12:01:26,936][129146] Mean Reward across all agents: 1249.992476918887
[37m[1m[2023-06-25 12:01:26,936][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:01:32,592][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:01:32,592][129146] Reward + Measures: [[1066.99780774    0.44370005    0.36849999    0.62830001    0.16830002]
[37m[1m [ 459.82581562    0.29590002    0.40920001    0.46499997    0.2728    ]
[37m[1m [1165.15019095    0.435         0.32999998    0.61989999    0.18770002]
[37m[1m ...
[37m[1m [ 827.06151518    0.4341        0.37509999    0.53960001    0.2086    ]
[37m[1m [ 986.86777524    0.38839999    0.33040002    0.56669998    0.21710001]
[37m[1m [1055.26574888    0.42840001    0.3558        0.61190003    0.14189999]]
[37m[1m[2023-06-25 12:01:32,593][129146] Max Reward on eval: 1230.4241677718005
[37m[1m[2023-06-25 12:01:32,593][129146] Min Reward on eval: -332.7944696607534
[37m[1m[2023-06-25 12:01:32,593][129146] Mean Reward across all agents: 792.57062187102
[37m[1m[2023-06-25 12:01:32,593][129146] Average Trajectory Length: 992.7443333333333
[36m[2023-06-25 12:01:32,597][129146] mean_value=-64.68537385220249, max_value=1089.745775256285
[37m[1m[2023-06-25 12:01:32,600][129146] New mean coefficients: [[ 1.9420413  0.2530371 -1.5546155  3.2531013 -0.3932293]]
[37m[1m[2023-06-25 12:01:32,601][129146] Moving the mean solution point...
[36m[2023-06-25 12:01:42,228][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 12:01:42,228][129146] FPS: 398959.45
[36m[2023-06-25 12:01:42,230][129146] itr=1207, itrs=2000, Progress: 60.35%
[36m[2023-06-25 12:01:53,834][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 12:01:53,834][129146] FPS: 331629.13
[36m[2023-06-25 12:01:58,751][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:01:58,756][129146] Reward + Measures: [[1277.11931115    0.40174431    0.31024468    0.668845      0.18267366]]
[37m[1m[2023-06-25 12:01:58,756][129146] Max Reward on eval: 1277.1193111518392
[37m[1m[2023-06-25 12:01:58,757][129146] Min Reward on eval: 1277.1193111518392
[37m[1m[2023-06-25 12:01:58,757][129146] Mean Reward across all agents: 1277.1193111518392
[37m[1m[2023-06-25 12:01:58,757][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:02:04,292][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:02:04,293][129146] Reward + Measures: [[ 613.69359921    0.62099999    0.25770003    0.63730001    0.34460002]
[37m[1m [ 446.69532821    0.49210006    0.29720002    0.50880003    0.29569998]
[37m[1m [ 461.95809978    0.46380001    0.33899999    0.52820003    0.2536    ]
[37m[1m ...
[37m[1m [-488.29002162    0.84930003    0.53240001    0.78490001    0.81070006]
[37m[1m [1008.06328975    0.35279998    0.41149998    0.5165        0.23539999]
[37m[1m [ 420.07976848    0.49149999    0.2586        0.69669998    0.46930003]]
[37m[1m[2023-06-25 12:02:04,293][129146] Max Reward on eval: 1273.7662458749487
[37m[1m[2023-06-25 12:02:04,294][129146] Min Reward on eval: -853.9581996846828
[37m[1m[2023-06-25 12:02:04,294][129146] Mean Reward across all agents: 470.35733433964486
[37m[1m[2023-06-25 12:02:04,294][129146] Average Trajectory Length: 994.3916666666667
[36m[2023-06-25 12:02:04,299][129146] mean_value=-98.83711026370862, max_value=1525.102833308559
[37m[1m[2023-06-25 12:02:04,302][129146] New mean coefficients: [[ 2.2006378   1.2503042  -0.10718834  3.1863437  -0.16268153]]
[37m[1m[2023-06-25 12:02:04,303][129146] Moving the mean solution point...
[36m[2023-06-25 12:02:14,040][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 12:02:14,040][129146] FPS: 394449.51
[36m[2023-06-25 12:02:14,042][129146] itr=1208, itrs=2000, Progress: 60.40%
[36m[2023-06-25 12:02:25,587][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 12:02:25,588][129146] FPS: 333303.69
[36m[2023-06-25 12:02:30,528][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:02:30,528][129146] Reward + Measures: [[1307.81119787    0.40834767    0.31296533    0.67735165    0.17898899]]
[37m[1m[2023-06-25 12:02:30,528][129146] Max Reward on eval: 1307.8111978666711
[37m[1m[2023-06-25 12:02:30,529][129146] Min Reward on eval: 1307.8111978666711
[37m[1m[2023-06-25 12:02:30,529][129146] Mean Reward across all agents: 1307.8111978666711
[37m[1m[2023-06-25 12:02:30,529][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:02:35,928][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:02:35,929][129146] Reward + Measures: [[1181.61787693    0.43249997    0.39590001    0.65609998    0.21110001]
[37m[1m [1000.46594683    0.36579999    0.2811        0.59579998    0.2185    ]
[37m[1m [1016.46345056    0.44459996    0.48899999    0.62129998    0.27250001]
[37m[1m ...
[37m[1m [  -3.45876234    0.52939999    0.42120001    0.51319999    0.2678    ]
[37m[1m [-348.22705511    0.4499        0.44060001    0.412         0.31029999]
[37m[1m [1172.25962291    0.43990001    0.36320001    0.67839998    0.21959999]]
[37m[1m[2023-06-25 12:02:35,929][129146] Max Reward on eval: 1299.1454753246392
[37m[1m[2023-06-25 12:02:35,930][129146] Min Reward on eval: -446.7246773695457
[37m[1m[2023-06-25 12:02:35,930][129146] Mean Reward across all agents: 789.6885350738573
[37m[1m[2023-06-25 12:02:35,930][129146] Average Trajectory Length: 990.1553333333333
[36m[2023-06-25 12:02:35,935][129146] mean_value=31.220338409667008, max_value=1799.1454753246392
[37m[1m[2023-06-25 12:02:35,938][129146] New mean coefficients: [[ 1.6244242   0.3775326   0.03024766  2.2276287  -0.22436592]]
[37m[1m[2023-06-25 12:02:35,939][129146] Moving the mean solution point...
[36m[2023-06-25 12:02:45,610][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 12:02:45,610][129146] FPS: 397139.00
[36m[2023-06-25 12:02:45,612][129146] itr=1209, itrs=2000, Progress: 60.45%
[36m[2023-06-25 12:02:57,111][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 12:02:57,111][129146] FPS: 334738.56
[36m[2023-06-25 12:03:01,895][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:03:01,896][129146] Reward + Measures: [[1360.80015511    0.41447669    0.31337333    0.67417032    0.18330933]]
[37m[1m[2023-06-25 12:03:01,896][129146] Max Reward on eval: 1360.8001551126565
[37m[1m[2023-06-25 12:03:01,896][129146] Min Reward on eval: 1360.8001551126565
[37m[1m[2023-06-25 12:03:01,896][129146] Mean Reward across all agents: 1360.8001551126565
[37m[1m[2023-06-25 12:03:01,896][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:03:07,410][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:03:07,411][129146] Reward + Measures: [[1226.10446334    0.37199998    0.2958        0.61250001    0.1706    ]
[37m[1m [ 329.8486017     0.34325662    0.21779044    0.45517731    0.21659575]
[37m[1m [-119.17517121    0.5607        0.20060001    0.65889996    0.32460004]
[37m[1m ...
[37m[1m [ 615.66389586    0.50520003    0.24660002    0.63789999    0.13380001]
[37m[1m [ 632.70095808    0.56639999    0.45709997    0.59690005    0.13680001]
[37m[1m [1167.57310815    0.47280002    0.27010003    0.63409996    0.21110001]]
[37m[1m[2023-06-25 12:03:07,411][129146] Max Reward on eval: 1359.2067031898769
[37m[1m[2023-06-25 12:03:07,411][129146] Min Reward on eval: -550.9921496324474
[37m[1m[2023-06-25 12:03:07,412][129146] Mean Reward across all agents: 783.578795490176
[37m[1m[2023-06-25 12:03:07,412][129146] Average Trajectory Length: 990.2856666666667
[36m[2023-06-25 12:03:07,418][129146] mean_value=13.956974155483865, max_value=1663.5667767485604
[37m[1m[2023-06-25 12:03:07,421][129146] New mean coefficients: [[ 1.4976141   1.129498   -0.04302874  2.4451206  -0.47888795]]
[37m[1m[2023-06-25 12:03:07,422][129146] Moving the mean solution point...
[36m[2023-06-25 12:03:17,116][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 12:03:17,116][129146] FPS: 396189.73
[36m[2023-06-25 12:03:17,118][129146] itr=1210, itrs=2000, Progress: 60.50%
[37m[1m[2023-06-25 12:03:24,716][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001190
[36m[2023-06-25 12:03:36,520][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 12:03:36,521][129146] FPS: 331399.51
[36m[2023-06-25 12:03:41,372][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:03:41,373][129146] Reward + Measures: [[1389.29271429    0.42207563    0.31149033    0.68022132    0.18247598]]
[37m[1m[2023-06-25 12:03:41,373][129146] Max Reward on eval: 1389.2927142902702
[37m[1m[2023-06-25 12:03:41,373][129146] Min Reward on eval: 1389.2927142902702
[37m[1m[2023-06-25 12:03:41,374][129146] Mean Reward across all agents: 1389.2927142902702
[37m[1m[2023-06-25 12:03:41,374][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:03:46,848][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:03:46,849][129146] Reward + Measures: [[692.21205787   0.50150007   0.34709999   0.49650002   0.17180002]
[37m[1m [-31.81507355   0.47260004   0.0691       0.51999998   0.43710002]
[37m[1m [-66.15783088   0.2703       0.71430004   0.69999999   0.5898    ]
[37m[1m ...
[37m[1m [583.71421453   0.46349999   0.149        0.69749999   0.28379998]
[37m[1m [-27.09292145   0.55930001   0.1305       0.52750003   0.51410002]
[37m[1m [700.12205417   0.2836       0.2333       0.60320002   0.3154    ]]
[37m[1m[2023-06-25 12:03:46,849][129146] Max Reward on eval: 1485.6398549044739
[37m[1m[2023-06-25 12:03:46,849][129146] Min Reward on eval: -2061.643561711814
[37m[1m[2023-06-25 12:03:46,850][129146] Mean Reward across all agents: 433.48345047601384
[37m[1m[2023-06-25 12:03:46,850][129146] Average Trajectory Length: 984.8013333333333
[36m[2023-06-25 12:03:46,856][129146] mean_value=-36.31044859894566, max_value=1589.202927902895
[37m[1m[2023-06-25 12:03:46,859][129146] New mean coefficients: [[ 1.9118733   1.2684044   0.9541476   3.1397922  -0.01938662]]
[37m[1m[2023-06-25 12:03:46,860][129146] Moving the mean solution point...
[36m[2023-06-25 12:03:56,626][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 12:03:56,627][129146] FPS: 393263.50
[36m[2023-06-25 12:03:56,629][129146] itr=1211, itrs=2000, Progress: 60.55%
[36m[2023-06-25 12:04:08,005][129146] train() took 11.35 seconds to complete
[36m[2023-06-25 12:04:08,006][129146] FPS: 338345.99
[36m[2023-06-25 12:04:12,759][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:04:12,760][129146] Reward + Measures: [[1414.89378437    0.43136498    0.31401268    0.68616164    0.18073134]]
[37m[1m[2023-06-25 12:04:12,760][129146] Max Reward on eval: 1414.8937843695508
[37m[1m[2023-06-25 12:04:12,760][129146] Min Reward on eval: 1414.8937843695508
[37m[1m[2023-06-25 12:04:12,761][129146] Mean Reward across all agents: 1414.8937843695508
[37m[1m[2023-06-25 12:04:12,761][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:04:18,108][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:04:18,108][129146] Reward + Measures: [[1052.77223463    0.37760001    0.37920004    0.53600001    0.1699    ]
[37m[1m [ 287.70172351    0.34920001    0.65179998    0.62779999    0.58149999]
[37m[1m [1380.22324555    0.41549999    0.29319999    0.66550004    0.17879999]
[37m[1m ...
[37m[1m [ 648.0344675     0.64489996    0.4614        0.80569994    0.0812    ]
[37m[1m [1071.97163389    0.39910001    0.3545        0.56730002    0.14300001]
[37m[1m [ 568.79546342    0.3416        0.54170007    0.59939998    0.3594    ]]
[37m[1m[2023-06-25 12:04:18,108][129146] Max Reward on eval: 1417.54654619582
[37m[1m[2023-06-25 12:04:18,109][129146] Min Reward on eval: -1582.9045033597795
[37m[1m[2023-06-25 12:04:18,109][129146] Mean Reward across all agents: 440.24542811777116
[37m[1m[2023-06-25 12:04:18,109][129146] Average Trajectory Length: 992.7113333333333
[36m[2023-06-25 12:04:18,117][129146] mean_value=19.959236928496903, max_value=1548.98514448705
[37m[1m[2023-06-25 12:04:18,120][129146] New mean coefficients: [[ 1.8420674   1.3504326   0.6929932   2.9388514  -0.40110046]]
[37m[1m[2023-06-25 12:04:18,121][129146] Moving the mean solution point...
[36m[2023-06-25 12:04:27,813][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 12:04:27,813][129146] FPS: 396273.78
[36m[2023-06-25 12:04:27,815][129146] itr=1212, itrs=2000, Progress: 60.60%
[36m[2023-06-25 12:04:39,310][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 12:04:39,310][129146] FPS: 334750.72
[36m[2023-06-25 12:04:44,157][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:04:44,157][129146] Reward + Measures: [[1445.36313861    0.43601334    0.32087266    0.68721229    0.17524898]]
[37m[1m[2023-06-25 12:04:44,158][129146] Max Reward on eval: 1445.3631386063078
[37m[1m[2023-06-25 12:04:44,158][129146] Min Reward on eval: 1445.3631386063078
[37m[1m[2023-06-25 12:04:44,158][129146] Mean Reward across all agents: 1445.3631386063078
[37m[1m[2023-06-25 12:04:44,158][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:04:49,634][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:04:49,635][129146] Reward + Measures: [[ 260.49176692    0.42080003    0.60270005    0.5406        0.61910003]
[37m[1m [ -22.24911212    0.79119998    0.77540004    0.70109999    0.77350008]
[37m[1m [ -80.1468974     0.38913685    0.29598251    0.48054084    0.21054557]
[37m[1m ...
[37m[1m [1009.73310873    0.62390006    0.30930001    0.67989999    0.19770001]
[37m[1m [ 831.42641856    0.345         0.26990002    0.63990003    0.3118    ]
[37m[1m [1190.32384246    0.4262        0.35899997    0.67129993    0.2392    ]]
[37m[1m[2023-06-25 12:04:49,635][129146] Max Reward on eval: 1461.7135909388949
[37m[1m[2023-06-25 12:04:49,635][129146] Min Reward on eval: -829.5730647581164
[37m[1m[2023-06-25 12:04:49,635][129146] Mean Reward across all agents: 590.3241325799355
[37m[1m[2023-06-25 12:04:49,636][129146] Average Trajectory Length: 997.7286666666666
[36m[2023-06-25 12:04:49,643][129146] mean_value=-34.83026763217878, max_value=1169.097938192608
[37m[1m[2023-06-25 12:04:49,646][129146] New mean coefficients: [[ 1.5534341   0.5727364   0.33973333  2.6162977  -0.4822366 ]]
[37m[1m[2023-06-25 12:04:49,647][129146] Moving the mean solution point...
[36m[2023-06-25 12:04:59,385][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 12:04:59,385][129146] FPS: 394411.73
[36m[2023-06-25 12:04:59,387][129146] itr=1213, itrs=2000, Progress: 60.65%
[36m[2023-06-25 12:05:10,930][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 12:05:10,930][129146] FPS: 333378.97
[36m[2023-06-25 12:05:15,674][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:05:15,674][129146] Reward + Measures: [[1477.5413501     0.43348235    0.319855      0.68878227    0.17220832]]
[37m[1m[2023-06-25 12:05:15,675][129146] Max Reward on eval: 1477.5413501045682
[37m[1m[2023-06-25 12:05:15,675][129146] Min Reward on eval: 1477.5413501045682
[37m[1m[2023-06-25 12:05:15,675][129146] Mean Reward across all agents: 1477.5413501045682
[37m[1m[2023-06-25 12:05:15,675][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:05:21,116][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:05:21,116][129146] Reward + Measures: [[-120.88362908    0.95809996    0.93300003    0.95679998    0.94760001]
[37m[1m [ 847.12903049    0.4059        0.29269999    0.51410002    0.2043    ]
[37m[1m [ 593.76065063    0.44080001    0.29350001    0.6573        0.23290001]
[37m[1m ...
[37m[1m [ 520.35677407    0.70599997    0.2017        0.82539999    0.25060001]
[37m[1m [1337.6262836     0.4447        0.31290004    0.71320003    0.21440001]
[37m[1m [ 624.62616879    0.3010225     0.32231838    0.40404281    0.24434491]]
[37m[1m[2023-06-25 12:05:21,116][129146] Max Reward on eval: 1506.4338461254258
[37m[1m[2023-06-25 12:05:21,117][129146] Min Reward on eval: -1035.237878907082
[37m[1m[2023-06-25 12:05:21,117][129146] Mean Reward across all agents: 747.1882152698591
[37m[1m[2023-06-25 12:05:21,117][129146] Average Trajectory Length: 998.2993333333333
[36m[2023-06-25 12:05:21,124][129146] mean_value=96.13305108781152, max_value=1755.817718423449
[37m[1m[2023-06-25 12:05:21,127][129146] New mean coefficients: [[ 1.6474954  0.2165575  0.5240035  2.5741677 -0.1833295]]
[37m[1m[2023-06-25 12:05:21,128][129146] Moving the mean solution point...
[36m[2023-06-25 12:05:30,851][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 12:05:30,851][129146] FPS: 395001.48
[36m[2023-06-25 12:05:30,853][129146] itr=1214, itrs=2000, Progress: 60.70%
[36m[2023-06-25 12:05:42,388][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 12:05:42,388][129146] FPS: 333611.20
[36m[2023-06-25 12:05:47,194][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:05:47,195][129146] Reward + Measures: [[1526.33470824    0.42987564    0.32381433    0.68335092    0.16751666]]
[37m[1m[2023-06-25 12:05:47,195][129146] Max Reward on eval: 1526.3347082387904
[37m[1m[2023-06-25 12:05:47,195][129146] Min Reward on eval: 1526.3347082387904
[37m[1m[2023-06-25 12:05:47,195][129146] Mean Reward across all agents: 1526.3347082387904
[37m[1m[2023-06-25 12:05:47,196][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:05:52,669][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:05:52,670][129146] Reward + Measures: [[1052.06484189    0.44099998    0.32570001    0.616         0.18260001]
[37m[1m [ 399.23899727    0.53960007    0.38669997    0.57510006    0.1445    ]
[37m[1m [1268.35437095    0.45500001    0.34050003    0.64209998    0.17      ]
[37m[1m ...
[37m[1m [1130.64579975    0.48380002    0.34989998    0.6548        0.1539    ]
[37m[1m [ 371.9946156     0.58960003    0.43990001    0.62330002    0.1195    ]
[37m[1m [1242.66331434    0.42139998    0.36390001    0.65359998    0.164     ]]
[37m[1m[2023-06-25 12:05:52,670][129146] Max Reward on eval: 1497.6107820224715
[37m[1m[2023-06-25 12:05:52,670][129146] Min Reward on eval: -761.3101123295143
[37m[1m[2023-06-25 12:05:52,670][129146] Mean Reward across all agents: 826.1920736641629
[37m[1m[2023-06-25 12:05:52,671][129146] Average Trajectory Length: 999.805
[36m[2023-06-25 12:05:52,675][129146] mean_value=-165.72467471274888, max_value=1225.0989695355297
[37m[1m[2023-06-25 12:05:52,677][129146] New mean coefficients: [[ 1.2745545  -0.14632452 -0.11371994  1.7993324  -0.6019553 ]]
[37m[1m[2023-06-25 12:05:52,678][129146] Moving the mean solution point...
[36m[2023-06-25 12:06:02,444][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 12:06:02,444][129146] FPS: 393286.59
[36m[2023-06-25 12:06:02,446][129146] itr=1215, itrs=2000, Progress: 60.75%
[36m[2023-06-25 12:06:14,055][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 12:06:14,056][129146] FPS: 331468.45
[36m[2023-06-25 12:06:18,911][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:06:18,912][129146] Reward + Measures: [[1589.31222524    0.42224199    0.32897332    0.67199504    0.16189866]]
[37m[1m[2023-06-25 12:06:18,912][129146] Max Reward on eval: 1589.312225241342
[37m[1m[2023-06-25 12:06:18,912][129146] Min Reward on eval: 1589.312225241342
[37m[1m[2023-06-25 12:06:18,912][129146] Mean Reward across all agents: 1589.312225241342
[37m[1m[2023-06-25 12:06:18,913][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:06:24,591][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:06:24,592][129146] Reward + Measures: [[-154.23876695    0.4619        0.20799999    0.53049994    0.3766    ]
[37m[1m [ 232.67446109    0.29210001    0.14219999    0.1841        0.11140001]
[37m[1m [1076.83412166    0.5151        0.54470003    0.68049997    0.19039999]
[37m[1m ...
[37m[1m [ -65.41410697    0.1141        0.099         0.06780001    0.0625    ]
[37m[1m [-175.36956172    0.26589999    0.27290002    0.25910002    0.1963    ]
[37m[1m [ 634.48882788    0.36840001    0.55940002    0.49959999    0.38820001]]
[37m[1m[2023-06-25 12:06:24,592][129146] Max Reward on eval: 1585.458945872239
[37m[1m[2023-06-25 12:06:24,592][129146] Min Reward on eval: -766.7430230345374
[37m[1m[2023-06-25 12:06:24,593][129146] Mean Reward across all agents: 649.3437874237055
[37m[1m[2023-06-25 12:06:24,593][129146] Average Trajectory Length: 971.8976666666666
[36m[2023-06-25 12:06:24,600][129146] mean_value=132.02246284415756, max_value=1698.757861668407
[37m[1m[2023-06-25 12:06:24,603][129146] New mean coefficients: [[ 1.570941   -0.05696494  0.62025195  1.7481549  -0.6318374 ]]
[37m[1m[2023-06-25 12:06:24,604][129146] Moving the mean solution point...
[36m[2023-06-25 12:06:34,341][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 12:06:34,341][129146] FPS: 394452.03
[36m[2023-06-25 12:06:34,343][129146] itr=1216, itrs=2000, Progress: 60.80%
[36m[2023-06-25 12:06:45,906][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 12:06:45,906][129146] FPS: 332789.76
[36m[2023-06-25 12:06:50,847][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:06:50,848][129146] Reward + Measures: [[1659.27889345    0.41519433    0.337405      0.65359938    0.15605533]]
[37m[1m[2023-06-25 12:06:50,848][129146] Max Reward on eval: 1659.278893451944
[37m[1m[2023-06-25 12:06:50,848][129146] Min Reward on eval: 1659.278893451944
[37m[1m[2023-06-25 12:06:50,848][129146] Mean Reward across all agents: 1659.278893451944
[37m[1m[2023-06-25 12:06:50,849][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:06:56,387][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:06:56,388][129146] Reward + Measures: [[ 266.18077459    0.71899998    0.36919999    0.74239993    0.72770005]
[37m[1m [1247.74583779    0.48450002    0.29910001    0.7001        0.21230002]
[37m[1m [1243.25459959    0.5061        0.2674        0.66839999    0.22189999]
[37m[1m ...
[37m[1m [1448.31897631    0.44210002    0.32449999    0.63560003    0.1718    ]
[37m[1m [ 725.13202336    0.55050004    0.30020002    0.74019998    0.18980001]
[37m[1m [ 388.19385599    0.83160001    0.58059996    0.90240002    0.91790003]]
[37m[1m[2023-06-25 12:06:56,388][129146] Max Reward on eval: 1670.2573824934195
[37m[1m[2023-06-25 12:06:56,388][129146] Min Reward on eval: -952.6119377975817
[37m[1m[2023-06-25 12:06:56,389][129146] Mean Reward across all agents: 599.3917492283421
[37m[1m[2023-06-25 12:06:56,389][129146] Average Trajectory Length: 992.6809999999999
[36m[2023-06-25 12:06:56,396][129146] mean_value=-40.84012953791198, max_value=1379.5148365899386
[37m[1m[2023-06-25 12:06:56,399][129146] New mean coefficients: [[ 1.5645415  -0.3137686   0.63429165  2.1633573  -0.485803  ]]
[37m[1m[2023-06-25 12:06:56,400][129146] Moving the mean solution point...
[36m[2023-06-25 12:07:06,263][129146] train() took 9.86 seconds to complete
[36m[2023-06-25 12:07:06,263][129146] FPS: 389385.13
[36m[2023-06-25 12:07:06,265][129146] itr=1217, itrs=2000, Progress: 60.85%
[36m[2023-06-25 12:07:17,735][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 12:07:17,735][129146] FPS: 335488.19
[36m[2023-06-25 12:07:22,562][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:07:22,562][129146] Reward + Measures: [[1729.92262476    0.40288597    0.35056201    0.63769466    0.14892833]]
[37m[1m[2023-06-25 12:07:22,563][129146] Max Reward on eval: 1729.9226247576692
[37m[1m[2023-06-25 12:07:22,563][129146] Min Reward on eval: 1729.9226247576692
[37m[1m[2023-06-25 12:07:22,563][129146] Mean Reward across all agents: 1729.9226247576692
[37m[1m[2023-06-25 12:07:22,563][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:07:27,997][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:07:27,998][129146] Reward + Measures: [[ 212.145297      0.35260147    0.26328382    0.42075586    0.20025735]
[37m[1m [ 665.66800772    0.49829999    0.46810004    0.73510003    0.0803    ]
[37m[1m [ 918.56419059    0.45300004    0.34710002    0.67369998    0.28070003]
[37m[1m ...
[37m[1m [ 967.97061551    0.41359997    0.30610001    0.65560001    0.24000001]
[37m[1m [ -40.17343515    0.7391001     0.81300002    0.83490002    0.76020002]
[37m[1m [1482.05949935    0.46709999    0.36880001    0.64039993    0.14590001]]
[37m[1m[2023-06-25 12:07:27,998][129146] Max Reward on eval: 1690.9070338575634
[37m[1m[2023-06-25 12:07:27,998][129146] Min Reward on eval: -923.5532617798774
[37m[1m[2023-06-25 12:07:27,999][129146] Mean Reward across all agents: 639.3595971342461
[37m[1m[2023-06-25 12:07:27,999][129146] Average Trajectory Length: 990.651
[36m[2023-06-25 12:07:28,003][129146] mean_value=-268.7364307362883, max_value=1941.3483645126994
[37m[1m[2023-06-25 12:07:28,006][129146] New mean coefficients: [[ 1.416145    0.1676597   0.26924628  2.5734282  -0.27868214]]
[37m[1m[2023-06-25 12:07:28,007][129146] Moving the mean solution point...
[36m[2023-06-25 12:07:37,755][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 12:07:37,756][129146] FPS: 393969.21
[36m[2023-06-25 12:07:37,758][129146] itr=1218, itrs=2000, Progress: 60.90%
[36m[2023-06-25 12:07:49,216][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 12:07:49,216][129146] FPS: 335941.07
[36m[2023-06-25 12:07:53,987][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:07:53,988][129146] Reward + Measures: [[1757.79454149    0.39388233    0.34862769    0.64847964    0.15183166]]
[37m[1m[2023-06-25 12:07:53,988][129146] Max Reward on eval: 1757.7945414932508
[37m[1m[2023-06-25 12:07:53,988][129146] Min Reward on eval: 1757.7945414932508
[37m[1m[2023-06-25 12:07:53,988][129146] Mean Reward across all agents: 1757.7945414932508
[37m[1m[2023-06-25 12:07:53,989][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:07:59,390][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:07:59,391][129146] Reward + Measures: [[1002.90807088    0.4901        0.32139999    0.72140002    0.14839999]
[37m[1m [ 874.77670416    0.37369999    0.25760004    0.61410004    0.20380001]
[37m[1m [1146.41854431    0.31160003    0.3396        0.54730004    0.19960001]
[37m[1m ...
[37m[1m [ 471.21687609    0.49379998    0.58759999    0.65700001    0.49509999]
[37m[1m [1376.83774011    0.31020001    0.3062        0.52570003    0.18880001]
[37m[1m [1800.60006864    0.4007        0.34030002    0.5934        0.14660001]]
[37m[1m[2023-06-25 12:07:59,391][129146] Max Reward on eval: 1800.6000686405926
[37m[1m[2023-06-25 12:07:59,391][129146] Min Reward on eval: -1177.737207281345
[37m[1m[2023-06-25 12:07:59,391][129146] Mean Reward across all agents: 749.7671953872605
[37m[1m[2023-06-25 12:07:59,392][129146] Average Trajectory Length: 992.598
[36m[2023-06-25 12:07:59,397][129146] mean_value=23.906275681702027, max_value=1585.343796751802
[37m[1m[2023-06-25 12:07:59,400][129146] New mean coefficients: [[ 1.3879473   0.05197576  0.460993    2.6789706  -0.16049105]]
[37m[1m[2023-06-25 12:07:59,401][129146] Moving the mean solution point...
[36m[2023-06-25 12:08:09,144][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 12:08:09,145][129146] FPS: 394191.66
[36m[2023-06-25 12:08:09,147][129146] itr=1219, itrs=2000, Progress: 60.95%
[36m[2023-06-25 12:08:20,535][129146] train() took 11.36 seconds to complete
[36m[2023-06-25 12:08:20,535][129146] FPS: 337903.82
[36m[2023-06-25 12:08:25,320][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:08:25,321][129146] Reward + Measures: [[1790.67620383    0.3836827     0.34816134    0.64900869    0.15226267]]
[37m[1m[2023-06-25 12:08:25,321][129146] Max Reward on eval: 1790.6762038280133
[37m[1m[2023-06-25 12:08:25,321][129146] Min Reward on eval: 1790.6762038280133
[37m[1m[2023-06-25 12:08:25,322][129146] Mean Reward across all agents: 1790.6762038280133
[37m[1m[2023-06-25 12:08:25,322][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:08:30,754][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:08:30,760][129146] Reward + Measures: [[1362.0391067     0.52310002    0.3414        0.54080003    0.18080001]
[37m[1m [1634.11696588    0.39510003    0.30900002    0.4499        0.171     ]
[37m[1m [1509.78972463    0.39160004    0.34319997    0.61320001    0.18190001]
[37m[1m ...
[37m[1m [ 537.40802699    0.38820001    0.26610002    0.47049999    0.28369999]
[37m[1m [1059.0968483     0.53350002    0.322         0.6516        0.18080001]
[37m[1m [1578.13902539    0.40910003    0.34390002    0.46110001    0.1445    ]]
[37m[1m[2023-06-25 12:08:30,760][129146] Max Reward on eval: 1872.955151616386
[37m[1m[2023-06-25 12:08:30,760][129146] Min Reward on eval: -109.38798232864356
[37m[1m[2023-06-25 12:08:30,761][129146] Mean Reward across all agents: 980.7405960924483
[37m[1m[2023-06-25 12:08:30,761][129146] Average Trajectory Length: 999.5456666666666
[36m[2023-06-25 12:08:30,766][129146] mean_value=-22.816069011688718, max_value=1693.839623416462
[37m[1m[2023-06-25 12:08:30,769][129146] New mean coefficients: [[ 1.1008954  -0.57407016  0.66354746  1.9308006  -0.23305687]]
[37m[1m[2023-06-25 12:08:30,770][129146] Moving the mean solution point...
[36m[2023-06-25 12:08:40,384][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 12:08:40,384][129146] FPS: 399482.57
[36m[2023-06-25 12:08:40,387][129146] itr=1220, itrs=2000, Progress: 61.00%
[37m[1m[2023-06-25 12:08:48,017][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001200
[36m[2023-06-25 12:08:59,814][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 12:08:59,814][129146] FPS: 331658.70
[36m[2023-06-25 12:09:04,572][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:09:04,572][129146] Reward + Measures: [[1822.7306494     0.36974066    0.34936032    0.65394533    0.15234967]]
[37m[1m[2023-06-25 12:09:04,572][129146] Max Reward on eval: 1822.7306493998594
[37m[1m[2023-06-25 12:09:04,573][129146] Min Reward on eval: 1822.7306493998594
[37m[1m[2023-06-25 12:09:04,573][129146] Mean Reward across all agents: 1822.7306493998594
[37m[1m[2023-06-25 12:09:04,573][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:09:10,024][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:09:10,025][129146] Reward + Measures: [[1796.01220397    0.38509998    0.38609999    0.59860003    0.13280001]
[37m[1m [ 293.7762923     0.4454        0.17109999    0.80579996    0.35220003]
[37m[1m [ 445.00774407    0.53439999    0.28140002    0.7317        0.23770002]
[37m[1m ...
[37m[1m [ 997.1785627     0.33460003    0.30560002    0.77330005    0.22930001]
[37m[1m [1273.07498705    0.37540001    0.32389998    0.63830006    0.20469999]
[37m[1m [ -36.79837399    0.69709998    0.33220002    0.86409998    0.30990002]]
[37m[1m[2023-06-25 12:09:10,025][129146] Max Reward on eval: 1866.693990967283
[37m[1m[2023-06-25 12:09:10,025][129146] Min Reward on eval: -639.5849872224848
[37m[1m[2023-06-25 12:09:10,025][129146] Mean Reward across all agents: 1023.5080714181005
[37m[1m[2023-06-25 12:09:10,026][129146] Average Trajectory Length: 991.3356666666666
[36m[2023-06-25 12:09:10,031][129146] mean_value=129.17286021070555, max_value=2248.6438191895113
[37m[1m[2023-06-25 12:09:10,034][129146] New mean coefficients: [[ 0.7024137  -0.8177346   0.10075808  2.187593   -0.05999079]]
[37m[1m[2023-06-25 12:09:10,035][129146] Moving the mean solution point...
[36m[2023-06-25 12:09:19,659][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 12:09:19,659][129146] FPS: 399075.63
[36m[2023-06-25 12:09:19,662][129146] itr=1221, itrs=2000, Progress: 61.05%
[36m[2023-06-25 12:09:31,029][129146] train() took 11.34 seconds to complete
[36m[2023-06-25 12:09:31,029][129146] FPS: 338632.62
[36m[2023-06-25 12:09:35,725][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:09:35,726][129146] Reward + Measures: [[1804.12054403    0.35446367    0.34370434    0.67485136    0.15918367]]
[37m[1m[2023-06-25 12:09:35,726][129146] Max Reward on eval: 1804.1205440293652
[37m[1m[2023-06-25 12:09:35,726][129146] Min Reward on eval: 1804.1205440293652
[37m[1m[2023-06-25 12:09:35,726][129146] Mean Reward across all agents: 1804.1205440293652
[37m[1m[2023-06-25 12:09:35,727][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:09:41,124][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:09:41,125][129146] Reward + Measures: [[1541.99142953    0.35109997    0.33310002    0.65649998    0.16760002]
[37m[1m [ 676.9920868     0.57860005    0.54840004    0.6886        0.2782    ]
[37m[1m [1384.19316687    0.4075        0.30230004    0.55409998    0.1813    ]
[37m[1m ...
[37m[1m [ 743.25281087    0.52630001    0.29249999    0.66320002    0.21350001]
[37m[1m [ 445.66242957    0.54689997    0.4368        0.65490007    0.42840001]
[37m[1m [ 882.50873453    0.59299999    0.46470004    0.73199999    0.16470002]]
[37m[1m[2023-06-25 12:09:41,125][129146] Max Reward on eval: 1803.059675744921
[37m[1m[2023-06-25 12:09:41,125][129146] Min Reward on eval: -939.9210545828566
[37m[1m[2023-06-25 12:09:41,125][129146] Mean Reward across all agents: 1044.0711799396672
[37m[1m[2023-06-25 12:09:41,126][129146] Average Trajectory Length: 999.5493333333333
[36m[2023-06-25 12:09:41,131][129146] mean_value=-21.963585017672717, max_value=1800.5225763384287
[37m[1m[2023-06-25 12:09:41,134][129146] New mean coefficients: [[ 0.30412325 -0.7048006   0.14154093  2.2347543   0.03642101]]
[37m[1m[2023-06-25 12:09:41,135][129146] Moving the mean solution point...
[36m[2023-06-25 12:09:50,750][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 12:09:50,751][129146] FPS: 399432.61
[36m[2023-06-25 12:09:50,753][129146] itr=1222, itrs=2000, Progress: 61.10%
[36m[2023-06-25 12:10:02,190][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 12:10:02,190][129146] FPS: 336483.47
[36m[2023-06-25 12:10:06,992][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:10:06,997][129146] Reward + Measures: [[1747.32624038    0.33905599    0.32355499    0.71990395    0.17871967]]
[37m[1m[2023-06-25 12:10:06,997][129146] Max Reward on eval: 1747.326240375333
[37m[1m[2023-06-25 12:10:06,998][129146] Min Reward on eval: 1747.326240375333
[37m[1m[2023-06-25 12:10:06,998][129146] Mean Reward across all agents: 1747.326240375333
[37m[1m[2023-06-25 12:10:06,998][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:10:12,435][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:10:12,436][129146] Reward + Measures: [[756.76517781   0.31158113   0.30551407   0.62295741   0.21398036]
[37m[1m [112.30945906   0.37550002   0.40449998   0.39649999   0.28839999]
[37m[1m [569.65088063   0.33406493   0.23971601   0.58579826   0.24083655]
[37m[1m ...
[37m[1m [340.25818917   0.54329389   0.13983987   0.70683926   0.39975891]
[37m[1m [966.574388     0.32780001   0.28749999   0.64130002   0.2368    ]
[37m[1m [707.40080248   0.32654643   0.33610579   0.56745827   0.20913382]]
[37m[1m[2023-06-25 12:10:12,436][129146] Max Reward on eval: 1807.4254718137904
[37m[1m[2023-06-25 12:10:12,437][129146] Min Reward on eval: -643.5583351056091
[37m[1m[2023-06-25 12:10:12,437][129146] Mean Reward across all agents: 831.9457840112514
[37m[1m[2023-06-25 12:10:12,437][129146] Average Trajectory Length: 954.5243333333333
[36m[2023-06-25 12:10:12,441][129146] mean_value=-128.77615916496634, max_value=1583.7329065305073
[37m[1m[2023-06-25 12:10:12,444][129146] New mean coefficients: [[ 0.05485454 -0.3604387   0.5774718   2.685944    0.31243703]]
[37m[1m[2023-06-25 12:10:12,445][129146] Moving the mean solution point...
[36m[2023-06-25 12:10:22,218][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 12:10:22,219][129146] FPS: 392996.48
[36m[2023-06-25 12:10:22,221][129146] itr=1223, itrs=2000, Progress: 61.15%
[36m[2023-06-25 12:10:33,642][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 12:10:33,642][129146] FPS: 337032.21
[36m[2023-06-25 12:10:38,447][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:10:38,447][129146] Reward + Measures: [[1676.09719684    0.3188723     0.31532899    0.76837695    0.20078567]]
[37m[1m[2023-06-25 12:10:38,447][129146] Max Reward on eval: 1676.0971968421281
[37m[1m[2023-06-25 12:10:38,448][129146] Min Reward on eval: 1676.0971968421281
[37m[1m[2023-06-25 12:10:38,448][129146] Mean Reward across all agents: 1676.0971968421281
[37m[1m[2023-06-25 12:10:38,448][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:10:44,014][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:10:44,015][129146] Reward + Measures: [[1355.22152833    0.31710002    0.34999999    0.49709997    0.13770001]
[37m[1m [ 935.46564671    0.40629998    0.35780001    0.60369998    0.20720001]
[37m[1m [1366.89247559    0.39560002    0.33360001    0.62470001    0.1499    ]
[37m[1m ...
[37m[1m [1420.43304096    0.3506        0.33090004    0.65289992    0.2067    ]
[37m[1m [ 348.39592476    0.35269997    0.26949999    0.45430002    0.2502    ]
[37m[1m [ 465.54881508    0.3996        0.51929998    0.43619999    0.3964    ]]
[37m[1m[2023-06-25 12:10:44,015][129146] Max Reward on eval: 1731.380512786028
[37m[1m[2023-06-25 12:10:44,016][129146] Min Reward on eval: -1350.464635407622
[37m[1m[2023-06-25 12:10:44,016][129146] Mean Reward across all agents: 835.6131266441489
[37m[1m[2023-06-25 12:10:44,016][129146] Average Trajectory Length: 978.7706666666667
[36m[2023-06-25 12:10:44,020][129146] mean_value=-8.695227244129782, max_value=1970.3660133889352
[37m[1m[2023-06-25 12:10:44,023][129146] New mean coefficients: [[-0.3309203  -0.27378425 -0.12578887  2.9842134   0.1609274 ]]
[37m[1m[2023-06-25 12:10:44,024][129146] Moving the mean solution point...
[36m[2023-06-25 12:10:53,712][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 12:10:53,712][129146] FPS: 396467.96
[36m[2023-06-25 12:10:53,714][129146] itr=1224, itrs=2000, Progress: 61.20%
[36m[2023-06-25 12:11:05,151][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 12:11:05,151][129146] FPS: 336475.93
[36m[2023-06-25 12:11:09,944][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:11:09,944][129146] Reward + Measures: [[1599.3963004     0.30563733    0.30643767    0.80686361    0.21369901]]
[37m[1m[2023-06-25 12:11:09,945][129146] Max Reward on eval: 1599.396300395752
[37m[1m[2023-06-25 12:11:09,945][129146] Min Reward on eval: 1599.396300395752
[37m[1m[2023-06-25 12:11:09,945][129146] Mean Reward across all agents: 1599.396300395752
[37m[1m[2023-06-25 12:11:09,945][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:11:15,328][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:11:15,329][129146] Reward + Measures: [[1156.68846161    0.52380002    0.38850003    0.70380002    0.15710001]
[37m[1m [1541.42906908    0.43770003    0.3321        0.74340004    0.18990001]
[37m[1m [ 980.79751688    0.52539998    0.26109999    0.73290002    0.1559    ]
[37m[1m ...
[37m[1m [ 995.15192575    0.4276        0.26799998    0.73089999    0.21329999]
[37m[1m [1327.17500256    0.4982        0.29430002    0.75040001    0.1507    ]
[37m[1m [ 848.50796096    0.26260003    0.39649999    0.56529999    0.31440002]]
[37m[1m[2023-06-25 12:11:15,329][129146] Max Reward on eval: 1582.4392326059285
[37m[1m[2023-06-25 12:11:15,329][129146] Min Reward on eval: -564.6513194757281
[37m[1m[2023-06-25 12:11:15,329][129146] Mean Reward across all agents: 950.8365874949037
[37m[1m[2023-06-25 12:11:15,330][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:11:15,337][129146] mean_value=290.24802081717723, max_value=1864.3903862880193
[37m[1m[2023-06-25 12:11:15,339][129146] New mean coefficients: [[-0.63677764 -0.5341266  -0.01116079  2.2550132  -0.00369921]]
[37m[1m[2023-06-25 12:11:15,341][129146] Moving the mean solution point...
[36m[2023-06-25 12:11:24,996][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 12:11:24,997][129146] FPS: 397756.52
[36m[2023-06-25 12:11:24,999][129146] itr=1225, itrs=2000, Progress: 61.25%
[36m[2023-06-25 12:11:36,411][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 12:11:36,412][129146] FPS: 337202.92
[36m[2023-06-25 12:11:41,189][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:11:41,189][129146] Reward + Measures: [[1515.6458284     0.29493868    0.294184      0.83562028    0.224941  ]]
[37m[1m[2023-06-25 12:11:41,190][129146] Max Reward on eval: 1515.6458283966358
[37m[1m[2023-06-25 12:11:41,190][129146] Min Reward on eval: 1515.6458283966358
[37m[1m[2023-06-25 12:11:41,190][129146] Mean Reward across all agents: 1515.6458283966358
[37m[1m[2023-06-25 12:11:41,190][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:11:46,834][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:11:46,840][129146] Reward + Measures: [[ 906.55900527    0.29790851    0.25169656    0.65662849    0.28577775]
[37m[1m [ 861.26376081    0.47409996    0.3802        0.65630001    0.32540002]
[37m[1m [ 994.51983812    0.33900002    0.25209999    0.6789        0.22650002]
[37m[1m ...
[37m[1m [1236.04804513    0.42069998    0.32440001    0.75800002    0.24029998]
[37m[1m [1359.77131035    0.32679999    0.27830002    0.81599998    0.23979998]
[37m[1m [1290.93545294    0.31120002    0.324         0.71439999    0.21960001]]
[37m[1m[2023-06-25 12:11:46,840][129146] Max Reward on eval: 1548.5878004746278
[37m[1m[2023-06-25 12:11:46,840][129146] Min Reward on eval: -89.23448196901009
[37m[1m[2023-06-25 12:11:46,841][129146] Mean Reward across all agents: 1041.20801200865
[37m[1m[2023-06-25 12:11:46,841][129146] Average Trajectory Length: 990.851
[36m[2023-06-25 12:11:46,846][129146] mean_value=113.73340439608575, max_value=2045.785999148176
[37m[1m[2023-06-25 12:11:46,849][129146] New mean coefficients: [[-0.3930147  -0.8517969  -0.03195155  2.1103115   0.10112921]]
[37m[1m[2023-06-25 12:11:46,850][129146] Moving the mean solution point...
[36m[2023-06-25 12:11:56,533][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 12:11:56,533][129146] FPS: 396657.21
[36m[2023-06-25 12:11:56,536][129146] itr=1226, itrs=2000, Progress: 61.30%
[36m[2023-06-25 12:12:07,986][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 12:12:07,987][129146] FPS: 336149.50
[36m[2023-06-25 12:12:12,726][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:12:12,726][129146] Reward + Measures: [[1473.81350789    0.27756798    0.283847      0.85628468    0.23112634]]
[37m[1m[2023-06-25 12:12:12,726][129146] Max Reward on eval: 1473.8135078890236
[37m[1m[2023-06-25 12:12:12,727][129146] Min Reward on eval: 1473.8135078890236
[37m[1m[2023-06-25 12:12:12,727][129146] Mean Reward across all agents: 1473.8135078890236
[37m[1m[2023-06-25 12:12:12,727][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:12:18,117][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:12:18,117][129146] Reward + Measures: [[1349.59627195    0.30860001    0.3132        0.76190007    0.22390001]
[37m[1m [1437.92949569    0.29809999    0.30599999    0.78920001    0.2299    ]
[37m[1m [ 376.47227332    0.52270001    0.08440001    0.83789998    0.44490001]
[37m[1m ...
[37m[1m [ 321.31250734    0.52940005    0.0794        0.84009993    0.46110001]
[37m[1m [ 336.00458323    0.52329999    0.1035        0.83040011    0.42860004]
[37m[1m [ 911.00217292    0.37360001    0.22070001    0.74330002    0.26610002]]
[37m[1m[2023-06-25 12:12:18,117][129146] Max Reward on eval: 1540.9211873724591
[37m[1m[2023-06-25 12:12:18,118][129146] Min Reward on eval: -115.71602492090896
[37m[1m[2023-06-25 12:12:18,118][129146] Mean Reward across all agents: 943.547341028916
[37m[1m[2023-06-25 12:12:18,118][129146] Average Trajectory Length: 996.8879999999999
[36m[2023-06-25 12:12:18,124][129146] mean_value=319.860446955442, max_value=1777.7245180135592
[37m[1m[2023-06-25 12:12:18,127][129146] New mean coefficients: [[-0.5938671  -1.119952   -0.51839197  2.7296672   0.19525608]]
[37m[1m[2023-06-25 12:12:18,128][129146] Moving the mean solution point...
[36m[2023-06-25 12:12:27,867][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 12:12:27,867][129146] FPS: 394386.17
[36m[2023-06-25 12:12:27,869][129146] itr=1227, itrs=2000, Progress: 61.35%
[36m[2023-06-25 12:12:39,286][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 12:12:39,287][129146] FPS: 337024.67
[36m[2023-06-25 12:12:44,053][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:12:44,053][129146] Reward + Measures: [[1428.38929379    0.26838499    0.26772067    0.87288231    0.23717567]]
[37m[1m[2023-06-25 12:12:44,054][129146] Max Reward on eval: 1428.3892937868518
[37m[1m[2023-06-25 12:12:44,054][129146] Min Reward on eval: 1428.3892937868518
[37m[1m[2023-06-25 12:12:44,054][129146] Mean Reward across all agents: 1428.3892937868518
[37m[1m[2023-06-25 12:12:44,054][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:12:49,554][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:12:49,554][129146] Reward + Measures: [[1378.37149247    0.33829999    0.23410001    0.84899998    0.222     ]
[37m[1m [ 941.697534      0.31329998    0.35419998    0.63529998    0.21269999]
[37m[1m [ 899.83072497    0.37279999    0.34889999    0.62659997    0.1723    ]
[37m[1m ...
[37m[1m [1142.98150289    0.30670002    0.3574        0.69020003    0.20300002]
[37m[1m [1069.48621088    0.31820002    0.40850002    0.6408        0.25869998]
[37m[1m [ 755.66209762    0.49439999    0.20939998    0.64300001    0.21990001]]
[37m[1m[2023-06-25 12:12:49,555][129146] Max Reward on eval: 1474.0619890994626
[37m[1m[2023-06-25 12:12:49,555][129146] Min Reward on eval: -206.89444526713342
[37m[1m[2023-06-25 12:12:49,555][129146] Mean Reward across all agents: 1010.6273040648391
[37m[1m[2023-06-25 12:12:49,556][129146] Average Trajectory Length: 999.7439999999999
[36m[2023-06-25 12:12:49,561][129146] mean_value=56.08198444181205, max_value=1938.8257847121217
[37m[1m[2023-06-25 12:12:49,564][129146] New mean coefficients: [[-0.5807561  -0.8822642  -0.50339264  2.6977189  -0.04051761]]
[37m[1m[2023-06-25 12:12:49,565][129146] Moving the mean solution point...
[36m[2023-06-25 12:12:59,274][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 12:12:59,275][129146] FPS: 395574.14
[36m[2023-06-25 12:12:59,277][129146] itr=1228, itrs=2000, Progress: 61.40%
[36m[2023-06-25 12:13:10,728][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 12:13:10,728][129146] FPS: 336064.92
[36m[2023-06-25 12:13:15,609][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:13:15,610][129146] Reward + Measures: [[827.49900463   0.32172567   0.27268898   0.87111765   0.31882998]]
[37m[1m[2023-06-25 12:13:15,610][129146] Max Reward on eval: 827.4990046311251
[37m[1m[2023-06-25 12:13:15,610][129146] Min Reward on eval: 827.4990046311251
[37m[1m[2023-06-25 12:13:15,611][129146] Mean Reward across all agents: 827.4990046311251
[37m[1m[2023-06-25 12:13:15,611][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:13:21,092][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:13:21,144][129146] Reward + Measures: [[782.79060941   0.3522       0.25320002   0.84720004   0.30660003]
[37m[1m [-51.6905143    0.78970003   0.55580002   0.96400005   0.79699999]
[37m[1m [  2.11528489   0.49110004   0.1595       0.70230001   0.39299998]
[37m[1m ...
[37m[1m [650.87831097   0.34709999   0.22940002   0.58190006   0.20030001]
[37m[1m [226.45143313   0.49000001   0.19920002   0.90100002   0.50400001]
[37m[1m [817.99447667   0.33519998   0.31410003   0.77420002   0.25960001]]
[37m[1m[2023-06-25 12:13:21,144][129146] Max Reward on eval: 1053.4455347672979
[37m[1m[2023-06-25 12:13:21,145][129146] Min Reward on eval: -1187.0033293379704
[37m[1m[2023-06-25 12:13:21,145][129146] Mean Reward across all agents: 385.31248570430506
[37m[1m[2023-06-25 12:13:21,145][129146] Average Trajectory Length: 999.9466666666666
[36m[2023-06-25 12:13:21,154][129146] mean_value=448.9236012399602, max_value=1444.7073145318777
[37m[1m[2023-06-25 12:13:21,157][129146] New mean coefficients: [[-0.8818592   0.03110367 -0.2905751   2.44551     0.09023584]]
[37m[1m[2023-06-25 12:13:21,158][129146] Moving the mean solution point...
[36m[2023-06-25 12:13:30,827][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 12:13:30,827][129146] FPS: 397217.61
[36m[2023-06-25 12:13:30,829][129146] itr=1229, itrs=2000, Progress: 61.45%
[36m[2023-06-25 12:13:42,425][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 12:13:42,425][129146] FPS: 331942.43
[36m[2023-06-25 12:13:47,144][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:13:47,144][129146] Reward + Measures: [[82.33616721  0.61749333  0.001379    0.90117526  0.74275893]]
[37m[1m[2023-06-25 12:13:47,144][129146] Max Reward on eval: 82.3361672057093
[37m[1m[2023-06-25 12:13:47,144][129146] Min Reward on eval: 82.3361672057093
[37m[1m[2023-06-25 12:13:47,145][129146] Mean Reward across all agents: 82.3361672057093
[37m[1m[2023-06-25 12:13:47,145][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:13:52,708][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:13:52,714][129146] Reward + Measures: [[-341.95065909    0.9806        0.            0.99300003    0.97840005]
[37m[1m [-756.21759404    0.72180003    0.6803        0.80380005    0.80649996]
[37m[1m [ -13.89045       0.8544001     0.6383        0.88420004    0.81420004]
[37m[1m ...
[37m[1m [-238.71236181    0.92210001    0.60359997    0.9799        0.96250004]
[37m[1m [ 233.09347828    0.829         0.65399998    0.87449998    0.0008    ]
[37m[1m [ -65.45240524    0.9745        0.82230008    0.9939        0.98660004]]
[37m[1m[2023-06-25 12:13:52,714][129146] Max Reward on eval: 392.1512391770608
[37m[1m[2023-06-25 12:13:52,715][129146] Min Reward on eval: -1920.6738602820783
[37m[1m[2023-06-25 12:13:52,715][129146] Mean Reward across all agents: -610.926039777627
[37m[1m[2023-06-25 12:13:52,715][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:13:52,719][129146] mean_value=-746.6156536224904, max_value=578.2629061830114
[37m[1m[2023-06-25 12:13:52,721][129146] New mean coefficients: [[ 0.6935084  -0.12082213 -0.55508804  2.6698985  -0.00604992]]
[37m[1m[2023-06-25 12:13:52,723][129146] Moving the mean solution point...
[36m[2023-06-25 12:14:02,418][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 12:14:02,418][129146] FPS: 396130.65
[36m[2023-06-25 12:14:02,421][129146] itr=1230, itrs=2000, Progress: 61.50%
[37m[1m[2023-06-25 12:14:09,873][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001210
[36m[2023-06-25 12:14:21,605][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 12:14:21,605][129146] FPS: 333019.99
[36m[2023-06-25 12:14:26,410][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:14:26,410][129146] Reward + Measures: [[382.88709303   0.47710496   0.09248167   0.83965868   0.31661066]]
[37m[1m[2023-06-25 12:14:26,410][129146] Max Reward on eval: 382.887093027861
[37m[1m[2023-06-25 12:14:26,411][129146] Min Reward on eval: 382.887093027861
[37m[1m[2023-06-25 12:14:26,411][129146] Mean Reward across all agents: 382.887093027861
[37m[1m[2023-06-25 12:14:26,411][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:14:31,884][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:14:31,885][129146] Reward + Measures: [[523.75868189   0.4066       0.15009999   0.80009997   0.27120003]
[37m[1m [483.44912688   0.4165       0.14380001   0.81730002   0.27429998]
[37m[1m [580.0117574    0.33704847   0.20781031   0.73095465   0.24040413]
[37m[1m ...
[37m[1m [329.44457658   0.59430003   0.0406       0.86989993   0.36160001]
[37m[1m [463.97497379   0.43200001   0.15109999   0.83160001   0.28049999]
[37m[1m [492.70187462   0.57089996   0.06500001   0.83500004   0.33629999]]
[37m[1m[2023-06-25 12:14:31,885][129146] Max Reward on eval: 795.1486643280543
[37m[1m[2023-06-25 12:14:31,885][129146] Min Reward on eval: 53.6041130818252
[37m[1m[2023-06-25 12:14:31,886][129146] Mean Reward across all agents: 376.4799878720207
[37m[1m[2023-06-25 12:14:31,886][129146] Average Trajectory Length: 998.679
[36m[2023-06-25 12:14:31,892][129146] mean_value=447.3694634275054, max_value=971.663745134726
[37m[1m[2023-06-25 12:14:31,894][129146] New mean coefficients: [[ 0.68795955 -0.69397384 -0.1957125   1.1173515  -0.00706057]]
[37m[1m[2023-06-25 12:14:31,895][129146] Moving the mean solution point...
[36m[2023-06-25 12:14:41,587][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 12:14:41,587][129146] FPS: 396291.44
[36m[2023-06-25 12:14:41,589][129146] itr=1231, itrs=2000, Progress: 61.55%
[36m[2023-06-25 12:14:53,162][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 12:14:53,162][129146] FPS: 332594.90
[36m[2023-06-25 12:14:57,986][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:14:57,986][129146] Reward + Measures: [[708.53582481   0.39884603   0.19240966   0.73995966   0.28655168]]
[37m[1m[2023-06-25 12:14:57,987][129146] Max Reward on eval: 708.5358248068329
[37m[1m[2023-06-25 12:14:57,987][129146] Min Reward on eval: 708.5358248068329
[37m[1m[2023-06-25 12:14:57,987][129146] Mean Reward across all agents: 708.5358248068329
[37m[1m[2023-06-25 12:14:57,987][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:15:03,528][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:15:03,528][129146] Reward + Measures: [[651.54111186   0.39480001   0.1981       0.73440003   0.29140002]
[37m[1m [675.37195755   0.4021       0.1884       0.74620003   0.2897    ]
[37m[1m [703.47635262   0.37120003   0.20870002   0.72900003   0.26830003]
[37m[1m ...
[37m[1m [723.31869354   0.37809998   0.19939999   0.7445001    0.26999998]
[37m[1m [726.36729526   0.39630005   0.18499999   0.75439996   0.27400002]
[37m[1m [668.3470545    0.43190002   0.17570001   0.74860001   0.29249999]]
[37m[1m[2023-06-25 12:15:03,528][129146] Max Reward on eval: 832.8288001400418
[37m[1m[2023-06-25 12:15:03,529][129146] Min Reward on eval: 361.3413056401332
[37m[1m[2023-06-25 12:15:03,529][129146] Mean Reward across all agents: 707.4413274466889
[37m[1m[2023-06-25 12:15:03,529][129146] Average Trajectory Length: 998.794
[36m[2023-06-25 12:15:03,532][129146] mean_value=-90.32011098635184, max_value=1087.0972367406007
[37m[1m[2023-06-25 12:15:03,534][129146] New mean coefficients: [[ 0.73202413 -0.22988537 -0.11796343  2.9017756   0.19165324]]
[37m[1m[2023-06-25 12:15:03,535][129146] Moving the mean solution point...
[36m[2023-06-25 12:15:13,302][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 12:15:13,302][129146] FPS: 393249.91
[36m[2023-06-25 12:15:13,304][129146] itr=1232, itrs=2000, Progress: 61.60%
[36m[2023-06-25 12:15:24,747][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 12:15:24,747][129146] FPS: 336380.96
[36m[2023-06-25 12:15:29,500][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:15:29,500][129146] Reward + Measures: [[710.74120925   0.40612301   0.18615833   0.75448596   0.290084  ]]
[37m[1m[2023-06-25 12:15:29,500][129146] Max Reward on eval: 710.7412092530201
[37m[1m[2023-06-25 12:15:29,500][129146] Min Reward on eval: 710.7412092530201
[37m[1m[2023-06-25 12:15:29,501][129146] Mean Reward across all agents: 710.7412092530201
[37m[1m[2023-06-25 12:15:29,501][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:15:34,886][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:15:34,886][129146] Reward + Measures: [[551.1969389    0.51060003   0.14600001   0.79370004   0.36290002]
[37m[1m [417.87908906   0.74079996   0.24560001   0.8075       0.5345    ]
[37m[1m [570.58784916   0.4258       0.18719999   0.7597       0.31110001]
[37m[1m ...
[37m[1m [669.2238266    0.46449995   0.16140001   0.7938       0.34260002]
[37m[1m [599.37072846   0.53639996   0.17930001   0.82269996   0.39550003]
[37m[1m [329.68935731   0.7658       0.1505       0.85900003   0.52500004]]
[37m[1m[2023-06-25 12:15:34,886][129146] Max Reward on eval: 811.7667749407731
[37m[1m[2023-06-25 12:15:34,887][129146] Min Reward on eval: -182.28990690737555
[37m[1m[2023-06-25 12:15:34,887][129146] Mean Reward across all agents: 506.776066713964
[37m[1m[2023-06-25 12:15:34,887][129146] Average Trajectory Length: 931.809
[36m[2023-06-25 12:15:34,891][129146] mean_value=66.67090498512795, max_value=1070.1075530106202
[37m[1m[2023-06-25 12:15:34,894][129146] New mean coefficients: [[ 0.7287851   0.41452655 -0.3067251   3.4773974   0.09915379]]
[37m[1m[2023-06-25 12:15:34,895][129146] Moving the mean solution point...
[36m[2023-06-25 12:15:44,491][129146] train() took 9.60 seconds to complete
[36m[2023-06-25 12:15:44,492][129146] FPS: 400196.39
[36m[2023-06-25 12:15:44,494][129146] itr=1233, itrs=2000, Progress: 61.65%
[36m[2023-06-25 12:15:55,907][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 12:15:55,907][129146] FPS: 337263.57
[36m[2023-06-25 12:16:00,615][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:16:00,616][129146] Reward + Measures: [[702.34126379   0.42608169   0.17837435   0.77794135   0.30649933]]
[37m[1m[2023-06-25 12:16:00,616][129146] Max Reward on eval: 702.3412637902741
[37m[1m[2023-06-25 12:16:00,616][129146] Min Reward on eval: 702.3412637902741
[37m[1m[2023-06-25 12:16:00,616][129146] Mean Reward across all agents: 702.3412637902741
[37m[1m[2023-06-25 12:16:00,617][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:16:06,227][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:16:06,228][129146] Reward + Measures: [[776.28460822   0.4104       0.2271       0.75910008   0.28770003]
[37m[1m [660.73229464   0.4303       0.19319999   0.77969998   0.31690001]
[37m[1m [708.71052621   0.4549       0.17470001   0.79880005   0.3163    ]
[37m[1m ...
[37m[1m [839.85171097   0.36890003   0.245        0.7428       0.26750001]
[37m[1m [659.31874657   0.37350002   0.26160002   0.75990003   0.28199998]
[37m[1m [728.90998869   0.28049999   0.294        0.64250004   0.21640001]]
[37m[1m[2023-06-25 12:16:06,228][129146] Max Reward on eval: 923.5294078227947
[37m[1m[2023-06-25 12:16:06,228][129146] Min Reward on eval: 387.3688855804852
[37m[1m[2023-06-25 12:16:06,229][129146] Mean Reward across all agents: 713.5215933527643
[37m[1m[2023-06-25 12:16:06,229][129146] Average Trajectory Length: 999.7226666666667
[36m[2023-06-25 12:16:06,231][129146] mean_value=-170.4386232955056, max_value=487.6146906581503
[37m[1m[2023-06-25 12:16:06,233][129146] New mean coefficients: [[ 1.281676    0.2831244  -0.11954758  2.884818   -0.21358919]]
[37m[1m[2023-06-25 12:16:06,234][129146] Moving the mean solution point...
[36m[2023-06-25 12:16:15,894][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 12:16:15,895][129146] FPS: 397578.71
[36m[2023-06-25 12:16:15,897][129146] itr=1234, itrs=2000, Progress: 61.70%
[36m[2023-06-25 12:16:27,338][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 12:16:27,339][129146] FPS: 336311.73
[36m[2023-06-25 12:16:32,198][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:16:32,199][129146] Reward + Measures: [[775.21435997   0.42302057   0.18026821   0.77989215   0.2978864 ]]
[37m[1m[2023-06-25 12:16:32,199][129146] Max Reward on eval: 775.2143599672208
[37m[1m[2023-06-25 12:16:32,199][129146] Min Reward on eval: 775.2143599672208
[37m[1m[2023-06-25 12:16:32,199][129146] Mean Reward across all agents: 775.2143599672208
[37m[1m[2023-06-25 12:16:32,200][129146] Average Trajectory Length: 999.9663333333333
[36m[2023-06-25 12:16:37,702][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:16:37,702][129146] Reward + Measures: [[614.77257344   0.38260004   0.2076       0.69480002   0.23150001]
[37m[1m [799.34260987   0.3836       0.18700001   0.76770002   0.26890001]
[37m[1m [580.86659112   0.34490004   0.21689999   0.66660005   0.2313    ]
[37m[1m ...
[37m[1m [822.5531461    0.32599998   0.22860003   0.67160004   0.2277    ]
[37m[1m [621.82541474   0.46700001   0.1948       0.74350005   0.31389999]
[37m[1m [833.6551689    0.37630001   0.23390003   0.75220007   0.26030001]]
[37m[1m[2023-06-25 12:16:37,702][129146] Max Reward on eval: 957.4670749089564
[37m[1m[2023-06-25 12:16:37,703][129146] Min Reward on eval: 365.15031226606806
[37m[1m[2023-06-25 12:16:37,703][129146] Mean Reward across all agents: 741.5757032595001
[37m[1m[2023-06-25 12:16:37,703][129146] Average Trajectory Length: 998.8096666666667
[36m[2023-06-25 12:16:37,706][129146] mean_value=-203.29289362796055, max_value=725.1674071849698
[37m[1m[2023-06-25 12:16:37,708][129146] New mean coefficients: [[ 0.37881726  0.3212278  -0.15466812  2.392114   -0.07305479]]
[37m[1m[2023-06-25 12:16:37,709][129146] Moving the mean solution point...
[36m[2023-06-25 12:16:47,473][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 12:16:47,473][129146] FPS: 393360.55
[36m[2023-06-25 12:16:47,475][129146] itr=1235, itrs=2000, Progress: 61.75%
[36m[2023-06-25 12:16:59,094][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 12:16:59,094][129146] FPS: 331183.09
[36m[2023-06-25 12:17:04,044][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:17:04,044][129146] Reward + Measures: [[783.25350069   0.42656401   0.173941     0.80015594   0.29840332]]
[37m[1m[2023-06-25 12:17:04,044][129146] Max Reward on eval: 783.2535006866717
[37m[1m[2023-06-25 12:17:04,045][129146] Min Reward on eval: 783.2535006866717
[37m[1m[2023-06-25 12:17:04,045][129146] Mean Reward across all agents: 783.2535006866717
[37m[1m[2023-06-25 12:17:04,045][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:17:09,521][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:17:09,522][129146] Reward + Measures: [[793.27751146   0.44849998   0.1718       0.80150002   0.30509999]
[37m[1m [631.1782432    0.43239999   0.18350001   0.76710004   0.32909998]
[37m[1m [766.54723522   0.4068       0.20079999   0.77329999   0.29790002]
[37m[1m ...
[37m[1m [814.33736187   0.42869997   0.17779998   0.8064       0.2802    ]
[37m[1m [785.30152966   0.40980002   0.16900001   0.81750005   0.26440001]
[37m[1m [743.76891673   0.43439999   0.20320001   0.7743001    0.2112    ]]
[37m[1m[2023-06-25 12:17:09,522][129146] Max Reward on eval: 922.6159147640341
[37m[1m[2023-06-25 12:17:09,522][129146] Min Reward on eval: 245.56220731801295
[37m[1m[2023-06-25 12:17:09,522][129146] Mean Reward across all agents: 725.9195666207587
[37m[1m[2023-06-25 12:17:09,523][129146] Average Trajectory Length: 999.7643333333333
[36m[2023-06-25 12:17:09,525][129146] mean_value=-63.04822240692947, max_value=1297.2584082812798
[37m[1m[2023-06-25 12:17:09,528][129146] New mean coefficients: [[ 0.5931705   0.49692988 -0.39175376  2.2838902   0.2492111 ]]
[37m[1m[2023-06-25 12:17:09,529][129146] Moving the mean solution point...
[36m[2023-06-25 12:17:19,327][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 12:17:19,327][129146] FPS: 391983.71
[36m[2023-06-25 12:17:19,329][129146] itr=1236, itrs=2000, Progress: 61.80%
[36m[2023-06-25 12:17:30,948][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 12:17:30,948][129146] FPS: 331233.09
[36m[2023-06-25 12:17:35,800][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:17:35,800][129146] Reward + Measures: [[784.74897198   0.44337067   0.16579767   0.81965494   0.30712834]]
[37m[1m[2023-06-25 12:17:35,801][129146] Max Reward on eval: 784.7489719766635
[37m[1m[2023-06-25 12:17:35,801][129146] Min Reward on eval: 784.7489719766635
[37m[1m[2023-06-25 12:17:35,801][129146] Mean Reward across all agents: 784.7489719766635
[37m[1m[2023-06-25 12:17:35,801][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:17:41,369][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:17:41,370][129146] Reward + Measures: [[869.11887992   0.4086       0.17640001   0.8179       0.28519997]
[37m[1m [421.6996329    0.60770005   0.1286       0.81380004   0.38469997]
[37m[1m [584.16437748   0.4522       0.1415       0.83579999   0.33220002]
[37m[1m ...
[37m[1m [118.82373007   0.58850002   0.13530001   0.75290006   0.32290003]
[37m[1m [569.76854021   0.45250002   0.14479999   0.81709999   0.31280002]
[37m[1m [841.9289808    0.3529       0.2105       0.78800005   0.26449999]]
[37m[1m[2023-06-25 12:17:41,370][129146] Max Reward on eval: 1007.7585867939924
[37m[1m[2023-06-25 12:17:41,370][129146] Min Reward on eval: -93.33713274961046
[37m[1m[2023-06-25 12:17:41,371][129146] Mean Reward across all agents: 616.8088680226402
[37m[1m[2023-06-25 12:17:41,371][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:17:41,374][129146] mean_value=-0.8200424739754575, max_value=1256.2826481166796
[37m[1m[2023-06-25 12:17:41,377][129146] New mean coefficients: [[ 0.4533218   0.66649234 -0.38555464  2.6928775   0.01686159]]
[37m[1m[2023-06-25 12:17:41,378][129146] Moving the mean solution point...
[36m[2023-06-25 12:17:51,079][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 12:17:51,079][129146] FPS: 395881.31
[36m[2023-06-25 12:17:51,082][129146] itr=1237, itrs=2000, Progress: 61.85%
[36m[2023-06-25 12:18:02,684][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 12:18:02,684][129146] FPS: 331756.84
[36m[2023-06-25 12:18:07,515][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:18:07,515][129146] Reward + Measures: [[736.79630692   0.47979262   0.151655     0.8386237    0.33123899]]
[37m[1m[2023-06-25 12:18:07,516][129146] Max Reward on eval: 736.7963069225692
[37m[1m[2023-06-25 12:18:07,516][129146] Min Reward on eval: 736.7963069225692
[37m[1m[2023-06-25 12:18:07,516][129146] Mean Reward across all agents: 736.7963069225692
[37m[1m[2023-06-25 12:18:07,516][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:18:12,997][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:18:12,998][129146] Reward + Measures: [[747.0652419    0.47399998   0.13639998   0.86550009   0.3046    ]
[37m[1m [648.3545893    0.51750004   0.1401       0.83900005   0.36810002]
[37m[1m [745.36963908   0.47750002   0.15050001   0.8466       0.32820001]
[37m[1m ...
[37m[1m [652.59636236   0.51159996   0.14219999   0.84399998   0.34290001]
[37m[1m [779.29774565   0.46580002   0.1424       0.84689993   0.26690003]
[37m[1m [684.1303054    0.48800001   0.1353       0.86310005   0.34029999]]
[37m[1m[2023-06-25 12:18:12,998][129146] Max Reward on eval: 817.0204058776028
[37m[1m[2023-06-25 12:18:12,999][129146] Min Reward on eval: 553.4249317498994
[37m[1m[2023-06-25 12:18:12,999][129146] Mean Reward across all agents: 715.4268411214417
[37m[1m[2023-06-25 12:18:12,999][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:18:13,002][129146] mean_value=82.97155774496218, max_value=800.5326113252731
[37m[1m[2023-06-25 12:18:13,005][129146] New mean coefficients: [[ 1.3078513   1.1515895  -0.93058574  3.082987    0.14616728]]
[37m[1m[2023-06-25 12:18:13,006][129146] Moving the mean solution point...
[36m[2023-06-25 12:18:22,801][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 12:18:22,801][129146] FPS: 392114.29
[36m[2023-06-25 12:18:22,803][129146] itr=1238, itrs=2000, Progress: 61.90%
[36m[2023-06-25 12:18:34,311][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 12:18:34,312][129146] FPS: 334415.26
[36m[2023-06-25 12:18:39,096][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:18:39,096][129146] Reward + Measures: [[731.48852006   0.51542199   0.13576233   0.85964167   0.35503966]]
[37m[1m[2023-06-25 12:18:39,097][129146] Max Reward on eval: 731.4885200638082
[37m[1m[2023-06-25 12:18:39,097][129146] Min Reward on eval: 731.4885200638082
[37m[1m[2023-06-25 12:18:39,097][129146] Mean Reward across all agents: 731.4885200638082
[37m[1m[2023-06-25 12:18:39,097][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:18:44,659][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:18:44,659][129146] Reward + Measures: [[712.13772808   0.49860001   0.1213       0.86479998   0.32430002]
[37m[1m [656.29062962   0.5564       0.13940001   0.85269994   0.38580003]
[37m[1m [716.04688163   0.47740003   0.1209       0.87319994   0.30669999]
[37m[1m ...
[37m[1m [736.00901816   0.51019996   0.1454       0.85190004   0.31029996]
[37m[1m [740.14203412   0.49810001   0.12880002   0.87080002   0.3317    ]
[37m[1m [708.35795063   0.48370001   0.1182       0.875        0.30590001]]
[37m[1m[2023-06-25 12:18:44,659][129146] Max Reward on eval: 947.5513268150622
[37m[1m[2023-06-25 12:18:44,660][129146] Min Reward on eval: 496.07348952419125
[37m[1m[2023-06-25 12:18:44,660][129146] Mean Reward across all agents: 729.4972358350684
[37m[1m[2023-06-25 12:18:44,660][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:18:44,664][129146] mean_value=92.88385276924835, max_value=1286.8734574258094
[37m[1m[2023-06-25 12:18:44,667][129146] New mean coefficients: [[ 1.6908822   0.95433855 -0.872931    3.4886117   0.12090984]]
[37m[1m[2023-06-25 12:18:44,668][129146] Moving the mean solution point...
[36m[2023-06-25 12:18:54,265][129146] train() took 9.60 seconds to complete
[36m[2023-06-25 12:18:54,265][129146] FPS: 400187.56
[36m[2023-06-25 12:18:54,267][129146] itr=1239, itrs=2000, Progress: 61.95%
[36m[2023-06-25 12:19:05,691][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 12:19:05,691][129146] FPS: 336837.31
[36m[2023-06-25 12:19:10,607][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:19:10,608][129146] Reward + Measures: [[798.55212873   0.49264729   0.13886701   0.8597163    0.340729  ]]
[37m[1m[2023-06-25 12:19:10,608][129146] Max Reward on eval: 798.5521287345238
[37m[1m[2023-06-25 12:19:10,608][129146] Min Reward on eval: 798.5521287345238
[37m[1m[2023-06-25 12:19:10,608][129146] Mean Reward across all agents: 798.5521287345238
[37m[1m[2023-06-25 12:19:10,609][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:19:16,129][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:19:16,130][129146] Reward + Measures: [[729.59592563   0.48650002   0.12960002   0.86440003   0.3513    ]
[37m[1m [707.13457405   0.52489996   0.11790001   0.85529995   0.35609996]
[37m[1m [681.66600488   0.5266       0.1303       0.85159999   0.36050001]
[37m[1m ...
[37m[1m [732.57314592   0.51770002   0.126        0.86700004   0.34940001]
[37m[1m [647.96662705   0.43759999   0.1202       0.8387       0.32430002]
[37m[1m [793.90632339   0.46529999   0.12719999   0.86040002   0.30759999]]
[37m[1m[2023-06-25 12:19:16,130][129146] Max Reward on eval: 906.2528046478371
[37m[1m[2023-06-25 12:19:16,130][129146] Min Reward on eval: 526.8886728417826
[37m[1m[2023-06-25 12:19:16,131][129146] Mean Reward across all agents: 742.4391026526305
[37m[1m[2023-06-25 12:19:16,131][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:19:16,134][129146] mean_value=39.08363513809916, max_value=676.4777905695485
[37m[1m[2023-06-25 12:19:16,137][129146] New mean coefficients: [[ 1.4258463   0.9651203  -0.9130373   3.1596594   0.15089934]]
[37m[1m[2023-06-25 12:19:16,138][129146] Moving the mean solution point...
[36m[2023-06-25 12:19:25,909][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 12:19:25,909][129146] FPS: 393078.60
[36m[2023-06-25 12:19:25,911][129146] itr=1240, itrs=2000, Progress: 62.00%
[37m[1m[2023-06-25 12:19:33,624][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001220
[36m[2023-06-25 12:19:45,448][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 12:19:45,448][129146] FPS: 330856.41
[36m[2023-06-25 12:19:50,191][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:19:50,192][129146] Reward + Measures: [[916.56819745   0.45980069   0.14704166   0.85732472   0.31719831]]
[37m[1m[2023-06-25 12:19:50,192][129146] Max Reward on eval: 916.5681974483506
[37m[1m[2023-06-25 12:19:50,192][129146] Min Reward on eval: 916.5681974483506
[37m[1m[2023-06-25 12:19:50,192][129146] Mean Reward across all agents: 916.5681974483506
[37m[1m[2023-06-25 12:19:50,193][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:19:55,694][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:19:55,695][129146] Reward + Measures: [[ 309.39243423    0.55430001    0.09670001    0.83099997    0.32010004]
[37m[1m [ 547.1468901     0.465         0.062         0.85260004    0.29849997]
[37m[1m [1042.05892977    0.38470003    0.14429998    0.84790003    0.28349999]
[37m[1m ...
[37m[1m [ 743.26352423    0.51209998    0.12639999    0.87140006    0.35859999]
[37m[1m [ 358.72244977    0.50640005    0.0551        0.85479993    0.32940003]
[37m[1m [ 787.04368793    0.48400003    0.14620002    0.84580004    0.30160004]]
[37m[1m[2023-06-25 12:19:55,695][129146] Max Reward on eval: 1346.6788818648085
[37m[1m[2023-06-25 12:19:55,695][129146] Min Reward on eval: 130.22857961173867
[37m[1m[2023-06-25 12:19:55,696][129146] Mean Reward across all agents: 801.9828692248262
[37m[1m[2023-06-25 12:19:55,696][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:19:55,701][129146] mean_value=92.79112723335844, max_value=1718.1292662372464
[37m[1m[2023-06-25 12:19:55,704][129146] New mean coefficients: [[ 1.9090569   0.7000667  -0.6053236   2.6140969  -0.10373746]]
[37m[1m[2023-06-25 12:19:55,705][129146] Moving the mean solution point...
[36m[2023-06-25 12:20:05,386][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 12:20:05,387][129146] FPS: 396702.47
[36m[2023-06-25 12:20:05,389][129146] itr=1241, itrs=2000, Progress: 62.05%
[36m[2023-06-25 12:20:16,867][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 12:20:16,868][129146] FPS: 335361.84
[36m[2023-06-25 12:20:21,644][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:20:21,645][129146] Reward + Measures: [[1080.78929877    0.4070437     0.15158366    0.84858268    0.28159431]]
[37m[1m[2023-06-25 12:20:21,645][129146] Max Reward on eval: 1080.7892987667453
[37m[1m[2023-06-25 12:20:21,645][129146] Min Reward on eval: 1080.7892987667453
[37m[1m[2023-06-25 12:20:21,646][129146] Mean Reward across all agents: 1080.7892987667453
[37m[1m[2023-06-25 12:20:21,646][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:20:27,272][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:20:27,272][129146] Reward + Measures: [[ 948.3499594     0.32620001    0.22119999    0.77070004    0.2687    ]
[37m[1m [1012.00025126    0.37260002    0.26719999    0.74510002    0.27360001]
[37m[1m [ 953.97189801    0.3407        0.176         0.77139997    0.25869998]
[37m[1m ...
[37m[1m [1004.45590904    0.31459999    0.18410002    0.78820002    0.25050002]
[37m[1m [ 590.70002859    0.35350001    0.2404        0.66119999    0.27239999]
[37m[1m [1036.57942694    0.3802        0.2059        0.83290005    0.28439999]]
[37m[1m[2023-06-25 12:20:27,272][129146] Max Reward on eval: 1294.358643638098
[37m[1m[2023-06-25 12:20:27,273][129146] Min Reward on eval: -401.6917777978582
[37m[1m[2023-06-25 12:20:27,273][129146] Mean Reward across all agents: 856.7473452780524
[37m[1m[2023-06-25 12:20:27,273][129146] Average Trajectory Length: 997.0596666666667
[36m[2023-06-25 12:20:27,277][129146] mean_value=-89.78503386748784, max_value=1351.7792627934434
[37m[1m[2023-06-25 12:20:27,280][129146] New mean coefficients: [[ 2.2820961   0.3165464  -0.3610695   1.5711079  -0.23387629]]
[37m[1m[2023-06-25 12:20:27,281][129146] Moving the mean solution point...
[36m[2023-06-25 12:20:36,996][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 12:20:36,996][129146] FPS: 395304.54
[36m[2023-06-25 12:20:36,999][129146] itr=1242, itrs=2000, Progress: 62.10%
[36m[2023-06-25 12:20:48,585][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 12:20:48,586][129146] FPS: 332092.00
[36m[2023-06-25 12:20:53,413][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:20:53,413][129146] Reward + Measures: [[1229.27265377    0.35845       0.155632      0.83655763    0.25330201]]
[37m[1m[2023-06-25 12:20:53,413][129146] Max Reward on eval: 1229.2726537655217
[37m[1m[2023-06-25 12:20:53,414][129146] Min Reward on eval: 1229.2726537655217
[37m[1m[2023-06-25 12:20:53,414][129146] Mean Reward across all agents: 1229.2726537655217
[37m[1m[2023-06-25 12:20:53,414][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:20:58,922][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:20:58,923][129146] Reward + Measures: [[1139.29393005    0.3933        0.16010001    0.83249998    0.2703    ]
[37m[1m [1127.96732446    0.37689999    0.1596        0.83840001    0.26730001]
[37m[1m [1071.41819611    0.43360001    0.1603        0.82390004    0.29529998]
[37m[1m ...
[37m[1m [1037.13930082    0.41739997    0.1513        0.83400005    0.2685    ]
[37m[1m [1159.47558272    0.3969        0.1569        0.82540005    0.27420002]
[37m[1m [1180.14302045    0.3565        0.1807        0.81669998    0.26430002]]
[37m[1m[2023-06-25 12:20:58,923][129146] Max Reward on eval: 1328.570624885778
[37m[1m[2023-06-25 12:20:58,923][129146] Min Reward on eval: 891.345496005821
[37m[1m[2023-06-25 12:20:58,923][129146] Mean Reward across all agents: 1149.754295158002
[37m[1m[2023-06-25 12:20:58,924][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:20:58,928][129146] mean_value=47.41717917131136, max_value=265.9967799836236
[37m[1m[2023-06-25 12:20:58,930][129146] New mean coefficients: [[ 1.8549206   0.780343   -0.60679644  1.7494162   0.07628995]]
[37m[1m[2023-06-25 12:20:58,932][129146] Moving the mean solution point...
[36m[2023-06-25 12:21:08,710][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 12:21:08,710][129146] FPS: 392757.80
[36m[2023-06-25 12:21:08,713][129146] itr=1243, itrs=2000, Progress: 62.15%
[36m[2023-06-25 12:21:20,151][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 12:21:20,151][129146] FPS: 336416.92
[36m[2023-06-25 12:21:24,987][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:21:24,988][129146] Reward + Measures: [[1379.67086816    0.31400397    0.15959133    0.83046532    0.23292598]]
[37m[1m[2023-06-25 12:21:24,988][129146] Max Reward on eval: 1379.6708681639727
[37m[1m[2023-06-25 12:21:24,988][129146] Min Reward on eval: 1379.6708681639727
[37m[1m[2023-06-25 12:21:24,988][129146] Mean Reward across all agents: 1379.6708681639727
[37m[1m[2023-06-25 12:21:24,988][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:21:30,423][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:21:30,424][129146] Reward + Measures: [[ 896.78693852    0.50529999    0.20039999    0.7288        0.28070003]
[37m[1m [ 779.41959338    0.32319999    0.19060002    0.67110002    0.19930001]
[37m[1m [1032.5222903     0.4542        0.2376        0.73219997    0.25630003]
[37m[1m ...
[37m[1m [1195.26469029    0.37200004    0.14590001    0.80920011    0.25700003]
[37m[1m [1215.37205914    0.26439998    0.2299        0.70880002    0.1955    ]
[37m[1m [1032.68449289    0.40559998    0.206         0.73019999    0.23699999]]
[37m[1m[2023-06-25 12:21:30,424][129146] Max Reward on eval: 1471.3024375272566
[37m[1m[2023-06-25 12:21:30,424][129146] Min Reward on eval: -4.395546381245367
[37m[1m[2023-06-25 12:21:30,424][129146] Mean Reward across all agents: 1001.2640120827748
[37m[1m[2023-06-25 12:21:30,424][129146] Average Trajectory Length: 996.0673333333333
[36m[2023-06-25 12:21:30,429][129146] mean_value=108.17687093719913, max_value=1624.2532474094676
[37m[1m[2023-06-25 12:21:30,432][129146] New mean coefficients: [[1.6277053  0.44948485 0.26568627 0.6408347  0.14008321]]
[37m[1m[2023-06-25 12:21:30,433][129146] Moving the mean solution point...
[36m[2023-06-25 12:21:40,216][129146] train() took 9.78 seconds to complete
[36m[2023-06-25 12:21:40,216][129146] FPS: 392604.64
[36m[2023-06-25 12:21:40,218][129146] itr=1244, itrs=2000, Progress: 62.20%
[36m[2023-06-25 12:21:51,616][129146] train() took 11.37 seconds to complete
[36m[2023-06-25 12:21:51,617][129146] FPS: 337597.06
[36m[2023-06-25 12:21:56,457][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:21:56,457][129146] Reward + Measures: [[1526.46720654    0.268529      0.16530932    0.82626063    0.22009467]]
[37m[1m[2023-06-25 12:21:56,458][129146] Max Reward on eval: 1526.467206544226
[37m[1m[2023-06-25 12:21:56,458][129146] Min Reward on eval: 1526.467206544226
[37m[1m[2023-06-25 12:21:56,458][129146] Mean Reward across all agents: 1526.467206544226
[37m[1m[2023-06-25 12:21:56,458][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:22:01,852][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:22:01,853][129146] Reward + Measures: [[ 940.56207353    0.2823        0.17739999    0.63710004    0.1692    ]
[37m[1m [1018.18694666    0.41420004    0.22930001    0.79960001    0.2545    ]
[37m[1m [ 904.25487062    0.37730002    0.1516        0.7906        0.22250001]
[37m[1m ...
[37m[1m [1146.06299976    0.3633        0.1515        0.83770007    0.2323    ]
[37m[1m [1009.77877674    0.35310003    0.13770001    0.7626        0.2023    ]
[37m[1m [1375.24160427    0.24960001    0.1691        0.69400007    0.20479999]]
[37m[1m[2023-06-25 12:22:01,853][129146] Max Reward on eval: 1623.3563228288665
[37m[1m[2023-06-25 12:22:01,854][129146] Min Reward on eval: 748.1887609726458
[37m[1m[2023-06-25 12:22:01,854][129146] Mean Reward across all agents: 1237.0929126572717
[37m[1m[2023-06-25 12:22:01,854][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:22:01,859][129146] mean_value=250.14364648683306, max_value=2051.760454747581
[37m[1m[2023-06-25 12:22:01,862][129146] New mean coefficients: [[ 2.5498602  -0.34116223  0.6017642   0.15434372 -0.4472208 ]]
[37m[1m[2023-06-25 12:22:01,863][129146] Moving the mean solution point...
[36m[2023-06-25 12:22:11,491][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 12:22:11,492][129146] FPS: 398879.90
[36m[2023-06-25 12:22:11,494][129146] itr=1245, itrs=2000, Progress: 62.25%
[36m[2023-06-25 12:22:22,890][129146] train() took 11.37 seconds to complete
[36m[2023-06-25 12:22:22,890][129146] FPS: 337734.12
[36m[2023-06-25 12:22:27,644][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:22:27,644][129146] Reward + Measures: [[1654.23116438    0.23434733    0.17675067    0.80914503    0.21017534]]
[37m[1m[2023-06-25 12:22:27,644][129146] Max Reward on eval: 1654.231164383168
[37m[1m[2023-06-25 12:22:27,645][129146] Min Reward on eval: 1654.231164383168
[37m[1m[2023-06-25 12:22:27,645][129146] Mean Reward across all agents: 1654.231164383168
[37m[1m[2023-06-25 12:22:27,645][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:22:33,006][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:22:33,007][129146] Reward + Measures: [[1518.05729947    0.2638        0.15720001    0.83680004    0.2217    ]
[37m[1m [1534.88760198    0.27620003    0.234         0.7536        0.2084    ]
[37m[1m [1483.79108009    0.26449999    0.20010002    0.8075        0.22359999]
[37m[1m ...
[37m[1m [1539.12403568    0.27140003    0.17260002    0.78369999    0.21729998]
[37m[1m [1570.95641271    0.273         0.22239998    0.78290004    0.21269999]
[37m[1m [1583.82263982    0.2647        0.20969999    0.76359999    0.2095    ]]
[37m[1m[2023-06-25 12:22:33,007][129146] Max Reward on eval: 1686.4502265628544
[37m[1m[2023-06-25 12:22:33,008][129146] Min Reward on eval: 1322.38545842217
[37m[1m[2023-06-25 12:22:33,008][129146] Mean Reward across all agents: 1576.1872797767398
[37m[1m[2023-06-25 12:22:33,008][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:22:33,013][129146] mean_value=108.19820344756285, max_value=202.31912340126883
[37m[1m[2023-06-25 12:22:33,016][129146] New mean coefficients: [[ 2.1879067  -0.52228224  0.833153   -0.64425814 -0.34306222]]
[37m[1m[2023-06-25 12:22:33,017][129146] Moving the mean solution point...
[36m[2023-06-25 12:22:42,690][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 12:22:42,690][129146] FPS: 397052.03
[36m[2023-06-25 12:22:42,692][129146] itr=1246, itrs=2000, Progress: 62.30%
[36m[2023-06-25 12:22:54,094][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 12:22:54,095][129146] FPS: 337497.53
[36m[2023-06-25 12:22:58,754][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:22:58,754][129146] Reward + Measures: [[1765.05167211    0.21817367    0.18658265    0.78365529    0.20472433]]
[37m[1m[2023-06-25 12:22:58,754][129146] Max Reward on eval: 1765.05167210679
[37m[1m[2023-06-25 12:22:58,755][129146] Min Reward on eval: 1765.05167210679
[37m[1m[2023-06-25 12:22:58,755][129146] Mean Reward across all agents: 1765.05167210679
[37m[1m[2023-06-25 12:22:58,755][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:23:04,412][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:23:04,412][129146] Reward + Measures: [[1681.05472445    0.24029998    0.18719999    0.78859997    0.21000002]
[37m[1m [1641.81488754    0.2392        0.16590001    0.77930003    0.20770001]
[37m[1m [1530.48473911    0.25189999    0.17450002    0.76389998    0.2059    ]
[37m[1m ...
[37m[1m [1739.57670528    0.23619998    0.1938        0.7827        0.20939998]
[37m[1m [1513.81727714    0.2606        0.18050002    0.76249999    0.21569999]
[37m[1m [1658.93554303    0.24089999    0.17900001    0.7863        0.20910001]]
[37m[1m[2023-06-25 12:23:04,412][129146] Max Reward on eval: 1824.9595843549819
[37m[1m[2023-06-25 12:23:04,413][129146] Min Reward on eval: 1291.7533488069778
[37m[1m[2023-06-25 12:23:04,413][129146] Mean Reward across all agents: 1643.0938618159557
[37m[1m[2023-06-25 12:23:04,413][129146] Average Trajectory Length: 999.8653333333333
[36m[2023-06-25 12:23:04,418][129146] mean_value=77.93329177878529, max_value=1912.854137857296
[37m[1m[2023-06-25 12:23:04,420][129146] New mean coefficients: [[ 1.5331887  -0.67009616  0.9138249  -0.21890432 -0.3553263 ]]
[37m[1m[2023-06-25 12:23:04,421][129146] Moving the mean solution point...
[36m[2023-06-25 12:23:14,210][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 12:23:14,210][129146] FPS: 392359.36
[36m[2023-06-25 12:23:14,212][129146] itr=1247, itrs=2000, Progress: 62.35%
[36m[2023-06-25 12:23:25,764][129146] train() took 11.53 seconds to complete
[36m[2023-06-25 12:23:25,764][129146] FPS: 333114.44
[36m[2023-06-25 12:23:30,628][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:23:30,634][129146] Reward + Measures: [[1870.8811642     0.20773932    0.18842468    0.76657242    0.19872509]]
[37m[1m[2023-06-25 12:23:30,634][129146] Max Reward on eval: 1870.8811642028559
[37m[1m[2023-06-25 12:23:30,634][129146] Min Reward on eval: 1870.8811642028559
[37m[1m[2023-06-25 12:23:30,635][129146] Mean Reward across all agents: 1870.8811642028559
[37m[1m[2023-06-25 12:23:30,635][129146] Average Trajectory Length: 999.838
[36m[2023-06-25 12:23:36,157][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:23:36,158][129146] Reward + Measures: [[1698.65955419    0.24949999    0.2309        0.70770001    0.20820001]
[37m[1m [1502.64915986    0.29350001    0.2739        0.68349999    0.2191    ]
[37m[1m [1779.21021504    0.21949999    0.18099999    0.76860005    0.1881    ]
[37m[1m ...
[37m[1m [ 958.12441259    0.3761        0.38780001    0.60739994    0.2422    ]
[37m[1m [1100.72962542    0.39020002    0.36719999    0.63760006    0.24070001]
[37m[1m [1781.30882215    0.22910002    0.21860002    0.7184        0.1998    ]]
[37m[1m[2023-06-25 12:23:36,158][129146] Max Reward on eval: 1911.7374439376406
[37m[1m[2023-06-25 12:23:36,158][129146] Min Reward on eval: 549.6096806999179
[37m[1m[2023-06-25 12:23:36,159][129146] Mean Reward across all agents: 1527.3705676632123
[37m[1m[2023-06-25 12:23:36,159][129146] Average Trajectory Length: 999.3969999999999
[36m[2023-06-25 12:23:36,164][129146] mean_value=368.0116913257209, max_value=2411.737443937641
[37m[1m[2023-06-25 12:23:36,167][129146] New mean coefficients: [[ 1.8749158  -1.0384839   0.5838607  -0.74134666 -0.4040153 ]]
[37m[1m[2023-06-25 12:23:36,168][129146] Moving the mean solution point...
[36m[2023-06-25 12:23:45,909][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 12:23:45,909][129146] FPS: 394269.55
[36m[2023-06-25 12:23:45,912][129146] itr=1248, itrs=2000, Progress: 62.40%
[36m[2023-06-25 12:23:57,495][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 12:23:57,496][129146] FPS: 332192.81
[36m[2023-06-25 12:24:02,283][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:24:02,283][129146] Reward + Measures: [[1966.52642598    0.20217334    0.19019867    0.74991232    0.19234399]]
[37m[1m[2023-06-25 12:24:02,283][129146] Max Reward on eval: 1966.5264259800383
[37m[1m[2023-06-25 12:24:02,283][129146] Min Reward on eval: 1966.5264259800383
[37m[1m[2023-06-25 12:24:02,283][129146] Mean Reward across all agents: 1966.5264259800383
[37m[1m[2023-06-25 12:24:02,284][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:24:07,699][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:24:07,699][129146] Reward + Measures: [[1254.48284279    0.33700001    0.29629999    0.67280006    0.18349999]
[37m[1m [1646.69186736    0.28709999    0.23459999    0.70320004    0.18350001]
[37m[1m [1909.39903529    0.1978        0.2175        0.73180002    0.18720001]
[37m[1m ...
[37m[1m [1465.65014588    0.3443        0.25050002    0.67309999    0.16530001]
[37m[1m [1385.60251809    0.3263        0.25100002    0.63930005    0.16690001]
[37m[1m [1394.13519744    0.25930002    0.27809998    0.70610005    0.19330001]]
[37m[1m[2023-06-25 12:24:07,699][129146] Max Reward on eval: 1989.8539940151386
[37m[1m[2023-06-25 12:24:07,700][129146] Min Reward on eval: -203.0905915713607
[37m[1m[2023-06-25 12:24:07,700][129146] Mean Reward across all agents: 1182.4234210941984
[37m[1m[2023-06-25 12:24:07,700][129146] Average Trajectory Length: 998.1313333333333
[36m[2023-06-25 12:24:07,706][129146] mean_value=146.2147154316045, max_value=1687.3457377039945
[37m[1m[2023-06-25 12:24:07,708][129146] New mean coefficients: [[ 2.2887902  -1.2144314   0.57631326 -0.5417548  -0.5156533 ]]
[37m[1m[2023-06-25 12:24:07,709][129146] Moving the mean solution point...
[36m[2023-06-25 12:24:17,445][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 12:24:17,446][129146] FPS: 394485.19
[36m[2023-06-25 12:24:17,448][129146] itr=1249, itrs=2000, Progress: 62.45%
[36m[2023-06-25 12:24:29,142][129146] train() took 11.67 seconds to complete
[36m[2023-06-25 12:24:29,142][129146] FPS: 329111.04
[36m[2023-06-25 12:24:34,015][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:24:34,015][129146] Reward + Measures: [[1091.88382295    0.38782299    0.39053065    0.63108033    0.19981132]]
[37m[1m[2023-06-25 12:24:34,015][129146] Max Reward on eval: 1091.8838229460243
[37m[1m[2023-06-25 12:24:34,015][129146] Min Reward on eval: 1091.8838229460243
[37m[1m[2023-06-25 12:24:34,016][129146] Mean Reward across all agents: 1091.8838229460243
[37m[1m[2023-06-25 12:24:34,016][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:24:39,432][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:24:39,432][129146] Reward + Measures: [[ 721.96410342    0.28740001    0.38769999    0.58850002    0.24749999]
[37m[1m [ 257.93616675    0.25631508    0.44520965    0.51355296    0.30180335]
[37m[1m [ 160.32422657    0.36139998    0.53780001    0.46310002    0.47740003]
[37m[1m ...
[37m[1m [ 349.82531699    0.3882775     0.34281275    0.55799162    0.16064863]
[37m[1m [ 369.96787945    0.40650001    0.45010003    0.53400004    0.36260003]
[37m[1m [1064.9084641     0.3337        0.3876        0.62190002    0.1973    ]]
[37m[1m[2023-06-25 12:24:39,433][129146] Max Reward on eval: 1245.3460641816316
[37m[1m[2023-06-25 12:24:39,433][129146] Min Reward on eval: -442.15095556720627
[37m[1m[2023-06-25 12:24:39,433][129146] Mean Reward across all agents: 694.3612367510775
[37m[1m[2023-06-25 12:24:39,433][129146] Average Trajectory Length: 955.554
[36m[2023-06-25 12:24:39,436][129146] mean_value=-441.7216812510698, max_value=1276.532806690281
[37m[1m[2023-06-25 12:24:39,438][129146] New mean coefficients: [[ 2.3238215  -1.0335064   0.27878278  0.10572863 -0.27128386]]
[37m[1m[2023-06-25 12:24:39,439][129146] Moving the mean solution point...
[36m[2023-06-25 12:24:49,162][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 12:24:49,162][129146] FPS: 395032.27
[36m[2023-06-25 12:24:49,164][129146] itr=1250, itrs=2000, Progress: 62.50%
[37m[1m[2023-06-25 12:24:56,882][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001230
[36m[2023-06-25 12:25:08,516][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 12:25:08,517][129146] FPS: 336276.94
[36m[2023-06-25 12:25:13,301][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:25:13,301][129146] Reward + Measures: [[1235.28938699    0.36048767    0.39425796    0.58519065    0.17302068]]
[37m[1m[2023-06-25 12:25:13,301][129146] Max Reward on eval: 1235.2893869945817
[37m[1m[2023-06-25 12:25:13,302][129146] Min Reward on eval: 1235.2893869945817
[37m[1m[2023-06-25 12:25:13,302][129146] Mean Reward across all agents: 1235.2893869945817
[37m[1m[2023-06-25 12:25:13,302][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:25:18,711][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:25:18,711][129146] Reward + Measures: [[1239.87529367    0.37420002    0.37090001    0.59560007    0.1751    ]
[37m[1m [1234.33079116    0.35789999    0.4197        0.58840001    0.17820001]
[37m[1m [1269.77874059    0.34800002    0.4021        0.59200001    0.16680001]
[37m[1m ...
[37m[1m [1051.53062625    0.34629998    0.42220002    0.55419999    0.1586    ]
[37m[1m [1051.42089782    0.36339998    0.40450001    0.58019996    0.18160002]
[37m[1m [1229.88332628    0.36029997    0.39860001    0.583         0.1698    ]]
[37m[1m[2023-06-25 12:25:18,711][129146] Max Reward on eval: 1385.6396015400765
[37m[1m[2023-06-25 12:25:18,712][129146] Min Reward on eval: 864.0207591530489
[37m[1m[2023-06-25 12:25:18,712][129146] Mean Reward across all agents: 1189.4190751792044
[37m[1m[2023-06-25 12:25:18,712][129146] Average Trajectory Length: 999.932
[36m[2023-06-25 12:25:18,714][129146] mean_value=-268.75734797983154, max_value=490.076994359453
[37m[1m[2023-06-25 12:25:18,717][129146] New mean coefficients: [[ 2.954494  -1.1220334  1.1726985 -0.6131305 -0.4905957]]
[37m[1m[2023-06-25 12:25:18,718][129146] Moving the mean solution point...
[36m[2023-06-25 12:25:28,401][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 12:25:28,401][129146] FPS: 396623.24
[36m[2023-06-25 12:25:28,403][129146] itr=1251, itrs=2000, Progress: 62.55%
[36m[2023-06-25 12:25:39,857][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 12:25:39,857][129146] FPS: 335965.45
[36m[2023-06-25 12:25:44,631][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:25:44,631][129146] Reward + Measures: [[1403.07758284    0.33286232    0.40178901    0.54550767    0.14474399]]
[37m[1m[2023-06-25 12:25:44,631][129146] Max Reward on eval: 1403.0775828393826
[37m[1m[2023-06-25 12:25:44,632][129146] Min Reward on eval: 1403.0775828393826
[37m[1m[2023-06-25 12:25:44,632][129146] Mean Reward across all agents: 1403.0775828393826
[37m[1m[2023-06-25 12:25:44,632][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:25:50,041][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:25:50,042][129146] Reward + Measures: [[ 779.32894742    0.4131        0.3811        0.54830003    0.12260001]
[37m[1m [1444.87304685    0.2669        0.33199999    0.44029999    0.1402    ]
[37m[1m [ 785.26918205    0.29537067    0.33105966    0.48719165    0.13997792]
[37m[1m ...
[37m[1m [1040.95543857    0.3427        0.37380001    0.53299999    0.12159999]
[37m[1m [1031.89065718    0.32049999    0.3743        0.55059999    0.14890002]
[37m[1m [ 636.66400565    0.3187395     0.28775364    0.46437368    0.16006091]]
[37m[1m[2023-06-25 12:25:50,042][129146] Max Reward on eval: 1637.1541527485474
[37m[1m[2023-06-25 12:25:50,043][129146] Min Reward on eval: -652.3403816029706
[37m[1m[2023-06-25 12:25:50,043][129146] Mean Reward across all agents: 614.9347500111842
[37m[1m[2023-06-25 12:25:50,043][129146] Average Trajectory Length: 984.3786666666666
[36m[2023-06-25 12:25:50,047][129146] mean_value=-263.72231845672667, max_value=1880.7506631156061
[37m[1m[2023-06-25 12:25:50,049][129146] New mean coefficients: [[ 2.8770099  -1.2474883   0.22837359 -0.7945096  -0.60553044]]
[37m[1m[2023-06-25 12:25:50,050][129146] Moving the mean solution point...
[36m[2023-06-25 12:25:59,825][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 12:25:59,826][129146] FPS: 392904.50
[36m[2023-06-25 12:25:59,828][129146] itr=1252, itrs=2000, Progress: 62.60%
[36m[2023-06-25 12:26:11,341][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 12:26:11,341][129146] FPS: 334350.58
[36m[2023-06-25 12:26:15,993][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:26:15,994][129146] Reward + Measures: [[1528.91176613    0.31000668    0.40704265    0.51038295    0.12693034]]
[37m[1m[2023-06-25 12:26:15,994][129146] Max Reward on eval: 1528.911766125399
[37m[1m[2023-06-25 12:26:15,994][129146] Min Reward on eval: 1528.911766125399
[37m[1m[2023-06-25 12:26:15,994][129146] Mean Reward across all agents: 1528.911766125399
[37m[1m[2023-06-25 12:26:15,995][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:26:21,474][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:26:21,475][129146] Reward + Measures: [[ 728.01432653    0.27520001    0.38860002    0.43589997    0.19820002]
[37m[1m [ 845.9046644     0.32069999    0.37079999    0.47490001    0.16770001]
[37m[1m [1047.52434793    0.40549999    0.3689        0.56039995    0.1487    ]
[37m[1m ...
[37m[1m [ 860.23042112    0.27470002    0.35620001    0.41610003    0.17120001]
[37m[1m [ 854.34464366    0.41770002    0.3554        0.56980002    0.15620001]
[37m[1m [1083.87661207    0.26184118    0.3878563     0.41101599    0.1582353 ]]
[37m[1m[2023-06-25 12:26:21,475][129146] Max Reward on eval: 1665.3508156777127
[37m[1m[2023-06-25 12:26:21,476][129146] Min Reward on eval: -41.63044464528794
[37m[1m[2023-06-25 12:26:21,476][129146] Mean Reward across all agents: 1058.5037322864532
[37m[1m[2023-06-25 12:26:21,476][129146] Average Trajectory Length: 996.5343333333333
[36m[2023-06-25 12:26:21,479][129146] mean_value=-106.8199959495401, max_value=2104.079760682981
[37m[1m[2023-06-25 12:26:21,482][129146] New mean coefficients: [[ 2.8882685  -1.4630163   0.07565331 -1.3504903  -0.3501243 ]]
[37m[1m[2023-06-25 12:26:21,483][129146] Moving the mean solution point...
[36m[2023-06-25 12:26:31,209][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 12:26:31,209][129146] FPS: 394894.55
[36m[2023-06-25 12:26:31,211][129146] itr=1253, itrs=2000, Progress: 62.65%
[36m[2023-06-25 12:26:42,602][129146] train() took 11.37 seconds to complete
[36m[2023-06-25 12:26:42,603][129146] FPS: 337823.57
[36m[2023-06-25 12:26:47,406][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:26:47,407][129146] Reward + Measures: [[1674.37785983    0.28992847    0.411199      0.46823934    0.11172424]]
[37m[1m[2023-06-25 12:26:47,407][129146] Max Reward on eval: 1674.3778598345996
[37m[1m[2023-06-25 12:26:47,407][129146] Min Reward on eval: 1674.3778598345996
[37m[1m[2023-06-25 12:26:47,407][129146] Mean Reward across all agents: 1674.3778598345996
[37m[1m[2023-06-25 12:26:47,408][129146] Average Trajectory Length: 999.6236666666666
[36m[2023-06-25 12:26:52,773][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:26:52,774][129146] Reward + Measures: [[1228.60148199    0.28389999    0.2933        0.46890002    0.15530001]
[37m[1m [1512.32513727    0.31740001    0.29729998    0.5345        0.13340001]
[37m[1m [1185.92142467    0.24029998    0.37030002    0.39589998    0.14830001]
[37m[1m ...
[37m[1m [ 438.91877604    0.30937159    0.28569433    0.38232866    0.21880701]
[37m[1m [ 321.13448373    0.22492941    0.21062942    0.29444703    0.12204118]
[37m[1m [1545.9547527     0.28650004    0.28670001    0.47810003    0.1191    ]]
[37m[1m[2023-06-25 12:26:52,774][129146] Max Reward on eval: 1797.4925610620528
[37m[1m[2023-06-25 12:26:52,774][129146] Min Reward on eval: -416.6995027987039
[37m[1m[2023-06-25 12:26:52,775][129146] Mean Reward across all agents: 1004.512338782354
[37m[1m[2023-06-25 12:26:52,775][129146] Average Trajectory Length: 980.1233333333333
[36m[2023-06-25 12:26:52,779][129146] mean_value=-77.59431539179704, max_value=1762.0435146614404
[37m[1m[2023-06-25 12:26:52,781][129146] New mean coefficients: [[ 2.8561888  -0.98305666 -0.15547591 -1.1384892  -0.424772  ]]
[37m[1m[2023-06-25 12:26:52,782][129146] Moving the mean solution point...
[36m[2023-06-25 12:27:02,454][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 12:27:02,454][129146] FPS: 397111.08
[36m[2023-06-25 12:27:02,456][129146] itr=1254, itrs=2000, Progress: 62.70%
[36m[2023-06-25 12:27:13,854][129146] train() took 11.37 seconds to complete
[36m[2023-06-25 12:27:13,854][129146] FPS: 337625.71
[36m[2023-06-25 12:27:18,490][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:27:18,490][129146] Reward + Measures: [[1802.51463611    0.2695578     0.41151845    0.43247193    0.0966808 ]]
[37m[1m[2023-06-25 12:27:18,491][129146] Max Reward on eval: 1802.5146361139098
[37m[1m[2023-06-25 12:27:18,491][129146] Min Reward on eval: 1802.5146361139098
[37m[1m[2023-06-25 12:27:18,491][129146] Mean Reward across all agents: 1802.5146361139098
[37m[1m[2023-06-25 12:27:18,491][129146] Average Trajectory Length: 998.5946666666666
[36m[2023-06-25 12:27:23,777][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:27:23,778][129146] Reward + Measures: [[1866.76403456    0.26370001    0.40850002    0.4068        0.11140001]
[37m[1m [1144.56253271    0.19857149    0.26950613    0.34475395    0.14283028]
[37m[1m [1402.84015953    0.23695925    0.33286533    0.36368147    0.12977512]
[37m[1m ...
[37m[1m [1802.77598202    0.29190001    0.42160001    0.44860002    0.13940001]
[37m[1m [1682.22965611    0.25119999    0.37790003    0.41770002    0.1074    ]
[37m[1m [1866.53657838    0.2638        0.36500001    0.42030001    0.0891    ]]
[37m[1m[2023-06-25 12:27:23,778][129146] Max Reward on eval: 2011.611025196733
[37m[1m[2023-06-25 12:27:23,778][129146] Min Reward on eval: 280.69771372647085
[37m[1m[2023-06-25 12:27:23,779][129146] Mean Reward across all agents: 1407.423919377966
[37m[1m[2023-06-25 12:27:23,779][129146] Average Trajectory Length: 975.6233333333333
[36m[2023-06-25 12:27:23,784][129146] mean_value=218.5108181747125, max_value=2054.220542127313
[37m[1m[2023-06-25 12:27:23,787][129146] New mean coefficients: [[ 2.389247   -0.9786095  -0.2040277  -1.2631524  -0.12913379]]
[37m[1m[2023-06-25 12:27:23,788][129146] Moving the mean solution point...
[36m[2023-06-25 12:27:33,436][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 12:27:33,436][129146] FPS: 398088.65
[36m[2023-06-25 12:27:33,438][129146] itr=1255, itrs=2000, Progress: 62.75%
[36m[2023-06-25 12:27:44,818][129146] train() took 11.36 seconds to complete
[36m[2023-06-25 12:27:44,819][129146] FPS: 338174.39
[36m[2023-06-25 12:27:49,523][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:27:49,524][129146] Reward + Measures: [[1933.34800971    0.26211473    0.42461881    0.41167465    0.08461218]]
[37m[1m[2023-06-25 12:27:49,524][129146] Max Reward on eval: 1933.3480097138531
[37m[1m[2023-06-25 12:27:49,524][129146] Min Reward on eval: 1933.3480097138531
[37m[1m[2023-06-25 12:27:49,524][129146] Mean Reward across all agents: 1933.3480097138531
[37m[1m[2023-06-25 12:27:49,525][129146] Average Trajectory Length: 999.2536666666666
[36m[2023-06-25 12:27:55,106][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:27:55,107][129146] Reward + Measures: [[1657.50186934    0.24390002    0.38120002    0.4269        0.11240001]
[37m[1m [1858.47612795    0.24299999    0.40760002    0.37970001    0.08889999]
[37m[1m [1293.49866515    0.22590001    0.3276        0.31549999    0.08280001]
[37m[1m ...
[37m[1m [2020.46756368    0.24990001    0.4341        0.37800002    0.0784    ]
[37m[1m [2011.08759984    0.25560001    0.42600003    0.38570002    0.0856    ]
[37m[1m [2098.55110946    0.2642        0.45110002    0.4007        0.0595    ]]
[37m[1m[2023-06-25 12:27:55,107][129146] Max Reward on eval: 2105.823753964086
[37m[1m[2023-06-25 12:27:55,107][129146] Min Reward on eval: 1293.4986651456536
[37m[1m[2023-06-25 12:27:55,108][129146] Mean Reward across all agents: 1863.1602659382625
[37m[1m[2023-06-25 12:27:55,108][129146] Average Trajectory Length: 994.5546666666667
[36m[2023-06-25 12:27:55,114][129146] mean_value=945.5445581662913, max_value=2400.4016997513545
[37m[1m[2023-06-25 12:27:55,117][129146] New mean coefficients: [[ 2.4507313  -0.547814   -0.19979498 -2.3860922  -0.09116133]]
[37m[1m[2023-06-25 12:27:55,118][129146] Moving the mean solution point...
[36m[2023-06-25 12:28:04,894][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 12:28:04,895][129146] FPS: 392843.36
[36m[2023-06-25 12:28:04,897][129146] itr=1256, itrs=2000, Progress: 62.80%
[36m[2023-06-25 12:28:16,552][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 12:28:16,552][129146] FPS: 330259.38
[36m[2023-06-25 12:28:21,285][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:28:21,286][129146] Reward + Measures: [[2069.34097453    0.25496289    0.43743351    0.38745794    0.07349884]]
[37m[1m[2023-06-25 12:28:21,286][129146] Max Reward on eval: 2069.340974528648
[37m[1m[2023-06-25 12:28:21,286][129146] Min Reward on eval: 2069.340974528648
[37m[1m[2023-06-25 12:28:21,286][129146] Mean Reward across all agents: 2069.340974528648
[37m[1m[2023-06-25 12:28:21,286][129146] Average Trajectory Length: 997.3743333333333
[36m[2023-06-25 12:28:26,648][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:28:26,649][129146] Reward + Measures: [[ 919.78385528    0.37211308    0.25568503    0.53606826    0.24284934]
[37m[1m [1213.95865646    0.34370002    0.4822        0.33969998    0.12540001]
[37m[1m [1417.67686912    0.27500001    0.3281        0.50489998    0.16159999]
[37m[1m ...
[37m[1m [1875.38919351    0.28380001    0.47150001    0.38530001    0.08820001]
[37m[1m [1817.67672859    0.25149998    0.43660003    0.39379999    0.09370001]
[37m[1m [1364.95505577    0.31759998    0.48140001    0.39250001    0.1602    ]]
[37m[1m[2023-06-25 12:28:26,649][129146] Max Reward on eval: 2169.349382539606
[37m[1m[2023-06-25 12:28:26,649][129146] Min Reward on eval: 9.905278578598518
[37m[1m[2023-06-25 12:28:26,649][129146] Mean Reward across all agents: 1345.1314405018059
[37m[1m[2023-06-25 12:28:26,650][129146] Average Trajectory Length: 986.6373333333333
[36m[2023-06-25 12:28:26,654][129146] mean_value=46.11058324636043, max_value=2416.9212472509125
[37m[1m[2023-06-25 12:28:26,656][129146] New mean coefficients: [[ 2.2026167  -0.02516764  0.19122902 -2.1441457   0.08390987]]
[37m[1m[2023-06-25 12:28:26,657][129146] Moving the mean solution point...
[36m[2023-06-25 12:28:36,203][129146] train() took 9.54 seconds to complete
[36m[2023-06-25 12:28:36,204][129146] FPS: 402325.74
[36m[2023-06-25 12:28:36,206][129146] itr=1257, itrs=2000, Progress: 62.85%
[36m[2023-06-25 12:28:47,620][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 12:28:47,620][129146] FPS: 337160.91
[36m[2023-06-25 12:28:52,314][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:28:52,314][129146] Reward + Measures: [[2169.09582962    0.24981116    0.44622481    0.36748219    0.06263027]]
[37m[1m[2023-06-25 12:28:52,315][129146] Max Reward on eval: 2169.0958296221806
[37m[1m[2023-06-25 12:28:52,315][129146] Min Reward on eval: 2169.0958296221806
[37m[1m[2023-06-25 12:28:52,315][129146] Mean Reward across all agents: 2169.0958296221806
[37m[1m[2023-06-25 12:28:52,315][129146] Average Trajectory Length: 996.419
[36m[2023-06-25 12:28:57,778][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:28:57,778][129146] Reward + Measures: [[1430.96625283    0.26589999    0.33430001    0.39199999    0.1338    ]
[37m[1m [1038.50790538    0.19146483    0.28703475    0.27425429    0.09643239]
[37m[1m [1673.79011802    0.3073        0.4296        0.47129998    0.09520001]
[37m[1m ...
[37m[1m [1429.10698373    0.23580001    0.29910001    0.37040001    0.1409    ]
[37m[1m [ 850.23556397    0.23270002    0.42039999    0.45140001    0.17760001]
[37m[1m [2043.20474741    0.24159999    0.41260001    0.38030002    0.0835    ]]
[37m[1m[2023-06-25 12:28:57,779][129146] Max Reward on eval: 2233.6954609960317
[37m[1m[2023-06-25 12:28:57,779][129146] Min Reward on eval: 161.6229129433894
[37m[1m[2023-06-25 12:28:57,779][129146] Mean Reward across all agents: 1391.3607453524214
[37m[1m[2023-06-25 12:28:57,779][129146] Average Trajectory Length: 975.9793333333333
[36m[2023-06-25 12:28:57,783][129146] mean_value=-155.7959097790127, max_value=1684.0610356358522
[37m[1m[2023-06-25 12:28:57,785][129146] New mean coefficients: [[ 1.7754879   0.37445286 -0.06304553 -1.9967251   0.2418469 ]]
[37m[1m[2023-06-25 12:28:57,786][129146] Moving the mean solution point...
[36m[2023-06-25 12:29:07,586][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 12:29:07,587][129146] FPS: 391909.20
[36m[2023-06-25 12:29:07,589][129146] itr=1258, itrs=2000, Progress: 62.90%
[36m[2023-06-25 12:29:19,330][129146] train() took 11.72 seconds to complete
[36m[2023-06-25 12:29:19,330][129146] FPS: 327725.74
[36m[2023-06-25 12:29:24,178][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:29:24,178][129146] Reward + Measures: [[2278.48452029    0.24407102    0.45915192    0.34452996    0.0537978 ]]
[37m[1m[2023-06-25 12:29:24,179][129146] Max Reward on eval: 2278.4845202918655
[37m[1m[2023-06-25 12:29:24,179][129146] Min Reward on eval: 2278.4845202918655
[37m[1m[2023-06-25 12:29:24,179][129146] Mean Reward across all agents: 2278.4845202918655
[37m[1m[2023-06-25 12:29:24,179][129146] Average Trajectory Length: 996.1346666666666
[36m[2023-06-25 12:29:29,534][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:29:29,534][129146] Reward + Measures: [[2249.29917425    0.26180002    0.48540002    0.31350002    0.0379    ]
[37m[1m [2128.06300151    0.2421        0.41360003    0.36970001    0.0768    ]
[37m[1m [ 710.28677244    0.26909646    0.22025786    0.51140827    0.20371631]
[37m[1m ...
[37m[1m [2289.67532555    0.24609999    0.46250001    0.32699999    0.0497    ]
[37m[1m [1483.80208145    0.30340001    0.46860003    0.333         0.11550001]
[37m[1m [1290.48138167    0.27302673    0.25159082    0.50790763    0.17833817]]
[37m[1m[2023-06-25 12:29:29,534][129146] Max Reward on eval: 2392.861090188008
[37m[1m[2023-06-25 12:29:29,535][129146] Min Reward on eval: 710.2867724446551
[37m[1m[2023-06-25 12:29:29,535][129146] Mean Reward across all agents: 1960.7894870806042
[37m[1m[2023-06-25 12:29:29,535][129146] Average Trajectory Length: 986.9763333333333
[36m[2023-06-25 12:29:29,540][129146] mean_value=332.74587854498225, max_value=2731.2508447039872
[37m[1m[2023-06-25 12:29:29,543][129146] New mean coefficients: [[ 2.7473574  -0.0179944  -0.31453225 -1.5843757  -0.16897283]]
[37m[1m[2023-06-25 12:29:29,544][129146] Moving the mean solution point...
[36m[2023-06-25 12:29:39,272][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 12:29:39,272][129146] FPS: 394832.67
[36m[2023-06-25 12:29:39,274][129146] itr=1259, itrs=2000, Progress: 62.95%
[36m[2023-06-25 12:29:50,857][129146] train() took 11.56 seconds to complete
[36m[2023-06-25 12:29:50,857][129146] FPS: 332218.68
[36m[2023-06-25 12:29:55,637][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:29:55,638][129146] Reward + Measures: [[2429.8957092     0.24203964    0.46673092    0.3310833     0.04772595]]
[37m[1m[2023-06-25 12:29:55,638][129146] Max Reward on eval: 2429.895709196231
[37m[1m[2023-06-25 12:29:55,638][129146] Min Reward on eval: 2429.895709196231
[37m[1m[2023-06-25 12:29:55,639][129146] Mean Reward across all agents: 2429.895709196231
[37m[1m[2023-06-25 12:29:55,639][129146] Average Trajectory Length: 996.4606666666666
[36m[2023-06-25 12:30:01,107][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:30:01,108][129146] Reward + Measures: [[1776.83460663    0.26145801    0.39307258    0.38215125    0.12392405]
[37m[1m [1279.48807375    0.23698059    0.33003497    0.3493804     0.16495182]
[37m[1m [1193.45278706    0.2588        0.33579999    0.45179996    0.22069998]
[37m[1m ...
[37m[1m [1015.9578472     0.24239998    0.35749999    0.43480006    0.2122    ]
[37m[1m [2173.17303281    0.27360001    0.39900002    0.3554        0.08080001]
[37m[1m [1742.46581588    0.30689999    0.41910002    0.41160002    0.0665    ]]
[37m[1m[2023-06-25 12:30:01,108][129146] Max Reward on eval: 2419.4947201260134
[37m[1m[2023-06-25 12:30:01,108][129146] Min Reward on eval: 439.04193458233493
[37m[1m[2023-06-25 12:30:01,108][129146] Mean Reward across all agents: 1681.6484201917608
[37m[1m[2023-06-25 12:30:01,109][129146] Average Trajectory Length: 993.615
[36m[2023-06-25 12:30:01,113][129146] mean_value=11.779983465598464, max_value=2056.337129181826
[37m[1m[2023-06-25 12:30:01,115][129146] New mean coefficients: [[ 2.8434844  -0.05863079 -0.53354067 -0.9263261  -0.01513171]]
[37m[1m[2023-06-25 12:30:01,116][129146] Moving the mean solution point...
[36m[2023-06-25 12:30:10,862][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 12:30:10,863][129146] FPS: 394076.17
[36m[2023-06-25 12:30:10,865][129146] itr=1260, itrs=2000, Progress: 63.00%
[37m[1m[2023-06-25 12:30:18,929][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001240
[36m[2023-06-25 12:30:30,676][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 12:30:30,677][129146] FPS: 332667.87
[36m[2023-06-25 12:30:35,511][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:30:35,512][129146] Reward + Measures: [[2550.59446018    0.23652714    0.46501172    0.31347531    0.03897264]]
[37m[1m[2023-06-25 12:30:35,512][129146] Max Reward on eval: 2550.5944601757124
[37m[1m[2023-06-25 12:30:35,512][129146] Min Reward on eval: 2550.5944601757124
[37m[1m[2023-06-25 12:30:35,512][129146] Mean Reward across all agents: 2550.5944601757124
[37m[1m[2023-06-25 12:30:35,513][129146] Average Trajectory Length: 995.91
[36m[2023-06-25 12:30:41,048][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:30:41,048][129146] Reward + Measures: [[1500.6319293     0.33380002    0.41330001    0.43759999    0.0903    ]
[37m[1m [ 708.21836724    0.19970001    0.29679999    0.28459999    0.11310001]
[37m[1m [2257.47267509    0.2304        0.43179998    0.2965        0.0405    ]
[37m[1m ...
[37m[1m [1881.15617094    0.24627033    0.42535827    0.34476265    0.07778682]
[37m[1m [2094.09890735    0.20279999    0.42860004    0.29299998    0.07970001]
[37m[1m [2000.51154058    0.24820001    0.3973        0.3159        0.05950001]]
[37m[1m[2023-06-25 12:30:41,049][129146] Max Reward on eval: 2575.0699399928562
[37m[1m[2023-06-25 12:30:41,049][129146] Min Reward on eval: 178.40934794850764
[37m[1m[2023-06-25 12:30:41,049][129146] Mean Reward across all agents: 1686.7166353710236
[37m[1m[2023-06-25 12:30:41,049][129146] Average Trajectory Length: 978.903
[36m[2023-06-25 12:30:41,054][129146] mean_value=70.48229624987488, max_value=2718.068841453409
[37m[1m[2023-06-25 12:30:41,056][129146] New mean coefficients: [[ 2.4286602  -0.21992624 -0.57465076 -1.1050248   0.03703058]]
[37m[1m[2023-06-25 12:30:41,057][129146] Moving the mean solution point...
[36m[2023-06-25 12:30:50,977][129146] train() took 9.92 seconds to complete
[36m[2023-06-25 12:30:50,977][129146] FPS: 387190.96
[36m[2023-06-25 12:30:50,979][129146] itr=1261, itrs=2000, Progress: 63.05%
[36m[2023-06-25 12:31:02,460][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 12:31:02,460][129146] FPS: 335293.70
[36m[2023-06-25 12:31:07,210][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:31:07,210][129146] Reward + Measures: [[2697.85442678    0.23511897    0.4656595     0.30143917    0.03222479]]
[37m[1m[2023-06-25 12:31:07,210][129146] Max Reward on eval: 2697.8544267779935
[37m[1m[2023-06-25 12:31:07,211][129146] Min Reward on eval: 2697.8544267779935
[37m[1m[2023-06-25 12:31:07,211][129146] Mean Reward across all agents: 2697.8544267779935
[37m[1m[2023-06-25 12:31:07,211][129146] Average Trajectory Length: 996.9553333333333
[36m[2023-06-25 12:31:12,602][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:31:12,602][129146] Reward + Measures: [[ 376.21927344    0.50929993    0.33239999    0.54360002    0.14260001]
[37m[1m [1638.26096706    0.22788154    0.35358587    0.30775419    0.09614334]
[37m[1m [2049.43801458    0.24349999    0.44949999    0.31560001    0.06640001]
[37m[1m ...
[37m[1m [2772.14664375    0.25          0.4614        0.3251        0.0376    ]
[37m[1m [1057.02884636    0.36900002    0.4587        0.44560003    0.0787    ]
[37m[1m [ 781.76858949    0.38282403    0.41311899    0.42517883    0.11656278]]
[37m[1m[2023-06-25 12:31:12,603][129146] Max Reward on eval: 2772.146643751906
[37m[1m[2023-06-25 12:31:12,603][129146] Min Reward on eval: -26.76458689626306
[37m[1m[2023-06-25 12:31:12,603][129146] Mean Reward across all agents: 1438.432797252754
[37m[1m[2023-06-25 12:31:12,603][129146] Average Trajectory Length: 940.3563333333333
[36m[2023-06-25 12:31:12,607][129146] mean_value=-230.27037354002383, max_value=2689.0414461491628
[37m[1m[2023-06-25 12:31:12,610][129146] New mean coefficients: [[ 1.7764723   0.33134675 -0.44722036 -1.1752      0.24332494]]
[37m[1m[2023-06-25 12:31:12,611][129146] Moving the mean solution point...
[36m[2023-06-25 12:31:22,265][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 12:31:22,266][129146] FPS: 397822.41
[36m[2023-06-25 12:31:22,268][129146] itr=1262, itrs=2000, Progress: 63.10%
[36m[2023-06-25 12:31:33,665][129146] train() took 11.37 seconds to complete
[36m[2023-06-25 12:31:33,665][129146] FPS: 337636.66
[36m[2023-06-25 12:31:38,453][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:31:38,453][129146] Reward + Measures: [[2821.58867063    0.23222189    0.46958193    0.29157215    0.02686483]]
[37m[1m[2023-06-25 12:31:38,453][129146] Max Reward on eval: 2821.588670634311
[37m[1m[2023-06-25 12:31:38,453][129146] Min Reward on eval: 2821.588670634311
[37m[1m[2023-06-25 12:31:38,454][129146] Mean Reward across all agents: 2821.588670634311
[37m[1m[2023-06-25 12:31:38,454][129146] Average Trajectory Length: 994.8186666666667
[36m[2023-06-25 12:31:44,043][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:31:44,044][129146] Reward + Measures: [[1743.7261925     0.2848281     0.43554717    0.32172376    0.04681673]
[37m[1m [2238.06815546    0.25439999    0.38850001    0.31350002    0.0542    ]
[37m[1m [2537.68290429    0.2608        0.48390004    0.31550002    0.0222    ]
[37m[1m ...
[37m[1m [ 476.68695284    0.3152512     0.38913593    0.39948219    0.17462616]
[37m[1m [1554.62460407    0.32339999    0.42859998    0.36180001    0.0489    ]
[37m[1m [1740.36040569    0.29188237    0.46823531    0.38197058    0.06797647]]
[37m[1m[2023-06-25 12:31:44,044][129146] Max Reward on eval: 2949.434967356711
[37m[1m[2023-06-25 12:31:44,044][129146] Min Reward on eval: -211.39283691694436
[37m[1m[2023-06-25 12:31:44,045][129146] Mean Reward across all agents: 1720.0156788648706
[37m[1m[2023-06-25 12:31:44,045][129146] Average Trajectory Length: 962.0026666666666
[36m[2023-06-25 12:31:44,050][129146] mean_value=219.17599129708765, max_value=3011.5346878318114
[37m[1m[2023-06-25 12:31:44,052][129146] New mean coefficients: [[ 1.8974578   0.07827279 -0.9266745  -0.96800053  0.08266377]]
[37m[1m[2023-06-25 12:31:44,053][129146] Moving the mean solution point...
[36m[2023-06-25 12:31:53,924][129146] train() took 9.87 seconds to complete
[36m[2023-06-25 12:31:53,924][129146] FPS: 389108.50
[36m[2023-06-25 12:31:53,926][129146] itr=1263, itrs=2000, Progress: 63.15%
[36m[2023-06-25 12:32:05,375][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 12:32:05,375][129146] FPS: 336102.66
[36m[2023-06-25 12:32:10,192][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:32:10,193][129146] Reward + Measures: [[2971.75788236    0.22865024    0.47210059    0.28587896    0.02485366]]
[37m[1m[2023-06-25 12:32:10,193][129146] Max Reward on eval: 2971.7578823627546
[37m[1m[2023-06-25 12:32:10,193][129146] Min Reward on eval: 2971.7578823627546
[37m[1m[2023-06-25 12:32:10,193][129146] Mean Reward across all agents: 2971.7578823627546
[37m[1m[2023-06-25 12:32:10,194][129146] Average Trajectory Length: 993.4883333333333
[36m[2023-06-25 12:32:15,643][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:32:15,644][129146] Reward + Measures: [[2955.44980786    0.23220001    0.4341        0.29839998    0.0278    ]
[37m[1m [2878.51681912    0.24252398    0.44599533    0.30437547    0.03614094]
[37m[1m [2784.28356739    0.2376        0.42820001    0.27719998    0.0261    ]
[37m[1m ...
[37m[1m [2485.77587455    0.23757695    0.38008919    0.32615992    0.05840066]
[37m[1m [2890.57966694    0.23150001    0.43799996    0.29660001    0.0347    ]
[37m[1m [2907.26969936    0.22677521    0.46271983    0.29526612    0.02519752]]
[37m[1m[2023-06-25 12:32:15,644][129146] Max Reward on eval: 3116.51745958214
[37m[1m[2023-06-25 12:32:15,644][129146] Min Reward on eval: 1953.798686578896
[37m[1m[2023-06-25 12:32:15,645][129146] Mean Reward across all agents: 2787.6131619597345
[37m[1m[2023-06-25 12:32:15,645][129146] Average Trajectory Length: 987.8296666666666
[36m[2023-06-25 12:32:15,651][129146] mean_value=352.6047846049402, max_value=2271.5571275350426
[37m[1m[2023-06-25 12:32:15,654][129146] New mean coefficients: [[ 1.0125136   0.53736526 -0.65593827 -0.8635499   0.20722136]]
[37m[1m[2023-06-25 12:32:15,654][129146] Moving the mean solution point...
[36m[2023-06-25 12:32:25,407][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 12:32:25,408][129146] FPS: 393798.44
[36m[2023-06-25 12:32:25,410][129146] itr=1264, itrs=2000, Progress: 63.20%
[36m[2023-06-25 12:32:36,959][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 12:32:36,959][129146] FPS: 333216.21
[36m[2023-06-25 12:32:41,691][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:32:41,691][129146] Reward + Measures: [[3105.13598145    0.2314443     0.47260171    0.28437072    0.01991846]]
[37m[1m[2023-06-25 12:32:41,691][129146] Max Reward on eval: 3105.1359814547463
[37m[1m[2023-06-25 12:32:41,692][129146] Min Reward on eval: 3105.1359814547463
[37m[1m[2023-06-25 12:32:41,692][129146] Mean Reward across all agents: 3105.1359814547463
[37m[1m[2023-06-25 12:32:41,692][129146] Average Trajectory Length: 994.769
[36m[2023-06-25 12:32:47,304][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:32:47,305][129146] Reward + Measures: [[2154.93356122    0.3152        0.45370004    0.42070004    0.08009999]
[37m[1m [1132.48258623    0.24339473    0.30318537    0.36750004    0.12830819]
[37m[1m [1826.05604009    0.23729999    0.32389998    0.3339        0.10550001]
[37m[1m ...
[37m[1m [2299.62670582    0.21109998    0.3766        0.32100001    0.075     ]
[37m[1m [2493.04748967    0.21250001    0.40009999    0.30840001    0.0666    ]
[37m[1m [1273.22069691    0.20333806    0.27344963    0.26600662    0.09203675]]
[37m[1m[2023-06-25 12:32:47,305][129146] Max Reward on eval: 3194.682530591509
[37m[1m[2023-06-25 12:32:47,305][129146] Min Reward on eval: 31.315444953221593
[37m[1m[2023-06-25 12:32:47,305][129146] Mean Reward across all agents: 1983.12067841679
[37m[1m[2023-06-25 12:32:47,306][129146] Average Trajectory Length: 963.8833333333333
[36m[2023-06-25 12:32:47,309][129146] mean_value=-168.60347513705858, max_value=782.4689980019774
[37m[1m[2023-06-25 12:32:47,311][129146] New mean coefficients: [[ 0.6383631   0.55102986 -0.23894748 -1.2167926   0.08941595]]
[37m[1m[2023-06-25 12:32:47,312][129146] Moving the mean solution point...
[36m[2023-06-25 12:32:56,968][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 12:32:56,968][129146] FPS: 397776.81
[36m[2023-06-25 12:32:56,970][129146] itr=1265, itrs=2000, Progress: 63.25%
[36m[2023-06-25 12:33:08,379][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 12:33:08,379][129146] FPS: 337268.73
[36m[2023-06-25 12:33:13,206][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:33:13,206][129146] Reward + Measures: [[3239.07099493    0.22860546    0.47850153    0.27864239    0.0171779 ]]
[37m[1m[2023-06-25 12:33:13,206][129146] Max Reward on eval: 3239.070994926387
[37m[1m[2023-06-25 12:33:13,207][129146] Min Reward on eval: 3239.070994926387
[37m[1m[2023-06-25 12:33:13,207][129146] Mean Reward across all agents: 3239.070994926387
[37m[1m[2023-06-25 12:33:13,207][129146] Average Trajectory Length: 996.659
[36m[2023-06-25 12:33:18,577][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:33:18,578][129146] Reward + Measures: [[1165.76666045    0.29159999    0.39470002    0.39220002    0.0605    ]
[37m[1m [1143.17306866    0.29010001    0.391         0.45889997    0.1584    ]
[37m[1m [ 171.45127911    0.3479        0.4605        0.41629997    0.35820001]
[37m[1m ...
[37m[1m [1713.60233576    0.26809999    0.42209998    0.2969        0.0439    ]
[37m[1m [1575.61986334    0.29280004    0.36559999    0.47779998    0.08130001]
[37m[1m [1158.14826525    0.30450001    0.40740004    0.42430001    0.0947    ]]
[37m[1m[2023-06-25 12:33:18,578][129146] Max Reward on eval: 3262.9377517679704
[37m[1m[2023-06-25 12:33:18,578][129146] Min Reward on eval: -223.71756890743563
[37m[1m[2023-06-25 12:33:18,578][129146] Mean Reward across all agents: 1566.0319839468384
[37m[1m[2023-06-25 12:33:18,578][129146] Average Trajectory Length: 993.329
[36m[2023-06-25 12:33:18,581][129146] mean_value=-362.9322925071951, max_value=3410.5174590777096
[37m[1m[2023-06-25 12:33:18,584][129146] New mean coefficients: [[ 1.04544     0.29826766 -0.39898252 -1.3759538  -0.0056436 ]]
[37m[1m[2023-06-25 12:33:18,585][129146] Moving the mean solution point...
[36m[2023-06-25 12:33:28,356][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 12:33:28,356][129146] FPS: 393083.42
[36m[2023-06-25 12:33:28,358][129146] itr=1266, itrs=2000, Progress: 63.30%
[36m[2023-06-25 12:33:39,782][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 12:33:39,783][129146] FPS: 336828.79
[36m[2023-06-25 12:33:44,579][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:33:44,580][129146] Reward + Measures: [[3413.4162061     0.22738709    0.47327003    0.2703335     0.01387748]]
[37m[1m[2023-06-25 12:33:44,580][129146] Max Reward on eval: 3413.4162061041266
[37m[1m[2023-06-25 12:33:44,580][129146] Min Reward on eval: 3413.4162061041266
[37m[1m[2023-06-25 12:33:44,580][129146] Mean Reward across all agents: 3413.4162061041266
[37m[1m[2023-06-25 12:33:44,580][129146] Average Trajectory Length: 998.5103333333333
[36m[2023-06-25 12:33:49,980][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:33:49,981][129146] Reward + Measures: [[1337.49064041    0.33010003    0.32089999    0.44850001    0.1013    ]
[37m[1m [ 442.56590076    0.40839693    0.305751      0.45443749    0.17568545]
[37m[1m [2913.30488532    0.23539999    0.42869997    0.3211        0.0439    ]
[37m[1m ...
[37m[1m [ 315.23329598    0.40675521    0.29057327    0.53702414    0.17038445]
[37m[1m [1603.77086141    0.33290002    0.34709999    0.41009998    0.0989    ]
[37m[1m [2424.87341364    0.23480001    0.36390001    0.3698        0.0781    ]]
[37m[1m[2023-06-25 12:33:49,981][129146] Max Reward on eval: 3416.852499386668
[37m[1m[2023-06-25 12:33:49,981][129146] Min Reward on eval: 315.2332959793508
[37m[1m[2023-06-25 12:33:49,982][129146] Mean Reward across all agents: 1854.2172875245528
[37m[1m[2023-06-25 12:33:49,982][129146] Average Trajectory Length: 986.9813333333333
[36m[2023-06-25 12:33:49,985][129146] mean_value=-187.14667557649744, max_value=2211.577370904945
[37m[1m[2023-06-25 12:33:49,988][129146] New mean coefficients: [[ 1.3171821   0.40572706 -0.42593598 -1.2202319   0.02800892]]
[37m[1m[2023-06-25 12:33:49,989][129146] Moving the mean solution point...
[36m[2023-06-25 12:33:59,647][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 12:33:59,647][129146] FPS: 397659.41
[36m[2023-06-25 12:33:59,650][129146] itr=1267, itrs=2000, Progress: 63.35%
[36m[2023-06-25 12:34:11,092][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 12:34:11,092][129146] FPS: 336298.57
[36m[2023-06-25 12:34:15,864][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:34:15,864][129146] Reward + Measures: [[3530.62627086    0.22357875    0.48146006    0.26437917    0.01324875]]
[37m[1m[2023-06-25 12:34:15,865][129146] Max Reward on eval: 3530.62627085777
[37m[1m[2023-06-25 12:34:15,865][129146] Min Reward on eval: 3530.62627085777
[37m[1m[2023-06-25 12:34:15,865][129146] Mean Reward across all agents: 3530.62627085777
[37m[1m[2023-06-25 12:34:15,865][129146] Average Trajectory Length: 996.6116666666667
[36m[2023-06-25 12:34:21,302][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:34:21,308][129146] Reward + Measures: [[1588.04297036    0.33090001    0.41929999    0.45140001    0.0919    ]
[37m[1m [1388.15221491    0.27959999    0.34160003    0.43980002    0.1646    ]
[37m[1m [1968.28126888    0.2211        0.41140005    0.3251        0.0623    ]
[37m[1m ...
[37m[1m [2567.30446794    0.24949999    0.44069996    0.33829999    0.0722    ]
[37m[1m [3139.50105628    0.23600002    0.51020002    0.2678        0.0127    ]
[37m[1m [1970.74195331    0.32910001    0.39340004    0.4111        0.14740001]]
[37m[1m[2023-06-25 12:34:21,308][129146] Max Reward on eval: 3505.6000047796406
[37m[1m[2023-06-25 12:34:21,309][129146] Min Reward on eval: 444.96989514628075
[37m[1m[2023-06-25 12:34:21,309][129146] Mean Reward across all agents: 2045.6793195252228
[37m[1m[2023-06-25 12:34:21,309][129146] Average Trajectory Length: 994.538
[36m[2023-06-25 12:34:21,312][129146] mean_value=-247.9855880439679, max_value=2510.02859362707
[37m[1m[2023-06-25 12:34:21,315][129146] New mean coefficients: [[ 1.4908001   0.45392114 -0.4169014  -1.2166661   0.01271482]]
[37m[1m[2023-06-25 12:34:21,316][129146] Moving the mean solution point...
[36m[2023-06-25 12:34:31,030][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 12:34:31,030][129146] FPS: 395343.73
[36m[2023-06-25 12:34:31,033][129146] itr=1268, itrs=2000, Progress: 63.40%
[36m[2023-06-25 12:34:42,455][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 12:34:42,455][129146] FPS: 336928.37
[36m[2023-06-25 12:34:47,260][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:34:47,260][129146] Reward + Measures: [[1784.22080722    0.32556656    0.30732495    0.43604964    0.09475976]]
[37m[1m[2023-06-25 12:34:47,261][129146] Max Reward on eval: 1784.22080721526
[37m[1m[2023-06-25 12:34:47,261][129146] Min Reward on eval: 1784.22080721526
[37m[1m[2023-06-25 12:34:47,261][129146] Mean Reward across all agents: 1784.22080721526
[37m[1m[2023-06-25 12:34:47,261][129146] Average Trajectory Length: 998.6669999999999
[36m[2023-06-25 12:34:52,683][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:34:52,684][129146] Reward + Measures: [[1825.42786437    0.34880003    0.30920002    0.42229995    0.103     ]
[37m[1m [1799.05085151    0.33189997    0.31670004    0.45880005    0.08639999]
[37m[1m [  31.9728156     0.39519998    0.23670001    0.40120003    0.21420002]
[37m[1m ...
[37m[1m [1134.22471431    0.30845729    0.26112902    0.4107655     0.1199284 ]
[37m[1m [1497.45093807    0.33810002    0.33000001    0.4941        0.0998    ]
[37m[1m [ 578.06761734    0.27270001    0.28740001    0.49670002    0.22610001]]
[37m[1m[2023-06-25 12:34:52,684][129146] Max Reward on eval: 2113.302041592193
[37m[1m[2023-06-25 12:34:52,684][129146] Min Reward on eval: -406.3184788282204
[37m[1m[2023-06-25 12:34:52,684][129146] Mean Reward across all agents: 1123.9338536422797
[37m[1m[2023-06-25 12:34:52,685][129146] Average Trajectory Length: 984.1276666666666
[36m[2023-06-25 12:34:52,687][129146] mean_value=-447.65139068614536, max_value=1798.2262905671591
[37m[1m[2023-06-25 12:34:52,690][129146] New mean coefficients: [[ 1.6476985   0.26508284 -0.16669652 -0.76337767 -0.07021013]]
[37m[1m[2023-06-25 12:34:52,691][129146] Moving the mean solution point...
[36m[2023-06-25 12:35:02,351][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 12:35:02,351][129146] FPS: 397566.63
[36m[2023-06-25 12:35:02,354][129146] itr=1269, itrs=2000, Progress: 63.45%
[36m[2023-06-25 12:35:13,815][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 12:35:13,816][129146] FPS: 335736.21
[36m[2023-06-25 12:35:18,569][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:35:18,570][129146] Reward + Measures: [[1924.61699396    0.31708997    0.31887084    0.4171049     0.08075251]]
[37m[1m[2023-06-25 12:35:18,570][129146] Max Reward on eval: 1924.6169939629442
[37m[1m[2023-06-25 12:35:18,570][129146] Min Reward on eval: 1924.6169939629442
[37m[1m[2023-06-25 12:35:18,570][129146] Mean Reward across all agents: 1924.6169939629442
[37m[1m[2023-06-25 12:35:18,570][129146] Average Trajectory Length: 997.7203333333333
[36m[2023-06-25 12:35:24,285][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:35:24,286][129146] Reward + Measures: [[1419.73507648    0.35760003    0.36089998    0.4217        0.13030002]
[37m[1m [1063.40047596    0.31878099    0.20530446    0.44147015    0.2010788 ]
[37m[1m [1742.37755259    0.2958        0.33570001    0.42080003    0.09869999]
[37m[1m ...
[37m[1m [1495.44299374    0.35169998    0.3888        0.43430001    0.1543    ]
[37m[1m [1896.81967496    0.3389        0.31599998    0.43830004    0.1043    ]
[37m[1m [1594.43973428    0.33540002    0.36970001    0.4068        0.11859999]]
[37m[1m[2023-06-25 12:35:24,286][129146] Max Reward on eval: 1988.096057059567
[37m[1m[2023-06-25 12:35:24,286][129146] Min Reward on eval: -30.807233755523338
[37m[1m[2023-06-25 12:35:24,286][129146] Mean Reward across all agents: 1378.1410607055645
[37m[1m[2023-06-25 12:35:24,287][129146] Average Trajectory Length: 996.0496666666667
[36m[2023-06-25 12:35:24,290][129146] mean_value=-103.50051619492629, max_value=2165.434498582199
[37m[1m[2023-06-25 12:35:24,293][129146] New mean coefficients: [[ 1.8728008   0.22489004 -0.38031584  0.41763484  0.01052885]]
[37m[1m[2023-06-25 12:35:24,294][129146] Moving the mean solution point...
[36m[2023-06-25 12:35:33,945][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 12:35:33,945][129146] FPS: 397954.56
[36m[2023-06-25 12:35:33,947][129146] itr=1270, itrs=2000, Progress: 63.50%
[37m[1m[2023-06-25 12:35:41,648][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001250
[36m[2023-06-25 12:35:53,287][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 12:35:53,288][129146] FPS: 336046.32
[36m[2023-06-25 12:35:58,022][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:35:58,023][129146] Reward + Measures: [[2111.65774144    0.30253813    0.32625249    0.39326531    0.06716014]]
[37m[1m[2023-06-25 12:35:58,023][129146] Max Reward on eval: 2111.657741436177
[37m[1m[2023-06-25 12:35:58,023][129146] Min Reward on eval: 2111.657741436177
[37m[1m[2023-06-25 12:35:58,023][129146] Mean Reward across all agents: 2111.657741436177
[37m[1m[2023-06-25 12:35:58,024][129146] Average Trajectory Length: 998.0756666666666
[36m[2023-06-25 12:36:03,491][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:36:03,497][129146] Reward + Measures: [[1768.34754428    0.30710003    0.30070001    0.47119999    0.13510001]
[37m[1m [1612.00496685    0.28370002    0.32220003    0.46919999    0.1635    ]
[37m[1m [2205.40082882    0.28740001    0.34200001    0.36129999    0.0553    ]
[37m[1m ...
[37m[1m [1473.51789892    0.3019        0.31060001    0.48540002    0.1751    ]
[37m[1m [1968.91871868    0.29970002    0.33140001    0.35970002    0.0643    ]
[37m[1m [1735.08691021    0.30890003    0.3026        0.4765        0.14830001]]
[37m[1m[2023-06-25 12:36:03,497][129146] Max Reward on eval: 2283.421405216446
[37m[1m[2023-06-25 12:36:03,497][129146] Min Reward on eval: -207.8143956322514
[37m[1m[2023-06-25 12:36:03,497][129146] Mean Reward across all agents: 1576.4917483852028
[37m[1m[2023-06-25 12:36:03,498][129146] Average Trajectory Length: 973.4159999999999
[36m[2023-06-25 12:36:03,501][129146] mean_value=-135.43502346306897, max_value=741.4050117781023
[37m[1m[2023-06-25 12:36:03,503][129146] New mean coefficients: [[ 1.680138    0.4167996  -0.41233745  0.10720727  0.05730812]]
[37m[1m[2023-06-25 12:36:03,505][129146] Moving the mean solution point...
[36m[2023-06-25 12:36:13,136][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 12:36:13,136][129146] FPS: 398759.38
[36m[2023-06-25 12:36:13,139][129146] itr=1271, itrs=2000, Progress: 63.55%
[36m[2023-06-25 12:36:24,620][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 12:36:24,620][129146] FPS: 335162.17
[36m[2023-06-25 12:36:29,391][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:36:29,392][129146] Reward + Measures: [[2286.97495658    0.29489034    0.3388111     0.37589085    0.05433452]]
[37m[1m[2023-06-25 12:36:29,392][129146] Max Reward on eval: 2286.9749565765487
[37m[1m[2023-06-25 12:36:29,392][129146] Min Reward on eval: 2286.9749565765487
[37m[1m[2023-06-25 12:36:29,393][129146] Mean Reward across all agents: 2286.9749565765487
[37m[1m[2023-06-25 12:36:29,393][129146] Average Trajectory Length: 999.366
[36m[2023-06-25 12:36:34,843][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:36:34,849][129146] Reward + Measures: [[1900.82829428    0.29790002    0.2836        0.40180001    0.1037    ]
[37m[1m [1105.58207625    0.3712        0.23860002    0.46970007    0.1776    ]
[37m[1m [1449.81966314    0.26884875    0.3105503     0.38996717    0.10810661]
[37m[1m ...
[37m[1m [1900.72463191    0.28408828    0.30427033    0.35533333    0.07414506]
[37m[1m [1667.26789041    0.3321        0.25209999    0.43249997    0.1461    ]
[37m[1m [1737.34613393    0.26440001    0.31300002    0.32260001    0.0494    ]]
[37m[1m[2023-06-25 12:36:34,849][129146] Max Reward on eval: 2329.0633202847325
[37m[1m[2023-06-25 12:36:34,849][129146] Min Reward on eval: -10.945073397411033
[37m[1m[2023-06-25 12:36:34,850][129146] Mean Reward across all agents: 1543.8696509312972
[37m[1m[2023-06-25 12:36:34,850][129146] Average Trajectory Length: 975.259
[36m[2023-06-25 12:36:34,854][129146] mean_value=-163.38172589723092, max_value=1619.8175490104682
[37m[1m[2023-06-25 12:36:34,856][129146] New mean coefficients: [[ 1.6115777   0.7864114  -0.4197155  -0.7968025   0.09603726]]
[37m[1m[2023-06-25 12:36:34,858][129146] Moving the mean solution point...
[36m[2023-06-25 12:36:44,545][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 12:36:44,546][129146] FPS: 396447.17
[36m[2023-06-25 12:36:44,548][129146] itr=1272, itrs=2000, Progress: 63.60%
[36m[2023-06-25 12:36:56,020][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 12:36:56,021][129146] FPS: 335395.15
[36m[2023-06-25 12:37:00,882][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:37:00,883][129146] Reward + Measures: [[2436.18816585    0.28746587    0.34829843    0.36283463    0.04699969]]
[37m[1m[2023-06-25 12:37:00,883][129146] Max Reward on eval: 2436.1881658466473
[37m[1m[2023-06-25 12:37:00,883][129146] Min Reward on eval: 2436.1881658466473
[37m[1m[2023-06-25 12:37:00,884][129146] Mean Reward across all agents: 2436.1881658466473
[37m[1m[2023-06-25 12:37:00,884][129146] Average Trajectory Length: 998.0406666666667
[36m[2023-06-25 12:37:06,386][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:37:06,387][129146] Reward + Measures: [[2172.11379712    0.31039998    0.3434        0.39089999    0.0691    ]
[37m[1m [ 687.25342818    0.50459999    0.1499        0.53820002    0.25139999]
[37m[1m [2060.54450406    0.32340002    0.33460003    0.40779996    0.0775    ]
[37m[1m ...
[37m[1m [2102.2873302     0.2985        0.33020002    0.3876        0.053     ]
[37m[1m [2374.01615363    0.28820002    0.35870001    0.37219998    0.0537    ]
[37m[1m [2363.48893652    0.31360003    0.31479999    0.40040001    0.0713    ]]
[37m[1m[2023-06-25 12:37:06,387][129146] Max Reward on eval: 2551.4481020243607
[37m[1m[2023-06-25 12:37:06,387][129146] Min Reward on eval: -486.72139995206453
[37m[1m[2023-06-25 12:37:06,387][129146] Mean Reward across all agents: 1393.7511051860552
[37m[1m[2023-06-25 12:37:06,387][129146] Average Trajectory Length: 995.8059999999999
[36m[2023-06-25 12:37:06,392][129146] mean_value=36.48492910156592, max_value=1828.138564867454
[37m[1m[2023-06-25 12:37:06,395][129146] New mean coefficients: [[ 1.3790047   1.1315805  -0.3888258  -0.48669946  0.22663829]]
[37m[1m[2023-06-25 12:37:06,396][129146] Moving the mean solution point...
[36m[2023-06-25 12:37:16,164][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 12:37:16,164][129146] FPS: 393188.88
[36m[2023-06-25 12:37:16,166][129146] itr=1273, itrs=2000, Progress: 63.65%
[36m[2023-06-25 12:37:27,612][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 12:37:27,612][129146] FPS: 336188.87
[36m[2023-06-25 12:37:32,415][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:37:32,415][129146] Reward + Measures: [[2576.41582869    0.28352684    0.34633595    0.36180761    0.04332915]]
[37m[1m[2023-06-25 12:37:32,415][129146] Max Reward on eval: 2576.4158286852844
[37m[1m[2023-06-25 12:37:32,416][129146] Min Reward on eval: 2576.4158286852844
[37m[1m[2023-06-25 12:37:32,416][129146] Mean Reward across all agents: 2576.4158286852844
[37m[1m[2023-06-25 12:37:32,416][129146] Average Trajectory Length: 998.887
[36m[2023-06-25 12:37:38,003][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:37:38,004][129146] Reward + Measures: [[1478.98085716    0.3125        0.36410004    0.3484        0.0655    ]
[37m[1m [1595.1672376     0.2724753     0.31327873    0.34477875    0.08254069]
[37m[1m [2183.56278741    0.27260002    0.2904        0.39079997    0.0873    ]
[37m[1m ...
[37m[1m [1851.51390638    0.2737        0.34          0.3554        0.0578    ]
[37m[1m [1019.94896092    0.23099999    0.25260001    0.30780002    0.1087    ]
[37m[1m [1260.742166      0.33020002    0.37970001    0.39199999    0.0724    ]]
[37m[1m[2023-06-25 12:37:38,004][129146] Max Reward on eval: 2467.136949485494
[37m[1m[2023-06-25 12:37:38,004][129146] Min Reward on eval: -96.08535218956531
[37m[1m[2023-06-25 12:37:38,005][129146] Mean Reward across all agents: 1221.1789120444628
[37m[1m[2023-06-25 12:37:38,005][129146] Average Trajectory Length: 986.5553333333334
[36m[2023-06-25 12:37:38,007][129146] mean_value=-580.4738996841562, max_value=1776.0883341232293
[37m[1m[2023-06-25 12:37:38,010][129146] New mean coefficients: [[ 1.3041034   0.68215084 -0.21105994  0.08327323  0.23724444]]
[37m[1m[2023-06-25 12:37:38,011][129146] Moving the mean solution point...
[36m[2023-06-25 12:37:47,811][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 12:37:47,811][129146] FPS: 391904.58
[36m[2023-06-25 12:37:47,813][129146] itr=1274, itrs=2000, Progress: 63.70%
[36m[2023-06-25 12:37:59,376][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 12:37:59,376][129146] FPS: 332799.55
[36m[2023-06-25 12:38:04,197][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:38:04,198][129146] Reward + Measures: [[2748.03253444    0.27273405    0.35646093    0.34536603    0.03239083]]
[37m[1m[2023-06-25 12:38:04,198][129146] Max Reward on eval: 2748.032534435028
[37m[1m[2023-06-25 12:38:04,198][129146] Min Reward on eval: 2748.032534435028
[37m[1m[2023-06-25 12:38:04,198][129146] Mean Reward across all agents: 2748.032534435028
[37m[1m[2023-06-25 12:38:04,199][129146] Average Trajectory Length: 998.5866666666666
[36m[2023-06-25 12:38:09,697][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:38:09,698][129146] Reward + Measures: [[1257.78560915    0.3138963     0.24629752    0.44592592    0.18136545]
[37m[1m [1504.4834524     0.35020003    0.2726        0.42350006    0.16060001]
[37m[1m [1006.84196646    0.2985        0.25630003    0.4501        0.19000001]
[37m[1m ...
[37m[1m [ 632.89613063    0.31448808    0.23998256    0.50182754    0.26114956]
[37m[1m [2516.10315289    0.28849998    0.3743        0.36520001    0.0458    ]
[37m[1m [2051.8868829     0.33470002    0.27180001    0.44569999    0.1245    ]]
[37m[1m[2023-06-25 12:38:09,698][129146] Max Reward on eval: 2732.5665701642633
[37m[1m[2023-06-25 12:38:09,699][129146] Min Reward on eval: -391.76408386755503
[37m[1m[2023-06-25 12:38:09,699][129146] Mean Reward across all agents: 1589.6577854075745
[37m[1m[2023-06-25 12:38:09,699][129146] Average Trajectory Length: 986.1506666666667
[36m[2023-06-25 12:38:09,703][129146] mean_value=-179.46464384887665, max_value=1652.1486038414296
[37m[1m[2023-06-25 12:38:09,706][129146] New mean coefficients: [[ 1.3102425   0.2910237  -0.23866513  0.95827764  0.21475169]]
[37m[1m[2023-06-25 12:38:09,707][129146] Moving the mean solution point...
[36m[2023-06-25 12:38:19,462][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 12:38:19,462][129146] FPS: 393701.11
[36m[2023-06-25 12:38:19,465][129146] itr=1275, itrs=2000, Progress: 63.75%
[36m[2023-06-25 12:38:30,913][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 12:38:30,913][129146] FPS: 336228.83
[36m[2023-06-25 12:38:35,729][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:38:35,729][129146] Reward + Measures: [[1536.22387767    0.22869515    0.27066416    0.34706652    0.0929294 ]]
[37m[1m[2023-06-25 12:38:35,729][129146] Max Reward on eval: 1536.2238776737008
[37m[1m[2023-06-25 12:38:35,729][129146] Min Reward on eval: 1536.2238776737008
[37m[1m[2023-06-25 12:38:35,730][129146] Mean Reward across all agents: 1536.2238776737008
[37m[1m[2023-06-25 12:38:35,730][129146] Average Trajectory Length: 958.8423333333333
[36m[2023-06-25 12:38:41,133][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:38:41,134][129146] Reward + Measures: [[1414.42554072    0.25675496    0.26730466    0.3338353     0.10311218]
[37m[1m [1179.35235033    0.24340545    0.26667601    0.41890237    0.11156426]
[37m[1m [1659.02981879    0.2372521     0.27364731    0.36192456    0.08738264]
[37m[1m ...
[37m[1m [1590.23952099    0.2225        0.2746        0.37650001    0.08850001]
[37m[1m [1742.11486387    0.2105        0.27990001    0.3495        0.0763    ]
[37m[1m [1940.63776522    0.2383875     0.28281251    0.3934125     0.08680625]]
[37m[1m[2023-06-25 12:38:41,134][129146] Max Reward on eval: 2112.7013100868558
[37m[1m[2023-06-25 12:38:41,134][129146] Min Reward on eval: 529.5411374847463
[37m[1m[2023-06-25 12:38:41,134][129146] Mean Reward across all agents: 1395.8168431508257
[37m[1m[2023-06-25 12:38:41,135][129146] Average Trajectory Length: 942.079
[36m[2023-06-25 12:38:41,138][129146] mean_value=-177.67403628895917, max_value=697.7547010405631
[37m[1m[2023-06-25 12:38:41,140][129146] New mean coefficients: [[1.8540318  0.27423534 0.0397383  0.69512177 0.19812018]]
[37m[1m[2023-06-25 12:38:41,141][129146] Moving the mean solution point...
[36m[2023-06-25 12:38:50,897][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 12:38:50,897][129146] FPS: 393694.91
[36m[2023-06-25 12:38:50,899][129146] itr=1276, itrs=2000, Progress: 63.80%
[36m[2023-06-25 12:39:02,350][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 12:39:02,351][129146] FPS: 336148.06
[36m[2023-06-25 12:39:07,020][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:39:07,021][129146] Reward + Measures: [[1758.17902318    0.23277844    0.27692285    0.35724625    0.0890012 ]]
[37m[1m[2023-06-25 12:39:07,021][129146] Max Reward on eval: 1758.1790231753714
[37m[1m[2023-06-25 12:39:07,021][129146] Min Reward on eval: 1758.1790231753714
[37m[1m[2023-06-25 12:39:07,021][129146] Mean Reward across all agents: 1758.1790231753714
[37m[1m[2023-06-25 12:39:07,022][129146] Average Trajectory Length: 972.289
[36m[2023-06-25 12:39:12,412][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:39:12,412][129146] Reward + Measures: [[1886.6057861     0.23600002    0.27770001    0.39390001    0.1061    ]
[37m[1m [1704.91537341    0.23992932    0.25420776    0.42830521    0.11708621]
[37m[1m [1379.75752444    0.23275995    0.24883752    0.32382411    0.09488376]
[37m[1m ...
[37m[1m [ 880.60292078    0.20381676    0.24573255    0.30052945    0.10143999]
[37m[1m [1174.02611552    0.19063278    0.25713554    0.26537535    0.06243131]
[37m[1m [1870.13216972    0.2717917     0.30555376    0.39108735    0.11262411]]
[37m[1m[2023-06-25 12:39:12,412][129146] Max Reward on eval: 2123.645365374966
[37m[1m[2023-06-25 12:39:12,413][129146] Min Reward on eval: 413.17597981679137
[37m[1m[2023-06-25 12:39:12,413][129146] Mean Reward across all agents: 1489.348147252534
[37m[1m[2023-06-25 12:39:12,413][129146] Average Trajectory Length: 937.2346666666666
[36m[2023-06-25 12:39:12,416][129146] mean_value=-284.90638410883383, max_value=414.10460720691594
[37m[1m[2023-06-25 12:39:12,418][129146] New mean coefficients: [[ 1.7972624   0.34708506 -0.2567742   0.41834235  0.10468233]]
[37m[1m[2023-06-25 12:39:12,419][129146] Moving the mean solution point...
[36m[2023-06-25 12:39:22,054][129146] train() took 9.63 seconds to complete
[36m[2023-06-25 12:39:22,054][129146] FPS: 398647.71
[36m[2023-06-25 12:39:22,056][129146] itr=1277, itrs=2000, Progress: 63.85%
[36m[2023-06-25 12:39:33,500][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 12:39:33,500][129146] FPS: 336384.19
[36m[2023-06-25 12:39:38,234][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:39:38,234][129146] Reward + Measures: [[1995.6093192     0.23965026    0.29043534    0.35358381    0.07769243]]
[37m[1m[2023-06-25 12:39:38,234][129146] Max Reward on eval: 1995.6093192038088
[37m[1m[2023-06-25 12:39:38,235][129146] Min Reward on eval: 1995.6093192038088
[37m[1m[2023-06-25 12:39:38,235][129146] Mean Reward across all agents: 1995.6093192038088
[37m[1m[2023-06-25 12:39:38,235][129146] Average Trajectory Length: 976.0496666666667
[36m[2023-06-25 12:39:43,676][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:39:43,682][129146] Reward + Measures: [[1693.72884791    0.22491443    0.31825265    0.33076158    0.05702969]
[37m[1m [1763.97973376    0.25970003    0.26300001    0.36220002    0.0922    ]
[37m[1m [1667.78954862    0.29278985    0.30995083    0.39211696    0.07210848]
[37m[1m ...
[37m[1m [1798.35097136    0.2555941     0.28446469    0.40644121    0.10133529]
[37m[1m [1417.29651286    0.2339814     0.26587144    0.3226465     0.05914452]
[37m[1m [1686.88993323    0.30160004    0.24229999    0.44549999    0.1426    ]]
[37m[1m[2023-06-25 12:39:43,682][129146] Max Reward on eval: 2297.3232976800064
[37m[1m[2023-06-25 12:39:43,682][129146] Min Reward on eval: 941.9032320211234
[37m[1m[2023-06-25 12:39:43,683][129146] Mean Reward across all agents: 1638.4459420506466
[37m[1m[2023-06-25 12:39:43,683][129146] Average Trajectory Length: 966.7446666666666
[36m[2023-06-25 12:39:43,685][129146] mean_value=-274.5561343593712, max_value=626.3435020522768
[37m[1m[2023-06-25 12:39:43,688][129146] New mean coefficients: [[ 1.6413378   0.28303593 -0.13322565 -0.0154146  -0.08877891]]
[37m[1m[2023-06-25 12:39:43,689][129146] Moving the mean solution point...
[36m[2023-06-25 12:39:53,497][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 12:39:53,498][129146] FPS: 391579.48
[36m[2023-06-25 12:39:53,500][129146] itr=1278, itrs=2000, Progress: 63.90%
[36m[2023-06-25 12:40:05,061][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 12:40:05,061][129146] FPS: 332828.01
[36m[2023-06-25 12:40:09,784][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:40:09,790][129146] Reward + Measures: [[2199.10057091    0.23996106    0.29618391    0.34830567    0.07174206]]
[37m[1m[2023-06-25 12:40:09,790][129146] Max Reward on eval: 2199.1005709086935
[37m[1m[2023-06-25 12:40:09,791][129146] Min Reward on eval: 2199.1005709086935
[37m[1m[2023-06-25 12:40:09,791][129146] Mean Reward across all agents: 2199.1005709086935
[37m[1m[2023-06-25 12:40:09,791][129146] Average Trajectory Length: 984.2136666666667
[36m[2023-06-25 12:40:15,407][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:40:15,408][129146] Reward + Measures: [[1396.82053443    0.28218219    0.30334577    0.41156504    0.07808008]
[37m[1m [1590.82676985    0.27439076    0.29917145    0.37983838    0.08812493]
[37m[1m [2037.24662065    0.23280001    0.25659999    0.3484        0.09510001]
[37m[1m ...
[37m[1m [1663.76864093    0.26289999    0.30809999    0.39999998    0.08279999]
[37m[1m [1380.53162236    0.24395433    0.29720151    0.36973539    0.07881301]
[37m[1m [1082.30075993    0.20837183    0.18704662    0.2971476     0.12548836]]
[37m[1m[2023-06-25 12:40:15,408][129146] Max Reward on eval: 2532.8387229280315
[37m[1m[2023-06-25 12:40:15,408][129146] Min Reward on eval: 370.3047973493114
[37m[1m[2023-06-25 12:40:15,409][129146] Mean Reward across all agents: 1756.5636922203742
[37m[1m[2023-06-25 12:40:15,409][129146] Average Trajectory Length: 971.4893333333333
[36m[2023-06-25 12:40:15,411][129146] mean_value=-300.0038983942293, max_value=810.06932442917
[37m[1m[2023-06-25 12:40:15,414][129146] New mean coefficients: [[ 2.2837338  -0.26585656 -0.28666133  0.3948186  -0.12607116]]
[37m[1m[2023-06-25 12:40:15,415][129146] Moving the mean solution point...
[36m[2023-06-25 12:40:25,058][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 12:40:25,058][129146] FPS: 398303.56
[36m[2023-06-25 12:40:25,060][129146] itr=1279, itrs=2000, Progress: 63.95%
[36m[2023-06-25 12:40:36,570][129146] train() took 11.48 seconds to complete
[36m[2023-06-25 12:40:36,570][129146] FPS: 334444.47
[36m[2023-06-25 12:40:41,350][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:40:41,350][129146] Reward + Measures: [[1895.87879301    0.21391422    0.30271184    0.28718936    0.0674432 ]]
[37m[1m[2023-06-25 12:40:41,351][129146] Max Reward on eval: 1895.8787930051597
[37m[1m[2023-06-25 12:40:41,351][129146] Min Reward on eval: 1895.8787930051597
[37m[1m[2023-06-25 12:40:41,351][129146] Mean Reward across all agents: 1895.8787930051597
[37m[1m[2023-06-25 12:40:41,351][129146] Average Trajectory Length: 932.6706666666666
[36m[2023-06-25 12:40:46,859][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:40:46,859][129146] Reward + Measures: [[1688.3374132     0.20429154    0.30178317    0.28819123    0.05896349]
[37m[1m [1744.217094      0.20742781    0.30119735    0.26859006    0.05637351]
[37m[1m [1650.93066608    0.19726217    0.30541405    0.27776441    0.0646803 ]
[37m[1m ...
[37m[1m [1608.02138822    0.17378761    0.25193334    0.26332399    0.06826447]
[37m[1m [2281.560579      0.20971148    0.30766231    0.30295494    0.06888443]
[37m[1m [1938.65589556    0.23559408    0.29220238    0.31905293    0.1000944 ]]
[37m[1m[2023-06-25 12:40:46,860][129146] Max Reward on eval: 2399.8427588550376
[37m[1m[2023-06-25 12:40:46,860][129146] Min Reward on eval: 1047.530553451844
[37m[1m[2023-06-25 12:40:46,860][129146] Mean Reward across all agents: 1797.654178165622
[37m[1m[2023-06-25 12:40:46,860][129146] Average Trajectory Length: 923.2196666666666
[36m[2023-06-25 12:40:46,863][129146] mean_value=-217.32979554770264, max_value=616.7966456651362
[37m[1m[2023-06-25 12:40:46,866][129146] New mean coefficients: [[ 1.9727771  -0.16850261 -0.2322638   0.76214063 -0.17577666]]
[37m[1m[2023-06-25 12:40:46,867][129146] Moving the mean solution point...
[36m[2023-06-25 12:40:56,659][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 12:40:56,659][129146] FPS: 392215.65
[36m[2023-06-25 12:40:56,661][129146] itr=1280, itrs=2000, Progress: 64.00%
[37m[1m[2023-06-25 12:41:04,250][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001260
[36m[2023-06-25 12:41:15,956][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 12:41:15,957][129146] FPS: 334089.36
[36m[2023-06-25 12:41:20,678][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:41:20,679][129146] Reward + Measures: [[2177.52442057    0.22298153    0.31226924    0.29265314    0.06442106]]
[37m[1m[2023-06-25 12:41:20,679][129146] Max Reward on eval: 2177.5244205724875
[37m[1m[2023-06-25 12:41:20,679][129146] Min Reward on eval: 2177.5244205724875
[37m[1m[2023-06-25 12:41:20,679][129146] Mean Reward across all agents: 2177.5244205724875
[37m[1m[2023-06-25 12:41:20,679][129146] Average Trajectory Length: 949.735
[36m[2023-06-25 12:41:26,117][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:41:26,118][129146] Reward + Measures: [[2590.19076253    0.2414        0.32300001    0.3154        0.065     ]
[37m[1m [2568.43804459    0.2414        0.32589999    0.31619999    0.0768    ]
[37m[1m [2519.15682985    0.2402        0.32589999    0.3073        0.0706    ]
[37m[1m ...
[37m[1m [2413.72237769    0.22454683    0.31791803    0.29301983    0.06203243]
[37m[1m [2249.81871772    0.21280096    0.31245461    0.30645695    0.07262211]
[37m[1m [2567.91205316    0.2471        0.32720003    0.31979999    0.07920001]]
[37m[1m[2023-06-25 12:41:26,118][129146] Max Reward on eval: 2676.5287218708313
[37m[1m[2023-06-25 12:41:26,118][129146] Min Reward on eval: 1518.2558752717828
[37m[1m[2023-06-25 12:41:26,119][129146] Mean Reward across all agents: 2256.4405697382977
[37m[1m[2023-06-25 12:41:26,119][129146] Average Trajectory Length: 958.9133333333333
[36m[2023-06-25 12:41:26,123][129146] mean_value=82.51164600521297, max_value=480.70720386582525
[37m[1m[2023-06-25 12:41:26,125][129146] New mean coefficients: [[ 2.7143562  -0.34204572 -0.31291693  0.8859912  -0.18192649]]
[37m[1m[2023-06-25 12:41:26,126][129146] Moving the mean solution point...
[36m[2023-06-25 12:41:35,786][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 12:41:35,787][129146] FPS: 397572.74
[36m[2023-06-25 12:41:35,789][129146] itr=1281, itrs=2000, Progress: 64.05%
[36m[2023-06-25 12:41:47,202][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 12:41:47,202][129146] FPS: 337166.54
[36m[2023-06-25 12:41:52,002][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:41:52,003][129146] Reward + Measures: [[2412.30074305    0.23058699    0.3207041     0.29781461    0.06238396]]
[37m[1m[2023-06-25 12:41:52,003][129146] Max Reward on eval: 2412.3007430480993
[37m[1m[2023-06-25 12:41:52,003][129146] Min Reward on eval: 2412.3007430480993
[37m[1m[2023-06-25 12:41:52,004][129146] Mean Reward across all agents: 2412.3007430480993
[37m[1m[2023-06-25 12:41:52,004][129146] Average Trajectory Length: 965.7783333333333
[36m[2023-06-25 12:41:57,682][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:41:57,683][129146] Reward + Measures: [[2165.15127482    0.21173055    0.2887744     0.28879657    0.06655665]
[37m[1m [2109.23723625    0.22860001    0.2931        0.30290005    0.09940001]
[37m[1m [2233.85367034    0.24760595    0.30025706    0.3141225     0.06875914]
[37m[1m ...
[37m[1m [2400.14065241    0.24030165    0.30926663    0.33563727    0.08203045]
[37m[1m [1872.63864464    0.2125212     0.29067275    0.28579697    0.07869697]
[37m[1m [2236.77302958    0.24016666    0.32393336    0.30130002    0.08163334]]
[37m[1m[2023-06-25 12:41:57,683][129146] Max Reward on eval: 2735.5314541013913
[37m[1m[2023-06-25 12:41:57,683][129146] Min Reward on eval: 1507.701283541345
[37m[1m[2023-06-25 12:41:57,684][129146] Mean Reward across all agents: 2213.725353960478
[37m[1m[2023-06-25 12:41:57,684][129146] Average Trajectory Length: 960.009
[36m[2023-06-25 12:41:57,687][129146] mean_value=-109.20064430707274, max_value=622.5200698025515
[37m[1m[2023-06-25 12:41:57,689][129146] New mean coefficients: [[ 2.3377345  -0.5548899  -0.19957879  0.931752   -0.1193669 ]]
[37m[1m[2023-06-25 12:41:57,690][129146] Moving the mean solution point...
[36m[2023-06-25 12:42:07,555][129146] train() took 9.86 seconds to complete
[36m[2023-06-25 12:42:07,555][129146] FPS: 389338.00
[36m[2023-06-25 12:42:07,558][129146] itr=1282, itrs=2000, Progress: 64.10%
[36m[2023-06-25 12:42:19,214][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 12:42:19,214][129146] FPS: 330163.46
[36m[2023-06-25 12:42:23,912][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:42:23,912][129146] Reward + Measures: [[2652.35131163    0.2398375     0.32460749    0.30221581    0.05967313]]
[37m[1m[2023-06-25 12:42:23,912][129146] Max Reward on eval: 2652.351311626151
[37m[1m[2023-06-25 12:42:23,913][129146] Min Reward on eval: 2652.351311626151
[37m[1m[2023-06-25 12:42:23,913][129146] Mean Reward across all agents: 2652.351311626151
[37m[1m[2023-06-25 12:42:23,913][129146] Average Trajectory Length: 976.4323333333333
[36m[2023-06-25 12:42:29,356][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:42:29,357][129146] Reward + Measures: [[2211.36270414    0.22468865    0.32467341    0.28779426    0.05801054]
[37m[1m [2693.46654474    0.2581        0.33899999    0.3132        0.0643    ]
[37m[1m [2582.25509593    0.23650001    0.33489999    0.29310003    0.0483    ]
[37m[1m ...
[37m[1m [2259.83975065    0.22865561    0.30343252    0.29510397    0.0537067 ]
[37m[1m [2651.08678796    0.2418        0.34199998    0.3073        0.0553    ]
[37m[1m [2296.84193559    0.2221        0.33000001    0.30109999    0.0557    ]]
[37m[1m[2023-06-25 12:42:29,357][129146] Max Reward on eval: 2954.8389194928227
[37m[1m[2023-06-25 12:42:29,357][129146] Min Reward on eval: 1826.6496142665915
[37m[1m[2023-06-25 12:42:29,358][129146] Mean Reward across all agents: 2534.1716819204257
[37m[1m[2023-06-25 12:42:29,358][129146] Average Trajectory Length: 978.5166666666667
[36m[2023-06-25 12:42:29,361][129146] mean_value=-3.8610456746218187, max_value=413.8108406133456
[37m[1m[2023-06-25 12:42:29,364][129146] New mean coefficients: [[ 1.6246883  -0.5354191  -0.27810144 -0.13918    -0.03445327]]
[37m[1m[2023-06-25 12:42:29,365][129146] Moving the mean solution point...
[36m[2023-06-25 12:42:39,137][129146] train() took 9.77 seconds to complete
[36m[2023-06-25 12:42:39,137][129146] FPS: 393018.73
[36m[2023-06-25 12:42:39,139][129146] itr=1283, itrs=2000, Progress: 64.15%
[36m[2023-06-25 12:42:50,757][129146] train() took 11.59 seconds to complete
[36m[2023-06-25 12:42:50,758][129146] FPS: 331214.17
[36m[2023-06-25 12:42:55,532][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:42:55,532][129146] Reward + Measures: [[2878.22865358    0.24233504    0.32681909    0.29979846    0.05658859]]
[37m[1m[2023-06-25 12:42:55,532][129146] Max Reward on eval: 2878.228653584407
[37m[1m[2023-06-25 12:42:55,533][129146] Min Reward on eval: 2878.228653584407
[37m[1m[2023-06-25 12:42:55,533][129146] Mean Reward across all agents: 2878.228653584407
[37m[1m[2023-06-25 12:42:55,533][129146] Average Trajectory Length: 986.4636666666667
[36m[2023-06-25 12:43:00,942][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:43:00,942][129146] Reward + Measures: [[2628.45376556    0.2696        0.34399998    0.34130001    0.07410001]
[37m[1m [2496.11302031    0.25310001    0.33519998    0.30850002    0.0671    ]
[37m[1m [ 273.71910096    0.39515576    0.43548366    0.2920866     0.28175363]
[37m[1m ...
[37m[1m [2318.66781755    0.20650001    0.31670001    0.26880002    0.0542    ]
[37m[1m [1333.77296232    0.2583465     0.34109339    0.3117376     0.075363  ]
[37m[1m [2287.65140479    0.22020678    0.32096368    0.30416903    0.0551477 ]]
[37m[1m[2023-06-25 12:43:00,942][129146] Max Reward on eval: 3124.8634254534727
[37m[1m[2023-06-25 12:43:00,943][129146] Min Reward on eval: -76.50012899942521
[37m[1m[2023-06-25 12:43:00,943][129146] Mean Reward across all agents: 2035.6483853024156
[37m[1m[2023-06-25 12:43:00,943][129146] Average Trajectory Length: 938.699
[36m[2023-06-25 12:43:00,946][129146] mean_value=-351.67354275213904, max_value=765.6236654045879
[37m[1m[2023-06-25 12:43:00,949][129146] New mean coefficients: [[ 1.7194446  -0.5441468  -0.13691877  0.33725503  0.04664656]]
[37m[1m[2023-06-25 12:43:00,950][129146] Moving the mean solution point...
[36m[2023-06-25 12:43:10,657][129146] train() took 9.71 seconds to complete
[36m[2023-06-25 12:43:10,657][129146] FPS: 395638.96
[36m[2023-06-25 12:43:10,659][129146] itr=1284, itrs=2000, Progress: 64.20%
[36m[2023-06-25 12:43:22,252][129146] train() took 11.57 seconds to complete
[36m[2023-06-25 12:43:22,252][129146] FPS: 331981.02
[36m[2023-06-25 12:43:26,992][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:43:26,993][129146] Reward + Measures: [[3022.66491798    0.24678686    0.32284555    0.29759753    0.05772883]]
[37m[1m[2023-06-25 12:43:26,993][129146] Max Reward on eval: 3022.6649179823053
[37m[1m[2023-06-25 12:43:26,993][129146] Min Reward on eval: 3022.6649179823053
[37m[1m[2023-06-25 12:43:26,994][129146] Mean Reward across all agents: 3022.6649179823053
[37m[1m[2023-06-25 12:43:26,994][129146] Average Trajectory Length: 986.906
[36m[2023-06-25 12:43:32,436][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:43:32,437][129146] Reward + Measures: [[2797.59271149    0.24815543    0.30653706    0.31353334    0.07252884]
[37m[1m [2610.01937465    0.23403768    0.30296522    0.28911304    0.05600724]
[37m[1m [2335.61622575    0.21639846    0.28406933    0.26380023    0.06931867]
[37m[1m ...
[37m[1m [2223.15849384    0.22086266    0.2795589     0.28352737    0.08282369]
[37m[1m [2621.34028824    0.24063154    0.31238261    0.31508398    0.08174641]
[37m[1m [2945.51522263    0.2361        0.3075        0.30130002    0.0682    ]]
[37m[1m[2023-06-25 12:43:32,437][129146] Max Reward on eval: 3287.976037896215
[37m[1m[2023-06-25 12:43:32,438][129146] Min Reward on eval: 1505.7814339210804
[37m[1m[2023-06-25 12:43:32,438][129146] Mean Reward across all agents: 2748.2926413611226
[37m[1m[2023-06-25 12:43:32,438][129146] Average Trajectory Length: 972.3506666666666
[36m[2023-06-25 12:43:32,442][129146] mean_value=9.581513031794167, max_value=718.9127619176206
[37m[1m[2023-06-25 12:43:32,445][129146] New mean coefficients: [[ 1.2295399  -0.11367401 -0.15061954 -0.25799522  0.10984937]]
[37m[1m[2023-06-25 12:43:32,446][129146] Moving the mean solution point...
[36m[2023-06-25 12:43:42,251][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 12:43:42,251][129146] FPS: 391708.66
[36m[2023-06-25 12:43:42,254][129146] itr=1285, itrs=2000, Progress: 64.25%
[36m[2023-06-25 12:43:53,926][129146] train() took 11.64 seconds to complete
[36m[2023-06-25 12:43:53,927][129146] FPS: 329756.95
[36m[2023-06-25 12:43:58,657][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:43:58,658][129146] Reward + Measures: [[3181.06271349    0.23517208    0.31844017    0.28116757    0.04350752]]
[37m[1m[2023-06-25 12:43:58,658][129146] Max Reward on eval: 3181.062713492529
[37m[1m[2023-06-25 12:43:58,658][129146] Min Reward on eval: 3181.062713492529
[37m[1m[2023-06-25 12:43:58,659][129146] Mean Reward across all agents: 3181.062713492529
[37m[1m[2023-06-25 12:43:58,659][129146] Average Trajectory Length: 990.293
[36m[2023-06-25 12:44:04,058][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:44:04,064][129146] Reward + Measures: [[3365.61747229    0.24430001    0.32730004    0.28690001    0.0353    ]
[37m[1m [3332.47324191    0.2476        0.32339999    0.27770001    0.0411    ]
[37m[1m [3103.09971186    0.24023977    0.31935874    0.27956551    0.04943996]
[37m[1m ...
[37m[1m [3213.40890538    0.23629999    0.3096        0.28870001    0.0509    ]
[37m[1m [3025.41031895    0.23119998    0.315         0.3008        0.0667    ]
[37m[1m [2966.20673818    0.2254        0.30040002    0.26210004    0.0525    ]]
[37m[1m[2023-06-25 12:44:04,065][129146] Max Reward on eval: 3423.614297774248
[37m[1m[2023-06-25 12:44:04,065][129146] Min Reward on eval: 1673.3457280903472
[37m[1m[2023-06-25 12:44:04,066][129146] Mean Reward across all agents: 2981.5547291831845
[37m[1m[2023-06-25 12:44:04,066][129146] Average Trajectory Length: 981.8273333333333
[36m[2023-06-25 12:44:04,074][129146] mean_value=135.8900833355693, max_value=572.1968409652491
[37m[1m[2023-06-25 12:44:04,078][129146] New mean coefficients: [[1.0270492  0.1725797  0.01683046 0.01476571 0.1972885 ]]
[37m[1m[2023-06-25 12:44:04,080][129146] Moving the mean solution point...
[36m[2023-06-25 12:44:13,827][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 12:44:13,827][129146] FPS: 394044.71
[36m[2023-06-25 12:44:13,830][129146] itr=1286, itrs=2000, Progress: 64.30%
[36m[2023-06-25 12:44:25,390][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 12:44:25,390][129146] FPS: 332898.99
[36m[2023-06-25 12:44:30,168][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:44:30,169][129146] Reward + Measures: [[3325.55361761    0.23804362    0.31194386    0.27701193    0.04169524]]
[37m[1m[2023-06-25 12:44:30,169][129146] Max Reward on eval: 3325.55361760873
[37m[1m[2023-06-25 12:44:30,169][129146] Min Reward on eval: 3325.55361760873
[37m[1m[2023-06-25 12:44:30,169][129146] Mean Reward across all agents: 3325.55361760873
[37m[1m[2023-06-25 12:44:30,169][129146] Average Trajectory Length: 991.9193333333333
[36m[2023-06-25 12:44:35,779][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:44:35,779][129146] Reward + Measures: [[3316.47409138    0.23400001    0.31870005    0.28430003    0.0434    ]
[37m[1m [2917.39588908    0.21780001    0.30860001    0.26190004    0.0539    ]
[37m[1m [3422.10895259    0.24150001    0.33160001    0.26430002    0.0292    ]
[37m[1m ...
[37m[1m [3252.78620318    0.2448        0.31720001    0.2904        0.0429    ]
[37m[1m [3479.41698896    0.2436        0.30810001    0.27800003    0.0409    ]
[37m[1m [2953.95805712    0.22719999    0.30290002    0.2694        0.0386    ]]
[37m[1m[2023-06-25 12:44:35,779][129146] Max Reward on eval: 3557.948795271851
[37m[1m[2023-06-25 12:44:35,780][129146] Min Reward on eval: 2684.5454539146535
[37m[1m[2023-06-25 12:44:35,780][129146] Mean Reward across all agents: 3287.3539320647064
[37m[1m[2023-06-25 12:44:35,780][129146] Average Trajectory Length: 993.103
[36m[2023-06-25 12:44:35,785][129146] mean_value=148.77239144935308, max_value=517.7284771722734
[37m[1m[2023-06-25 12:44:35,787][129146] New mean coefficients: [[ 1.3132329   0.20071486 -0.21401753  0.6650455   0.17248408]]
[37m[1m[2023-06-25 12:44:35,788][129146] Moving the mean solution point...
[36m[2023-06-25 12:44:45,584][129146] train() took 9.79 seconds to complete
[36m[2023-06-25 12:44:45,585][129146] FPS: 392065.10
[36m[2023-06-25 12:44:45,587][129146] itr=1287, itrs=2000, Progress: 64.35%
[36m[2023-06-25 12:44:57,239][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 12:44:57,239][129146] FPS: 330272.96
[36m[2023-06-25 12:45:02,144][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:45:02,145][129146] Reward + Measures: [[3485.18031228    0.24114396    0.30484197    0.27837661    0.04335412]]
[37m[1m[2023-06-25 12:45:02,145][129146] Max Reward on eval: 3485.1803122821543
[37m[1m[2023-06-25 12:45:02,145][129146] Min Reward on eval: 3485.1803122821543
[37m[1m[2023-06-25 12:45:02,145][129146] Mean Reward across all agents: 3485.1803122821543
[37m[1m[2023-06-25 12:45:02,145][129146] Average Trajectory Length: 994.3283333333333
[36m[2023-06-25 12:45:07,744][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:45:07,745][129146] Reward + Measures: [[3111.40918064    0.22260001    0.27759999    0.26100001    0.0426    ]
[37m[1m [3371.76300825    0.24150001    0.30109999    0.2931        0.0573    ]
[37m[1m [3182.95578438    0.23705311    0.30955312    0.27853337    0.04467654]
[37m[1m ...
[37m[1m [3209.02648415    0.2326        0.29460001    0.27140003    0.045     ]
[37m[1m [2623.37937587    0.23928872    0.31851408    0.31991127    0.0603155 ]
[37m[1m [3401.77194075    0.24089999    0.30610001    0.2983        0.0553    ]]
[37m[1m[2023-06-25 12:45:07,745][129146] Max Reward on eval: 3620.994579678588
[37m[1m[2023-06-25 12:45:07,746][129146] Min Reward on eval: 921.6985774340923
[37m[1m[2023-06-25 12:45:07,746][129146] Mean Reward across all agents: 3015.4827530799807
[37m[1m[2023-06-25 12:45:07,746][129146] Average Trajectory Length: 974.0793333333334
[36m[2023-06-25 12:45:07,750][129146] mean_value=-181.97744758098196, max_value=738.8766374444576
[37m[1m[2023-06-25 12:45:07,753][129146] New mean coefficients: [[ 1.2380179   0.1899481  -0.01595113  1.0716858   0.18939178]]
[37m[1m[2023-06-25 12:45:07,754][129146] Moving the mean solution point...
[36m[2023-06-25 12:45:17,554][129146] train() took 9.80 seconds to complete
[36m[2023-06-25 12:45:17,554][129146] FPS: 391909.83
[36m[2023-06-25 12:45:17,556][129146] itr=1288, itrs=2000, Progress: 64.40%
[36m[2023-06-25 12:45:29,249][129146] train() took 11.67 seconds to complete
[36m[2023-06-25 12:45:29,250][129146] FPS: 329078.57
[36m[2023-06-25 12:45:34,028][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:45:34,028][129146] Reward + Measures: [[3601.92031537    0.24185787    0.30312851    0.27567884    0.03829961]]
[37m[1m[2023-06-25 12:45:34,028][129146] Max Reward on eval: 3601.920315374916
[37m[1m[2023-06-25 12:45:34,028][129146] Min Reward on eval: 3601.920315374916
[37m[1m[2023-06-25 12:45:34,029][129146] Mean Reward across all agents: 3601.920315374916
[37m[1m[2023-06-25 12:45:34,029][129146] Average Trajectory Length: 994.5456666666666
[36m[2023-06-25 12:45:39,360][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:45:39,366][129146] Reward + Measures: [[3244.761381      0.23459999    0.33750004    0.26989999    0.0551    ]
[37m[1m [2867.00153983    0.23223773    0.29387259    0.26591864    0.04239289]
[37m[1m [3679.88039425    0.24349999    0.3064        0.2739        0.0391    ]
[37m[1m ...
[37m[1m [3585.86298198    0.24650002    0.30629998    0.28439999    0.047     ]
[37m[1m [3216.17430228    0.23099999    0.29140002    0.27349997    0.0438    ]
[37m[1m [3531.58725584    0.23940001    0.31740001    0.27820003    0.0399    ]]
[37m[1m[2023-06-25 12:45:39,366][129146] Max Reward on eval: 3789.4168003154455
[37m[1m[2023-06-25 12:45:39,367][129146] Min Reward on eval: 2187.204296498222
[37m[1m[2023-06-25 12:45:39,367][129146] Mean Reward across all agents: 3417.831399379874
[37m[1m[2023-06-25 12:45:39,367][129146] Average Trajectory Length: 989.8613333333333
[36m[2023-06-25 12:45:39,371][129146] mean_value=-36.75320072866028, max_value=319.1582244471424
[37m[1m[2023-06-25 12:45:39,374][129146] New mean coefficients: [[ 0.64801836  0.3323558  -0.07467368  0.08985269  0.22503659]]
[37m[1m[2023-06-25 12:45:39,375][129146] Moving the mean solution point...
[36m[2023-06-25 12:45:49,122][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 12:45:49,123][129146] FPS: 394010.16
[36m[2023-06-25 12:45:49,125][129146] itr=1289, itrs=2000, Progress: 64.45%
[36m[2023-06-25 12:46:00,769][129146] train() took 11.62 seconds to complete
[36m[2023-06-25 12:46:00,769][129146] FPS: 330461.17
[36m[2023-06-25 12:46:05,493][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:46:05,494][129146] Reward + Measures: [[3739.15112927    0.24654476    0.30663592    0.27544522    0.03464385]]
[37m[1m[2023-06-25 12:46:05,494][129146] Max Reward on eval: 3739.1511292714404
[37m[1m[2023-06-25 12:46:05,494][129146] Min Reward on eval: 3739.1511292714404
[37m[1m[2023-06-25 12:46:05,494][129146] Mean Reward across all agents: 3739.1511292714404
[37m[1m[2023-06-25 12:46:05,495][129146] Average Trajectory Length: 995.6659999999999
[36m[2023-06-25 12:46:10,944][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:46:10,944][129146] Reward + Measures: [[3687.89736913    0.24099998    0.3075        0.2775        0.037     ]
[37m[1m [3592.8716053     0.2441        0.31680003    0.28660002    0.0351    ]
[37m[1m [3419.17649654    0.22690001    0.2956        0.2739        0.0464    ]
[37m[1m ...
[37m[1m [3308.58956079    0.23194829    0.30209428    0.26770577    0.03753793]
[37m[1m [3468.75891576    0.23980001    0.30629998    0.28639999    0.0574    ]
[37m[1m [3706.55538337    0.24700001    0.32160002    0.27040002    0.0338    ]]
[37m[1m[2023-06-25 12:46:10,945][129146] Max Reward on eval: 3852.0495894822757
[37m[1m[2023-06-25 12:46:10,945][129146] Min Reward on eval: 2536.2980786071394
[37m[1m[2023-06-25 12:46:10,945][129146] Mean Reward across all agents: 3615.0165262410574
[37m[1m[2023-06-25 12:46:10,945][129146] Average Trajectory Length: 995.6496666666667
[36m[2023-06-25 12:46:10,949][129146] mean_value=23.349409902538184, max_value=314.5959313179251
[37m[1m[2023-06-25 12:46:10,952][129146] New mean coefficients: [[ 0.54899603  0.17166737 -0.12161595 -0.18347806  0.11142047]]
[37m[1m[2023-06-25 12:46:10,953][129146] Moving the mean solution point...
[36m[2023-06-25 12:46:20,559][129146] train() took 9.60 seconds to complete
[36m[2023-06-25 12:46:20,559][129146] FPS: 399824.16
[36m[2023-06-25 12:46:20,561][129146] itr=1290, itrs=2000, Progress: 64.50%
[37m[1m[2023-06-25 12:46:28,299][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001270
[36m[2023-06-25 12:46:39,919][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 12:46:39,920][129146] FPS: 336226.39
[36m[2023-06-25 12:46:44,725][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:46:44,725][129146] Reward + Measures: [[3834.06333317    0.2504147     0.30672905    0.28259805    0.03562198]]
[37m[1m[2023-06-25 12:46:44,725][129146] Max Reward on eval: 3834.0633331729887
[37m[1m[2023-06-25 12:46:44,726][129146] Min Reward on eval: 3834.0633331729887
[37m[1m[2023-06-25 12:46:44,726][129146] Mean Reward across all agents: 3834.0633331729887
[37m[1m[2023-06-25 12:46:44,726][129146] Average Trajectory Length: 997.3683333333333
[36m[2023-06-25 12:46:50,225][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:46:50,225][129146] Reward + Measures: [[1610.18568964    0.22466938    0.25518981    0.28563261    0.07702246]
[37m[1m [3324.75571637    0.24100001    0.31070003    0.33070001    0.05830001]
[37m[1m [2836.7489418     0.24879999    0.34010002    0.29789999    0.0686    ]
[37m[1m ...
[37m[1m [3461.67613679    0.25530002    0.32800004    0.27379999    0.0344    ]
[37m[1m [3562.27596996    0.25030002    0.31          0.31110001    0.0506    ]
[37m[1m [1176.67292712    0.19340955    0.21706262    0.20883571    0.06278062]]
[37m[1m[2023-06-25 12:46:50,226][129146] Max Reward on eval: 3981.6019978758877
[37m[1m[2023-06-25 12:46:50,226][129146] Min Reward on eval: -120.41424812861223
[37m[1m[2023-06-25 12:46:50,226][129146] Mean Reward across all agents: 2707.167798611809
[37m[1m[2023-06-25 12:46:50,227][129146] Average Trajectory Length: 948.6123333333333
[36m[2023-06-25 12:46:50,230][129146] mean_value=-511.6928945688112, max_value=1256.3973982897949
[37m[1m[2023-06-25 12:46:50,232][129146] New mean coefficients: [[ 0.68445593  0.20134702 -0.16658114  0.21352696  0.09531856]]
[37m[1m[2023-06-25 12:46:50,233][129146] Moving the mean solution point...
[36m[2023-06-25 12:47:00,170][129146] train() took 9.93 seconds to complete
[36m[2023-06-25 12:47:00,170][129146] FPS: 386517.88
[36m[2023-06-25 12:47:00,172][129146] itr=1291, itrs=2000, Progress: 64.55%
[36m[2023-06-25 12:47:11,750][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 12:47:11,750][129146] FPS: 332381.57
[36m[2023-06-25 12:47:16,613][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:47:16,613][129146] Reward + Measures: [[3932.01171656    0.2526184     0.30677095    0.27985802    0.03092112]]
[37m[1m[2023-06-25 12:47:16,614][129146] Max Reward on eval: 3932.011716562138
[37m[1m[2023-06-25 12:47:16,614][129146] Min Reward on eval: 3932.011716562138
[37m[1m[2023-06-25 12:47:16,614][129146] Mean Reward across all agents: 3932.011716562138
[37m[1m[2023-06-25 12:47:16,614][129146] Average Trajectory Length: 997.4196666666667
[36m[2023-06-25 12:47:22,119][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:47:22,120][129146] Reward + Measures: [[4032.85639425    0.25670001    0.31299999    0.27720001    0.0295    ]
[37m[1m [3873.79524482    0.24779999    0.29850003    0.27959999    0.0329    ]
[37m[1m [3852.76296036    0.2471        0.3062        0.27739999    0.0346    ]
[37m[1m ...
[37m[1m [3943.69248587    0.25130001    0.3163        0.2748        0.0281    ]
[37m[1m [3858.42774896    0.25050002    0.30580002    0.28210002    0.033     ]
[37m[1m [3705.34493266    0.25049999    0.30770001    0.2861        0.0367    ]]
[37m[1m[2023-06-25 12:47:22,120][129146] Max Reward on eval: 4087.57910085693
[37m[1m[2023-06-25 12:47:22,120][129146] Min Reward on eval: 2820.9190136345046
[37m[1m[2023-06-25 12:47:22,120][129146] Mean Reward across all agents: 3886.6085035446285
[37m[1m[2023-06-25 12:47:22,121][129146] Average Trajectory Length: 997.9593333333333
[36m[2023-06-25 12:47:22,125][129146] mean_value=118.10622309538276, max_value=607.2184329125894
[37m[1m[2023-06-25 12:47:22,128][129146] New mean coefficients: [[ 0.4679111   0.23406087 -0.16557589 -0.6789871   0.25648493]]
[37m[1m[2023-06-25 12:47:22,129][129146] Moving the mean solution point...
[36m[2023-06-25 12:47:31,893][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 12:47:31,893][129146] FPS: 393349.31
[36m[2023-06-25 12:47:31,896][129146] itr=1292, itrs=2000, Progress: 64.60%
[36m[2023-06-25 12:47:43,302][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 12:47:43,302][129146] FPS: 337375.81
[36m[2023-06-25 12:47:48,009][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:47:48,010][129146] Reward + Measures: [[4009.21046401    0.25367606    0.30371627    0.27860659    0.03251953]]
[37m[1m[2023-06-25 12:47:48,010][129146] Max Reward on eval: 4009.2104640070943
[37m[1m[2023-06-25 12:47:48,010][129146] Min Reward on eval: 4009.2104640070943
[37m[1m[2023-06-25 12:47:48,010][129146] Mean Reward across all agents: 4009.2104640070943
[37m[1m[2023-06-25 12:47:48,011][129146] Average Trajectory Length: 997.3206666666666
[36m[2023-06-25 12:47:53,511][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:47:53,511][129146] Reward + Measures: [[3904.99597915    0.2606        0.31829998    0.2845        0.0275    ]
[37m[1m [2685.6301036     0.22855115    0.2966176     0.30552727    0.0819629 ]
[37m[1m [4007.10725847    0.25039998    0.30320001    0.2802        0.035     ]
[37m[1m ...
[37m[1m [3649.68853185    0.2604        0.32429999    0.2784        0.0231    ]
[37m[1m [3317.70622848    0.23450001    0.2951        0.2969        0.0566    ]
[37m[1m [1595.32184786    0.29139999    0.31680003    0.26999998    0.0718    ]]
[37m[1m[2023-06-25 12:47:53,512][129146] Max Reward on eval: 4118.348540823534
[37m[1m[2023-06-25 12:47:53,512][129146] Min Reward on eval: 1595.3218478582567
[37m[1m[2023-06-25 12:47:53,512][129146] Mean Reward across all agents: 3532.8332912816463
[37m[1m[2023-06-25 12:47:53,512][129146] Average Trajectory Length: 996.507
[36m[2023-06-25 12:47:53,515][129146] mean_value=-326.7888604881408, max_value=742.2483610313334
[37m[1m[2023-06-25 12:47:53,517][129146] New mean coefficients: [[ 0.28801808  0.08031538 -0.19555758 -0.36970598  0.20234108]]
[37m[1m[2023-06-25 12:47:53,518][129146] Moving the mean solution point...
[36m[2023-06-25 12:48:03,356][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 12:48:03,356][129146] FPS: 390428.35
[36m[2023-06-25 12:48:03,358][129146] itr=1293, itrs=2000, Progress: 64.65%
[36m[2023-06-25 12:48:14,853][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 12:48:14,854][129146] FPS: 334853.98
[36m[2023-06-25 12:48:19,674][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:48:19,675][129146] Reward + Measures: [[4101.20455984    0.25481483    0.30372396    0.2752912     0.03148976]]
[37m[1m[2023-06-25 12:48:19,675][129146] Max Reward on eval: 4101.204559842518
[37m[1m[2023-06-25 12:48:19,675][129146] Min Reward on eval: 4101.204559842518
[37m[1m[2023-06-25 12:48:19,675][129146] Mean Reward across all agents: 4101.204559842518
[37m[1m[2023-06-25 12:48:19,675][129146] Average Trajectory Length: 997.725
[36m[2023-06-25 12:48:25,087][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:48:25,088][129146] Reward + Measures: [[1667.49247058    0.3143293     0.30461547    0.36685416    0.16227348]
[37m[1m [3066.65146002    0.28593847    0.33348975    0.33182308    0.08472564]
[37m[1m [3953.30973435    0.25830001    0.308         0.2845        0.0334    ]
[37m[1m ...
[37m[1m [4073.91747861    0.25209999    0.3105        0.28210002    0.0313    ]
[37m[1m [3236.65536082    0.28050002    0.3012        0.33070001    0.08670001]
[37m[1m [2282.62559621    0.22276719    0.25402936    0.31628403    0.08983853]]
[37m[1m[2023-06-25 12:48:25,088][129146] Max Reward on eval: 4163.237136849295
[37m[1m[2023-06-25 12:48:25,088][129146] Min Reward on eval: 615.5264603858843
[37m[1m[2023-06-25 12:48:25,089][129146] Mean Reward across all agents: 3230.237233450162
[37m[1m[2023-06-25 12:48:25,089][129146] Average Trajectory Length: 985.5433333333333
[36m[2023-06-25 12:48:25,092][129146] mean_value=-409.37823880208373, max_value=795.7410911209911
[37m[1m[2023-06-25 12:48:25,094][129146] New mean coefficients: [[ 0.09392054  0.10029268 -0.16941354 -0.71647835  0.19458643]]
[37m[1m[2023-06-25 12:48:25,095][129146] Moving the mean solution point...
[36m[2023-06-25 12:48:34,715][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 12:48:34,715][129146] FPS: 399249.49
[36m[2023-06-25 12:48:34,718][129146] itr=1294, itrs=2000, Progress: 64.70%
[36m[2023-06-25 12:48:46,140][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 12:48:46,141][129146] FPS: 336893.38
[36m[2023-06-25 12:48:50,938][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:48:50,938][129146] Reward + Measures: [[4157.49985405    0.25395626    0.30167782    0.27273372    0.02818294]]
[37m[1m[2023-06-25 12:48:50,939][129146] Max Reward on eval: 4157.4998540530305
[37m[1m[2023-06-25 12:48:50,939][129146] Min Reward on eval: 4157.4998540530305
[37m[1m[2023-06-25 12:48:50,939][129146] Mean Reward across all agents: 4157.4998540530305
[37m[1m[2023-06-25 12:48:50,939][129146] Average Trajectory Length: 998.9266666666666
[36m[2023-06-25 12:48:56,382][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:48:56,382][129146] Reward + Measures: [[2478.8232889     0.2490861     0.30178809    0.2620531     0.04093722]
[37m[1m [3190.12305021    0.26910001    0.30620003    0.28740001    0.0329    ]
[37m[1m [3532.57099857    0.25842556    0.29967675    0.27108836    0.01812558]
[37m[1m ...
[37m[1m [3226.3289593     0.2494        0.29389998    0.26220003    0.029     ]
[37m[1m [3173.19070606    0.26037315    0.31436321    0.27544579    0.03166766]
[37m[1m [3990.39124582    0.26149997    0.30110002    0.27900001    0.0326    ]]
[37m[1m[2023-06-25 12:48:56,383][129146] Max Reward on eval: 4256.2087857757
[37m[1m[2023-06-25 12:48:56,383][129146] Min Reward on eval: 1958.3473984717943
[37m[1m[2023-06-25 12:48:56,383][129146] Mean Reward across all agents: 3641.2571925626153
[37m[1m[2023-06-25 12:48:56,383][129146] Average Trajectory Length: 991.2463333333333
[36m[2023-06-25 12:48:56,386][129146] mean_value=-385.4542770441118, max_value=198.84157418763334
[37m[1m[2023-06-25 12:48:56,388][129146] New mean coefficients: [[ 0.12188019  0.14065978 -0.15593246  0.04286194  0.17342296]]
[37m[1m[2023-06-25 12:48:56,390][129146] Moving the mean solution point...
[36m[2023-06-25 12:49:06,139][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 12:49:06,139][129146] FPS: 393963.14
[36m[2023-06-25 12:49:06,141][129146] itr=1295, itrs=2000, Progress: 64.75%
[36m[2023-06-25 12:49:17,605][129146] train() took 11.44 seconds to complete
[36m[2023-06-25 12:49:17,605][129146] FPS: 335673.28
[36m[2023-06-25 12:49:22,349][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:49:22,349][129146] Reward + Measures: [[4246.52040495    0.2591067     0.29981631    0.28241405    0.03534728]]
[37m[1m[2023-06-25 12:49:22,350][129146] Max Reward on eval: 4246.520404950451
[37m[1m[2023-06-25 12:49:22,350][129146] Min Reward on eval: 4246.520404950451
[37m[1m[2023-06-25 12:49:22,350][129146] Mean Reward across all agents: 4246.520404950451
[37m[1m[2023-06-25 12:49:22,350][129146] Average Trajectory Length: 998.414
[36m[2023-06-25 12:49:27,787][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:49:27,788][129146] Reward + Measures: [[3919.45574079    0.25600001    0.30450001    0.2897        0.0587    ]
[37m[1m [4098.37240157    0.25069997    0.29370001    0.29389998    0.0396    ]
[37m[1m [4299.88747148    0.25170001    0.3091        0.2854        0.0304    ]
[37m[1m ...
[37m[1m [4248.70417805    0.25780001    0.29840001    0.28640002    0.0326    ]
[37m[1m [4163.36636815    0.25749999    0.30229998    0.29390001    0.0455    ]
[37m[1m [4239.55262748    0.2467        0.3062        0.28690001    0.025     ]]
[37m[1m[2023-06-25 12:49:27,788][129146] Max Reward on eval: 4396.444987607747
[37m[1m[2023-06-25 12:49:27,788][129146] Min Reward on eval: 2913.1765799197833
[37m[1m[2023-06-25 12:49:27,788][129146] Mean Reward across all agents: 4046.6126529416365
[37m[1m[2023-06-25 12:49:27,789][129146] Average Trajectory Length: 996.843
[36m[2023-06-25 12:49:27,793][129146] mean_value=0.38949138685577056, max_value=426.275454190225
[37m[1m[2023-06-25 12:49:27,796][129146] New mean coefficients: [[-0.13022994  0.17129733 -0.20547229 -0.2175254   0.07855439]]
[37m[1m[2023-06-25 12:49:27,797][129146] Moving the mean solution point...
[36m[2023-06-25 12:49:37,422][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 12:49:37,422][129146] FPS: 399035.69
[36m[2023-06-25 12:49:37,424][129146] itr=1296, itrs=2000, Progress: 64.80%
[36m[2023-06-25 12:49:48,847][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 12:49:48,848][129146] FPS: 336938.97
[36m[2023-06-25 12:49:53,527][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:49:53,527][129146] Reward + Measures: [[4183.88067806    0.25781187    0.29699409    0.28180775    0.03849483]]
[37m[1m[2023-06-25 12:49:53,527][129146] Max Reward on eval: 4183.880678057459
[37m[1m[2023-06-25 12:49:53,527][129146] Min Reward on eval: 4183.880678057459
[37m[1m[2023-06-25 12:49:53,528][129146] Mean Reward across all agents: 4183.880678057459
[37m[1m[2023-06-25 12:49:53,528][129146] Average Trajectory Length: 996.8736666666666
[36m[2023-06-25 12:49:59,055][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:49:59,055][129146] Reward + Measures: [[4116.13322439    0.25869998    0.30669999    0.28330001    0.0312    ]
[37m[1m [4004.69491442    0.25830001    0.30490002    0.29050002    0.0305    ]
[37m[1m [4011.81013094    0.2516        0.2872        0.29340002    0.0496    ]
[37m[1m ...
[37m[1m [4173.20056942    0.26010001    0.3026        0.27590001    0.0368    ]
[37m[1m [4110.33330811    0.26190001    0.30340001    0.28640002    0.0321    ]
[37m[1m [4126.92522023    0.25560004    0.31620002    0.27939996    0.0308    ]]
[37m[1m[2023-06-25 12:49:59,055][129146] Max Reward on eval: 4334.325839029392
[37m[1m[2023-06-25 12:49:59,056][129146] Min Reward on eval: 3461.559858193621
[37m[1m[2023-06-25 12:49:59,056][129146] Mean Reward across all agents: 4122.580317432779
[37m[1m[2023-06-25 12:49:59,056][129146] Average Trajectory Length: 997.3756666666667
[36m[2023-06-25 12:49:59,058][129146] mean_value=-103.54271211589509, max_value=147.00238044033586
[37m[1m[2023-06-25 12:49:59,061][129146] New mean coefficients: [[-0.38195825  0.4180368  -0.11932481 -0.25024664  0.12773636]]
[37m[1m[2023-06-25 12:49:59,062][129146] Moving the mean solution point...
[36m[2023-06-25 12:50:08,906][129146] train() took 9.84 seconds to complete
[36m[2023-06-25 12:50:08,906][129146] FPS: 390168.42
[36m[2023-06-25 12:50:08,908][129146] itr=1297, itrs=2000, Progress: 64.85%
[36m[2023-06-25 12:50:20,513][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 12:50:20,513][129146] FPS: 331699.74
[36m[2023-06-25 12:50:25,278][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:50:25,279][129146] Reward + Measures: [[4052.9170973     0.25814939    0.30048454    0.28960696    0.04473421]]
[37m[1m[2023-06-25 12:50:25,279][129146] Max Reward on eval: 4052.9170973034265
[37m[1m[2023-06-25 12:50:25,279][129146] Min Reward on eval: 4052.9170973034265
[37m[1m[2023-06-25 12:50:25,279][129146] Mean Reward across all agents: 4052.9170973034265
[37m[1m[2023-06-25 12:50:25,280][129146] Average Trajectory Length: 998.6216666666667
[36m[2023-06-25 12:50:30,650][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:50:30,651][129146] Reward + Measures: [[3975.99744702    0.26000002    0.29429999    0.27659997    0.0425    ]
[37m[1m [3938.04215303    0.26159999    0.3012        0.3046        0.0513    ]
[37m[1m [3780.04540075    0.26190001    0.28979999    0.30739999    0.0584    ]
[37m[1m ...
[37m[1m [3675.19137972    0.24439998    0.28140002    0.27059999    0.0416    ]
[37m[1m [4031.14774802    0.25579998    0.30149999    0.27970001    0.0397    ]
[37m[1m [4042.11663119    0.26440001    0.3062        0.2933        0.0436    ]]
[37m[1m[2023-06-25 12:50:30,651][129146] Max Reward on eval: 4175.243591465429
[37m[1m[2023-06-25 12:50:30,651][129146] Min Reward on eval: 3457.434551102249
[37m[1m[2023-06-25 12:50:30,651][129146] Mean Reward across all agents: 3969.6208790671517
[37m[1m[2023-06-25 12:50:30,652][129146] Average Trajectory Length: 996.4209999999999
[36m[2023-06-25 12:50:30,653][129146] mean_value=-201.25021344094418, max_value=151.02532807566968
[37m[1m[2023-06-25 12:50:30,656][129146] New mean coefficients: [[-0.16276453  0.2813977  -0.14809608  0.23214534  0.27455845]]
[37m[1m[2023-06-25 12:50:30,657][129146] Moving the mean solution point...
[36m[2023-06-25 12:50:40,398][129146] train() took 9.74 seconds to complete
[36m[2023-06-25 12:50:40,399][129146] FPS: 394266.94
[36m[2023-06-25 12:50:40,401][129146] itr=1298, itrs=2000, Progress: 64.90%
[36m[2023-06-25 12:50:51,849][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 12:50:51,849][129146] FPS: 336228.64
[36m[2023-06-25 12:50:56,594][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:50:56,595][129146] Reward + Measures: [[3844.39413333    0.25885978    0.29621941    0.30150825    0.05601448]]
[37m[1m[2023-06-25 12:50:56,595][129146] Max Reward on eval: 3844.39413332939
[37m[1m[2023-06-25 12:50:56,595][129146] Min Reward on eval: 3844.39413332939
[37m[1m[2023-06-25 12:50:56,595][129146] Mean Reward across all agents: 3844.39413332939
[37m[1m[2023-06-25 12:50:56,596][129146] Average Trajectory Length: 997.4486666666667
[36m[2023-06-25 12:51:02,079][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:51:02,080][129146] Reward + Measures: [[3579.95818615    0.24240001    0.2816        0.27779999    0.0524    ]
[37m[1m [3835.58032327    0.25680003    0.2976        0.3019        0.0596    ]
[37m[1m [3912.55949066    0.26350003    0.30430001    0.301         0.0514    ]
[37m[1m ...
[37m[1m [3914.04258017    0.25479999    0.29749998    0.29110003    0.0483    ]
[37m[1m [3956.30892612    0.26159999    0.29250002    0.29920003    0.0518    ]
[37m[1m [3821.65844063    0.26290002    0.30250001    0.30329999    0.0576    ]]
[37m[1m[2023-06-25 12:51:02,080][129146] Max Reward on eval: 4052.3932361032116
[37m[1m[2023-06-25 12:51:02,080][129146] Min Reward on eval: 3169.7701136947553
[37m[1m[2023-06-25 12:51:02,080][129146] Mean Reward across all agents: 3797.769026220785
[37m[1m[2023-06-25 12:51:02,080][129146] Average Trajectory Length: 996.9046666666667
[36m[2023-06-25 12:51:02,082][129146] mean_value=-272.8859256532012, max_value=20.19333258830011
[37m[1m[2023-06-25 12:51:02,084][129146] New mean coefficients: [[-0.44982737  0.33825654 -0.01765256 -0.3187187   0.28027236]]
[37m[1m[2023-06-25 12:51:02,085][129146] Moving the mean solution point...
[36m[2023-06-25 12:51:11,776][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 12:51:11,776][129146] FPS: 396314.37
[36m[2023-06-25 12:51:11,778][129146] itr=1299, itrs=2000, Progress: 64.95%
[36m[2023-06-25 12:51:23,213][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 12:51:23,213][129146] FPS: 336537.51
[36m[2023-06-25 12:51:27,992][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:51:27,993][129146] Reward + Measures: [[3643.40068569    0.25790203    0.29745132    0.30907467    0.06218075]]
[37m[1m[2023-06-25 12:51:27,993][129146] Max Reward on eval: 3643.4006856912865
[37m[1m[2023-06-25 12:51:27,993][129146] Min Reward on eval: 3643.4006856912865
[37m[1m[2023-06-25 12:51:27,994][129146] Mean Reward across all agents: 3643.4006856912865
[37m[1m[2023-06-25 12:51:27,994][129146] Average Trajectory Length: 994.881
[36m[2023-06-25 12:51:33,511][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:51:33,512][129146] Reward + Measures: [[3711.40527273    0.26050001    0.30210003    0.31060001    0.0668    ]
[37m[1m [3669.88826522    0.25970003    0.29860002    0.31720001    0.0684    ]
[37m[1m [3577.03059031    0.25760001    0.30560002    0.31579998    0.0691    ]
[37m[1m ...
[37m[1m [3720.47356201    0.2622        0.3003        0.3193        0.0571    ]
[37m[1m [3570.00110613    0.2572        0.30380002    0.32159999    0.0645    ]
[37m[1m [3711.93304334    0.2581        0.3026        0.31440002    0.0533    ]]
[37m[1m[2023-06-25 12:51:33,512][129146] Max Reward on eval: 3850.3023461206117
[37m[1m[2023-06-25 12:51:33,512][129146] Min Reward on eval: 3225.1174363733035
[37m[1m[2023-06-25 12:51:33,513][129146] Mean Reward across all agents: 3634.011020079598
[37m[1m[2023-06-25 12:51:33,513][129146] Average Trajectory Length: 997.8879999999999
[36m[2023-06-25 12:51:33,514][129146] mean_value=-411.86517912069013, max_value=-163.55075918701232
[36m[2023-06-25 12:51:33,516][129146] XNES is restarting with a new solution whose measures are [0.20150001 0.2854     0.41870004 0.3872    ] and objective is 122.68158577361609
[36m[2023-06-25 12:51:33,517][129146] Emitter restarted. Changing the mean agent...
[37m[1m[2023-06-25 12:51:33,520][129146] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]]
[37m[1m[2023-06-25 12:51:33,521][129146] Moving the mean solution point...
[36m[2023-06-25 12:51:43,334][129146] train() took 9.81 seconds to complete
[36m[2023-06-25 12:51:43,334][129146] FPS: 391363.08
[36m[2023-06-25 12:51:43,337][129146] itr=1300, itrs=2000, Progress: 65.00%
[37m[1m[2023-06-25 12:51:51,319][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001280
[36m[2023-06-25 12:52:03,145][129146] train() took 11.61 seconds to complete
[36m[2023-06-25 12:52:03,145][129146] FPS: 330633.29
[36m[2023-06-25 12:52:07,896][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:52:07,896][129146] Reward + Measures: [[89.25222602  0.20181046  0.27245048  0.40596801  0.36884513]]
[37m[1m[2023-06-25 12:52:07,896][129146] Max Reward on eval: 89.25222601870539
[37m[1m[2023-06-25 12:52:07,896][129146] Min Reward on eval: 89.25222601870539
[37m[1m[2023-06-25 12:52:07,897][129146] Mean Reward across all agents: 89.25222601870539
[37m[1m[2023-06-25 12:52:07,897][129146] Average Trajectory Length: 993.6903333333333
[36m[2023-06-25 12:52:13,251][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:52:13,251][129146] Reward + Measures: [[-39.84140683   0.19509999   0.26040003   0.39490002   0.38200003]
[37m[1m [308.02665143   0.24069999   0.25830001   0.42120001   0.3335    ]
[37m[1m [336.61843717   0.20109999   0.28650001   0.38789999   0.30060002]
[37m[1m ...
[37m[1m [ 83.83227844   0.19750001   0.23709999   0.39280003   0.34380001]
[37m[1m [267.11562167   0.26350001   0.3389       0.41389999   0.3091    ]
[37m[1m [-64.87509842   0.2079       0.28879997   0.42090002   0.35460001]]
[37m[1m[2023-06-25 12:52:13,252][129146] Max Reward on eval: 513.1507782433182
[37m[1m[2023-06-25 12:52:13,252][129146] Min Reward on eval: -232.41251793211558
[37m[1m[2023-06-25 12:52:13,252][129146] Mean Reward across all agents: 115.11940672800648
[37m[1m[2023-06-25 12:52:13,252][129146] Average Trajectory Length: 992.7173333333333
[36m[2023-06-25 12:52:13,257][129146] mean_value=90.23169855806432, max_value=759.4020300143084
[37m[1m[2023-06-25 12:52:13,260][129146] New mean coefficients: [[ 1.4654332  -0.45104128  0.06581193 -1.1244738   0.72498584]]
[37m[1m[2023-06-25 12:52:13,261][129146] Moving the mean solution point...
[36m[2023-06-25 12:52:22,827][129146] train() took 9.56 seconds to complete
[36m[2023-06-25 12:52:22,827][129146] FPS: 401478.26
[36m[2023-06-25 12:52:22,830][129146] itr=1301, itrs=2000, Progress: 65.05%
[36m[2023-06-25 12:52:34,354][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 12:52:34,354][129146] FPS: 334012.92
[36m[2023-06-25 12:52:39,166][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:52:39,166][129146] Reward + Measures: [[181.40566876   0.19663574   0.27677765   0.40802827   0.36452588]]
[37m[1m[2023-06-25 12:52:39,166][129146] Max Reward on eval: 181.4056687619644
[37m[1m[2023-06-25 12:52:39,167][129146] Min Reward on eval: 181.4056687619644
[37m[1m[2023-06-25 12:52:39,167][129146] Mean Reward across all agents: 181.4056687619644
[37m[1m[2023-06-25 12:52:39,167][129146] Average Trajectory Length: 995.8963333333332
[36m[2023-06-25 12:52:44,584][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:52:44,590][129146] Reward + Measures: [[233.79793596   0.205        0.29730001   0.4488       0.42120001]
[37m[1m [ 57.16727782   0.22189999   0.26910001   0.3865       0.34010002]
[37m[1m [336.71587256   0.20070003   0.33260003   0.44129997   0.42230001]
[37m[1m ...
[37m[1m [432.04506246   0.18540001   0.2701       0.49270001   0.42899999]
[37m[1m [282.6280534    0.36409998   0.3265       0.4725       0.31420001]
[37m[1m [466.02748589   0.17919999   0.30510002   0.45580003   0.39830002]]
[37m[1m[2023-06-25 12:52:44,590][129146] Max Reward on eval: 508.259251672402
[37m[1m[2023-06-25 12:52:44,591][129146] Min Reward on eval: -118.20560801210813
[37m[1m[2023-06-25 12:52:44,591][129146] Mean Reward across all agents: 202.6475182579348
[37m[1m[2023-06-25 12:52:44,591][129146] Average Trajectory Length: 996.0833333333333
[36m[2023-06-25 12:52:44,596][129146] mean_value=27.58060667599847, max_value=916.8692230410641
[37m[1m[2023-06-25 12:52:44,599][129146] New mean coefficients: [[ 3.817831  -1.6503832  1.68783   -1.1704597  1.660213 ]]
[37m[1m[2023-06-25 12:52:44,600][129146] Moving the mean solution point...
[36m[2023-06-25 12:52:54,359][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 12:52:54,360][129146] FPS: 393509.25
[36m[2023-06-25 12:52:54,362][129146] itr=1302, itrs=2000, Progress: 65.10%
[36m[2023-06-25 12:53:05,924][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 12:53:05,924][129146] FPS: 332826.75
[36m[2023-06-25 12:53:10,634][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:53:10,634][129146] Reward + Measures: [[320.37267463   0.18337423   0.28230709   0.40329513   0.35246944]]
[37m[1m[2023-06-25 12:53:10,634][129146] Max Reward on eval: 320.37267463095776
[37m[1m[2023-06-25 12:53:10,635][129146] Min Reward on eval: 320.37267463095776
[37m[1m[2023-06-25 12:53:10,635][129146] Mean Reward across all agents: 320.37267463095776
[37m[1m[2023-06-25 12:53:10,635][129146] Average Trajectory Length: 997.5103333333333
[36m[2023-06-25 12:53:16,035][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:53:16,036][129146] Reward + Measures: [[512.12757506   0.22210002   0.2904       0.4585       0.3272    ]
[37m[1m [431.31362744   0.20369999   0.27720001   0.42150003   0.33629999]
[37m[1m [294.89453981   0.2067       0.26280004   0.42379999   0.3585    ]
[37m[1m ...
[37m[1m [577.48969597   0.259        0.33630002   0.41540003   0.2942    ]
[37m[1m [404.15107749   0.2253       0.3179       0.38830003   0.29630002]
[37m[1m [386.9726921    0.20630001   0.32799998   0.45769998   0.38330004]]
[37m[1m[2023-06-25 12:53:16,036][129146] Max Reward on eval: 686.0512773522117
[37m[1m[2023-06-25 12:53:16,036][129146] Min Reward on eval: -2.4255588131869446
[37m[1m[2023-06-25 12:53:16,036][129146] Mean Reward across all agents: 441.30701227084626
[37m[1m[2023-06-25 12:53:16,036][129146] Average Trajectory Length: 997.6193333333333
[36m[2023-06-25 12:53:16,041][129146] mean_value=-105.2674358495248, max_value=643.3718877265054
[37m[1m[2023-06-25 12:53:16,044][129146] New mean coefficients: [[ 5.1268625 -1.3251464  2.5189784 -1.5042633  2.175874 ]]
[37m[1m[2023-06-25 12:53:16,045][129146] Moving the mean solution point...
[36m[2023-06-25 12:53:25,667][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 12:53:25,667][129146] FPS: 399169.34
[36m[2023-06-25 12:53:25,669][129146] itr=1303, itrs=2000, Progress: 65.15%
[36m[2023-06-25 12:53:37,120][129146] train() took 11.42 seconds to complete
[36m[2023-06-25 12:53:37,120][129146] FPS: 336153.52
[36m[2023-06-25 12:53:41,848][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:53:41,853][129146] Reward + Measures: [[475.47583569   0.18281795   0.296909     0.40053424   0.33004904]]
[37m[1m[2023-06-25 12:53:41,853][129146] Max Reward on eval: 475.47583569185497
[37m[1m[2023-06-25 12:53:41,854][129146] Min Reward on eval: 475.47583569185497
[37m[1m[2023-06-25 12:53:41,854][129146] Mean Reward across all agents: 475.47583569185497
[37m[1m[2023-06-25 12:53:41,854][129146] Average Trajectory Length: 996.6126666666667
[36m[2023-06-25 12:53:47,289][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:53:47,290][129146] Reward + Measures: [[585.88161369   0.20420001   0.32339999   0.42249998   0.3125    ]
[37m[1m [590.02171237   0.1724       0.29700002   0.38680002   0.31240001]
[37m[1m [581.28785081   0.15980001   0.3096       0.36050001   0.32680002]
[37m[1m ...
[37m[1m [456.47513242   0.17480001   0.28590003   0.3617       0.2877    ]
[37m[1m [498.70572862   0.15280001   0.30249998   0.40410003   0.39410001]
[37m[1m [588.17746462   0.20750001   0.35640001   0.41580001   0.32300001]]
[37m[1m[2023-06-25 12:53:47,290][129146] Max Reward on eval: 638.3584018818103
[37m[1m[2023-06-25 12:53:47,290][129146] Min Reward on eval: 230.8499896639958
[37m[1m[2023-06-25 12:53:47,290][129146] Mean Reward across all agents: 477.0086539094763
[37m[1m[2023-06-25 12:53:47,291][129146] Average Trajectory Length: 996.384
[36m[2023-06-25 12:53:47,295][129146] mean_value=-142.24066693986893, max_value=949.1232339930923
[37m[1m[2023-06-25 12:53:47,297][129146] New mean coefficients: [[ 5.298824  -2.3855033  2.578277  -1.6520051  2.7420008]]
[37m[1m[2023-06-25 12:53:47,298][129146] Moving the mean solution point...
[36m[2023-06-25 12:53:57,000][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 12:53:57,001][129146] FPS: 395857.62
[36m[2023-06-25 12:53:57,003][129146] itr=1304, itrs=2000, Progress: 65.20%
[36m[2023-06-25 12:54:08,676][129146] train() took 11.65 seconds to complete
[36m[2023-06-25 12:54:08,676][129146] FPS: 329642.32
[36m[2023-06-25 12:54:13,563][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:54:13,564][129146] Reward + Measures: [[591.43430808   0.17856522   0.31149101   0.39142561   0.31557575]]
[37m[1m[2023-06-25 12:54:13,564][129146] Max Reward on eval: 591.4343080785945
[37m[1m[2023-06-25 12:54:13,564][129146] Min Reward on eval: 591.4343080785945
[37m[1m[2023-06-25 12:54:13,564][129146] Mean Reward across all agents: 591.4343080785945
[37m[1m[2023-06-25 12:54:13,565][129146] Average Trajectory Length: 998.9116666666666
[36m[2023-06-25 12:54:19,102][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:54:19,102][129146] Reward + Measures: [[643.05916001   0.19760001   0.36739999   0.42529997   0.36920005]
[37m[1m [616.49051405   0.20720001   0.38800001   0.41120005   0.34979999]
[37m[1m [590.51985016   0.17340001   0.46599999   0.37860003   0.41880003]
[37m[1m ...
[37m[1m [643.07059973   0.17529999   0.4632       0.43560001   0.43220001]
[37m[1m [417.45598455   0.1585       0.4131       0.37439999   0.40169999]
[37m[1m [564.0698318    0.1753       0.41750002   0.42080003   0.42730004]]
[37m[1m[2023-06-25 12:54:19,103][129146] Max Reward on eval: 805.0298422294261
[37m[1m[2023-06-25 12:54:19,103][129146] Min Reward on eval: 349.5302765556902
[37m[1m[2023-06-25 12:54:19,103][129146] Mean Reward across all agents: 590.18470426045
[37m[1m[2023-06-25 12:54:19,103][129146] Average Trajectory Length: 999.6343333333333
[36m[2023-06-25 12:54:19,108][129146] mean_value=-59.4549857132406, max_value=1256.859762218323
[37m[1m[2023-06-25 12:54:19,110][129146] New mean coefficients: [[ 4.248884  -3.601719   3.7667346 -1.5643489  2.306969 ]]
[37m[1m[2023-06-25 12:54:19,111][129146] Moving the mean solution point...
[36m[2023-06-25 12:54:29,020][129146] train() took 9.91 seconds to complete
[36m[2023-06-25 12:54:29,020][129146] FPS: 387623.88
[36m[2023-06-25 12:54:29,022][129146] itr=1305, itrs=2000, Progress: 65.25%
[36m[2023-06-25 12:54:40,630][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 12:54:40,631][129146] FPS: 331484.57
[36m[2023-06-25 12:54:45,560][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:54:45,560][129146] Reward + Measures: [[691.46757272   0.17758369   0.33324781   0.39573604   0.31592521]]
[37m[1m[2023-06-25 12:54:45,561][129146] Max Reward on eval: 691.4675727212467
[37m[1m[2023-06-25 12:54:45,561][129146] Min Reward on eval: 691.4675727212467
[37m[1m[2023-06-25 12:54:45,561][129146] Mean Reward across all agents: 691.4675727212467
[37m[1m[2023-06-25 12:54:45,561][129146] Average Trajectory Length: 998.6483333333333
[36m[2023-06-25 12:54:50,948][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:54:50,948][129146] Reward + Measures: [[473.12214114   0.08880001   0.50279999   0.44840002   0.47760001]
[37m[1m [509.72480242   0.12800001   0.32430002   0.35750002   0.30870003]
[37m[1m [566.17782536   0.12810001   0.5399       0.49190003   0.50240004]
[37m[1m ...
[37m[1m [642.74940988   0.14146154   0.37878463   0.43207693   0.40752307]
[37m[1m [514.04770318   0.1585       0.55790001   0.30539998   0.4533    ]
[37m[1m [584.579097     0.12840001   0.33430001   0.34310001   0.3003    ]]
[37m[1m[2023-06-25 12:54:50,949][129146] Max Reward on eval: 840.0977041651146
[37m[1m[2023-06-25 12:54:50,949][129146] Min Reward on eval: 335.3653268453898
[37m[1m[2023-06-25 12:54:50,949][129146] Mean Reward across all agents: 613.8027275755161
[37m[1m[2023-06-25 12:54:50,949][129146] Average Trajectory Length: 999.5383333333333
[36m[2023-06-25 12:54:50,954][129146] mean_value=116.96820565484478, max_value=1256.3979270750774
[37m[1m[2023-06-25 12:54:50,957][129146] New mean coefficients: [[ 3.9481952  -4.75786     3.163537   -0.75064033  1.943785  ]]
[37m[1m[2023-06-25 12:54:50,958][129146] Moving the mean solution point...
[36m[2023-06-25 12:55:00,866][129146] train() took 9.91 seconds to complete
[36m[2023-06-25 12:55:00,867][129146] FPS: 387621.94
[36m[2023-06-25 12:55:00,869][129146] itr=1306, itrs=2000, Progress: 65.30%
[36m[2023-06-25 12:55:12,418][129146] train() took 11.52 seconds to complete
[36m[2023-06-25 12:55:12,418][129146] FPS: 333204.69
[36m[2023-06-25 12:55:17,277][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:55:17,278][129146] Reward + Measures: [[773.34521637   0.17941776   0.34843057   0.4031446    0.32416782]]
[37m[1m[2023-06-25 12:55:17,278][129146] Max Reward on eval: 773.345216370526
[37m[1m[2023-06-25 12:55:17,278][129146] Min Reward on eval: 773.345216370526
[37m[1m[2023-06-25 12:55:17,279][129146] Mean Reward across all agents: 773.345216370526
[37m[1m[2023-06-25 12:55:17,279][129146] Average Trajectory Length: 998.5076666666666
[36m[2023-06-25 12:55:22,723][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:55:22,724][129146] Reward + Measures: [[512.44472774   0.11259999   0.54279995   0.45860001   0.46490002]
[37m[1m [655.03620964   0.178        0.33670002   0.40159997   0.359     ]
[37m[1m [792.3712112    0.15480001   0.36719999   0.4526       0.40939999]
[37m[1m ...
[37m[1m [726.48026165   0.1092       0.48880002   0.45700002   0.41879997]
[37m[1m [680.04515791   0.17660001   0.33720002   0.43809995   0.36030003]
[37m[1m [595.17371576   0.07340001   0.51200002   0.4691       0.49040005]]
[37m[1m[2023-06-25 12:55:22,724][129146] Max Reward on eval: 905.7476444353233
[37m[1m[2023-06-25 12:55:22,724][129146] Min Reward on eval: 310.3215466922353
[37m[1m[2023-06-25 12:55:22,724][129146] Mean Reward across all agents: 660.5416031734886
[37m[1m[2023-06-25 12:55:22,725][129146] Average Trajectory Length: 999.8046666666667
[36m[2023-06-25 12:55:22,731][129146] mean_value=165.88945031175365, max_value=1273.4544503331185
[37m[1m[2023-06-25 12:55:22,733][129146] New mean coefficients: [[ 4.8228946 -3.4984386  5.1336684 -0.8132936  3.0804443]]
[37m[1m[2023-06-25 12:55:22,734][129146] Moving the mean solution point...
[36m[2023-06-25 12:55:32,414][129146] train() took 9.68 seconds to complete
[36m[2023-06-25 12:55:32,414][129146] FPS: 396791.84
[36m[2023-06-25 12:55:32,416][129146] itr=1307, itrs=2000, Progress: 65.35%
[36m[2023-06-25 12:55:43,852][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 12:55:43,852][129146] FPS: 336499.79
[36m[2023-06-25 12:55:48,636][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:55:48,636][129146] Reward + Measures: [[848.90191011   0.17879157   0.3602716    0.40614721   0.32619253]]
[37m[1m[2023-06-25 12:55:48,637][129146] Max Reward on eval: 848.9019101145228
[37m[1m[2023-06-25 12:55:48,637][129146] Min Reward on eval: 848.9019101145228
[37m[1m[2023-06-25 12:55:48,637][129146] Mean Reward across all agents: 848.9019101145228
[37m[1m[2023-06-25 12:55:48,637][129146] Average Trajectory Length: 999.8689999999999
[36m[2023-06-25 12:55:54,122][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:55:54,122][129146] Reward + Measures: [[634.77917259   0.0771       0.40970001   0.3901       0.37240002]
[37m[1m [639.61177975   0.09119999   0.40880004   0.40850002   0.34769997]
[37m[1m [629.14820376   0.0625       0.38949999   0.4276       0.39700001]
[37m[1m ...
[37m[1m [601.53947305   0.0772       0.39799997   0.38699996   0.38470003]
[37m[1m [806.0350342    0.14570002   0.40459999   0.42560002   0.3283    ]
[37m[1m [564.58885975   0.057        0.45240003   0.43509999   0.4127    ]]
[37m[1m[2023-06-25 12:55:54,122][129146] Max Reward on eval: 946.6222399092046
[37m[1m[2023-06-25 12:55:54,123][129146] Min Reward on eval: 348.6125802749302
[37m[1m[2023-06-25 12:55:54,123][129146] Mean Reward across all agents: 701.707627369023
[37m[1m[2023-06-25 12:55:54,123][129146] Average Trajectory Length: 999.1866666666666
[36m[2023-06-25 12:55:54,129][129146] mean_value=114.00577013498888, max_value=1231.2403546153569
[37m[1m[2023-06-25 12:55:54,132][129146] New mean coefficients: [[ 3.812004  -4.725299   5.915784  -1.7763555  2.729979 ]]
[37m[1m[2023-06-25 12:55:54,133][129146] Moving the mean solution point...
[36m[2023-06-25 12:56:03,963][129146] train() took 9.83 seconds to complete
[36m[2023-06-25 12:56:03,963][129146] FPS: 390703.28
[36m[2023-06-25 12:56:03,965][129146] itr=1308, itrs=2000, Progress: 65.40%
[36m[2023-06-25 12:56:15,395][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 12:56:15,395][129146] FPS: 336686.57
[36m[2023-06-25 12:56:20,211][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:56:20,212][129146] Reward + Measures: [[908.19856759   0.173034     0.38440168   0.41357535   0.341212  ]]
[37m[1m[2023-06-25 12:56:20,212][129146] Max Reward on eval: 908.1985675887819
[37m[1m[2023-06-25 12:56:20,212][129146] Min Reward on eval: 908.1985675887819
[37m[1m[2023-06-25 12:56:20,212][129146] Mean Reward across all agents: 908.1985675887819
[37m[1m[2023-06-25 12:56:20,213][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:56:25,645][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:56:25,650][129146] Reward + Measures: [[548.77134456   0.041        0.55190003   0.46620002   0.565     ]
[37m[1m [675.69100767   0.14039999   0.47909999   0.46259999   0.48720002]
[37m[1m [549.22143014   0.048        0.5614       0.47890002   0.59799999]
[37m[1m ...
[37m[1m [754.33548067   0.0842       0.53280002   0.45240003   0.49659997]
[37m[1m [386.67281536   0.0235       0.55320001   0.48070002   0.59820002]
[37m[1m [282.09794966   0.023        0.65400004   0.58269995   0.67180002]]
[37m[1m[2023-06-25 12:56:25,650][129146] Max Reward on eval: 983.1357567260973
[37m[1m[2023-06-25 12:56:25,651][129146] Min Reward on eval: 209.109943627975
[37m[1m[2023-06-25 12:56:25,651][129146] Mean Reward across all agents: 588.9651444572422
[37m[1m[2023-06-25 12:56:25,651][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:56:25,658][129146] mean_value=342.7386839206055, max_value=958.512384244247
[37m[1m[2023-06-25 12:56:25,660][129146] New mean coefficients: [[ 5.024195  -6.588599   7.1301947 -1.9584653  3.8291168]]
[37m[1m[2023-06-25 12:56:25,661][129146] Moving the mean solution point...
[36m[2023-06-25 12:56:35,316][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 12:56:35,316][129146] FPS: 397808.87
[36m[2023-06-25 12:56:35,318][129146] itr=1309, itrs=2000, Progress: 65.45%
[36m[2023-06-25 12:56:46,840][129146] train() took 11.50 seconds to complete
[36m[2023-06-25 12:56:46,840][129146] FPS: 333997.40
[36m[2023-06-25 12:56:51,663][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:56:51,663][129146] Reward + Measures: [[964.74686256   0.16483642   0.41701221   0.42124569   0.36051416]]
[37m[1m[2023-06-25 12:56:51,663][129146] Max Reward on eval: 964.7468625582711
[37m[1m[2023-06-25 12:56:51,664][129146] Min Reward on eval: 964.7468625582711
[37m[1m[2023-06-25 12:56:51,664][129146] Mean Reward across all agents: 964.7468625582711
[37m[1m[2023-06-25 12:56:51,664][129146] Average Trajectory Length: 999.7956666666666
[36m[2023-06-25 12:56:57,159][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:56:57,159][129146] Reward + Measures: [[455.87387688   0.1505       0.43590003   0.45159999   0.55540001]
[37m[1m [506.38996715   0.1715       0.414        0.44479999   0.52270001]
[37m[1m [456.4549655    0.19310001   0.41009998   0.43170005   0.52160001]
[37m[1m ...
[37m[1m [466.28359624   0.14400001   0.42160001   0.47419998   0.53610003]
[37m[1m [447.88830911   0.101        0.41500002   0.41969997   0.52260005]
[37m[1m [607.72706043   0.10090001   0.47150001   0.4233       0.49220005]]
[37m[1m[2023-06-25 12:56:57,159][129146] Max Reward on eval: 853.6382158809108
[37m[1m[2023-06-25 12:56:57,160][129146] Min Reward on eval: 72.05345339133055
[37m[1m[2023-06-25 12:56:57,160][129146] Mean Reward across all agents: 503.30443079938743
[37m[1m[2023-06-25 12:56:57,160][129146] Average Trajectory Length: 999.2136666666667
[36m[2023-06-25 12:56:57,164][129146] mean_value=254.26519928642116, max_value=1098.4700101815338
[37m[1m[2023-06-25 12:56:57,167][129146] New mean coefficients: [[ 5.949853  -7.287062   7.0672474 -1.6687474  3.4962275]]
[37m[1m[2023-06-25 12:56:57,168][129146] Moving the mean solution point...
[36m[2023-06-25 12:57:06,900][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 12:57:06,900][129146] FPS: 394630.08
[36m[2023-06-25 12:57:06,903][129146] itr=1310, itrs=2000, Progress: 65.50%
[37m[1m[2023-06-25 12:57:14,789][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001290
[36m[2023-06-25 12:57:26,485][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 12:57:26,486][129146] FPS: 334261.83
[36m[2023-06-25 12:57:31,207][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:57:31,207][129146] Reward + Measures: [[1011.82942916    0.15647602    0.44696313    0.43200129    0.37277937]]
[37m[1m[2023-06-25 12:57:31,207][129146] Max Reward on eval: 1011.8294291606459
[37m[1m[2023-06-25 12:57:31,208][129146] Min Reward on eval: 1011.8294291606459
[37m[1m[2023-06-25 12:57:31,208][129146] Mean Reward across all agents: 1011.8294291606459
[37m[1m[2023-06-25 12:57:31,208][129146] Average Trajectory Length: 999.817
[36m[2023-06-25 12:57:36,605][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:57:36,605][129146] Reward + Measures: [[324.66220149   0.0562       0.58109999   0.37399998   0.6002    ]
[37m[1m [424.39071202   0.0506       0.667        0.42120001   0.64700001]
[37m[1m [357.77119741   0.0425       0.62         0.37630001   0.67110002]
[37m[1m ...
[37m[1m [132.98708693   0.11310001   0.46250001   0.37939999   0.4817    ]
[37m[1m [199.65813898   0.11659999   0.5147       0.47670004   0.55400002]
[37m[1m [677.18598199   0.08889999   0.56309998   0.40939999   0.4901    ]]
[37m[1m[2023-06-25 12:57:36,605][129146] Max Reward on eval: 717.995278486266
[37m[1m[2023-06-25 12:57:36,606][129146] Min Reward on eval: -34.90521306226147
[37m[1m[2023-06-25 12:57:36,606][129146] Mean Reward across all agents: 337.4585809264486
[37m[1m[2023-06-25 12:57:36,606][129146] Average Trajectory Length: 999.8833333333333
[36m[2023-06-25 12:57:36,611][129146] mean_value=203.44088015393112, max_value=1041.297306098236
[37m[1m[2023-06-25 12:57:36,614][129146] New mean coefficients: [[ 6.383771  -5.836165   5.127949  -2.0411744  3.6783154]]
[37m[1m[2023-06-25 12:57:36,615][129146] Moving the mean solution point...
[36m[2023-06-25 12:57:46,231][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 12:57:46,231][129146] FPS: 399394.97
[36m[2023-06-25 12:57:46,233][129146] itr=1311, itrs=2000, Progress: 65.55%
[36m[2023-06-25 12:57:57,645][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 12:57:57,646][129146] FPS: 337193.42
[36m[2023-06-25 12:58:02,411][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:58:02,411][129146] Reward + Measures: [[1078.6843691     0.14442366    0.47822028    0.43365762    0.38572362]]
[37m[1m[2023-06-25 12:58:02,412][129146] Max Reward on eval: 1078.6843690952758
[37m[1m[2023-06-25 12:58:02,412][129146] Min Reward on eval: 1078.6843690952758
[37m[1m[2023-06-25 12:58:02,412][129146] Mean Reward across all agents: 1078.6843690952758
[37m[1m[2023-06-25 12:58:02,412][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:58:07,798][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:58:07,798][129146] Reward + Measures: [[932.37600612   0.1249       0.4594       0.4244       0.3926    ]
[37m[1m [470.68368231   0.13429999   0.63819999   0.3585       0.58420002]
[37m[1m [638.39356181   0.0974       0.42179999   0.40159997   0.4303    ]
[37m[1m ...
[37m[1m [751.94691429   0.0697       0.61219996   0.43249997   0.51599997]
[37m[1m [742.22506105   0.1117       0.41190001   0.39300001   0.40790001]
[37m[1m [730.17233742   0.0915       0.55470002   0.42010003   0.4777    ]]
[37m[1m[2023-06-25 12:58:07,799][129146] Max Reward on eval: 1042.6858035474318
[37m[1m[2023-06-25 12:58:07,799][129146] Min Reward on eval: 278.5494570620591
[37m[1m[2023-06-25 12:58:07,799][129146] Mean Reward across all agents: 740.7100626228824
[37m[1m[2023-06-25 12:58:07,799][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:58:07,804][129146] mean_value=146.47501535251766, max_value=978.469839326819
[37m[1m[2023-06-25 12:58:07,807][129146] New mean coefficients: [[ 5.9125123  -5.547393    4.186408    0.10449147  2.895373  ]]
[37m[1m[2023-06-25 12:58:07,808][129146] Moving the mean solution point...
[36m[2023-06-25 12:58:17,422][129146] train() took 9.61 seconds to complete
[36m[2023-06-25 12:58:17,422][129146] FPS: 399491.33
[36m[2023-06-25 12:58:17,425][129146] itr=1312, itrs=2000, Progress: 65.60%
[36m[2023-06-25 12:58:28,842][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 12:58:28,843][129146] FPS: 337030.09
[36m[2023-06-25 12:58:33,663][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:58:33,664][129146] Reward + Measures: [[1153.16682018    0.1378243     0.49688941    0.44022331    0.38229468]]
[37m[1m[2023-06-25 12:58:33,664][129146] Max Reward on eval: 1153.166820184973
[37m[1m[2023-06-25 12:58:33,664][129146] Min Reward on eval: 1153.166820184973
[37m[1m[2023-06-25 12:58:33,664][129146] Mean Reward across all agents: 1153.166820184973
[37m[1m[2023-06-25 12:58:33,664][129146] Average Trajectory Length: 999.948
[36m[2023-06-25 12:58:39,071][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:58:39,072][129146] Reward + Measures: [[893.09031091   0.1132       0.51850003   0.43859997   0.42609999]
[37m[1m [908.15036584   0.0856       0.5916       0.46710005   0.46259999]
[37m[1m [676.47711192   0.0534       0.65820003   0.49520001   0.5467    ]
[37m[1m ...
[37m[1m [881.3468845    0.12779999   0.60330003   0.43920001   0.47839999]
[37m[1m [696.17663927   0.11420001   0.61079997   0.41220003   0.47890002]
[37m[1m [933.27304099   0.1804       0.47189999   0.42680001   0.41149998]]
[37m[1m[2023-06-25 12:58:39,072][129146] Max Reward on eval: 1144.0248908178182
[37m[1m[2023-06-25 12:58:39,072][129146] Min Reward on eval: 365.58690717647477
[37m[1m[2023-06-25 12:58:39,073][129146] Mean Reward across all agents: 780.7588827859661
[37m[1m[2023-06-25 12:58:39,073][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:58:39,079][129146] mean_value=279.37779296755895, max_value=1441.470941870101
[37m[1m[2023-06-25 12:58:39,082][129146] New mean coefficients: [[ 5.1327057  -4.9970946   3.8887203   1.5498787   0.82542324]]
[37m[1m[2023-06-25 12:58:39,083][129146] Moving the mean solution point...
[36m[2023-06-25 12:58:48,785][129146] train() took 9.70 seconds to complete
[36m[2023-06-25 12:58:48,785][129146] FPS: 395859.15
[36m[2023-06-25 12:58:48,788][129146] itr=1313, itrs=2000, Progress: 65.65%
[36m[2023-06-25 12:59:00,269][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 12:59:00,270][129146] FPS: 335146.22
[36m[2023-06-25 12:59:05,088][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:59:05,088][129146] Reward + Measures: [[1252.69491715    0.15049067    0.47812435    0.43966967    0.35623068]]
[37m[1m[2023-06-25 12:59:05,088][129146] Max Reward on eval: 1252.694917146711
[37m[1m[2023-06-25 12:59:05,089][129146] Min Reward on eval: 1252.694917146711
[37m[1m[2023-06-25 12:59:05,089][129146] Mean Reward across all agents: 1252.694917146711
[37m[1m[2023-06-25 12:59:05,089][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:59:10,545][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:59:10,546][129146] Reward + Measures: [[625.97839437   0.0462       0.69450003   0.64580005   0.63799995]
[37m[1m [523.60598278   0.0169       0.78820002   0.7209       0.6649    ]
[37m[1m [393.49640572   0.0321       0.72130007   0.70150006   0.71099997]
[37m[1m ...
[37m[1m [376.42958708   0.0109       0.8075       0.73900002   0.73250002]
[37m[1m [883.62650325   0.0602       0.66499996   0.59969997   0.49840003]
[37m[1m [534.43633679   0.0162       0.78310007   0.72500002   0.66930002]]
[37m[1m[2023-06-25 12:59:10,546][129146] Max Reward on eval: 1349.4448748328955
[37m[1m[2023-06-25 12:59:10,546][129146] Min Reward on eval: 296.5783962467511
[37m[1m[2023-06-25 12:59:10,546][129146] Mean Reward across all agents: 705.0906124444673
[37m[1m[2023-06-25 12:59:10,547][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:59:10,555][129146] mean_value=623.3535889023516, max_value=1633.846717218717
[37m[1m[2023-06-25 12:59:10,557][129146] New mean coefficients: [[ 5.2448487  -4.279741    2.5746007   2.1274576  -0.38287997]]
[37m[1m[2023-06-25 12:59:10,558][129146] Moving the mean solution point...
[36m[2023-06-25 12:59:20,281][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 12:59:20,281][129146] FPS: 395022.56
[36m[2023-06-25 12:59:20,283][129146] itr=1314, itrs=2000, Progress: 65.70%
[36m[2023-06-25 12:59:31,753][129146] train() took 11.45 seconds to complete
[36m[2023-06-25 12:59:31,753][129146] FPS: 335512.61
[36m[2023-06-25 12:59:36,456][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:59:36,456][129146] Reward + Measures: [[1341.18968375    0.158832      0.46456665    0.43868566    0.33259532]]
[37m[1m[2023-06-25 12:59:36,457][129146] Max Reward on eval: 1341.1896837482607
[37m[1m[2023-06-25 12:59:36,457][129146] Min Reward on eval: 1341.1896837482607
[37m[1m[2023-06-25 12:59:36,457][129146] Mean Reward across all agents: 1341.1896837482607
[37m[1m[2023-06-25 12:59:36,458][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:59:42,034][129146] Finished Evaluation Step
[37m[1m[2023-06-25 12:59:42,034][129146] Reward + Measures: [[ 613.29094598    0.03310001    0.72970003    0.51220006    0.65040004]
[37m[1m [ 735.50716711    0.048         0.64320004    0.51980001    0.55540007]
[37m[1m [ 709.21536673    0.0267        0.68879998    0.51389998    0.61689997]
[37m[1m ...
[37m[1m [1155.63529302    0.1435        0.45070001    0.47559997    0.37480003]
[37m[1m [1212.27023934    0.1362        0.53290004    0.46310002    0.3845    ]
[37m[1m [ 994.09693353    0.0711        0.62350005    0.47930002    0.4632    ]]
[37m[1m[2023-06-25 12:59:42,035][129146] Max Reward on eval: 1328.0246685419697
[37m[1m[2023-06-25 12:59:42,035][129146] Min Reward on eval: 434.6438741016085
[37m[1m[2023-06-25 12:59:42,035][129146] Mean Reward across all agents: 873.0776539056495
[37m[1m[2023-06-25 12:59:42,035][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 12:59:42,041][129146] mean_value=162.9274139303709, max_value=1148.9788774591957
[37m[1m[2023-06-25 12:59:42,044][129146] New mean coefficients: [[ 4.6446457 -3.9520187  1.1155181  2.450001  -0.4219707]]
[37m[1m[2023-06-25 12:59:42,045][129146] Moving the mean solution point...
[36m[2023-06-25 12:59:51,766][129146] train() took 9.72 seconds to complete
[36m[2023-06-25 12:59:51,767][129146] FPS: 395060.99
[36m[2023-06-25 12:59:51,769][129146] itr=1315, itrs=2000, Progress: 65.75%
[36m[2023-06-25 13:00:03,370][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 13:00:03,370][129146] FPS: 331734.71
[36m[2023-06-25 13:00:08,210][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:00:08,211][129146] Reward + Measures: [[1409.021         0.15580933    0.45902035    0.43958867    0.31947798]]
[37m[1m[2023-06-25 13:00:08,211][129146] Max Reward on eval: 1409.0210000013005
[37m[1m[2023-06-25 13:00:08,211][129146] Min Reward on eval: 1409.0210000013005
[37m[1m[2023-06-25 13:00:08,212][129146] Mean Reward across all agents: 1409.0210000013005
[37m[1m[2023-06-25 13:00:08,212][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:00:13,659][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:00:13,659][129146] Reward + Measures: [[1024.47397377    0.10570001    0.45369998    0.44350001    0.3495    ]
[37m[1m [1265.83693081    0.1222        0.51979995    0.45190001    0.39300004]
[37m[1m [1302.58259567    0.16419999    0.51400006    0.44940001    0.34779999]
[37m[1m ...
[37m[1m [ 482.47282002    0.0361        0.72640002    0.49890003    0.65130001]
[37m[1m [ 763.80973457    0.0582        0.62660003    0.4499        0.49990001]
[37m[1m [ 832.1670538     0.0796        0.58310002    0.48830006    0.52689999]]
[37m[1m[2023-06-25 13:00:13,659][129146] Max Reward on eval: 1417.6773657922283
[37m[1m[2023-06-25 13:00:13,660][129146] Min Reward on eval: 364.71137124076483
[37m[1m[2023-06-25 13:00:13,660][129146] Mean Reward across all agents: 1020.5064490948179
[37m[1m[2023-06-25 13:00:13,660][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:00:13,666][129146] mean_value=146.7293542988701, max_value=939.1091918784393
[37m[1m[2023-06-25 13:00:13,668][129146] New mean coefficients: [[ 4.006395   -2.8089316   0.00785017  2.269508   -0.11486474]]
[37m[1m[2023-06-25 13:00:13,669][129146] Moving the mean solution point...
[36m[2023-06-25 13:00:23,356][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 13:00:23,356][129146] FPS: 396489.04
[36m[2023-06-25 13:00:23,359][129146] itr=1316, itrs=2000, Progress: 65.80%
[36m[2023-06-25 13:00:35,017][129146] train() took 11.63 seconds to complete
[36m[2023-06-25 13:00:35,017][129146] FPS: 330103.59
[36m[2023-06-25 13:00:39,846][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:00:39,846][129146] Reward + Measures: [[1501.60231471    0.16606818    0.43946487    0.43880317    0.29597577]]
[37m[1m[2023-06-25 13:00:39,846][129146] Max Reward on eval: 1501.6023147140486
[37m[1m[2023-06-25 13:00:39,847][129146] Min Reward on eval: 1501.6023147140486
[37m[1m[2023-06-25 13:00:39,847][129146] Mean Reward across all agents: 1501.6023147140486
[37m[1m[2023-06-25 13:00:39,847][129146] Average Trajectory Length: 999.7456666666666
[36m[2023-06-25 13:00:45,229][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:00:45,230][129146] Reward + Measures: [[1416.52218667    0.20010002    0.41370001    0.44229999    0.28649998]
[37m[1m [1357.29605312    0.19579999    0.48719999    0.44820005    0.33540002]
[37m[1m [1207.9839635     0.1734        0.39880002    0.46240005    0.28640002]
[37m[1m ...
[37m[1m [1277.87076356    0.15179999    0.47480002    0.456         0.34210002]
[37m[1m [ 864.21139483    0.0854        0.57499999    0.48090002    0.46180001]
[37m[1m [1074.29281428    0.14130001    0.41660005    0.45390001    0.3229    ]]
[37m[1m[2023-06-25 13:00:45,230][129146] Max Reward on eval: 1516.5168047690881
[37m[1m[2023-06-25 13:00:45,230][129146] Min Reward on eval: 707.1682807322359
[37m[1m[2023-06-25 13:00:45,230][129146] Mean Reward across all agents: 1187.9334593998321
[37m[1m[2023-06-25 13:00:45,231][129146] Average Trajectory Length: 999.8003333333334
[36m[2023-06-25 13:00:45,234][129146] mean_value=59.21096442987491, max_value=1658.8443849572864
[37m[1m[2023-06-25 13:00:45,237][129146] New mean coefficients: [[ 3.8322976 -1.1536115 -1.5646279  2.4242468 -0.6674161]]
[37m[1m[2023-06-25 13:00:45,238][129146] Moving the mean solution point...
[36m[2023-06-25 13:00:54,800][129146] train() took 9.56 seconds to complete
[36m[2023-06-25 13:00:54,801][129146] FPS: 401636.33
[36m[2023-06-25 13:00:54,803][129146] itr=1317, itrs=2000, Progress: 65.85%
[36m[2023-06-25 13:01:06,295][129146] train() took 11.47 seconds to complete
[36m[2023-06-25 13:01:06,295][129146] FPS: 334885.08
[36m[2023-06-25 13:01:11,030][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:01:11,031][129146] Reward + Measures: [[1576.16275206    0.176293      0.43408397    0.43494168    0.28007299]]
[37m[1m[2023-06-25 13:01:11,031][129146] Max Reward on eval: 1576.1627520568115
[37m[1m[2023-06-25 13:01:11,031][129146] Min Reward on eval: 1576.1627520568115
[37m[1m[2023-06-25 13:01:11,031][129146] Mean Reward across all agents: 1576.1627520568115
[37m[1m[2023-06-25 13:01:11,032][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:01:16,387][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:01:16,388][129146] Reward + Measures: [[1517.35201112    0.19490002    0.45429999    0.43140003    0.2888    ]
[37m[1m [1471.79378622    0.2273        0.4492        0.44580004    0.2922    ]
[37m[1m [1230.90919314    0.21870001    0.48649999    0.46650001    0.3497    ]
[37m[1m ...
[37m[1m [1226.24474646    0.184         0.42300001    0.40739998    0.25390002]
[37m[1m [1221.08657723    0.117         0.56059998    0.47549996    0.43310004]
[37m[1m [1318.48417951    0.15640001    0.49789998    0.48000002    0.35600001]]
[37m[1m[2023-06-25 13:01:16,388][129146] Max Reward on eval: 1583.1484115714206
[37m[1m[2023-06-25 13:01:16,388][129146] Min Reward on eval: 772.3163415280171
[37m[1m[2023-06-25 13:01:16,388][129146] Mean Reward across all agents: 1278.7564717796847
[37m[1m[2023-06-25 13:01:16,389][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:01:16,392][129146] mean_value=-57.179497775800534, max_value=905.913217757666
[37m[1m[2023-06-25 13:01:16,395][129146] New mean coefficients: [[ 2.9815903   0.07073188 -2.8025146   2.6544619  -0.9815405 ]]
[37m[1m[2023-06-25 13:01:16,396][129146] Moving the mean solution point...
[36m[2023-06-25 13:01:26,069][129146] train() took 9.67 seconds to complete
[36m[2023-06-25 13:01:26,069][129146] FPS: 397028.63
[36m[2023-06-25 13:01:26,072][129146] itr=1318, itrs=2000, Progress: 65.90%
[36m[2023-06-25 13:01:37,522][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 13:01:37,523][129146] FPS: 336053.67
[36m[2023-06-25 13:01:42,317][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:01:42,317][129146] Reward + Measures: [[1629.2575653     0.178124      0.4193033     0.43514633    0.26997435]]
[37m[1m[2023-06-25 13:01:42,318][129146] Max Reward on eval: 1629.2575653020751
[37m[1m[2023-06-25 13:01:42,318][129146] Min Reward on eval: 1629.2575653020751
[37m[1m[2023-06-25 13:01:42,318][129146] Mean Reward across all agents: 1629.2575653020751
[37m[1m[2023-06-25 13:01:42,318][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:01:47,790][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:01:47,795][129146] Reward + Measures: [[1564.74423168    0.17450002    0.4928        0.46760002    0.30680001]
[37m[1m [1220.20025208    0.1409        0.49020001    0.46990004    0.27420002]
[37m[1m [ 999.93991356    0.14049999    0.47590002    0.4639        0.31040001]
[37m[1m ...
[37m[1m [1548.75566383    0.1591        0.44969997    0.45280001    0.26530001]
[37m[1m [1494.90530914    0.17750001    0.36770001    0.40430003    0.25130001]
[37m[1m [1231.6395115     0.17110001    0.43050003    0.4348        0.2588    ]]
[37m[1m[2023-06-25 13:01:47,796][129146] Max Reward on eval: 1658.036223398801
[37m[1m[2023-06-25 13:01:47,796][129146] Min Reward on eval: 589.8907111661276
[37m[1m[2023-06-25 13:01:47,796][129146] Mean Reward across all agents: 1347.2295302835091
[37m[1m[2023-06-25 13:01:47,797][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:01:47,801][129146] mean_value=-60.932880265057904, max_value=1945.7105729799193
[37m[1m[2023-06-25 13:01:47,804][129146] New mean coefficients: [[ 2.6801913  -0.713018   -1.4369923   2.8740718   0.57960457]]
[37m[1m[2023-06-25 13:01:47,805][129146] Moving the mean solution point...
[36m[2023-06-25 13:01:57,688][129146] train() took 9.88 seconds to complete
[36m[2023-06-25 13:01:57,688][129146] FPS: 388605.23
[36m[2023-06-25 13:01:57,690][129146] itr=1319, itrs=2000, Progress: 65.95%
[36m[2023-06-25 13:02:09,102][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 13:02:09,102][129146] FPS: 337229.47
[36m[2023-06-25 13:02:13,835][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:02:13,835][129146] Reward + Measures: [[1692.01835881    0.174859      0.41705331    0.44702002    0.26263633]]
[37m[1m[2023-06-25 13:02:13,836][129146] Max Reward on eval: 1692.0183588124414
[37m[1m[2023-06-25 13:02:13,836][129146] Min Reward on eval: 1692.0183588124414
[37m[1m[2023-06-25 13:02:13,836][129146] Mean Reward across all agents: 1692.0183588124414
[37m[1m[2023-06-25 13:02:13,836][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:02:19,547][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:02:19,548][129146] Reward + Measures: [[1442.87059682    0.1849        0.35929999    0.47849998    0.27400002]
[37m[1m [ 982.84805103    0.20720001    0.37500003    0.50940001    0.3321    ]
[37m[1m [1468.50364009    0.1247        0.49110004    0.43660003    0.3179    ]
[37m[1m ...
[37m[1m [ 437.94262259    0.27110001    0.32269999    0.51789999    0.3522    ]
[37m[1m [1361.51140064    0.14170001    0.51190001    0.50230002    0.33130002]
[37m[1m [1416.72220196    0.1587        0.40970001    0.45570001    0.30050001]]
[37m[1m[2023-06-25 13:02:19,548][129146] Max Reward on eval: 1697.4539700573891
[37m[1m[2023-06-25 13:02:19,548][129146] Min Reward on eval: 342.18841691333216
[37m[1m[2023-06-25 13:02:19,548][129146] Mean Reward across all agents: 1395.6148373339456
[37m[1m[2023-06-25 13:02:19,548][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:02:19,552][129146] mean_value=47.75843767898957, max_value=708.4711781419767
[37m[1m[2023-06-25 13:02:19,555][129146] New mean coefficients: [[ 2.1665912   0.55720395 -3.4712987   2.3906991  -0.03115302]]
[37m[1m[2023-06-25 13:02:19,556][129146] Moving the mean solution point...
[36m[2023-06-25 13:02:29,203][129146] train() took 9.65 seconds to complete
[36m[2023-06-25 13:02:29,203][129146] FPS: 398115.13
[36m[2023-06-25 13:02:29,206][129146] itr=1320, itrs=2000, Progress: 66.00%
[37m[1m[2023-06-25 13:02:36,786][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001300
[36m[2023-06-25 13:02:48,419][129146] train() took 11.43 seconds to complete
[36m[2023-06-25 13:02:48,419][129146] FPS: 335935.66
[36m[2023-06-25 13:02:53,221][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:02:53,221][129146] Reward + Measures: [[1731.2590241     0.18008834    0.40458101    0.45466134    0.25422901]]
[37m[1m[2023-06-25 13:02:53,222][129146] Max Reward on eval: 1731.2590241020625
[37m[1m[2023-06-25 13:02:53,222][129146] Min Reward on eval: 1731.2590241020625
[37m[1m[2023-06-25 13:02:53,222][129146] Mean Reward across all agents: 1731.2590241020625
[37m[1m[2023-06-25 13:02:53,222][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:02:58,690][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:02:58,690][129146] Reward + Measures: [[1298.40444756    0.21799998    0.42570001    0.50669998    0.32890001]
[37m[1m [ 996.75484541    0.2093        0.49950001    0.54590005    0.42560002]
[37m[1m [1365.4639614     0.1115        0.41799998    0.46809998    0.31680003]
[37m[1m ...
[37m[1m [1455.73591193    0.3163        0.43210003    0.43470001    0.21440001]
[37m[1m [1276.98140889    0.16940002    0.45940003    0.49380001    0.35929999]
[37m[1m [1519.55089811    0.14670001    0.4104        0.42939997    0.27200001]]
[37m[1m[2023-06-25 13:02:58,690][129146] Max Reward on eval: 1752.2173829992068
[37m[1m[2023-06-25 13:02:58,691][129146] Min Reward on eval: 523.0754578247085
[37m[1m[2023-06-25 13:02:58,691][129146] Mean Reward across all agents: 1427.3016525765825
[37m[1m[2023-06-25 13:02:58,691][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:02:58,696][129146] mean_value=66.44492936620688, max_value=1748.1068513639736
[37m[1m[2023-06-25 13:02:58,698][129146] New mean coefficients: [[ 1.3792186  1.6322439 -3.6602015  3.1953537 -0.1787611]]
[37m[1m[2023-06-25 13:02:58,699][129146] Moving the mean solution point...
[36m[2023-06-25 13:03:08,275][129146] train() took 9.57 seconds to complete
[36m[2023-06-25 13:03:08,275][129146] FPS: 401100.68
[36m[2023-06-25 13:03:08,277][129146] itr=1321, itrs=2000, Progress: 66.05%
[36m[2023-06-25 13:03:19,705][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 13:03:19,705][129146] FPS: 336722.04
[36m[2023-06-25 13:03:24,421][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:03:24,422][129146] Reward + Measures: [[1785.37733959    0.18442866    0.39294001    0.46319902    0.24622865]]
[37m[1m[2023-06-25 13:03:24,422][129146] Max Reward on eval: 1785.377339594509
[37m[1m[2023-06-25 13:03:24,422][129146] Min Reward on eval: 1785.377339594509
[37m[1m[2023-06-25 13:03:24,422][129146] Mean Reward across all agents: 1785.377339594509
[37m[1m[2023-06-25 13:03:24,423][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:03:30,036][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:03:30,037][129146] Reward + Measures: [[1494.8384234     0.12409999    0.45930001    0.44230005    0.30440003]
[37m[1m [1496.53300208    0.1737        0.35800001    0.4533        0.21870001]
[37m[1m [1373.79189417    0.17420001    0.40559998    0.48449999    0.28260002]
[37m[1m ...
[37m[1m [1098.02736541    0.27680001    0.33580002    0.49379998    0.24180003]
[37m[1m [1676.93865966    0.1885        0.38299999    0.46100003    0.25409999]
[37m[1m [1517.060183      0.18910001    0.42490003    0.49509999    0.27500001]]
[37m[1m[2023-06-25 13:03:30,037][129146] Max Reward on eval: 1824.1348183142486
[37m[1m[2023-06-25 13:03:30,037][129146] Min Reward on eval: 781.1860923780011
[37m[1m[2023-06-25 13:03:30,037][129146] Mean Reward across all agents: 1447.1002146007195
[37m[1m[2023-06-25 13:03:30,038][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:03:30,041][129146] mean_value=-32.673449745679356, max_value=1463.075436717994
[37m[1m[2023-06-25 13:03:30,044][129146] New mean coefficients: [[ 0.4834373  1.3568215 -2.2047238  3.2303274  0.6162423]]
[37m[1m[2023-06-25 13:03:30,045][129146] Moving the mean solution point...
[36m[2023-06-25 13:03:39,691][129146] train() took 9.64 seconds to complete
[36m[2023-06-25 13:03:39,691][129146] FPS: 398170.24
[36m[2023-06-25 13:03:39,693][129146] itr=1322, itrs=2000, Progress: 66.10%
[36m[2023-06-25 13:03:51,120][129146] train() took 11.40 seconds to complete
[36m[2023-06-25 13:03:51,121][129146] FPS: 336778.02
[36m[2023-06-25 13:03:55,823][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:03:55,824][129146] Reward + Measures: [[1793.46505325    0.18219167    0.38293201    0.48835766    0.24416402]]
[37m[1m[2023-06-25 13:03:55,824][129146] Max Reward on eval: 1793.4650532477187
[37m[1m[2023-06-25 13:03:55,824][129146] Min Reward on eval: 1793.4650532477187
[37m[1m[2023-06-25 13:03:55,824][129146] Mean Reward across all agents: 1793.4650532477187
[37m[1m[2023-06-25 13:03:55,825][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:04:01,260][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:04:01,261][129146] Reward + Measures: [[1198.65750379    0.17950001    0.38729998    0.52310002    0.32539999]
[37m[1m [1474.59393992    0.155         0.433         0.47130004    0.2771    ]
[37m[1m [1757.58253157    0.20150001    0.38410002    0.4605        0.23810001]
[37m[1m ...
[37m[1m [1304.46949132    0.31820002    0.36579999    0.47010002    0.2552    ]
[37m[1m [1534.2295473     0.21530001    0.354         0.49759999    0.2554    ]
[37m[1m [1114.589279      0.28470001    0.3522        0.49330002    0.28309998]]
[37m[1m[2023-06-25 13:04:01,261][129146] Max Reward on eval: 1831.9926967320148
[37m[1m[2023-06-25 13:04:01,261][129146] Min Reward on eval: 970.1470557775698
[37m[1m[2023-06-25 13:04:01,262][129146] Mean Reward across all agents: 1587.7050501054855
[37m[1m[2023-06-25 13:04:01,262][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:04:01,266][129146] mean_value=-3.602727777951162, max_value=1685.546762657402
[37m[1m[2023-06-25 13:04:01,269][129146] New mean coefficients: [[ 0.87492704  0.6668582  -0.9595425   2.5702324   1.0256722 ]]
[37m[1m[2023-06-25 13:04:01,270][129146] Moving the mean solution point...
[36m[2023-06-25 13:04:10,864][129146] train() took 9.59 seconds to complete
[36m[2023-06-25 13:04:10,864][129146] FPS: 400311.62
[36m[2023-06-25 13:04:10,866][129146] itr=1323, itrs=2000, Progress: 66.15%
[36m[2023-06-25 13:04:22,273][129146] train() took 11.38 seconds to complete
[36m[2023-06-25 13:04:22,274][129146] FPS: 337347.13
[36m[2023-06-25 13:04:27,051][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:04:27,052][129146] Reward + Measures: [[1827.70390547    0.18568298    0.37661266    0.499291      0.24418201]]
[37m[1m[2023-06-25 13:04:27,052][129146] Max Reward on eval: 1827.703905468656
[37m[1m[2023-06-25 13:04:27,052][129146] Min Reward on eval: 1827.703905468656
[37m[1m[2023-06-25 13:04:27,053][129146] Mean Reward across all agents: 1827.703905468656
[37m[1m[2023-06-25 13:04:27,053][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:04:32,530][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:04:32,535][129146] Reward + Measures: [[1712.76175969    0.1681        0.38550001    0.5126        0.27089998]
[37m[1m [1701.062551      0.19280003    0.36749998    0.47919998    0.26319999]
[37m[1m [1741.67793549    0.15530001    0.3944        0.50570005    0.2685    ]
[37m[1m ...
[37m[1m [1699.46621289    0.21510001    0.35730004    0.48049998    0.26009998]
[37m[1m [1789.77264548    0.19230001    0.36979997    0.48860002    0.25540003]
[37m[1m [1340.28909355    0.20120001    0.34370002    0.43849999    0.23740001]]
[37m[1m[2023-06-25 13:04:32,536][129146] Max Reward on eval: 1879.9447028453112
[37m[1m[2023-06-25 13:04:32,536][129146] Min Reward on eval: 919.3572504277574
[37m[1m[2023-06-25 13:04:32,536][129146] Mean Reward across all agents: 1631.0214082786956
[37m[1m[2023-06-25 13:04:32,536][129146] Average Trajectory Length: 999.7886666666666
[36m[2023-06-25 13:04:32,540][129146] mean_value=7.0847724985004845, max_value=1405.042339178165
[37m[1m[2023-06-25 13:04:32,543][129146] New mean coefficients: [[0.61930215 0.9183582  0.65815043 1.7919095  1.7904522 ]]
[37m[1m[2023-06-25 13:04:32,544][129146] Moving the mean solution point...
[36m[2023-06-25 13:04:42,162][129146] train() took 9.62 seconds to complete
[36m[2023-06-25 13:04:42,162][129146] FPS: 399343.67
[36m[2023-06-25 13:04:42,164][129146] itr=1324, itrs=2000, Progress: 66.20%
[36m[2023-06-25 13:04:53,740][129146] train() took 11.55 seconds to complete
[36m[2023-06-25 13:04:53,740][129146] FPS: 332466.23
[36m[2023-06-25 13:04:58,418][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:04:58,418][129146] Reward + Measures: [[1854.07891901    0.18425032    0.38268167    0.51328135    0.24976   ]]
[37m[1m[2023-06-25 13:04:58,418][129146] Max Reward on eval: 1854.0789190095375
[37m[1m[2023-06-25 13:04:58,419][129146] Min Reward on eval: 1854.0789190095375
[37m[1m[2023-06-25 13:04:58,419][129146] Mean Reward across all agents: 1854.0789190095375
[37m[1m[2023-06-25 13:04:58,419][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:05:03,856][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:05:03,857][129146] Reward + Measures: [[1774.3863676     0.18910001    0.3612        0.4928        0.24039999]
[37m[1m [ 965.50508162    0.0895        0.43090001    0.4454        0.27149999]
[37m[1m [1516.64285884    0.1313        0.4278        0.51990002    0.28530002]
[37m[1m ...
[37m[1m [1699.98885984    0.1399        0.38989997    0.4804        0.25419998]
[37m[1m [1712.70248593    0.1904        0.40240002    0.52990001    0.26970002]
[37m[1m [1767.43810408    0.18340001    0.41140005    0.53949994    0.27059999]]
[37m[1m[2023-06-25 13:05:03,857][129146] Max Reward on eval: 1889.1446654716972
[37m[1m[2023-06-25 13:05:03,857][129146] Min Reward on eval: 632.3638508319273
[37m[1m[2023-06-25 13:05:03,858][129146] Mean Reward across all agents: 1541.9391451529755
[37m[1m[2023-06-25 13:05:03,858][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:05:03,863][129146] mean_value=418.47764057705336, max_value=2267.4381040823414
[37m[1m[2023-06-25 13:05:03,865][129146] New mean coefficients: [[0.7336076  0.744751   1.1465094  0.03841603 2.9927158 ]]
[37m[1m[2023-06-25 13:05:03,866][129146] Moving the mean solution point...
[36m[2023-06-25 13:05:13,734][129146] train() took 9.87 seconds to complete
[36m[2023-06-25 13:05:13,735][129146] FPS: 389210.83
[36m[2023-06-25 13:05:13,737][129146] itr=1325, itrs=2000, Progress: 66.25%
[36m[2023-06-25 13:05:25,274][129146] train() took 11.51 seconds to complete
[36m[2023-06-25 13:05:25,275][129146] FPS: 333566.24
[36m[2023-06-25 13:05:30,102][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:05:30,103][129146] Reward + Measures: [[1904.14091491    0.18724333    0.39526132    0.50140065    0.25534901]]
[37m[1m[2023-06-25 13:05:30,103][129146] Max Reward on eval: 1904.1409149138547
[37m[1m[2023-06-25 13:05:30,103][129146] Min Reward on eval: 1904.1409149138547
[37m[1m[2023-06-25 13:05:30,103][129146] Mean Reward across all agents: 1904.1409149138547
[37m[1m[2023-06-25 13:05:30,103][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:05:35,560][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:05:35,565][129146] Reward + Measures: [[1445.08269128    0.24910001    0.40219998    0.51040006    0.29170001]
[37m[1m [1532.01387968    0.2067        0.44959998    0.48930001    0.3321    ]
[37m[1m [1790.96072493    0.2149        0.39859998    0.50749999    0.25369999]
[37m[1m ...
[37m[1m [1821.67690652    0.2167        0.43059999    0.49600002    0.27860001]
[37m[1m [1222.13035122    0.1998        0.55420005    0.48990002    0.44029999]
[37m[1m [1505.9439244     0.17019999    0.41820002    0.46549997    0.29530001]]
[37m[1m[2023-06-25 13:05:35,566][129146] Max Reward on eval: 1939.453970038588
[37m[1m[2023-06-25 13:05:35,566][129146] Min Reward on eval: 494.414909631724
[37m[1m[2023-06-25 13:05:35,566][129146] Mean Reward across all agents: 1392.3084249597068
[37m[1m[2023-06-25 13:05:35,567][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:05:35,573][129146] mean_value=131.87202059517256, max_value=1496.3569044139242
[37m[1m[2023-06-25 13:05:35,576][129146] New mean coefficients: [[ 0.8088562   2.0023232   2.8094726  -0.08284666  3.1643698 ]]
[37m[1m[2023-06-25 13:05:35,577][129146] Moving the mean solution point...
[36m[2023-06-25 13:05:45,394][129146] train() took 9.82 seconds to complete
[36m[2023-06-25 13:05:45,394][129146] FPS: 391227.22
[36m[2023-06-25 13:05:45,397][129146] itr=1326, itrs=2000, Progress: 66.30%
[36m[2023-06-25 13:05:56,962][129146] train() took 11.54 seconds to complete
[36m[2023-06-25 13:05:56,963][129146] FPS: 332723.31
[36m[2023-06-25 13:06:01,860][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:06:01,861][129146] Reward + Measures: [[1941.87986629    0.181613      0.41752735    0.49300098    0.26485798]]
[37m[1m[2023-06-25 13:06:01,861][129146] Max Reward on eval: 1941.8798662920622
[37m[1m[2023-06-25 13:06:01,861][129146] Min Reward on eval: 1941.8798662920622
[37m[1m[2023-06-25 13:06:01,861][129146] Mean Reward across all agents: 1941.8798662920622
[37m[1m[2023-06-25 13:06:01,862][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:06:07,588][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:06:07,588][129146] Reward + Measures: [[ 705.59926999    0.33460003    0.53029996    0.61780006    0.48639998]
[37m[1m [ 621.29522233    0.17900001    0.58410001    0.56990004    0.60440004]
[37m[1m [1809.21054891    0.1955        0.4138        0.50720006    0.27110001]
[37m[1m ...
[37m[1m [1380.77271547    0.23          0.47070003    0.59040004    0.35189998]
[37m[1m [1694.96372226    0.1736        0.45160004    0.49689999    0.33450001]
[37m[1m [ 686.43951246    0.06220001    0.74650002    0.67120004    0.75800002]]
[37m[1m[2023-06-25 13:06:07,588][129146] Max Reward on eval: 2000.7934828153113
[37m[1m[2023-06-25 13:06:07,589][129146] Min Reward on eval: 530.8872608538062
[37m[1m[2023-06-25 13:06:07,589][129146] Mean Reward across all agents: 1308.415828722731
[37m[1m[2023-06-25 13:06:07,589][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:06:07,597][129146] mean_value=521.6321440355479, max_value=1806.5471409405907
[37m[1m[2023-06-25 13:06:07,600][129146] New mean coefficients: [[ 0.22135991  3.4187102   2.6521199  -0.42076832  3.9536743 ]]
[37m[1m[2023-06-25 13:06:07,601][129146] Moving the mean solution point...
[36m[2023-06-25 13:06:17,360][129146] train() took 9.76 seconds to complete
[36m[2023-06-25 13:06:17,360][129146] FPS: 393570.42
[36m[2023-06-25 13:06:17,362][129146] itr=1327, itrs=2000, Progress: 66.35%
[36m[2023-06-25 13:06:28,852][129146] train() took 11.46 seconds to complete
[36m[2023-06-25 13:06:28,852][129146] FPS: 335026.88
[36m[2023-06-25 13:06:33,573][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:06:33,574][129146] Reward + Measures: [[1872.34578521    0.18683898    0.45231438    0.48680329    0.28946799]]
[37m[1m[2023-06-25 13:06:33,574][129146] Max Reward on eval: 1872.3457852068168
[37m[1m[2023-06-25 13:06:33,574][129146] Min Reward on eval: 1872.3457852068168
[37m[1m[2023-06-25 13:06:33,574][129146] Mean Reward across all agents: 1872.3457852068168
[37m[1m[2023-06-25 13:06:33,575][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:06:39,030][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:06:39,031][129146] Reward + Measures: [[1115.40994436    0.13780001    0.51310003    0.39790002    0.3915    ]
[37m[1m [1683.97218669    0.20200002    0.47620001    0.50760001    0.32720003]
[37m[1m [1497.17264695    0.1622        0.47089997    0.43600002    0.3513    ]
[37m[1m ...
[37m[1m [ 831.08273347    0.0523        0.59579998    0.48429999    0.53579998]
[37m[1m [ 692.96812508    0.0575        0.57880002    0.46610004    0.60079998]
[37m[1m [ 986.36393422    0.0722        0.58250004    0.4765        0.49920002]]
[37m[1m[2023-06-25 13:06:39,031][129146] Max Reward on eval: 1818.3479370835819
[37m[1m[2023-06-25 13:06:39,031][129146] Min Reward on eval: 261.1281689283322
[37m[1m[2023-06-25 13:06:39,031][129146] Mean Reward across all agents: 1118.1703213402316
[37m[1m[2023-06-25 13:06:39,032][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:06:39,036][129146] mean_value=-51.265593059840974, max_value=914.1302308003612
[37m[1m[2023-06-25 13:06:39,039][129146] New mean coefficients: [[-0.05974013  2.6353104   1.7790759   0.12412977  4.610972  ]]
[37m[1m[2023-06-25 13:06:39,040][129146] Moving the mean solution point...
[36m[2023-06-25 13:06:48,768][129146] train() took 9.73 seconds to complete
[36m[2023-06-25 13:06:48,768][129146] FPS: 394788.07
[36m[2023-06-25 13:06:48,771][129146] itr=1328, itrs=2000, Progress: 66.40%
[36m[2023-06-25 13:07:00,202][129146] train() took 11.41 seconds to complete
[36m[2023-06-25 13:07:00,203][129146] FPS: 336638.94
[36m[2023-06-25 13:07:04,978][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:07:04,978][129146] Reward + Measures: [[1762.35046053    0.18970467    0.47572336    0.48344165    0.32228166]]
[37m[1m[2023-06-25 13:07:04,979][129146] Max Reward on eval: 1762.3504605254748
[37m[1m[2023-06-25 13:07:04,979][129146] Min Reward on eval: 1762.3504605254748
[37m[1m[2023-06-25 13:07:04,979][129146] Mean Reward across all agents: 1762.3504605254748
[37m[1m[2023-06-25 13:07:04,979][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:07:10,490][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:07:10,495][129146] Reward + Measures: [[-283.64791278    0.1005        0.13160001    0.19726668    0.15496667]
[37m[1m [-159.04879897    0.15610337    0.22275625    0.31530544    0.22851716]
[37m[1m [ 917.12890319    0.122         0.33970001    0.33080003    0.31200001]
[37m[1m ...
[37m[1m [1321.16775056    0.14640002    0.38229999    0.4127        0.30820003]
[37m[1m [ 586.8283183     0.1393        0.3369        0.3707        0.32640001]
[37m[1m [ 433.61719025    0.12444916    0.32402879    0.31683388    0.31722376]]
[37m[1m[2023-06-25 13:07:10,496][129146] Max Reward on eval: 1742.182263457938
[37m[1m[2023-06-25 13:07:10,496][129146] Min Reward on eval: -743.0028526877344
[37m[1m[2023-06-25 13:07:10,496][129146] Mean Reward across all agents: 265.8247013310213
[37m[1m[2023-06-25 13:07:10,497][129146] Average Trajectory Length: 839.076
[36m[2023-06-25 13:07:10,499][129146] mean_value=-1136.2879285631777, max_value=1625.6038339469874
[37m[1m[2023-06-25 13:07:10,502][129146] New mean coefficients: [[0.3171421 3.7521787 1.1750243 0.7073986 2.2927365]]
[37m[1m[2023-06-25 13:07:10,503][129146] Moving the mean solution point...
[36m[2023-06-25 13:07:20,166][129146] train() took 9.66 seconds to complete
[36m[2023-06-25 13:07:20,166][129146] FPS: 397447.52
[36m[2023-06-25 13:07:20,169][129146] itr=1329, itrs=2000, Progress: 66.45%
[36m[2023-06-25 13:07:31,584][129146] train() took 11.39 seconds to complete
[36m[2023-06-25 13:07:31,585][129146] FPS: 337119.49
[36m[2023-06-25 13:07:36,392][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:07:36,392][129146] Reward + Measures: [[1603.67894876    0.19749467    0.52075666    0.49438533    0.36867964]]
[37m[1m[2023-06-25 13:07:36,392][129146] Max Reward on eval: 1603.6789487624696
[37m[1m[2023-06-25 13:07:36,392][129146] Min Reward on eval: 1603.6789487624696
[37m[1m[2023-06-25 13:07:36,393][129146] Mean Reward across all agents: 1603.6789487624696
[37m[1m[2023-06-25 13:07:36,393][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:07:41,922][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:07:41,922][129146] Reward + Measures: [[1044.94132302    0.26550001    0.46900001    0.64709997    0.42330003]
[37m[1m [1051.77629916    0.12730001    0.6825        0.54659998    0.60109997]
[37m[1m [1217.36232559    0.15140001    0.6049        0.56819999    0.48800001]
[37m[1m ...
[37m[1m [1145.85978052    0.186         0.49590001    0.45890003    0.40850002]
[37m[1m [ 780.58143858    0.1777        0.52389997    0.50520003    0.45620003]
[37m[1m [1119.5775144     0.1629        0.39229998    0.41169998    0.32160002]]
[37m[1m[2023-06-25 13:07:41,923][129146] Max Reward on eval: 1741.834813941829
[37m[1m[2023-06-25 13:07:41,923][129146] Min Reward on eval: -245.856543017691
[37m[1m[2023-06-25 13:07:41,923][129146] Mean Reward across all agents: 891.4683292620681
[37m[1m[2023-06-25 13:07:41,923][129146] Average Trajectory Length: 999.674
[36m[2023-06-25 13:07:41,927][129146] mean_value=-145.90726629286942, max_value=1456.0410495796991
[37m[1m[2023-06-25 13:07:41,930][129146] New mean coefficients: [[ 0.8996701   5.402157    0.8786208   1.3847525  -0.10906982]]
[37m[1m[2023-06-25 13:07:41,931][129146] Moving the mean solution point...
[36m[2023-06-25 13:07:51,680][129146] train() took 9.75 seconds to complete
[36m[2023-06-25 13:07:51,680][129146] FPS: 393952.79
[36m[2023-06-25 13:07:51,682][129146] itr=1330, itrs=2000, Progress: 66.50%
[37m[1m[2023-06-25 13:07:59,485][129146] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001310
[36m[2023-06-25 13:08:11,284][129146] train() took 11.58 seconds to complete
[36m[2023-06-25 13:08:11,285][129146] FPS: 331554.90
[36m[2023-06-25 13:08:15,979][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:08:15,980][129146] Reward + Measures: [[1706.86951181    0.22556368    0.501369      0.48435304    0.32439464]]
[37m[1m[2023-06-25 13:08:15,980][129146] Max Reward on eval: 1706.8695118082476
[37m[1m[2023-06-25 13:08:15,980][129146] Min Reward on eval: 1706.8695118082476
[37m[1m[2023-06-25 13:08:15,980][129146] Mean Reward across all agents: 1706.8695118082476
[37m[1m[2023-06-25 13:08:15,980][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:08:21,324][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:08:21,325][129146] Reward + Measures: [[1261.89835627    0.23639999    0.55739999    0.55970001    0.42739996]
[37m[1m [1344.04914984    0.23510002    0.53479999    0.57790005    0.39850003]
[37m[1m [ 774.11022111    0.24660002    0.67330009    0.52740002    0.60080004]
[37m[1m ...
[37m[1m [1323.53623036    0.20580001    0.50419998    0.60179996    0.36990002]
[37m[1m [1137.89705177    0.2534        0.47830001    0.64910001    0.40180001]
[37m[1m [1452.86403202    0.2464        0.45530006    0.54350001    0.32599998]]
[37m[1m[2023-06-25 13:08:21,325][129146] Max Reward on eval: 1749.70956585058
[37m[1m[2023-06-25 13:08:21,326][129146] Min Reward on eval: 546.3682689855981
[37m[1m[2023-06-25 13:08:21,326][129146] Mean Reward across all agents: 1222.8274013076964
[37m[1m[2023-06-25 13:08:21,326][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:08:21,332][129146] mean_value=294.7055655528562, max_value=1637.8970517650014
[37m[1m[2023-06-25 13:08:21,334][129146] New mean coefficients: [[ 1.3857596   6.5698986  -0.19218594  1.2992983  -0.12465915]]
[37m[1m[2023-06-25 13:08:21,335][129146] Moving the mean solution point...
[36m[2023-06-25 13:08:31,026][129146] train() took 9.69 seconds to complete
[36m[2023-06-25 13:08:31,027][129146] FPS: 396309.82
[36m[2023-06-25 13:08:31,029][129146] itr=1331, itrs=2000, Progress: 66.55%
[36m[2023-06-25 13:08:42,548][129146] train() took 11.49 seconds to complete
[36m[2023-06-25 13:08:42,548][129146] FPS: 334084.60
[36m[2023-06-25 13:08:47,344][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:08:47,344][129146] Reward + Measures: [[1793.75211351    0.23869699    0.48214126    0.47494733    0.292319  ]]
[37m[1m[2023-06-25 13:08:47,344][129146] Max Reward on eval: 1793.7521135116033
[37m[1m[2023-06-25 13:08:47,345][129146] Min Reward on eval: 1793.7521135116033
[37m[1m[2023-06-25 13:08:47,345][129146] Mean Reward across all agents: 1793.7521135116033
[37m[1m[2023-06-25 13:08:47,345][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:08:52,687][129146] Finished Evaluation Step
[37m[1m[2023-06-25 13:08:52,688][129146] Reward + Measures: [[1535.84211038    0.2823        0.58480006    0.49960002    0.34709999]
[37m[1m [1451.77192963    0.31810001    0.4691        0.52870005    0.27870002]
[37m[1m [1416.31971206    0.37180001    0.50439996    0.49120003    0.25510001]
[37m[1m ...
[37m[1m [1331.24724329    0.3585        0.4391        0.44619998    0.2395    ]
[37m[1m [1366.99598855    0.31800002    0.47890005    0.56269997    0.31619999]
[37m[1m [1253.85430114    0.34549999    0.62210006    0.48230001    0.3506    ]]
[37m[1m[2023-06-25 13:08:52,688][129146] Max Reward on eval: 1751.9538568071323
[37m[1m[2023-06-25 13:08:52,688][129146] Min Reward on eval: 394.0271587315307
[37m[1m[2023-06-25 13:08:52,689][129146] Mean Reward across all agents: 1361.607574365735
[37m[1m[2023-06-25 13:08:52,689][129146] Average Trajectory Length: 1000.0
[36m[2023-06-25 13:08:52,695][129146] mean_value=208.42718816997217, max_value=1401.6325945875146
[37m[1m[2023-06-25 13:08:52,698][129146] New mean coefficients: [[ 1.1544907   7.222546   -0.53771496  1.3471469   0.50196767]]
[37m[1m[2023-06-25 13:08:52,699][129146] Moving the mean solution point...