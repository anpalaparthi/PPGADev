ppo_normal_ant_seed_1111
feet contact, is_energy_measures =  False
is energy measures =  False
[36m[2023-09-12 16:59:43,482][487488] Environment: ant, obs_shape: (87,), action_shape: (8,)[0m
wandb: Currently logged in as: anishapv (qdrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/icaros/Documents/PPGADev/wandb/run-20230912_165944-854z3zap
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppo_normal_ant_seed_1111
wandb: â­ï¸ View project at https://wandb.ai/qdrl/PPGA
wandb: ğŸš€ View run at https://wandb.ai/qdrl/PPGA/runs/854z3zap
[36m[2023-09-12 17:00:07,345][487488] Avg FPS so far: 260852.78, env steps: 3840000, train time: 14.72, avg reward: -383.3691555786133[0m
[36m[2023-09-12 17:00:17,312][487488] Avg FPS so far: 311085.60, env steps: 7680000, train time: 24.69, avg reward: -78.95607348069548[0m
[36m[2023-09-12 17:00:27,075][487488] Avg FPS so far: 334390.55, env steps: 11520000, train time: 34.45, avg reward: 186.37842480659484[0m
[36m[2023-09-12 17:00:36,866][487488] Avg FPS so far: 347187.66, env steps: 15360000, train time: 44.24, avg reward: 515.4795753479004[0m
[36m[2023-09-12 17:00:46,790][487488] Avg FPS so far: 354466.40, env steps: 19200000, train time: 54.17, avg reward: 1078.9631684112549[0m
[36m[2023-09-12 17:00:56,622][487488] Avg FPS so far: 360011.61, env steps: 23040000, train time: 64.00, avg reward: 1763.4626161956787[0m
[36m[2023-09-12 17:01:06,486][487488] Avg FPS so far: 363926.70, env steps: 26880000, train time: 73.86, avg reward: 2169.636951904297[0m
[36m[2023-09-12 17:01:16,440][487488] Avg FPS so far: 366519.17, env steps: 30720000, train time: 83.82, avg reward: 2572.393236541748[0m
[36m[2023-09-12 17:01:26,508][487488] Avg FPS so far: 368117.42, env steps: 34560000, train time: 93.88, avg reward: 3243.2654248046874[0m
[36m[2023-09-12 17:01:36,453][487488] Avg FPS so far: 369841.77, env steps: 38400000, train time: 103.83, avg reward: 3717.8178100585938[0m
[36m[2023-09-12 17:01:46,424][487488] Avg FPS so far: 371178.89, env steps: 42240000, train time: 113.80, avg reward: 3950.451039428711[0m
[36m[2023-09-12 17:01:56,285][487488] Avg FPS so far: 372631.96, env steps: 46080000, train time: 123.66, avg reward: 3886.633416748047[0m
[36m[2023-09-12 17:02:06,071][487488] Avg FPS so far: 374081.49, env steps: 49920000, train time: 133.45, avg reward: 4399.3781854248045[0m
[36m[2023-09-12 17:02:15,890][487488] Avg FPS so far: 375246.86, env steps: 53760000, train time: 143.27, avg reward: 4700.911881103516[0m
[36m[2023-09-12 17:02:25,754][487488] Avg FPS so far: 376152.11, env steps: 57600000, train time: 153.13, avg reward: 5188.354602050781[0m
[36m[2023-09-12 17:02:35,771][487488] Avg FPS so far: 376594.88, env steps: 61440000, train time: 163.15, avg reward: 5250.4464501953125[0m
[36m[2023-09-12 17:02:45,721][487488] Avg FPS so far: 377129.66, env steps: 65280000, train time: 173.10, avg reward: 5515.790802001953[0m
[36m[2023-09-12 17:02:55,687][487488] Avg FPS so far: 377576.32, env steps: 69120000, train time: 183.06, avg reward: 5554.171624145508[0m
[36m[2023-09-12 17:03:05,650][487488] Avg FPS so far: 377980.99, env steps: 72960000, train time: 193.03, avg reward: 5643.121850585938[0m
[36m[2023-09-12 17:03:15,496][487488] Avg FPS so far: 378565.70, env steps: 76800000, train time: 202.87, avg reward: 5883.289468383789[0m
[36m[2023-09-12 17:03:25,376][487488] Avg FPS so far: 379033.89, env steps: 80640000, train time: 212.75, avg reward: 5979.762184448242[0m
[36m[2023-09-12 17:03:35,225][487488] Avg FPS so far: 379514.25, env steps: 84480000, train time: 222.60, avg reward: 6055.179248657227[0m
[36m[2023-09-12 17:03:45,041][487488] Avg FPS so far: 380007.04, env steps: 88320000, train time: 232.42, avg reward: 5974.083463134765[0m
[36m[2023-09-12 17:03:55,056][487488] Avg FPS so far: 380148.65, env steps: 92160000, train time: 242.43, avg reward: 6202.421881103515[0m
[36m[2023-09-12 17:04:04,967][487488] Avg FPS so far: 380435.80, env steps: 96000000, train time: 252.34, avg reward: 6281.67041015625[0m
[36m[2023-09-12 17:04:14,984][487488] Avg FPS so far: 380546.37, env steps: 99840000, train time: 262.36, avg reward: 6352.444600830078[0m
[36m[2023-09-12 17:04:24,932][487488] Avg FPS so far: 380746.10, env steps: 103680000, train time: 272.31, avg reward: 6598.301284179687[0m
[36m[2023-09-12 17:04:34,752][487488] Avg FPS so far: 381104.51, env steps: 107520000, train time: 282.13, avg reward: 6664.175483398438[0m
[36m[2023-09-12 17:04:44,666][487488] Avg FPS so far: 381315.08, env steps: 111360000, train time: 292.04, avg reward: 6699.5259765625[0m
[36m[2023-09-12 17:04:54,565][487488] Avg FPS so far: 381532.11, env steps: 115200000, train time: 301.94, avg reward: 6594.488487548828[0m
[36m[2023-09-12 17:05:04,343][487488] Avg FPS so far: 381882.69, env steps: 119040000, train time: 311.72, avg reward: 6686.992492675781[0m
[36m[2023-09-12 17:05:14,299][487488] Avg FPS so far: 382001.60, env steps: 122880000, train time: 321.67, avg reward: 6789.677412719727[0m
[36m[2023-09-12 17:05:24,210][487488] Avg FPS so far: 382163.38, env steps: 126720000, train time: 331.59, avg reward: 6611.256701660156[0m
[36m[2023-09-12 17:05:34,100][487488] Avg FPS so far: 382340.49, env steps: 130560000, train time: 341.48, avg reward: 6694.226273803711[0m
[36m[2023-09-12 17:05:43,993][487488] Avg FPS so far: 382504.80, env steps: 134400000, train time: 351.37, avg reward: 6738.536419067383[0m
[36m[2023-09-12 17:05:54,020][487488] Avg FPS so far: 382517.75, env steps: 138240000, train time: 361.39, avg reward: 6907.21025390625[0m
[36m[2023-09-12 17:06:03,908][487488] Avg FPS so far: 382672.75, env steps: 142080000, train time: 371.28, avg reward: 6960.385529785156[0m
[36m[2023-09-12 17:06:13,714][487488] Avg FPS so far: 382902.25, env steps: 145920000, train time: 381.09, avg reward: 6985.4717578125[0m
[36m[2023-09-12 17:06:23,498][487488] Avg FPS so far: 383141.49, env steps: 149760000, train time: 390.87, avg reward: 6973.270583496093[0m
[36m[2023-09-12 17:06:33,548][487488] Avg FPS so far: 383115.76, env steps: 153600000, train time: 400.92, avg reward: 6925.890181884765[0m
[36m[2023-09-12 17:06:43,417][487488] Avg FPS so far: 383259.20, env steps: 157440000, train time: 410.79, avg reward: 6983.868662109375[0m
[36m[2023-09-12 17:06:53,259][487488] Avg FPS so far: 383421.19, env steps: 161280000, train time: 420.63, avg reward: 6955.496502685547[0m
[36m[2023-09-12 17:07:03,154][487488] Avg FPS so far: 383527.61, env steps: 165120000, train time: 430.53, avg reward: 6970.967390136719[0m
[36m[2023-09-12 17:07:13,117][487488] Avg FPS so far: 383570.38, env steps: 168960000, train time: 440.49, avg reward: 6899.713726806641[0m
[36m[2023-09-12 17:07:23,007][487488] Avg FPS so far: 383673.68, env steps: 172800000, train time: 450.38, avg reward: 7102.462927246093[0m
[36m[2023-09-12 17:07:32,958][487488] Avg FPS so far: 383721.78, env steps: 176640000, train time: 460.33, avg reward: 7184.457742919922[0m
wandb: Waiting for W&B process to finish... (failed 15). Press Control-C to abort syncing.
wandb: - 0.003 MB of 0.021 MB uploaded (0.000 MB deduped)wandb: \ 0.021 MB of 0.021 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                 Average Episodic Reward â–â–â–â–‚â–‚â–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                Env step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                                     FPS â–â–„â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                  Update â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                 charts/actor_avg_logstd â–ˆâ–‡â–†â–†â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            charts/average_rew_magnitude â–â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                             global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                                 len/len â–â–ƒâ–‚â–‚â–„â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                             len/len_max â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                             len/len_min â–â–â–â–â–â–â–â–‚â–â–‚â–‚â–‚â–‚â–…â–‚â–„â–ƒâ–‚â–ƒâ–ƒâ–„â–‚â–‚â–ƒâ–…â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–ˆâ–‚â–‚â–ƒ
wandb:                        losses/approx_kl â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–ƒâ–„â–ƒâ–„â–„â–…â–„â–„â–„â–†â–ƒâ–„â–…â–…â–†â–…â–…â–…â–…â–„â–„â–„â–…â–‡â–…â–…â–…â–…â–ˆâ–†â–…
wandb:                         losses/clipfrac â–â–â–â–‚â–‚â–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–†â–†â–‡â–‡â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡
wandb:                          losses/entropy â–ˆâ–‡â–†â–†â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:               losses/explained_variance â–â–ƒâ–‡â–ˆâ–ˆâ–‡â–†â–…â–…â–„â–„â–…â–…â–…â–ƒâ–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚
wandb: losses/move_mean_agent=False/value_loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–‚
wandb:                    losses/old_approx_kl â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–ƒâ–…â–„â–…â–„â–…â–„â–†â–„â–„â–…â–…â–†â–…â–…â–…â–…â–„â–„â–„â–†â–‡â–…â–…â–†â–†â–ˆâ–†â–†
wandb:                      losses/policy_loss â–ƒâ–â–â–â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–„â–‚â–†â–„â–‡â–…â–…â–„â–†â–ƒâ–†â–…â–‡â–†â–…â–‡â–†â–†â–…â–…â–†â–†â–†â–…â–†â–†â–†â–ˆâ–‡â–†
wandb:                       losses/value_loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–‚
wandb:                               perf/_fps â–â–„â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                           reward/reward â–â–â–â–‚â–‚â–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                       reward/reward_max â–â–â–â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                       reward/reward_min â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–…â–ƒâ–„â–ƒâ–‚â–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–…â–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ƒâ–ƒâ–ƒ
wandb:                           train/act_max â–ˆâ–…â–†â–„â–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–â–‚â–â–â–‚â–â–‚â–‚â–â–‚â–‚â–â–â–‚
wandb:                           train/act_min â–â–„â–…â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆ
wandb:                           train/adv_max â–â–‚â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–„â–…â–…â–…â–…â–…â–…â–…â–…â–…â–„â–…â–ˆâ–…â–†â–†â–†â–‡â–ˆâ–†â–‡â–‡â–‡â–„â–…â–†â–…â–ˆ
wandb:                          train/adv_mean â–â–†â–ˆâ–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–…â–…â–†â–†â–†â–…â–†â–…â–†â–…â–†â–†â–…â–…â–„â–…â–†â–†â–†â–…â–†â–…â–†â–„
wandb:                           train/adv_min â–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                           train/adv_std â–â–â–â–â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                  train/obs_running_mean â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                   train/obs_running_std â–ˆâ–ˆâ–‡â–†â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                       train/policy_loss â–ƒâ–â–â–â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–„â–‚â–†â–„â–‡â–…â–…â–„â–†â–ƒâ–†â–…â–‡â–†â–…â–‡â–†â–†â–…â–…â–†â–†â–†â–…â–†â–†â–†â–ˆâ–‡â–†
wandb:                         train/ratio_max â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–‚â–„â–„â–„â–„â–…â–„â–„â–„â–„â–„â–ƒâ–ˆâ–ˆâ–„â–…â–…â–„â–‡â–†â–…â–„â–‡â–…â–†â–…â–ƒâ–ˆâ–…â–„â–…
wandb:                         train/ratio_min â–‡â–ˆâ–†â–‡â–…â–…â–ƒâ–„â–ƒâ–†â–‚â–…â–â–„â–…â–ƒâ–…â–„â–‚â–…â–ƒâ–â–„â–„â–…â–ƒâ–ƒâ–ƒâ–„â–„â–†â–‚â–ƒâ–ƒâ–„â–„â–â–‚â–ƒâ–ƒ
wandb:                             train/value â–‚â–â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–‡â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…
wandb:                        train/value_loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–‚
wandb: 
wandb: Run summary:
wandb:                 Average Episodic Reward 7184.45774
wandb:                                Env step 176640000
wandb:                                     FPS 383721.7813
wandb:                                  Update 460
wandb:                 charts/actor_avg_logstd -2.12796
wandb:            charts/average_rew_magnitude 7.19967
wandb:                             global_step 176640000
wandb:                                 len/len 990.48
wandb:                             len/len_max 1000.0
wandb:                             len/len_min 207.0
wandb:                        losses/approx_kl 0.02195
wandb:                         losses/clipfrac 0.31366
wandb:                          losses/entropy -0.70905
wandb:               losses/explained_variance 0.28229
wandb: losses/move_mean_agent=False/value_loss 0.08821
wandb:                    losses/old_approx_kl 0.02371
wandb:                      losses/policy_loss -0.00044
wandb:                       losses/value_loss 0.08821
wandb:                               perf/_fps 383721.7813
wandb:                           reward/reward 7184.45774
wandb:                       reward/reward_max 7417.55469
wandb:                       reward/reward_min 1360.74011
wandb:                           train/act_max 2.07422
wandb:                           train/act_min -1.72168
wandb:                           train/adv_max 417.45691
wandb:                          train/adv_mean -1.80102
wandb:                           train/adv_min -683.39496
wandb:                           train/adv_std 63.56629
wandb:                  train/obs_running_mean 0.08952
wandb:                   train/obs_running_std 0.59935
wandb:                       train/policy_loss -0.00044
wandb:                         train/ratio_max 3.36382
wandb:                         train/ratio_min 0.23546
wandb:                             train/value 0.85723
wandb:                        train/value_loss 0.08821
wandb: 
wandb: ğŸš€ View run ppo_normal_ant_seed_1111 at: https://wandb.ai/qdrl/PPGA/runs/854z3zap
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230912_165944-854z3zap/logs
